<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="or---130">OR - 130</h2>
<ul>
<li><details>
<summary>
(2023). Technical note—average cost optimality in partially
observable lost-sales inventory systems. <em>OR</em>, <em>71</em>(6),
2390–2396. (<a href="https://doi.org/10.1287/opre.2022.2305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a partially observable lost-sales inventory system, in which the inventory level is observed only when it reaches zero. We use the vanishing discount factor approach to prove the existence of a stationary optimal policy for the average cost minimization. As our main methodological contribution, we provide a way to verify the key condition of the vanishing discount factor approach—the uniform boundedness of the relative discounted value function. To accomplish that, we construct a valid policy, which, in a certain sense, “copies” the actions of another policy for the process with a different initial state. To the best of our knowledge, this paper is the first one on partially observable inventory models under the average cost criterion.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2305},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2390-2396},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Average cost optimality in partially observable lost-sales inventory systems},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—on the strength of relaxations of weakly
coupled stochastic dynamic programs. <em>OR</em>, <em>71</em>(6),
2374–2389. (<a href="https://doi.org/10.1287/opre.2022.2287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many stochastic dynamic programs (DPs) have a weakly coupled structure in that a set of linking constraints in each period couples an otherwise independent collection of subproblems. Two widely studied approximations of such problems are approximate linear programs (ALPs), which involve optimizing value function approximations that additively separate across subproblems, and Lagrangian relaxations, which involve relaxing the linking constraints. It is well known that both of these approximations provide upper bounds on the optimal value function in all states and that the ALP provides a tighter upper bound in the initial state. The purpose of this short paper is to provide theoretical justification for the fact that these upper bounds are often close if not identical. We show that (i) for any weakly coupled DP, the difference between these two upper bounds—the relaxation gap —is bounded from above in terms of the integrality gap of the separation problems associated with the ALP. (ii) If subproblem rewards are uniformly bounded and some broadly applicable conditions on the linking constraints hold, the relaxation gap is bounded from above by a constant that is independent of the number of subproblems. (iii) When the linking constraints are independent of subproblem states and have a unimodular structure, the relaxation gap equals zero. The conditions for (iii) hold in several widely studied problems: generalizations of restless bandit problems, online stochastic matching problems, network revenue management problems, and price-directed control of relocating resources. These findings generalize and unify existing results. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2287 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2287},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2374-2389},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On the strength of relaxations of weakly coupled stochastic dynamic programs},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced balancing of bias-variance tradeoff in stochastic
estimation: A minimax perspective. <em>OR</em>, <em>71</em>(6),
2352–2373. (<a href="https://doi.org/10.1287/opre.2022.2319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biased stochastic estimators, such as finite differences for noisy gradient estimation, often contain parameters that need to be properly chosen to balance impacts from the bias and the variance. Although the optimal order of these parameters in terms of the simulation budget can be readily established, the precise best values depend on model characteristics that are typically unknown in advance. We introduce a framework to construct new classes of estimators based on judicious combinations of simulation runs on sequences of tuning parameter values, such that the estimators consistently outperform a given tuning parameter choice in the conventional approach, regardless of the unknown model characteristics. We argue the outperformance via what we call the asymptotic minimax risk ratio, obtained by minimizing the worst-case asymptotic ratio between the mean square errors of our estimators and the conventional one, where the worst case is over any possible values of the model unknowns. In particular, when the minimax ratio is less than 1, the calibrated estimator is guaranteed to perform better asymptotically. We identify this minimax ratio for general classes of weighted estimators and the regimes where this ratio is less than 1. Moreover, we show that the best weighting scheme is characterized by a sum of two components with distinct decay rates. We explain how this arises from bias-variance balancing that combats the adversarial selection of the model constants, which can be analyzed via a tractable reformulation of a nonconvex optimization problem. Funding: This work was supported by the National Science Foundation, Division of Civil, Mechanical and Manufacturing Innovation (1523453, 1542020, 1834710). Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2319 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2319},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2352-2373},
  shortjournal = {Oper. Res.},
  title        = {Enhanced balancing of bias-variance tradeoff in stochastic estimation: A minimax perspective},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information retrieval under network uncertainty: Robust
internet ranking. <em>OR</em>, <em>71</em>(6), 2328–2351. (<a
href="https://doi.org/10.1287/opre.2022.2298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet ranking algorithms play a crucial role in information technologies and numerical analysis due to their efficiency in high dimensions and wide range of possible applications, including scientometrics and systemic risk in finance (SinkRank, DebtRank, etc.). The traditional approach to internet ranking goes back to the seminal work of Sergey Brin and Larry Page, who developed the initial method PageRank (PR) in order to rank websites in search engine results. Recent works have studied robust reformulations of the PageRank model for the case when links in the network structure may vary; that is, some links may appear or disappear influencing the transportation matrix defined by the network structure. We make a further step forward, allowing the network to vary not only in links, but also in the number of nodes. We focus on growing network structures and propose a new robust formulation of the PageRank problem for uncertain networks with fixed growth rate. Defining the robust PageRank in terms of a nonconvex optimization problem, we bound our formulation from above by a convex but nonsmooth optimization problem. Driven by the approximation quality, we analyze the resulting optimality gap theoretically and demonstrate cases for its reduction. In the numerical part of the article, we propose some techniques which allow us to obtain the solution efficiently for middle-size networks avoiding all nonsmooth points. Furthermore, we propose a coordinate-wise descent method with near-optimal step size and address high-dimensional cases using multinomial transition probabilities. We analyze the impact of the network growth on ranking and numerically assess the approximation quality using real-world data sets on movie repositories and on journals on computational complexity. Funding: This work was supported by the Austrian Science Fund [Grant J3674-N26]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.2298 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2298},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2328-2351},
  shortjournal = {Oper. Res.},
  title        = {Information retrieval under network uncertainty: Robust internet ranking},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On finite adaptability in two-stage distributionally robust
optimization. <em>OR</em>, <em>71</em>(6), 2307–2327. (<a
href="https://doi.org/10.1287/opre.2022.2273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real applications, practitioners prefer policies that are interpretable and easy to implement. This tendency is magnified in sequential decision-making settings. In this paper, we leverage the concept of finite adaptability to construct policies for two-stage optimization problems. More specifically, we focus on the general setting of distributional uncertainties affecting the right-hand sides of constraints, because in a broad range of applications, uncertainties do not affect the objective function and recourse matrix. The aim is to construct policies that have provable performance bounds. This is done by partitioning the uncertainty realization and assigning a contingent decision to each piece. We first show that the optimal partitioning can be characterized by translated orthants , which only require the problem structure and hence are free of modeling assumptions. We then prove that finding the optimal partitioning is hard and propose a specific partitioning scheme with orthants, allowing the efficient computation of orthant-based policies via solving a mixed-integer optimization problem of a moderate size. By leveraging the geometry of this partitioning, we provide performance bounds of the orthant-based policies, which also generalize the existing bounds in the literature. These bounds offer multiple theoretical insights on the performance, for example, its independence on problem parameters. We also assess suboptimality in more general settings and provide techniques to obtain lower bounds. The proposed policies are applied to a stylized inventory routing problem with mixed-integer recourse. We also study the case of a pharmacy retailer by comparing alternative methods regarding computational performance and robustness to parameter variation. Funding: E. Han is funded by the Southern Methodist University start-up fund for this research. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2273 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2273},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2307-2327},
  shortjournal = {Oper. Res.},
  title        = {On finite adaptability in two-stage distributionally robust optimization},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite-sample guarantees for wasserstein distributionally
robust optimization: Breaking the curse of dimensionality. <em>OR</em>,
<em>71</em>(6), 2291–2306. (<a
href="https://doi.org/10.1287/opre.2022.2326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wasserstein distributionally robust optimization (DRO) aims to find robust and generalizable solutions by hedging against data perturbations in Wasserstein distance. Despite its recent empirical success in operations research and machine learning, existing performance guarantees for generic loss functions are either overly conservative because of the curse of dimensionality or plausible only in large sample asymptotics. In this paper, we develop a nonasymptotic framework for analyzing the out-of-sample performance for Wasserstein robust learning and the generalization bound for its related Lipschitz and gradient regularization problems. To the best of our knowledge, this gives the first finite-sample guarantee for generic Wasserstein DRO problems without suffering from the curse of dimensionality. Our results highlight that Wasserstein DRO, with a properly chosen radius, balances between the empirical mean of the loss and the variation of the loss, measured by the Lipschitz norm or the gradient norm of the loss. Our analysis is based on two novel methodological developments that are of independent interest: (1) a new concentration inequality controlling the decay rate of large deviation probabilities by the variation of the loss and (2) a localized Rademacher complexity theory based on the variation of the loss. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2326 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2326},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2291-2306},
  shortjournal = {Oper. Res.},
  title        = {Finite-sample guarantees for wasserstein distributionally robust optimization: Breaking the curse of dimensionality},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The anchor-robust project scheduling problem. <em>OR</em>,
<em>71</em>(6), 2267–2290. (<a
href="https://doi.org/10.1287/opre.2022.2315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In project scheduling with uncertain processing times, the decision maker often needs to compute a baseline schedule in advance while guaranteeing that some jobs will not be rescheduled later. Standard robust approaches either produce a schedule with a very large makespan or offer no guarantee on starting times of the jobs. The concept of anchor-robustness is introduced as a middle ground between these approaches. A subset of jobs is said to be anchored if the starting times of its jobs in the baseline schedule can be guaranteed. The Anchor-Robust Project Scheduling Problem (AnchRobPSP) is proposed as a robust two-stage problem to find a baseline schedule of bounded makespan and a max-weight subset of anchored jobs. AnchRobPSP is considered for several uncertainty sets, such as box or budgeted uncertainty sets. Dedicated graph models are presented. In particular, the existence of a compact mixed integer programming reformulation for budgeted uncertainty is proven. AnchRobPSP is shown to be NP-hard even for budgeted uncertainty. Polynomial and pseudopolynomial algorithms are devised for box uncertainty and special cases of budgeted uncertainty. According to numerical results, the proposed approaches solve large-scale instances and outperform classical affine decisions rules for AnchRobPSP. Insights on the price of anchor-robustness are given based on computations.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2315},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2267-2290},
  shortjournal = {Oper. Res.},
  title        = {The anchor-robust project scheduling problem},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal subscription planning for digital goods.
<em>OR</em>, <em>71</em>(6), 2245–2266. (<a
href="https://doi.org/10.1287/opre.2023.2468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a media service provider that gives users access to digital goods through subscription. In our model, different types of users with heterogeneous usage rates repeatedly use a platform over a period of time. There are multiple item types on the platform, and the value of an item to a user is random and depends on both the user type and the item type. The design of the platform’s subscription planning comprises selecting a subscription fee for each set of item types. Before the beginning of the subscription period, given the subscription planning, users decide which set of item types to subscribe to (if any). During the subscription period, a user pays zero price to use an item if it has subscribed to a set that includes it and pays its rental price otherwise. For an exogenously given rental price, we establish the sufficient and necessary condition for the optimality of grand subscription—offering a single subscription set that includes all items. We then consider a setting in which the platform chooses both the subscription fee for each set of item types and the rental price of each item type and establish that there exist subscription fees proportional to the cardinality of each set of item types (with no rental offers) that achieve a logarithmic approximation (in the number of item types) of the optimal revenue. Finally, we demonstrate that our performance guarantee is tight for a class of problem instances. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2468 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2468},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2245-2266},
  shortjournal = {Oper. Res.},
  title        = {Optimal subscription planning for digital goods},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—optimizing risk-balancing return under
discrete choice models. <em>OR</em>, <em>71</em>(6), 2232–2244. (<a
href="https://doi.org/10.1287/opre.2023.2465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine a firm’s pricing decision when managing a broad product line with the goal of optimally balancing the expected return on product investment with the revenue or profit risk associated with uncertain customer choices. We consider the multinomial logit (MNL) model and the mean-variance objective function and illustrate how the level of risk tolerance influences the firm’s optimal markups. We show that the solution approach and results generalize to the nested logit (NL) choice model and alternative risk-adjusted objectives. This paper is the first in the literature addressing risk-sensitive pricing under discrete choice models. Our analysis presents how the firm’s optimal pricing decision evolves with increasing risk sensitivity. The optimal risk-balancing solution stands in contrast to the profit-maximizing solution: for example, (i) the firm’s distaste for risk not only causes the firm to discount its products in exchange for lower profit volatility but also reduces the firm’s incentive to price differentiate among the products; (ii) although risk drives the firm to attain a higher total market share through lower prices at the cost of lower profit, certain products may gain, whereas others may lose in market share; and (iii) the optimal risk-balancing markups follow the sequence of product quality, whereas the optimal adjusted markups (i.e., markups adjusted for product-specific price sensitivity) follow the reverse sequence of quality. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2023.2465 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2465},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2232-2244},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Optimizing risk-balancing return under discrete choice models},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scheduling advertising on cable television. <em>OR</em>,
<em>71</em>(6), 2217–2231. (<a
href="https://doi.org/10.1287/opre.2022.2430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement scheduling is a daily essential operational process in the television business. Efficient distribution of viewers among advertisers allows the television network to satisfy contracts and increase ad sale revenues. Ad scheduling is a challenging multiperiod, mixed-integer programming problem in which the network must create schedules to meet advertisers’ campaign goals and maximize ad revenues. Each campaign must meet a specific target group of viewers and a unique set of constraints. Moreover, the number of viewers is uncertain. To solve this problem, we develop and implement a practical approach that combines mathematical programming and machine learning to create daily schedules. These schedules are of high quality according to standard business metrics and the small integer programming gap. Leading networks in the United States and India using our methods experience a 3\%–5\% revenue increase. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2430 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2430},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2217-2231},
  shortjournal = {Oper. Res.},
  title        = {Scheduling advertising on cable television},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revenue management with heterogeneous resources: Unit
resource capacities, advance bookings, and itineraries over time
intervals. <em>OR</em>, <em>71</em>(6), 2196–2216. (<a
href="https://doi.org/10.1287/opre.2022.2427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study revenue management problems with heterogeneous resources, each with unit capacity. An arriving customer makes a booking request for a particular interval of days in the future. We offer an assortment of resources in response to each booking request. The customer makes a choice within the assortment to use the chosen resource for her desired interval of days. The goal is to find a policy that determines an assortment of resources to offer to each customer to maximize the total expected revenue over a finite selling horizon. The problem has two useful features. First, each resource is unique with unit capacity. Second, each customer uses the chosen resource for a number of consecutive days. We consider static policies that offer each assortment of resources with a fixed probability. We show that we can efficiently perform rollout on any static policy, allowing us to build on any static policy and construct an even better policy. Next, we develop two static policies, each of which is derived from linear and polynomial approximations of the value functions. We give performance guarantees for both policies, so the rollout policies based on these static policies inherit the same guarantee. Last, we develop an approach for computing an upper bound on the optimal total expected revenue. Our results for efficient rollout, static policies, and upper bounds all exploit the aforementioned two useful features of our problem. We use our model to manage hotel bookings based on a data set from a real-world boutique hotel, demonstrating that our rollout approach can provide remarkably good policies and our upper bounds can significantly improve those provided by existing techniques. Funding: This work was supported by the National Science Foundation [Grants CMMI-1824860 and CMMI-1825406]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.2427 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2427},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2196-2216},
  shortjournal = {Oper. Res.},
  title        = {Revenue management with heterogeneous resources: Unit resource capacities, advance bookings, and itineraries over time intervals},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From proper scoring rules to max-min optimal forecast
aggregation. <em>OR</em>, <em>71</em>(6), 2175–2195. (<a
href="https://doi.org/10.1287/opre.2022.2414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper forges a strong connection between two seemingly unrelated forecasting problems: incentive-compatible forecast elicitation and forecast aggregation. Proper scoring rules are the well-known solution to the former problem. To each such rule s , we associate a corresponding method of aggregation, mapping expert forecasts and expert weights to a “consensus forecast,” which we call quasi-arithmetic (QA) pooling with respect to s . We justify this correspondence in several ways: QA pooling with respect to the two most well-studied scoring rules (quadratic and logarithmic) corresponds to the two most well-studied forecast aggregation methods (linear and logarithmic); given a scoring rule s used for payment, a forecaster agent who subcontracts several experts, paying them in proportion to their weights, is best off aggregating the experts’ reports using QA pooling with respect to s , meaning this strategy maximizes its worst-case profit (over the possible outcomes); the score of an aggregator who uses QA pooling is concave in the experts’ weights (as a consequence, online gradient descent can be used to learn appropriate expert weights from repeated experiments with low regret); and the class of all QA pooling methods is characterized by a natural set of axioms (generalizing classical work by Kolmogorov on quasi-arithmetic means). Funding: This work was supported by the Division of Computing and Communication Foundations [Grant CCF-1813188], the Army Research Office [Grant W911NF1910294], and the Division of Graduate Education [Grant DGE-2036197]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2414 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2414},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2175-2195},
  shortjournal = {Oper. Res.},
  title        = {From proper scoring rules to max-min optimal forecast aggregation},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Customized dynamic pricing when customers develop a habit or
satiation. <em>OR</em>, <em>71</em>(6), 2158–2174. (<a
href="https://doi.org/10.1287/opre.2022.2412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a dynamic pricing problem in which a firm chooses prices over multiple periods when consumers are state dependent; that is, they develop a habit or satiation from their past consumption. We first derive an intertemporal demand function to capture how demand in one period depends on the price in that period and consumption in previous periods through habit or satiation. Subsequently, we formulate the optimal price setting problem for a firm over a multiperiod horizon. We establish that this problem is jointly concave in prices and then characterize the temporal trends in the optimal prices. These trends in optimal prices are a net outcome of two opposite effects: (i) the progressive buildup of habit or satiation from consumption, and (ii) the progressive deterioration of the habit or satiation developed in prior periods. Based on the relative strengths of these two effects, the optimal prices follow a penetration policy (prices increase over time), a skimming policy (prices decrease over time), a skimming-penetration policy (U-shaped prices), or a penetration-skimming policy (inverse U-shaped prices). Subsequently, we provide several extensions including bounds on prices and optimal profit and nonstationary state dependence. Numerical studies show that ignoring habit and satiation effects in customers can be significantly costly for a firm. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.2412 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2412},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2158-2174},
  shortjournal = {Oper. Res.},
  title        = {Customized dynamic pricing when customers develop a habit or satiation},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—data-driven profit estimation error in the
newsvendor model. <em>OR</em>, <em>71</em>(6), 2146–2157. (<a
href="https://doi.org/10.1287/opre.2023.0070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this note, we identify a statistically significant error in naively estimating the expected profit in a data-driven newsvendor model, and we show how to correct the error. In particular, we analyze a newsvendor model where the continuous demand distribution is not known, and only a sample of demand data is available. In this context, an empirical demand distribution, that is induced by the sample of data, is used in place of the (unknown) true distribution. The quantity at the critical percentile 1 − c / p of the empirical distribution is known as the sample average approximation order quantity, where p is the unit revenue and c the unit cost. We prove that, if the empirical distribution is used to estimate the expected profit, this estimate exhibits a positive, statistically significant bias. We derive a closed-form expression for this bias that only depends on p and c and the sample of data. The bias expression can then be used to design an adjusted expected profit estimate, which we prove is asymptotically unbiased. Numerical hypothesis testing experiments confirm that the unadjusted estimation error is statistically significant, whereas the adjusted estimation error is not significantly different from zero. The bias is not negligible in our numerical experiments: For lognormally and normally distributed demand, the unadjusted error is 2.4\% and 3.0\% of the true expected profit, respectively. A more detailed exploration with exact finite-sample results for exponentially distributed demand demonstrates that the estimation error percentage can be much larger. Funding: A. F. Siegel gratefully acknowledges the support of the Grant I. Butterbaugh Professorship. M. R. Wagner gratefully acknowledges the support of a Neal and Jan Dempsey Faculty Fellowship. Both are from the Michael G. Foster School of Business, University of Washington. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0070 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0070},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2146-2157},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Data-driven profit estimation error in the newsvendor model},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computation of systemic risk measures: A mixed-integer
programming approach. <em>OR</em>, <em>71</em>(6), 2130–2145. (<a
href="https://doi.org/10.1287/opre.2021.0040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systemic risk is concerned with the instability of a financial system whose members are interdependent in the sense that the failure of a few institutions may trigger a chain of defaults throughout the system. Recently, several systemic risk measures have been proposed in the literature that are used to determine capital requirements for the members subject to joint risk considerations. We address the problem of computing systemic risk measures for systems with sophisticated clearing mechanisms. In particular, we consider an extension of the Rogers–Veraart network model where the operating cash flows are unrestricted in sign. We propose a mixed-integer programming problem that can be used to compute clearing vectors in this model. Because of the binary variables in this problem, the corresponding (set-valued) systemic risk measure fails to have convex values in general. We associate nonconvex vector optimization problems with the systemic risk measure and provide theoretical results related to the weighted-sum and Pascoletti–Serafini scalarizations of this problem. Finally, we test the proposed formulations on computational examples and perform sensitivity analyses with respect to some model-specific and structural parameters. Funding: N. Meimanjan acknowledges support from the Oesterreichische Nationalbank [Project 17793] and the Austrian Science Fund [Project W1260-N35]. Supplemental Material: The e-companion and data files are available at https://doi.org/10.1287/opre.2021.0040 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0040},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2130-2145},
  shortjournal = {Oper. Res.},
  title        = {Computation of systemic risk measures: A mixed-integer programming approach},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven hospital admission control: A learning approach.
<em>OR</em>, <em>71</em>(6), 2111–2129. (<a
href="https://doi.org/10.1287/opre.2020.0481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The choice of care unit upon admission to the hospital is a challenging task because of the wide variety of patient characteristics, uncertain needs of patients, and limited number of beds in intensive and intermediate care units. The care unit placement decisions involve capturing the trade-off between the benefit of better health outcomes versus the opportunity cost of reserving higher level of care beds for potentially more complex patients arriving in the future. By focusing on reducing the readmission risk of patients, we develop an online algorithm for care unit placement under the presence of limited reusable hospital beds. The algorithm is designed to (i) adaptively learn the readmission risk of patients through batch learning with delayed feedback and (ii) choose the best care unit placement for a patient based on the observed information and the occupancy level of the care units. We prove that our online algorithm admits a Bayesian regret bound. We also investigate and assess the effectiveness of our methodology using hospital system data. Our numerical experiments demonstrate that our methodology outperforms different benchmark policies. Funding: This work was supported by the National Science Foundation [Grant CMMI 1548201]. C. Shi was supported by Amazon Research Award. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2020.0481 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0481},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2111-2129},
  shortjournal = {Oper. Res.},
  title        = {Data-driven hospital admission control: A learning approach},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inventory control and learning for one-warehouse multistore
system with censored demand. <em>OR</em>, <em>71</em>(6), 2092–2110. (<a
href="https://doi.org/10.1287/opre.2021.0694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by our collaboration with one of the largest fast-fashion retailers in Europe, we study a two-echelon inventory control problem called the one-warehouse multistore (OWMS) problem when the demand distribution is unknown. This system has a central warehouse that receives an initial replenishment and distributes its inventory to multiple stores in each time period during a finite horizon. The goal is to minimize the total expected cost, which consists of shipment, holding, lost-sales, and end-of-horizon disposal costs. The OWMS system is ubiquitous in supply chain management, yet its optimal policy is notoriously difficult to calculate even under the complete demand distribution case. In this work, we consider the OWMS problem when the demand is censored and its distribution is unknown a priori. The main challenge under the censored demand case is the difficulty in generating unbiased demand estimation. In order to address this, we propose a primal-dual algorithm in which we continuously learn the demand and make inventory control decisions on the fly. Results show that our approach has great theoretical and empirical performances. Funding: The work of M. Gümüş was supported in part by research grants from the Natural Sciences and Engineering Research Council of Canada [Grant 217601 NSERC RGPIN-2019-06091]; and the Institute for Data Valorization [IVADO G254088]. The work of S. Miao was supported by the Strategic Management Society Strategy Research Foundation [Grant SRF-2015DP-0063]; the Social Science and Humanities Research Council of Canada [Grant 752-2014-0378]; and the Natural Sciences and Engineering Research Council of Canada [Grant G259160 NSERC RGPIN-2022-03247]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0694 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0694},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2092-2110},
  shortjournal = {Oper. Res.},
  title        = {Inventory control and learning for one-warehouse multistore system with censored demand},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Portfolio selection, periodic evaluations and risk taking.
<em>OR</em>, <em>71</em>(6), 2078–2091. (<a
href="https://doi.org/10.1287/opre.2021.0780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a continuous-time portfolio selection problem faced by an agent with S-shaped preference who maximizes the utilities derived from the portfolio’s periodic performance over an infinite horizon. The periodic reward structure creates subtle incentive distortion. In some cases, local risk aversion is induced, which discourages the agent from risk taking in the extreme bad states of the world. In some other cases, eventual ruin of the portfolio is inevitable, and the agent underinvests in the good states of the world to manipulate the basis of subsequent performance evaluations. We outline several important elements of incentive design to contain the long-term portfolio risk. Funding: H. Zheng is supported in part by the Engineering and Physical Sciences Research Council [Grant EP/V00833/1]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0780 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0780},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2078-2091},
  shortjournal = {Oper. Res.},
  title        = {Portfolio selection, periodic evaluations and risk taking},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymptotic optimality of semi-open-loop policies in markov
decision processes with large lead times. <em>OR</em>, <em>71</em>(6),
2061–2077. (<a href="https://doi.org/10.1287/opre.2021.0088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a generic Markov decision process (MDP) with two controls: one control taking effect immediately and the other control whose effect is delayed by a positive lead time. As the lead time grows, one naturally expects that the effect of the delayed action only weakly depends on the current state, and decoupling the delayed action from the current state could provide good controls. The purpose of this paper is to substantiate this decoupling intuition by establishing asymptotic optimality of semi-open-loop policies, which specify open-loop controls for the delayed action and closed-loop controls for the immediate action. For MDPs defined on general spaces with uniformly bounded cost functions and a fast mixing property, we construct a periodic semi-open-loop policy for each lead time value and show that these policies are asymptotically optimal as the lead time goes to infinity. For MDPs defined on Euclidean spaces with linear dynamics and convex structures (convex cost functions and constraint sets), we impose another set of conditions under which semi-open-loop policies (actually, constant delayed-control policies) are asymptotically optimal. Moreover, we verify that these conditions hold for a broad class of inventory models, in which there are multiple controls with nonidentical lead times. Funding: Research of the first three authors was partly supported by the National Science Foundation [Grant CMMI-1635160]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0088 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0088},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2061-2077},
  shortjournal = {Oper. Res.},
  title        = {Asymptotic optimality of semi-open-loop policies in markov decision processes with large lead times},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ADMM-based distributed optimization method for solving
security-constrained alternating current optimal power flow.
<em>OR</em>, <em>71</em>(6), 2045–2060. (<a
href="https://doi.org/10.1287/opre.2023.2486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study efficient and robust computational methods for solving the security-constrained alternating current optimal power flow (SC-ACOPF) problem, a two-stage nonlinear optimization problem with disjunctive constraints, that is central to the operation of electric power grids. The first-stage problem in SC-ACOPF determines the operation of the power grid in normal condition, whereas the second-stage problem responds to various contingencies of losing generators, transmission lines, and transformers. The two stages are coupled through disjunctive constraints, which model generators’ active and reactive power output changes responding to system-wide active power imbalance and voltage deviations after contingencies. Real-world SC-ACOPF problems may involve power grids with more than 30,000 buses and 22,000 contingencies and need to be solved within 10–45 minutes to get a base case solution with high feasibility and reasonably good generation cost. We develop a comprehensive algorithmic framework to solve SC-ACOPF that meets the challenge of speed, solution quality, and computation robustness. In particular, we develop a smoothing technique to approximate disjunctive constraints by a smooth structure that can be handled by interior-point solvers; we design a distributed optimization algorithm to efficiently generate first-stage solutions; we propose a screening procedure to prioritize contingencies; and finally, we develop a reliable and parallel computation architecture that integrates all algorithmic components. Extensive tests on industry-scale systems demonstrate the superior performance of the proposed algorithms. History: This paper has been accepted for the Operations Research Special Issue on Computational Advances in Short-Term Power System Operations. Funding: The authors acknowledge the continued support of ARPA-E [Grant DE-AR0001089] and the National Science Foundation [Grant 1751747]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2486 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2486},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2045-2060},
  shortjournal = {Oper. Res.},
  title        = {An ADMM-based distributed optimization method for solving security-constrained alternating current optimal power flow},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A decomposition algorithm with fast identification of
critical contingencies for large-scale security-constrained AC-OPF.
<em>OR</em>, <em>71</em>(6), 2031–2044. (<a
href="https://doi.org/10.1287/opre.2023.2453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A decomposition algorithm for solving large-scale security-constrained AC optimal power flow problems is presented. The formulation considered is the one used in the Advanced Research Projects Agency-Energy Grid Optimization Competition, Challenge 1, held from November 2018 through October 2019. Algorithmic strategies are proposed for contingency selection, fast contingency evaluation, handling complementarity constraints, avoiding issues related to degeneracy, and exploiting parallelism. The results of numerical experiments are provided to demonstrate the effectiveness of the proposed techniques as compared with alternative strategies. History: This paper has been accepted for the Operations Research Special Issue on Computational Advances in Short-Term Power System Operations. Funding: This work was supported by the Advanced Research Projects Agency-Energy [Grant DE-AR0001073].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2453},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2031-2044},
  shortjournal = {Oper. Res.},
  title        = {A decomposition algorithm with fast identification of critical contingencies for large-scale security-constrained AC-OPF},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A surrogate-based asynchronous decomposition technique for
realistic security-constrained optimal power flow problems. <em>OR</em>,
<em>71</em>(6), 2015–2030. (<a
href="https://doi.org/10.1287/opre.2022.0229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a decomposition approach for obtaining good feasible solutions for the security-constrained, alternating-current, optimal power flow (SC-AC-OPF) problem at an industrial scale and under real-world time and computational limits. The approach was designed while preparing and participating in ARPA-E’s Grid Optimization Competition (GOC) Challenge 1. The challenge focused on a near-real-time version of the SC-AC-OPF problem, where a base operating point is optimized, taking into account possible single-element contingencies, after which the system adapts its operating point following the response of automatic frequency droop controllers and voltage regulators. Our solution approach for this problem relies on state-of-the-art nonlinear programming algorithms, and it employs nonconvex relaxations for complementarity constraints, a specialized two-stage decomposition technique with sparse approximations of recourse terms and contingency ranking and prescreening. The paper describes and justifies our approach and outlines the features of its implementation, including functions and derivatives evaluation, warm-starting strategies, and asynchronous parallelism. We discuss the results of the independent benchmark of our approach by ARPA-E’s GOC team in Challenge 1, where it was found to consistently produce high-quality solutions across a wide range of network sizes and difficulty, and conclude by outlining future extensions of the approach. History: This paper has been accepted for the Operations Research Special Issue on Computational Advances in Short-Term Power System Operations. Funding: This work was performed under the auspices of the U.S. Department of Energy by the Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. This research was supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0229},
  journal      = {Operations Research},
  number       = {6},
  pages        = {2015-2030},
  shortjournal = {Oper. Res.},
  title        = {A surrogate-based asynchronous decomposition technique for realistic security-constrained optimal power flow problems},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recent developments in security-constrained AC optimal power
flow: Overview of challenge 1 in the ARPA-e grid optimization
competition. <em>OR</em>, <em>71</em>(6), 1997–2014. (<a
href="https://doi.org/10.1287/opre.2022.0315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimal power-flow problem is central to many tasks in the design and operation of electric power grids. This problem seeks the minimum-cost operating point for an electric power grid while satisfying both engineering requirements and physical laws describing how power travels through the electric network. By additionally considering the possibility of component failures and using an accurate alternating current (AC) power-flow model of the electric network, the security-constrained AC optimal power flow (SC-AC-OPF) problem is of paramount practical relevance. To assess recent progress in solution algorithms for SC-AC-OPF problems and spur new innovations, the U.S. Department of Energy’s Advanced Research Projects Agency–Energy organized Challenge 1 of the Grid Optimization (GO) competition. This special issue includes papers authored by the top three teams in Challenge 1 of the GO Competition (Teams gollnlp, GO-SNIP, and GMI-GO). To introduce these papers and provide context about the competition, this paper describes the SC-AC-OPF problem formulation used in the competition, overviews historical developments and the state of the art in SC-AC-OPF algorithms, discusses the competition, and summarizes the algorithms used by these three teams. History: This paper has been accepted for the Operations Research Special Issue on Computational Advances in Short-Term Power System Operations. Funding: I. Aravena and C. G. Petra contributed to this work under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344, with support of the Exascale Computing Project [17-SC-20-SC], a collaborative effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration. The contributions of D. K. Molzhan, F. E. Curtis, S. Tu, A. Wächter, E. Wei, and E. Wong were supported by the Advanced Research Projects Agency-Energy (ARPA-E), U.S. Department of Energy [Grant DE-AR0001073]. X. Andy Sun acknowledges the continued support of ARPA-E [Award DE-AR0001089] and the National Science Foundation [Award 1751747]. The Pacific Northwest National Laboratory (PNNL) information, data, or work presented herein was funded in part by ARPA-E under Award 13/CJ000/09/03. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2022.0315 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0315},
  journal      = {Operations Research},
  number       = {6},
  pages        = {1997-2014},
  shortjournal = {Oper. Res.},
  title        = {Recent developments in security-constrained AC optimal power flow: Overview of challenge 1 in the ARPA-E grid optimization competition},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization formulations for storage devices with disjoint
operating modes. <em>OR</em>, <em>71</em>(6), 1978–1996. (<a
href="https://doi.org/10.1287/opre.2023.2482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a storage device, such as a pumped storage hydroelectric generator, that has a state of charge together with mutually exclusive and disjoint charging and generating modes. We develop valid inequalities for a storage model that uses binary variables to represent the charging and generating modes. To investigate the model, we consider two contexts, stand-alone and large-scale. The stand-alone context involves the hydroelectric generator purchasing or selling electricity based on known or forecast prices. We consider properties of an optimization formulation with an objective that evaluates the profit from the sale of net generation and value of stored energy, present conditions for the optimum of the continuous relaxation of this optimization formulation to have binary values for the charging and generation commitment variables, and demonstrate the result numerically with a small example system. Analysis of the stand-alone context helps to explain why the combination of features in the storage model results in a difficult problem. The large-scale context embeds the model into a unit commitment and dispatch formulation for multiple generators. For several large-scale test cases, we numerically verify that the valid inequalities can improve the computation compared with the standard model in the literature. History: This paper has been accepted for the Operations Research Special Issue on Computational Advances in Short-Term Power System Operations. Funding: This work was supported by U.S. Department of Energy’s Office of Energy Efficiency and Renewable Energy (EERE) under the Water Power Technologies Office [Grant DE-EE0008781]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2482 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2482},
  journal      = {Operations Research},
  number       = {6},
  pages        = {1978-1996},
  shortjournal = {Oper. Res.},
  title        = {Optimization formulations for storage devices with disjoint operating modes},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unit commitment problem with energy storage under correlated
renewables uncertainty. <em>OR</em>, <em>71</em>(6), 1960–1977. (<a
href="https://doi.org/10.1287/opre.2021.0211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensive integration of renewable generation in electricity systems is significantly increasing the variability and correlation in power availability and the need for energy storage capacity. This increased uncertainty and storage capacity should be considered in operational decisions such as the short-term unit commitment (UC) problem. In this work, we formulate a day-ahead UC problem with energy storage, considering multistage correlated uncertainty on renewables’ power availability. We solve this multistage stochastic unit commitment (MSUC) problem with integer variables in the first stage using a new variant of SDDP that can explicitly deal with temporal correlations. Our computational results on the IEEE 118-bus system demonstrate the significance of considering multistage uncertainty and correlations, comparing our solution with other multistage solutions, two-stage solutions, and deterministic solutions typically used by industry. We also solve the MSUC problem for a representation of the Chilean power system, finding superior UC solutions for scenarios where adapting generation to the unfolding uncertainty is costly. Finally, we demonstrate that the MSUC approach can be used to define a more efficient deterministic UC solution, outperforming the current industry practice. History: This article is part of the Operations Research Special Issue on Computational Advances in Short-Term Power System Operation. Funding: This work was supported by the Instituto de Sistemas Complejos de Ingeniería [Grants ANID AFB 220003, FONDECYT 1201844, and FONDECYT 1231924].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0211},
  journal      = {Operations Research},
  number       = {6},
  pages        = {1960-1977},
  shortjournal = {Oper. Res.},
  title        = {Unit commitment problem with energy storage under correlated renewables uncertainty},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the structure of decision diagram–representable
mixed-integer programs with application to unit commitment. <em>OR</em>,
<em>71</em>(6), 1943–1959. (<a
href="https://doi.org/10.1287/opre.2022.2353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, decision diagrams (DDs) have been used to model and solve integer programming and combinatorial optimization problems. Despite successful performance of DDs in solving various discrete optimization problems, their extension to model mixed-integer programs (MIPs), such as those appearing in energy applications, is lacking. More broadly, the question of which problem structures admit a DD representation is still open in the DD community. In this paper, we address this question by introducing a geometric decomposition framework based on rectangular formations that provides both necessary and sufficient conditions for a general MIP to be representable by DDs. As a special case, we show that any bounded mixed-integer linear program admits a DD representation through a specialized Benders decomposition technique. The resulting DD encodes both integer and continuous variables and, therefore, is amenable to the addition of feasibility and optimality cuts through refinement procedures. As an application for this framework, we develop a novel solution methodology for the unit commitment problem (UCP) in the wholesale electricity market. Computational experiments conducted on a stochastic variant of the UCP show a significant improvement of the solution time for the proposed method when compared with the outcome of modern solvers. History: This paper has been accepted for the Operations Research Special Issue on Computational Advances in Short-Term Power System Operations. Funding: This work was supported by the Iowa Economic Development Authority [Grant 20IEC005]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2353 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2353},
  journal      = {Operations Research},
  number       = {6},
  pages        = {1943-1959},
  shortjournal = {Oper. Res.},
  title        = {On the structure of decision Diagram–Representable mixed-integer programs with application to unit commitment},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pricing under uncertainty in multi-interval real-time
markets. <em>OR</em>, <em>71</em>(6), 1928–1942. (<a
href="https://doi.org/10.1287/opre.2022.2314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has demonstrated that real-time auctions can generate the need for side payments, even if the market clearing models are convex, because of the rolling nature of real-time market clearing. This observation has inspired proposals for modifying the real-time market-clearing model in order to account for binding past decisions. We extend this analysis in order to account for uncertainty by proposing a real-time market-clearing model with look-ahead and an endogenous representation of uncertainty. We define two different types of expected lost opportunity cost as performance metrics. Our market-clearing model provides the price signal minimizing one of these metrics using the Stochastic Gradient Descent algorithm. We present results from a case study of the ISO New England system under a scenario of significant renewable energy penetration while accounting for ramp rates, storage, and transmission constraints. History: This paper has been accepted for the Operations Research Special Issue on Computational Advances in Short-Term Power System Operations. Funding: This work has received funding from the European Research Council (ERC) under the European Union Horizon 2020 research and innovation program [Grant agreement 850540]; FEVER under Horizon 2020 [Grant 864537]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2314 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2314},
  journal      = {Operations Research},
  number       = {6},
  pages        = {1928-1942},
  shortjournal = {Oper. Res.},
  title        = {Pricing under uncertainty in multi-interval real-time markets},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preface to special issue on computational advances in
short-term power system operations. <em>OR</em>, <em>71</em>(6),
1925–1927. (<a href="https://doi.org/10.1287/opre.intro.v71.n6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {History: This paper has been accepted for the Operations Research Special Issue on Computational Advances in Short-Term Power System Operations.},
  archive      = {J_OR},
  doi          = {10.1287/opre.intro.v71.n6},
  journal      = {Operations Research},
  number       = {6},
  pages        = {1925-1927},
  shortjournal = {Oper. Res.},
  title        = {Preface to special issue on computational advances in short-term power system operations},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—optimal procurement in remanufacturing
systems with uncertain used-item condition. <em>OR</em>, <em>71</em>(5),
iii–vii. (<a href="https://doi.org/10.1287/opre.2023.2483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a single-product remanufacture-to-order system with multiple uncertain quality levels for used items, random procurement lead times, and lost sales. The quality level of a used item is revealed only after it is acquired and inspected; the remanufacturing cost is lower for a higher-quality item. We model this system as a Markov decision process and seek an optimal policy that specifies when a used item should be procured, whether an arriving demand for the remanufactured product should be satisfied, and which available item should be remanufactured to meet this demand. We characterize the optimal procurement policy as following a new type of strategy: state-dependent noncongestive acquisition. This strategy makes decisions, taking into account the system congestion level measured as the number of available items and their quality levels. We also show that it is always optimal to meet the demand with the highest-quality item among the available ones. We conclude with extensions of our model to limited cases when the used-item condition is known a priori (for two quality levels) and remanufacture-to-stock systems in which the standard push strategy is optimal in the remanufacturing stage. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2483 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2483},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Optimal procurement in remanufacturing systems with uncertain used-item condition},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—the complexity of the pricing problem of the
set partitioning formulation of vehicle routing problems. <em>OR</em>,
<em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2023.2481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art exact algorithms for vehicle routing problems are branch-price-and-cut algorithms that make use of a set partitioning formulation. Only exponential time algorithms are known for the corresponding pricing problem. In this technical note, it is proven that the pricing problem is strongly NP-hard for various vehicle routing problems. This justifies the common use of route relaxations that modify the set partitioning formulation with the aim to allow pseudopolynomial pricing algorithms at the expense of a weaker LP bound.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2481},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—The complexity of the pricing problem of the set partitioning formulation of vehicle routing problems},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Does the prohibition of trade-through hurt liquidity
demanders? <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2023.2454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The order protect rule (OPR) in the United States generally prohibits any trade-through, that is, a market order that is not executed at the best possible price among fast (electronic and automated) trading venues. By deriving upper and lower bounds for the difference in the execution costs in a dynamic model, we find that, although trade-through allows for flexible trading strategies and may benefit the liquidity demander, the benefit is insignificant in most cases, especially for small trades and stocks with fast resilience. Therefore, considering other benefits of the OPR studied in the literature, this study supports the regulation and suggests that the current separate regulations for fast and slow venues may be extended to differentiate stocks with fast and slow resilience speeds. Funding: P. Gao received financial support from the National Natural Science Foundation of China [Grants 72201234 and 72192805] and the Research Grant Council of Hong Kong (the Collaborative Research Fund) [Grant C6032-21G]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2454 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2454},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Does the prohibition of trade-through hurt liquidity demanders?},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exit spirals in coupled networked markets. <em>OR</em>,
<em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2023.2439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strategic agents choose whether to be active in networked markets. The value of being active depends on the activity choices of specific counterparties. Several markets are coupled when agents’ participation decisions are complements across markets. We model the problem of an analyst assessing the robustness of coupled networked markets during a crisis—an exogenous negative payoff shock—based only on partial information about the network structure. We give conditions under which exit spirals emerge—abrupt collapses of activity following shocks. Market coupling is a pervasive cause of fragility, creating exit spirals even between networks that are individually robust. The robustness of a coupled network system can be improved if one of two markets is replaced by a centralized one or if links become more correlated across markets. History: C. Aymanns would like to acknowledge generous support from the LSE Systemic Risk Centre. C.-P. Georg gratefully acknowledges support from the Algorand Foundation. B. Golub gratefully acknowledges funding from the National Science Foundation [Grant SES-1629446 and SES-1847860].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2439},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Exit spirals in coupled networked markets},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint inventory and scheduling control in a repair facility.
<em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2023.2459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study inventory and repair scheduling decisions of a maintenance service provider for repairable capital goods. Because of high downtime costs, the service provider keeps spare parts on stock to replace broken parts quickly. The service provider should determine the inventory level of spare parts for each component and the repair scheduling policy. Furthermore, in case of a stock-out, the service provider should decide whether to back-order the demand or execute an emergency repair, which is an urgent but expensive repair operation for a broken part followed by a fast form of installation. The objective is to minimize the long-run average inventory holding, back-order, and emergency repair costs. We formulate the repairable network as a closed queueing system and consider an asymptotic regime in which the repair facility is in the conventional heavy-traffic regime. Then, we formulate and solve a Brownian control problem (BCP). From the optimal BCP solution, we derive a simple and intuitive decision rule stating if the emergency repairs are necessary to achieve a close-to-optimal system performance. Moreover, we propose a simple, intuitive, and easy-to-implement heuristic control policy and demonstrate its close-to-optimal performance via numerical experiments. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2459 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2459},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Joint inventory and scheduling control in a repair facility},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian inventory control: Accelerated demand learning via
exploration boosts. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2023.2467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate Bayesian inventory control problems where parameters of the demand distribution are not known a priori but need to be learned using right-censored sales data. A Bayesian framework is adopted for demand learning, and the corresponding control problem is analyzed via Bayesian dynamic programming (BDP). In the Bayesian setting, it is known that the BDP-optimal decision is equal to the sum of the myopic-optimal decision plus a nonnegative “exploration boost.” The goal of this paper is to (i) identify those applications in which adding an exploration boost is important and (ii) characterize the form of the exploration boost. In contrast to recent research that suggests that ignoring the exploration boost (i.e., adopting the myopic policy) can perform reasonably well in certain settings, we show that for applications with moderate time horizons and high parameter uncertainty, the optimality gap between the myopic policy and the BDP-optimal policy can be arbitrarily large and in particular, grows in proportion to the posterior index of dispersion of the unknown mean demand. With regard to characterizing the form of the BDP-optimal exploration boost, we prove that the exploration boost is also proportional to the posterior index of dispersion of the unknown mean demand. This characterization expresses in clear terms the way in which the statistical learning and inventory control are jointly optimized; when there is a high degree of parameter uncertainty (encoded as a large posterior index of dispersion), inventory decisions are boosted to induce a higher chance of observing more sales data so as to more quickly resolve statistical uncertainty (i.e., accelerated demand learning), and to not do so will necessarily lead to poor performance. Funding: The work of Y.-T. Chuang was supported by the Ministry of Science and Technology, Taiwan [Grant MOST 111-2221-E-006-093-MY2] and E.SUN Commercial Bank [E.SUN Academic Award]. The work of M. J. Kim was supported by the Natural Sciences and Engineering Research Council of Canada [NSERC Discovery Grant RGPIN-2015-04019]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2467 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2467},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Bayesian inventory control: Accelerated demand learning via exploration boosts},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A nonparametric algorithm for optimal stopping based on
robust optimization. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2023.2461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal stopping is a fundamental class of stochastic dynamic optimization problems with numerous applications in finance and operations management. We introduce a new approach for solving computationally-demanding stochastic optimal stopping problems with known probability distributions. The approach uses simulation to construct a robust optimization problem that approximates the stochastic optimal stopping problem to any arbitrary accuracy; we then solve the robust optimization problem to obtain near-optimal Markovian stopping rules for the stochastic optimal stopping problem. In this paper, we focus on designing algorithms for solving the robust optimization problems that approximate the stochastic optimal stopping problems. These robust optimization problems are challenging to solve because they require optimizing over the infinite-dimensional space of all Markovian stopping rules. We overcome this challenge by characterizing the structure of optimal Markovian stopping rules for the robust optimization problems. In particular, we show that optimal Markovian stopping rules for the robust optimization problems have a structure that is surprisingly simple and finite-dimensional. We leverage this structure to develop an exact reformulation of the robust optimization problem as a zero-one bilinear program over totally unimodular constraints. We show that the bilinear program can be solved in polynomial time in special cases, establish computational complexity results for general cases, and develop polynomial-time heuristics by relating the bilinear program to the maximal closure problem from graph theory. Numerical experiments demonstrate that our algorithms for solving the robust optimization problems are practical and can outperform state-of-the-art simulation-based algorithms in the context of widely-studied stochastic optimal stopping problems from high-dimensional option pricing. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2461 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2461},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {A nonparametric algorithm for optimal stopping based on robust optimization},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal leveraged portfolio selection under quasi-elastic
market impact. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2023.2462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study optimal portfolio choice under leveraging to improve portfolio performance when trade execution faces market impact. We consider a quasi-elastic market with continuous trading in which temporary liquidity costs are sufficiently large relative to permanent impact. The resulting convex optimization model is used to show analytically that an unlevered portfolio maximizing the Sharpe ratio is no longer a tangency portfolio, and increasing the portfolio target mean leads to severely undermining the risk-adjusted returns and requiring increased portfolio leverage. This paper develops theoretical properties underlying the relationships among target mean, leverage, and Sharpe ratio in optimal portfolio selection under market impact. The Sharpe-leverage efficient frontiers under market impact are consistently dominated when setting higher return targets. Moreover, leverage-constrained and less risk-averse investors ignoring liquidity costs ex ante suffer the most losses in expected utility. Detailed computational analyses are provided using real-world data to support and highlight our analytical findings. Funding: The research of the corresponding author, J. Chen, was partially supported by the National Natural Science Foundation of China [Grants 72171012 and 11801023]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2462 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2462},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Optimal leveraged portfolio selection under quasi-elastic market impact},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonrobust strong knapsack cuts for capacitated location
routing and related problems. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2023.2458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capacitated location-routing problem consists in, given a set of locations and a set of customers, determining in which locations one should install depots with limited capacity, and for each depot, design a number of routes to supply customer demands. We provide a formulation that includes depot variables, edge variables, assignment variables, and an exponential number of route variables, together with some new families of valid inequalities, leading to a branch-cut-and-price algorithm. The main original methodological contribution of the article is the route load knapsack cuts, a family of nonrobust cuts, defined over the route variables, devised to strengthen the depot capacity constraints. We explore the monotonicity and the superadditivity properties of those cuts to adapt the labeling algorithm, used in the pricing, for handling the additional dual variables efficiently. Computational experiments show that several capacitated location-routing previously unsolved instances from the literature can now be solved to optimality. Additional experiments with hard instances of the vehicle routing problem with capacitated multiple depots and with instances of the vehicle routing problem with time windows and shifts indicate that the newly proposed cuts are also effective for those problems. Funding: This work was supported by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior [PrInt UFF 88881], the Conselho Nacional de Desenvolvimento Científico e Tecnológico [Grant 313601/2018-6], and the Fundação Carlos Chagas Filho de Amparo à Pesquisa do Estado do Rio de Janeiro [Faperj E-26/202.887/2017]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2458 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2458},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Nonrobust strong knapsack cuts for capacitated location routing and related problems},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—pricing in on-demand and one-way
vehicle-sharing networks. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2023.2446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the dynamic pricing problem that arises in the context of an on-demand vehicle sharing system with one-way trips. Existing results show that a static pricing policy that arises from solving a maximum flow relaxation of the problem guarantees a performance ratio that is bounded by K /( N + K −1) when travel times are negligible and by 1 − 𝑂 ( 1 / 𝐾 ‾ ‾ √ ) 1 − O ( 1 / K ) 1−O(1/K) otherwise, where K is the number of vehicles and N is the number of locations. In this paper, we build on these results by providing an alternative approach to bounding the performance of static pricing policies. Our approach is startlingly simple, producing, upon the application of a well-known recursive relationship that relates system availability in a system with K vehicles to one with K −1 vehicles, a sequence of bounds that are increasingly tight. The worst of these bounds is given by 𝐾 / ( 𝑁 + 𝐾 − 1 + Λ / 𝜇 ) K / ( N + K − 1 + Λ / μ ) K/(N+K−1+Λ/μ) , where Λ is the total demand (sum of all trip requests) rate and 1 / 𝜇 1 / μ 1/μ is the average trip travel time, implying a convergence rate that is at least of order 1 − 𝑂 ( 1 / 𝐾 ) 1 − O ( 1 / K ) 1−O(1/K) in the number of vehicles for fixed Λ / 𝜇 Λ / μ Λ/μ . The same recursive relationship can be used to obtain a bound that is independent of Λ / 𝜇 Λ / μ Λ/μ and that is tighter than previous bounds, implying a convergence rate that is at least of order 1 − 𝑂 ( 1 / 𝐾 ‾ ‾ √ ) 1 − O ( 1 / K ) 1−O(1/K) . The approach also yields a parameterized family of static pricing policies that are asymptotically optimal and that generalize static pricing policies previously proposed in the literature. Moreover, the best static pricing policy this approach produces is optimal among those that require a demand balance constraint with a performance that can be significantly higher. Funding: This work was supported by the National Science Foundation [Grant SCC-1831140].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2446},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Pricing in on-demand and one-way vehicle-sharing networks},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—dynamic mechanism design with capacity
constraint. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2023.2449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a project assignment problem, where a principal needs to assign multiple projects to an agent. The agent is privately informed about the cost, which could be high or low. The agent’s type evolves stochastically over time. We fully characterize the optimal mechanism via a sequence of deadlines and show that the presence of the capacity constraint reduces the principal’s payoff and delays the assignment of projects. In particular, as the number of projects increases, the limit optimal contract may be strictly bounded away from the optimal contract when there are infinitely many projects, and the principal’s payoff may be strictly below that in the setting without the capacity constraint. Funding: This work was supported by the National Natural Science Foundation of China [Grant 72122023]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2449 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2449},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Dynamic mechanism design with capacity constraint},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strategic release of information in platforms: Entry,
competition, and welfare. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-sided platforms play an important role in reducing frictions and facilitating trade, and in doing so they increasingly engage in collecting and processing data about supply and demand. This paper establishes that platforms have an incentive to strategically disclose (coarse) information about demand to the supply side, as this can considerably boost their profits. However, this practice may also adversely affect the welfare of consumers. By optimally designing its information disclosure policy, a platform can influence the entry and pricing decisions of its potential suppliers. In general, it is optimal for the platform to disclose its information only partially to either “nudge” entry when it is a priori costly for suppliers to join or, conversely, discourage it when suppliers do not have access to attractive outside options. On the other hand, consumers may end up being worse off, as they have access to fewer trading options and/or face higher prices compared with when the platform refrains from sharing any demand information to its potential suppliers. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2402 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2402},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Strategic release of information in platforms: Entry, competition, and welfare},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive discretization in online reinforcement learning.
<em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discretization-based approaches to solving online reinforcement learning problems are studied extensively on applications such as resource allocation and cache management. The two major questions in designing discretization-based algorithms are how to create the discretization and when to refine it. There are several experimental results investigating heuristic approaches to these questions but little theoretical treatment. In this paper, we provide a unified theoretical analysis of model-free and model-based, tree-based adaptive hierarchical partitioning methods for online reinforcement learning. We show how our algorithms take advantage of inherent problem structure by providing guarantees that scale with respect to the “zooming” instead of the ambient dimension, an instance-dependent quantity measuring the benignness of the optimal Q h ⋆ function. Many applications in computing systems and operations research require algorithms that compete on three facets: low sample complexity, mild storage requirements, and low computational burden for policy evaluation and training. Our algorithms are easily adapted to operating constraints, and our theory provides explicit bounds across each of the three facets. Funding: This work is supported by funding from the National Science Foundation [Grants ECCS-1847393, DMS-1839346, CCF-1948256, and CNS-1955997] and the Army Research Laboratory [Grant W911NF-17-1-0094]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2396 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2396},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Adaptive discretization in online reinforcement learning},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robustness of proactive intensive care unit transfer
policies. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients whose transfer to the intensive care unit (ICU) is unplanned are prone to higher mortality rates and longer length of stay. Recent advances in machine learning to predict patient deterioration have introduced the possibility of proactive transfer from the ward to the ICU. In this work, we study the problem of finding robust patient transfer policies that account for the important problem of uncertainty in statistical estimates because of data limitations when optimizing to improve overall patient care. We propose a Markov decision process model to capture the evolution of patient health, where the states represent a measure of patient severity. Under fairly general assumptions, we show that an optimal transfer policy has a threshold structure (i.e., that it transfers all patients above a certain severity level to the ICU (subject to available capacity)). As model parameters are typically determined based on statistical estimations from real-world data, they are inherently subject to misspecification and estimation errors. This is an important issue, which can lead to choosing significantly suboptimal policies. We account for this parameter uncertainty by deriving a robust policy that optimizes the worst-case reward across all plausible values of the model parameters. We are able to show that the robust policy also has a threshold structure under fairly general assumptions and that it is more aggressive in transferring patients than the optimal nominal policy, which does not take into account parameter uncertainty. We present computational experiments using a data set of hospitalizations at 21 Kaiser Permanente Northern California hospitals and present empirical evidence of the sensitivity of various hospital metrics (mortality, length of stay, and average ICU occupancy) to small changes in the parameters. Although threshold policies are a simplification of the actual complex sequence of decisions leading (or not) to a transfer to the ICU, our work provides useful insights into the impact of parameter uncertainty on deriving simple policies for proactive ICU transfer that have strong empirical performance and theoretical guarantees. Funding: J. Grand-Clément is a Hi! Paris chair holder and is supported by the Agence Nationale de la Recherche [Grant ANR-11-LABX-0047]. The research of V. Goyal was supported in part by the Defense Advanced Research Projects Agency [Grant Lagrange] and the Division of Civil, Mechanical and Manufacturing Innovation [Grant 1636046]. This work was also supported by the Division of Civil, Mechanical and Manufacturing Innovation [Grants 1350059 and 1351838] and the Columbia Business School [Deming Fellowship 2019]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2403 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2403},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Robustness of proactive intensive care unit transfer policies},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential fair allocation: Achieving the optimal
envy-efficiency trade-off curve. <em>OR</em>, <em>71</em>(5), iii–vii.
(<a href="https://doi.org/10.1287/opre.2022.2397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of dividing limited resources to individuals arriving over T rounds. Each round has a random number of individuals arrive, and individuals can be characterized by their type (i.e., preferences over the different resources). A standard notion of fairness in this setting is that an allocation simultaneously satisfy envy-freeness and efficiency. The former is an individual guarantee, requiring that each agent prefers the agent’s own allocation over the allocation of any other; in contrast, efficiency is a global property, requiring that the allocations clear the available resources. For divisible resources, when the number of individuals of each type are known up front, the desiderata are simultaneously achievable for a large class of utility functions. However, in an online setting when the number of individuals of each type are only revealed round by round, no policy can guarantee these desiderata simultaneously, and hence, the best one can do is to try and allocate so as to approximately satisfy the two properties. We show that, in the online setting, the two desired properties (envy-freeness and efficiency) are in direct contention in that any algorithm achieving additive counterfactual envy-freeness up to a factor of L T necessarily suffers an efficiency loss of at least 1 / L T . We complement this uncertainty principle with a simple algorithm, G uarded- H ope , which allocates resources based on an adaptive threshold policy and is able to achieve any fairness–efficiency point on this frontier. Our results provide guarantees for fair online resource allocation with high probability for multiple resource and multiple type settings. In simulation results, our algorithm provides allocations close to the optimal fair solution in hindsight, motivating its use in practical applications as the algorithm is able to adapt to any desired fairness efficiency trade-off. Funding: This work was supported by the National Science Foundation [Grants ECCS-1847393, DMS-1839346, CCF-1948256, and CNS-1955997] and the Army Research Laboratory [Grant W911NF-17-1-0094]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2397 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2397},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Sequential fair allocation: Achieving the optimal envy-efficiency trade-off curve},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The price of incentivizing exploration: A characterization
via thompson sampling and sample complexity. <em>OR</em>,
<em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider incentivized exploration : a version of multiarmed bandits where the choice of arms is controlled by self-interested agents and the algorithm can only issue recommendations. The algorithm controls the flow of information, and the information asymmetry can incentivize the agents to explore. Prior work achieves optimal regret rates up to multiplicative factors that become arbitrarily large depending on the Bayesian priors and scale exponentially in the number of arms. A more basic problem of sampling each arm once runs into similar factors. We focus on the price of incentives : the loss in performance, broadly construed, incurred for the sake of incentive compatibility. We prove that Thompson sampling, a standard bandit algorithm, is incentive compatible if initialized with sufficiently many data points. The performance loss because of incentives is, therefore, limited to the initial rounds when these data points are collected. The problem is largely reduced to that of sample complexity. How many rounds are needed? We address this question, providing matching upper and lower bounds and instantiating them in various corollaries. Typically, the optimal sample complexity is polynomial in the number of arms and exponential in the “strength of beliefs.” Funding: This work was supported by the National Science Foundation [Grant 1650114 (Graduate Research Fellowship)] and a William R. and Sara Hart Kimball Stanford Graduate Fellowship.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2401},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {The price of incentivizing exploration: A characterization via thompson sampling and sample complexity},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using hospital admission predictions at triage for improving
patient length of stay in emergency departments. <em>OR</em>,
<em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long boarding times have long been recognized as one of the main reasons behind emergency department (ED) crowding. One of the suggestions made in the literature to reduce boarding times was to predict, at the time of triage, whether a patient will eventually be admitted to the hospital and if the prediction turns out to be “admit,” start preparations for the patient’s transfer to the main hospital early in the ED visit. However, there has been no systematic effort in developing a method to help determine whether an estimate for the probability of admit would be considered high enough to request a bed early, whether this determination should depend on ED census, and what the potential benefits of adopting such a policy would be. This paper aims to help fill this gap. The methodology we propose estimates hospital admission probabilities using standard logistic regression techniques. To determine whether a given probability of admission is high enough to qualify a bed request early, we develop and analyze two mathematical decision models. Both models are simplified representations and thus, do not lead to directly implementable policies. However, building on the solutions to these simple models, we propose two policies that can be used in practice. Then, using data from an academic hospital ED in the southeastern United States, we develop a simulation model, investigate the potential benefits of adopting the two policies, and compare their performances with that under a simple benchmark policy. We find that both policies can bring modest to substantial benefits, with the state-dependent policy outperforming the state-independent one particularly under conditions when the ED experiences more than usual levels of patient demand. Funding: This work was supported by the National Science Foundation [Grants CMMI-1234212 and CMMI-1635574]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2405 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2405},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Using hospital admission predictions at triage for improving patient length of stay in emergency departments},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inventory sharing for perishable products: Application to
platelet inventory management in hospital blood banks. <em>OR</em>,
<em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platelets are critical blood products. The management of platelet inventory is particularly challenging because of its perishable nature with a short shelf life. Motivated by a platelet inventory management problem at a two-location hospital system, we study how the wastage of platelets and, more broadly, perishable products can be reduced through inventory sharing. In particular, we consider a system with two locations and a single product (e.g., a two-hospital system sharing blood products, such as platelets). Each location faces a stochastic demand, and products can be transshipped from one location to the other after demand realization. At each location, products are issued in a first-in, first-out manner. Although the state of such a complex system consists of the inventory levels of different product ages at both locations, interestingly, we show that the direction of transshipment can be determined by simply comparing the age of the oldest products at each location after meeting demand. Based on this and other structural results, we then prove that a myopic transshipment policy is optimal for a special case motivated by our case study and serves as a lower bound on the optimal transshipment quantity for more general settings. Our analysis also sheds light on how inventory sharing affects the optimal inventory levels for perishable products. Because of its competitive performance and simplicity, our proposed myopic transshipment policy has been implemented by our partner hospital system, which led to a reduction of approximately 20\% in platelet outdates. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2410 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2410},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Inventory sharing for perishable products: Application to platelet inventory management in hospital blood banks},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tight guarantees for static threshold policies in the
prophet secretary problem. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the prophet secretary problem, n values are drawn independently from known distributions and presented in a uniformly random order. A decision maker must accept or reject each value when it is presented and may accept at most k values in total. The objective is to maximize the expected sum of accepted values. We analyze the performance of static threshold policies , which accept the first k values exceeding a fixed threshold (or all such values, if fewer than k exist). We show that an appropriate threshold guarantees γ k = 1 − e − k k k / k ! times the value of the offline optimal solution. Note that γ 1 = 1 − 1 / e , and by Stirling’s approximation, γ k ≈ 1 − 1 / 2 π k . This represents the best-known guarantee for the prophet secretary problem for all k &gt; 1 and is tight for all k for the class of static threshold policies. We provide two simple methods for setting the threshold. Our first method sets a threshold such that k · γ k values are accepted in expectation, and offers an optimal guarantee for all k . Our second sets a threshold such that the expected number of values exceeding the threshold is equal to k . This approach gives an optimal guarantee if k &gt; 4 but gives suboptimal guarantees for k ≤ 4 . Our proofs use a new result for optimizing sums of independent Bernoulli random variables, which extends a result of Hoeffding from 1956 and could be of independent interest. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.2419 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2419},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Tight guarantees for static threshold policies in the prophet secretary problem},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—stochastic scheduling with abandonment:
Necessary and sufficient conditions for the optimality of a strict
priority policy. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a stochastic scheduling problem in clearing systems with two types of jobs, each characterized by a general service time distribution, an exponentially distributed lifetime, and a reward. A job abandons the system if its waiting time in the queue is larger than its lifetime. Preemption is not allowed. The objective is to maximize the total expected reward. When service times are homogeneous, we provide a set of necessary and sufficient conditions for the optimality of a strict priority policy. When service times are heterogeneous and exponentially distributed, we conjecture a set of necessary conditions, which would also be sufficient when one parameter is identical (reward or lifetime rate) for the two types of jobs.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2285},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Stochastic scheduling with abandonment: Necessary and sufficient conditions for the optimality of a strict priority policy},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—dual approach for two-stage robust nonlinear
optimization. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adjustable robust minimization problems where the objective or constraints depend in a convex way on the adjustable variables are generally difficult to solve. In this paper, we reformulate the original adjustable robust nonlinear problem with a polyhedral uncertainty set into an equivalent adjustable robust linear problem, for which all existing approaches for adjustable robust linear problems can be used. The reformulation is obtained by first dualizing over the adjustable variables and then over the uncertain parameters. The polyhedral structure of the uncertainty set then appears in the linear constraints of the dualized problem, and the nonlinear functions of the adjustable variables in the original problem appear in the uncertainty set of the dualized problem. We show how to recover linear decision rules to the original primal problem and how to generate bounds on its optimal objective value. Funding: The research of F. J. C. T. de Ruiter is partially supported by the Netherlands Organisation for Scientific Research (NWO) [Talent Grant 406-14-067]. J. Zhen is partially supported by the NWO [Grant 613.001.208]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2289 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2289},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Dual approach for two-stage robust nonlinear optimization},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Algorithms and complexities of matching variants in
covariate balancing. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here, we study several variants of matching problems that arise in covariate balancing. Covariate balancing problems can be viewed as variants of matching, or b-matching, with global side constraints. We present here a comprehensive complexity study of the covariate balancing problems providing polynomial time algorithms, or a proof of NP-hardness. The polynomial time algorithms described are mostly combinatorial and rely on network flow techniques. In addition, we present several fixed-parameter tractable results for problems where the number of covariates and the number of levels of each covariate are seen as a parameter. Funding: This work was supported by National Science Foundation [Grants CMMI-1760102 and NSF 2112533]; Israel Science Foundation (308/18). Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2286 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2286},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Algorithms and complexities of matching variants in covariate balancing},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gradient-based algorithms for convex discrete optimization
via simulation. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new sequential simulation–optimization algorithms for general convex optimization via simulation problems with high-dimensional discrete decision space. The performance of each choice of discrete decision variables is evaluated via stochastic simulation replications. If an upper bound on the overall level of uncertainties is known, our proposed simulation–optimization algorithms utilize the discrete convex structure and are guaranteed with high probability to find a solution that is close to the best within any given user-specified precision level. The proposed algorithms work for any general convex problem, and the efficiency is demonstrated by proven upper bounds on simulation costs. The upper bounds demonstrate a polynomial dependence on the dimension and scale of the decision space. For some discrete optimization via simulation problems, a gradient estimator may be available at low costs along with a single simulation replication. By integrating gradient estimators, which are possibly biased, we propose simulation–optimization algorithms to achieve optimality guarantees with a reduced dependence on the dimension under moderate assumptions on the bias. Funding: H. Zhang and J. Lavaei were funded by grants from Air Force Office of Scientific Research (AFOSR), Army Research Office (ARO), Office of Naval Research (ONR), National Science Foundation (NSF) and C3.ai Digital Transformation Institute. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2295 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2295},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Gradient-based algorithms for convex discrete optimization via simulation},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonconvex piecewise linear functions: Advanced formulations
and simple modeling tools. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2019.1973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present novel mixed-integer programming (MIP) formulations for optimization over nonconvex piecewise linear functions. We exploit recent advances in the systematic construction of MIP formulations to derive new formulations for univariate functions using a geometric approach and for bivariate functions using a combinatorial approach. All formulations are strong, small (so-called logarithmic formulations), and have other desirable computational properties. We present extensive experiments in which they exhibit substantial computational performance improvements over existing approaches. To accompany these advanced formulations, we present PiecewiseLinearOpt , an extension of the JuMP modeling language in Julia that implements our models (alongside other formulations from the literature) through a high-level interface, hiding the complexity of the formulations from the end user. Funding: This work was supported by the National Science Foundation [Grant CMMI-1351619].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1973},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Nonconvex piecewise linear functions: Advanced formulations and simple modeling tools},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Every choice function is pro-con rationalizable.
<em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an agent who is endowed with two sets of orderings: pro- and con-orderings. For each choice set, if an alternative is the top-ranked by a pro-ordering ( con-ordering ), then this is a pro ( con ) for choosing that alternative. The alternative with more pros than cons is chosen from each choice set. Each ordering may have a weight reflecting its salience. In this case, the probability that an alternative is chosen equals the difference between the total weights of its pros and cons. We show that every nuance of the rich human choice behavior can be captured via this structured model. Our technique requires a generalization of the Ford-Fulkerson theorem, which may be of independent interest. As an application of our results, we show that every choice rule is plurality-rationalizable. Funding: K. Yildiz is grateful for the hospitality of New York University, Department of Economics during his visit in the 2017–2018 academic year, and the support from the Scientific and Research Council of Turkey (TUBITAK) [Grant 1059B191601712].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2312},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Every choice function is pro-con rationalizable},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stability and sample-based approximations of composite
stochastic optimization problems. <em>OR</em>, <em>71</em>(5), iii–vii.
(<a href="https://doi.org/10.1287/opre.2022.2308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization under uncertainty and risk is indispensable in many practical situations. Our paper addresses stability of optimization problems using composite risk functionals that are subjected to multiple measure perturbations. Our main focus is the asymptotic behavior of data-driven formulations with empirical or smoothing estimators such as kernels or wavelets applied to some or to all functions of the compositions. We analyze the properties of the new estimators and we establish strong law of large numbers, consistency, and bias reduction potential under fairly general assumptions. Our results are germane to risk-averse optimization and to data science in general. Funding: This work was supported by the Office of Naval Research [Grant N00014-21-1-2161].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2308},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Stability and sample-based approximations of composite stochastic optimization problems},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient resource allocation contracts to reduce adverse
events. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the allocation of online visits to product, service, and content suppliers in the platform economy, we consider a dynamic contract design problem in which a principal constantly determines the allocation of a resource (online visits) to multiple agents. Although agents are capable of running the business, they introduce adverse events, the frequency of which depends on each agent’s effort level. We study continuous-time dynamic contracts that utilize resource allocation and monetary transfers to induce agents to exert effort and reduce the arrival rate of adverse events. In contrast to the single-agent case, in which efficiency is not achievable, we show that efficient and incentive-compatible contracts, which allocate all resources and induce agents to exert constant effort, generally exist with two or more agents. We devise an iterative algorithm that characterizes and calculates such contracts, and we specify the profit-maximizing contract for the principal. Furthermore, we provide efficient and incentive-compatible dynamic contracts that can be expressed in closed form and are therefore easy to understand and implement in practice. Funding: Y. Liang acknowledges support from the National Key R&amp;D Program of China [Grant 2020AAA0103801] and the National Natural Science Foundation of China [Grant 71872095]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2322 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2322},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Efficient resource allocation contracts to reduce adverse events},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximum entropy distributions with applications to graph
simulation. <em>OR</em>, <em>71</em>(5), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of sampling uniformly from discrete or continuous product sets subject to linear constraints. This family of problems includes sampling weighted bipartite, directed, and undirected graphs with given degree sequences. We analyze two candidate distributions for sampling from the target set. The first one maximizes entropy subject to satisfying the constraints in expectation. The second one is the distribution from an exponential family that maximizes the minimum probability over the target set. Our main result gives a condition under which the maximum entropy and the max-min distributions coincide. For the discrete case, we also develop a sequential procedure that updates the maximum entropy distribution after some components have been sampled. This procedure sacrifices the uniformity of the samples in exchange for always sampling a valid point in the target set. To address the loss of uniformity, we use importance sampling weights. The quality of these weights is affected by the order in which the components are simulated. We propose an adaptive rule for this order that reduces the skewness of the weights in numerical examples. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2323 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2323},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Maximum entropy distributions with applications to graph simulation},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximate submodularity in network design problems.
<em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network design problems, such as flexibility design, are ubiquitous in modern marketplaces where firms constantly innovate new ways to match supply and demand. We develop a primal-dual based approach to analyze the flexibility design problem, and establish that the problem possesses a novel structural property. The property, which we call cover modularity , can be interpreted as an approximate form of submodularity in the sense that local changes in the objective function can be used to bound global changes. We use this structure to analyze a class of greedy heuristics and establish the first constant factor approximation guarantee for solving the general flexibility design problem. Furthermore, we identify a significant practical byproduct of our primal-dual analysis: The dual solutions we construct can be used as surrogates to guide the heuristics, leading to order of magnitude gains in computational efficiency without loss of optimization performance. Finally, we extend our analysis by demonstrating the presence of cover modularity in a general class of linear programming formulations, indicating applicability of our approach to a wide range of network design problems distinct from flexibility design. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.2408 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2408},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Approximate submodularity in network design problems},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic programming principles for mean-field controls with
learning. <em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic programming principle (DPP) is fundamental for control and optimization, including Markov decision problems (MDPs), reinforcement learning (RL), and, more recently, mean-field controls (MFCs). However, in the learning framework of MFCs, the DPP has not been rigorously established, despite its critical importance for algorithm designs. In this paper, we first present a simple example in MFCs with learning where the DPP fails with a misspecified Q function and then propose the correct form of Q function in an appropriate space for MFCs with learning. This particular form of Q function is different from the classical one and is called the IQ function. In the special case when the transition probability and the reward are independent of the mean-field information, it integrates the classical Q function for single-agent RL over the state-action distribution. In other words, MFCs with learning can be viewed as lifting the classical RLs by replacing the state-action space with its probability distribution space. This identification of the IQ function enables us to establish precisely the DPP in the learning framework of MFCs. Finally, we illustrate through numerical experiments the time consistency of this IQ function. Funding: Financial support from the Coleman Fung Chair Endowment Fund and the Tsinghua-Berkeley-Shenzhen-Institute is gratefully acknowledged.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2395},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Dynamic programming principles for mean-field controls with learning},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Production planning with risk hedging under a conditional
value at risk objective. <em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central problem in planning production capacity is how to effectively manage demand risk. We develop a model that integrates capacity planning and risk hedging decisions under a popular risk measure, conditional value at risk ( CVaR ). The CVaR objective generalizes the usual risk-neutral objective (such as the expected payoff) and allows for explicit modeling of the degree of aversion to downside risk (associated with low demand). The starting point of our model is to incorporate the impact on demand from a financial asset (including for instance, a tradable market index as a proxy for the general economy). This way, in addition to the capacity decision at the beginning of the planning horizon, there is also a dynamic hedging strategy throughout the horizon, and the latter plays the role of both mitigating demand risk and supplementing the payoff. The hedging strategy is restricted to partial information and constrained with a cap on loss (pathwise). To find the optimal hedging strategy, we construct and solve a dual problem to derive the optimal terminal wealth from hedging; the real-time hedging strategy is then mapped out via the martingale representation theorem. With the hedging strategy optimized, we show that optimizing the production quantity is a concave maximization problem. With both production and hedging (jointly) optimized, we provide a complete characterization of the efficient frontier and quantify the improvement over the production-only model. Furthermore, via sensitivity and asymptotic analyses, we spell out the impacts of the loss cap and the risk aversion level, along with other qualitative insights. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2423 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2423},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Production planning with risk hedging under a conditional value at risk objective},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Store-wide shelf-space allocation with ripple effects
driving traffic. <em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2023.2437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a store layout, product categories grouped into shelves, and historical sales data, we investigate how the allocation of product categories can be optimized in a fashion that guides in-store traffic and stimulates impulse buying. The latter constitutes an important shopping behavior that amounts to over 50\% of the revenue in some retail settings. Considering a small-scale grocery store in Beirut, we analyze 40,000 customer receipts in order to relate in-store customer traffic to product shelf allocations and the store layout. This prompts the development of a predictive regression model that estimates traffic densities along a shelf as a function of the shelf-space allocation and the location of the shelf in the store. This traffic model captures a “ripple effect”—that is, the change in traffic throughout the store resulting from any change in product allocation to shelves. The customer traffic model is embedded within a mixed-integer nonlinear program that sets the shelf allocations across the entire store, thereby prescribing better location and shelf-space decisions for all product categories in a way that maximizes impulse buying. To overcome the computational challenges posed by the model, we develop a linear approximation for the traffic construct in the objective function, while keeping bilinear terms in the formulation, in order to derive lower and upper bounds on the optimal objective value. Our methodology produces a layout that yields a 65\% improvement in the expected impulse profit for the grocery store in Beirut. Managerial insights into the structure of the proposed store configuration are also discussed. Specifically, the allocation of a fast-mover to a shelf directly drives traffic, not only through adjacent shelves, but also, indirectly, through more distant shelves that lead to it. This, in turn, creates advantageous locations for high-impulse products, which may not be in the immediate vicinity of fast-movers. Finally, the study suggests that, from an impulse profit perspective, the location of a product category in the store is more important than adjusting the amount of shelf space that it is allocated. This challenges the classical research approach whereby extensive effort is invested to determine the relative space of products along a single shelf, taken in isolation, without considering its location in the store. Supplemental Material: The online companion is available at https://doi.org/10.1287/opre.2023.2437 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2437},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Store-wide shelf-space allocation with ripple effects driving traffic},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). General equilibrium in a heterogeneous-agent
incomplete-market economy with many consumption goods and a risk-free
bond. <em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2023.2442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a pure-exchange incomplete-market economy with heterogeneous agents. In each period, the agents choose how much to save (i.e., invest in a risk-free bond), how much to consume, and which bundle of goods to consume while their endowments are fluctuating. We focus on a competitive stationary equilibrium (CSE) in which the wealth distribution is invariant, the agents maximize their expected discounted utility, and both the prices of consumption goods and the interest rate are market-clearing. Our main contribution is to extend some general equilibrium results to an incomplete-market Bewley-type economy with many consumption goods. Under mild conditions on the agents’ preferences, we show that the aggregate demand for goods depends only on their relative prices and that the aggregate demand for savings is homogeneous of degree in prices, and we prove the existence of a CSE. When the agents’ preferences can be represented by a CES (constant elasticity of substitution) utility function with an elasticity of substitution that is higher than or equal to one, we prove that the CSE is unique. Under the same preferences, we show that a higher inequality of endowments does not change the equilibrium prices of goods and decreases the equilibrium interest rate. Our results shed light on the impact of market incompleteness on the properties of general equilibrium models.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2442},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {General equilibrium in a heterogeneous-agent incomplete-market economy with many consumption goods and a risk-free bond},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—new bounds for cardinality-constrained
assortment optimization under the nested logit model. <em>OR</em>,
<em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2023.2469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the cardinality-constrained assortment optimization problem under the nested logit model where there is a constraint that limits the number of products that can be offered within each nest. The problem is known to be intractable if the nest dissimilarity parameters are larger than one or there is a no-purchase alternative within each nest. Although these conditions often come up in practice, the existing solution approaches cannot handle them. We propose a solution method to obtain heuristic assortments with provable worst-case performance guarantees that hold even when the nest dissimilarity parameters are larger than one or there is a no-purchase alternative within each nest. We obtain a tractable upper bound that can be used to assess the practical performance of our solution approach. Computational experiments indicate that the heuristic assortments perform very well, with optimality gaps being smaller than 1\% on average. Our analysis also provides sharper performance bounds for the unconstrained assortment optimization problem under the nested logit model. Funding: S. Kunnumkal acknowledges the financial support of the Indian School of Business. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2469 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2469},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—New bounds for cardinality-constrained assortment optimization under the nested logit model},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contextual search in the presence of adversarial
corruptions. <em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study contextual search, a generalization of binary search in higher dimensions, which captures settings such as feature-based dynamic pricing. Standard formulations of this problem assume that agents act in accordance with a specific homogeneous response model. In practice, however, some responses may be adversarially corrupted. Existing algorithms heavily depend on the assumed response model being (approximately) accurate for all agents and have poor performance in the presence of even a few such arbitrary misspecifications. We initiate the study of contextual search when some of the agents can behave in ways inconsistent with the underlying response model. In particular, we provide two algorithms, one based on multidimensional binary search methods and one based on gradient descent. We show that these algorithms attain near-optimal regret in the absence of adversarial corruptions and their performance degrades gracefully with the number of such agents, providing the first results for contextual search in any adversarial noise model. Our techniques draw inspiration from learning theory, game theory, high-dimensional geometry, and convex analysis. Funding: C. Podimata was partially supported by a Microsoft Dissertation Grant, a Siebel Scholarship, the National Science Foundation [Grant CCF-1718549], and the Harvard Data Science Initiative. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2365 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2365},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Contextual search in the presence of adversarial corruptions},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Product ranking in the presence of social learning.
<em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies product ranking mechanisms of a monopolistic online platform in the presence of social learning. The products’ quality is initially unknown, but consumers can sequentially learn it as online reviews accumulate. A salient aspect of our problem is that consumers, who want to purchase a product from a list of items displayed by the platform, incur a search cost while scrolling down the list. In this setting, the social learning dynamics, and hence the demand, is affected by the interplay of two unique features: substitution and ranking effects. The platform can influence the social learning dynamics by adjusting the ranking of the products to ultimately maximize the revenue collected from commission fees for sold items. To formulate the problem in a tractable form, we use a large-market (fluid) approximation and show that consumers eventually learn the products’ quality and characterize the speed of learning. Armed with this backing, we formulate the platform’s ranking problem in the fluid setting, where we assume the perspective of an uninformed platform that does not know the true quality vector but rather learns it through consumers’ review process. We compare different ranking policies based on the worst-case regret with respect to a fully informed platform benchmark. Our analysis yields three main insights. First, a greedy policy that maximizes immediate revenue by displaying products based on current ratings may incur highly suboptimal worst-case regret, as it may relegate the most profitable products to the lowest positions in the ranking if their current rating is not high enough. Second, a simple variant of the greedy policy can sufficiently alleviate the regret by balancing the trade-off between exploration and exploitation. Third, we characterize the critical level of search cost for which the regret does not grow exponentially with the number of products. Funding: M. Scarsini’s work was partially supported by the Progetti di Rilevante Interesse Nazionale 2017 [Grant “Algorithms, Games, and Digital Markets”] and by the European Cooperation in Science and Technology (Action European Network for Game Theory). Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2372 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2372},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Product ranking in the presence of social learning},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential submodular maximization and applications to
ranking an assortment of products. <em>OR</em>, <em>71</em>(4), iii–vi.
(<a href="https://doi.org/10.1287/opre.2022.2370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a submodular maximization problem motivated by applications in online retail. A platform displays a list of products to a user in response to a search query. The user inspects the first k items in the list for a k chosen at random from a given distribution and decides whether to purchase an item from that set based on a choice model. The goal of the platform is to maximize the engagement of the shopper defined as the probability of purchase. This problem gives rise to a less-studied variation of submodular maximization, in which we are asked to choose an ordering of a set of elements to maximize a linear combination of different submodular functions. First, using a reduction to maximizing submodular functions over matroids, we give an optimal ( 1 − 1 / e ) -approximation for this problem. We then consider a variant in which the platform cares not only about user engagement, but also about diversification across various groups of users—that is, guaranteeing a certain probability of purchase in each group. We characterize the polytope of feasible solutions and give a bicriteria ( ( 1 − 1 / e ) 2 , ( 1 − 1 / e ) 2 ) -approximation for this problem by rounding an approximate solution of a linear-programming (LP) relaxation. For rounding, we rely on our reduction and the particular rounding techniques for matroid polytopes. For the special case in which underlying submodular functions are coverage functions—which is practically relevant in online retail—we propose an alternative LP relaxation and a simpler randomized rounding for the problem. This approach yields to an optimal bicriteria ( 1 − 1 / e , 1 − 1 / e ) -approximation algorithm for the special case of the problem with coverage functions. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2370 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2370},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Sequential submodular maximization and applications to ranking an assortment of products},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning product rankings robust to fake users. <em>OR</em>,
<em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many online platforms, customers’ decisions are substantially influenced by product rankings as most customers only examine a few top-ranked products. This induces a race for visibility among sellers, who may be incentivized to artificially inflate their position by employing fake users as exemplified by the emergence of click farms. Motivated by such fraudulent behavior, we study the problem of learning product rankings when a platform faces a mixture of real and fake users who are indistinguishable from one another. We first show that existing learning algorithms—that are optimal in the absence of fake users—may converge to highly suboptimal rankings under manipulation. To overcome this deficiency, we develop efficient learning algorithms under two informational settings: when the platform is aware of the number of fake users and when it is agnostic to this number. For both these settings, we prove that our algorithms converge to the optimal ranking, yet being robust to the aforementioned fraudulent behavior; we also present worst case performance guarantees for our methods and show that they outperform existing algorithms. At a high level, our work employs several novel approaches to guarantee robustness, such as (i) encoding product relationships using graphs and (ii) implementing multiple levels of learning as well as judicious cross-learning. Overall, our results indicate that online platforms can effectively combat fraudulent users even when they are completely oblivious to the number and identity of the fake users. Funding: This work was supported by the Natural Sciences and Engineering Research Council of Canada [Discovery Grant RGPIN-2022-04323], the Massachusetts Institute of Technology Junior Faculty Research Assistance Program, and the TD Management and Data Analytics Laboratory. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2380 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2380},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Learning product rankings robust to fake users},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint assortment optimization and customization under a
mixture of multinomial logit models: On the value of personalized
assortments. <em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a joint assortment optimization and customization problem under a mixture of multinomial logit models. In this problem, a firm faces customers of different types, each making a choice within an offered assortment according to the multinomial logit model with different parameters. The problem takes place in two stages. In the first stage, the firm picks an assortment of products to carry the subject to a cardinality constraint. In the second stage, a customer of a certain type arrives into the system. Observing the type of the customer, the firm customizes the assortment that it carries by, possibly, dropping products from the assortment. The goal of the firm is to find an assortment of products to carry and a customized assortment to offer to each customer type that can arrive in the second stage to maximize the expected revenue from a customer visit. The problem arises, for example, in online platforms, where retailers commit to a selection of products before the start of the selling season; but they can potentially customize the displayed assortment for each customer type. We refer to this problem as the Customized Assortment Problem ( CAP ). Letting m be the number of customer types, we show that the optimal expected revenue of ( CAP ) can be Ω ( m ) times greater than the optimal expected revenue of the corresponding model without customization and this bound is tight. We establish that ( CAP ) is NP-hard to approximate within a factor better than 1 − 1 / e , so we focus on providing an approximation framework for ( CAP ). As our main technical contribution, we design a novel algorithm, which we refer to as Augmented Greedy; building on it, we give a Ω ( 1 / log m ) -approximation algorithm to ( CAP ). Also, we present a fully polynomial-time approximation scheme for ( CAP ) when the number of customer types is constant. Considering the case where we have a cardinally constraint on the assortment offered to each customer type in the second stage of ( CAP ), we give a Ω ( 1 / m log m ) -approximation algorithm. In our computational experiments, we demonstrate the value of customization by using a data set from Expedia and check the practical performance of our approximation algorithm. Funding: This work was supported by the Cornell Tech Urban Tech Grant (O. El Housni and H. Topaloglu) and the National Science Foundation [Grant CMMI 1825406] (H. Topaloglu). Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.2384 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2384},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Joint assortment optimization and customization under a mixture of multinomial logit models: On the value of personalized assortments},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The stability of MNL-based demand under dynamic customer
substitution and its algorithmic implications. <em>OR</em>,
<em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the dynamic assortment planning problem under the widely utilized multinomial logit choice model (MNL). In this single-period assortment optimization and inventory management problem, the retailer jointly decides on an assortment, that is, a subset of products to be offered, as well as on the inventory levels of these products, aiming to maximize the expected revenue subject to a capacity constraint on the total number of units stocked. The demand process is formed by a stochastic stream of arriving customers, who dynamically substitute between products according to the MNL model. Although this dynamic setting is extensively studied, the best known approximation algorithm guarantees an expected revenue of at least 0.139 times the optimum, assuming that the demand distribution has an increasing failure rate. In this paper, we establish novel stochastic inequalities showing that, for any given inventory levels, the expected demand of each offered product is “stable” under basic algorithmic operations, such as scaling the MNL preference weights and shifting inventory across comparable products. We exploit this sensitivity analysis to devise the first approximation scheme for dynamic assortment planning under the MNL model, allowing one to efficiently compute inventory levels that approach the optimal expected revenue within any degree of accuracy. The running time of this algorithm is polynomial in all instance parameters except for an exponential dependency on log Δ , where Δ = w max w min stands for the ratio of the extremal MNL preference weights. Finally, we conduct simulations on simple synthetic instances with uniform preference weights (i.e., Δ = 1 ) . Using our approximation scheme to derive tight upper bounds, we gain some insights into the performance of several heuristics proposed by previous literature. Funding: This work was supported by the Israel Science Foundation [Grants 1407/20, 148/16].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2391},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {The stability of MNL-based demand under dynamic customer substitution and its algorithmic implications},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—bidding in multidimensional auctions when the
qualities of all bidders matter. <em>OR</em>, <em>71</em>(4), iii–vi.
(<a href="https://doi.org/10.1287/opre.2022.2378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multidimensional auctions, bidders compete in both quality and price, which are combined by a score rule. A well-known problem in procurement management is that nonprice attributes are often poorly measured and unreliably estimated. Adjustments of the reported quality based on the qualities of rival bids can enhance the reliability of the measurement process. We develop a general model of score function that is dependent on the qualities offered by all bidders who differ in terms of their production efficiency. We derive the ex ante equilibrium quality and price in a first-score and second-score auction environment when the cost is additive separable in terms of quality and production inefficiency. Bidders symmetrically select the quality that maximizes their margin for a given score, assuming that opponents act similarly. We demonstrate that the expected procurement cost is the same in the second-score auction and the first-score auction. We apply the findings in different forms of quality adjustments and show that the standard unadjusted auction yields the highest equilibrium quality and price, resulting in the highest expected procurement cost for the buyer. The findings could stimulate drastic changes in the conduct of procurement management, allowing the score rule to be determined by integrating all competing bidder qualities. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2378 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2378},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Bidding in multidimensional auctions when the qualities of all bidders matter},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revenue management of a professional services firm with
quality revelation. <em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Professional service firms (PSFs) such as management consulting, law, accounting, investment banking, architecture, advertising, and home-repair companies provide services for complicated turnkey projects. A firm bids for a project and, if successful in the bid, assigns employees to work on the project. We formulate this as a revenue management problem under two assumptions: a quality-revelation setup, where the employees that would be assigned to the project are committed ex ante, as part of the bid, and a quality-reputation setup, where the bid’s win probability depends on past performance, say, an average of the quality of past jobs. We first model a stylized Markov chain model of the problem amenable to analysis and show that up-front revelation of the assigned employees has subtle advantages. Subsequent to this analysis, we develop an operational stochastic dynamic programming framework under the revelation model to aid the firm in this bidding and assignment process. We show that the problem is computationally challenging and provide a series of bounds and solution methods to approximate the stochastic dynamic program. Based on our model and computational methods, we are able to address a number of interesting business questions for a PSF, such as the optimal utilization levels and the value of each employee type. Our methodology provides management with a tool kit for bidding on projects as well as to perform workforce analytics and to make staffing decisions. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2351 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2351},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Revenue management of a professional services firm with quality revelation},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Economic behavior of information acquisition: Impact on peer
grading in massive open online courses. <em>OR</em>, <em>71</em>(4),
iii–vi. (<a href="https://doi.org/10.1287/opre.2021.2131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A critical issue in operating massive open online courses (MOOCs) is the scalability of providing feedback. Because it is not feasible for instructors to grade a large number of students’ assignments, MOOCs use peer grading systems. This study investigates the efficacy of that practice when student graders are rational economic agents. We characterize grading as a process of (a) acquiring information to assess an assignment’s quality and (b) reporting a score. This process entails a tradeoff between the cost of acquiring information and the benefits of accurate grading. Because the true quality is not observable, any measure of inaccuracy must reference the actions of other graders, which motivates student graders to behave strategically. We present the unique equilibrium information level and reporting strategy of a homogeneous group of student graders and then examine the outcome of peer grading. We show how both the peer grading structure and the nature of MOOC courses affect peer grading accuracy, and we identify conditions under which the process fails. There is a systematic grading bias toward the mean, which discourages students from learning. To improve current practice, we introduce a scale-shift grading scheme, theoretically examine how it can improve grading accuracy and adjust grading bias and discuss how it can be practically implemented. History: This paper was accepted for the Operations Research Special Issue Honoring Kenneth Arrow: Mathematical Models of Individual and Group Decision Making in Operations Research that appeared June 30, 2022.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2131},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Economic behavior of information acquisition: Impact on peer grading in massive open online courses},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Voxel-based solution approaches to the three-dimensional
irregular packing problem. <em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on the three-dimensional (3D) packing problem has largely focused on packing boxes for the transportation of goods. As a result, there has been little focus on packing irregular shapes in the operational research literature. New technologies have raised the practical importance of 3D irregular packing problems and the need for efficient solutions. In this work, we address the variant of the problem where the aim is to place a set of 3D irregular items in a container, while minimizing the container height, analogous to the strip packing problem. In order to solve this problem, we need to address two critical components; efficient computation of the geometry and finding high-quality solutions. In this work, we explore the potential of voxels, the 3D equivalent of pixels, as the geometric representation of the irregular items. In this discretised space, we develop a geometric tool that extends the concept of the nofit polygon to the 3D case. This enables us to provide an integer linear programming formulation for this problem that can solve some small instances. For practical size problems, we design metaheuristic optimisation approaches. Because the literature is limited, we introduce new benchmark instances. Some are randomly generated and some represent realistic models from the additive manufacturing area. Our results on the literature benchmark data and on our new instances show that our metaheuristic techniques achieve the best known solutions for a wide variety of problems in practical computation times. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2260 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2260},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Voxel-based solution approaches to the three-dimensional irregular packing problem},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerated MM algorithms for inference of ranking scores
from comparison data. <em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of assigning ranking scores to items based on observed comparison data (e.g., paired comparisons, choice, and full ranking outcomes) has been of continued interest in a wide range of applications, including information search, aggregation of social opinions, electronic commerce, online gaming platforms, and, more recently, evaluation of machine learning algorithms. The key problem is to compute ranking scores, which are of interest for quantifying the strength of skills, relevancies, or preferences, and the prediction of ranking outcomes when the ranking scores are estimates of parameters of a statistical model of ranking outcomes. One of the most popular statistical models of ranking outcomes is the Bradley–Terry model for paired comparisons (equivalent to multinomial logit model) and its extensions to choice and full ranking outcomes. The problem of computing ranking scores under these statistical models amounts to estimating model parameters. In this paper, we study a popular method for inference of the Bradley–Terry model parameters—namely, the MM algorithm, where MM refers to minorization-maximization—for maximum likelihood estimation and maximum a posteriori probability estimation. This class of models includes the Bradley–Terry model of paired comparisons, the Rao–Kupper model of paired comparisons allowing for tie outcomes, the Luce choice model, and the Plackett–Luce ranking model. We establish tight characterizations of the convergence rate for the MM algorithm and show that it is essentially equivalent to that of a gradient descent algorithm. For the maximum likelihood estimation, the convergence is shown to be linear with the rate crucially determined by the algebraic connectivity of the matrix of item pair co-occurrences in observed comparison data. For the Bayesian inference, the convergence rate is also shown to be linear, with the rate determined by a parameter of the prior distribution in a way that can make the convergence arbitrarily slow for small values of this parameter. We propose a simple modification of the classical MM algorithm that avoids the observed slow convergence issue and accelerates the convergence. The key component of the accelerated MM algorithm is a parameter rescaling performed at each iteration step that is carefully chosen based on our theoretical analysis and characterisation of the convergence rate. Our experimental results, performed on both synthetic and real-world data, demonstrate the identified slow convergence issue of the classic MM algorithm and show that significant efficiency gains can be obtained by our new proposed method. Funding: S.-Y. Yun was supported by the National Research Foundation of Korea (NRF) [Grant 2019028324] and the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) [Grant 2019-0-00075 to the Artificial Intelligence Graduate School Program (KAIST) funded by the Korea government (MSIT)]. K. Zhou was supported by an ESRC 1 + 3 PhD studentship.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2264},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Accelerated MM algorithms for inference of ranking scores from comparison data},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization-based scenario reduction for data-driven
two-stage stochastic optimization. <em>OR</em>, <em>71</em>(4), iii–vi.
(<a href="https://doi.org/10.1287/opre.2022.2265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel, optimization-based method that takes into account the objective and problem structure for reducing the number of scenarios, m , needed for solving two-stage stochastic optimization problems. We develop a corresponding convex optimization-based algorithm and show that, as the number of scenarios increase, the proposed method recovers the SAA solution. We report computational results with both synthetic and real-world data sets that show that the proposed method has significantly better performance for m = 1 − 2\% of n in relation to other state of the art methods (importance sampling, Monte Carlo sampling, and Wasserstein scenario reduction with squared Euclidean norm). Additionally, we propose variants of classical scenario reduction algorithms (which rely on the Euclidean norm) and show that these variants consistently outperform their traditional versions. Supplemental Material : The online appendix is available at https://doi.org/10.1287/opre.2022.2265 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2265},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Optimization-based scenario reduction for data-driven two-stage stochastic optimization},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust dynamic pricing with demand learning in the presence
of outlier customers. <em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a dynamic pricing problem under model misspecification. To characterize model misspecification, we adopt the ε -contamination model—the most fundamental model in robust statistics and machine learning. In particular, for a selling horizon of length T , the online ε -contamination model assumes that demands are realized according to a typical unknown demand function only for ( 1 − ε ) T periods. For the rest of ε T periods, an outlier purchase can happen with arbitrary demand functions. The challenges brought by the presence of outlier customers are mainly due to the fact that arrivals of outliers and their exhibited demand behaviors are completely arbitrary, therefore calling for robust estimation and exploration strategies that can handle any outlier arrival and demand patterns. We first consider unconstrained dynamic pricing without any inventory constraint. In this case, we adopt the Follow-the-Regularized-Leader algorithm to hedge against outlier purchase behavior. Then, we introduce inventory constraints. When the inventory is insufficient, we study a robust bisection-search algorithm to identify the clearance price—that is, the price at which the initial inventory is expected to clear at the end of T periods. Finally, we study the general dynamic pricing case, where a retailer has no clue whether the inventory is sufficient or not. In this case, we design a meta-algorithm that combines the previous two policies. All algorithms are fully adaptive, without requiring prior knowledge of the outlier proportion parameter ε . Simulation study shows that our policy outperforms existing policies in the literature. Funding: X. Chen is supported by the National Science Foundation (NSF) [Grant IIS-1845444]. Supplemental Material: The supplementary material is available at https://doi.org/10.1287/opre.2022.2280 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2280},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Robust dynamic pricing with demand learning in the presence of outlier customers},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Managing queues with different resource requirements.
<em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Queueing models that are used to capture various service settings typically assume that customers require a single unit of resource (server) to be processed. However, there are many service settings where such an assumption may fail to capture the heterogeneity in resource requirements of different customers. We propose a multiserver queueing model with multiple customer classes in which customers from different classes may require different amounts of resources to be served. We study the optimal scheduling policy for such systems. To balance holding costs, service rates, resource requirement, and priority-induced idleness, we develop an index-based policy that we refer to as the idle-avoid c μ / m rule. For a two-class two-server model, where policy-induced idleness can have a big impact on system performance, we characterize cases where the idle-avoid c μ / m rule is optimal. In other cases, we establish a uniform performance bound on the amount of suboptimality incurred by the idle-avoid c μ / m rule. For general multiclass multiserver queues, we establish the asymptotic optimality of the idle-avoid c μ / m rule in the many-server regime. For long-time horizons, we show that the idle-avoid c μ / m is throughput optimal. Our theoretical results, along with numerical experiments, provide support for the good and robust performance of the proposed policy. Funding: J. Dong was supported in part by the National Science Foundation [Grant CMMI-1762544]. N. Zychlinski was supported in part by the Eric and Wendy Schmidt Postdoctoral Award for Women in Mathematical and Computing Sciences and the Israeli Council for Higher Education.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2284},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Managing queues with different resource requirements},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—an approximate dynamic programming approach
to the incremental knapsack problem. <em>OR</em>, <em>71</em>(4),
iii–vi. (<a href="https://doi.org/10.1287/opre.2022.2268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the incremental knapsack problem, where one wishes to sequentially pack items into a knapsack whose capacity expands over a finite planning horizon, with the objective of maximizing time-averaged profits. Although various approximation algorithms were developed under mitigating structural assumptions, obtaining nontrivial performance guarantees for this problem in its utmost generality has remained an open question thus far. In this paper, we devise a polynomial-time approximation scheme for general instances of the incremental knapsack problem, which is the strongest guarantee possible given existing hardness results. In contrast to earlier work, our algorithmic approach exploits an approximate dynamic programming formulation. Starting with a simple exponentially sized dynamic program, we prove that an appropriate composition of state pruning ideas yields a polynomially sized state space with negligible loss of optimality. The analysis of this formulation synthesizes various techniques, including new problem decompositions, parsimonious counting arguments, and efficient rounding methods, that may be of broader interest.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2268},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—An approximate dynamic programming approach to the incremental knapsack problem},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Technical note—the elliptical potential lemma for general
distributions with an application to linear thompson sampling.
<em>OR</em>, <em>71</em>(4), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this note, we introduce a general version of the well-known elliptical potential lemma that is a widely used technique in the analysis of algorithms in sequential learning and decision-making problems. We consider a stochastic linear bandit setting where decision makers sequentially choose among a set of given actions, observe their noisy rewards, and aim to maximize their cumulative expected reward over a decision-making horizon. The elliptical potential lemma is a key tool for quantifying uncertainty in estimating parameters of the reward function, but it requires the noise and the prior distributions to be Gaussian. Our general elliptical potential lemma relaxes this Gaussian requirement, which is a highly nontrivial extension for a number of reasons; unlike the Gaussian case, there is no closed-form solution for the covariance matrix of the posterior distribution, the covariance matrix is not a deterministic function of the actions, and the covariance matrix is not decreasing with respect to the semidefinite inequality. Although this result is of broad interest, we showcase an application of it to prove an improved Bayesian regret bound for the well-known Thompson sampling algorithm in stochastic linear bandits with changing action sets where prior and noise distributions are general. This bound is minimax optimal up to constants. Funding: This work was supported by the National Science Foundation [Grant 1554140].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2274},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—The elliptical potential lemma for general distributions with an application to linear thompson sampling},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preface to the special issue on behavioral queueing science:
The need for a multidisciplinary approach. <em>OR</em>, <em>71</em>(3),
iii–vi. (<a href="https://doi.org/10.1287/opre.2023.2452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2452},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Preface to the special issue on behavioral queueing science: The need for a multidisciplinary approach},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Air passenger preferences: An international comparison
affects boarding theory. <em>OR</em>, <em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Managerial optimization challenges in service industries often entail the need to ensure customer satisfaction. For example, in airplane boarding, the boarding time should be minimized, but to ensure customer satisfaction, the process must not be too stressful for passengers. However, many authors assume that total boarding time minimization and customer satisfaction are complementary goals, even though there is little empirical knowledge on the topic. We challenge this assumption and contend that the discomfort perceived by an individual primarily depends on their personal boarding time, that is, the time spent waiting to be seated, and only somewhat on the total boarding time of all passengers. It is known that letting slow passengers (e.g., passengers with overhead bin luggage) enter the plane first reduces the total boarding time. However, in the theory section of our paper, we show that the average individual boarding time is minimized by applying the contrary procedure, that is, having slow passengers enter last. This policy modestly increases total boarding time but greatly reduces average individual boarding time. Moreover, we propose a new boarding policy that offers the best of both worlds. Thus, if it is true that passengers care greatly about their individual boarding times, then airlines should rethink their policies. To evaluate this and other hypotheses, we conduct an international survey on air passenger preferences with 1,500 participants equally drawn from Germany, Israel, and the United States. In addition to providing some interesting results on flight preferences in general, the research confirms our hypothesis on individual boarding time. History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science. Funding: This work was supported by the German Science Foundation (Deutsche Forschungsgemeinschaft) through the grant “Airplane Boarding” [Grant JA 2311/3-1]. Supplemental Material: The data and e-companion are available at https://doi.org/10.1287/opre.2021.2148 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2148},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Air passenger preferences: An international comparison affects boarding theory},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Who is next: Patient prioritization under emergency
department blocking. <em>OR</em>, <em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upon arrival at emergency departments (EDs), patients are classified into different triage levels indicating their urgency. Using data from a large hospital in Canada, we find that, within the same triage level, the average waiting time (time from triage to initial assessment by a physician) of patients who are discharged is shorter than that of patients who are admitted for middle- and low-acuity patients, suggesting that the order in which patients are served deviates from first-come, first-served, and to a certain extent, discharged patients are prioritized over admitted patients. This observation is intriguing as, among patients of the same triage level, admitted patients—who need further care in the hospital—should be deemed no less urgent than discharged patients who only need treatment at the ED. To understand how ED decision makers choose the next patient for treatment, we estimate a discrete-choice model and find that ED decision makers apply urgency-specific delay-dependent prioritization. Moreover, we find that, when the ED blocking level is sufficiently low, admitted patients are prioritized over discharged patients for high-acuity patients, whereas disposition does not affect the prioritization of middle- and low-acuity patients. When the ED blocking level becomes sufficiently high, decision makers start to prioritize discharged patients in an effort to avoid further blocking the ED. We then analyze a stylized model to explain the rationale behind the change in decision makers’ prioritization behavior as the ED blocking level increases. Using a simulation study, we demonstrate how policies inspired by our findings improve ED operations by reducing the average patient waiting time and length of stay, resulting in significant cost savings for hospitals. We also show how to leverage our findings to improve the accuracy of ED waiting time predictions. By testing and highlighting the central role of decision makers’ patient prioritization behavior, this paper advances our understanding of ED operations and patient flow. History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science. Funding: This work was partially supported by grants from the Research Grants Council of the Hong Kong Special Administration Region [Project No. CRF C7162-20GF, CityU 21500517, and CityU 11504620] and by grants from the Natural Science Foundation of China [Project No. 72091211]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.2187 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2187},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Who is next: Patient prioritization under emergency department blocking},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The gatekeeper’s dilemma: “When should i transfer this
customer?” <em>OR</em>, <em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many service encounters, frontline workers (often referred to as gatekeepers) have the discretion to attempt to resolve a customer request or to transfer the customer to an expert service provider. Motivated by an incentive redesign at a call center of a midsize U.S.-based bank, we formulate and solve an analytical model of the gatekeeper’s transfer response to different incentive schemes and congestion levels. We then test several model predictions experimentally. Our experiments show that human behavior matches the predictions qualitatively but not always in magnitude. Specifically, transfer rates are disproportionately low in the presence of monetary penalties for transferring even after controlling for the economic (dis)incentive to transfer, suggesting an overreaction to transfer cost. In contrast, the transfer response to congestion information shows no systematic bias. Taken together, these results advance our understanding of cognitive capabilities and rationality limits on human server behavior in queueing systems. History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2211},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {The gatekeeper’s dilemma: “When should i transfer this customer?”},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pooling agents for customer-intensive services. <em>OR</em>,
<em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In customer-intensive services where service quality increases with service time, service providers commonly pool their agents and give performance bonuses that reward agents for achieving greater customer satisfaction and serving more customers. Conventional wisdom suggests that pooling agents reduce customer wait time while performance bonuses motivate agents to produce high-quality service, both of which should boost customer satisfaction. However, our queueing-game-theoretic analysis reveals that when agents act strategically, they may choose to speed up under pooling in an attempt to serve more customers, thus undermining service quality. If this happens, pooling can backfire and result in both lower customer satisfaction and agent payoff. We propose a simple solution to resolve this issue: pooling a portion of the performance bonuses (incentive pooling) in conjunction with pooling agents (operational pooling). History : This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science. Supplemental Material : The online appendices are available at https://doi.org/10.1287/opre.2022.2259 . Funding: Z. Wang acknowledges support from the National Natural Science Foundation of China [Grants 72001118, 72132007].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2259},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Pooling agents for customer-intensive services},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Need for speed: The impact of in-process delays on customer
behavior in online retail. <em>OR</em>, <em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impact of delays has been widely studied in various offline services. The focus of this study is online services, and we explore the impact of in-process delays—measured by website speed—on customer behavior. We leverage novel retail and website speed data to investigate how delays impact online sales and how customer sensitivity to in-process delays varies across the different stages of a customer’s shopping journey. We estimate sizable adverse effects of website slowdowns on online sales. Using threshold regression models, we show that customers exhibit diminishing sensitivity to increases in website slowdowns. Our results suggest that waiting times affect customer abandonment differently at different stages of the shopping journey. Customers are more sensitive to slowdowns at the checkout stage. Our findings have implications for website design decisions such as improving website speed at the checkout stage, selecting third-party content providers, and customizing the design of mobile and desktop channels. The paper’s results are especially relevant in the current regulatory environment with ongoing policy debates about net neutrality. History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2262 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2262},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Need for speed: The impact of in-process delays on customer behavior in online retail},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mismanaging diagnostic accuracy under congestion.
<em>OR</em>, <em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To study the effect of congestion on the fundamental tradeoff between diagnostic accuracy and speed, we empirically test the predictions of a formal sequential testing model in a setting where the gathering of additional information can improve diagnostic accuracy but may also take time and increase congestion as a result. The efficient management of such systems requires a careful balance of congestion-sensitive stopping rules. These include diagnoses made based on very little or no diagnostic information and the stopping of diagnostic processes while waiting for information. We test these rules under controlled laboratory conditions and link the observed biases to system dynamics and performance. Our data show that decision makers (DMs) stop diagnostic processes too quickly at low congestion levels where information acquisition is relatively cheap. However, they fail to stop quickly enough when increasing congestion requires the DM to diagnose without testing or diagnose while waiting for test results. Essentially, DMs are insufficiently sensitive to congestion. As a result of these behavioral patterns, DMs manage the system with both lower-than-optimal diagnostic accuracy and higher-than-optimal congestion cost, underperforming on both sides of the accuracy/speed tradeoff. History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science. Funding: This research was partially funded by the Deutsche Forschungsgemeinschaft [Grant VE 897/4-1]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2292 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2292},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Mismanaging diagnostic accuracy under congestion},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Early reservation for follow-up appointments in a
slotted-service queue. <em>OR</em>, <em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an appointment-based slotted-service queue with the goal of maximizing service volume. Returning customers prefer to be served by the same service agent as in their previous visit. This model captures aspects of a whole host of settings, including medical clinics, law firms, and tutoring services. We consider a simple strategy that a service provider may use to reduce balking among returning customers—designate some returning customers as high-priority customers. These customers are placed at the head of the queue when they call for a follow-up appointment. In an appointment-based system, this policy can be implemented by booking a high-priority returning customer’s appointment right before he or she leaves the service facility. We focus on a need-based policy in which the decision to prioritize some customers depends on their return probability. We analyze three systems: an open-access system, a traditional appointment system, and a carve-out system. We show that in an open-access system, the service provider should never prioritize returning customers in order to maximize the throughput rate. However, it is always optimal to prioritize some customers in a traditional appointment system. In the carve-out system, which may be modeled as a system with two parallel queues, the optimal policy varies depending on which queue is more congested. In the traditional system, we prove that the throughput rate is a quasi-concave function of the threshold under the assumption that returning customers see time averages. This allows service systems to determine optimal operating policies that are both easy to implement and provably optimal. History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science. Funding: This work was supported by Fonds de Recherche du Québec-Société et Culture [Grant 295837], National Natural Science Foundation of China [Grant 72061127002, 71731009], and Natural Sciences and Engineering Research Council of Canada [Grant RGPIN 2019-05539]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2299 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2299},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Early reservation for follow-up appointments in a slotted-service queue},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). To batch or not to batch? Impact of admission batching on
emergency department boarding time and physician productivity.
<em>OR</em>, <em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the behavior of batching by discretionary workers in the first stage of a two-stage queuing system and explore the trade-off it causes between their productivity and second stage wait times. Specifically, we focus on the behavior of batching admissions by emergency department (ED) physicians. Using data from a large hospital, we show that the probability of batching admissions is increasing in the hour of an ED physician’s shift, and that batched patients experience a 4.7\% longer delay from hospital admission to receiving an inpatient bed. Using a mediation analysis, we show that this effect is partially due to the increase in the coefficient of variation of inpatient bed requests caused by batching. However, we also find that batching admissions is associated with an average of 10.0\% more patients seen in a shift, and a 2.6 minute reduction in a physician’s average throughput time. An important implication of our work is that workers may induce delays in downstream stages, caused by practices that increase their productivity. History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2335 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2335},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {To batch or not to batch? impact of admission batching on emergency department boarding time and physician productivity},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physician discretion and patient pick-up: How familiarity
encourages multitasking in the emergency department. <em>OR</em>,
<em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient demand for emergency medical services continues to rise from all-time highs. Physicians generally respond to the rising demand by increasing the level of multitasking. What leads emergency department (ED) physicians to select which patients, and how many patients, to treat? Queuing models frequently assume individual servers operate independently of other servers. In contrast, we consider how familiarity between peer physicians affects patient selection and the chosen multitasking level, a process more commonly known in the ED as “patient pick-up.” Using observations from two EDs, we explore whether familiarity alters patient pick-up behavior, we determine the effect of familiarity on multitasking, and we measure the combined impact of familiarity and multitasking on other ED outcomes. Among ED physicians, greater average familiarity leads to an increase in patient pick-up rate, observed multitasking, and shorter patient wait time, with no identifiable negative impact to patient processing time or length of stay. Moreover, the effects intensify at the end of a physician’s shift and for patients in severe condition. Within more familiar groups, physicians appear willing to exert more effort. Our study clarifies how the benefits materialize and illustrates why researchers must consider server familiarity moving forward. History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2350},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Physician discretion and patient pick-up: How familiarity encourages multitasking in the emergency department},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Balancing agent retention and waiting time in service
platforms. <em>OR</em>, <em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many service industries, the speed of service and support by experienced employees are two major drivers of service quality. When demand for a service is variable and the staffing requirements cannot be adjusted quickly, choosing capacity levels requires making a tradeoff between service speed and operating costs, both of which depend on worker utilization. However, recent business models have enabled service systems to access a large pool of employees with flexible working hours that are compensated through piece-rates. Although this business model can operate at low levels of utilization without increasing operating costs, a different tradeoff emerges: The service platform must control employee turnover, which may increase when employees are working at low levels of utilization. Hence, to make staffing decisions and manage worker utilization, it is necessary to understand both customer conversion and employee retention, measuring their sensitivity to service time and utilization, respectively. In our application, we study an outbound call center that operates with a pool of flexible agents working remotely to sell auto insurance. We develop an econometric approach to model customer behavior that captures two key features of outbound calls: time sensitivity and employee heterogeneity. We find a strong impact of contact time on customer behavior: Conversion rates drop by 31\% when the time to make the first outbound call increases from 5 to 30 minutes. In addition, we use a survival model to measure how agent retention is affected by utilization (which determined by workload and total staffing capacity) and find that, for more experienced worker, a 10\% increase in utilization translates into a 33\% decrease in weekly agent attrition. These empirical models of customer and agent behavior are combined to illustrate how to balance customer conversion and employee retention, showing that both are relevant to plan staffing and allocate workload in the context of an on-demand service platform. History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science. Funding: This work was supported by Agencia Nacional de Investigación y Desarrollo (ANID) [Grant PIA/APOYO AFB220003] and FONDECYT-Chile 1181201. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2418 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2418},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Balancing agent retention and waiting time in service platforms},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nudging patient choice: Reducing no-shows using waits
framing messaging. <em>OR</em>, <em>71</em>(3), iii–vi. (<a
href="https://doi.org/10.1287/opre.2023.2444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient no-shows for scheduled medical appointments are of great concern for many healthcare providers. In this paper, we tackle the no-show problem by applying insights from behavioral science. Specifically, we “nudge” patients into arriving for their scheduled appointment using text reminders of their upcoming visit. We conduct a field experiment at an outpatient specialty clinic, where we add to the standard message an additional line of text that indicates a potentially long wait for the next available appointment (we call this intervention “waits framing”). Based on a difference-in-differences estimation strategy, we find that waits framing messaging significantly reduces no-shows by a factor of 28.6\%. We also find heterogeneous patient responses to the nudge. First, the increased salience of waiting time is more effective in reducing no-shows among patients who are more sensitive to wait. In addition, the effectiveness of our nudge also depends on the novelty and credibility of the information in the message. For example, the waits framing effect is stronger when the patient is less familiar with the provider (i.e., the information is new), and if the scheduled provider is in higher demand (i.e., the information is credible). In a laboratory experiment, we uncover the mechanism that underlies the nudge—waits framing serves to increase the perceived cost of missing an appointment. Through the combination of field and designed laboratory studies, we provide both external and internal validity to the effects of waits framing. Our results have implications at the system level; the message framing leads to a significant improvement in capacity utilization and patient throughput. These findings contribute to the literature on behavioral queuing by showing that through appropriately framed messages, queue operators can influence the waiting cost perceptions of individuals and thereby engender a desired queuing response such as a reduction in queue abandonment. History: This paper has been accepted for the Operations Research Special Issue on Behavioral Queueing Science. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2023.2444 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2444},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Nudging patient choice: Reducing no-shows using waits framing messaging},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk-based robust statistical learning by stochastic
difference-of-convex value-function optimization. <em>OR</em>,
<em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the use of a variant of the conditional value-at-risk (CVaR) risk measure, called the interval conditional value-at-risk (In-CVaR), for the treatment of outliers in statistical learning by excluding the risks associated with the left and right tails of the loss. The risk-based robust learning task is to minimize the In-CVaR risk measure of a random functional that is the composite of a piecewise affine loss function with a potentially nonsmooth difference-of-convex statistical learning model. With the optimization formula of CVaR, the objective function of the minimization problem is the difference of two convex functions each being the optimal objective value of a univariate convex stochastic program. An algorithm that combines sequential sampling and convexification is developed, and its subsequential almost-sure convergence to a critical point is established. Numerical experiments demonstrate the effectiveness of the In-CVaR–based estimator computed by the sampling-based algorithm for robust regression and classification. Overall, this research extends the traditional approaches for treating outliers by allowing nonsmooth and nonconvex statistical learning models, employing a population risk-based objective, and applying a sampling-based algorithm with the stationarity guarantee for solving the resulting nonconvex and nonsmooth stochastic program. Funding: The first author’s research is partially supported by the National Science Foundation of China [Grant 72188101]. This work was also based on research supported by the National Science Foundation [Grant IIS-1632971] and the Air Force Office of Scientific Research [Grant FA9550-18-1-0382].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2248},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Risk-based robust statistical learning by stochastic difference-of-convex value-function optimization},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On system-wide safety staffing of large-scale parallel
server networks. <em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a “system-wide safety staffing” (SWSS) parameter for multiclass multipool networks of any tree topology, Markovian or non-Markovian, in the Halfin-Whitt regime. This parameter can be regarded as the optimal reallocation of the capacity fluctuations (positive or negative) of order n when each server pool uses a square-root staffing rule. We provide an explicit form of the SWSS as a function of the system parameters, which is derived using a graph theoretic approach based on Gaussian elimination. For Markovian networks, we give an equivalent characterization of the SWSS parameter via the drift parameters of the limiting diffusion. We show that if the SWSS parameter is negative, the limiting diffusion and the diffusion-scaled queueing processes are transient under any Markov control and cannot have a stationary distribution when this parameter is zero. If it is positive, we show that the diffusion-scaled queueing processes are uniformly stabilizable ; that is, there exists a scheduling policy under which the stationary distributions of the controlled processes are tight over the size of the network. In addition, there exists a control under which the limiting controlled diffusion is exponentially ergodic. Thus, we identified a necessary and sufficient condition for the uniform stabilizability of such networks in the Halfin-Whitt regime. We use a constant control resulting from the leaf elimination algorithm to stabilize the limiting controlled diffusion while a family of Markov scheduling policies that are easy to compute are used to stabilize the diffusion-scaled processes. Finally, we show that under these controls the processes are exponentially ergodic and the stationary distributions have exponential tails. Funding: This work was supported by the National Science Foundation [Grants DMS-1715210, CMMI-1635410, and DMS/CMMI-1715875], the Office of Naval Research [Grant N00014-16-1-2956], and the Army Research Office [Grant W911NF-17-1-0019].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2256},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {On system-wide safety staffing of large-scale parallel server networks},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized exact scheduling: A minimal-variance distributed
deadline scheduler. <em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many modern schedulers can dynamically adjust their service capacity to match the incoming workload. At the same time, however, unpredictability and instability in service capacity often incur operational and infrastructural costs. In this paper, we seek to characterize optimal distributed algorithms that maximize the predictability, stability, or both when scheduling jobs with deadlines. Specifically, we show that Exact Scheduling minimizes both the stationary mean and variance of the service capacity subject to strict demand and deadline requirements. For more general settings, we characterize the minimal-variance distributed policies with soft demand requirements, soft deadline requirements, or both. The performance of the optimal distributed policies is compared with that of the optimal centralized policy by deriving closed-form bounds and by testing centralized and distributed algorithms using real data from the Caltech electrical vehicle charging facility and many pieces of synthetic data from different arrival distributions. Moreover, we derive the Pareto-optimality condition for distributed policies that balance the variance and mean square of the service capacity. Finally, we discuss a scalable partially centralized algorithm that uses centralized information to boost performance and a method to deal with missing information on service requirements. Funding: This work was supported by Japan Science and Technology Agency, PRESTO [Grant JPMJPR2136], Japan and Agencia Nacional de Investigación e Innovación?FSE [Grant FSE_1_2018_1_153050], Uruguay.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2232},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Generalized exact scheduling: A minimal-variance distributed deadline scheduler},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A convex reformulation and an outer approximation for a
large class of binary quadratic programs. <em>OR</em>, <em>71</em>(2),
iii–vi. (<a href="https://doi.org/10.1287/opre.2021.2241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a general modeling and solving framework for a large class of binary quadratic programs subject to variable partitioning constraints. Problems in this class have a wide range of applications as many binary quadratic programs with linear constraints can be represented in this form. By exploiting the structure of the partitioning constraints, we propose mixed-integer nonlinear programming (MINLP) and mixed-integer linear programming (MILP) reformulations and show the relationship between the two models in terms of the relaxation strength. Our solution methodology relies on a convex reformulation of the proposed MINLP and a branch-and-cut algorithm based on outer approximation cuts, in which the cuts are generated on the fly by efficiently solving separation subproblems. To evaluate the robustness and efficiency of our solution method, we perform extensive computational experiments on various quadratic combinatorial optimization problems. The results show that our approach outperforms the state-of-the-art solver applied to different MILP reformulations of the corresponding problems. Funding: This work was supported by the Natural Sciences and Engineering Research Council of Canada [Grants RGPIN- 2020-05395 and RGPIN 2015-06289] and Fonds de Recherche du Québec - Nature et technologies [FRQNT NC-198928].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2241},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {A convex reformulation and an outer approximation for a large class of binary quadratic programs},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A strongly polynomial algorithm for linear exchange markets.
<em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a strongly polynomial algorithm for computing an equilibrium in Arrow-Debreu exchange markets with linear utilities. Our algorithm is based on a variant of the weakly polynomial Duan–Mehlhorn (DM) algorithm. We use the DM algorithm as a subroutine to identify revealed edges—that is, pairs of agents and goods that must correspond to the best bang-per-buck transactions in every equilibrium solution. Every time a new revealed edge is found, we use another subroutine that decides if there is an optimal solution using the current set of revealed edges or, if none exists, finds the solution that approximately minimizes the violation of the demand and supply constraints. This task can be reduced to solving a linear program (LP). Even though we are unable to solve this LP in strongly polynomial time, we show that it can be approximated by a simpler LP with two variables per inequality that is solvable in strongly polynomial time. Funding: Financial support from the Division of Computing and Communication Foundations, National Science Foundation (NSF) [Grants 1755619 and 1942321] and from European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme [Grant ScaleOpt-757481] is gratefully acknowledged.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2258},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {A strongly polynomial algorithm for linear exchange markets},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and simple solutions of blotto games. <em>OR</em>,
<em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Colonel Blotto game, which was initially introduced by Borel in 1921, two colonels simultaneously distribute their troops across different battlefields. The winner of each battlefield is determined independently by a winner-takes-all rule. The ultimate payoff for each colonel is the number of battlefields won. The Colonel Blotto game is commonly used for analyzing a wide range of applications from the U.S. Presidential election to innovative technology competitions to advertising, sports, and politics. There are persistent efforts to find the optimal strategies for the Colonel Blotto game. However, the first polynomial-time algorithm for that has very recently been provided by Ahmadinejad, Dehghani, Hajiaghayi, Lucier, Mahini, and Seddighin. Their algorithm consists of an exponential size linear program (LP), which they solve using the ellipsoid method. Because of the use of the ellipsoid method, despite its significant theoretical importance, this algorithm is highly impractical. In general, even the simplex method (despite its exponential running time in practice) performs better than the ellipsoid method in practice. In this paper, we provide the first polynomial-size LP formulation of the optimal strategies for the Colonel Blotto game using linear extension techniques. Roughly speaking, we consider the natural representation of the strategy space polytope and transform it to a higher dimensional strategy space, which interestingly has exponentially fewer facets. In other words, we add a few variables to the LP such that, surprisingly, the number of constraints drops down to a polynomial. We use this polynomial-size LP to provide a simpler and significantly faster algorithm for finding optimal strategies of the Colonel Blotto game. We further show this representation is asymptotically tight, which means there exists no other linear representation of the strategy space with fewer constraints. We also extend our approach to multidimensional Colonel Blotto games, in which players may have different sorts of budgets, such as money, time, human resources, etc. By implementing this algorithm, we are able to run tests that were previously impossible to solve in a reasonable time. This information allows us to observe some interesting properties of Colonel Blotto; for example, we find out the behavior of players in the discrete model is very similar to the continuous model Roberson solved. Funding: This work was supported in part by NSF CAREER award CCF-1053605, NSF BIGDATA [Grant IIS-1546108], NSF AF:Medium [Grant CCF-1161365], DARPA GRAPHS/AFOSR [Grant FA9550-12-1-0423], and another DARPA SIMPLEX grant.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2261},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Fast and simple solutions of blotto games},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A first-order approach to accelerated value iteration.
<em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov decision processes (MDPs) are used to model stochastic systems in many applications. Several efficient algorithms to compute optimal policies have been studied in the literature, including value iteration (VI) and policy iteration. However, these do not scale well, especially when the discount factor for the infinite horizon discounted reward, λ , gets close to one. In particular, the running time scales as O ( 1 / ( 1 − λ ) ) for these algorithms. In this paper, our goal is to design new algorithms that scale better than previous approaches when λ approaches 1. Our main contribution is to present a connection between VI and gradient descent and adapt the ideas of acceleration and momentum in convex optimization to design faster algorithms for MDPs. We prove theoretical guarantees of faster convergence of our algorithms for the computation of the value function of a policy, where the running times of our algorithms scale as O ( 1 / 1 − λ ) for reversible MDP instances. The improvement is quite analogous to Nesterov’s acceleration and momentum in convex optimization. We also provide a lower bound on the convergence properties of any first-order algorithm for solving MDPs, presenting a family of MDPs instances for which no algorithm can converge faster than VI when the number of iterations is smaller than the number of states. We introduce safe accelerated value iteration (S-AVI), which alternates between accelerated updates and value iteration updates. Our algorithm S-AVI is worst-case optimal and retains the theoretical convergence properties of VI while exhibiting strong empirical performances and providing significant speedups when compared with classical approaches (up to one order of magnitude in many cases) for a large test bed of MDP instances. Funding: J. Grand-Clément is supported by a grant of the French National Research Agency (ANR), “Investissements d’Avenir” [Grant LabEx Ecodec/ANR-11-LABX-0047] and by Hi! Paris. The research of V. Goyal was supported in part by the Defense Advanced Research Projects Agency [Grant Lagrange] and the Division of Civil, Mechanical and Manufacturing Innovation [Grant 1636046].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2269},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {A first-order approach to accelerated value iteration},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiobjective optimization for politically fair
districting: A scalable multilevel approach. <em>OR</em>,
<em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Political districting in the United States is a decennial process of redrawing the boundaries of congressional and state legislative districts. The notion of fairness in political districting has been an important topic of subjective debate, with district plans affecting a wide range of stakeholders, including the voters, candidates, and political parties. Even though districting as an optimization problem has been well studied, existing models primarily rely on nonpolitical fairness measures such as the compactness of districts. This paper presents mixed integer linear programming (MILP) models for districting with political fairness criteria based on fundamental fairness principles such as vote-seat proportionality (efficiency gap), partisan (a)symmetry, and competitiveness. A multilevel algorithm is presented to tackle the computational challenge of solving large practical instances of these MILPs. This algorithm coarsens a large graph input by a series of graph contractions and solves an exact biobjective problem at the coarsest graph using the ϵ − constraint method. A case study on congressional districting in Wisconsin demonstrates that district plans constituting the approximate Pareto-front are geographically compact, as well as efficient (i.e., proportional), symmetric, or competitive. An algorithmically transparent districting process that incorporates the goals of multiple stakeholders requires a multiobjective approach like the one presented in this study. To promote transparency and facilitate future research, the data, code, and district plans are made publicly available. Funding: The third author was supported by the Air Force Office of Scientific Research [Grant FA9550-19-1-0106]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2311 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2311},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Multiobjective optimization for politically fair districting: A scalable multilevel approach},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online matching with stochastic rewards: Optimal competitive
ratio via path-based formulation. <em>OR</em>, <em>71</em>(2), iii–vi.
(<a href="https://doi.org/10.1287/opre.2022.2345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of online matching with stochastic rewards is a generalization of the online bipartite matching problem where each edge has a probability of success. When a match is made it succeeds with the probability of the corresponding edge. We consider the more general vertex-weighted version of the problem and give two new results. First, we show that a natural generalization of the perturbed-greedy algorithm is ( 1 − 1 / e ) competitive when probabilities decompose as a product of two factors, one corresponding to each vertex of the edge. This is the best achievable guarantee as it includes the case of identical probabilities and, in particular, the classical online bipartite matching problem. Second, we give a deterministic 0.596 competitive algorithm for the previously well-studied case of fully heterogeneous but vanishingly small edge probabilities. A key contribution of our approach is the use of novel path-based formulations and a generalization of the primal-dual scheme. These allow us to compare against the natural benchmarks of adaptive offline algorithms that know the sequence of arrivals and the edge probabilities in advance but not the outcomes of potential matches. These ideas may be of independent interest in other online settings with postallocation stochasticity. Funding: This research was partially supported by the National Science Foundation [Grant CMMI 1636046 and Award CMMI 1351838]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2345 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2345},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Online matching with stochastic rewards: Optimal competitive ratio via path-based formulation},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differential privacy in personalized pricing with
nonparametric demand models. <em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, the advance of information technology and abundant personal data facilitate the application of algorithmic personalized pricing. However, this leads to the growing concern of potential violation of privacy because of adversarial attack. To address the privacy issue, this paper studies a dynamic personalized pricing problem with unknown nonparametric demand models under data privacy protection. Two concepts of data privacy, which have been widely applied in practices, are introduced: central differential privacy (CDP) and local differential privacy (LDP) , which is proved to be stronger than CDP in many cases. We develop two algorithms that make pricing decisions and learn the unknown demand on the fly while satisfying the CDP and LDP guarantee, respectively. In particular, for the algorithm with CDP guarantee, the regret is proved to be at most O ˜ ( T ( d + 2 ) / ( d + 4 ) + ε − 1 T d / ( d + 4 ) ) . Here, the parameter T denotes the length of the time horizon, d is the dimension of the personalized information vector, and the key parameter ε &gt; 0 measures the strength of privacy (smaller ε indicates a stronger privacy protection). Conversely, for the algorithm with LDP guarantee, its regret is proved to be at most O ˜ ( ε − 2 / ( d + 2 ) T ( d + 1 ) / ( d + 2 ) ) , which is near optimal as we prove a lower bound of Ω ( ε − 2 / ( d + 2 ) T ( d + 1 ) / ( d + 2 ) / d 7 / 3 ) for any algorithm with LDP guarantee. Funding: X. Chen is supported by the National Science Foundation [Grant IIS-1845444]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2347 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2347},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Differential privacy in personalized pricing with nonparametric demand models},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revenue management under a mixture of independent demand and
multinomial logit models. <em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider assortment optimization problems when customers choose under a mixture of independent demand and multinomial logit models. In the assortment optimization setting, each product has a fixed revenue associated with it. The customers choose among the products according to our mixture choice model. The goal is to find an assortment that maximizes the expected revenue from a customer. We show that we can find the optimal assortment by solving a linear program. We establish that the optimal assortment becomes larger as the relative size of the customer segment with the independent demand model increases. Moreover, we show that the Pareto-efficient assortments that maximize a weighted average of the expected revenue and the total purchase probability are nested, in the sense that the Pareto-efficient assortments become larger as the weight on the total purchase probability increases. Considering the assortment optimization problem with a capacity constraint on the offered assortment, we show that the problem is NP-hard, even when each product consumes unit capacity, so that we have a constraint on the number of offered products. We give a fully polynomial-time approximation scheme. In the assortment-based network revenue-management problem, we have resources with limited capacities, and each product consumes a combination of resources. The goal is to find a policy for deciding which assortment of products to offer to each arriving customer to maximize the total expected revenue over a finite selling horizon. A standard linear-programming approximation for this problem includes one decision variable for each subset of products. We show that this linear program can be reduced to an equivalent one of substantially smaller size. We give an expectation-maximization algorithm to estimate the parameters of our mixture model. Our computational experiments indicate that our mixture model can provide improvements in predicting customer purchases and identifying profitable assortments. Funding: The work of Y. Cao was partly supported by the National Natural Science Foundation of China [Grant 72131010]. The work of H. Topaloglu was partly supported by the National Science Foundation [Grant CMMI-1825406]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2333 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2333},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Revenue management under a mixture of independent demand and multinomial logit models},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the design of public debate in social networks.
<em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model of the joint evolution of opinions and social relationships in a setting in which social influence decays over time. The dynamics are based on bounded confidence: social connections between individuals with distant opinions are severed, whereas new connections are formed between individuals with similar opinions. Our model naturally gives rise to strong diversity, that is, the persistence of heterogeneous opinions in connected societies, a phenomenon that most existing models fail to capture. The intensity of social interactions is the key parameter that governs the dynamics. First, it determines the asymptotic distribution of opinions. In particular, increasing the intensity of social interactions brings society closer to consensus. Second, it determines the risk of polarization, which is shown to increase with the intensity of social interactions. Our results allow us to frame the problem of the design of public debates in a formal setting. We, hence, characterize the optimal strategy for a social planner who controls the intensity of the public debate and, thus, faces a trade-off between the pursuit of social consensus and the risk of polarization. We also consider applications to political campaigning and show that both minority and majority candidates can have incentives to lead society toward polarization. Funding: The authors acknowledge financial support from the European Union’s Horizon 2020 research and innovation program [Grants 721846, “Expectations and Social Influence Dynamics in Economics (ExSIDE)” and 956107 “Economic Policy in Complex Environments”]. A. Mandel acknowledges support from the European Union’s Horizon 2020 research and innovation program [Grants 884565, “Enabling Positive Tipping Points Toward Clean-Energy Transitions in Coal and Carbon Intensive Regions”].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2356},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {On the design of public debate in social networks},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributionally robust losses for latent covariate
mixtures. <em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While modern large-scale data sets often consist of heterogeneous subpopulations—for example, multiple demographic groups or multiple text corpora—the standard practice of minimizing average loss fails to guarantee uniformly low losses across all subpopulations. We propose a convex procedure that controls the worst case performance over all subpopulations of a given size. Our procedure comes with finite-sample (nonparametric) convergence guarantees on the worst-off subpopulation. Empirically, we observe on lexical similarity, wine quality, and recidivism prediction tasks that our worst case procedure learns models that do well against unseen subpopulations. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2363 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2363},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Distributionally robust losses for latent covariate mixtures},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal pricing and introduction timing of technology
upgrades in subscription-based services. <em>OR</em>, <em>71</em>(2),
iii–vi. (<a href="https://doi.org/10.1287/opre.2022.2364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of subscription-based services, many technologies improve over time, and service providers can provide increasingly powerful service upgrades to their customers but at a launching cost and the expense of the sales of existing products. We propose a model of technology upgrades and characterize the optimal pricing and timing of technology introductions for a service provider who price-discriminates among customers based on their upgrade experience in the face of customers who are averse to switching to improved offerings. We first characterize optimal discriminatory pricing for the infinite horizon pricing problem with fixed introduction times. We reduce the optimal pricing problem to a tractable optimization problem and propose an efficient algorithm for solving it. Our algorithm computes optimal discriminatory prices within a fraction of a second even for large problem instances. We then show that periodic introduction times, combined with optimal pricing, enjoy optimality guarantees. In particular, we first show that, as long as the introduction intervals are constrained to be nonincreasing, it is optimal to have periodic introductions after an initial warm-up phase. When allowing general introduction intervals, we show that periodic introduction intervals after some time are optimal in a more restricted sense. Numerical experiments suggest that it is generally optimal to have periodic introductions after an initial warm-up phase. Finally, we focus on a setting in which the firm does not price-discriminate based on customers’ experience. We show both analytically and numerically that in the nondiscriminatory setting, a simple policy of Myerson (i.e., myopic) pricing and periodic introductions enjoys good performance guarantees. Funding: This material is based upon work supported by INSEAD and University Pierre et Marie Curie [Grant ELICIT], as well as by the National Science Foundation [Grant 2110707]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2364 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2364},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Optimal pricing and introduction timing of technology upgrades in subscription-based services},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infinite-dimensional fisher markets and tractable fair
division. <em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear Fisher markets are a fundamental economic model with diverse applications. In the finite-dimensional case of n buyers and m items, a market equilibrium can be computed using the celebrated Eisenberg-Gale convex program. Motivated by large-scale Internet advertising and fair division applications, we consider a generalization of a linear Fisher market where there is a finite set of buyers and a measurable item space. We introduce generalizations of the Eisenberg-Gale convex program and its dual to this setting, which leads to infinite-dimensional Banach-space optimization problems. We show that these convex programs always have optimal solutions, and these optimal solutions correspond to market equilibria. In particular, a market equilibrium always exists. We also show that Karush-Kuhn-Tucker–type optimality conditions for these convex programs imply the defining properties of market equilibria and are necessary and sufficient for a solution pair to be optimal. Then, we show that, similar to the classical finite-dimensional case, a market equilibrium is Pareto optimal, envy-free, and proportional. Moreover, when the item space measure is atomless (e.g., the Lebesgue measure), we show that there always exists a pure equilibrium allocation, which can be viewed as a generalized fair division, that is, a Pareto optimal, envy-free, and proportional partition of the item space. This leads to generalizations of classical results on the existence and characterizations of fair division of a measurable set. When the item space is a closed interval and buyers have piecewise linear valuations, we show that the infinite-dimensional Eisenberg-Gale–type convex program can be reformulated as a finite-dimensional convex conic program, which can be solved efficiently using off-the-shelf optimization software. Based on the convex conic reformulation, we also develop the first polynomial-time algorithm for finding a fair division of an interval under piecewise linear valuations. For general buyer valuations or a very large number of buyers, we propose computing market equilibria using stochastic optimization and give high-probability convergence guarantees. Finally, we show that most of the above results easily extend to the case of quasilinear utilities. Funding: Y. Gao is supported by the Columbia Engineering Presidential Fellowship and the Columbia Engineering Cheung-Kong Innovation Doctoral Fellowship. C. Kroer is supported by the Office of Naval Research Young Investigator Program [Grant N00014-22-1-2530]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2344 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2344},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Infinite-dimensional fisher markets and tractable fair division},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stall economy: The value of mobility in retail on wheels.
<em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban open space emerges as a new territory to embrace retail innovations. Selling products in public spaces with wheeled stalls can potentially become ubiquitous in our future cities. Transition into such a “stall economy” paradigm is being spurred by the rapidly advancing self-driving technologies. Motivated by this transformation, this paper provides models, theory, and insights of spatial queueing systems, in which one server moves around to meet mobile customers/machines and in which the “last 100 meters” are expensive. Specifically, we study two service modes: (i) on-demand, first come, first served and (ii) spatially and temporally pooling customer demands. In each mode, we derive the dependence of customer waiting and stall repositioning on two key decisions: the service zone size and the walking distance imposed on customers to meet a stall. In particular, for the on-demand mode, we propose and solve a “rendezvous problem” to analytically characterize the spatial distribution of the stall-customer meeting locations. We also propose a stylized joint truck-stall routing model to capture the inventory replenishment operations. Our main finding is that the stall economy potentially profits more than stationary retail, not only because of the mobility of stalls for providing proximity to customers, but also because of its operational flexibilities that allow for avoiding the “last 100 meters” and pooling demands. In a broader sense, this work looks toward an expanded scope of future retail empowered by self-driving technologies. Funding: W. Qi acknowledges the support from the National Natural Science Foundation of China [Grants 72188101, 72272014] and the Institute for Data Valorization (IVADO) [Grant PRF-2019-1761010386]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2404 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2404},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Stall economy: The value of mobility in retail on wheels},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Market making and incentives design in the presence of a
dark pool: A stackelberg actor–critic approach. <em>OR</em>,
<em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the issue of a market maker acting at the same time in the lit and dark pools of an exchange. The exchange wishes to establish a suitable make–take fee policy to attract transactions on its venues. We first solve the stochastic control problem of the market maker without the intervention of the exchange. Then, we derive the equations defining the optimal contract to be set between the market maker and the exchange. This contract depends on the trading flows generated by the market maker’s activity on the two venues. In both cases, we show existence and uniqueness in the viscosity sense of the solutions of the Hamilton–Jacobi–Bellman equations associated to the market maker and exchange’s problems. We finally design an actor–critic algorithm inspired by deep reinforcement learning methods, enabling us to approximate efficiently the optimal controls of the market maker and the optimal incentives to be provided by the exchange. Funding: This work benefits from the financial support of the Chaires Analytics and Models for Regulation; Financial Risk; Deep Finance, Statistics, Machine Learning and Systematic Methods in Finance; Finance and Sustainable Development. B. Baldacci and M. Rosenbaum gratefully acknowledge the financial support of the European Research Council [Grant 679836 Staqamof]. T. Mastrolia gratefully acknowledges the support of the Agence Nationale de la recherche projects PACMAN ANR-16-CE05-0027 and ReLISCoP ANR-21-CE40-0001.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2406},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Market making and incentives design in the presence of a dark pool: A stackelberg Actor–Critic approach},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disruption and rerouting in supply chain networks.
<em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study systemic risk in a supply chain network where firms are connected through purchase orders. Firms can be hit by cost or demand shocks, which can cause defaults. These shocks propagate through the supply chain network via input-output linkages between buyers and suppliers. Firms endogenously take contingency plans to mitigate the impact generated from disruptions. We show that, as long as firms have large initial equity buffers, network fragility is low if both buyer diversification and supplier diversification are low. We find that a single-sourcing strategy is beneficial for a firm only if the default probability of the firm’s supplier is low. Otherwise, a multiple-sourcing strategy is ex post more cost effective for a firm. Funding: J.R. Birge acknowledges financial support from the University of Chicago Booth School of Business. The research of A. Capponi has been supported by the NSF/CMMI CAREER-1752326 award. P.-C. Chen acknowledges financial support from the Research Grant Council of Hong Kong [Early Career Scheme Grant 27210118]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2409 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2409},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Disruption and rerouting in supply chain networks},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online passenger flow control in metro lines. <em>OR</em>,
<em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd management during peak commuting hours is a key challenge facing oversaturated metro systems worldwide, which results in serious safety concerns and uneven service experience for commuters on different origin-destination (o-d) pairs. This paper develops real-time passenger flow control policies to manage the inflow of crowds at each station, to optimize the total load carried or revenue earned (efficiency), and to ensure that adequate service is provided to passengers on each o-d pair (fairness), as much as possible. For given train capacity, we use Blackwell’s approachability theorem and Fenchel duality to characterize the attainable service level of each o-d pair. We use these insights to develop online policies for crowd control problems. Numerical experiments on a set of transit data from Beijing show that our approach performs well compared with existing benchmarks in the literature. Funding: This work was supported in part by the National Natural Science Foundation of China [Grants 72288101 and 72101042], the Natural Science Foundation of Chongqing, China [Grant CSTB2022NSCQ-MSX1667], and the 2019 Academic Research Fund Tier 3 of the Ministry of Education—Singapore [Grant MOE-2019-T3-1-010]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2417 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2417},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Online passenger flow control in metro lines},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beta and coskewness pricing: Perspective from probability
weighting. <em>OR</em>, <em>71</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security market line is often flat or downward-sloping. We hypothesize that probability weighting plays a role and one ought to differentiate between periods in which agents overweight extreme events and those in which they underweight them. Overweighting inflates the probability of extremely bad events and demands greater compensation for beta risk, whereas underweighting does the opposite. Unconditional on probability weighting, these two effects offset each other, resulting in a flat or slightly negative return–beta relationship. Similarly, overweighting the tails enhances the negative relationship between return and coskewness, whereas underweighting reduces it. We derive a three-moment conditional capital asset pricing model for a market with rank-dependent utility agents to make these predictions, and we support our theory through an extensive empirical study. Funding: This work was supported by National Natural Science Foundation of China [Grants 71971083, 71931004, 72171138]; Program for Innovative Research Team of Shanghai University of Finance and Economics [Grant 2020110930]; Open Research Fund of Key Laboratory of Advanced Theory and Application in Statistics and Data Science-Ministry of Education, East China Normal University; a start-up grant and the Nie Center for Intelligent Asset Management at Columbia University. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2421 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2421},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Beta and coskewness pricing: Perspective from probability weighting},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The value of coordination in multimarket bidding of grid
energy storage. <em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2021.2247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of a storage owner who trades in a multisettlement electricity market comprising an auction-based day-ahead market and a continuous intraday market. We show in a stylized model that a coordinated policy that reserves capacity for the intraday market is optimal and that the gap to a sequential policy increases with intraday price volatility and market liquidity. To assess the value of coordination in a realistic setting, we develop a multistage stochastic program for day-ahead bidding and hourly intraday trading along with a corresponding stochastic price model. We show how tight upper bounds can be obtained based on calculating optimal bilinear penalties for a novel information relaxation scheme. To calculate lower bounds, we propose a scenario tree generation method that lends itself to deriving an implementable policy based on reoptimization. We use these methods to quantify the value of coordination by comparing our policy with a sequential policy that does not coordinate day-ahead and intraday bids. In a case study, we find that coordinated bidding is most valuable for flexible storage assets with high price impact, like pumped-hydro storage. For small assets with low price impact, like battery storage, participation in the day-ahead auction is less important and intraday trading appears to be sufficient. For less flexible assets, like large hydro reservoirs without pumps, intraday trading is hardly profitable as most profit is made in the day-ahead market. A comparison of lower and upper bounds demonstrates that our policy is near-optimal for all considered assets.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2247},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {The value of coordination in multimarket bidding of grid energy storage},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The analytics of bed shortages: Coherent metric, prediction,
and optimization. <em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2021.2231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bed shortages in hospitals usually have a negative impact on patient satisfaction and medical outcomes. In practice, healthcare managers often use bed occupancy rates (BORs) as a metric to understand bed utilization, which is insufficient in capturing the risk of bed shortages. We propose the bed shortage index (BSI) to capture more facets of bed shortage risk than traditional metrics such as the occupancy rate, the probability of shortages, and expected shortages. The BSI is based on the riskiness index by Aumann and Serrano, and it is calibrated to coincide with BORs when the daily arrivals in the hospital unit are Poisson distributed. Our metric can be tractably computed and does not require additional assumptions or approximations. As such, it can be consistently used across the descriptive, predictive, and prescriptive analytical approaches. We also propose optimization models to plan for bed capacity via this metric. These models can be efficiently solved on a large scale via a sequence of linear optimization problems. The first maximizes total elective throughput while managing the metric under a specified threshold. The second determines the optimal scheduling policy by lexicographically minimizing the steady-state daily BSI for a given number of scheduled admissions. We validate these models using real data from a hospital and test them against data-driven simulations. We apply these models to study the real-world problem of long stayers to predict the impact of transferring them to community hospitals as a result of an aging population. Funding: J. Xie was partially supported by the National Natural Science Foundation of China [Grant 71921001]. This project was funded by Singapore Ministry of Education [Grant MOE2016-SSRTG-059], SPIRE.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2231},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {The analytics of bed shortages: Coherent metric, prediction, and optimization},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simulation-based prediction. <em>OR</em>, <em>71</em>(1),
iii–v. (<a href="https://doi.org/10.1287/opre.2021.2229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the use of simulation in computing predictors in settings in which real-world observations are collected. A major challenge is that the state description underlying the simulation will typically include information that is not observed in the real system. This makes it challenging to initialize simulations that are aligned with the most recent observation collected in the real-world system, especially when the simulation does not visit the most recently observed value frequently. Our estimation methodology involves the use of “splitting,” so that multiple simulations are launched from states that are closely aligned with the most recently collected real-world observation. We provide estimators both in the setting that the observed real-world values are discrete and are continuous, with kernel smoothing methods being systematically exploited in the continuous setting. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.2229 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2229},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Simulation-based prediction},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust satisficing. <em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2021.2238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general framework for robust satisficing that favors solutions for which a risk-aware objective function would best attain an acceptable target even when the actual probability distribution deviates from the empirical distribution. The satisficing decision maker specifies an acceptable target, or loss of optimality compared with the empirical optimization model, as a trade-off for the model’s ability to withstand greater uncertainty. We axiomatize the decision criterion associated with robust satisficing, termed as the fragility measure , and present its representation theorem. Focusing on Wasserstein distance measure, we present tractable robust satisficing models for risk-based linear optimization, combinatorial optimization, and linear optimization problems with recourse. Serendipitously, the insights to the approximation of the linear optimization problems with recourse also provide a recipe for approximating solutions for hard stochastic optimization problems without relatively complete recourse. We perform numerical studies on a portfolio optimization problem and a network lot-sizing problem. We show that the solutions to the robust satisficing models are more effective in improving the out-of-sample performance evaluated on a variety of metrics, hence alleviating the optimizer’s curse. Funding: D. Z. Long is supported by the Hong Kong Research Grants Council [Grant 14207819]. M. Sim and M. Zhou are supported by the Ministry of Education, Singapore, under its 2019 Academic Research Fund Tier 3 [Grant MOE-2019-T3-1-010]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2021.2238 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2238},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Robust satisficing},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic pricing and matching for two-sided queues.
<em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2021.2233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by applications from gig economy and online marketplaces, we study a two-sided queueing system under joint pricing and matching controls. The queueing system is modeled by a bipartite graph, where the vertices represent customer or server types and the edges represent compatible customer-server pairs. Both customers and servers sequentially arrive to the system and join separate queues according to their types. The arrival rates of different types depend on the prices set by the system operator and the expected waiting time. At any point in time, the system operator can choose certain customers to match with compatible servers. The objective is to maximize the long-run average profit for the system. We first propose a fluid approximation-based pricing and maximum-weight (max-weight) matching policy, which achieves an 𝑂 ( 𝜂 √ ) O ( η ) O(η) optimality rate when all the arrival rates are scaled by η . We further show that a two-price and max-weight matching policy achieves an improved 𝑂 ( 𝜂 1 / 3 ) O ( η 1 / 3 ) O(η1/3) optimality rate. Under a broad class of pricing policies, we prove that any matching policy has an optimality rate that is lower bounded by Ω ( 𝜂 1 / 3 ) Ω ( η 1 / 3 ) Ω(η1/3) . Thus, the latter policy achieves the optimal rate with respect to η . We also demonstrate the advantage of max-weight matching with respect to the number of server and customer types n . Under a complete resource pooling condition, we show that max-weight matching achieves 𝑂 ( 𝑛 √ ) O ( n ) O(n) and 𝑂 ( 𝑛 1 / 3 ) O ( n 1 / 3 ) O(n1/3) optimality rates for static and two-price policies, respectively, and the latter matches the lower bound Ω ( 𝑛 1 / 3 ) Ω ( n 1 / 3 ) Ω(n1/3) . In comparison, the randomized matching policy may have an Ω ( 𝑛 ) Ω ( n ) Ω(n) optimality rate. Funding: This work was supported in part by the National Science Foundation [Grant CMMI-2145661]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.2233 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2233},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Dynamic pricing and matching for two-sided queues},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The best of many worlds: Dual mirror descent for online
allocation problems. <em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2021.2242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online allocation problems with resource constraints are central problems in revenue management and online advertising. In these problems, requests arrive sequentially during a finite horizon and, for each request, a decision maker needs to choose an action that consumes a certain amount of resources and generates reward. The objective is to maximize cumulative rewards subject to a constraint on the total consumption of resources. In this paper, we consider a data-driven setting in which the reward and resource consumption of each request are generated using an input model that is unknown to the decision maker. We design a general class of algorithms that attain good performance in various input models without knowing which type of input they are facing. In particular, our algorithms are asymptotically optimal under independent and identically distributed inputs as well as various nonstationary stochastic input models, and they attain an asymptotically optimal fixed competitive ratio when the input is adversarial. Our algorithms operate in the Lagrangian dual space: they maintain a dual multiplier for each resource that is updated using online mirror descent. By choosing the reference function accordingly, we recover the dual subgradient descent and dual multiplicative weights update algorithm. The resulting algorithms are simple, fast, and do not require convexity in the revenue function, consumption function, and action space, in contrast to existing methods for online allocation problems. We discuss applications to network revenue management, online bidding in repeated auctions with budget constraints, online proportional matching with high entropy, and personalized assortment optimization with limited inventory. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2021.2242 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2242},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {The best of many worlds: Dual mirror descent for online allocation problems},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual bounds for periodical stochastic programs. <em>OR</em>,
<em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2021.2245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss construction of the dual of a periodical formulation of infinite-horizon linear stochastic programs with a discount factor. The dual problem is used for computing a deterministic upper bound for the optimal value of the considered multistage stochastic program. Numerical experiments demonstrate behavior of that upper bound, especially when the discount factor is close to one. Funding: Financial support from the National Science Foundation [Grant 1633196] is gratefully acknowledged.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2245},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Dual bounds for periodical stochastic programs},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subset selection with shrinkage: Sparse linear modeling when
the SNR is low. <em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a seemingly unexpected and relatively less understood overfitting aspect of a fundamental tool in sparse linear modeling—best subset selection—which minimizes the residual sum of squares subject to a constraint on the number of nonzero coefficients. Whereas the best subset selection procedure is often perceived as the “gold standard” in sparse learning when the signal-to-noise ratio (SNR) is high, its predictive performance deteriorates when the SNR is low. In particular, it is outperformed by continuous shrinkage methods, such as ridge regression and the Lasso. We investigate the behavior of best subset selection in the high-noise regimes and propose an alternative approach based on a regularized version of the least-squares criterion. Our proposed estimators (a) mitigate, to a large extent, the poor predictive performance of best subset selection in the high-noise regimes; and (b) perform favorably, while generally delivering substantially sparser models, relative to the best predictive models available via ridge regression and the Lasso. We conduct an extensive theoretical analysis of the predictive properties of the proposed approach and provide justification for its superior predictive performance relative to best subset selection when the noise level is high. Our estimators can be expressed as solutions to mixed-integer second-order conic optimization problems and, hence, are amenable to modern computational tools from mathematical optimization. Funding: R. Mazumder acknowledges research funding from the Office of Naval Research [Grants N000141512342 and N000141812298] and the National Science Foundation [Grant NSF-IIS-1718258]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2276 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2276},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Subset selection with shrinkage: Sparse linear modeling when the SNR is low},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Offline multi-action policy learning: Generalization and
optimization. <em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many settings, a decision maker wishes to learn a rule, or policy, that maps from observable characteristics of an individual to an action. Examples include selecting offers, prices, advertisements, or emails to send to consumers, choosing a bid to submit in a contextual first-price auctions, and determining which medication to prescribe to a patient. In this paper, we study the offline multi-action policy learning problem with observational data and where the policy may need to respect budget constraints or belong to a restricted policy class such as decision trees. By using the standard augmented inverse propensity weight estimator, we design and implement a policy learning algorithm that achieves asymptotically minimax-optimal regret. To the best of our knowledge, this is the first result of this type in the multi-action setup, and it provides a substantial performance improvement over the existing learning algorithms. We then consider additional computational challenges that arise in implementing our method for the case where the policy is restricted to take the form of a decision tree. We propose two different approaches: one using a mixed integer program formulation and the other using a tree-search based algorithm. Funding: This work was supported by the National Science Foundation [Grants CCF-2106508 and DMS-1916163] and the Office of Naval Research [Grant N00014-17-1-2131].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2271},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Offline multi-action policy learning: Generalization and optimization},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting COVID-19 and analyzing the effect of government
interventions. <em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We developed DELPHI, a novel epidemiological model for predicting detected cases and deaths in the prevaccination era of the COVID-19 pandemic. The model allows for underdetection of infections and effects of government interventions. We have applied DELPHI across more than 200 geographical areas since early April 2020 and recorded 6\% and 11\% two-week, out-of-sample median mean absolute percentage error on predicting cases and deaths, respectively. DELPHI compares favorably with other top COVID-19 epidemiological models and predicted in 2020 the large-scale epidemics in many areas, including the United States, United Kingdom, and Russia, months in advance. We further illustrate two downstream applications of DELPHI, enabled by the model’s flexible parametric formulation of the effect of government interventions. First, we quantify the impact of government interventions on the pandemic’s spread. We predict, that in the absence of any interventions, more than 14 million individuals would have perished by May 17, 2020, whereas 280,000 deaths could have been avoided if interventions around the world had started one week earlier. Furthermore, we find that mass gathering restrictions and school closings were associated with the largest average reductions in infection rates at 29.9 ± 6.9\% and 17.3 ± 6.7\% , respectively. The most stringent policy, stay at home, was associated with an average reduction in infection rate by 74.4 ± 3.7\% from baseline across countries that implemented it. In the second application, we demonstrate how DELPHI can predict future COVID-19 incidence under alternative governmental policies and discuss how Janssen Pharmaceuticals used such analyses to select the locations of its Phase III trial for its leading single-dose vaccine candidate Ad26.Cov2.S. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2306 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2306},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Forecasting COVID-19 and analyzing the effect of government interventions},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lagrangian inference for ranking problems. <em>OR</em>,
<em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel combinatorial inference framework to conduct general uncertainty quantification in ranking problems. We consider the widely adopted Bradley-Terry-Luce (BTL) model, where each item is assigned a positive preference score that determines the Bernoulli distributions of pairwise comparisons’ outcomes. Our proposed method aims to infer general ranking properties of the BTL model. The general ranking properties include the “local” properties such as if an item is preferred over another and the “global” properties such as if an item is among the top K -ranked items. We further generalize our inferential framework to multiple testing problems where we control the false discovery rate (FDR) and apply the method to infer the top- K ranked items. We also derive the information-theoretic lower bound to justify the minimax optimality of the proposed method. We conduct extensive numerical studies using both synthetic and real data sets to back up our theory. Funding: E. X. Fang was partially supported by the National Science Foundation [Grants DMS-1820702, DMS-1953196, and DMS-2015539] and a Whitehead Scholarship. J. Lu was partially supported by the National Science Foundation [Grant DMS-1916211], and the National Institutes of Health [Grants R35CA220523-04, R01ES32418-01, and U01CA209414-0]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2313 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2313},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Lagrangian inference for ranking problems},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust spectral clustering algorithm for sub-gaussian
mixture models with outliers. <em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of clustering data sets in the presence of arbitrary outliers. Traditional clustering algorithms such as k -means and spectral clustering are known to perform poorly for data sets contaminated with even a small number of outliers. In this paper, we develop a provably robust spectral clustering algorithm that applies a simple rounding scheme to denoise a Gaussian kernel matrix built from the data points and uses vanilla spectral clustering to recover the cluster labels of data points. We analyze the performance of our algorithm under the assumption that the “good” data points are generated from a mixture of sub-Gaussians (we term these “inliers”), whereas the outlier points can come from any arbitrary probability distribution. For this general class of models, we show that the misclassification error decays at an exponential rate in the signal-to-noise ratio, provided the number of outliers is a small fraction of the inlier points. Surprisingly, this derived error bound matches with the best-known bound for semidefinite programs (SDPs) under the same setting without outliers. We conduct extensive experiments on a variety of simulated and real-world data sets to demonstrate that our algorithm is less sensitive to outliers compared with other state-of-the-art algorithms proposed in the literature. Funding: G. A. Hanasusanto was supported by the National Science Foundation Grants NSF ECCS-1752125 and NSF CCF-2153606. P. Sarkar gratefully acknowledges support from the National Science Foundation Grants NSF DMS-1713082, NSF HDR-1934932 and NSF 2019844. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2317 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2317},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {A robust spectral clustering algorithm for sub-gaussian mixture models with outliers},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential mechanisms with ex post individual rationality.
<em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study optimal mechanisms for selling multiple products to a buyer who learns her values for those products sequentially. A mechanism may use static prices or adjust them over time, and it may sell the products separately or as bundles. We study mechanisms that provide the buyer a nonnegative ex post utility. We show that there exists an optimal mechanism that determines the allocation of each product as soon as the buyer learns her value for that product. This observation allows us to solve for optimal mechanisms recursively. We use this recursive characterization to show that static mechanisms are suboptimal if the buyer first learns her values for products that are ex ante less valuable. Under this condition, the ability to bundle products is less profitable than the ability to adjust prices dynamically. Funding: This work was supported by ONR [Grant N00014-12-1-0999] and NSF Awards CCF-0953960 (CAREER), CCF-1551875 and SES-1254768. Part of this work was done while the authors were visiting the Simons Institute for Theory of Computing. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2332 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2332},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Sequential mechanisms with ex post individual rationality},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partial recovery in the graph alignment problem.
<em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the graph alignment problem, which is the problem of recovering, given two graphs, a one-to-one mapping between nodes that maximizes edge overlap. This problem can be viewed as a noisy version of the well-known graph isomorphism problem and appears in many applications, including social network deanonymization and cellular biology. Our focus here is on partial recovery ; that is, we look for a one-to-one mapping that is correct on a fraction of the nodes of the graph rather than on all of them, and we assume that the two input graphs to the problem are correlated Erdős–Rényi graphs of parameters ( n , q , s ). Our main contribution is then to give necessary and sufficient conditions on ( n , q , s ) under which partial recovery is possible with high probability as the number of nodes n goes to infinity. In particular, we show that it is possible to achieve partial recovery in the 𝑛𝑞𝑠 = Θ ( 1 ) nqs = Θ ( 1 ) nqs=Θ(1) regime under certain additional assumptions. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2022.2355 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2355},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Partial recovery in the graph alignment problem},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A branch-and-repair method for three-dimensional bin
selection and packing in e-commerce. <em>OR</em>, <em>71</em>(1), iii–v.
(<a href="https://doi.org/10.1287/opre.2022.2369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of shipped parcels is continuously growing and e-commerce retailers and logistics service providers are seeking to improve logistics, particularly last-mile delivery. Since unused transportation space is a major problem in parcel distribution, one option is to improve the selection of the right parcel size for an order and the optimal packing pattern, which is known as the three-dimensional bin packing problem (3D-BPP). Further, the available portfolio of parcel types significantly influences the unused space. Therefore, we introduce the three-dimensional bin selection problem (3D-BSP) to find a portfolio of parcel types for a large set of orders. To solve the 3D-BPP with rotation of items, we propose an efficient mixed-integer linear programming formulation and symmetry-breaking constraints that are also used in the 3D-BSP for the subproblem. To solve large instances of the latter, we introduce a branch-and-repair method that improves branch-and-check. We show that our decomposition allows to relax the majority of binary decision variables in the master problem and avoids weak combinatorial cuts without further lifting. Further, we use problem-specific acceleration techniques. The numerical results based on a real-world online retailer data set show that our reformulation reduces the run time compared with existing mixed-integer linear programs for 3D-BPPs by 30\% on average. For the 3D-BSP, the branch-and-repair method reduces the run time by more than two orders of magnitude compared with the mixed-integer programming formulation and can even solve instances with millions of binary decision variables and constraints efficiently. We analyze the trade-off between the costs of variety (depending on the number of parcel types) and costs for unused space. Increasing the number of parcel types reduces the unused space significantly. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2369 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2369},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {A branch-and-repair method for three-dimensional bin selection and packing in E-commerce},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Policy analytics in public school operations. <em>OR</em>,
<em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Getting students to the right school at the right time can pose a challenge for school districts in the United States, which must balance educational objectives with operational ones, often on a shoestring budget. Examples of such operational challenges include deciding which students should attend, how they should travel to school, and what time classes should start. From an optimizer’s perspective, these decision problems are difficult to solve in isolation, and present a formidable challenge to solve together. In this paper, we develop an optimization-based approach to three key problems in school operations: school assignment, school bus routing, and school start time selection. Our methodology is comprehensive, flexible enough to accommodate a variety of problem specifics, and relies on a tractable decomposition approach. In particular, it comprises a new algorithm for jointly scheduling school buses and selecting school start times that leverages a simplifying assumption of fixed route arrival times, and a postimprovement heuristic to jointly optimize assignment, bus routing, and scheduling. We evaluate our methodology on simulated and real data from Boston Public Schools, with the case study of a summer program for special education students. Using summer 2019 data, we find that replacing the actual student-to-school assignment with our method could lead to total cost savings of up to 8\%. A simplified version of our assignment algorithm was used by the district in the summer of 2021 to analyze the cost tradeoffs between several scenarios and ultimately select and assign students to schools for the summer.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2373},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Policy analytics in public school operations},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Work more tomorrow: Resolving present bias in project
management. <em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Project management is responsible for almost 30\% of the world’s economic activity, with an annual value of $27 trillion. Traditionally, the frequent late delivery of projects is attributed to Parkinson’s Law, which incorporates laziness, procrastination, and self-protection against reduced deadlines in the future. Incentive schemes are widely designed and implemented to eliminate Parkinson’s Law. Yet many projects are nonetheless delivered late. To explain this, we show computationally that a significant part of project delay is not explained by Parkinson’s Law but can be explained by time-inconsistent behavior, that is, present bias. We therefore propose a new incentive scheme that directly addresses present bias in projects. Our work is among the first to develop a multiperiod model of time-inconsistent preferences for projects. Using concepts from savings theory, we design a Work More Tomorrow (WMT) incentive scheme to mitigate the effect of present bias in project execution. The WMT scheme extends the underlying economic theory of present bias to consider network effects that arise in projects. We show, both theoretically and computationally, that the WMT scheme outperforms the most widely used benchmark incentive scheme by delivering significant improvements in on-time frequency and expected project tardiness at lower cost. Moreover, these improvements increase with project size. An extensive sensitivity analysis reveals the characteristics of projects that respond best to the WMT scheme. We also identify several managerial insights that arise from viewing the frequent delays in project management practice through the lens of a multiperiod model of time-inconsistent behavior. Funding: This work was supported by the National Key R&amp;D Program of China [Grants 2021YFA1000100, 2021YFA1000104]; National Natural Science Foundation of China [Grants 71971083, 71931004, 71732003, 72171138, 71671106]; Program for Innovative Research Team of Shanghai University of Finance and Economics [Grant 2020110930]; Open Research Fund of Key Laboratory of Advanced Theory and Application in Statistics and Data Science-Ministry of Education; and the Berry Professorship at the Fisher College of Business, The Ohio State University. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2379 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2379},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Work more tomorrow: Resolving present bias in project management},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An approximate analysis of dynamic pricing, outsourcing, and
scheduling policies for a multiclass make-to-stock queue in the heavy
traffic regime. <em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a make-to-stock manufacturing system selling multiple products to price-sensitive customers. The system manager seeks to maximize the long-run average profit by making dynamic pricing, outsourcing, and scheduling decisions. First, she adjusts prices dynamically depending on the system state. Second, when the backlog of work is judged to be excessive, she may outsource new orders, thereby incurring outsourcing costs. Third, she decides dynamically which product to prioritize in the manufacturing process (i.e., she makes dynamic scheduling decisions). This problem appears analytically intractable. Thus, we resort to an approximate analysis in the heavy traffic regime and consider the resulting Brownian control problem. We solve this problem explicitly by exploiting the solution to a particular Riccati equation. The optimal solution to the Brownian control problem is a two-sided barrier policy with drift rate control. Outsourcing and idling processes are used to keep the workload process above the lower reflecting barrier and below the upper reflecting barrier, respectively. Between the two barriers, a state-dependent drift rate is used to control the workload. By interpreting this solution in the context of the original model, we propose a joint dynamic pricing, outsourcing, and scheduling policy, and we demonstrate its effectiveness through a simulation study. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2361 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2361},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {An approximate analysis of dynamic pricing, outsourcing, and scheduling policies for a multiclass make-to-stock queue in the heavy traffic regime},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Achieving high individual service levels without safety
stock? Optimal rationing policy of pooled resources. <em>OR</em>,
<em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource pooling is a fundamental concept that has many applications in operations management for reducing and hedging uncertainty. An important problem in resource pooling is to decide (1) the capacity level of pooled resources in anticipation of random demand of multiple customers and (2) how the capacity should be allocated to fulfill customer demands after demand realization. In this paper, we present a general framework to study this two-stage problem when customers require individual and possibly different service levels. Our modeling framework generalizes and unifies many existing models in the literature and includes second-stage allocation costs. We propose a simple randomized rationing policy for any fixed feasible capacity level. Our main result is the optimality of this policy for very general service level constraints, including type I and type II constraints and beyond. The result follows from a semi-infinite linear programming formulation of the problem and its dual. As a corollary, we also prove the optimality of index policies for a large class of problems when the set of feasible fulfilled demands is a polymatroid. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2386 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2386},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {Achieving high individual service levels without safety stock? optimal rationing policy of pooled resources},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The inventory routing problem under uncertainty.
<em>OR</em>, <em>71</em>(1), iii–v. (<a
href="https://doi.org/10.1287/opre.2022.2407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an uncertain inventory routing problem with a finite horizon. The supplier acts as a central planner who determines the replenishment quantities and also, the delivery times and routes to all retailers. We allow ambiguity in the probability distribution of each retailer’s uncertain demand. Adopting a service-level viewpoint, we minimize the risk of uncertain inventory levels violating a prespecified acceptable range. We quantify that risk using a novel decision criterion, the service violation index , that accounts for how often and how severely the inventory requirement is violated. The solutions proposed here are adaptive in that they vary with the realization of uncertain demand. We provide algorithms to solve the problem exactly and then, demonstrate the superiority of our solutions by comparing them with several benchmarks. Funding: D.Z. Long is supported by the National Natural Science Foundation of China [Grants 71971187, 72231002]. J. Qi is supported by the Hong Kong Research Grants Council [Grants 16213417, 16209918]. L. Zhang is supported by the National Natural Science Foundation of China [Grants 72171156, 72231002]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2407 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2407},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-v},
  shortjournal = {Oper. Res.},
  title        = {The inventory routing problem under uncertainty},
  volume       = {71},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
