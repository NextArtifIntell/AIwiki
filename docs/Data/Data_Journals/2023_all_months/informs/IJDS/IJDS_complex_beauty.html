<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijds---11">IJDS - 11</h2>
<ul>
<li><details>
<summary>
(2023). Credit risk modeling with graph machine learning.
<em>IJDS</em>, <em>2</em>(2), 197–217. (<a
href="https://doi.org/10.1287/ijds.2022.00018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate credit ratings are an essential ingredient in the decision-making process for investors, rating agencies, bond portfolio managers, bankers, and policy makers, as well as an important input for risk management and regulation. Credit ratings are traditionally generated from models that use financial statement data and market data, which are tabular (numeric and categorical). Using machine learning methods, we construct a network of firms using U.S. Securities and Exchange Commission (SEC) filings (denoted CorpNet) to enhance the traditional tabular data set with a corporate graph. We show that this generates accurate rating predictions with comparable and better performance to tabular models. We ensemble graph convolutional networks with highly-performant ensembled machine learning models using AutoGluon. This paper demonstrates both transductive and inductive methodologies to extend credit scoring models based on tabular data, which have been used by the ratings industry for decades, to the class of machine learning models on networks. The methodology is extensible to other financial machine learning models that may be enhanced using a corporate graph. History: David Martens served as the senior editor for this article. Data Ethics &amp; Reproducibility Note: No data ethics considerations are foreseen related to this article. The paper deals with corporate credit risk and not consumer credit, which usually entails issues around privacy and bias. The code capsule is available on Code Ocean at https://codeocean.com/capsule/5230264/tree/v2 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.00018 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.00018},
  journal      = {INFORMS Journal on Data Science},
  number       = {2},
  pages        = {197-217},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Credit risk modeling with graph machine learning},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable hierarchical deep learning model for
noninvasive alzheimer’s disease diagnosis. <em>IJDS</em>, <em>2</em>(2),
183–196. (<a href="https://doi.org/10.1287/ijds.2020.0005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease is one of the leading causes of death in the world. Alzheimer’s is typically diagnosed through expensive imaging methods, such as positron emission tomography (PET) scan and magnetic resonance imaging (MRI), as well as invasive methods, such as cerebrospinal fluid analysis. In this study, we develop an interpretable hierarchical deep learning model to detect the presence of Alzheimer’s disease from transcripts of interviews of individuals who were asked to describe a picture. Our deep recurrent neural network employs a novel three-level hierarchical attention over self-attention (AoS3) mechanism to model the temporal dependencies of longitudinal data. We demonstrate the interpretability of the model with the importance score of words, sentences, and transcripts extracted from our AoS3 model. Numerical results demonstrate that our deep learning model can detect Alzheimer’s disease from the transcripts of patient interviews with 96\% accuracy when tested on the DementiaBank data set. Our interpretable neural network model can help diagnose Alzheimer’s disease in a noninvasive and affordable manner, improve patient outcomes, and result in cost containment. History: Rema Padman served as the senior editor for this article. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/2881658/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2020.0005 ). The study involves secondary use of already-collected data. None of the authors were part of the original study team. The authors had no interaction with living individuals and had no access to protected health information (PHI) or private identifiable information about living individuals.},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2020.0005},
  journal      = {INFORMS Journal on Data Science},
  number       = {2},
  pages        = {183-196},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Interpretable hierarchical deep learning model for noninvasive alzheimer’s disease diagnosis},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diversity subsampling: Custom subsamples from large data
sets. <em>IJDS</em>, <em>2</em>(2), 161–182. (<a
href="https://doi.org/10.1287/ijds.2022.00017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling from a large unlabeled (i.e., no response values are available yet) data set is useful in many supervised learning contexts to provide a global view of the data based on only a fraction of the observations. In this paper, we borrow concepts from the well-known sampling/importance resampling technique, which samples from a specified probability distribution, to develop a diversity subsampling approach that selects a subsample from the original data with no prior knowledge of its underlying probability distribution. The goal is to produce a subsample that is independently and uniformly distributed over the support of distribution from which the data are drawn, to the maximum extent possible. We give an asymptotic performance guarantee of the proposed method and provide experimental results to show that the proposed method performs well for typical finite-size data. We also compare the proposed method with competing diversity subsampling algorithms and demonstrate numerically that subsamples selected by the proposed method are closer to a uniform sample than subsamples selected by other methods. The proposed diversity subsampling (DS) algorithm is more efficient than known methods. It takes only a few minutes to select tens of thousands of subsample points from a data set of size one million. Our DS algorithm easily generalizes to select subsamples following distributions other than uniform. We provide a Python package (FADS) that implements the proposed method. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: This work was supported by the National Science Foundation [Grant CMMI-1436574], Northwestern University, the Advanced Research Projects Agency-Energy, and the U.S. Department of Energy [Award DE-AR0001209]. Data Ethics &amp; Reproducibility Note: No data ethics considerations are foreseen related to this article. The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.8309237.v3 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.00017 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.00017},
  journal      = {INFORMS Journal on Data Science},
  number       = {2},
  pages        = {161-182},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Diversity subsampling: Custom subsamples from large data sets},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling financial products and their supply chains.
<em>IJDS</em>, <em>2</em>(2), 138–160. (<a
href="https://doi.org/10.1287/ijds.2020.0006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this paper is to explore how novel financial datasets and machine learning methods can be applied to model and understand financial products. We focus on residential mortgage backed securities, resMBS, which were at the heart of the 2008 US financial crisis. These securities are contained within a prospectus and have a complex waterfall payoff structure. Multiple financial institutions form a supply chain to create the prospectuses. To model this supply chain, we use unsupervised probabilistic methods, particularly dynamic topics models (DTM), to extract a set of features reflecting community (topic) formation and temporal evolution along the chain. We then provide insight into the performance of the resMBS securities and the impact of the supply chain communities through a series of increasingly comprehensive models. First, models at the security level directly identify salient features of resMBS securities that impact their performance. We then extend the model to include prospectus level features and demonstrate that the composition of the prospectus is significant. Our model also shows that communities along the supply chain that are associated with the generation of the prospectuses and securities have an impact on performance. We are the first to show that toxic communities that are closely linked to financial institutions that played a key role in the subprime crisis can increase the risk of failure of resMBS securities. History: Olivia Sheng served as the senior editor for this article. Funding: This research was partially supported by National Science Foundation [Grant CNS1305368] and National Institute of Standards and Technology [Grant 70NANB15H194]. Data Ethics &amp; Reproducibility Note: No data ethics considerations are foreseen related to this article. The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.8845455.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2020.0006 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2020.0006},
  journal      = {INFORMS Journal on Data Science},
  number       = {2},
  pages        = {138-160},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Modeling financial products and their supply chains},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiblock parameter calibration in computer models.
<em>IJDS</em>, <em>2</em>(2), 116–137. (<a
href="https://doi.org/10.1287/ijds.2023.0029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter calibration aims to estimate unobservable parameters used in a computer model by using physical process responses and computer model outputs. In the literature, existing studies calibrate all parameters simultaneously using an entire data set. However, in certain applications, some parameters are associated with only a subset of data. For example, in the building energy simulation, cooling (heating) season parameters should be calibrated using data collected during the cooling (heating) season only. This study provides a new multiblock calibration approach that considers such heterogeneity. Unlike existing studies that build emulators for the computer model response, such as the widely used Bayesian calibration approach, we consider multiple loss functions to be minimized, each for a block of parameters that use the corresponding data set, and estimate the parameters using a nonlinear optimization technique. We present the convergence properties under certain conditions and quantify the parameter estimation uncertainties. The superiority of our approach is demonstrated through numerical studies and a real-world building energy simulation case study. History: Bianca Maria Colosimo served as the senior editor for this article. Funding: This work was partially supported by the National Science Foundation [Grants CMMI-1662553, CMMI-2226348, and CBET-1804321]. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/8623151/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0029 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0029},
  journal      = {INFORMS Journal on Data Science},
  number       = {2},
  pages        = {116-137},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Multiblock parameter calibration in computer models},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A nonparametric subspace analysis approach with application
to anomaly detection ensembles. <em>IJDS</em>, <em>2</em>(2), 99–115.
(<a href="https://doi.org/10.1287/ijds.2023.0027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying anomalies in multidimensional data sets is an important yet challenging task in many real-world applications. A special case arises when anomalies are occluded in a small subset of attributes. We propose a new subspace analysis approach, called agglomerative attribute grouping (AAG), that searches for subspaces composed of highly correlative (in the general sense) attributes. Such correlations among attributes can better reflect the behavior of normal observations and hence, can be used to improve the identification of abnormal data samples. The proposed AAG algorithm relies on a generalized multiattribute measure (derived from information theory measures over attributes’ partitions) for evaluating the “information distance” among various subsets of attributes. To determine the set of subspaces, AAG applies a variation of the well-known agglomerative clustering algorithm with the proposed measure as the underlying distance function, whereas in contrast to existing methods, AAG does not require any tuning of parameters. Finally, the set of informative subspaces can be used to improve subspace-based analytical tasks, such as anomaly detection, novelty detection, forecasting, and clustering. Extensive evaluation over real-world data sets demonstrates that (i) in the vast majority of cases, AAG outperforms both classical and state-of-the-art subspace analysis methods when used in anomaly and novelty detection ensembles; (ii) it often generates fewer subspaces with fewer attributes each, thus resulting in faster training times for the anomaly and novelty detection ensemble; and (iii) the generated subspaces can also be useful in other analytical tasks, such as clustering and forecasting. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: This research was partially supported by the Israeli Ministry of Economy (METRO 450 Consortium within the frame of MAGNET program) as well as by the Koret foundation grant for Smart Cities and Digital Living 2030. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/2526218/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0027 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0027},
  journal      = {INFORMS Journal on Data Science},
  number       = {2},
  pages        = {99-115},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {A nonparametric subspace analysis approach with application to anomaly detection ensembles},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial board. <em>IJDS</em>, <em>2</em>(1), C2. (<a
href="https://doi.org/10.1287/ijds.2023.eb.v2n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.eb.v2n1},
  journal      = {INFORMS Journal on Data Science},
  number       = {1},
  pages        = {C2},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Editorial board},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Covariate dependent sparse functional data analysis.
<em>IJDS</em>, <em>2</em>(1), 81–98. (<a
href="https://doi.org/10.1287/ijds.2023.0025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a method to incorporate covariate information into sparse functional data analysis. The method aims at cases where each subject has a limited number of longitudinal measurements and is associated with static covariates. This research is motivated by several use cases in practice. One representative example is void swelling, a nuclear-specific material degradation mechanism. Void swelling is affected by many covariates, including alloy composition and irradiation type. How to accurately model the complicated joint effects of such covariates on the swelling process is the key to mitigating the effect of swelling and ensuring safe operation. Unlike most of the existing methods, the proposed method can handle high-dimensional covariates with the informative covariate identification procedure and sparse and irregularly spaced measurements, that is, does not require complete or dense observations. The main innovation of the proposed method is that we model the variation coming from covariates and the variation left conditioned on covariates, such that the functional principal component analysis and Gaussian process can be conducted in a unified manner. We also propose a systematic approach to identify important covariates in the hypothesis testing context. The methodology is demonstrated on applications in nuclear engineering and healthcare and simulation studies.},
  archive      = {J_IJDS},
  author       = {Minhee Kim and Todd Allen and Kaibo Liu},
  doi          = {10.1287/ijds.2023.0025},
  journal      = {INFORMS Journal on Data Science},
  number       = {1},
  pages        = {81-98},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Covariate dependent sparse functional data analysis},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential adversarial anomaly detection for one-class event
data. <em>IJDS</em>, <em>2</em>(1), 45–59. (<a
href="https://doi.org/10.1287/ijds.2023.0026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the sequential anomaly detection problem in the one-class setting when only the anomalous sequences are available and propose an adversarial sequential detector by solving a minimax problem to find an optimal detector against the worst-case sequences from a generator. The generator captures the dependence in sequential events using the marked point process model. The detector sequentially evaluates the likelihood of a test sequence and compares it with a time-varying threshold, also learned from data through the minimax problem. We demonstrate our proposed method’s good performance using numerical experiments on simulations and proprietary large-scale credit card fraud data sets. The proposed method can generally apply to detecting anomalous sequences.},
  archive      = {J_IJDS},
  author       = {Shixiang Zhu and Henry Shaowu Yuchi and Minghe Zhang and Yao Xie},
  doi          = {10.1287/ijds.2023.0026},
  journal      = {INFORMS Journal on Data Science},
  number       = {1},
  pages        = {45-59},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Sequential adversarial anomaly detection for one-class event data},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rhetoric mining: A new text-analytics approach for
quantifying persuasion. <em>IJDS</em>, <em>2</em>(1), 24–44. (<a
href="https://doi.org/10.1287/ijds.2022.0024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rhetoric mining is a novel text-analytics method for quantifying persuasion based on rhetorical analysis theory. Our mixed-methodology approach combines qualitative context analysis with automated tagging and quantification of rhetorical moves. Rhetorical moves are complex discursive patterns and, thus, require a sequence-based text-mining approach, rather than the simpler word-based frequency analyses. We apply a sequence-alignment method to detect semantically equivalent sequences with high precision and efficiency. We illustrate our method by analyzing arguments used to justify stock picks in an online investment community. For these data, we detect and quantify the rhetorical moves of ethos (personal versus cited expertise), hedging (confidence versus uncertainty), and evidence type (product-, company-, or stock-based evidence). We use rhetoric mining to identify argument styles of persuasiveness (pitches that receive community recommendations) and trustworthiness (pitches that are written by successful investors). Rhetoric mining provides a new analytic lens in Information Systems research to analyze the influence of persuasion in consumer decision making.},
  archive      = {J_IJDS},
  author       = {Michelle M. H. Şeref and Onur Şeref and Alan S. Abrahams and Shawndra B. Hill and Quinn Warnick},
  doi          = {10.1287/ijds.2022.0024},
  journal      = {INFORMS Journal on Data Science},
  number       = {1},
  pages        = {24-44},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Rhetoric mining: A new text-analytics approach for quantifying persuasion},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How can IJDS authors, reviewers, and editors use (and
misuse) generative AI? <em>IJDS</em>, <em>2</em>(1), 1–9. (<a
href="https://doi.org/10.1287/ijds.2023.0007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDS},
  author       = {Galit Shmueli and Bianca Maria Colosimo and David Martens and Rema Padman and Maytal Saar-Tsechansky and Olivia R. Liu Sheng and W. Nick Street and Kwok-Leung Tsui},
  doi          = {10.1287/ijds.2023.0007},
  journal      = {INFORMS Journal on Data Science},
  number       = {1},
  pages        = {1-9},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {How can IJDS authors, reviewers, and editors use (and misuse) generative AI?},
  volume       = {2},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
