<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDSMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijdsms---8">IJDSMS - 8</h2>
<ul>
<li><details>
<summary>
(2023). Polytopes and machine learning. <em>IJDSMS</em>,
<em>1</em>(2), 181–211. (<a
href="https://doi.org/10.1142/S281093922350003X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce machine learning methodology to the study of lattice polytopes. With supervised learning techniques, we predict standard properties such as volume, dual volume, and reflexivity with accuracies up to 100%. We focus on 2d polygons and 3d polytopes with Plücker coordinates as input, which outperform the usual vertex representation.},
  archive      = {J_IJDSMS},
  author       = {Jiakang Bao and Yang-Hui He and Edward Hirst and Johannes Hofscheier and Alexander Kasprzyk and Suvajit Majumder},
  doi          = {10.1142/S281093922350003X},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {2},
  pages        = {181-211},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Polytopes and machine learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simplifying polylogarithms with machine learning.
<em>IJDSMS</em>, <em>1</em>(2), 135–179. (<a
href="https://doi.org/10.1142/S2810939223500028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polylogarithmic functions, such as the logarithm or dilogarithm, satisfy a number of algebraic identities. For the logarithm, all the identities follow from the product rule. For the dilogarithm and higher-weight classical polylogarithms, the identities can involve five functions or more. In many calculations relevant to particle physics, complicated combinations of polylogarithms often arise from Feynman integrals. Although the initial expressions resulting from the integration usually simplify, it is often difficult to know which identities to apply and in what order. To address this bottleneck, we explore to what extent machine learning methods can help. We consider both a reinforcement learning approach, where the identities are analogous to moves in a game, and a transformer network approach, where the problem is viewed analogously to a language-translation task. While both methods are effective, the transformer network appears more powerful and holds promise for practical use in symbolic manipulation tasks in mathematical physics.},
  archive      = {J_IJDSMS},
  author       = {Aurélien Dersy and Matthew D. Schwartz and Xiaoyuan Zhang},
  doi          = {10.1142/S2810939223500028},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {2},
  pages        = {135-179},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Simplifying polylogarithms with machine learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning class numbers of real quadratic fields.
<em>IJDSMS</em>, <em>1</em>(2), 107–134. (<a
href="https://doi.org/10.1142/S2810939223500016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We implement and interpret various supervised learning experiments involving real quadratic fields with class numbers 1, 2 and 3. We quantify the relative difficulties in separating class numbers of matching/different parity from a data-scientific perspective, apply the methodology of feature analysis and principal component analysis, and use symbolic classification to develop machine-learned formulas for class numbers 1, 2 and 3 that apply to our dataset.},
  archive      = {J_IJDSMS},
  author       = {Malik Amir and Yang-Hui He and Kyu-Hwan Lee and Thomas Oliver and Eldar Sultanow},
  doi          = {10.1142/S2810939223500016},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {2},
  pages        = {107-134},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Machine learning class numbers of real quadratic fields},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning etudes in conformal field theories.
<em>IJDSMS</em>, <em>1</em>(1), 71–104. (<a
href="https://doi.org/10.1142/S2810939222500058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate that various aspects of Conformal Field Theory are amenable to machine learning. Relatively modest feed-forward neural networks are able to distinguish between scale and conformal invariance of a three-point function and identify a crossing-symmetric four-point function to nearly 100% accuracy. Furthermore, neural networks are also able to identify conformal blocks appearing in a putative CFT four-point function and predict the values of the corresponding operator product expansions (OPE) coefficients. Neural networks also successfully classify primary operators by their quantum numbers under discrete symmetries in the CFT from examining OPE data. We also demonstrate that neural networks are able to learn the available OPE data for scalar correlation function in the 3D Ising model and predict the twists of higher-spin operators that appear in scalar OPE channels by regression.},
  archive      = {J_IJDSMS},
  author       = {Heng-Yu Chen and Yang-Hui He and Shailesh Lal and M. Zaid Zaz},
  doi          = {10.1142/S2810939222500058},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {71-104},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Machine learning etudes in conformal field theories},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A visualization of the classical musical tradition.
<em>IJDSMS</em>, <em>1</em>(1), 63–69. (<a
href="https://doi.org/10.1142/S2810939222500022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the spirit of cross-disciplinarity and data mining, we take a mathematician’s perspective on some musicology. A study of around 13,000 musical compositions from the Western classical tradition is carried out, spanning 33 major composers from the Baroque to the Romantic, with a focus on the usage of major/ minor key signatures. A two-dimensional (2D) chromatic diagram is proposed to succinctly visualize the data. The diagram is found to be useful not only in distinguishing style and period, but also in tracking the career development of a particular composer.},
  archive      = {J_IJDSMS},
  author       = {Yang-Hui He},
  doi          = {10.1142/S2810939222500022},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {63-69},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {A visualization of the classical musical tradition},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Calabi–yau metrics, energy functionals and machine learning.
<em>IJDSMS</em>, <em>1</em>(1), 49–61. (<a
href="https://doi.org/10.1142/S2810939222500034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we apply machine learning to the problem of finding numerical Calabi–Yau metrics. We extend previous work on learning approximate Ricci-flat metrics calculated using Donaldson’s algorithm to the much more accurate “optimal” metrics of Headrick and Nassar. We show that machine learning is able to predict the Kähler potential of a Calabi–Yau metric having seen only a small sample of training data.},
  archive      = {J_IJDSMS},
  author       = {Anthony Ashmore and Lucille Calmon and Yang-Hui He and Burt A. Ovrut},
  doi          = {10.1142/S2810939222500034},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {49-61},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Calabi–Yau metrics, energy functionals and machine learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Machine-learning mathematical structures. <em>IJDSMS</em>,
<em>1</em>(1), 23–47. (<a
href="https://doi.org/10.1142/S2810939222500010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We review, for a general audience, a variety of recent experiments on extracting structure from machine-learning mathematical data that have been compiled over the years. Focusing on supervised machine-learning on labeled data from different fields ranging from geometry to representation theory, from combinatorics to number theory, we present a comparative study of the accuracies on different problems. The paradigm should be useful for conjecture formulation, finding more efficient methods of computation, as well as probing into certain hierarchy of structures in mathematics. Based on various colloquia, seminars and conference talks in 2020, this is a contribution to the launch of the journal “Data Science in the Mathematical Sciences.”},
  archive      = {J_IJDSMS},
  author       = {Yang-Hui He},
  doi          = {10.1142/S2810939222500010},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {23-47},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Machine-learning mathematical structures},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning algebraic structures: Preliminary investigations.
<em>IJDSMS</em>, <em>1</em>(1), 3–22. (<a
href="https://doi.org/10.1142/S2810939222500046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we employ techniques of machine learning, exemplified by support vector machines and neural classifiers, to initiate the study of whether artificial intelligence (AI) can “learn” algebraic structures. Using finite groups and finite rings as a concrete playground, we find that questions such as identification of simple groups by “looking” at the Cayley table or correctly matching addition and multiplication tables for finite rings can, at least for structures of small size, be performed by the AI, even after having been trained only on small number of cases. These results are in tandem with recent investigations on whether AI can solve certain classes of problems in algebraic geometry.},
  archive      = {J_IJDSMS},
  author       = {Yang-Hui He and Minhyong Kim},
  doi          = {10.1142/S2810939222500046},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {3-22},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Learning algebraic structures: Preliminary investigations},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
