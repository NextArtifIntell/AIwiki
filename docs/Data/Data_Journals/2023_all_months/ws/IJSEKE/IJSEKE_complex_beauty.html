<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJSEKE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijseke---73">IJSEKE - 73</h2>
<ul>
<li><details>
<summary>
(2023). Agile effort estimation: Comparing the accuracy and
efficiency of planning poker, bucket system, and affinity estimation
methods. <em>IJSEKE</em>, <em>33</em>(11n12), 1923–1950. (<a
href="https://doi.org/10.1142/S021819402350064X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Published studies on agile effort estimation predominantly focus on comparisons of the accuracy of different estimation methods, while efficiency comparisons, i.e. how much time the estimation methods consume was not in the forefront. However, for practical use in software development, the time required can be a very important cost factor for enterprises, especially when the accuracy of different agile effort estimations is similar. In this study, we thus try to advance the current standard accuracy comparison between methods by introducing efficiency, i.e. time it takes to use a method as an additional dimension of comparison. We conduct this comparison between three agile effort estimation methods that were not yet compared in the literature, namely, Planning Poker, Bucket System and Affinity Estimation. For the comparison, we used eight student teams with 29 students who had to use all the effort estimation methods during the course where they had to finish a programming project in 3 weeks. The results indicate that after the students get used to using the different methods the accuracy between them is not statistically significantly different, however, the efficiency is. On average, Bucket System and Affinity Estimation methods take half as much time as Planning Poker.},
  archive      = {J_IJSEKE},
  author       = {Marko Poženel and Luka Fürst and Damjan Vavpotič and Tomaž Hovelja},
  doi          = {10.1142/S021819402350064X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1923-1950},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Agile effort estimation: Comparing the accuracy and efficiency of planning poker, bucket system, and affinity estimation methods},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CodeGen-search: A code generation model incorporating
similar sample information. <em>IJSEKE</em>, <em>33</em>(11n12),
1899–1921. (<a href="https://doi.org/10.1142/S0218194023500584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code generation has a positive significance in supporting software development, reducing labor intensity, and improving development efficiency. Some scholars use similar code information to enhance the quality of code generation. However, to improve the efficiency and accuracy of programming in daily development tasks, developers often search for similar samples as references. They get the code’s syntactic structure and semantic information from similar samples to assist in programming development. Inspired by this, we argue that similar samples are helpful for code generation. This paper proposes a CodeGen-Search model to improve code generation quality by incorporating similar samples. To fully utilize the information of similar samples, the model adopts the “pre-training + fine-tuning” pattern. The model uses a minimum edit distance algorithm to find some similar samples with natural language (NL), and uses different encoders to extract the features of the NL and the code in similar samples. Experimental results show that our model efficiently improves the quality of the generated code. Compared to the state-of-the-art model, the CodeGen-Search model improves the BLEU by 1.5%, the Rough by 0.8% on the HS dataset, and the StrAcc by 0.5% on the ATIS dataset.},
  archive      = {J_IJSEKE},
  author       = {HongWei Li and JiangLing Kuang and MaoSheng Zhong and ZhiXiang Wang and Gen Liu and GanLin Liu and YingJian Xiao},
  doi          = {10.1142/S0218194023500584},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1899-1921},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {CodeGen-search: A code generation model incorporating similar sample information},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Software industry perception of technical debt and its
management. <em>IJSEKE</em>, <em>33</em>(11n12), 1865–1898. (<a
href="https://doi.org/10.1142/S0218194023500602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technical debt (TD) expresses the lack of internal quality directly affecting software evolution. Therefore, it has gained the attention of software researchers and practitioners recently. Software researchers have performed empirical studies to observe the perspective of TD in different software cultures and organizations. However, it is important to replicate such studies in more places and with more practitioners to strengthen the perception of TD. In this paper, we present the results of a set of new research questions from an evolved survey design of a survey replication in the Uruguayan software industry to characterize how the software industry professionals understand, perceive, and adopt TD management (TDM) activities. The results allow us to observe that different participant contexts (startups, government, job roles) show different levels of awareness and perception of TD. Details in the form of the adoption of each TDM activity were presented. We could observe some difficulties in conducting some TDM activities that the practitioners consider very important, especially in TDM and monitoring. Differences in specific organizational contexts like startups and government could indicate the need for research efforts in other software engineering communities that meet their specific TD challenges and needs.},
  archive      = {J_IJSEKE},
  author       = {Cecilia Apa and Martín Solari and Diego Vallespir and Guilherme Horta Travassos},
  doi          = {10.1142/S0218194023500602},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1865-1898},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Software industry perception of technical debt and its management},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative inference acceleration integrating DNN
partitioning and task offloading in mobile edge computing.
<em>IJSEKE</em>, <em>33</em>(11n12), 1835–1863. (<a
href="https://doi.org/10.1142/S0218194023410085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing environment, intelligent inference services driven by DNN are highly sensitive to latency. Recently, collaborative inference between User Devices and Edge Servers (ESs) based on Deep Neural Networks (DNN) partition has achieved success in service acceleration. However, most of the existing collaborative acceleration schemes are partitioned for a single DNN inference task, which cannot quickly make partition decisions for a set of concurrent inference tasks, and often sacrifice inference accuracy. In addition, due to the limited resources of ESs, there is resource competition among concurrent requests, which makes the partitioned tasks cannot be offloaded to ESs in time for processing. Therefore, designing an efficient offloading scheme becomes essential. The task offloading schemes based on deep reinforcement learning can solve complex decision-making problems in high-dimensional state space, but they have problems such as insufficient sample diversity and easily falling into local optimum. In this paper, a Collaborative Inference Acceleration Scheme integrating DNN Partitioning and Task Offloading (CIAS-PnO) is proposed. First, while ensuring inference accuracy, the Collaborative DNN Layer Partitioning (CDLP) algorithm is designed with the goal of optimal latency. CDLP can reduce the problem scale of concurrent inference tasks partition by pruning operation and determine the partition decisions in time. Then, the Distributed Soft Actor-Critic (SAC)-based Partition Task Offloading algorithm (DSACO) is designed. DSACO supports SAC Agents to explore samples in parallel and share learning experiences, and uses the automatic entropy adjustment mechanism to improve the exploration efficiency of Agents, so as to avoid falling into local optimum and achieve efficient offloading of partition tasks. Experimental results on DNN benchmarks show that compared with the baseline acceleration schemes, CIAS-PnO achieves more than 19.8% acceleration performance improvement, and has higher convergence performance and task success rate.},
  archive      = {J_IJSEKE},
  author       = {Wenxiu Xu and Yin Yin and Ningjiang Chen and Huan Tu},
  doi          = {10.1142/S0218194023410085},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1835-1863},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Collaborative inference acceleration integrating DNN partitioning and task offloading in mobile edge computing},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STMultiple: Sparse transformer based on RFID for
multi-object activity recognition. <em>IJSEKE</em>, <em>33</em>(11n12),
1813–1833. (<a href="https://doi.org/10.1142/S0218194023410073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensing techniques for Human Activity Recognition (HAR) have been widely studied in recent years. At present, research on HAR based on Radio Frequency Identification (RFID) is changing from the tag attachment method to the tag non-attachment method. Affected by multipath, the current solutions in tag non-attachment scenarios mainly focus on single-object activity recognition, which is not suitable for multi-object scenarios. To address these issues, we propose STMultiple, a novel tag non-attachment activity recognition model for multi-object. The model first preprocesses the raw signal with filter and phase calibration, then it applies dilated convolution in the frequency domain to extract multi-object activity features, finally the feature pyramid structure and ProbSparse are used to optimize the vanilla Transformer-Encoder to enhance the activity recognition ability. Extensive experiments show that STMultiple can achieve recognition accuracy of up to 97.93% and down to about 90% in challenging environments ranging from two to five users, which has excellent performance compared to several state-of-the-art methods.},
  archive      = {J_IJSEKE},
  author       = {Shunwen Shen and Mulan Yang and Xuehan Hou and Lvqing Yang and Sien Chen and Wensheng Dong and Bo Yu and Qingkai Wang},
  doi          = {10.1142/S0218194023410073},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1813-1833},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {STMultiple: Sparse transformer based on RFID for multi-object activity recognition},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SHAMROQ: A software engineering methodology to extract
deontic expressions from the code of federal regulations — a
single-case, embedded case study. <em>IJSEKE</em>, <em>33</em>(11n12),
1787–1812. (<a href="https://doi.org/10.1142/S021819402341005X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research provides a comprehensive analysis of deontic expressions within the Code of Federal Regulations (CFR) Title 48, Federal Acquisition Regulations System, specifically focusing on obligations, permissions, prohibitions, and dispensations. Utilizing SHAMROQ, a systematic and rigorous methodology, the authors extract, classify, and analyze these expressions, quantify their prevalence, and identify common linguistic patterns within the legal text. The results show that obligations (71.3%) form most deontic expressions in CFR 48, indicating the heavily prescriptive nature of the document. Permissions also form a significant part (21.9%), suggesting the liberties and allowances are embedded within the regulatory framework. In contrast, prohibitions (5.4%) and dispensations (1.4%) are less frequent, indicating that the document leans more towards defining what is required or allowed rather than what is explicitly forbidden or exempted. This research also highlights the challenges encountered during the extraction process, providing insights into the complexities of parsing legal texts and the intricacies of deontic language. These challenges range from the technical difficulties of parsing a complex hierarchical document to the conceptual challenges of defining precise rulesets for regulations and provisions. In summary, the results deepen the understanding of regulatory compliance in software engineering and contribute to the development of more effective and efficient automated extraction tools.},
  archive      = {J_IJSEKE},
  author       = {Patrick D. Cook and Susan A. Mengel and Siva Parameswaran},
  doi          = {10.1142/S021819402341005X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1787-1812},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {SHAMROQ: A software engineering methodology to extract deontic expressions from the code of federal regulations — a single-case, embedded case study},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing code summarization with graph embedding and
pre-trained model. <em>IJSEKE</em>, <em>33</em>(11n12), 1765–1786. (<a
href="https://doi.org/10.1142/S0218194023410024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code summarization is a task that aims at automatically producing descriptions of source code. Recently many deep-learning-based approaches have been proposed to generate accurate code summaries, among which pre-trained models (PTMs) for programming languages have achieved promising results. It is well known that source code written in programming languages is highly structured and unambiguous. Though previous work pre-trained the model with well-design tasks to learn universal representation from a large scale of data, they have not considered structure information during the fine-tuning stage. To make full use of both the pre-trained programming language model and the structure information of source code, we utilize Flow-Augmented Abstract Syntax Tree (FA-AST) of source code for structure information and propose GraphPLBART — Graph-augmented Programming Language and Bi-directional Auto-Regressive Transformer, which can effectively introduce structure information to a well PTM through a cross attention layer. Compared with the best-performing baselines, GraphPLBART still improves by 3.2%, 7.1%, and 1.2% in terms of BLEU, METEOR, and ROUGE-L, respectively, on Java dataset, and also improves by 4.0%, 6.3%, and 2.1% on Python dataset. Further experiment shows that the structure information from FA-AST has significant benefits for the performance of GraphPLBART. In addition, our meticulous manual evaluation experiment further reinforces the superiority of our proposed approach. This demonstrates its remarkable abstract quality and solidifies its position as a promising solution in the field of code summarization.},
  archive      = {J_IJSEKE},
  author       = {Lixuan Li and Jie Li and Yihui Xu and Hao Zhu and Xiaofang Zhang},
  doi          = {10.1142/S0218194023410024},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1765-1786},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Enhancing code summarization with graph embedding and pre-trained model},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing multi-behavior recommendations through capturing
dynamic preferences. <em>IJSEKE</em>, <em>33</em>(11n12), 1749–1763. (<a
href="https://doi.org/10.1142/S0218194023410012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-behavior recommendation has gained significant attention in recent years for its ability to outperform single-behavior models. Current research related to multi-behavior models leaves room for improvement in the following two areas. First, the noise carried by individual behaviors and the additional noise generated during behavior processing is often overlooked, and these can ultimately degrade recommendation performance. Second, the specific time period of behavioral interactions and the frequency of interactions within that time period are also not taken into account. To address the above limitations, we propose a multi-behavior recommendation model integrating dynamic preferences (MB-DP) that captures dynamic interests while smoothing and denoising multi-behavior information. MB-DP extracts low and high-order semantics from various behaviors and unifies the measurements to generate interaction predictions. Additionally, it analyzes the interaction time and frequency of each behavior using gated recurrent units to capture the dynamic preferences of users and improve the prediction values. Extensive experimental results on two real-world datasets show that MB-DP significantly improves recommendation performance compared to the state-of-the-art baselines.},
  archive      = {J_IJSEKE},
  author       = {Cairong Yan and Xiaopeng Guan and Haixia Han and Zhaohui Zhang and Yanting Zhang},
  doi          = {10.1142/S0218194023410012},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1749-1763},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Enhancing multi-behavior recommendations through capturing dynamic preferences},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic drilling sampling method and evaluation model for
big streaming data. <em>IJSEKE</em>, <em>33</em>(11n12), 1725–1748. (<a
href="https://doi.org/10.1142/S0218194023410036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The big data sampling method for real-time and high-speed streaming data is prone to lose the value and information of a large amount of discrete data, and it is not easy to make an efficient and accurate evaluation of the value characteristics of streaming data. The SDSLA sampling method based on mineral drilling exploration can evaluate the valuable information of streaming data containing many discrete data in real-time, but when the range of discrete data is irregular, it has low sampling accuracy for discrete data. Based on the SDSLA algorithm, we propose a dynamic drilling sampling method SDDS, which takes well as the analysis unit, dynamically changes the size and position of the well, and accurately locates the position and range of discrete data. A new model SDVEM is further proposed for data valuation, which evaluates the sample set from discrete, centralized, and overall dimensions. Experiments show that compared with the SDSLA algorithm, the sample sampled by the SDDS algorithm has higher evaluation accuracy, and the probability distribution of the sample is closer to the original streaming data, with the AOCV indicator being nearly 10% higher. In addition, the SDDS algorithm can achieve over 90% accuracy, recall, and F1 score for training and testing neural networks with small sampling rates, all of which are higher than the SDSLA algorithm. In summary, the SDDS algorithm not only accurately evaluates the value characteristics of streaming data but also facilitates the training of neural network models, which has important research significance in big data estimation.},
  archive      = {J_IJSEKE},
  author       = {Zhaohui Zhang and Pei Zhang and Peng Zhang and Fujuan Xu and Chaochao Hu and Pengwei Wang},
  doi          = {10.1142/S0218194023410036},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1725-1748},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A dynamic drilling sampling method and evaluation model for big streaming data},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Influential spreaders identification by fusing network
topology. <em>IJSEKE</em>, <em>33</em>(11n12), 1701–1724. (<a
href="https://doi.org/10.1142/S0218194023410097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of network science, complex network analysis has received extensive attention. In recent years, influential spreaders identification has become a hot topic in the research of complex networks. In general, influential spreader identification algorithms are mainly divided into centrality-based algorithms and topology-based algorithms. However, centrality-based algorithms have to face the information limitation problem that leads to low accuracy for identifying influential spreaders. Topology-based algorithms have both structural and positional limitation problems that lead to low accuracy for identifying peripheral influential spreaders. In this paper, we focus on improving the situation and propose two influential spreader identification algorithms, NSC (neighborhood structure centrality) and NPC (neighborhood position centrality) from both the perspective of the centrality and the network topology. NSC algorithm collects various types of network structure information through structure embedding and clustering, so as to solve missing network structure information problem. NPC algorithm calculates neighborhood location information by improving the k-shell algorithm to tackle location limitation problem. Experimental results with fourteen baseline algorithms show that our proposed algorithms NSC and NPC can achieve higher accuracy.},
  archive      = {J_IJSEKE},
  author       = {Ziyi Zhang and Rong Yan and Wei Yuan and Lintao Zhang},
  doi          = {10.1142/S0218194023410097},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1701-1724},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Influential spreaders identification by fusing network topology},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OC-detector: Detecting smart contract vulnerabilities based
on clustering opcode instructions. <em>IJSEKE</em>, <em>33</em>(11n12),
1673–1700. (<a href="https://doi.org/10.1142/S0218194023410061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contracts are programs running on blockchain. In recent years, due to the persistent occurrence of security-related accidents in smart contracts, the effective detection of vulnerabilities in smart contracts has received extensive attention from researchers and engineers. Machine learning-based vulnerability detection techniques have the advantage that they do not need expert rules for determining vulnerabilities. However, existing approaches cannot identify vulnerabilities when the versions of smart contract compilers are updated. In this paper, we propose OC-Detector (Opcode Clustering Detector), a smart contract vulnerability detection approach based on clustering opcode instructions. OC-Detector learns the characteristics of opcode instructions to cluster them and replaces opcode instructions belonging to the same cluster with the ID of the cluster. After that, the similarity between the contract under analysis and contracts in the vulnerability database is calculated to identify vulnerabilities. The experimental results demonstrate that OC-Detector improves the F 1 value of detecting vulnerabilities from 0.04 to 0.40 compared to DC-Hunter, Securify, SmartCheck and Osiris. Additionally, compared to DC-Hunter, the F 1 value is improved by 0.27 when detecting vulnerabilities in smart contracts compiled by different versions of compilers.},
  archive      = {J_IJSEKE},
  author       = {Xiguo Gu and Liwei Zheng and Huiwen Yang and Shifan Liu and Zhanqi Cui},
  doi          = {10.1142/S0218194023410061},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1673-1700},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {OC-detector: Detecting smart contract vulnerabilities based on clustering opcode instructions},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Demystifying practices, challenges and expected features of
using GitHub copilot. <em>IJSEKE</em>, <em>33</em>(11n12), 1653–1672.
(<a href="https://doi.org/10.1142/S0218194023410048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advances in machine learning, there is a growing interest in AI-enabled tools for autocompleting source code. GitHub Copilot, also referred to as the “AI Pair Programmer”, has been trained on billions of lines of open source GitHub code, and is one of such tools that has been increasingly used since its launch in June 2021. However, little effort has been devoted to understanding the practices, challenges, and expected features of using Copilot in programming for auto-completed source code from the point of view of practitioners. To this end, we conducted an empirical study by collecting and analyzing the data from Stack Overflow (SO) and GitHub Discussions. More specifically, we searched and manually collected 303 SO posts and 927 GitHub discussions related to the usage of Copilot. We identified the programming languages, Integrated Development Environments (IDEs), technologies used with Copilot, functions implemented, benefits, limitations, and challenges when using Copilot. The results show that when practitioners use Copilot: (1) The major programming languages used with Copilot are JavaScript and Python , (2) the main IDE used with Copilot is Visual Studio Code , (3) the most common used technology with Copilot is Node.js , (4) the leading function implemented by Copilot is data processing , (5) the main purpose of users using Copilot is to help generate code , (6) the significant benefit of using Copilot is useful code generation , (7) the main limitation encountered by practitioners when using Copilot is difficulty of integration , and (8) the most common expected feature is that Copilot can be integrated with more IDEs . Our results suggest that using Copilot is like a double-edged sword, which requires developers to carefully consider various aspects when deciding whether or not to use it. Our study provides empirically grounded foundations that could inform software developers and practitioners, as well as provide a basis for future investigations on the role of Copilot as an AI pair programmer in software development.},
  archive      = {J_IJSEKE},
  author       = {Beiqi Zhang and Peng Liang and Xiyu Zhou and Aakash Ahmad and Muhammad Waseem},
  doi          = {10.1142/S0218194023410048},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1653-1672},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Demystifying practices, challenges and expected features of using GitHub copilot},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Foreword. <em>IJSEKE</em>, <em>33</em>(11n12), 1651. (<a
href="https://doi.org/10.1142/S0218194023020011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJSEKE},
  author       = {Shi-Kuo Chang and Giuseppe Polese},
  doi          = {10.1142/S0218194023020011},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1651},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Foreword},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting just-in-time code comment updating via programming
context and refactor. <em>IJSEKE</em>, <em>33</em>(10), 1619–1649. (<a
href="https://doi.org/10.1142/S0218194023500456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comments are summary descriptions of code snippets. When analyzing and maintaining programs, developers tend to read tidy comments rather than lengthy code. To prevent developers from misunderstanding the program or leading to potential bugs, ensuring the consistency and co-evolution of comments and the corresponding code is an integral development activity in practice. Nevertheless, when modifying code, developers sometimes neglect to update the relevant comments, resulting in inconsistency. Such comments may pose threats to the comprehension and maintenance of the software. In our study, we propose an overall approach named Context and Refactor based Comment Updater (CRCU). CRCU is a Just-In-Time (JIT) comment updater for specific commits. It takes a commit-id as input and updates all the method comments in this commit according to the code change. CRCU could be viewed as an optimization and augmentation of existing comment updaters, especially those that rely only on neural networks. Compared to the existing comment updaters, CRCU fully leverages the programming context and refactoring types of the modified methods to improve its performance. In addition, several customized enhancements in data pre-processing are introduced in CRCU to handle and filter out low-quality commits. We conduct extensive experiments to evaluate the effectiveness of CRCU. The evaluation results show that CRCU combined with the state-of-the-art approaches could improve the average Accuracy by 6.87% and reduce the developers’ edits by 0.298 on average.},
  archive      = {J_IJSEKE},
  author       = {Xiangbo Mi and Jingxuan Zhang and Yixuan Tang and Yue Ju and Jinpeng Lan},
  doi          = {10.1142/S0218194023500456},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1619-1649},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Boosting just-in-time code comment updating via programming context and refactor},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EVaDe: Efficient and lightweight mirai variants detection
via approximate largest submatrix search. <em>IJSEKE</em>,
<em>33</em>(10), 1599–1618. (<a
href="https://doi.org/10.1142/S0218194023500444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Mirai botnet, notorious for launching significant Distributed Denial of Service (DDoS) attacks and crippling portions of internet services in late 2016, has emerged as a significant threat. Its threat is magnified by the open-source nature of the original Mirai code, which enables a propagation and evolution rate that surpasses traditional malware and frequently defies common sense. As the primary targets of Mirai attacks, Internet of Things (IoT) devices must promptly adapt to the evolving variations of the Mirai threat scenario. In practice, however, IoT devices are frequently constrained by insufficient security detection resources. Therefore, there is an urgent need for a lightweight framework capable of handling Mirai variants and dynamically updating its rule set in order to effectively counter the threat. In response to these challenges, we present Efficient and lightweight Mirai Variants Detection ( EVaDe ), a novel, lightweight framework for detecting Mirai. EVaDe unleashes the power of sample function mining to efficiently automate the generation of detection rules, requiring limited hardware resources while maintaining effectiveness against Mirai and its numerous variants. In addition, to improve the efficacy of rule generation, we propose a sophisticated algorithm designed to optimize the maximum submatrix problem, thereby facilitating the efficient and rapid extraction of malicious rules from the sample group. We validated the experiments on actual IoT devices with significantly compressed performance overheads. An average sample detection time of 5 ms to make sure the system can be deployed in real production. According to the result, the approach has an average detection rate of 95% for Mirai and its variants, which beats every other well-known piece of commercial antivirus software on the market by 3% to 56%.},
  archive      = {J_IJSEKE},
  author       = {Xuguo Wang and Ligeng Chen and Yuyang Wang and Hao Huang and Bing Mao},
  doi          = {10.1142/S0218194023500444},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1599-1618},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {EVaDe: Efficient and lightweight mirai variants detection via approximate largest submatrix search},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Python API misuse mining and classification based on hybrid
analysis and attention mechanism. <em>IJSEKE</em>, <em>33</em>(10),
1567–1597. (<a href="https://doi.org/10.1142/S0218194023500432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {APIs play a crucial role in contemporary software development, streamlining implementation and maintenance processes. However, improper API usage can result in significant issues such as unexpected outcomes, security vulnerabilities and system crashes. To detect API misuses, current methods primarily rely on comparing established API usage patterns with target points for automated detection, mainly based on pre-validated datasets. Nonetheless, there is a scarcity of publicly available datasets on API misuses and their corresponding fixes, which hinders data-driven research. Moreover, most existing techniques concentrate on statically typed languages, such as Java and C, with only a few addressing dynamic languages like Python effectively, due to difficulties in handling dynamic features. Therefore, it is essential to identify Python API misuses and their fixes automatically and promptly. In this paper, we introduce HatPAM, a Hybrid Analysis and Attention-based Python API-Misuse Miner, which (a) provides a method for automatically mining true-positive commits related to Python API-misuse fixes from GitHub and (b) presents the subsequent processing for classifying Python API misuses in true-positive cases. Particularly, HatPAM applies hybrid static analysis and introduces a structure-based attention mechanism to examine syntax, semantics and structural features in Python code context, and considers the consistency between code and developers’ natural intent to significantly reduce false-positive cases. Evaluation on six popular Python projects reveals that HatPAM outperforms various state-of-the-art baselines, achieving up to 92.2% Precision , 86.7% Recall and 89.3% F1-score , indicating its capability to identify and classify Python API-misuse commits.},
  archive      = {J_IJSEKE},
  author       = {Xincheng He and Xiaojin Liu and Lei Xu},
  doi          = {10.1142/S0218194023500432},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1567-1597},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Python API misuse mining and classification based on hybrid analysis and attention mechanism},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nimbus++: Revisiting efficient function signature recovery
with depth data analysis. <em>IJSEKE</em>, <em>33</em>(10), 1537–1565.
(<a href="https://doi.org/10.1142/S0218194023500420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function signature recovery is vital for many binary analysis tasks, led by control-flow integrity enhancement. To minimize human effort, existing works attempt to replace rule-based methods with learning-based methods. These works put a lot of work into improving the system’s performance, but this had the unintended consequence of increasing resource usage. However, recovering the function signature is more about providing information for subsequent tasks, e.g. reverse engineering, so both efficiency and performance are significant. To identify the fundamental factors that increase efficiency, we attempt to optimize data-driven systems throughout their lifecycle from a data perspective. To this end, we perform detailed data analysis on a carefully collected dataset. After analysis and exploration, selective input is adopted and a multi-task learning (MTL) structure is introduced for function feature recovery to make full use of mutual information, and the computing resource overhead is optimized based on the observation of information deviation and sub-task relationship. The resource usage of the entire process is significantly reduced by our suggested solution, named Nimbus++ for efficient function signature recovery, without sacrificing performance. Our test findings demonstrate that we even surpass the state-of-the-art method’s prediction accuracy across all function signature recovery tasks by about 1% with just about 12.5% of the processing time.},
  archive      = {J_IJSEKE},
  author       = {Ligeng Chen and Yi Qian and Yuyang Wang and Bing Mao},
  doi          = {10.1142/S0218194023500420},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1537-1565},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Nimbus++: Revisiting efficient function signature recovery with depth data analysis},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An approach based on machine learning for the cybersecurity
of blockchain-based smart internet of medical things (IoMT) networks.
<em>IJSEKE</em>, <em>33</em>(10), 1513–1535. (<a
href="https://doi.org/10.1142/S0218194023500419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a hybrid blockchain architecture for Internet of Medical Things (IoMT) systems, aiming to enhance their security and performance. The proposed approach combines artificial intelligence (AI) models with blockchain technology to create a safe and efficient healthcare system. The study focuses on addressing the challenges related to data storage, data management, real-time medical applications, and system precision in IoMT. Through experimental evaluations, the effectiveness of the proposed techniques in terms of communication overhead, transaction performance, and privacy preservation is demonstrated. The results highlight the potential of leveraging AI and blockchain to improve the overall functionality of IoMT systems.},
  archive      = {J_IJSEKE},
  author       = {Mohammed Naif Alatawi},
  doi          = {10.1142/S0218194023500419},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1513-1535},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An approach based on machine learning for the cybersecurity of blockchain-based smart internet of medical things (IoMT) networks},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimal WSN coverage based on adapted transit search
algorithm. <em>IJSEKE</em>, <em>33</em>(10), 1489–1512. (<a
href="https://doi.org/10.1142/S0218194023400016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wireless sensor network (WSN) coverage is one of the most significant impacts on the quality of service that directly determines the efficiency reality of applications. The distribution of sensor nodes in the WSN determines the size of the network monitoring coverage area, whether there is duplicate coverage, and monitoring blind regions. This study introduces an optimal coverage strategy for the sensor node positions in the sensing region based on an adapted transit search (ATS) algorithm. The transit search (TS) algorithm is a recently developed metaheuristic algorithm with several advantages, e.g. simple concept, robust process, and ease of implementation; still, TS has limitations in the ratios of exploration and exploitation for avoiding the local optimum trap when dealing with complicated node coverage optimization situations. The ATS is implemented by adapting and updating equations with stochastic reverse learning and multi-direction strategies to prevent its original algorithm drawbacks. The experimental analysis is carried out to demonstrate the efficiency of the designed coverage scheme in terms of various metrics, e.g. coverage rate, positioning errors, converge speed, and executed time. Compared experimental-result shows that the ATS scheme offers the WSN applicability coverage model to perform the deployment network application with excellent quality. Significantly, the coverage rate archived of the ATS is 87%, but the other methods are only below or equally 84% in the same comparison conditions.},
  archive      = {J_IJSEKE},
  author       = {Thi-Kien Dao and Trong-The Nguyen and Truong-Giang Ngo and Trinh-Dong Nguyen},
  doi          = {10.1142/S0218194023400016},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1489-1512},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An optimal WSN coverage based on adapted transit search algorithm},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NISe: Non-invasive secure framework for multi-access edge
computing. <em>IJSEKE</em>, <em>33</em>(9), 1467–1488. (<a
href="https://doi.org/10.1142/S0218194023500390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the emerging security challenges in Multi-Access Edge Computing (MEC), it is imperative that solutions go beyond the current infrastructure-centric measures. These methods, including authentication and access control, are insufficient to combat malware that conceals itself within ME applications. The acknowledged flaws in the ME application layer necessitate an immediate call for creative solutions. In this work, we propose a non-invasive security architecture for MEC, meticulously designed to strike a balance between performance burden and security protection capabilities. The objective of the design contains three major aspects, i.e. user experience, service density and serviceability. We conduct a thorough evaluation that enables us to quantify the significance of high bandwidth, low user experience latency and MEC serviceability. The experimental results and ablation studies indicate that our proposed method effectively balances user experience and security capabilities. This not only provides a practical and cost-effective solution but also establishes a strong precedent for the community to develop a secure MEC with superior performance in real-world production environments.},
  archive      = {J_IJSEKE},
  author       = {Xuguo Wang and Ligeng Chen and Yu Liang and Hao Huang},
  doi          = {10.1142/S0218194023500390},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1467-1488},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {NISe: Non-invasive secure framework for multi-access edge computing},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empirical study on GitHub sponsor mechanism.
<em>IJSEKE</em>, <em>33</em>(9), 1439–1465. (<a
href="https://doi.org/10.1142/S0218194023500377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From May 2019, GitHub launched sponsor mechanism indicating that GitHub is moving towards deeper integration of open source development and economic support. It will bring more comprehensive and diversified support to the open source community. However, the number of developers profiting from the sponsor mechanism follows a long tail distribution. Our study found that only 31% of developers who started the sponsor mechanism received rewards, and 39.3% of them only received a reward of one dollar. Our work focuses on identifying what factors affect the availability of sponsorship for developers in open source community. We start by defining 45 features to characterize the developers in four dimensions i.e. Personality , Advertisement , Repository and Behavior . The results of statistical analysis indicate that most of the proposed features differ significantly between the ones who received rewards (short for MTs_Yes ) from those that are not. After that, we build machine learning model based on the proposed features to predict MTs_Yes . Compared with the existing work, results show that our method outperforms baselines by 30% for AUC (Area Under the Curve) . In addition, we investigated the relative contribution of features in detecting MTs_Yes and analyzed the important features by using an interpretable model SHAP . Finally, based on the experimental results, we put forward corresponding and practical suggestions for developers who want to receive rewards so as to make the community of open source projects develop more harmonious.},
  archive      = {J_IJSEKE},
  author       = {Ziyuan Zhang and Yiqian Yang and Haolan He and Jie Chen},
  doi          = {10.1142/S0218194023500377},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1439-1465},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An empirical study on GitHub sponsor mechanism},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing accessibility to data in data-intensive web
applications by using intelligent web prefetching methodologies.
<em>IJSEKE</em>, <em>33</em>(9), 1405–1438. (<a
href="https://doi.org/10.1142/S0218194023500365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-intensive Web Applications built using client–server architectures usually provide prefetching mechanisms to enhance data accessibility. Prefetching is a strategy of retrieving data before it is requested so that it can be ready when the user requests it. Prefetching reduces the load on the web server by making data available before the user requests it. Prefetching can be used for static content, such as images and web pages, as well as dynamic content, such as search results. Prefetching can also be used to improve the performance of web applications, as the data is available quickly. There are several scheduling methods, such as time-based scheduling, event-based scheduling and priority-based scheduling, for prefetching to ensure that essential data is always ready when the user requests it. In this study, we focus on time-based scheduling for prefetching. We introduce time-based scheduling methodologies using sequential pattern mining techniques and long-term short memory-based deep learning strategies. To show the usefulness of these strategies, we develop a prototype application. We conduct an extensive experimental study to evaluate the performance of the proposed time-based scheduling methodologies using both performance and accuracy metrics. Based on the computed metrics, using proposed prefetching methods provided a promising cache hit rate when using the optimal cache size. The results show that the proposed prefetching methodologies are useful in data-intensive web applications for enhancing data accessibility. Work remains to investigate the use of attention-based sequence-to-sequence models in the web prefetching domain.},
  archive      = {J_IJSEKE},
  author       = {Tolga Buyuktanir and I. Onur Sigirci and Mehmet S. Aktas},
  doi          = {10.1142/S0218194023500365},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1405-1438},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Enhancing accessibility to data in data-intensive web applications by using intelligent web prefetching methodologies},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A combined usage of NLP libraries towards analyzing software
documents. <em>IJSEKE</em>, <em>33</em>(9), 1387–1404. (<a
href="https://doi.org/10.1142/S0218194023500353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software documents are commonly processed by natural language processing (NLP) libraries to extract information. The libraries provide similar functional APIs to achieve NLP tasks, numerous toolkits result in a problem of selection. In this work, we propose a method to combine the strengths of different NLP libraries to avoid the subjective selection of a specific NLP library. The combined usage is conducted through two steps, i.e. document-level selection of primary NLP library and sentence-level overwriting. The primary NLP library is determined according to the overlap degree of the results. The highest overlap degree indicated the most effective NLP library on a specific NLP task. Through sentence-level overwriting, the possible fine-gained improvements from other libraries are extracted to overwrite the outputs of primary library. We evaluate the combined method with six widely used NLP libraries and 200 documents from three different sources. The results show that the combined method can generally outperform all the studied NLP libraries in terms of accuracy. The finding means that our combined method can be used instead of individual NLP library for more effective results.},
  archive      = {J_IJSEKE},
  author       = {Xianglong Kong and Hangyi Zhuo and Zhechun Gu and Xinyun Cheng and Fan Zhang},
  doi          = {10.1142/S0218194023500353},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1387-1404},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A combined usage of NLP libraries towards analyzing software documents},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive semantic annotation tool for teachers based on
context-aware and internet of things. <em>IJSEKE</em>, <em>33</em>(9),
1355–1385. (<a href="https://doi.org/10.1142/S0218194023500341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotation has demonstrated its importance in several areas, notably in the modeling of annotation activity in the automation and adaptation phase. However, the context sensor is commonly manual or semi-automatic. The use of the Internet of Things with annotation gives a qualitative leap in the field of higher education and universities. In this field, teachers, during their pedagogical activities, require more information found in different textual documents, tools, and databases. In addition, they organized several meetings and took considerable time to make appropriate and right decisions during the deliberation process. In this paper, we propose an adaptive annotation tool based on the Internet of Things addressed for teachers during their educational tasks. The presented tool automatically adapts according to the context of the teacher’s tasks and it enables the annotation of all documents written in different languages. To consider all the parameters of the teaching activities when interfacing with the annotation and to give the useful information for teachers during decision-making, we have adopted an architecture based on the ontology of context and the multilingual ontology of deliberation and redemption. To implement this tool, we used Semantic Web and Internet of Things technology to achieve the desired architecture. Likewise, the usability of the proposed annotation tool has been tested with a real case in coordination with teachers and administrators during the deliberation operation to decide to rescue and redeem students at the university.},
  archive      = {J_IJSEKE},
  author       = {Aissa Bensattalah and Rachid Chalal and Fahima Nader},
  doi          = {10.1142/S0218194023500341},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1355-1385},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An adaptive semantic annotation tool for teachers based on context-aware and internet of things},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DetAC: Approach to detect access control vulnerability in
web application based on sitemap model with global information
representation. <em>IJSEKE</em>, <em>33</em>(9), 1327–1354. (<a
href="https://doi.org/10.1142/S0218194023500298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Access control vulnerabilities that lead to elevated privileges are among the most dangerous vulnerabilities in Web applications. Most of the existing detection methods use dynamic or static analysis techniques alone, which suffer from high manual involvement, low automation, high leakage rate, low page coverage, and other deficiencies. To this end, this paper proposes a novel access control vulnerability detection method (DetAC) based on a sitemap model with global information representation. This method first constructs a static site-wide sitemap model based on the page link addresses in the Web application source code through static analysis techniques. After that, the application is logged in and executed dynamically with different role users. During this process, execution traces and request parameters are collected and converted into annotations to fill the corresponding edges of the static site-wide sitemap model. Then, the sitemap model with global information representation is obtained. This model can represent both the global control flow and data flow of the application. Then DetAC analyzes the role-based and user-based access control policies of the Web application based on the node reachability and annotated data features of the model. And according to the information such as role, user, and access resources, it generates attack vectors to achieve different roles and the same role of different users to access each other’s resources. Finally, access control vulnerabilities are detected based on the equivalence of the results obtained using attack vector access and normal access to the Web application server. DetAC was validated on five real open-source Web applications, and the results showed that DetAC successfully detected up to 12 access control vulnerabilities, which are more than those of the traditional seven tools. The dynamic analysis page coverage rate was significantly improved during the detection process, reaching an average of 91.37%.},
  archive      = {J_IJSEKE},
  author       = {Jiadong Ren and Mingyou Wu and Bing Zhang and Ke Xu and Shangyang Li and Qian Wang and Yue Chang and Tao Cheng},
  doi          = {10.1142/S0218194023500298},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1327-1354},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {DetAC: Approach to detect access control vulnerability in web application based on sitemap model with global information representation},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Authentication and authorization management in SOA with the
focus on RESTful services. <em>IJSEKE</em>, <em>33</em>(8), 1293–1326.
(<a href="https://doi.org/10.1142/S0218194023500328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SOA is an architectural style that enables providing applications as services. Following the authentication procedure, most Web services-based applications use application-specific access control mechanisms to make authorization decisions. Services can interact with one another, sometimes relying on a trust-based relationship. However, if unauthorized access is gained to a particular service, it could potentially jeopardize the whole security system. REST, likewise, is an architectural style defined by a set of principles for creating network-based software structures. These concepts come together to form a coherent metaphor for the Web’s processes and interactions. In RESTful services, authentication and authorization play a tremendous role in terms of security, so services are constantly charged with authenticating users. Security as an essential aspect of services affects those servers necessarily containing the authentication mechanism, and they must authenticate each service for each of its requests. This study presents the mechanisms of authentication and authorization in RESTful services. A RESTful service’s authorization management framework is proposed and the possibility to manage service access authorization to specific services (resources) is described and implemented. The paper is concluded with the presentation of experimental results derived from the implementation of the REST services based on the proposed framework.},
  archive      = {J_IJSEKE},
  author       = {Arbër Beshiri},
  doi          = {10.1142/S0218194023500328},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1293-1326},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Authentication and authorization management in SOA with the focus on RESTful services},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bug localization with features crossing and structured
semantic information matching. <em>IJSEKE</em>, <em>33</em>(8),
1261–1291. (<a href="https://doi.org/10.1142/S0218194023500316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bug localization techniques aim to locate the relevant buggy source files according to the bug described by the given bug report, so as to improve the localization efficiency of developers and reduce the cost of software maintenance. The traditional bug localization techniques based on Information Retrieval (IR) usually use the classical text retrieval model and combines the specific domain knowledge features in software engineering to locate the bugs. However, there exists the vocabulary mismatch problem between the bug report and the source file, which may affect the performance of bug localization. Therefore, the relevant deep learning model was introduced later to compute the similarity between the bug report and the source file from the perspective of semantic features. Bug localization approaches based on IR and deep learning have their own advantages and disadvantages, so this paper proposes a model named LocFront which combines IR and deep learning. On the one hand, the Features Crossing module in LocFront carries out the deep crossing operation on the extracted software-specific features to fully acquire the linear and nonlinear relationships. On the other hand, the Structured Semantic Information Matching module in LocFront performs semantic matching on the structured information between the bug report and the source file. Then the Fusion module in LocFront fuses the results of the two above modules to obtain the final localization score. The experimental results on five benchmark datasets show that LocFront outperforms the state-of-the-art bug localization approaches.},
  archive      = {J_IJSEKE},
  author       = {Guoqing Xu and Xingqi Wang and Dan Wei and Yanli Shao and Bin Chen},
  doi          = {10.1142/S0218194023500316},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1261-1291},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Bug localization with features crossing and structured semantic information matching},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spectral test generation for boolean expressions.
<em>IJSEKE</em>, <em>33</em>(8), 1239–1260. (<a
href="https://doi.org/10.1142/S021819402350033X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method for testing Boolean expressions. It is based on spectral, aka Fourier analysis of Boolean functions which is exploited to generate test inputs. The approach has three important contributions: (i) It generates a relatively small test suite with a high capability of fault detection, (ii) The test suite is prioritized such that expected fault detection time is shorter, (iii) It is entirely mathematical relying on a simple and straightforward formula. The proposed method is formulated and evaluations are performed on both synthetic and real expressions. It is also compared with two common test generation criteria, MC/DC and Minimal MUMCUT. Evaluations show that the test suite generated by the spectral approach is relatively small while expressing the capability of a better and quicker fault detection. The approach presented in this paper provides a useful insight into how spectral/Fourier analysis of Boolean functions can be exploited in software testing.},
  archive      = {J_IJSEKE},
  author       = {Tolga Ayav},
  doi          = {10.1142/S021819402350033X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1239-1260},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Spectral test generation for boolean expressions},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based root cause localization in microservice systems
with protection mechanisms. <em>IJSEKE</em>, <em>33</em>(8), 1211–1238.
(<a href="https://doi.org/10.1142/S0218194023500304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service anomalies are difficult to locate accurately due to their propagation through service dependencies in microservice systems. Besides, the protection mechanisms are introduced into the microservice systems to ensure the stable operation of services. However, the existing approaches ignore the impact of protection mechanisms on the root cause localization of abnormal services. Specifically, the circuit breaking and rate limiting mechanisms can refuse service requests and thus change the way of anomaly propagation. Moreover, the different service request frequencies and latency make service dependencies change dynamically, resulting in the different probabilities of anomaly propagation among services. In this paper, we propose a novel framework named MicroGBPM to locate the root cause of abnormal services. We model the anomaly propagation among services as a dynamically constructed service attributed graph with metrics and traces when a failure occurs. To eliminate the impact of the protection mechanisms, we design a two-stage dynamic calibration strategy to adjust the probability of anomaly propagation among services. Then, we propose a random walking approach to calculate the root cause results by using the PageRank algorithm. The experimental results show that MicroGBPM improves the accuracy of root cause localization compared to other approaches in the microservice systems with protection mechanisms.},
  archive      = {J_IJSEKE},
  author       = {Wei Tian and Haitao Zhang and Neng Yang and Yepeng Zhang},
  doi          = {10.1142/S0218194023500304},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1211-1238},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Graph-based root cause localization in microservice systems with protection mechanisms},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised classification of UML class diagrams based on
f-KNB. <em>IJSEKE</em>, <em>33</em>(8), 1169–1210. (<a
href="https://doi.org/10.1142/S0218194023500286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Often most software development doesn’t start from scratch but applies previously developed artifacts. These reusable artifacts are involved in various phases of the software life cycle, ranging from requirements to maintenance. Software design as the high level of software development process has an important impact on the following stages, so its reuse is gaining more and more attention. Unified modeling language (UML) class diagram as a modeling tool has become a de facto standard of software design, and thus its reuse also becomes a concern accordingly. So far, the related research on the reuse of UML class diagrams has focused on matching and retrieval. As a large number of class diagrams enter the repository for reuse, classification has become an essential task. The classification is divided into unsupervised classification (also known as clustering) and supervised classification. In our previous work, we discussed the clustering of UML class diagrams. In this paper, we focus on only the supervised classification of UML class diagrams and propose a supervised classification method. A novel ensemble classifier F-KNB combining both dependent and independent construction ideas is built. The similarity of class diagrams is described, in which the semantic, structural and hybrid matching is defined, respectively. The extracted feature elements are used in base classifiers F-KNN and F-NBs that are constructed based on improved K -nearest neighbors (KNNs) and Naive Bayes (NB), respectively. A series of experimental results show that the proposed ensemble classifier F-KNB shows a good classification quality and efficiency under the condition of variable size and distribution of training samples.},
  archive      = {J_IJSEKE},
  author       = {Zhongchen Yuan and Zongmin Ma},
  doi          = {10.1142/S0218194023500286},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1169-1210},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Supervised classification of UML class diagrams based on F-KNB},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Software testing integration-based model (i-BM) framework
for recognizing measure fault output accuracy using machine learning
approach. <em>IJSEKE</em>, <em>33</em>(8), 1149–1168. (<a
href="https://doi.org/10.1142/S0218194023300026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In software development, the software testing phase is an important process in determining the quality level of the software. Software testing is a process of executing a program aimed at finding errors in module access, units, and involves the execution of the system being tested on a number of test inputs, and determining whether the output produced is correct. In this study, a model-based testing (MBT) called integration-based model (I-BM) framework will be developed. This I-BM framework integrates testing variables from several software testing methods, namely black-box testing, white-box testing, unit testing, system testing, and acceptance testing. The integrated variables are function, interface, structure, performance, requirement, documentation, positives, and negatives. Then, this framework will document software errors to form a dataset, which will be measured for the level of accuracy of expected manual fault output using neural network algorithm and support vector machine. From the experiment results, it shows that the accuracy level of predicting fault output values from the I-BM framework using the neural network algorithm is on average 80%, and it produces a superior SVM architecture model in predicting I-BM framework output errors with an accuracy value of 0.99, precision of 0.99, recall of 0.99, and F 1 -score of 0.99. Compared to other MBT, the IBM framework has the advantage of being a more comprehensive software testing model because it starts from the identification of problems, analysis, design, documentation of software testing, and recommendations for each fault output found. Thus, software errors can be classified systematically in the form of a dataset, and not only focus on software testing for product lines and module mappings.},
  archive      = {J_IJSEKE},
  author       = {Zulkifli Zulkifli and Ford Lumban Gaol and Agung Trisetyarso and Widodo Budiharto},
  doi          = {10.1142/S0218194023300026},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1149-1168},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Software testing integration-based model (I-BM) framework for recognizing measure fault output accuracy using machine learning approach},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding the role of stack overflow in supporting
software development tasks: A research perspective. <em>IJSEKE</em>,
<em>33</em>(7), 1119–1148. (<a
href="https://doi.org/10.1142/S0218194023500274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stack Overflow is a Q&amp;A website that is popular among developers and extensively used in software engineering (SE) research. A significant body of research has examined how Stack Overflow can assist with software development tasks, such as recommending APIs. However, while researchers have recognized the importance of Stack Overflow in SE research related to software development tasks, the specific ways in which it is utilized and the reasons for its widespread usage in research have not been thoroughly explored. To address these knowledge gaps, we conducted the first study to understand the role of Stack Overflow in assisting with SE research regarding software development tasks by systematically examining relevant and high-quality research works. Meanwhile, we carried out a qualitative survey to gain insight into why researchers choose to utilize Stack Overflow in SE research and to solicit suggestions for the better use of Stack Overflow in research. The study identifies trends in the research area, prominent researchers and organizations, and the types of tasks that utilize Stack Overflow in research, with coding and debugging being the most common. Moreover, it examines how Stack Overflow data is utilized in SE research regarding software development tasks, including searching, training models, and mining associations. Our qualitative survey of researchers indicates that the popularity of Stack Overflow stems from its comprehensive explanations of technical topics that are often not found in documentation or manuals. The findings provide a comprehensive understanding of the role of Stack Overflow in SE research regarding software development tasks, and offer actionable implications for both researchers and stakeholders of Stack Overflow to facilitate future research and improvements.},
  archive      = {J_IJSEKE},
  author       = {Wenhua Yang and Chaochao Shen},
  doi          = {10.1142/S0218194023500274},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1119-1148},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Understanding the role of stack overflow in supporting software development tasks: A research perspective},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-project transfer learning on lightweight code semantic
graphs for defect prediction. <em>IJSEKE</em>, <em>33</em>(7),
1095–1117. (<a href="https://doi.org/10.1142/S0218194023500262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep learning system (DLS) developed based on one software project for defect prediction may well be applied to the related code on the same project but is usually difficult to be applied to new or unknown software projects. To address this problem, we propose a Transferable Graph Convolutional Neural Network (TGCNN) that can learn defects from the lightweight semantic graphs of code and transfer the learned knowledge from the source project to the target project. We discuss how the semantic graph is constructed from code; how the TGCNN can learn from the graph; and how the learned knowledge can be transferred to a new or unknown project. We also conduct a controlled experiment to evaluate our method. The result shows that despite some limitations, our method performs considerably better than existing methods.},
  archive      = {J_IJSEKE},
  author       = {Dingbang Fang and Shaoying Liu and Yang Li},
  doi          = {10.1142/S0218194023500262},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1095-1117},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Cross-project transfer learning on lightweight code semantic graphs for defect prediction},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A case study of software project replacement: A time series
analysis. <em>IJSEKE</em>, <em>33</em>(7), 1063–1093. (<a
href="https://doi.org/10.1142/S0218194023500250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise software requires constant updates to keep it usable. These updates originate in correcting errors and mainly in new organizational demands. Over time, these demands generate a significant workload that becomes increasingly complex than the first requirements. For this reason, the organization providing the software may choose to continue updating the old product or make it obsolete and replace it with a new one. Identifying the ideal moment to carry out this migration involves, in addition to the costs of keeping the product obsolete for a while, the effort to develop a new one. This work addresses a case study that comprises fifteen years with two migrations of the software project. Due to the availability of the collection of activities performed by the development and support team, performed sequentially over time, the applicability of time series was possible. Furthermore, the historical base of the activities performed made it possible to use the time series decomposition to obtain its trend, seasonality and noise. Time series decomposition indicated many random events in the first migration, while in the second, the team self-regulated, but there were tension points. This study identified a preliminary model whose purpose is to determine when to develop a new software version.},
  archive      = {J_IJSEKE},
  author       = {Alexandre L’Erario and Thiago Arahn Detoni and Alessandro Silveira Duarte},
  doi          = {10.1142/S0218194023500250},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1063-1093},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A case study of software project replacement: A time series analysis},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic clone detection based on code feature fusion
learning. <em>IJSEKE</em>, <em>33</em>(7), 1039–1062. (<a
href="https://doi.org/10.1142/S0218194023500249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code clones are duplicated code snippets that significantly threaten software maintenance and the public corpora of code representation learning. Traditionally, code context and its structure information abstract syntax tree (AST), control flow graph (CFG) are typical representations of source code, and context-based models and structure-based models contributed significantly to the development of code clone detection. In this paper, we present a hybrid embedding model for code clone detection (HEM-CCD), a fusion method of token sequential information and graph-based structure information. We insert tokens’ global context information encoded by a bi-directional recurrent neural network into the AST-based graph for comprehensive code semantic representation. Then, feeding the graph into a gated graph neural network we generate code semantic vectors for similarity evaluation. We have implemented our model on two public clone datasets (BigCloneBench and GoogleCodeJam), and the results indicate that HEM-CCD outperforms several state-of-the-art approaches.},
  archive      = {J_IJSEKE},
  author       = {Qianjin Zhang and Dahai Jin and Yawen Wang and Yunzhan Gong},
  doi          = {10.1142/S0218194023500249},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1039-1062},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Semantic clone detection based on code feature fusion learning},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Studying the co-evolution of source code and acceptance
tests. <em>IJSEKE</em>, <em>33</em>(7), 1011–1037. (<a
href="https://doi.org/10.1142/S0218194023500237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing is a vital part of achieving good-quality software. Deploying untested code can cause system crashes and unexpected behavior. To reduce these problems, testing should evolve with coding. In addition, test suites should not remain static throughout the software versions. Since whenever software gets updated, new functionalities are added, or existing functionalities are changed, test suites should be updated along with the software. Software repositories contain valuable information about the software systems. Access to older versions and differentiating adjacent versions’ source code and acceptance test changes can provide information about the evolution process of the software. This research proposes a method and implementation to analyze 21 open-source real-world projects hosted on GitHub regarding the co-evolution of both software and its acceptance test suites. Related projects are retrieved from repositories, their versions are analyzed, graphs are created, and analysis related to the co-evolution process is performed. Observations show that the source code is getting updated more frequently than the acceptance tests. They indicate a pattern that source code and acceptance tests do not evolve together. Moreover, the analysis showed that a few acceptance tests test most of the functionalities that take a significant line of code.},
  archive      = {J_IJSEKE},
  author       = {Ali Görkem Yalçın and Tugkan Tuglular},
  doi          = {10.1142/S0218194023500237},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1011-1037},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Studying the co-evolution of source code and acceptance tests},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CodeLabeller: A web-based code annotation tool for java
design patterns and summaries. <em>IJSEKE</em>, <em>33</em>(7),
993–1009. (<a href="https://doi.org/10.1142/S0218194023500213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While constructing supervised learning models, we require labeled examples to build a corpus and train a machine learning model. However, most studies have built the labeled dataset manually, which, on many occasions, is a daunting task. To mitigate this problem, we have built an online tool called CodeLabeller. CodeLabeller is a web-based tool that aims to provide an efficient approach to handling the process of labeling source code files for supervised learning methods at scale by improving the data collection process throughout. CodeLabeller is tested by constructing a corpus of over a thousand source files obtained from a large collection of open source Java projects and labeling each Java source file with their respective design patterns and summaries. Twenty-five experts in the field of software engineering participated in a usability evaluation of the tool using the standard User Experience Questionnaire online survey. The survey results demonstrate that the tool achieves the Good standard on hedonic and pragmatic quality standards, is easy to use and meets the needs of annotating the corpus for supervised classifiers. Apart from assisting researchers in crowdsourcing a labeled dataset, the tool has practical applicability in software engineering education and assists in building expert ratings for software artefacts.},
  archive      = {J_IJSEKE},
  author       = {Najam Nazar and Norman Chen and Chun Yong Chong},
  doi          = {10.1142/S0218194023500213},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {993-1009},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {CodeLabeller: A web-based code annotation tool for java design patterns and summaries},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TC4MT: A specification-driven testing framework for model
transformations. <em>IJSEKE</em>, <em>33</em>(6), 953–991. (<a
href="https://doi.org/10.1142/S0218194023500225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model transformation is a core mechanism for Model-Driven Engineering (MDE). Writing complex programs such as model transformations (MT) is error-prone, and efficient testing techniques are required for their quality assurance. There are several challenges when it comes to testing MT, including the automatic generation of suitable input test models and the construction of test oracles based on verification properties. Many approaches to generating input models ensure coverage of a certain level of the source meta-model and some input/output model constraints. Furthermore, most transformation testing techniques are tailored to specific implementation languages or quality properties, which makes it difficult to reuse testing techniques for different languages due to their language-specific nature. The diversity of languages and verification properties raises the need for a black-box testing framework of MT that is independent of transformation implementation languages as well as supports systematic verification of the quality properties. In this paper, we clarify the basic elements of such a framework, and how to apply this framework for systematically testing MT. The main tasks of the model transformation testing process, including test design, test execution and evaluation, are defined and realized within this integrated framework.},
  archive      = {J_IJSEKE},
  author       = {Thi-Hanh Nguyen and Duc-Hanh Dang},
  doi          = {10.1142/S0218194023500225},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {953-991},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {TC4MT: A specification-driven testing framework for model transformations},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing answer selection via ad-hoc knowledge extraction
from unstructured web texts. <em>IJSEKE</em>, <em>33</em>(6), 933–951.
(<a href="https://doi.org/10.1142/S0218194023500201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answer selection aims to identify the most relevant answers to a given question from a set of candidates. It is the fundamental component of intelligent question answering system. To improve performance, it gradually becomes an effective strategy to integrate external structured knowledge bases (KBs) into the answer selection model. Due to expensive cost of construction and maintenance of such KBs, these models are suffering from domain barriers and information incompleteness. In this paper, we propose a two-stage extraction–comprehension answer selection model, which can extract ad-hoc knowledge from unstructured web texts to enhance the performance of answer selection. For the extraction, two types of snippets are extracted from unstructured web pages and utilized as the source of ad-hoc knowledge. For the comprehension, a selective attention mechanism is employed to extract and integrate ad-hoc knowledge from multiple text snippets obtained in the first stage, which can enrich the representation of question–answer pairs and more accurately identify the correct answers. By incorporating ad-hoc knowledge extracted from both types of snippets, the proposed model achieves state-of-the-art results on two public available benchmark datasets. In particular, on WikiQA, in terms of the two evaluation metrics (mean average precision and mean reciprocal rank), it achieves 9.9 % and 8.4 % higher than the previous non-pretraining-based models, and 3.4 % and 3.2 % higher than the pretraining-based models.},
  archive      = {J_IJSEKE},
  author       = {Shengwei Gu and Xiangfeng Luo and Hao Wang},
  doi          = {10.1142/S0218194023500201},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {933-951},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Enhancing answer selection via ad-hoc knowledge extraction from unstructured web texts},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Verification of safety for synchronous-reactive system using
bounded model checking. <em>IJSEKE</em>, <em>33</em>(6), 885–932. (<a
href="https://doi.org/10.1142/S0218194023500195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time embedded systems are increasingly applied in safety-critical areas, so guaranteeing the correctness of such systems by means of formal methods becomes particularly important. In this paper, we propose an optimized bounded model checking (BMC)-based formal verification approach for the verification of safety for synchronous-reactive (SR) models, which are often used to design systems with complicated control logic, especially the real-time embedded control systems. This method is based on the tackling of a series of challenging problems including the management of the logical clock, encoding of the contained ports, representation of the data types of ports, descriptions of behaviors of various components in a considered model, and formal consideration of the fixed-point semantics. We have implemented this proposed method in the prototype Ptolemy-Z3, and integrated this tool into the Ptolemy II environment. In addition, the experimental evaluation on 22 SR models has shown that our method performs better than the existing automatic verification method in Ptolemy II.},
  archive      = {J_IJSEKE},
  author       = {Xiaozhen Zhang and Zhaoming Yang and Hui Kong and Weiqiang Kong},
  doi          = {10.1142/S0218194023500195},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {885-932},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Verification of safety for synchronous-reactive system using bounded model checking},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mutation-based minimal test suite generation for boolean
expressions. <em>IJSEKE</em>, <em>33</em>(6), 865–884. (<a
href="https://doi.org/10.1142/S0218194023500183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boolean expressions are highly involved in control flows of programs and software specifications. Coverage criteria for Boolean expressions aim at producing minimal test suites to detect software faults. There exist various testing criteria, efficiency of which is usually evaluated through mutation analysis. This paper proposes an integer programming-based minimal test suite generation technique relying on mutation analysis. The proposed technique also takes into account the cost of fault detection. The technique is optimal such that the resulting test suite guarantees to detect all the mutants under given fault assumptions, while maximizing the average percentage of fault detection of a test suite. Therefore, the approach presented can also be considered as a reference method to check the efficiency of any common technique. The method is evaluated using four well-known real benchmark sets of Boolean expressions and is also exemplary compared with MCDC criterion. The results show that the test suites generated by the proposed method provide better fault coverage values and faster fault detection.},
  archive      = {J_IJSEKE},
  author       = {Tolga Ayav and Fevzi Belli},
  doi          = {10.1142/S0218194023500183},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {865-884},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Mutation-based minimal test suite generation for boolean expressions},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exact learning of qualitative constraint networks from
membership queries. <em>IJSEKE</em>, <em>33</em>(6), 837–863. (<a
href="https://doi.org/10.1142/S0218194023500171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Qualitative Constraint Network (QCN) is a constraint graph representing problems under qualitative temporal or spatial relations. More formally, a QCN includes a set of entities and a list of qualitative constraints defining the possible scenarios between these entities. Qualitative constraints are expressed as disjunctions of binary relations capturing the (incomplete) knowledge between the involved entities. QCNs effectively represent various real-world applications, including scheduling and planning, configuration, and Geographic Information Systems (GIS). It is, however, challenging to elicit, from the user, the QCN representing a given problem. To overcome this difficulty in practice, we propose a new algorithm for learning, through membership queries, a QCN from a non-expert. Membership queries are asked to elicit temporal or spatial relationships between pairs of temporal or spatial entities. To improve the time performance of our learning algorithm, constraint propagation and ordering heuristics are enforced. The goal is to reduce the number of membership queries needed to reach the target QCN. We conducted several experiments on randomly generated temporal and spatial QCN instances to assess the practical effect of constraint propagation and ordering heuristics. The results of the experiments are encouraging and promising.},
  archive      = {J_IJSEKE},
  author       = {Malek Mouhoub and Hamad Al Marri and Eisa Alanazi},
  doi          = {10.1142/S0218194023500171},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {837-863},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Exact learning of qualitative constraint networks from membership queries},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A methodology to analyze and estimate the software
development process using machine learning techniques. <em>IJSEKE</em>,
<em>33</em>(6), 815–835. (<a
href="https://doi.org/10.1142/S021819402350016X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the software development process and estimating the effort required for its completion is an essential task. In the case of Agile methodology, the values of the parameters used for estimation vary frequently as the scope of the project changes with changes in the requirements of the clients. Hence, the estimation done at the initial phase will not be appropriate until the completion of the project. Therefore, to overcome this issue, a methodology is proposed to estimate the duration of a project by applying machine learning techniques. The use-case point method is used for estimating the duration. Information about the number of use cases and values for environmental and technical factors is stored in a repository. Few values may be uncertain, and to estimate the effort for a new project with few unknown or uncertain values, the machine learning algorithm Gaussian Process Regression (GPR) is used. The repository information is taken as the training dataset, and the new project data is taken as the test dataset. The estimated value shows the accurate duration for the new project. The result is validated with a popular dataset.},
  archive      = {J_IJSEKE},
  author       = {R. Lalitha and P. Sreelekha},
  doi          = {10.1142/S021819402350016X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {815-835},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A methodology to analyze and estimate the software development process using machine learning techniques},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Path generation for a given performance evaluation value
interval by modifying bat algorithm with heuristic. <em>IJSEKE</em>,
<em>33</em>(5), 787–814. (<a
href="https://doi.org/10.1142/S0218194023500158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path generation means generating a path or a set of paths so that the generated path meets specified properties or constraints. To our knowledge, generating a path with the performance evaluation value of the path within a given value interval has received scant attention. This paper subtly formulates the path generation problem as an optimization problem by designing a reasonable fitness function, adapts the Markov decision process with reward model into a weighted digraph by eliminating multiple edges and non-goal dead nodes, constructs the path by using a priority-based indirect coding scheme, and finally modifies the bat algorithm with heuristic to solve the optimization problem. Simulation experiments were carried out for different objective functions, population size, number of nodes, and interval ranges. Experimental results demonstrate the effectiveness and superiority of the proposed algorithm.},
  archive      = {J_IJSEKE},
  author       = {Fujun Wang and Zining Cao and Zhen Li and Chao Xing and Hui Zong},
  doi          = {10.1142/S0218194023500158},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {787-814},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Path generation for a given performance evaluation value interval by modifying bat algorithm with heuristic},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empirical study on code smell introduction and removal in
deep learning software projects. <em>IJSEKE</em>, <em>33</em>(5),
765–786. (<a href="https://doi.org/10.1142/S0218194023500146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing popularity of Deep Learning (DL) software development, code quality issues arise in DL software development. Code smell is one of the factors which reduce the quality of source code. Several previous studies investigated the prevalence of code smell in DL software systems to evaluate the quality of DL source code. However, there is still a lack of understanding of the awareness of individual DL developers in code smell. To more deeply understand the code smell risk in DL software development, it is needed to investigate the code smell awareness of DL developers. In this paper, we present an empirical study on code smell practices of DL developers. Specifically, we performed a quantitative analysis on code smell introduction and removal practices of DL developers. We collected a dataset of code smell introduction and removal history of DL developers from several open source DL software GitHub repositories. We then quantitatively analyzed the collected dataset. As a result of the quantitative analysis, we observed the following three findings on code smell introduction and removal practices of DL developers. First, DL developers tend to perform code smell introduction practice more than code smell removal practice. Second, DL developers have slightly broader code smell introduction scope than code smell removal scope. Third, regular and irregular DL developers have less difference in both code smell introduction and removal practices. The results indicate that DL developers have very poor awareness on code smell risk. Our findings suggest that DL software development project managers should provide a helpful guideline that makes DL developers actively participate code smell removal tasks.},
  archive      = {J_IJSEKE},
  author       = {Jungil Kim and Eunjoo Lee},
  doi          = {10.1142/S0218194023500146},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {765-786},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An empirical study on code smell introduction and removal in deep learning software projects},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A formal approach for consistency management in UML models.
<em>IJSEKE</em>, <em>33</em>(5), 733–763. (<a
href="https://doi.org/10.1142/S0218194023500134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistency is a significant indicator to measure the correctness of a software system in its lifecycle. It is inevitable to introduce inconsistencies between different software artifacts in the software development process. In practice, developers perform consistency checking to detect inconsistencies, and apply their corresponding repairs to restore consistencies. Even if all inconsistencies can be repaired, how to preserve consistencies in the subsequent evolution should be considered. Consistency management (consistency checking and consistency preservation) is a challenging task, especially in the multi-view model-driven software development process. Although there are some efforts to discuss consistency management, most of them lack the support of formal methods. Our work aims to provide a framework for formal consistency management, which may be used in the practical software development process. A formal model, called a Structure model , is first presented for specifying the overall model-based structure of the software system. Next, the definition of consistency is given based on consistency rules. We then investigate consistency preservation under the following two situations. One is that if the initial system is inconsistent, then the consistency can be restored through repairs. The other is that if the initial system is consistent, then the consistency can be maintained through update propagation. To demonstrate the effectiveness of our approach, we finally present a case study with a prototype tool.},
  archive      = {J_IJSEKE},
  author       = {Hao Wen and Jinzhao Wu and Jianmin Jiang and Guofu Tang and Zhong Hong},
  doi          = {10.1142/S0218194023500134},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {733-763},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A formal approach for consistency management in UML models},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). XL-BPMN model-based service similarity measurement
technique. <em>IJSEKE</em>, <em>33</em>(5), 697–732. (<a
href="https://doi.org/10.1142/S0218194023500122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In service-oriented developments, existing studies do not give lots of efforts on a formalized and systematic method for measuring similarities between services for their reuse in business models. This deteriorates the reusability of the constructed service due to the developers’ intuition and informal service analyses. In this paper, we propose a technique for measuring similarity of services by analyzing syntax and semantics between services in the eXtended Layered business process modeling notation (XL-BPMN) model. First of all, the profiles of the formalized attributes for specifying services are defined, and the criteria for determining service similarities are established. To measure similarity between services, a technique both a syntactic similarity analysis facilitated by the XL-BPMN model-based edge counting method and a semantic similarity analysis based on meta data registry (MDR)-applied service attributes is specified. To automate analysis, a tool that can support the semantic similarity analysis technique is implemented. An online shopping mall system is investigated and evaluated to verify the effectiveness of the proposed technique. The similarity measurement technique, which is further formalized at upper business levels, can improve the accuracy of service analyses and enhance service reusability by distinguishing services with high similarity levels as common services.},
  archive      = {J_IJSEKE},
  author       = {Cheeyang Song and Eunsook Cho},
  doi          = {10.1142/S0218194023500122},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {697-732},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {XL-BPMN model-based service similarity measurement technique},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deriving thresholds of object-oriented metrics to predict
defect-proneness of classes: A large-scale meta-analysis.
<em>IJSEKE</em>, <em>33</em>(5), 651–695. (<a
href="https://doi.org/10.1142/S0218194023500110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies have explored the methods of deriving thresholds of object-oriented (i.e. OO) metrics. Unsupervised methods are mainly based on the distributions of metric values, while supervised methods principally rest on the relationships between metric values and defect-proneness of classes. The objective of this study is to empirically examine whether there are effective threshold values of OO metrics by analyzing existing threshold derivation methods with a large-scale meta-analysis. Based on five representative threshold derivation methods (i.e. VARL, ROC, BPP, MFM, and MGM) and 3268 releases from 65 Java projects, we first employ statistical meta-analysis and sensitivity analysis techniques to derive thresholds for 62 OO metrics on the training data. Then, we investigate the predictive performance of five candidate thresholds for each metric on the validation data to explore which of these candidate thresholds can be served as the threshold. Finally, we evaluate their predictive performance on the test data. The experimental results show that 26 of 62 metrics have the threshold effect and the derived thresholds by meta-analysis achieve promising results of GM values and significantly outperform almost all five representative (baseline) thresholds.},
  archive      = {J_IJSEKE},
  author       = {Yuanqing Mei and Yi Rong and Shiran Liu and Zhaoqiang Guo and Yibiao Yang and Hongmin Lu and Yutian Tang and Yuming Zhou},
  doi          = {10.1142/S0218194023500110},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {651-695},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Deriving thresholds of object-oriented metrics to predict defect-proneness of classes: A large-scale meta-analysis},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SQL#: A language for maintainable and debuggable database
queries. <em>IJSEKE</em>, <em>33</em>(5), 619–649. (<a
href="https://doi.org/10.1142/S0218194023500109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured Query Language (SQL) is the dominant language for managing relational databases. However, complex SQL queries are hard to write and maintain because of the intricate inter-table and inter-column relations. To this end, we propose a novel query language called SQL#, which allows programmers to construct complex queries module by module and explicitly specify the relations between different modules according to the logical steps of constructing queries. Besides, we design a SQL#-based system, aiming to facilitate the maintenance of SQL# queries. Specifically, the system renders a SQL# program into a hierarchical graph, which could help programmers understand the high-level structures of SQL# programs and the intricate relations between different components within SQL# programs. In addition, the system can ease the generation of the intermediate tables that correspond to the logical steps of constructing queries, which could help programmers debug complex SQL# queries. Notably, the design of SQL# makes it easy for the system to generate the hierarchical graph and the intermediate tables. Controlled experiments suggest that the SQL#-based system reduces the durations of writing and understanding database queries by 79% and 39%, respectively, compared to raw SQL code.},
  archive      = {J_IJSEKE},
  author       = {Yamin Hu and Hao Jiang and Hanlin Tang and Xin Lin and Zongyao Hu},
  doi          = {10.1142/S0218194023500109},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {619-649},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {SQL#: A language for maintainable and debuggable database queries},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A proposed framework for student’s skills-driven
personalization of cloud-based course content. <em>IJSEKE</em>,
<em>33</em>(4), 603–617. (<a
href="https://doi.org/10.1142/S0218194023500092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Engaging students’ personalized data in the aspects of education has been on focus by different researchers. This paper considers it vital for exploring the student’s progress, moreover, it could predict the student’s level which consequently leads to identifying the required student material to raise his current education level. Although the topic has been vital before the COVID-19 pandemic, however, the importance of the topic has increased exponentially ever since. The research supports the decision-makers in educational institutions as considering personalized data for the student’s educational tasks and activities proved the positive impact of raising the student level. The paper proposes a framework that considers the students’ personal data in predicting their learning skills as well as their educational level. The research included engaging five well-known clustering algorithms, one of the most successful classification algorithms, and a set of 10 features selection techniques. The research applied two main experiment phases, the first phase focused on predicting the students’ learning skills, and the second focused on predicting the students’ level. Two datasets are involved in the experiments and their sources are mentioned. The research revealed the success of the clustering and prediction tasks by applying the selected techniques to the datasets. The research concluded that the highest clustering algorithm accuracy is enhanced k-means (EKM) and the highest contributing features selection method is the evolutionary computation method.},
  archive      = {J_IJSEKE},
  author       = {Alaa A. Qaffas and Ibraheem Alharbi and Amira M. Idrees and Sherif A. Kholeif},
  doi          = {10.1142/S0218194023500092},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {603-617},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A proposed framework for student’s skills-driven personalization of cloud-based course content},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying notable tuples in multi-concept web tables.
<em>IJSEKE</em>, <em>33</em>(4), 575–602. (<a
href="https://doi.org/10.1142/S0218194023500080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying notable tuples in a web table is of great help for table understanding and table summarization. However, existing document-internal feature-based methods are inappropriate for identifying notable tuples in web tables. Additionally, for the web table describing multiple concepts, the notability evaluation of a tuple needs to take into account multiple entities as well as their importance in this tuple. In this paper, we investigate the task of identifying notable tuples in a multi-concept web table and propose a framework that includes three tasks: (1) identify multiple entity columns and their importance weights by building a column correlation graph based on types and relationships in the table; (2) obtain fine-grained entity notability scores based on entity link graph and provide solution for entity link failure and entity domain neglection; and (3) evaluate tuple notability by a weighted sum of notability scores of all entities in the tuple. Comprehensive evaluation of our approach is based on real-world web tables. The results demonstrate that our approach outperforms the state-of-the-art baselines by 4.6% on the precision of detecting multiple entity columns and by 12.5% on the metric normalized discounted cumulative gain (NDCG) of evaluating entity notability.},
  archive      = {J_IJSEKE},
  author       = {Yihai Xi and Ning Wang},
  doi          = {10.1142/S0218194023500080},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {575-602},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Identifying notable tuples in multi-concept web tables},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A hierarchical feature ensemble deep learning approach for
software defect prediction. <em>IJSEKE</em>, <em>33</em>(4), 543–573.
(<a href="https://doi.org/10.1142/S0218194023500079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction can detect modules that may have defects in advance and optimize resource allocation to improve test efficiency and reduce development costs. Traditional features cannot capture deep semantic and grammatical information, which limits the further development of software defect prediction. Therefore, it has gradually become a trend to use deep learning technology to automatically learn valuable deep features from source code or relevant data. However, most software defect prediction methods based on deep learning extraction features from a single information source or only use a single deep learning model, which leads to the fact that the extracted features are not comprehensive enough to affect the final prediction performance. In view of this, this paper proposes a Hierarchical Feature Ensemble Deep Learning (HFEDL) Approach for software defect prediction. Firstly, the HFEDL approach needs to obtain three types of information sources: abstract syntax tree (AST), class dependency network (CDN) and traditional features. Then, the Convolutional Neural Network (CNN) and the Bidirectional Long Short-Term Memory based on Attention mechanism (BiLSTM+Attention) are used to extract different valuable features from the three information sources and multiple prediction sub-models are constructed. Next, all the extracted features are fused by a filter mechanism to obtain more comprehensive features and construct a fusion prediction sub-model. Finally, all the sub-models are integrated by an ensemble learning method to obtain the final prediction model. We use 11 projects in the PROMISE defect repository and evaluate our approach in both non-effort-aware and effort-aware scenarios. The experimental results show that the prediction performance of our approach is superior to state-of-the-art methods in both scenarios.},
  archive      = {J_IJSEKE},
  author       = {Shenggang Zhang and Shujuan Jiang and Yue Yan},
  doi          = {10.1142/S0218194023500079},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {543-573},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A hierarchical feature ensemble deep learning approach for software defect prediction},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the succinctness of modal μ-calculus based on
covariant–contravariant refinement. <em>IJSEKE</em>, <em>33</em>(4),
513–542. (<a href="https://doi.org/10.1142/S0218194023500067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expressive power of logics is one of the major research topics in mathematical logic and computer science. One way of comparing the complexities of different formalisms (e.g. knowledge representation formalisms) stems from the perspective of representational succinctness. The concept of covariant–contravariant refinement (CC-refinement, for short) generalizes the concepts of refinement, simulation and bisimulation. We introduce an extension of the standard multi-agent modal μ -calculus system K μ with CC-refinement operators ( CCRML μ ) and show that CCRML μ is equivalently expressive to K μ . This paper presents the succinctness results of CCRML μ compared with K μ from two viewpoints based on the sets of covariant and contravariant actions. We establish a family of CC-refinement formulas for comparing the succinctness based on the well-known parity symmetric alternating automata, which is often used to describe modal μ -calculus formulas.},
  archive      = {J_IJSEKE},
  author       = {Huili Xing},
  doi          = {10.1142/S0218194023500067},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {513-542},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {On the succinctness of modal μ-calculus based on Covariant–Contravariant refinement},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Person event detection method in computer discipline domain
based on BiGRU and CNN in series. <em>IJSEKE</em>, <em>33</em>(4),
487–512. (<a href="https://doi.org/10.1142/S0218194023500055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knowledge graph of computer discipline domain plays a critical role in computer education, and the person event is an important part of the discipline knowledge graph. Adding person events to the graph will make the discipline knowledge graph richer and more interesting, and enhance enthusiasm of students for learning. The most crucial step in building the person event knowledge graph is the extraction of trigger words. Therefore, this paper proposes a method based on the serial fusion of gated recurrent neural network and convolutional neural network (SC-BiGRU-CNN) for person event detection in the computer discipline domain. We extract the global features of the text from the person event sentences through the BiGRU model, and input the extracted global features into the CNN model to further extract the fine-grained features of the text. And then the extracted features are used to classify the event trigger words. In addition, a dataset (CD-PED) for person event detection in the computer discipline domain is constructed to obtain trigger words and their types. We perform experiments on the public dataset MAVEN and the domain dataset CD-PED, respectively. The experimental results show that our approach has significantly improved the F 1 value compared with the baseline model on the domain dataset CD-PED.},
  archive      = {J_IJSEKE},
  author       = {Xiaoming Zhang and Xin Yang and Huiyong Wang},
  doi          = {10.1142/S0218194023500055},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {487-512},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Person event detection method in computer discipline domain based on BiGRU and CNN in series},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Three approaches for detecting direct output cheating in
program online judge systems. <em>IJSEKE</em>, <em>33</em>(4), 461–486.
(<a href="https://doi.org/10.1142/S0218194023500043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Program online judge (POJ) systems allow students to view questions, submit solution code, and receive scores automatically via the web. Most POJs use test cases for scoring. When a POJ is scored by test case pass rate or a problem that has only one test case, students can usually score by providing the direct output of the test cases (direct output cheating). Currently, there is only one work on detecting such cheating. However, its precision is very low. To solve this problem, three novel approaches are proposed to detect direct output cheating: (i) Line Statistics, which computes the proportion of output calls against other statements; (ii) the control flow graph (CFG) Search computes the maximum similarity between the CFG of a program and that of known samples; (iii) abstract syntax tree (AST) Search identifies cheating by matching rules that are summarized from ASTs of previously detected cheating attempts. A student’s code is marked as cheating if the similarity exceeds a predefined threshold; and a program is detected as cheating if the proportion exceeds a predefined threshold. The proposed approaches and three well-known code plagiarism detection tools (JPlag, Sherlock, and SIM) were evaluated using 100,000 submissions for 1153 problems from a POJ based on the C programming language. The F1 scores of these approaches were determined as 0.9752 (AST Search), 0.9440 (CFG Search), 0.7405 (Line Statistics), 0.6446 (JPlag), 0.1587 (Sherlock), and 0.0076 (SIM), respectively. The result indicates that (i) AST Search is most suitable for the detection of direct output cheating; (ii) traditional code search or plagiarism detection methods based on similarity calculations are not effective for complex cheat detection because these cheats are highly similar to normal code.},
  archive      = {J_IJSEKE},
  author       = {Jing Qiu and Chunmei Shi and Yuehua Lv},
  doi          = {10.1142/S0218194023500043},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {461-486},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Three approaches for detecting direct output cheating in program online judge systems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aligning software architecture training with software
industry requirements. <em>IJSEKE</em>, <em>33</em>(3), 435–460. (<a
href="https://doi.org/10.1142/S0218194023500031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The activities of software design, documenting, and evaluating the structure of software systems, referred to as Software Architecture, have been increasingly getting significant attention in industries. This situation is because of the explicit and prominent role assigned to quality attributes while developing software systems. By considering the high relevance of Software Architecture to industry, many academic institutes have introduced a course on Software Architecture as a part of undergraduate programs in Software Engineering. However, teachers offering this course face numerous challenges. Some of these challenges stem from how software architecture is practised in industries and others from teaching them in an academic setting. This paper describes an experience of designing a software architecture course that aligns the competencies expected from professional software architects with teaching practices imparting those competencies. Such an alignment is necessary to improve the employability of graduate students, make their progression from academic institutes to industries an effortless one, and for better learning outcomes. In the absence of such alignment, fresh graduates need to re-train, leading to training costs and delayed recruitment of fresh graduates by their prospective employers. The experience reports recurring challenges observed by earlier researchers, strategies to address them and our experience in implementing those strategies. The teaching strategies suggested are potentially helpful and practical, especially to less-experienced instructors teaching a course on Software Architecture.},
  archive      = {J_IJSEKE},
  author       = {Wilson Libardo Pantoja Yepez and Julio Ariel Hurtado Alegria and Arvind Kiweleker},
  doi          = {10.1142/S0218194023500031},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {435-460},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Aligning software architecture training with software industry requirements},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated priority prediction for bug reports using comment
intensiveness features and SMOTE data balancing. <em>IJSEKE</em>,
<em>33</em>(3), 415–433. (<a
href="https://doi.org/10.1142/S021819402350002X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The processing priorities for software bug reports are important for software maintenance. Predicting the priorities for bug reports is the subject of many software engineering studies. This study proposes a priority prediction method that uses comment intensiveness features and a Synthetic Minority Over-sampling Technique (SMOTE)-based data balancing scheme. Experiments use datasets for three open-source projects: Eclipse, Mozilla and OpenOffice. The effectiveness of the proposed approach is determined using five classification models: Multinomial Naïve Bayes, Support Vector Machines, Random Forest, Extra Trees and eXtreme Gradient Boosting. The results show that the CIS-SMOTE-based models achieve 0.6078 Precision, 0.4927 Recall, 0.4465 F1-score and 0.7836 Accuracy in priority perdition. The results also show that CIS-SMOTE-RF, CIS-SMOTE-ET and CIS-SMOTE-XGB outperform two advanced priority prediction approaches, eApp and cPur, in terms of all performance measures.},
  archive      = {J_IJSEKE},
  author       = {Anh-Hien Dao and Cheng-Zen Yang},
  doi          = {10.1142/S021819402350002X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {415-433},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Automated priority prediction for bug reports using comment intensiveness features and SMOTE data balancing},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating multi-exit BERT inference via curriculum
learning and knowledge distillation. <em>IJSEKE</em>, <em>33</em>(3),
395–413. (<a href="https://doi.org/10.1142/S0218194023500018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time deployment of bidirectional encoder representations from transformers (BERT) is limited by its slow inference caused by its large number of parameters. Recently, multi-exit architecture has garnered scholarly attention for its ability to achieve a trade-off between performance and efficiency. However, its early exits suffer from a considerable performance reduction compared to the final classifier. To accelerate inference with minimal compensation of performance, we propose a novel training paradigm for multi-exit BERT performing at two levels: training samples and intermediate features. Specifically, for the training samples level, we leverage curriculum learning to guide the training process and improve the generalization capacity of the model. For the intermediate features level, we employ layer-wise distillation learning from shallow to deep layers to resolve the performance deterioration of early exits. The experimental results obtained on the benchmark datasets of textual entailment and answer selection demonstrate that the proposed training paradigm is effective and achieves state-of-the-art results. Furthermore, the layer-wise distillation can completely replace vanilla distillation and deliver superior performance on text entailment datasets.},
  archive      = {J_IJSEKE},
  author       = {Shengwei Gu and Xiangfeng Luo and Xinzhi Wang and Yike Guo},
  doi          = {10.1142/S0218194023500018},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {395-413},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Accelerating multi-exit BERT inference via curriculum learning and knowledge distillation},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Which exceptions do we have to catch in the python code for
AI projects? <em>IJSEKE</em>, <em>33</em>(3), 375–394. (<a
href="https://doi.org/10.1142/S0218194022500814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Python is the most-widely used language in artificial intelligence (AI) projects requiring huge amount of CPU and memory resources, and long execution time for training. For saving the project duration and making AI software systems more reliable, it is inevitable to handle exceptions appropriately at the code level. However, handling exceptions highly relies on developer’s experience. This is because, as an interpreter-based programming language, it does not force a developer to catch exceptions during development. In order to resolve this issue, we propose an approach to suggesting appropriate exceptions for the AI code segments during development after training exceptions from the existing handling statements in the AI projects. This approach learns the appropriate token units for the exception code and pretrains the embedding model to capture the semantic features of the code. Additionally, the attention mechanism learns to catch the salient features of the exception code. For evaluating our approach, we collected 32,771 AI projects using two popular AI frameworks (i.e. Pytorch and Tensorflow) and we obtained the 0.94 of Area under the Precision-Recall Curve (AUPRC) on average. Experimental results show that the proposed method can support the developer’s exception handling with better exception proposal performance than the compared models.},
  archive      = {J_IJSEKE},
  author       = {Mingu Kang and Suntae Kim and Duksan Ryu and Jaehyuk Cho},
  doi          = {10.1142/S0218194022500814},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {375-394},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Which exceptions do we have to catch in the python code for AI projects?},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A hybrid multiple models transfer approach for
cross-project software defect prediction. <em>IJSEKE</em>,
<em>33</em>(3), 343–374. (<a
href="https://doi.org/10.1142/S0218194022500784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a new project, it is impossible to get a reliable prediction model because of the lack of sufficient training data. To solve the problem, researchers proposed cross-project defect prediction (CPDP). For CPDP, most researchers focus on how to reduce the distribution difference between training data and test data, and ignore the impact of class imbalance on prediction performance. This paper proposes a hybrid multiple models transfer approach (HMMTA) for cross-project software defect prediction. First, several instances that are most similar to each target project instance are selected from all source projects to form the training data. Second, the same number of instances as that of the defected class are randomly selected from all the non-defect class in each iteration. Next, instances selected from the non-defect classes and all defected class instances are combined to form the training data. Third, the transfer learning method called ETrAdaBoost is used to iteratively construct multiple prediction models. Finally, the prediction models obtained from multiple iterations are integrated by the ensemble learning method to obtain the final prediction model. We evaluate our approach on 53 projects from AEEEM, PROMISE, SOFTLAB and ReLink four defect repositories, and compare it with 10 baseline CPDP approaches. The experimental results show that the prediction performance of our approach significantly outperforms the state-of-the-art CPDP methods. Besides, we also find that our approach has the comparable prediction performance as within-project defect prediction (WPDP) approaches. These experimental results demonstrate the effectiveness of HMMTA approach for CPDP.},
  archive      = {J_IJSEKE},
  author       = {Shenggang Zhang and Shujuan Jiang and Yue Yan},
  doi          = {10.1142/S0218194022500784},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {343-374},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A hybrid multiple models transfer approach for cross-project software defect prediction},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Use of journey maps and personas in software requirements
elicitation. <em>IJSEKE</em>, <em>33</em>(3), 313–342. (<a
href="https://doi.org/10.1142/S0218194023300014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Requirements elicitation is a fundamental step in a software development process since it is at this stage that the software begins to be designed. In some situations, the problems related to the failure of the software development project are due to an incomplete requirements elicitation, resulting in solutions that do not understand all the necessary functionalities or do not incorporate innovation. Despite the various techniques offered by Requirements Engineering, situations such as the growing application market and the need for innovation further increase the importance of understanding the user’s different needs. In this paper, we investigated how the journey map and personas techniques are being used in requirements elicitation in both the literature and the industry, along with the advantages, disadvantages and challenges of using these techniques. We conducted systematic literature review to identify the personas and journey map techniques used in requirements elicitation in the literature and industry. In addition, we conducted a survey with 52 practitioners (software developers, users and managers) to investigate their perceptions of the use of journey maps and personas techniques in the requirements elicitation phase. Twenty-four primary studies were identified that address journey map and personas techniques in software requirements elicitation. In addition, most respondents stated that using these techniques facilitates understanding the requirements, providing better integration, collaboration and leveling of knowledge among the members of the software development teams. Our findings allow us to conclude that most of the software developers, users and managers that participated in the survey consider that the journey map and personas techniques are effective in helping understand the software requirements to be developed by the development teams.},
  archive      = {J_IJSEKE},
  author       = {Edna Dias Canedo and Angelica Toffano Seidel Calazans and Geovana Ramos Sousa Silva and Pedro Henrique Teixeira Costa and Eloisa Toffano Seidel Masson},
  doi          = {10.1142/S0218194023300014},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {313-342},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Use of journey maps and personas in software requirements elicitation},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparative study of ensemble techniques based on genetic
programming: A case study in semantic similarity assessment.
<em>IJSEKE</em>, <em>33</em>(2), 289–312. (<a
href="https://doi.org/10.1142/S0218194022500772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of assessing semantic similarity between pieces of text through computers has attracted considerable attention from industry and academia. New advances in neural computation have developed very sophisticated concepts, establishing a new state of the art in this respect. In this paper, we go one step further by proposing new techniques built on the existing methods. To do so, we bring to the table the stacking concept that has given such good results and propose a new architecture for ensemble learning based on genetic programming. As there are several possible variants, we compare them all and try to establish which one is the most appropriate to achieve successful results in this context. Analysis of the experiments indicates that Cartesian Genetic Programming seems to give better average results.},
  archive      = {J_IJSEKE},
  author       = {Jorge Martinez-Gil},
  doi          = {10.1142/S0218194022500772},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {289-312},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A comparative study of ensemble techniques based on genetic programming: A case study in semantic similarity assessment},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Matching logic for concurrent programs based on
rely/guarantee and abstract patterns. <em>IJSEKE</em>, <em>33</em>(2),
257–288. (<a href="https://doi.org/10.1142/S0218194022500759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper combines rely/guarantee, abstract patterns and matching logic to reason about concurrent programs in a modular and compositional manner. According to the separation property, the state can be divided into two disjoint parts, the local state and the shared state. We use matching logic to deal with the local state, and use rely/guarantee and abstract patterns to deal with the shared state. The power of rely/guarantee is to describe interference between concurrent programs. The advantage of abstract patterns is supporting fictional separation, which indicates that we logically consider abstract patterns to represent disjoint elements, although these elements are not disjoint under a certain implementation. By combining the advantages of rely/guarantee, abstract patterns and matching logic, our approach realize that clients of the module can be verified completely according to the specification of the module, regardless of the implementation of the module. In addition, we use several examples to illustrate our approach, define our logic judgments, and prove the soundness of our logic.},
  archive      = {J_IJSEKE},
  author       = {ShangBei Wang and WeiYu Dong},
  doi          = {10.1142/S0218194022500759},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {257-288},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Matching logic for concurrent programs based on Rely/Guarantee and abstract patterns},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A quality-driven iterative evolution approach for software
architecture. <em>IJSEKE</em>, <em>33</em>(2), 231–255. (<a
href="https://doi.org/10.1142/S0218194022500747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality attributes of software architecture (SA) determine whether SA can be easily understood, tested, modified and so on, so quality-driven architecture evolution is important for keeping the viability and competitiveness. SA evolution is a process, and it contains multiple steps, such as SA quality measurement, SA modification, code co-evolution and so on. In order to guarantee that the software can be continuously improved and iteratively evolved in the future, we need to focus on all steps. However, most existing approaches only focus on one aspect, so they did not pay attention to how to finish the evolution process. In this paper, we propose a quality-driven iterative evolution approach for SA. This approach focuses on the whole process. In the first step, we use a quantitative approach to measure the architecture quality. Then, we construct the conflict graph to detect conflicts between evolution requirements to generate the final evolution scheme. In the third step, we modify architecture based on the evolution scheme. Finally, we co-evolve file dependency graph (FDG) based on the modified architecture. By focusing on the above steps, our approach can support a complete quality-driven architecture evolution process and obtain the maximum benefit in terms of the combined SA quality. We conduct our experiments with four open source projects, the experimental results indicate that our approach can improve SA quality, and our approach can effectively co-evolve the FDG to lay the foundation for the next evolution.},
  archive      = {J_IJSEKE},
  author       = {Tong Wang and Bixin Li and Lingyuan Zhu},
  doi          = {10.1142/S0218194022500747},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {231-255},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A quality-driven iterative evolution approach for software architecture},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep tasks summarization for comprehending mixed tasks in a
commit. <em>IJSEKE</em>, <em>33</em>(2), 207–229. (<a
href="https://doi.org/10.1142/S0218194022500711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Version Control System (VCS), a developer frequently uploads multiple tasks such as adding features, code refactoring, and fixing bugs, into a single commit and crumbles each task’s summary when writing a commit message. It causes code readers to feel challenged in understanding the developer’s past tasks within the commit history. To resolve this issue, we propose an automatic approach to generating a task summary to help comprehend multiple mixed tasks in a commit and developed tool support named T ask s ummary Gen erator ( TsGen ). In our approach, we use the commit with a single task as input and identify the task to sort its elements sequentially. Then we generate feature vectors from each sorted element to train the Neural Machine Translation (NMT) model. Based on the trained NMT model, we generate the feature vector from each task of a commit with multiple tasks and put each of them into the model to provide the task summary. In evaluation, we compared the performance of TsGen with two existing methods for nine open-source projects. As a result, TsGen outperformed CoDiSum and Jiang’s NMT by 52.08% and 28.07% in BiLingual Evaluation Understudy (BLEU) scores. In addition, the human evaluation was carried out to demonstrate that TsGen helps understand mixed tasks in a commit and gained a 0.27 higher preference than the actual commit message.},
  archive      = {J_IJSEKE},
  author       = {Taeyoung Kim and Suntae Kim and Duksan Ryu and Jaehyuk Cho},
  doi          = {10.1142/S0218194022500711},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {207-229},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Deep tasks summarization for comprehending mixed tasks in a commit},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hierarchical model for quality evaluation of mixed source
software based on ISO/IEC 25010. <em>IJSEKE</em>, <em>33</em>(2),
181–205. (<a href="https://doi.org/10.1142/S021819402250070X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of mixed source software, the existing quality models are not able to better assess the community quality and autonomy controllability of mixed source software. To fill this gap, we propose a hierarchical model in this paper for quality assessment of mixed source software. In our model, the new attributes are proposed to meet the quality requirements of mixed source software based on the ISO/IEC 25010 standards, and a set of metrics are designed for the new attributes. The model evaluates the quality of mixed source software through quality attributes that have been quantified by the metrics. Applying our quality model to some mixed source software and comparing the model results with the actual situation, we verify whether our proposed two quality attributes, community intensity and autonomy controllability, can effectively assess the quality of mixed source software. The results of the experiments show that our model is indeed effective in assessing the quality of mixed source software. An important feature of our model is that the model has good flexibility, and the set of quality attributes and metrics can be adjusted freely, which provides a flexible and feasible way for various software quality assessment requirements.},
  archive      = {J_IJSEKE},
  author       = {Chunguang Zhang and Bixin Li and Lulu Wang and Haixin Xu and Tao Shao},
  doi          = {10.1142/S021819402250070X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {181-205},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A hierarchical model for quality evaluation of mixed source software based on ISO/IEC 25010},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From SATD recognition to an interpretation method based on
the dataset. <em>IJSEKE</em>, <em>33</em>(2), 157–180. (<a
href="https://doi.org/10.1142/S0218194022500693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technical debt describes a trade-off between short-term goals and long-term code quality during software development. Self-admitted technical debt (SATD), a type of technical debt, is intentionally introduced by developers. The existence of SATD is likely to leave hidden dangers for future changes in software systems, so identifying SATD is an essential task. Before this, many methods for recognizing SATD (such as pattern matching-based, natural language processing-based, text mining-based, etc.) have been proposed. This paper will present a pre-trained deep learning model to complete the SATD recognition task. An efficient deep learning model interpretation tool Captum can be used to understand the experimental results. At the same time, a new interpretation view is proposed for the matching-based model. Finally, combined with the research in this paper, reasonable suggestions are put forward for future SATD recognition tasks.},
  archive      = {J_IJSEKE},
  author       = {Yuan Meng and Bao Tie and Dawei Lin},
  doi          = {10.1142/S0218194022500693},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {157-180},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {From SATD recognition to an interpretation method based on the dataset},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic extraction of ontological explanation for machine
learning-based systems. <em>IJSEKE</em>, <em>33</em>(1), 133–156. (<a
href="https://doi.org/10.1142/S0218194022500802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has been implemented as a part of many software systems to support data-driven decisions and recommendations. The prominent machine learning technique is the artificial neural network, which lacks the explanation of how it produces the output. However, many application domains require algorithmic decision making to be transparent so explainability in these systems has been an important challenge. This paper proposes an automated framework that elicits the contributing rules describing how the neural network model makes decisions. The explainability of contributing rules can be measured and it is able to address issues in the training dataset. With the ontology representation of contributing rules, an individual decision can be automatically explained through ontology reasoning. We have developed a tool that supports applying our framework in practice. The evaluation has been conducted to assess the effectiveness of our framework using open datasets from different domains. The results prove that our framework performs well to explain the neural network models, as it can achieve the average accuracy of 81% to explain the subject models. Also, our framework takes significantly less time to process than the other technique.},
  archive      = {J_IJSEKE},
  author       = {Nacha Chondamrongkul and Punnarumol Temdee},
  doi          = {10.1142/S0218194022500802},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {133-156},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Automatic extraction of ontological explanation for machine learning-based systems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personalized learning path recommendation for e-learning
based on knowledge graph and graph convolutional network.
<em>IJSEKE</em>, <em>33</em>(1), 109–131. (<a
href="https://doi.org/10.1142/S0218194022500681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In e-learning, the increasing number of learning resources makes it difficult for learners to find suitable learning resources. In addition, learners may have different preferences and cognitive abilities for learning resources, where differences in learners’ cognitive abilities will lead to different importance of learning resources. Therefore, recommending personalized learning paths for learners has become a research hotspot. Considering learners’ preferences and the importance of learning resources, this paper proposes a learning path recommendation algorithm based on knowledge graph. We construct a multi-dimensional courses knowledge graph in computer field (MCCKG), and then propose a method based on graph convolutional network for modeling high-order correlations on the knowledge graph to more accurately capture learners’ preferences. Furthermore, the importance of learning resources is calculated by using the characteristics of learning resources in the MCCKG and learners’ characteristics. Finally, by weighting the two factors of learners’ preferences and the importance of learning resources, we recommend the optimal learning path for learners. Our method is evaluated from the aspects of learner’s satisfaction, algorithm effectiveness, etc. The experimental results show that the method proposed in this paper can recommend a personalized learning path to satisfy the needs of learners, thus reducing the workload of manually planning learning paths.},
  archive      = {J_IJSEKE},
  author       = {Xiaoming Zhang and Shan Liu and Huiyong Wang},
  doi          = {10.1142/S0218194022500681},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {109-131},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Personalized learning path recommendation for E-learning based on knowledge graph and graph convolutional network},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The effect of environmental metrics on software fault
prediction. <em>IJSEKE</em>, <em>33</em>(1), 85–108. (<a
href="https://doi.org/10.1142/S021819402250067X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, besides the software metrics, the environmental metrics such as experience of software engineer, similar project experience, size of the project, programming language, time spent on analysis and development are also explored to see whether they also affect the results of software fault prediction and what would be the success rates. The dataset for this study was generated from combining various data from 10 projects. A total of 36 metrics and 6676 test cases were evaluated. The errors occurred in the test cases are not just considered as an error, their priority and cases that cannot be tested are also taken into consideration. Nine fault levels are employed in models. Models are created with four different algorithms which have achieved a success rate of; 76% by the decision tree algorithm, 94% by the nearest neighbors algorithm, 90% by the random forests algorithm and 73% by the Adaboost Classifier Algorithm. It was observed that environmental metrics are indeed effective in software fault prediction and when applied with machine learning algorithms a high rate of success can be achieved.},
  archive      = {J_IJSEKE},
  author       = {Merve Odabasi and Ensar Gul},
  doi          = {10.1142/S021819402250067X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {85-108},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {The effect of environmental metrics on software fault prediction},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Matching logic based on ownership transfer. <em>IJSEKE</em>,
<em>33</em>(1), 55–84. (<a
href="https://doi.org/10.1142/S0218194022500668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We combine “ownership transfer” with matching logic to reason about fault-free partial correctness of shared-memory concurrent programs. As we all know, what really gives separation logic (concurrent separation logic) an edge is the ownership transfer of the heap. Inspired by this, we use matching logic to realize variable ownership (permission) and its transfer mechanism, which reveals the hidden principle behind “protected variables” of resource and “rely set” in extended CSL. In addition, variable ownership can replace Dijkstra’s semaphore blocking technique to achieve the critical section. Soundness is important to us, we provide a semantic model that supports the separation property and demonstrate the soundness of our logic based on trace semantics.},
  archive      = {J_IJSEKE},
  author       = {Shangbei Wang and Yintong Wang},
  doi          = {10.1142/S0218194022500668},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {55-84},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Matching logic based on ownership transfer},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling of security fault-tolerant requirements for secure
systems. <em>IJSEKE</em>, <em>33</em>(1), 23–53. (<a
href="https://doi.org/10.1142/S0218194022500644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security services can keep a system from security breaches for a while, but they are ultimately compromised as the system is deployed and used. This paper describes the modeling of security fault-tolerant (SFT) requirements, which can tolerate the failures of security services for systems. SFT requirements are specified together with the security services requirements so that they tolerate breaches of the security services. This paper addresses an approach for specifying and analyzing SFT requirements using a meta-model. Threats to systems are identified in the requirements specification and analysis phases, and SFT measures against the threats are described with security services. An electronic commerce system is selected to illustrate the approach.},
  archive      = {J_IJSEKE},
  author       = {Don Pathirage and Michael Shin and Dongsoo Jang},
  doi          = {10.1142/S0218194022500644},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {23-53},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Modeling of security fault-tolerant requirements for secure systems},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extension of a simulation software to incorporate
quality-related factors in investigations on software engineering
economics. <em>IJSEKE</em>, <em>33</em>(1), 1–21. (<a
href="https://doi.org/10.1142/S0218194022500553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The economics of software engineering [B. W. Boehm, Software Engineering Economics , Prentice-Hall, Englewood Cliffs, New Jersey, 1981] are strongly influenced by the way how software development processes are performed. Simulations give valuable insights in the performance of these processes. In order to research the economics of software engineering, a process analysis studio (PAS) was developed and presented [D. Kuhlen and A. Speck, Business process analysis by model checking, in Proc. 5th Int. Symp. Data-Driven Process Discovery and Analysis SIMPDA , eds. P. Ceravolo and S. Rinderle-Ma, 2015; D. Kuhlen and A. Speck, The way of designing a simulation software in order to evaluate the economic performance in software development, in ICCMS ’17 Proc. 8th Int. Conf. Computer Modeling and Simulation , Int. Conf. Proc. Series by ACM , 2016; D. Kuhlen, Kostensenkungspotenzialfunktion der Softwareproduktion, dissertation, Christian-Albrechts-Universität zu Kiel, 2019]. This simulation software was built to analyze the capacitive configurations of the software production. It helps to assess the impact of different configurations on the process output. However, this research emphasizes the impact of quality-related factors (QRFs) on the process output. This paper presents a proposal how to extend the simulation software model in order to facilitate qualitative analysis. In order to facilitate the understanding of the model and the evaluation of the contribution, offered by the proposal, Prolog codes are used.},
  archive      = {J_IJSEKE},
  author       = {David Kuhlen and Andreas Speck},
  doi          = {10.1142/S0218194022500553},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Extension of a simulation software to incorporate quality-related factors in investigations on software engineering economics},
  volume       = {33},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
