<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FDATA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="fdata---130">FDATA - 130</h2>
<ul>
<li><details>
<summary>
(2023a). Corrigendum: Towards an understanding of global brain data
governance: Ethical positions that underpin global brain data governance
discourse. <em>FDATA</em>, <em>6</em>, 1344345. (<a
href="https://doi.org/10.3389/fdata.2023.1344345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Ochang, Paschal and Eke, Damian and Stahl, Bernd Carsten},
  doi          = {10.3389/fdata.2023.1344345},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1344345},
  shortjournal = {Front. Big Data},
  title        = {Corrigendum: towards an understanding of global brain data governance: ethical positions that underpin global brain data governance discourse},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Corrigendum: Do you hear the people sing? Comparison of
synchronized URL and narrative themes in 2020 and 2023 french protests.
<em>FDATA</em>, <em>6</em>, 1343108. (<a
href="https://doi.org/10.3389/fdata.2023.1343108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Ng, Lynnette Hui Xian and Carley, Kathleen M.},
  doi          = {10.3389/fdata.2023.1343108},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1343108},
  shortjournal = {Front. Big Data},
  title        = {Corrigendum: Do you hear the people sing? comparison of synchronized URL and narrative themes in 2020 and 2023 french protests},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Corrigendum: Non-invasive detection of anemia using lip
mucosa images transfer learning convolutional neural networks.
<em>FDATA</em>, <em>6</em>, 1338363. (<a
href="https://doi.org/10.3389/fdata.2023.1338363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Mahmud, Shekhar and Mansour, Mohammed and Donmez, Turker Berk and Kutlu, Mustafa and Freeman, Chris},
  doi          = {10.3389/fdata.2023.1338363},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1338363},
  shortjournal = {Front. Big Data},
  title        = {Corrigendum: Non-invasive detection of anemia using lip mucosa images transfer learning convolutional neural networks},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Corrigendum: Anemia detection through non-invasive analysis
of lip mucosa images. <em>FDATA</em>, <em>6</em>, 1335213. (<a
href="https://doi.org/10.3389/fdata.2023.1335213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Mahmud, Shekhar and Donmez, Turker Berk and Mansour, Mohammed and Kutlu, Mustafa and Freeman, Chris},
  doi          = {10.3389/fdata.2023.1335213},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1335213},
  shortjournal = {Front. Big Data},
  title        = {Corrigendum: Anemia detection through non-invasive analysis of lip mucosa images},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Criminal clickbait: A panel data analysis on the
attractiveness of online advertisements offering stolen data.
<em>FDATA</em>, <em>6</em>, 1320569. (<a
href="https://doi.org/10.3389/fdata.2023.1320569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionFew studies have examined the sales of stolen account credentials on darkweb markets. In this study, we tested how advertisement characteristics affect the popularity of illicit online advertisements offering account credentials. Unlike previous criminological research, we take a novel approach by assessing the applicability of knowledge on regular consumer behaviours instead of theories explaining offender behaviour.MethodsWe scraped 1,565 unique advertisements offering credentials on a darkweb market. We used this panel data set to predict the simultaneous effects of the asking price, endorsement cues and title elements on advertisement popularity by estimating several hybrid panel data models.ResultsMost of our findings disconfirm our hypotheses. Asking price did not affect advertisement popularity. Endorsement cues, including vendor reputation and cumulative sales and views, had mixed and negative relationships, respectively, with advertisement popularity.DiscussionOur results might suggest that account credentials are not simply regular products, but high-risk commodities that, paradoxically, become less attractive as they gain popularity. This study highlights the necessity of a deeper understanding of illicit online market dynamics to improve theories on illicit consumer behaviours and assist cybersecurity experts in disrupting criminal business models more effectively. We propose several avenues for future experimental research to gain further insights into these illicit processes.},
  archive      = {J_FDATA},
  author       = {Madarie, Renushka and de Poot, Christianne and Weulen Kranenbarg, Marleen},
  doi          = {10.3389/fdata.2023.1320569},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1320569},
  shortjournal = {Front. Big Data},
  title        = {Criminal clickbait: A panel data analysis on the attractiveness of online advertisements offering stolen data},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond-accuracy: A review on diversity, serendipity, and
fairness in recommender systems based on graph neural networks.
<em>FDATA</em>, <em>6</em>, 1251072. (<a
href="https://doi.org/10.3389/fdata.2023.1251072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By providing personalized suggestions to users, recommender systems have become essential to numerous online platforms. Collaborative filtering, particularly graph-based approaches using Graph Neural Networks (GNNs), have demonstrated great results in terms of recommendation accuracy. However, accuracy may not always be the most important criterion for evaluating recommender systems&#39; performance, since beyond-accuracy aspects such as recommendation diversity, serendipity, and fairness can strongly influence user engagement and satisfaction. This review paper focuses on addressing these dimensions in GNN-based recommender systems, going beyond the conventional accuracy-centric perspective. We begin by reviewing recent developments in approaches that improve not only the accuracy-diversity trade-off but also promote serendipity, and fairness in GNN-based recommender systems. We discuss different stages of model development including data preprocessing, graph construction, embedding initialization, propagation layers, embedding fusion, score computation, and training methodologies. Furthermore, we present a look into the practical difficulties encountered in assuring diversity, serendipity, and fairness, while retaining high accuracy. Finally, we discuss potential future research directions for developing more robust GNN-based recommender systems that go beyond the unidimensional perspective of focusing solely on accuracy. This review aims to provide researchers and practitioners with an in-depth understanding of the multifaceted issues that arise when designing GNN-based recommender systems, setting our work apart by offering a comprehensive exploration of beyond-accuracy dimensions.},
  archive      = {J_FDATA},
  author       = {Duricic, Tomislav and Kowald, Dominik and Lacic, Emanuel and Lex, Elisabeth},
  doi          = {10.3389/fdata.2023.1251072},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1251072},
  shortjournal = {Front. Big Data},
  title        = {Beyond-accuracy: A review on diversity, serendipity, and fairness in recommender systems based on graph neural networks},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing knowledge discovery from unstructured data using a
deep learning approach to support subsurface modeling predictions.
<em>FDATA</em>, <em>6</em>, 1227189. (<a
href="https://doi.org/10.3389/fdata.2023.1227189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsurface interpretations and models rely on knowledge from subject matter experts who utilize unstructured information from images, maps, cross sections, and other products to provide context to measured data (e. g., cores, well logs, seismic surveys). To enhance such knowledge discovery, we advanced the National Energy Technology Laboratory&#39;s (NETL) Subsurface Trend Analysis (STA) workflow with an artificial intelligence (AI) deep learning approach for image embedding. NETL&#39;s STA method offers a validated science-based approach of combining geologic systems knowledge, statistical modeling, and datasets to improve predictions of subsurface properties. The STA image embedding tool quickly extracts images from unstructured knowledge products like publications, maps, websites, and presentations; categorically labels the images; and creates a repository for geologic domain postulation. Via a case study on geographic and subsurface literature of the Gulf of Mexico (GOM), results show the STA image embedding tool extracts images and correctly labels them with ~90 to ~95% accuracy.},
  archive      = {J_FDATA},
  author       = {Hoover, Brendan and Zaengle, Dakota and Mark-Moser, MacKenzie and Wingo, Patrick and Suhag, Anuj and Rose, Kelly},
  doi          = {10.3389/fdata.2023.1227189},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1227189},
  shortjournal = {Front. Big Data},
  title        = {Enhancing knowledge discovery from unstructured data using a deep learning approach to support subsurface modeling predictions},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Are machine learning, AI, and big data tools
ready to be used for sustainable development? Challenges, and
limitations of current approaches. <em>FDATA</em>, <em>6</em>, 1301903.
(<a href="https://doi.org/10.3389/fdata.2023.1301903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Omodei, Elisa and Kim, Dohyung and Garcia-Herranz, Manuel and Sekara, Vedran},
  doi          = {10.3389/fdata.2023.1301903},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1301903},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Are machine learning, AI, and big data tools ready to be used for sustainable development? challenges, and limitations of current approaches},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The metaverse digital environments: A scoping review of the
challenges, privacy and security issues. <em>FDATA</em>, <em>6</em>,
1301812. (<a href="https://doi.org/10.3389/fdata.2023.1301812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of the “metaverse” has garnered significant attention recently, positioned as the “next frontier” of the internet. This emerging digital realm carries substantial economic and financial implications for both IT and non-IT industries. However, the integration and evolution of these virtual universes bring forth a multitude of intricate issues and quandaries that demand resolution. Within this research endeavor, our objective was to delve into and appraise the array of challenges, privacy concerns, and security issues that have come to light during the development of metaverse virtual environments in the wake of the COVID-19 pandemic. Through a meticulous review and analysis of literature spanning from January 2020 to December 2022, we have meticulously identified and scrutinized 29 distinct challenges, along with 12 policy, privacy, and security matters intertwined with the metaverse. Among the challenges we unearthed, the foremost were concerns pertaining to the costs associated with hardware and software, implementation complexities, digital disparities, and the ethical and moral quandaries surrounding socio-control, collectively cited by 43%, 40%, and 33% of the surveyed articles, respectively. Turning our focus to policy, privacy, and security issues, the top three concerns that emerged from our investigation encompassed the formulation of metaverse rules and principles, the encroachment of privacy threats within the metaverse, and the looming challenges concerning data management, all mentioned in 43%, 40%, and 33% of the examined literature. In summation, the development of virtual environments within the metaverse is a multifaceted and dynamically evolving domain, offering both opportunities and hurdles for researchers and practitioners alike. It is our aspiration that the insights, challenges, and recommendations articulated in this report will catalyze extensive dialogues among industry stakeholders, governmental bodies, and other interested parties concerning the metaverse&#39;s destiny and the world they aim to construct or bequeath to future generations.},
  archive      = {J_FDATA},
  author       = {Tukur, Muhammad and Schneider, Jens and Househ, Mowafa and Dokoro, Ahmed Haruna and Ismail, Usman Idris and Dawaki, Muhammad and Agus, Marco},
  doi          = {10.3389/fdata.2023.1301812},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1301812},
  shortjournal = {Front. Big Data},
  title        = {The metaverse digital environments: A scoping review of the challenges, privacy and security issues},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TEE-graph: Efficient privacy and ownership protection for
cloud-based graph spectral analysis. <em>FDATA</em>, <em>6</em>,
1296469. (<a href="https://doi.org/10.3389/fdata.2023.1296469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionBig graphs like social network user interactions and customer rating matrices require significant computing resources to maintain. Data owners are now using public cloud resources for storage and computing elasticity. However, existing solutions do not fully address the privacy and ownership protection needs of the key involved parties: data contributors and the data owner who collects data from contributors.MethodsWe propose a Trusted Execution Environment (TEE) based solution: TEE-Graph for graph spectral analysis of outsourced graphs in the cloud. TEEs are new CPU features that can enable much more efficient confidential computing solutions than traditional software-based cryptographic ones. Our approach has several unique contributions compared to existing confidential graph analysis approaches. (1) It utilizes the unique TEE properties to ensure contributors&#39; new privacy needs, e.g., the right of revocation for shared data. (2) It implements efficient access-pattern protection with a differentially private data encoding method. And (3) it implements TEE-based special analysis algorithms: the Lanczos method and the Nystrom method for efficiently handling big graphs and protecting confidentiality from compromised cloud providers.ResultsThe TEE-Graph approach is much more efficient than software crypto approaches and also immune to access-pattern-based attacks. Compared with the best-known software crypto approach for graph spectral analysis, PrivateGraph, we have seen that TEE-Graph has 103−105 times lower computation, storage, and communication costs. Furthermore, the proposed access-pattern protection method incurs only about 10%-25% of the overall computation cost.DiscussionOur experimentation showed that TEE-Graph performs significantly better and has lower costs than typical software approaches. It also addresses the unique ownership and access-pattern issues that other TEE-related graph analytics approaches have not sufficiently studied. The proposed approach can be extended to other graph analytics problems with strong ownership and access-pattern protection.},
  archive      = {J_FDATA},
  author       = {Alam, A. K. M. Mubashwir and Chen, Keke},
  doi          = {10.3389/fdata.2023.1296469},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1296469},
  shortjournal = {Front. Big Data},
  title        = {TEE-graph: Efficient privacy and ownership protection for cloud-based graph spectral analysis},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Non-invasive detection of anemia using lip mucosa images
transfer learning convolutional neural networks. <em>FDATA</em>,
<em>6</em>, 1291329. (<a
href="https://doi.org/10.3389/fdata.2023.1291329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anemia is defined as a drop in the number of erythrocytes or hemoglobin concentration below normal levels in healthy people. The increase in paleness of the skin might vary based on the color of the skin, although there is currently no quantifiable measurement. The pallor of the skin is best visible in locations where the cuticle is thin, such as the interior of the mouth, lips, or conjunctiva. This work focuses on anemia-related pallors and their relationship to blood count values and artificial intelligence. In this study, a deep learning approach using transfer learning and Convolutional Neural Networks (CNN) was implemented in which VGG16, Xception, MobileNet, and ResNet50 architectures, were pre-trained to predict anemia using lip mucous images. A total of 138 volunteers (100 women and 38 men) participated in the work to develop the dataset that contains two image classes: healthy and anemic. Image processing was first performed on a single frame with only the mouth area visible, data argumentation was preformed, and then CNN models were applied to classify the dataset lip images. Statistical metrics were employed to discriminate the performance of the models in terms of Accuracy, Precision, Recal, and F1 Score. Among the CNN algorithms used, Xception was found to categorize the lip images with 99.28% accuracy, providing the best results. The other CNN architectures had accuracies of 96.38% for MobileNet, 95.65% for ResNet %, and 92.39% for VGG16. Our findings show that anemia may be diagnosed using deep learning approaches from a single lip image. This data set will be enhanced in the future to allow for real-time classification.},
  archive      = {J_FDATA},
  author       = {Mahmud, Shekhar and Mansour, Mohammed and Donmez, Turker Berk and Kutlu, Mustafa and Freeman, Chris},
  doi          = {10.3389/fdata.2023.1291329},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1291329},
  shortjournal = {Front. Big Data},
  title        = {Non-invasive detection of anemia using lip mucosa images transfer learning convolutional neural networks},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of a data processing method for the farmland
environmental monitoring based on improved spark components.
<em>FDATA</em>, <em>6</em>, 1282352. (<a
href="https://doi.org/10.3389/fdata.2023.1282352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularization of big data technology, agricultural data processing systems have become more intelligent. In this study, a data processing method for farmland environmental monitoring based on improved Spark components is designed. It introduces the FAST-Join (Join critical filtering sampling partition optimization) algorithm in the Spark component for equivalence association query optimization to improve the operating efficiency of the Spark component and cluster. The experimental results show that the amount of data written and read in Shuffle by Spark optimized by the FAST-join algorithm only accounts for 0.958 and 1.384% of the original data volume on average, and the calculation speed is 202.11% faster than the original. The average data processing time and occupied memory size of the Spark cluster are reduced by 128.22 and 76.75% compared with the originals. It also compared the cluster performance of the FAST-join and Equi-join algorithms. The Spark cluster optimized by the FAST-join algorithm reduced the processing time and occupied memory size by an average of 68.74 and 37.80% compared with the Equi-join algorithm, which shows that the FAST-join algorithm can effectively improve the efficiency of inter-data table querying and cluster computing.},
  archive      = {J_FDATA},
  author       = {Tang, Ruipeng and Aridas, Narendra Kumar and Talip, Mohamad Sofian Abu},
  doi          = {10.3389/fdata.2023.1282352},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1282352},
  shortjournal = {Front. Big Data},
  title        = {Design of a data processing method for the farmland environmental monitoring based on improved spark components},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating geometries of ReLU feedforward neural networks.
<em>FDATA</em>, <em>6</em>, 1274831. (<a
href="https://doi.org/10.3389/fdata.2023.1274831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the integration of multiple geometries present within a ReLU-based neural network. A ReLU neural network determines a piecewise affine linear continuous map, M, from an input space ℝm to an output space ℝn. The piecewise behavior corresponds to a polyhedral decomposition of ℝm. Each polyhedron in the decomposition can be labeled with a binary vector (whose length equals the number of ReLU nodes in the network) and with an affine linear function (which agrees with M when restricted to points in the polyhedron). We develop a toolbox that calculates the binary vector for a polyhedra containing a given data point with respect to a given ReLU FFNN. We utilize this binary vector to derive bounding facets for the corresponding polyhedron, extraction of “active” bits within the binary vector, enumeration of neighboring binary vectors, and visualization of the polyhedral decomposition (Python code is available at https://github.com/cglrtrgy/GoL_Toolbox). Polyhedra in the polyhedral decomposition of ℝm are neighbors if they share a facet. Binary vectors for neighboring polyhedra differ in exactly 1 bit. Using the toolbox, we analyze the Hamming distance between the binary vectors for polyhedra containing points from adversarial/nonadversarial datasets revealing distinct geometric properties. A bisection method is employed to identify sample points with a Hamming distance of 1 along the shortest Euclidean distance path, facilitating the analysis of local geometric interplay between Euclidean geometry and the polyhedral decomposition along the path. Additionally, we study the distribution of Chebyshev centers and related radii across different polyhedra, shedding light on the polyhedral shape, size, clustering, and aiding in the understanding of decision boundaries.},
  archive      = {J_FDATA},
  author       = {Liu, Yajing and Caglar, Turgay and Peterson, Christopher and Kirby, Michael},
  doi          = {10.3389/fdata.2023.1274831},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1274831},
  shortjournal = {Front. Big Data},
  title        = {Integrating geometries of ReLU feedforward neural networks},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and adaptive dynamics-on-graphs to dynamics-of-graphs
translation. <em>FDATA</em>, <em>6</em>, 1274135. (<a
href="https://doi.org/10.3389/fdata.2023.1274135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous networks in the real world change with time, producing dynamic graphs such as human mobility networks and brain networks. Typically, the “dynamics on graphs” (e.g., changing node attribute values) are visible, and they may be connected to and suggestive of the “dynamics of graphs” (e.g., evolution of the graph topology). Due to two fundamental obstacles, modeling and mapping between them have not been thoroughly explored: (1) the difficulty of developing a highly adaptable model without solid hypotheses and (2) the ineffectiveness and slowness of processing data with varying granularity. To solve these issues, we offer a novel scalable deep echo-state graph dynamics encoder for networks with significant temporal duration and dimensions. A novel neural architecture search (NAS) technique is then proposed and tailored for the deep echo-state encoder to ensure strong learnability. Extensive experiments on synthetic and actual application data illustrate the proposed method&#39;s exceptional effectiveness and efficiency.},
  archive      = {J_FDATA},
  author       = {Zhang, Lei and Chen, Zhiqian and Lu, Chang-Tien and Zhao, Liang},
  doi          = {10.3389/fdata.2023.1274135},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1274135},
  shortjournal = {Front. Big Data},
  title        = {Fast and adaptive dynamics-on-graphs to dynamics-of-graphs translation},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time arrhythmia detection using convolutional neural
networks. <em>FDATA</em>, <em>6</em>, 1270756. (<a
href="https://doi.org/10.3389/fdata.2023.1270756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases, such as heart attack and congestive heart failure, are the leading cause of death both in the United States and worldwide. The current medical practice for diagnosing cardiovascular diseases is not suitable for long-term, out-of-hospital use. A key to long-term monitoring is the ability to detect abnormal cardiac rhythms, i.e., arrhythmia, in real-time. Most existing studies only focus on the accuracy of arrhythmia classification, instead of runtime performance of the workflow. In this paper, we present our work on supporting real-time arrhythmic detection using convolutional neural networks, which take images of electrocardiogram (ECG) segments as input, and classify the arrhythmia conditions. To support real-time processing, we have carried out extensive experiments and evaluated the computational cost of each step of the classification workflow. Our results show that it is feasible to achieve real-time arrhythmic detection using convolutional neural networks. To further demonstrate the generalizability of this approach, we used the trained model with processed data collected by a customized wearable sensor from a lab setting, and the results shown that our approach is highly accurate and efficient. This research provides the potentials to enable in-home real-time heart monitoring based on 2D image data, which opens up opportunities for integrating both machine learning and traditional diagnostic approaches.},
  archive      = {J_FDATA},
  author       = {Vu, Thong and Petty, Tyler and Yakut, Kemal and Usman, Muhammad and Xue, Wei and Haas, Francis M. and Hirsh, Robert A. and Zhao, Xinghui},
  doi          = {10.3389/fdata.2023.1270756},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1270756},
  shortjournal = {Front. Big Data},
  title        = {Real-time arrhythmia detection using convolutional neural networks},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Impresso text reuse at scale. An interface for the
exploration of text reuse data in semantically enriched historical
newspapers. <em>FDATA</em>, <em>6</em>, 1249469. (<a
href="https://doi.org/10.3389/fdata.2023.1249469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text Reuse reveals meaningful reiterations of text in large corpora. Humanities researchers use text reuse to study, e.g., the posterior reception of influential texts or to reveal evolving publication practices of historical media. This research is often supported by interactive visualizations which highlight relations and differences between text segments. In this paper, we build on earlier work in this domain. We present impresso Text Reuse at Scale, the to our knowledge first interface which integrates text reuse data with other forms of semantic enrichment to enable a versatile and scalable exploration of intertextual relations in historical newspaper corpora. The Text Reuse at Scale interface was developed as part of the impresso project and combines powerful search and filter operations with close and distant reading perspectives. We integrate text reuse data with enrichments derived from topic modeling, named entity recognition and classification, language and document type detection as well as a rich set of newspaper metadata. We report on historical research objectives and common user tasks for the analysis of historical text reuse data and present the prototype interface together with the results of a user evaluation.},
  archive      = {J_FDATA},
  author       = {Düring, Marten and Romanello, Matteo and Ehrmann, Maud and Beelen, Kaspar and Guido, Daniele and Deseure, Brecht and Bunout, Estelle and Keck, Jana and Apostolopoulos, Petros},
  doi          = {10.3389/fdata.2023.1249469},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1249469},
  shortjournal = {Front. Big Data},
  title        = {Impresso text reuse at scale. an interface for the exploration of text reuse data in semantically enriched historical newspapers},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning estimation of northern hemisphere soil
freeze-thaw dynamics using satellite multi-frequency microwave
brightness temperature observations. <em>FDATA</em>, <em>6</em>,
1243559. (<a href="https://doi.org/10.3389/fdata.2023.1243559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite microwave sensors are well suited for monitoring landscape freeze-thaw (FT) transitions owing to the strong brightness temperature (TB) or backscatter response to changes in liquid water abundance between predominantly frozen and thawed conditions. The FT retrieval is also a sensitive climate indicator with strong biophysical importance. However, retrieval algorithms can have difficulty distinguishing the FT status of soils from that of overlying features such as snow and vegetation, while variable land conditions can also degrade performance. Here, we applied a deep learning model using a multilayer convolutional neural network driven by AMSR2 and SMAP TB records, and trained on surface (~0–5 cm depth) soil temperature FT observations. Soil FT states were classified for the local morning (6 a.m.) and evening (6 p.m.) conditions corresponding to SMAP descending and ascending orbital overpasses, mapped to a 9 km polar grid spanning a five-year (2016–2020) record and Northern Hemisphere domain. Continuous variable estimates of the probability of frozen or thawed conditions were derived using a model cost function optimized against FT observational training data. Model results derived using combined multi-frequency (1.4, 18.7, 36.5 GHz) TBs produced the highest soil FT accuracy over other models derived using only single sensor or single frequency TB inputs. Moreover, SMAP L-band (1.4 GHz) TBs provided enhanced soil FT information and performance gain over model results derived using only AMSR2 TB inputs. The resulting soil FT classification showed favorable and consistent performance against soil FT observations from ERA5 reanalysis (mean percent accuracy, MPA: 92.7%) and in situ weather stations (MPA: 91.0%). The soil FT accuracy was generally consistent between morning and afternoon predictions and across different land covers and seasons. The model also showed better FT accuracy than ERA5 against regional weather station measurements (91.0% vs. 86.1% MPA). However, model confidence was lower in complex terrain where FT spatial heterogeneity was likely beneath the effective model grain size. Our results provide a high level of precision in mapping soil FT dynamics to improve understanding of complex seasonal transitions and their influence on ecological processes and climate feedbacks, with the potential to inform Earth system model predictions.},
  archive      = {J_FDATA},
  author       = {Donahue, Kellen and Kimball, John S. and Du, Jinyang and Bunt, Fredrick and Colliander, Andreas and Moghaddam, Mahta and Johnson, Jesse and Kim, Youngwook and Rawlins, Michael A.},
  doi          = {10.3389/fdata.2023.1243559},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1243559},
  shortjournal = {Front. Big Data},
  title        = {Deep learning estimation of northern hemisphere soil freeze-thaw dynamics using satellite multi-frequency microwave brightness temperature observations},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Towards an understanding of global brain data governance:
Ethical positions that underpin global brain data governance discourse.
<em>FDATA</em>, <em>6</em>, 1240660. (<a
href="https://doi.org/10.3389/fdata.2023.1240660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThe study of the brain continues to generate substantial volumes of data, commonly referred to as “big brain data,” which serves various purposes such as the treatment of brain-related diseases, the development of neurotechnological devices, and the training of algorithms. This big brain data, generated in different jurisdictions, is subject to distinct ethical and legal principles, giving rise to various ethical and legal concerns during collaborative efforts. Understanding these ethical and legal principles and concerns is crucial, as it catalyzes the development of a global governance framework, currently lacking in this field. While prior research has advocated for a contextual examination of brain data governance, such studies have been limited. Additionally, numerous challenges, issues, and concerns surround the development of a contextually informed brain data governance framework. Therefore, this study aims to bridge these gaps by exploring the ethical foundations that underlie contextual stakeholder discussions on brain data governance.MethodIn this study we conducted a secondary analysis of interviews with 21 neuroscientists drafted from the International Brain Initiative (IBI), LATBrain Initiative and the Society of Neuroscientists of Africa (SONA) who are involved in various brain projects globally and employing ethical theories. Ethical theories provide the philosophical frameworks and principles that inform the development and implementation of data governance policies and practices.ResultsThe results of the study revealed various contextual ethical positions that underscore the ethical perspectives of neuroscientists engaged in brain data research globally.DiscussionThis research highlights the multitude of challenges and deliberations inherent in the pursuit of a globally informed framework for governing brain data. Furthermore, it sheds light on several critical considerations that require thorough examination in advancing global brain data governance.},
  archive      = {J_FDATA},
  author       = {Ochang, Paschal and Eke, Damian and Stahl, Bernd Carsten},
  doi          = {10.3389/fdata.2023.1240660},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1240660},
  shortjournal = {Front. Big Data},
  title        = {Towards an understanding of global brain data governance: Ethical positions that underpin global brain data governance discourse},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). No longer hype, not yet mainstream? Recalibrating city
digital twins’ expectations and reality: A case study perspective.
<em>FDATA</em>, <em>6</em>, 1236397. (<a
href="https://doi.org/10.3389/fdata.2023.1236397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the concept of digital twin has already consolidated in industry, its spinoff in the urban environment—in the form of a City Digital Twin (CDT)—is more recent. A CDT is a dynamic digital model of the physical city whereby the physical and the digital are integrated in both directions, thus mutually affecting each other in real time. Replicating the path of smart cities, literature remarks that agendas and discourses around CDTs remain (1) tech-centered, that is, focused on overcoming technical limitations and lacking a proper sociotechnical contextualization of digital twin technologies; (2) practice-first, entailing hands-on applications without a long-term strategic governance for the management of these same technologies. Building on that, the goal of this article is to move beyond high-level conceptualizations of CDT to (a) get a cognizant understanding of what a CDT can do, how, and for whom; (b) map the current state of development and implementation of CDTs in Europe. This will be done by looking at three case studies—Dublin, Helsinki, and Rotterdam—often considered as successful examples of CDTs in Europe. Through exiting literature and official documents, as well as by relying on primary interviews with tech experts and local officials, the article explores the maturity of these CDTs, along the Gartner&#39;s hype-mainstream curve of technological innovations. Findings show that, while all three municipalities have long-term plans to deliver an integrated, cyber-physical real-time modeling of the city, currently their CDTs are still at an early stage of development. The focus remains on technical barriers—e.g., integration of different data sources—overlooking the societal dimension, such as the systematic involvement of citizens. As for the governance, all cases embrace a multistakeholder approach; yet CDTs are still not used for policymaking and it remains to see how the power across stakeholders will be distributed in terms of access to, control of, and decisions about CDTs.},
  archive      = {J_FDATA},
  author       = {Calzati, Stefano},
  doi          = {10.3389/fdata.2023.1236397},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1236397},
  shortjournal = {Front. Big Data},
  title        = {No longer hype, not yet mainstream? recalibrating city digital twins&#39; expectations and reality: A case study perspective},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A community focused approach toward making healthy and
affordable daily diet recommendations. <em>FDATA</em>, <em>6</em>,
1086212. (<a href="https://doi.org/10.3389/fdata.2023.1086212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionMaintaining an affordable and nutritious diet can be challenging, especially for those living under the conditions of poverty. To fulfill a healthy diet, consumers must make difficult decisions within a complicated food landscape. Decisions must factor information on health and budget constraints, the food supply and pricing options at local grocery stores, and nutrition and portion guidelines provided by government services. Information to support food choice decisions is often inconsistent and challenging to find, making it difficult for consumers to make informed, optimal decisions. This is especially true for low-income and Supplemental Nutrition Assistance Program (SNAP) households which have additional time and cost constraints that impact their food purchases and ultimately leave them more susceptible to malnutrition and obesity. The goal of this paper is to demonstrate how the integration of data from local grocery stores and federal government databases can be used to assist specific communities in meeting their unique health and budget challenges.MethodsWe discuss many of the challenges of integrating multiple data sources, such as inconsistent data availability and misleading nutrition labels. We conduct a case study using linear programming to identify a healthy meal plan that stays within a limited SNAP budget and also adheres to the Dietary Guidelines for Americans. Finally, we explore the main drivers of cost of local food products with emphasis on the nutrients determined by the USDA as areas of focus: added sugars, saturated fat, and sodium.Results and discussionOur case study results suggest that such an optimization model can be used to facilitate food purchasing decisions within a given community. By focusing on the community level, our results will inform future work navigating the complex networks of food information to build global recommendation systems.},
  archive      = {J_FDATA},
  author       = {Germino, Joe and Szymanski, Annalisa and Eicher-Miller, Heather A. and Metoyer, Ronald and Chawla, Nitesh V.},
  doi          = {10.3389/fdata.2023.1086212},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1086212},
  shortjournal = {Front. Big Data},
  title        = {A community focused approach toward making healthy and affordable daily diet recommendations},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Women in AI medicine and public health 2022.
<em>FDATA</em>, <em>6</em>, 1303367. (<a
href="https://doi.org/10.3389/fdata.2023.1303367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Chang, Lin-Ching and Angelopoulou, Anastasia},
  doi          = {10.3389/fdata.2023.1303367},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1303367},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Women in AI medicine and public health 2022},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum: Applications and techniques for fast machine
learning in science. <em>FDATA</em>, <em>6</em>, 1301942. (<a
href="https://doi.org/10.3389/fdata.2023.1301942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Deiana, Allison McCarn and Tran, Nhan and Agar, Joshua and Blott, Michaela and Di Guglielmo, Giuseppe and Duarte, Javier and Harris, Philip and Hauck, Scott and Liu, Mia and Neubauer, Mark S. and Ngadiuba, Jennifer and Ogrenci-Memik, Seda and Pierini, Maurizio and Aarrestad, Thea and Bähr, Steffen and Becker, Jürgen and Berthold, Anne-Sophie and Bonventre, Richard J. and Müller Bravo, Tomás E. and Diefenthaler, Markus and Dong, Zhen and Fritzsche, Nick and Gholami, Amir and Govorkova, Ekaterina and Guo, Dongning and Hazelwood, Kyle J. and Herwig, Christian and Khan, Babar and Kim, Sehoon and Klijnsma, Thomas and Liu, Yaling and Lo, Kin Ho and Nguyen, Tri and Pezzullo, Gianantonio and Rasoulinezhad, Seyedramin and Rivera, Ryan A. and Scholberg, Kate and Selig, Justin and Sen, Sougata and Strukov, Dmitri and Tang, William and Thais, Savannah and Unger, Kai Lukas and Vilalta, Ricardo and von Krosigk, Belina and Wang, Shen and Warburton, Thomas K.},
  doi          = {10.3389/fdata.2023.1301942},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1301942},
  shortjournal = {Front. Big Data},
  title        = {Corrigendum: Applications and techniques for fast machine learning in science},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast parallelized DBSCAN algorithm based on OpenMp for
detection of criminals on streaming services. <em>FDATA</em>,
<em>6</em>, 1292923. (<a
href="https://doi.org/10.3389/fdata.2023.1292923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionStreaming services are highly popular today. Millions of people watch live streams or videos and listen to music.MethodsOne of the most popular streaming platforms is Twitch, and data from this type of service can be a good example for applying the parallel DBSCAN algorithm proposed in this paper. Unlike the classical approach to neighbor search, the proposed one avoids redundancy, i.e., the repetition of the same calculations. At the same time, this algorithm is based on the classical DBSCAN method with a full search for all neighbors, parallelization by subtasks, and OpenMP parallel computing technology.ResultsIn this work, without reducing the accuracy, we managed to speed up the solution based on the DBSCAN algorithm when analyzing medium-sized data. As a result, the acceleration rate tends to the number of cores of a multicore computer system and the efficiency to one.DiscussionBefore conducting numerical experiments, theoretical estimates of speed-up and efficiency were obtained, and they aligned with the results obtained, confirming their validity. The quality of the performed clustering was verified using the silhouette value. All experiments were conducted using different percentages of medium-sized datasets. The prospects of applying the proposed algorithm can be obtained in various fields such as advertising, marketing, cybersecurity, and sociology. It is worth mentioning that datasets of this kind are often used for detecting fraud on the Internet, making an algorithm capable of considering all neighbors a useful tool for such research.},
  archive      = {J_FDATA},
  author       = {Mochurad, Lesia and Sydor, Andrii and Ratinskiy, Oleh},
  doi          = {10.3389/fdata.2023.1292923},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1292923},
  shortjournal = {Front. Big Data},
  title        = {A fast parallelized DBSCAN algorithm based on OpenMp for detection of criminals on streaming services},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recommender systems for sustainability: Overview and
research issues. <em>FDATA</em>, <em>6</em>, 1284511. (<a
href="https://doi.org/10.3389/fdata.2023.1284511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainability development goals (SDGs) are regarded as a universal call to action with the overall objectives of planet protection, ending of poverty, and ensuring peace and prosperity for all people. In order to achieve these objectives, different AI technologies play a major role. Specifically, recommender systems can provide support for organizations and individuals to achieve the defined goals. Recommender systems integrate AI technologies such as machine learning, explainable AI (XAI), case-based reasoning, and constraint solving in order to find and explain user-relevant alternatives from a potentially large set of options. In this article, we summarize the state of the art in applying recommender systems to support the achievement of sustainability development goals. In this context, we discuss open issues for future research.},
  archive      = {J_FDATA},
  author       = {Felfernig, Alexander and Wundara, Manfred and Tran, Thi Ngoc Trang and Polat-Erdeniz, Seda and Lubos, Sebastian and El Mansi, Merfat and Garber, Damian and Le, Viet-Man},
  doi          = {10.3389/fdata.2023.1284511},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1284511},
  shortjournal = {Front. Big Data},
  title        = {Recommender systems for sustainability: Overview and research issues},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An overview of video recommender systems: State-of-the-art
and research issues. <em>FDATA</em>, <em>6</em>, 1281614. (<a
href="https://doi.org/10.3389/fdata.2023.1281614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video platforms have become indispensable components within a diverse range of applications, serving various purposes in entertainment, e-learning, corporate training, online documentation, and news provision. As the volume and complexity of video content continue to grow, the need for personalized access features becomes an inevitable requirement to ensure efficient content consumption. To address this need, recommender systems have emerged as helpful tools providing personalized video access. By leveraging past user-specific video consumption data and the preferences of similar users, these systems excel in recommending videos that are highly relevant to individual users. This article presents a comprehensive overview of the current state of video recommender systems (VRS), exploring the algorithms used, their applications, and related aspects. In addition to an in-depth analysis of existing approaches, this review also addresses unresolved research challenges within this domain. These unexplored areas offer exciting opportunities for advancements and innovations, aiming to enhance the accuracy and effectiveness of personalized video recommendations. Overall, this article serves as a valuable resource for researchers, practitioners, and stakeholders in the video domain. It offers insights into cutting-edge algorithms, successful applications, and areas that merit further exploration to advance the field of video recommendation.},
  archive      = {J_FDATA},
  author       = {Lubos, Sebastian and Felfernig, Alexander and Tautschnig, Markus},
  doi          = {10.3389/fdata.2023.1281614},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1281614},
  shortjournal = {Front. Big Data},
  title        = {An overview of video recommender systems: State-of-the-art and research issues},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Healthcare analytics—a literature review and proposed
research agenda. <em>FDATA</em>, <em>6</em>, 1277976. (<a
href="https://doi.org/10.3389/fdata.2023.1277976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research addresses the demanding need for research in healthcare analytics, by explaining how previous studies have used big data, AI, and machine learning to identify, address, or solve healthcare problems. Healthcare science methods are combined with contemporary data science techniques to examine the literature, identify research gaps, and propose a research agenda for researchers, academic institutions, and governmental healthcare organizations. The study contributes to the body of literature by providing a state-of-the-art review of healthcare analytics as well as proposing a research agenda to advance the knowledge in this area. The results of this research can be beneficial for both healthcare science and data science researchers as well as practitioners in the field.},
  archive      = {J_FDATA},
  author       = {Elragal, Rawan and Elragal, Ahmed and Habibipour, Abdolrasoul},
  doi          = {10.3389/fdata.2023.1277976},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1277976},
  shortjournal = {Front. Big Data},
  title        = {Healthcare analytics—A literature review and proposed research agenda},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How big is big data? A comprehensive survey of data
production, storage, and streaming in science and industry.
<em>FDATA</em>, <em>6</em>, 1271639. (<a
href="https://doi.org/10.3389/fdata.2023.1271639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The contemporary surge in data production is fueled by diverse factors, with contributions from numerous stakeholders across various sectors. Comparing the volumes at play among different big data entities is challenging due to the scarcity of publicly available data. This survey aims to offer a comprehensive perspective on the orders of magnitude involved in yearly data generation by some public and private leading organizations, using an array of online sources for estimation. These estimates are based on meaningful, individual data production metrics and plausible per-unit sizes. The primary objective is to offer insights into the comparative scales of major big data players, their sources, and data production flows, rather than striving for precise measurements or incorporating the latest updates. The results are succinctly conveyed through a visual representation of the relative data generation volumes across these entities.},
  archive      = {J_FDATA},
  author       = {Clissa, Luca and Lassnig, Mario and Rinaldi, Lorenzo},
  doi          = {10.3389/fdata.2023.1271639},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1271639},
  shortjournal = {Front. Big Data},
  title        = {How big is big data? a comprehensive survey of data production, storage, and streaming in science and industry},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Smart cities challenges, technologies and trends.
<em>FDATA</em>, <em>6</em>, 1258051. (<a
href="https://doi.org/10.3389/fdata.2023.1258051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Gupta, Namita},
  doi          = {10.3389/fdata.2023.1258051},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1258051},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Smart cities challenges, technologies and trends},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differential privacy in collaborative filtering recommender
systems: A review. <em>FDATA</em>, <em>6</em>, 1249997. (<a
href="https://doi.org/10.3389/fdata.2023.1249997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art recommender systems produce high-quality recommendations to support users in finding relevant content. However, through the utilization of users&#39; data for generating recommendations, recommender systems threaten users&#39; privacy. To alleviate this threat, often, differential privacy is used to protect users&#39; data via adding random noise. This, however, leads to a substantial drop in recommendation quality. Therefore, several approaches aim to improve this trade-off between accuracy and user privacy. In this work, we first overview threats to user privacy in recommender systems, followed by a brief introduction to the differential privacy framework that can protect users&#39; privacy. Subsequently, we review recommendation approaches that apply differential privacy, and we highlight research that improves the trade-off between recommendation quality and user privacy. Finally, we discuss open issues, e.g., considering the relation between privacy and fairness, and the users&#39; different needs for privacy. With this review, we hope to provide other researchers an overview of the ways in which differential privacy has been applied to state-of-the-art collaborative filtering recommender systems.},
  archive      = {J_FDATA},
  author       = {Müllner, Peter and Lex, Elisabeth and Schedl, Markus and Kowald, Dominik},
  doi          = {10.3389/fdata.2023.1249997},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1249997},
  shortjournal = {Front. Big Data},
  title        = {Differential privacy in collaborative filtering recommender systems: A review},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairness of recommender systems in the recruitment domain:
An analysis from technical and legal perspectives. <em>FDATA</em>,
<em>6</em>, 1245198. (<a
href="https://doi.org/10.3389/fdata.2023.1245198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RSs) have become an integral part of the hiring process, be it via job advertisement ranking systems (job recommenders) for the potential employee or candidate ranking systems (candidate recommenders) for the employer. As seen in other domains, RSs are prone to harmful biases, unfair algorithmic behavior, and even discrimination in a legal sense. Some cases, such as salary equity in regards to gender (gender pay gap), stereotypical job perceptions along gendered lines, or biases toward other subgroups sharing specific characteristics in candidate recommenders, can have profound ethical and legal implications. In this survey, we discuss the current state of fairness research considering the fairness definitions (e.g., demographic parity and equal opportunity) used in recruitment-related RSs (RRSs). We investigate from a technical perspective the approaches to improve fairness, like synthetic data generation, adversarial training, protected subgroup distributional constraints, and post-hoc re-ranking. Thereafter, from a legal perspective, we contrast the fairness definitions and the effects of the aforementioned approaches with existing EU and US law requirements for employment and occupation, and second, we ascertain whether and to what extent EU and US law permits such approaches to improve fairness. We finally discuss the advances that RSs have made in terms of fairness in the recruitment domain, compare them with those made in other domains, and outline existing open challenges.},
  archive      = {J_FDATA},
  author       = {Kumar, Deepak and Grosz, Tessa and Rekabsaz, Navid and Greif, Elisabeth and Schedl, Markus},
  doi          = {10.3389/fdata.2023.1245198},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1245198},
  shortjournal = {Front. Big Data},
  title        = {Fairness of recommender systems in the recruitment domain: An analysis from technical and legal perspectives},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Anemia detection through non-invasive analysis of lip
mucosa images. <em>FDATA</em>, <em>6</em>, 1241899. (<a
href="https://doi.org/10.3389/fdata.2023.1241899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to detect anemia using images of the lip mucosa, where the skin tissue is thin, and to confirm the feasibility of detecting anemia noninvasively and in the home environment using machine learning (ML). Data were collected from 138 patients, including 100 women and 38 men. Six ML algorithms: artificial neural network (ANN), decision tree (DT), k-nearest neighbors (KNN), logistic regression (LR), naive bayes (NB), and support vector machine (SVM) which are widely used in medical applications, were used to classify the collected data. Two different data types were obtained from participants&#39; images (RGB red color values and HSV saturation values) as features, with age, sex, and hemoglobin levels utilized to perform classification. The ML algorithm was used to analyze and classify images of the lip mucosa quickly and accurately, potentially increasing the efficiency of anemia screening programs. The accuracy, precision, recall, and F-measure were evaluated to assess how well ML models performed in predicting anemia. The results showed that NB reported the highest accuracy (96%) among the other ML models used. DT, KNN and ANN reported an accuracies of (93%), while LR and SVM had an accuracy of (79%) and (75%) receptively. This research suggests that employing ML approaches to identify anemia will help classify the diagnosis, which will then help to create efficient preventive measures. Compared to blood tests, this noninvasive procedure is more practical and accessible to patients. Furthermore, ML algorithms may be created and trained to assess lip mucosa photos at a minimal cost, making it an affordable screening method in regions with a shortage of healthcare resources.},
  archive      = {J_FDATA},
  author       = {Mahmud, Shekhar and Donmez, Turker Berk and Mansour, Mohammed and Kutlu, Mustafa and Freeman, Chris},
  doi          = {10.3389/fdata.2023.1241899},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1241899},
  shortjournal = {Front. Big Data},
  title        = {Anemia detection through non-invasive analysis of lip mucosa images},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental study and clustering of operating staff of
search systems in the sense of stress resistance. <em>FDATA</em>,
<em>6</em>, 1239017. (<a
href="https://doi.org/10.3389/fdata.2023.1239017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThe main goal of this study is to develop a methodology for the organization of experimental selection of operator personnel based on the analysis of their behavior under the influence of micro-stresses.MethodsA human-machine interface model has been developed, which considers the change in the functional state of the human operator. The presented concept of the difficulty of detecting the object of attention contributed to developing a particular sequence of ordinary test images with stressor images included in it and presented models of the flow of presenting test images to the recipient.ResultsWith the help of descriptive statistics, the parameters of individual box-plot diagrams were determined, and the recipient group was clustered.DiscussionOverall, the proposed approach based on the example of the conducted grouping makes it possible to ensure the objectivity and efficiency of the professional selection of applicants for operator specialties.},
  archive      = {J_FDATA},
  author       = {Shakhovska, Nataliya and Kaminskyi, Roman and Khudoba, Bohdan},
  doi          = {10.3389/fdata.2023.1239017},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1239017},
  shortjournal = {Front. Big Data},
  title        = {Experimental study and clustering of operating staff of search systems in the sense of stress resistance},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A stream processing abstraction framework. <em>FDATA</em>,
<em>6</em>, 1227156. (<a
href="https://doi.org/10.3389/fdata.2023.1227156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time analysis of large multimedia streams is nowadays made efficient by the existence of several Big Data streaming platforms, like Apache Flink and Samza. However, the use of such platforms is difficult due to the fact that facilities they offer are often too raw to be effectively exploited by analysts. We describe the evolution of RAM3S, a software infrastructure for the integration of Big Data stream processing platforms, to SPAF, an abstraction framework able to provide programmers with a simple but powerful API to ease the development of stream processing applications. By using SPAF, the programmer can easily implement real-time complex analyses of massive streams on top of a distributed computing infrastructure, able to manage the volume and velocity of Big Data streams, thus effectively transforming data into value.},
  archive      = {J_FDATA},
  author       = {Bartolini, Ilaria and Patella, Marco},
  doi          = {10.3389/fdata.2023.1227156},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1227156},
  shortjournal = {Front. Big Data},
  title        = {A stream processing abstraction framework},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Climbing crags recommender system in arco, italy: A
comparative study. <em>FDATA</em>, <em>6</em>, 1214029. (<a
href="https://doi.org/10.3389/fdata.2023.1214029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outdoor sport climbing is popular in Northern Italy due to its vast amount of rock climbing places (such as crags). New climbing crags appear yearly, creating an information overload problem for tourists who plan their sport climbing vacation. Recommender systems partly addressed this issue by suggesting climbing crags according to the most visited places or the number of suitable climbing routes. Unfortunately, these methods do not consider contextual information. However, in sport climbing, as in other outdoor activities, the possibility of visiting certain places depends on several contextual factors, for instance, a suitable season (winter/summer), parking space availability if traveling with a car, or the possibility of climbing with children if traveling with children. To address this limitation, we collected and analyzed the crag visits in Arco (Italy) from an online guidebook. We found that climbing contextual information, similar to users&#39; content preferences, can be modeled by a correlation between recorded visits and crags features. Based on that, we developed and evaluated a novel context-aware climbing crags recommender system Visit &amp;amp; Climb, which consists of three stages as follows: (1) contextual information and content tastes are learned automatically from the users&#39; logs by computing correlation between users&#39; visits and crags&#39; features; (2) those learned tastes are further made adjustable in a preference elicitation web interface; (3) the user receives recommendations on the map according to the number of visits made by a climber with similar learned tastes. To measure the quality of this system, we performed an offline evaluation (where we calculated Mean Average Precision, Recall, and Normalized Discounted Cumulative Gain for top-N), a formative study, and an online evaluation (in a within-subject design with experienced outdoor climbers N = 40, who tried three similar systems including Visit &amp;amp; Climb). Offline tests showed that the proposed system suggests crags to climbers accurately as the other classical models for top-N recommendations. Meanwhile, online tests indicated that the system provides a significantly higher level of information sufficiency than other systems in this domain. The overall results demonstrated that the developed system provides recommendations according to the users&#39; requirements, and incorporating contextual information and crag characteristics into the climbing recommender system leads to increased information sufficiency caused by transparency, which improves satisfaction and use intention.},
  archive      = {J_FDATA},
  author       = {Ivanova, Iustina and Wald, Mike},
  doi          = {10.3389/fdata.2023.1214029},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1214029},
  shortjournal = {Front. Big Data},
  title        = {Climbing crags recommender system in arco, italy: A comparative study},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A solution and practice for combining multi-source
heterogeneous data to construct enterprise knowledge graph.
<em>FDATA</em>, <em>6</em>, 1278153. (<a
href="https://doi.org/10.3389/fdata.2023.1278153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knowledge graph is one of the essential infrastructures of artificial intelligence. It is a challenge for knowledge engineering to construct a high-quality domain knowledge graph for multi-source heterogeneous data. We propose a complete process framework for constructing a knowledge graph that combines structured data and unstructured data, which includes data processing, information extraction, knowledge fusion, data storage, and update strategies, aiming to improve the quality of the knowledge graph and extend its life cycle. Specifically, we take the construction process of an enterprise knowledge graph as an example and integrate enterprise register information, litigation-related information, and enterprise announcement information to enrich the enterprise knowledge graph. For the unstructured text, we improve existing model to extract triples and the F1-score of our model reached 72.77%. The number of nodes and edges in our constructed enterprise knowledge graph reaches 1,430,000 and 3,170,000, respectively. Furthermore, for each type of multi-source heterogeneous data, we apply corresponding methods and strategies for information extraction and data storage and carry out a detailed comparative analysis of graph databases. From the perspective of practical use, the informative enterprise knowledge graph and its timely update can serve many actual business needs. Our proposed enterprise knowledge graph has been deployed in HuaRong RongTong (Beijing) Technology Co., Ltd. and is used by the staff as a powerful tool for corporate due diligence. The key features are reported and analyzed in the case study. Overall, this paper provides an easy-to-follow solution and practice for domain knowledge graph construction, as well as demonstrating its application in corporate due diligence.},
  archive      = {J_FDATA},
  author       = {Yan, Chenwei and Fang, Xinyue and Huang, Xiaotong and Guo, Chenyi and Wu, Ji},
  doi          = {10.3389/fdata.2023.1278153},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {1278153},
  shortjournal = {Front. Big Data},
  title        = {A solution and practice for combining multi-source heterogeneous data to construct enterprise knowledge graph},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and analyzing the action process of monoamine
hormones in depression: A petri nets-based intelligent approach.
<em>FDATA</em>, <em>6</em>, 1268503. (<a
href="https://doi.org/10.3389/fdata.2023.1268503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary society, the incidence of depression is increasing significantly around the world. At present, most of the treatment methods for depression are psychological counseling and drug therapy. However, this approach does not allow patients to visualize the logic of hormones at the pathological level. In order to better apply intelligence computing methods to the medical field, and to more easily analyze the relationship between norepinephrine and dopamine in depression, it is necessary to build an interpretable graphical model to analyze this relationship which is of great significance to help discover new treatment ideas and potential drug targets. Petri net (PN) is a mathematical and graphic tool used to simulate and study complex system processes. This article utilizes PN to study the relationship between norepinephrine and dopamine in depression. We use PN to model the relationship between the norepinephrine and dopamine, and then use the invariant method of PN to verify and analyze it. The mathematical model proposed in this article can explain the complex pathogenesis of depression and visualize the process of intracellular hormone-induced state changes. Finally, the experiment result suggests that our method provides some possible research directions and approaches for the development of antidepressant drugs.},
  archive      = {J_FDATA},
  author       = {Wang, Xuyue and Yu, Wangyang and Zhang, Chao and Wang, Jia and Hao, Fei and Li, Jin and Zhang, Jing},
  doi          = {10.3389/fdata.2023.1268503},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {1268503},
  shortjournal = {Front. Big Data},
  title        = {Modeling and analyzing the action process of monoamine hormones in depression: A petri nets-based intelligent approach},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benchmarking open source and paid services for speech to
text: An analysis of quality and input variety. <em>FDATA</em>,
<em>6</em>, 1210559. (<a
href="https://doi.org/10.3389/fdata.2023.1210559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionSpeech to text (STT) technology has seen increased usage in recent years for automating transcription of spoken language. To choose the most suitable tool for a given task, it is essential to evaluate the performance and quality of both open source and paid STT services.MethodsIn this paper, we conduct a benchmarking study of open source and paid STT services, with a specific focus on assessing their performance concerning the variety of input text. We utilizes ix datasets obtained from diverse sources, including interviews, lectures, and speeches, as input for the STT tools. The evaluation of the instruments employs the Word Error Rate (WER), a standard metric for STT evaluation.ResultsOur analysis of the results demonstrates significant variations in the performance of the STT tools based on the input text. Certain tools exhibit superior performance on specific types of audio samples compared to others. Our study provides insights into STT tool performance when handling substantial data volumes, as well as the challenges and opportunities posed by the multimedia nature of the data.DiscussionAlthough paid services generally demonstrate better accuracy and speed compared to open source alternatives, their performance remains dependent on the input text. The study highlights the need for considering specific requirements and characteristics of the audio samples when selecting an appropriate STT tool.},
  archive      = {J_FDATA},
  author       = {Ferraro, Antonino and Galli, Antonio and La Gatta, Valerio and Postiglione, Marco},
  doi          = {10.3389/fdata.2023.1210559},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {1210559},
  shortjournal = {Front. Big Data},
  title        = {Benchmarking open source and paid services for speech to text: An analysis of quality and input variety},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intelligent telemonitoring application for coronavirus
patients: ReCOVeryaID. <em>FDATA</em>, <em>6</em>, 1205766. (<a
href="https://doi.org/10.3389/fdata.2023.1205766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 emergency underscored the importance of resolving crucial issues of territorial health monitoring, such as overloaded phone lines, doctors exposed to infection, chronically ill patients unable to access hospitals, etc. In fact, it often happened that people would call doctors/hospitals just out of anxiety, not realizing that they were clogging up communications, thus causing problems for those who needed them most; such people, often elderly, have often felt lonely and abandoned by the health care system because of poor telemedicine. In addition, doctors were unable to follow up on the most serious cases or make sure that others did not worsen. Thus, uring the first pandemic wave we had the idea to design a system that could help people alleviate their fears and be constantly monitored by doctors both in hospitals and at home; consequently, we developed reCOVeryaID, a telemonitoring application for coronavirus patients. It is an autonomous application supported by a knowledge base that can react promptly and inform medical doctors if dangerous trends in the patient&#39;s short- and long-term vital signs are detected. In this paper, we also validate the knowledge-base rules in real-world settings by testing them on data from real patients infected with COVID-19.},
  archive      = {J_FDATA},
  author       = {D&#39;Auria, Daniela and Russo, Raffaele and Fedele, Alfonso and Addabbo, Federica and Calvanese, Diego},
  doi          = {10.3389/fdata.2023.1205766},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {1205766},
  shortjournal = {Front. Big Data},
  title        = {An intelligent telemonitoring application for coronavirus patients: ReCOVeryaID},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Review OSINT tool for social engineering. <em>FDATA</em>,
<em>6</em>, 1169636. (<a
href="https://doi.org/10.3389/fdata.2023.1169636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, we observed an increase in cyber threats, especially social engineering attacks. By social engineering, we mean a set of techniques and tools to collect information about a person or target to extort sensitive information. Such information might be used for (industrial) espionage, to blackmail the user, or represent the starting point to perform malicious cyber attacks against the individual or, more often, against the organization they work for. The human factor is often the most vulnerable element in the security of any system, and the mass of information we disseminate online largely facilitates social engineering activities. To prevent and mitigate social engineering attacks, Open Source INTelligence (OSINT) techniques and tools can be used to evaluate the level of exposition of an individual or an organization. OSINT is the collection of information through open sources, that is, sources not protected by copyright or privacy. The article reviews the main OSINT tools for countering and preventing social engineering attacks. Specifically, it proposes the different tools diving them accordingly to the specific information they allow to track (e-mail, social profiles, phone numbers, etc.).},
  archive      = {J_FDATA},
  author       = {Nobili, Martina},
  doi          = {10.3389/fdata.2023.1169636},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {1169636},
  shortjournal = {Front. Big Data},
  title        = {Review OSINT tool for social engineering},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How can data visualization support interdisciplinary
research? LuxTIME: Studying historical exposomics in belval.
<em>FDATA</em>, <em>6</em>, 1164885. (<a
href="https://doi.org/10.3389/fdata.2023.1164885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Luxembourg Time Machine (LuxTIME) is an interdisciplinary project that studies the historical exposome during the industrialization of the Minett region, located in the south of Luxembourg. Exposome research encompasses all external and internal non-genetic factors influencing the health of the population, such as air pollution, green spaces, noise, work conditions, physical activity, and diet. Due to the wide scope of the interdisciplinary project, the historical study of the exposome in Belval involved the collection of quantitative and qualitative data from the National Archive of Luxembourg, various local archives (e.g., the communes of Esch-sur-Alzette and Sanem), the National Library, the Library of National Statistics STATEC, the National Geoportal of Luxembourg, scientific data from other research centers, and information from newspapers and journals digitized in eluxemburgensia.1 The data collection and the resulting inventory were performed to create a proof of concept to critically test the potential of a multi-layered research design for the study of the historical exposome in Belval. The guiding navigation tool throughout the project was data visualization. It has facilitated the exploration of the data collected (or just the data) and the metadata. It has also been a valuable tool for mapping knowledge and defining the scope of the project. Furthermore, different data visualization techniques have helped us to reflect on the process of knowledge sharing, to understand how the relevance of certain topics changed throughout the project and why, and to learn about the publication process in different journals and the experience of the participants. Data visualization is used not only as a means to an end but also to embrace the idea of sandcastles using a speculative and process-oriented approach to advance knowledge within all research fields involved. LuxTIME has proven to be an ideal case study to explore the possibilities offered by different data visualization concepts and techniques resulting in a data visualization toolbox that could be evaluated and extended in other interdisciplinary projects.},
  archive      = {J_FDATA},
  author       = {Aurich, Dagny and Horaniet Ibañez, Aida},
  doi          = {10.3389/fdata.2023.1164885},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {1164885},
  shortjournal = {Front. Big Data},
  title        = {How can data visualization support interdisciplinary research? LuxTIME: Studying historical exposomics in belval},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The digitized chronic disease management model: Scalable
strategies for implementing standardized healthcare and big data
analytics in shanghai. <em>FDATA</em>, <em>6</em>, 1241296. (<a
href="https://doi.org/10.3389/fdata.2023.1241296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundChronic disease management (CDM) falls under production relations, and digital technology belongs to the realm of productivity. Production relations must adapt to the development of productivity. Simultaneously, the prevalence and burden of chronic diseases are becoming increasingly severe, leveraging digital technology to innovate chronic disease management model is essential.MethodsThe model was built to cover experts in a number of fields, including administrative officials, public health experts, information technology staff, clinical experts, general practitioners, nurses, metrologists. Integration of multiple big data platforms such as General Practitioner Contract Platform, Integrated Community Multimorbidity Management System and Municipal and District-Level Health Information Comprehensive Platform. This study fully analyzes the organizational structure, participants, service objects, facilities and equipment, digital technology, operation process, etc., required for new model in the era of big data.ResultsBased on information technology, we build Integrated Community Multimorbidity Care Model (ICMCM). This model is based on big data, is driven by “technology + mechanism,” and uses digital technology as a tool to achieve the integration of services, technology integration, and data integration, thereby providing patients with comprehensive people-centered services. In order to promote the implementation of the ICMCM, Shanghai has established an integrated chronic disease management information system, clarified the role of each module and institution, and achieved horizontal and vertical integration of data and services. Moreover, we adopt standardized service processes and accurate blood pressure and blood glucose measurement equipment to provide services for patients and upload data in real time. On the basis of Integrated Community Multimorbidity Care Model, a platform and index system have been established, and the platform&#39;s multidimensional cross-evaluation and indicators are used for management and visual display.ConclusionsThe Integrated Community Multimorbidity Care Model guides chronic disease management in other countries and regions. We have utilized models to achieve a combination of services and management that provide a grip on chronic disease management.},
  archive      = {J_FDATA},
  author       = {Sui, Mengyun and Cheng, Minna and Zhang, Sheng and Wang, Yuheng and Yan, Qinghua and Yang, Qinping and Wu, Fei and Xue, Long and Shi, Yan and Fu, Chen},
  doi          = {10.3389/fdata.2023.1241296},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1241296},
  shortjournal = {Front. Big Data},
  title        = {The digitized chronic disease management model: Scalable strategies for implementing standardized healthcare and big data analytics in shanghai},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-list interfaces for recommender systems: Survey and
future directions. <em>FDATA</em>, <em>6</em>, 1239705. (<a
href="https://doi.org/10.3389/fdata.2023.1239705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a long time, recommender systems presented their results in the form of simple item lists. In recent years, however, multi-list interfaces have become the de-facto standard in industry, presenting users with numerous collections of recommendations, one below the other, each containing items with common characteristics. Netflix&#39;s interface, for instance, shows movies from certain genres, new releases, and lists of curated content. Spotify recommends new songs and albums, podcasts on specific topics, and what similar users are listening to. Despite their popularity, research on these so-called “carousels” is still limited. Few authors have investigated how to simulate the user behavior and how to optimize the recommendation process accordingly. The number of studies involving users is even smaller, with sometimes conflicting results. Consequently, little is known about how to design carousel-based interfaces for achieving the best user experience. This mini review aims to organize the existing knowledge and outlines directions that may improve the multi-list presentation of recommendations in the future.},
  archive      = {J_FDATA},
  author       = {Loepp, Benedikt},
  doi          = {10.3389/fdata.2023.1239705},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1239705},
  shortjournal = {Front. Big Data},
  title        = {Multi-list interfaces for recommender systems: Survey and future directions},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Universal skepticism of ChatGPT: A review of early
literature on chat generative pre-trained transformer. <em>FDATA</em>,
<em>6</em>, 1224976. (<a
href="https://doi.org/10.3389/fdata.2023.1224976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ChatGPT, a new language model developed by OpenAI, has garnered significant attention in various fields since its release. This literature review provides an overview of early ChatGPT literature across multiple disciplines, exploring its applications, limitations, and ethical considerations. The review encompasses Scopus-indexed publications from November 2022 to April 2023 and includes 156 articles related to ChatGPT. The findings reveal a predominance of negative sentiment across disciplines, though subject-specific attitudes must be considered. The review highlights the implications of ChatGPT in many fields including healthcare, raising concerns about employment opportunities and ethical considerations. While ChatGPT holds promise for improved communication, further research is needed to address its capabilities and limitations. This literature review provides insights into early research on ChatGPT, informing future investigations and practical applications of chatbot technology, as well as development and usage of generative AI.},
  archive      = {J_FDATA},
  author       = {Watters, Casey and Lemanski, Michal K.},
  doi          = {10.3389/fdata.2023.1224976},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1224976},
  shortjournal = {Front. Big Data},
  title        = {Universal skepticism of ChatGPT: A review of early literature on chat generative pre-trained transformer},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Do you hear the people sing? Comparison of synchronized URL
and narrative themes in 2020 and 2023 french protests. <em>FDATA</em>,
<em>6</em>, 1221744. (<a
href="https://doi.org/10.3389/fdata.2023.1221744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionFrance has seen two key protests within the term of President Emmanuel Macron: one in 2020 against Islamophobia, and another in 2023 against the pension reform. During these protests, there is much chatter on online social media platforms like Twitter.MethodsIn this study, we aim to analyze the differences between the online chatter of the 2 years through a network-centric view, and in particular the synchrony of users. This study begins by identifying groups of accounts that work together through two methods: temporal synchronicity and narrative similarity. We also apply a bot detection algorithm to identify bots within these networks and analyze the extent of inorganic synchronization within the discourse of these events.ResultsOverall, our findings suggest that the synchrony of users in 2020 on Twitter is much higher than that of 2023, and there are more bot activity in 2020 compared to 2023.},
  archive      = {J_FDATA},
  author       = {Ng, Lynnette Hui Xian and Carley, Kathleen M.},
  doi          = {10.3389/fdata.2023.1221744},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1221744},
  shortjournal = {Front. Big Data},
  title        = {Do you hear the people sing? comparison of synchronized URL and narrative themes in 2020 and 2023 french protests},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial intelligence research strategy of the united
states: Critical assessment and policy recommendations. <em>FDATA</em>,
<em>6</em>, 1206139. (<a
href="https://doi.org/10.3389/fdata.2023.1206139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The foundations of Artificial Intelligence (AI), a field whose applications are of great use and concern for society, can be traced back to the early years of the second half of the 20th century. Since then, the field has seen increased research output and funding cycles followed by setbacks. The new millennium has seen unprecedented interest in AI progress and expectations with significant financial investments from the public and private sectors. However, the continual acceleration of AI capabilities and real-world applications is not guaranteed. Mainly, accountability of AI systems in the context of the interplay between AI and the broader society is essential for adopting AI systems via the trust placed in them. Continual progress in AI research and development (R&amp;amp;D) can help tackle humanity&#39;s most significant challenges to improve social good. The authors of this paper suggest that the careful design of forward-looking research policies serves a crucial function in avoiding potential future setbacks in AI research, development, and use. The United States (US) has kept its leading role in R&amp;amp;D, mainly shaping the global trends in the field. Accordingly, this paper presents a critical assessment of the US National AI R&amp;amp;D Strategic Plan and prescribes six recommendations to improve future research strategies in the US and around the globe.},
  archive      = {J_FDATA},
  author       = {Gursoy, Furkan and Kakadiaris, Ioannis A.},
  doi          = {10.3389/fdata.2023.1206139},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1206139},
  shortjournal = {Front. Big Data},
  title        = {Artificial intelligence research strategy of the united states: Critical assessment and policy recommendations},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-to-music sonification and user engagement.
<em>FDATA</em>, <em>6</em>, 1206081. (<a
href="https://doi.org/10.3389/fdata.2023.1206081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of transforming data into sounds for auditory display provides unique user experiences and new perspectives for analyzing and interpreting data. A research study for data transformation to sounds based on musical elements, called data-to-music sonification, reveals how musical characteristics can serve analytical purposes with enhanced user engagement. An existing user engagement scale has been applied to measure engagement levels in three conditions within melodic, rhythmic, and chordal contexts. This article reports findings from a user engagement study with musical traits and states the benefits and challenges of using musical characteristics in sonifications. The results can guide the design of future sonifications of multivariable data.},
  archive      = {J_FDATA},
  author       = {Middleton, Jonathan and Hakulinen, Jaakko and Tiitinen, Katariina and Hella, Juho and Keskinen, Tuuli and Huuskonen, Pertti and Culver, Jeffrey and Linna, Juhani and Turunen, Markku and Ziat, Mounia and Raisamo, Roope},
  doi          = {10.3389/fdata.2023.1206081},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1206081},
  shortjournal = {Front. Big Data},
  title        = {Data-to-music sonification and user engagement},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Authentication, access, and monitoring system for critical
areas with the use of artificial intelligence integrated into perimeter
security in a data center. <em>FDATA</em>, <em>6</em>, 1200390. (<a
href="https://doi.org/10.3389/fdata.2023.1200390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perimeter security in data centers helps protect systems and the data they store by preventing unauthorized access and protecting critical resources from potential threats. According to the report of the information security company SonicWall, in 2021, there was a 66% increase in the number of ransomware attacks. In addition, the message from the same company indicates that the total number of cyber threats detected in 2021 increased by 24% compared to 2019. Among these attacks, the infrastructure of data centers was compromised; for this reason, organizations include elements Physical such as security cameras, movement detection systems, authentication systems, etc., as an additional measure that contributes to perimeter security. This work proposes using artificial intelligence in the perimeter security of data centers. It allows the automation and optimization of security processes, which translates into greater efficiency and reliability in the operations that prevent intrusions through authentication, permit verification, and monitoring critical areas. It is crucial to ensure that AI-based perimeter security systems are designed to protect and respect user privacy. In addition, it is essential to regularly monitor the effectiveness and integrity of these systems to ensure that they function correctly and meet security standards.},
  archive      = {J_FDATA},
  author       = {Villegas-Ch, William and García-Ortiz, Joselin},
  doi          = {10.3389/fdata.2023.1200390},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1200390},
  shortjournal = {Front. Big Data},
  title        = {Authentication, access, and monitoring system for critical areas with the use of artificial intelligence integrated into perimeter security in a data center},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernel-wise difference minimization for convolutional neural
network compression in metaverse. <em>FDATA</em>, <em>6</em>, 1200382.
(<a href="https://doi.org/10.3389/fdata.2023.1200382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have achieved remarkable success in computer vision research. However, to further improve their performance, network models have become increasingly complex and require more memory and computational resources. As a result, model compression has become an essential area of research in recent years. In this study, we focus on the best-case scenario for Huffman coding, which involves data with lower entropy. Building on this concept, we formulate a compression with a filter-wise difference minimization problem and propose a novel algorithm to solve it. Our approach involves filter-level pruning, followed by minimizing the difference between filters. Additionally, we perform filter permutation to further enhance compression. Our proposed algorithm achieves a compression rate of 94× on Lenet-5 and 50× on VGG16. The results demonstrate the effectiveness of our method in significantly reducing the size of deep neural networks while maintaining a high level of accuracy. We believe that our approach holds great promise in advancing the field of model compression and can benefit various applications that require efficient neural network models. Overall, this study provides important insights and contributions toward addressing the challenges of model compression in deep neural networks.},
  archive      = {J_FDATA},
  author       = {Chang, Yi-Ting},
  doi          = {10.3389/fdata.2023.1200382},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1200382},
  shortjournal = {Front. Big Data},
  title        = {Kernel-wise difference minimization for convolutional neural network compression in metaverse},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proliferation of atmospheric datasets can hinder policy
making: A data blending technique offers a solution. <em>FDATA</em>,
<em>6</em>, 1198097. (<a
href="https://doi.org/10.3389/fdata.2023.1198097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of atmospheric datasets is a key outcome from the continued development and advancement of our collective scientific understanding. Yet often datasets describing ostensibly identical processes or atmospheric variables provide widely varying results. As an example, we analyze several datasets representing rainfall over Nepal. We show that estimates of extreme rainfall are highly variable depending on which dataset you choose to look at. This leads to confusion and inaction from policy-focused decision makers. Scientifically, we should use datasets that sample a range of creation methodologies and prioritize the use of data science techniques that have the flexibility to incorporate these multiple sources of data. We demonstrate the use of a statistically interpretable data blending technique to help discern and communicate a consensus result, rather than imposing a priori judgment on the choice of dataset, for the benefit of policy decision making.},
  archive      = {J_FDATA},
  author       = {Steptoe, Hamish and Economou, Theo},
  doi          = {10.3389/fdata.2023.1198097},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1198097},
  shortjournal = {Front. Big Data},
  title        = {Proliferation of atmospheric datasets can hinder policy making: A data blending technique offers a solution},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of methods for assigning causes of death from
verbal autopsies in india. <em>FDATA</em>, <em>6</em>, 1197471. (<a
href="https://doi.org/10.3389/fdata.2023.1197471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundPhysician-coded verbal autopsy (PCVA) is the most widely used method to determine causes of death (COD) in countries where medical certification of death is low. Computer-coded verbal autopsy (CCVA), an alternative method to PCVA for assigning the COD is considered to be efficient and cost-effective. However, the performance of CCVA as compared to PCVA is yet to be established in the Indian context.MethodsWe evaluated the performance of PCVA and three CCVA methods i.e., InterVA 5, InSilico, and Tariff 2.0 on verbal autopsies done using the WHO 2016 VA tool on 2,120 reference standard cases developed from five tertiary care hospitals of Delhi. PCVA methodology involved dual independent review with adjudication, where required. Metrics to assess performance were Cause Specific Mortality Fraction (CSMF), sensitivity, positive predictive value (PPV), CSMF Accuracy, and Kappa statistic.ResultsIn terms of the measures of the overall performance of COD assignment methods, for CSMF Accuracy, the PCVA method achieved the highest score of 0.79, followed by 0.67 for Tariff_2.0, 0.66 for Inter-VA and 0.62 for InSilicoVA. The PCVA method also achieved the highest agreement (57%) and Kappa scores (0.54). The PCVA method showed the highest sensitivity for 15 out of 20 causes of death.ConclusionOur study found that the PCVA method had the best performance out of all the four COD assignment methods that were tested in our study sample. In order to improve the performance of CCVA methods, multicentric studies with larger sample sizes need to be conducted using the WHO VA tool.},
  archive      = {J_FDATA},
  author       = {Benara, Sudhir K. and Sharma, Saurabh and Juneja, Atul and Nair, Saritha and Gulati, B. K. and Singh, Kh. Jitenkumar and Singh, Lucky and Yadav, Ved Prakash and Rao, Chalapati and Rao, M. Vishnu Vardhana},
  doi          = {10.3389/fdata.2023.1197471},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1197471},
  shortjournal = {Front. Big Data},
  title        = {Evaluation of methods for assigning causes of death from verbal autopsies in india},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovering anomalies in big data: A review focused on the
application of metaheuristics and machine learning techniques.
<em>FDATA</em>, <em>6</em>, 1179625. (<a
href="https://doi.org/10.3389/fdata.2023.1179625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase in available data from computer systems and their security threats, interest in anomaly detection has increased as well in recent years. The need to diagnose faults and cyberattacks has also focused scientific research on the automated classification of outliers in big data, as manual labeling is difficult in practice due to their huge volumes. The results obtained from data analysis can be used to generate alarms that anticipate anomalies and thus prevent system failures and attacks. Therefore, anomaly detection has the purpose of reducing maintenance costs as well as making decisions based on reports. During the last decade, the approaches proposed in the literature to classify unknown anomalies in log analysis, process analysis, and time series have been mainly based on machine learning and deep learning techniques. In this study, we provide an overview of current state-of-the-art methodologies, highlighting their advantages and disadvantages and the new challenges. In particular, we will see that there is no absolute best method, i.e., for any given dataset a different method may achieve the best result. Finally, we describe how the use of metaheuristics within machine learning algorithms makes it possible to have more robust and efficient tools.},
  archive      = {J_FDATA},
  author       = {Cavallaro, Claudia and Cutello, Vincenzo and Pavone, Mario and Zito, Francesco},
  doi          = {10.3389/fdata.2023.1179625},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1179625},
  shortjournal = {Front. Big Data},
  title        = {Discovering anomalies in big data: A review focused on the application of metaheuristics and machine learning techniques},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Arrival times by recurrent neural network for induced
seismic events from a permanent network. <em>FDATA</em>, <em>6</em>,
1174478. (<a href="https://doi.org/10.3389/fdata.2023.1174478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have developed a Recurrent Neural Network (RNN)-based phase picker for data obtained from a local seismic monitoring array specifically designated for induced seismicity analysis. The proposed algorithm was rigorously tested using real-world data from a network encompassing nine three-component stations. The algorithm is designed for multiple monitoring of repeated injection within the permanent array. For such an array, the RNN is initially trained on a foundational dataset, enabling the trained algorithm to accurately identify other induced events even if they occur in different regions of the array. Our RNN-based phase picker achieved an accuracy exceeding 80% for arrival time picking when compared to precise manual picking techniques. However, the event locations (based on the arrival picking) had to be further constrained to avoid false arrival picks. By utilizing these refined arrival times, we were able to locate seismic events and assess their magnitudes. The magnitudes of events processed automatically exhibited a discrepancy of up to 0.3 when juxtaposed with those derived from manual processing. Importantly, the efficacy of our results remains consistent irrespective of the specific training dataset employed, provided that the dataset originates from within the network.},
  archive      = {J_FDATA},
  author       = {Kolar, Petr and Waheed, Umair bin and Eisner, Leo and Matousek, Petr},
  doi          = {10.3389/fdata.2023.1174478},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1174478},
  shortjournal = {Front. Big Data},
  title        = {Arrival times by recurrent neural network for induced seismic events from a permanent network},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient community detection in multilayer networks using
boolean compositions. <em>FDATA</em>, <em>6</em>, 1144793. (<a
href="https://doi.org/10.3389/fdata.2023.1144793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks (or graphs) are used to model the dyadic relations between entities in complex systems. Analyzing the properties of the networks reveal important characteristics of the underlying system. However, in many disciplines, including social sciences, bioinformatics, and technological systems, multiple relations exist between entities. In such cases, a simple graph is not sufficient to model these multiple relations, and a multilayer network is a more appropriate model. In this paper, we explore community detection in multilayer networks. Specifically, we propose a novel network decoupling strategy for efficiently combining the communities in the different layers using the Boolean primitives AND, OR, and NOT. Our proposed method, network decoupling, is based on analyzing the communities in each network layer individually and then aggregating the analysis results. We (i) describe our network decoupling algorithms for finding communities, (ii) present how network decoupling can be used to express different types of communities in multilayer networks, and (iii) demonstrate the effectiveness of using network decoupling for detecting communities in real-world and synthetic data sets. Compared to other algorithms for detecting communities in multilayer networks, our proposed network decoupling method requires significantly lower computation time while producing results of high accuracy. Based on these results, we anticipate that our proposed network decoupling technique will enable a more detailed analysis of multilayer networks in an efficient manner.},
  archive      = {J_FDATA},
  author       = {Santra, Abhishek and Irany, Fariba Afrin and Madduri, Kamesh and Chakravarthy, Sharma and Bhowmick, Sanjukta},
  doi          = {10.3389/fdata.2023.1144793},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1144793},
  shortjournal = {Front. Big Data},
  title        = {Efficient community detection in multilayer networks using boolean compositions},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthetic biomedical data generation in support of in silico
clinical trials. <em>FDATA</em>, <em>6</em>, 1085571. (<a
href="https://doi.org/10.3389/fdata.2023.1085571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Living in the era of Big Data, one may advocate that the additional synthetic generation of data is redundant. However, to be able to truly say whether it is valid or not, one needs to focus more on the meaning and quality of data than on the quantity. In some domains, such as biomedical and translational sciences, data privacy still holds a higher importance than data sharing. This by default limits access to valuable research data. Intensive discussion, agreements, and conventions among different medical research players, as well as effective techniques and regulations for data anonymization, already made a big step toward simplification of data sharing. However, the situation with the availability of data about rare diseases or outcomes of novel treatments still requires costly and risky clinical trials and, thus, would greatly benefit from smart data generation. Clinical trials and tests on animals initiate a cyclic procedure that may involve multiple redesigns and retesting, which typically takes two or three years for medical devices and up to eight years for novel medicines, and costs between 10 and 20 million euros. The US Food and Drug Administration (FDA) acknowledges that for many novel devices, practical limitations require alternative approaches, such as computer modeling and engineering tests, to conduct large, randomized studies. In this article, we give an overview of global initiatives advocating for computer simulations in support of the 3R principles (Replacement, Reduction, and Refinement) in humane experimentation. We also present several research works that have developed methodologies of smart and comprehensive generation of synthetic biomedical data, such as virtual cohorts of patients, in support of In Silico Clinical Trials (ISCT) and discuss their common ground.},
  archive      = {J_FDATA},
  author       = {Simalatsar, Alena},
  doi          = {10.3389/fdata.2023.1085571},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {1085571},
  shortjournal = {Front. Big Data},
  title        = {Synthetic biomedical data generation in support of in silico clinical trials},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comorbidity network analysis using graphical models for
electronic health records. <em>FDATA</em>, <em>6</em>, 846202. (<a
href="https://doi.org/10.3389/fdata.2023.846202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ImportanceThe comorbidity network represents multiple diseases and their relationships in a graph. Understanding comorbidity networks among critical care unit (CCU) patients can help doctors diagnose patients faster, minimize missed diagnoses, and potentially decrease morbidity and mortality.ObjectiveThe main objective of this study was to identify the comorbidity network among CCU patients using a novel application of a machine learning method (graphical modeling method). The second objective was to compare the machine learning method with a traditional pairwise method in simulation.MethodThis cross-sectional study used CCU patients&#39; data from Medical Information Mart for the Intensive Care-3 (MIMIC-3) dataset, an electronic health record (EHR) of patients with CCU hospitalizations within Beth Israel Deaconess Hospital from 2001 to 2012. A machine learning method (graphical modeling method) was applied to identify the comorbidity network of 654 diagnosis categories among 46,511 patients.ResultsOut of the 654 diagnosis categories, the graphical modeling method identified a comorbidity network of 2,806 associations in 510 diagnosis categories. Two medical professionals reviewed the comorbidity network and confirmed that the associations were consistent with current medical understanding. Moreover, the strongest association in our network was between “poisoning by psychotropic agents” and “accidental poisoning by tranquilizers” (logOR 8.16), and the most connected diagnosis was “disorders of fluid, electrolyte, and acid–base balance” (63 associated diagnosis categories). Our method outperformed traditional pairwise comorbidity network methods in simulation studies. Some strongest associations between diagnosis categories were also identified, for example, “diagnoses of mitral and aortic valve” and “other rheumatic heart disease” (logOR: 5.15). Furthermore, our method identified diagnosis categories that were connected with most other diagnosis categories, for example, “disorders of fluid, electrolyte, and acid–base balance” was associated with 63 other diagnosis categories. Additionally, using a data-driven approach, our method partitioned the diagnosis categories into 14 modularity classes.Conclusion and relevanceOur graphical modeling method inferred a logical comorbidity network whose associations were consistent with current medical understanding and outperformed traditional network methods in simulation. Our comorbidity network method can potentially assist CCU doctors in diagnosing patients faster and minimizing missed diagnoses.},
  archive      = {J_FDATA},
  author       = {Zhao, Bo and Huepenbecker, Sarah and Zhu, Gen and Rajan, Suja S. and Fujimoto, Kayo and Luo, Xi},
  doi          = {10.3389/fdata.2023.846202},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {846202},
  shortjournal = {Front. Big Data},
  title        = {Comorbidity network analysis using graphical models for electronic health records},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Multimodal digital approaches to personalized
medicine. <em>FDATA</em>, <em>6</em>, 1242482. (<a
href="https://doi.org/10.3389/fdata.2023.1242482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Clay, Ieuan and De Luca, Valeria and Sano, Akane},
  doi          = {10.3389/fdata.2023.1242482},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {1242482},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Multimodal digital approaches to personalized medicine},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging extreme scale analytics, AI and digital twins for
maritime digitalization: The VesselAI architecture. <em>FDATA</em>,
<em>6</em>, 1220348. (<a
href="https://doi.org/10.3389/fdata.2023.1220348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modern maritime industry is producing data at an unprecedented rate. The capturing and processing of such data is integral to create added value for maritime companies and other maritime stakeholders, but their true potential can only be unlocked by innovative technologies such as extreme-scale analytics, AI, and digital twins, given that existing systems and traditional approaches are unable to effectively collect, store, and process big data. Such innovative systems are not only projected to effectively deal with maritime big data but to also create various tools that can assist maritime companies, in an evolving and complex environment that requires maritime vessels to increase their overall safety and performance and reduce their consumption and emissions. An integral challenge for developing these next-generation maritime applications lies in effectively combining and incorporating the aforementioned innovative technologies in an integrated system. Under this context, the current paper presents the architecture of VesselAI, an EU-funded project that aims to develop, validate, and demonstrate a novel holistic framework based on a combination of the state-of-the-art HPC, Big Data and AI technologies, capable of performing extreme-scale and distributed analytics for fuelling the next-generation digital twins in maritime applications and beyond.},
  archive      = {J_FDATA},
  author       = {Ilias, Loukas and Tsapelas, Giannis and Kapsalis, Panagiotis and Michalakopoulos, Vasilis and Kormpakis, Giorgos and Mouzakitis, Spiros and Askounis, Dimitris},
  doi          = {10.3389/fdata.2023.1220348},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {1220348},
  shortjournal = {Front. Big Data},
  title        = {Leveraging extreme scale analytics, AI and digital twins for maritime digitalization: The VesselAI architecture},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ki-cook: Clustering multimodal cooking representations
through knowledge-infused learning. <em>FDATA</em>, <em>6</em>, 1200840.
(<a href="https://doi.org/10.3389/fdata.2023.1200840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal recipe retrieval has gained prominence due to its ability to retrieve a text representation given an image representation and vice versa. Clustering these recipe representations based on similarity is essential to retrieve relevant information about unknown food images. Existing studies cluster similar recipe representations in the latent space based on class names. Due to inter-class similarity and intraclass variation, associating a recipe with a class name does not provide sufficient knowledge about recipes to determine similarity. However, recipe title, ingredients, and cooking actions provide detailed knowledge about recipes and are a better determinant of similar recipes. In this study, we utilized this additional knowledge of recipes, such as ingredients and recipe title, to identify similar recipes, emphasizing attention especially on rare ingredients. To incorporate this knowledge, we propose a knowledge-infused multimodal cooking representation learning network, Ki-Cook, built on the procedural attribute of the cooking process. To the best of our knowledge, this is the first study to adopt a comprehensive recipe similarity determinant to identify and cluster similar recipe representations. The proposed network also incorporates ingredient images to learn multimodal cooking representation. Since the motivation for clustering similar recipes is to retrieve relevant information for an unknown food image, we evaluated the ingredient retrieval task. We performed an empirical analysis to establish that our proposed model improves the Coverage of Ground Truth by 12% and the Intersection Over Union by 10% compared to the baseline models. On average, the representations learned by our model contain an additional 15.33% of rare ingredients compared to the baseline models. Owing to this difference, our qualitative evaluation shows a 39% improvement in clustering similar recipes in the latent space compared to the baseline models, with an inter-annotator agreement of the Fleiss kappa score of 0.35.},
  archive      = {J_FDATA},
  author       = {Venkataramanan, Revathy and Padhee, Swati and Rao, Saini Rohan and Kaoshik, Ronak and Sundara Rajan, Anirudh and Sheth, Amit},
  doi          = {10.3389/fdata.2023.1200840},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {1200840},
  shortjournal = {Front. Big Data},
  title        = {Ki-cook: Clustering multimodal cooking representations through knowledge-infused learning},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Wikipedia page views for health research: A review.
<em>FDATA</em>, <em>6</em>, 1199060. (<a
href="https://doi.org/10.3389/fdata.2023.1199060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wikipedia is an open-source online encyclopedia and one of the most-read sources of online health information. Likewise, Wikipedia page views have also been analyzed to inform public health services and policies. The present review analyzed 29 studies utilizing Wikipedia page views for health research. Most reviewed studies were published in recent years and emanated from high-income countries. Together with Wikipedia page views, most studies also used data from other internet sources, such as Google, Twitter, YouTube, and Reddit. The reviewed studies also explored various non-communicable diseases, infectious diseases, and health interventions to describe changes in the utilization of online health information from Wikipedia, to examine the effect of public events on public interest and information usage about health-related Wikipedia pages, to estimate and predict the incidence and prevalence of diseases, to predict data from other internet data sources, to evaluate the effectiveness of health education activities, and to explore the evolution of a health topic. Given some of the limitations in replicating some of the reviewed studies, future research can specify the specific Wikipedia page or pages analyzed, the language of the Wikipedia pages examined, dates of data collection, dates explored, type of data, and whether page views were limited to Internet users and whether web crawlers and redirects to the Wikipedia page were included. Future research can also explore public interest in other commonly read health topics available in Wikipedia, develop Wikipedia-based models that can be used to predict disease incidence and improve Wikipedia-based health education activities.},
  archive      = {J_FDATA},
  author       = {Alibudbud, Rowalt},
  doi          = {10.3389/fdata.2023.1199060},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {1199060},
  shortjournal = {Front. Big Data},
  title        = {Wikipedia page views for health research: A review},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the use of instagram images color histograms and
hashtags sets for automatic image annotation. <em>FDATA</em>,
<em>6</em>, 1149523. (<a
href="https://doi.org/10.3389/fdata.2023.1149523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color similarity has been a key feature for content-based image retrieval by contemporary search engines, such as Google. In this study, we compare the visual content information of images, obtained through color histograms, with their corresponding hashtag sets in the case of Instagram posts. In previous studies, we had concluded that less than 25% of Instagram hashtags are related to the actual visual content of the image they accompany. Thus, the use of Instagram images&#39; corresponding hashtags for automatic image annotation is questionable. In this study, we are answering this question through the computational comparison of images&#39; low-level characteristics with the semantic and syntactic information of their corresponding hashtags. The main conclusion of our study on 26 different subjects (concepts) is that color histograms and filtered hashtag sets, although related, should be better seen as a complementary source for image retrieval and automatic image annotation.},
  archive      = {J_FDATA},
  author       = {Giannoulakis, Stamatios and Tsapatsoulis, Nicolas and Djouvas, Constantinos},
  doi          = {10.3389/fdata.2023.1149523},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {1149523},
  shortjournal = {Front. Big Data},
  title        = {Evaluating the use of instagram images color histograms and hashtags sets for automatic image annotation},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessment of geothermal resource potential in changbaishan
utilizing high-precision gravity-based man-machine interactive inversion
technology. <em>FDATA</em>, <em>6</em>, 1139918. (<a
href="https://doi.org/10.3389/fdata.2023.1139918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the clean energy sources, geothermal resources have no negative impact in changing the climate. However, the accurate assessment and precise identification of the potential geothermal resource is still complex and dynamic. In this paper, ~2,000 large-scale high-precision gravity survey points are conducted in the north of the Tianchi caldera, Changbaishan. Advanced data processing technologies can provide straightforward information on deep geothermal resources (Hot source, caprock, geothermal reservoir and geothermal migration pathway). Upwards continuation and the technologies decode two dome shaped low and gentle anomalies (−48 × 10−5 m/s2−65 m/s2) and a positive gravity gradient anomaly (0.4 × 10−7 m/s2−1.6 × 10−5 m/s2) in large-scale high-precision gravity planar. According to two point five dimensional man-machine interactive inversion technology and the research on petrophysical parameters, the density of the shied-forming basalts in the two orthogonal gravity sections is 2.58 g/cm3. The relatively intermediate to high density (2.60–2.75 g/cm3) represents geothermal reservoir, and low density (low to 2.58 g/cm3) is the geothermal migration pathway. In addition, large-scale high-precision gravity planar with a solution of about 1/50,000 indicate that the north of the Tianchi caldera exits the sedimentary basin and uplift mountain geothermal system.},
  archive      = {J_FDATA},
  author       = {Xu, Zhi-He and Jiang, Ji-Yi and Gu, Guan-Wen and Sun, Zhen-Jun and Jiao, Xuan-Kai and Niu, Xing-Guo and Yu, Qin},
  doi          = {10.3389/fdata.2023.1139918},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {1139918},
  shortjournal = {Front. Big Data},
  title        = {Assessment of geothermal resource potential in changbaishan utilizing high-precision gravity-based man-machine interactive inversion technology},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Too much information is no information: How machine learning
and feature selection could help in understanding the motor control of
pointing. <em>FDATA</em>, <em>6</em>, 921355. (<a
href="https://doi.org/10.3389/fdata.2023.921355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this study was to develop the use of Machine Learning techniques as a means of multivariate analysis in studies of motor control. These studies generate a huge amount of data, the analysis of which continues to be largely univariate. We propose the use of machine learning classification and feature selection as a means of uncovering feature combinations that are altered between conditions. High dimensional electromyogram (EMG) vectors were generated as several arm and trunk muscles were recorded while subjects pointed at various angles above and below the gravity neutral horizontal plane. We used Linear Discriminant Analysis (LDA) to carry out binary classifications between the EMG vectors for pointing at a particular angle, vs. pointing at the gravity neutral direction. Classification success provided a composite index of muscular adjustments for various task constraints—in this case, pointing angles. In order to find the combination of features that were significantly altered between task conditions, we conducted a post classification feature selection i.e., investigated which combination of features had allowed for the classification. Feature selection was done by comparing the representations of each category created by LDA for the classification. In other words computing the difference between the representations of each class. We propose that this approach will help with comparing high dimensional EMG patterns in two ways; (i) quantifying the effects of the entire pattern rather than using single arbitrarily defined variables and (ii) identifying the parts of the patterns that convey the most information regarding the investigated effects.},
  archive      = {J_FDATA},
  author       = {Thomas, Elizabeth and Ali, Ferid Ben and Tolambiya, Arvind and Chambellant, Florian and Gaveau, Jérémie},
  doi          = {10.3389/fdata.2023.921355},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {921355},
  shortjournal = {Front. Big Data},
  title        = {Too much information is no information: How machine learning and feature selection could help in understanding the motor control of pointing},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Ethical challenges in AI-enhanced military
operations. <em>FDATA</em>, <em>6</em>, 1229252. (<a
href="https://doi.org/10.3389/fdata.2023.1229252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Helkala, Kirsi Marjaana and Lucas, George and Barrett, Edward and Syse, Henrik},
  doi          = {10.3389/fdata.2023.1229252},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {1229252},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Ethical challenges in AI-enhanced military operations},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Rising stars in data mining and management 2022.
<em>FDATA</em>, <em>6</em>, 1201798. (<a
href="https://doi.org/10.3389/fdata.2023.1201798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Chen, Feng and Yuan, Shuhan and Hu, Jilin and Li, Cheng-Te and Raymond, Rudy and Shou, Lidan},
  doi          = {10.3389/fdata.2023.1201798},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {1201798},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Rising stars in data mining and management 2022},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PME: Pruning-based multi-size embedding for recommender
systems. <em>FDATA</em>, <em>6</em>, 1195742. (<a
href="https://doi.org/10.3389/fdata.2023.1195742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedding is widely used in recommendation models to learn feature representations. However, the traditional embedding technique that assigns a fixed size to all categorical features may be suboptimal due to the following reasons. In recommendation domain, the majority of categorical features&#39; embeddings can be trained with less capacity without impacting model performance, thereby storing embeddings with equal length may incur unnecessary memory usage. Existing work that tries to allocate customized sizes for each feature usually either simply scales the embedding size with feature&#39;s popularity or formulates this size allocation problem as an architecture selection problem. Unfortunately, most of these methods either have large performance drop or incur significant extra time cost for searching proper embedding sizes. In this article, instead of formulating the size allocation problem as an architecture selection problem, we approach the problem from a pruning perspective and propose Pruning-based Multi-size Embedding (PME) framework. During the search phase, we prune the dimensions that have the least impact on model performance in the embedding to reduce its capacity. Then, we show that the customized size of each token can be obtained by transferring the capacity of its pruned embedding with significant less search cost. Experimental results validate that PME can efficiently find proper sizes and hence achieve strong performance while significantly reducing the number of parameters in the embedding layer.},
  archive      = {J_FDATA},
  author       = {Liu, Zirui and Song, Qingquan and Li, Li and Choi, Soo-Hyun and Chen, Rui and Hu, Xia},
  doi          = {10.3389/fdata.2023.1195742},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {1195742},
  shortjournal = {Front. Big Data},
  title        = {PME: Pruning-based multi-size embedding for recommender systems},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble-based classification approach for PM2.5
concentration forecasting using meteorological data. <em>FDATA</em>,
<em>6</em>, 1175259. (<a
href="https://doi.org/10.3389/fdata.2023.1175259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution is a serious challenge to humankind as it poses many health threats. It can be measured using the air quality index (AQI). Air pollution is the result of contamination of both outdoor and indoor environments. The AQI is being monitored by various institutions globally. The measured air quality data are kept mostly for public use. Using the previously calculated AQI values, the future values of AQI can be predicted, or the class/category value of the numeric value can be obtained. This forecast can be performed with more accuracy using supervised machine learning methods. In this study, multiple machine-learning approaches were used to classify PM2.5 values. The values for the pollutant PM2.5 were classified into different groups using machine learning algorithms such as logistic regression, support vector machines, random forest, extreme gradient boosting, and their grid search equivalents, along with the deep learning method multilayer perceptron. After performing multiclass classification using these algorithms, the parameters accuracy and per-class accuracy were used to compare the methods. As the dataset used was imbalanced, a SMOTE-based approach for balancing the dataset was used. Compared to all other classifiers that use the original dataset, the accuracy of the random forest multiclass classifier with SMOTE-based dataset balancing was found to provide better accuracy.},
  archive      = {J_FDATA},
  author       = {Saminathan, S. and Malathy, C.},
  doi          = {10.3389/fdata.2023.1175259},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {1175259},
  shortjournal = {Front. Big Data},
  title        = {Ensemble-based classification approach for PM2.5 concentration forecasting using meteorological data},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characterizing the visualization design space of distant and
close reading of poetic rhythm. <em>FDATA</em>, <em>6</em>, 1167708. (<a
href="https://doi.org/10.3389/fdata.2023.1167708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metrical and rhythmical poetry analysis is founded on the systematic statistical analysis and comparison of sonic devices (e.g., rhythmic patterns) that emerge from a combination of pre-established aesthetic and structural rules and the poet&#39;s abilities and creative genius to convey a given message adhering to the said constraints. These rhythmical patterns, which have been traditionally obtained by means of a careful close reading of the poems, in a process known as “scansion,” can now be obtained and made visible by automatic means. However, the visualization literature is still scarce on approaches that allow an insightful close and distant reading of the rhythmical patterns in a poetry corpus. In this work, we report our initial efforts in characterizing of the visualization design space of distant and close reading of poetic rhythm. By employing a digital version of a corpus of 11,268 verses originally written by the Spanish poet and playwright Federico García-Lorca (1898–1936), we could craft several prototypical visualizations representative of the inherent complexity of the problem which we expect to employ in future user studies and that we share here with the rest of the community to foster further discussion around this interesting topic.},
  archive      = {J_FDATA},
  author       = {Benito-Santos, Alejandro and Muñoz, Salvador and Therón Sánchez, Roberto and García Peñalvo, Francisco J.},
  doi          = {10.3389/fdata.2023.1167708},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {1167708},
  shortjournal = {Front. Big Data},
  title        = {Characterizing the visualization design space of distant and close reading of poetic rhythm},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MediLinker: A blockchain-based decentralized health
information management platform for patient-centric healthcare.
<em>FDATA</em>, <em>6</em>, 1146023. (<a
href="https://doi.org/10.3389/fdata.2023.1146023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients&#39; control over how their health information is stored has been an ongoing issue in health informatics. Currently, most patients&#39; health information is stored in centralized but siloed health information systems of healthcare institutions, rarely connected to or interoperable with other institutions outside of their specific health system. This centralized approach to the storage of health information is susceptible to breaches, though it can be mitigated using technology that allows for decentralized access. One promising technology that offers the possibility of decentralization, data protection, and interoperability is blockchain. In 2019, our interdisciplinary team from the University of Texas at Austin&#39;s Dell Medical School, School of Information, Department of Electrical and Computer Engineering, and Information Technology Services developed MediLinker—a blockchain-based decentralized health information management platform for patient-centric healthcare. This paper provides an overview of MediLinker and outlines its ongoing and future development and implementation. Overall, this paper contributes insights into the opportunities and challenges in developing and implementing blockchain-based technologies in healthcare.},
  archive      = {J_FDATA},
  author       = {Bautista, John Robert and Harrell, Daniel Toshio and Hanson, Ladd and de Oliveira, Eliel and Abdul-Moheeth, Mustafa and Meyer, Eric T. and Khurshid, Anjum},
  doi          = {10.3389/fdata.2023.1146023},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {1146023},
  shortjournal = {Front. Big Data},
  title        = {MediLinker: A blockchain-based decentralized health information management platform for patient-centric healthcare},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonstationary time series forecasting using
optimized-EVDHM-ARIMA for COVID-19. <em>FDATA</em>, <em>6</em>, 1081639.
(<a href="https://doi.org/10.3389/fdata.2023.1081639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Coronavirus (COVID-19) outbreak swept the world, infected millions of people, and caused many deaths. Multiple COVID-19 variations have been discovered since the initial case in December 2019, indicating that COVID-19 is highly mutable. COVID-19 variation “XE” is the most current of all COVID-19 variants found in January 2022. It is vital to detect the virus transmission rate and forecast instances of infection to be prepared for all scenarios, prepare healthcare services, and avoid deaths. Time-series forecasting helps predict future infected cases and determine the virus transmission rate to make timely decisions. A forecasting model for nonstationary time series has been created in this paper. The model comprises an optimized EigenValue Decomposition of Hankel Matrix (EVDHM) and an optimized AutoRegressive Integrated Moving Average (ARIMA). The Phillips Perron Test (PPT) has been used to determine whether a time series is nonstationary. A time series has been decomposed into components using EVDHM, and each component has been forecasted using ARIMA. The final forecasts have been formed by combining the predicted values of each component. A Genetic Algorithm (GA) to select ARIMA parameters resulting in the lowest Akaike Information Criterion (AIC) values has been used to discover the best ARIMA parameters. Another genetic algorithm has been used to optimize the decomposition results of EVDHM that ensures the minimum nonstationarity and maximal utilization of eigenvalues for each decomposed component.},
  archive      = {J_FDATA},
  author       = {Nagvanshi, Suraj Singh and Kaur, Inderjeet and Agarwal, Charu and Sharma, Ashish},
  doi          = {10.3389/fdata.2023.1081639},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {1081639},
  shortjournal = {Front. Big Data},
  title        = {Nonstationary time series forecasting using optimized-EVDHM-ARIMA for COVID-19},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). North-south scientific collaborations on research datasets:
A longitudinal analysis of the division of labor on genomic datasets
(1992–2021). <em>FDATA</em>, <em>6</em>, 1054655. (<a
href="https://doi.org/10.3389/fdata.2023.1054655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborations between scientists from the global north and global south (N-S collaborations) are a key driver of the “fourth paradigm of science” and have proven crucial to addressing global crises like COVID-19 and climate change. However, despite their critical role, N-S collaborations on datasets are not well understood. Science of science studies tend to rely on publications and patents to examine N-S collaboration patterns. To this end, the rise of global crises requiring N-S collaborations to produce and share data presents an urgent need to understand the prevalence, dynamics, and political economy of N-S collaborations on research datasets. In this paper, we employ a mixed methods case study research approach to analyze the frequency of and division of labor in N-S collaborations on datasets submitted to GenBank over 29 years (1992–2021). We find: (1) there is a low representation of N-S collaborations over the 29-year period. When they do occur, N-S collaborations display “burstiness” patterns, suggesting that N-S collaborations on datasets are formed and maintained reactively in the wake of global health crises such as infectious disease outbreaks; (2) The division of labor between datasets and publications is disproportionate to the global south in the early years, but becomes more overlapping after 2003. An exception in the case of countries with lower S&amp;amp;T capacity but high income, where these countries have a higher prevalence on datasets (e.g., United Arab Emirates). We qualitatively inspect a sample of N-S dataset collaborations to identify leadership patterns in dataset and publication authorship. The findings lead us to argue there is a need to include N-S dataset collaborations in measures of research outputs to nuance the current models and assessment tools of equity in N-S collaborations. The paper contributes to the SGDs objectives to develop data-driven metrics that can inform scientific collaborations on research datasets.},
  archive      = {J_FDATA},
  author       = {Bratt, Sarah and Langalia, Mrudang and Nanoti, Abhishek},
  doi          = {10.3389/fdata.2023.1054655},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {1054655},
  shortjournal = {Front. Big Data},
  title        = {North-south scientific collaborations on research datasets: A longitudinal analysis of the division of labor on genomic datasets (1992–2021)},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The influence of the pringle maneuver in laparoscopic
hepatectomy: Continuous monitor of hemodynamic change can predict the
perioperatively physiological reservation. <em>FDATA</em>, <em>6</em>,
1042516. (<a href="https://doi.org/10.3389/fdata.2023.1042516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ImportanceThis is the first study to investigate the correlation between intra-operative hemodynamic changes and postoperative physiological status.ObjectiveDesign, settings, and participantsPatients receiving laparoscopic hepatectomy were routinely monitored using FloTract for goal-directed fluid management. The Pringle maneuver was routinely performed during parenchymal dissection and the hemodynamic changes were prospectively recorded. We retrospectively analyzed the continuous hemodynamic data from FloTrac to compare with postoperative physiological outcomes.ExposureThe Pringle maneuver during laparoscopic hepatectomy.Main outcome(s) and measure(s)ResultsStroke volume variation that did not recover from the relief of the Pringle maneuver during the last application of Pringle maneuver predicted elevated postoperative MELD-Na scores.Conclusions and relevanceThe complexity of the hemodynamic data recorded by the FloTrac system during the Pringle Maneuver in laparoscopic hepatectomy can be effectively analyzed using the growth mixture modeling (GMM) method. The results can potentially predict the risk of short-term liver function deterioration.},
  archive      = {J_FDATA},
  author       = {Chen, Yi-Chan and Lee, Min-Hsuan and Hsueh, Shan-Ni and Liu, Chien-Liang and Hui, Chung-Kun and Soong, Ruey-Shyang},
  doi          = {10.3389/fdata.2023.1042516},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {1042516},
  shortjournal = {Front. Big Data},
  title        = {The influence of the pringle maneuver in laparoscopic hepatectomy: Continuous monitor of hemodynamic change can predict the perioperatively physiological reservation},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Does subnetting and port hardening influence human
adversarial decisions? An investigation via a HackIT tool.
<em>FDATA</em>, <em>6</em>, 988007. (<a
href="https://doi.org/10.3389/fdata.2023.988007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior research in cyber deception has investigated the effectiveness of the timing of deception on human decisions using simulation tools. However, there exists a gap in the literature on how the availability of subnets and port-hardening influence human decisions to attack a system. We tested the influence of subnets and port-hardening on human attack decisions in a simulated environment using the HackIT tool. Availability of subnets (present/absent) within a network and port-hardening (easy-to-attack/hard-to-attack) were varied across four between-subject conditions (N = 30 in each condition): with-subnet with easy-to-attack, with-subnet with hard-to-attack, without-subnet with easy-to-attack, and without-subnet with hard-to-attack. In with-subnet conditions, 40 systems were connected in a hybrid topology network with ten subnets connected linearly, and each subnet contained four connected systems. In without-subnet conditions, all 40 systems were connected in a bus topology. In hard-to-attack (easy-to-attack) conditions, the probabilities of successfully attacking real systems and honeypots were kept low (high) and high (low), respectively. In an experiment, human participants were randomly assigned to one of the four conditions to attack as many real systems as possible and steal credit card information. Results revealed a significant decrease in the proportion of real system attacks in the availability of subnetting and port hardening within the network. Also, more honeypots were attacked in with-subnet conditions than without-subnet conditions. Moreover, a significantly lower proportion of real systems were attacked in the port-hardened condition. This research highlights the implications of subnetting and port-hardening with honeypots to reduce real system attacks. These findings are relevant in developing advanced intrusion detection systems trained on hackers&#39; behavior.},
  archive      = {J_FDATA},
  author       = {Uttrani, Shashank and Aggarwal, Palvi and Dutt, Varun},
  doi          = {10.3389/fdata.2023.988007},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {988007},
  shortjournal = {Front. Big Data},
  title        = {Does subnetting and port hardening influence human adversarial decisions? an investigation via a HackIT tool},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Critical data and algorithm studies.
<em>FDATA</em>, <em>6</em>, 1193412. (<a
href="https://doi.org/10.3389/fdata.2023.1193412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Mayer, Katja and Pfeffer, Jürgen},
  doi          = {10.3389/fdata.2023.1193412},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1193412},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Critical data and algorithm studies},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AI-assisted diplomatic decision-making during
crises—challenges and opportunities. <em>FDATA</em>, <em>6</em>,
1183313. (<a href="https://doi.org/10.3389/fdata.2023.1183313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Pokhriyal, Neeti and Koebe, Till},
  doi          = {10.3389/fdata.2023.1183313},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1183313},
  shortjournal = {Front. Big Data},
  title        = {AI-assisted diplomatic decision-making during crises—Challenges and opportunities},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comprehensive review for healthcare data quality challenges
in blockchain technology. <em>FDATA</em>, <em>6</em>, 1173620. (<a
href="https://doi.org/10.3389/fdata.2023.1173620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are several features inherent in blockchain, including decentralized storage, distributed ledger, immutability, security and authentication, and it has shifted away from the hype to be used practically in different industries, such as in the healthcare sector. The use of blockchain technology has allowed the provision of improved services to industries. The objective of this paper is to demonstrate how the use of blockchain is influenced by data quality issues in the healthcare industry. The article is structured as a systematic literature review study that uses several articles issued in various databases from 2016 onwards. In this review study, 65 articles were chosen and grouped into a single key aspect of the challenge in the healthcare sector. The findings obtained were analyzed based on factors in three domains, classified as issues pertinent to the adoption, operational and technological domains. This review study aims to use the findings to provide support to the practitioners, stakeholders and professionals, whose purpose is to carry out and manage transformation projects pertinent to blockchain in the field of healthcare. In addition, the organizations would be facilitated in their decision-making processes when the potential blockchain users are made to comprehend the implicit factors related to blockchain.},
  archive      = {J_FDATA},
  author       = {AbuHalimeh, Ahmed and Ali, Omar},
  doi          = {10.3389/fdata.2023.1173620},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1173620},
  shortjournal = {Front. Big Data},
  title        = {Comprehensive review for healthcare data quality challenges in blockchain technology},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review on individual and multistakeholder fairness in
tourism recommender systems. <em>FDATA</em>, <em>6</em>, 1168692. (<a
href="https://doi.org/10.3389/fdata.2023.1168692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing use of Recommender Systems (RS) across various industries, including e-commerce, social media, news, travel, and tourism, has prompted researchers to examine these systems for any biases or fairness concerns. Fairness in RS is a multi-faceted concept ensuring fair outcomes for all stakeholders involved in the recommendation process, and its definition can vary based on the context and domain. This paper highlights the importance of evaluating RS from multiple stakeholders&#39; perspectives, specifically focusing on Tourism Recommender Systems (TRS). Stakeholders in TRS are categorized based on their main fairness criteria, and the paper reviews state-of-the-art research on TRS fairness from various viewpoints. It also outlines the challenges, potential solutions, and research gaps in developing fair TRS. The paper concludes that designing fair TRS is a multi-dimensional process that requires consideration not only of the other stakeholders but also of the environmental impact and effects of overtourism and undertourism.},
  archive      = {J_FDATA},
  author       = {Banerjee, Ashmi and Banik, Paromita and Wörndl, Wolfgang},
  doi          = {10.3389/fdata.2023.1168692},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1168692},
  shortjournal = {Front. Big Data},
  title        = {A review on individual and multistakeholder fairness in tourism recommender systems},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fused multi-modal similarity network as prior in guiding
brain imaging genetic association. <em>FDATA</em>, <em>6</em>, 1151893.
(<a href="https://doi.org/10.3389/fdata.2023.1151893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionBrain imaging genetics aims to explore the genetic architecture underlying brain structure and functions. Recent studies showed that the incorporation of prior knowledge, such as subject diagnosis information and brain regional correlation, can help identify significantly stronger imaging genetic associations. However, sometimes such information may be incomplete or even unavailable.MethodsIn this study, we explore a new data-driven prior knowledge that captures the subject-level similarity by fusing multi-modal similarity networks. It was incorporated into the sparse canonical correlation analysis (SCCA) model, which is aimed to identify a small set of brain imaging and genetic markers that explain the similarity matrix supported by both modalities. It was applied to amyloid and tau imaging data of the ADNI cohort, respectively.ResultsFused similarity matrix across imaging and genetic data was found to improve the association performance better or similarly well as diagnosis information, and therefore would be a potential substitute prior when the diagnosis information is not available (i.e., studies focused on healthy controls).DiscussionOur result confirmed the value of all types of prior knowledge in improving association identification. In addition, the fused network representing the subject relationship supported by multi-modal data showed consistently the best or equally best performance compared to the diagnosis network and the co-expression network.},
  archive      = {J_FDATA},
  author       = {He, Bing and Xie, Linhui and Varathan, Pradeep and Nho, Kwangsik and Risacher, Shannon L. and Saykin, Andrew J. and Yan, Jingwen and , The Alzheimer&#39;s Disease Neuroimaging Initiative},
  doi          = {10.3389/fdata.2023.1151893},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1151893},
  shortjournal = {Front. Big Data},
  title        = {Fused multi-modal similarity network as prior in guiding brain imaging genetic association},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Big data analytics and smart cities: Applications,
challenges, and opportunities. <em>FDATA</em>, <em>6</em>, 1149402. (<a
href="https://doi.org/10.3389/fdata.2023.1149402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban environments continuously generate larger and larger volumes of data, whose analysis can provide descriptive and predictive models as valuable support to inspire and develop data-driven Smart City applications. To this aim, Big data analysis and machine learning algorithms can play a fundamental role to bring improvements in city policies and urban issues. This paper introduces how Big Data analysis can be exploited to design and develop data-driven smart city services, and provides an overview on the most important Smart City applications, grouped in several categories. Then, it presents three real-case studies showing how data analysis methodologies can provide innovative solutions to deal with smart city issues. The first one is an approach for spatio-temporal crime forecasting (tested on Chicago crime data), the second one is methodology to discover mobility hotsposts and trajectory patterns from GPS data (tested on Beijing taxi traces), the third one is an approach to discover predictive epidemic patterns from mobility and infection data (tested on real COVID-19 data). The presented real-world cases prove that data analytics models can effectively support city managers in tackling smart city challenges and improving urban applications.},
  archive      = {J_FDATA},
  author       = {Cesario, Eugenio},
  doi          = {10.3389/fdata.2023.1149402},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1149402},
  shortjournal = {Front. Big Data},
  title        = {Big data analytics and smart cities: Applications, challenges, and opportunities},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling information diffusion in social media: Data-driven
observations. <em>FDATA</em>, <em>6</em>, 1135191. (<a
href="https://doi.org/10.3389/fdata.2023.1135191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately modeling information diffusion within and across social media platforms has many practical applications, such as estimating the size of the audience exposed to a particular narrative or testing intervention techniques for addressing misinformation. However, it turns out that real data reveal phenomena that pose significant challenges to modeling: events in the physical world affect in varying ways conversations on different social media platforms; coordinated influence campaigns may swing discussions in unexpected directions; a platform&#39;s algorithms direct who sees which message, which affects in opaque ways how information spreads. This article describes our research efforts in the SocialSim program of the Defense Advanced Research Projects Agency. As formulated by DARPA, the intent of the SocialSim research program was “to develop innovative technologies for high-fidelity computational simulation of online social behavior ... [focused] specifically on information spread and evolution.” In this article we document lessons we learned over the 4+ years of the recently concluded project. Our hope is that an accounting of our experience may prove useful to other researchers should they attempt a related project.},
  archive      = {J_FDATA},
  author       = {Iamnitchi, Adriana and Hall, Lawrence O. and Horawalavithana, Sameera and Mubang, Frederick and Ng, Kin Wai and Skvoretz, John},
  doi          = {10.3389/fdata.2023.1135191},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1135191},
  shortjournal = {Front. Big Data},
  title        = {Modeling information diffusion in social media: Data-driven observations},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Advances in AI for web integrity, equity, and well-being.
<em>FDATA</em>, <em>6</em>, 1125083. (<a
href="https://doi.org/10.3389/fdata.2023.1125083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {My research develops data mining, AI, and applied machine learning methods to combat malicious actors (sockpuppets, ban evaders, etc.) and dangerous content (misinformation, hate, etc.) on web platforms. My vision is to create a trustworthy online ecosystem for everyone and the next generation of socially-aware methods that promote health, equity, and integrity of users, communities, and platforms online. Broadly, in my research, I create novel graph, content (NLP, multimodality), and adversarial machine learning methods leveraging terabytes of data to detect, predict, and mitigate online threats. My interdisciplinary research innovates socio-technical solutions that I achieve by amalgamating computer science with social science theories. My research seeks to start a paradigm shift from the current slow and reactive approach against online harms to agile, proactive, and whole-of-society solutions. In this article, I shall describe my research efforts along four thrusts to achieve my goals: (1) Detection of harmful content and malicious actors across platforms, languages, and modalities; (2) Robust detection models against adversarial actors by predicting future malicious activities; (3) Attribution of the impact of harmful content in online and real world; and (4) Mitigation techniques to counter misinformation by professionals and non-expert crowds. Together, these thrusts give a set of holistic solutions to combat cyberharms. I am also passionate about putting my research into practice—my lab&#39;s models have been deployed on Flipkart, influenced Twitter&#39;s Birdwatch, and now being deployed on Wikipedia.},
  archive      = {J_FDATA},
  author       = {Kumar, Srijan},
  doi          = {10.3389/fdata.2023.1125083},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1125083},
  shortjournal = {Front. Big Data},
  title        = {Advances in AI for web integrity, equity, and well-being},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crime, inequality and public health: A survey of emerging
trends in urban data science. <em>FDATA</em>, <em>6</em>, 1124526. (<a
href="https://doi.org/10.3389/fdata.2023.1124526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban agglomerations are constantly and rapidly evolving ecosystems, with globalization and increasing urbanization posing new challenges in sustainable urban development well summarized in the United Nations&#39; Sustainable Development Goals (SDGs). The advent of the digital age generated by modern alternative data sources provides new tools to tackle these challenges with spatio-temporal scales that were previously unavailable with census statistics. In this review, we present how new digital data sources are employed to provide data-driven insights to study and track (i) urban crime and public safety; (ii) socioeconomic inequalities and segregation; and (iii) public health, with a particular focus on the city scale.},
  archive      = {J_FDATA},
  author       = {Luca, Massimiliano and Campedelli, Gian Maria and Centellegher, Simone and Tizzoni, Michele and Lepri, Bruno},
  doi          = {10.3389/fdata.2023.1124526},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1124526},
  shortjournal = {Front. Big Data},
  title        = {Crime, inequality and public health: A survey of emerging trends in urban data science},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sustainable energy policies from a complexity perspective.
<em>FDATA</em>, <em>6</em>, 1114796. (<a
href="https://doi.org/10.3389/fdata.2023.1114796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The energy policies of the countries have become a key aspect of development. They must be formulated to guarantee economic and social development, state security and compliance with the objectives of sustainable development. In this framework, generation technologies must be considered not only in terms of available natural resources but also in terms of possible contingency scenarios. The purpose of this article is to prioritize technologies by applying a fuzzy inference model and uncertainty model and to address the principles of complex thinking to a case study. The methodology considers the integral vision of the dimensions under the systemic, feedback, autonomy/dependence, holographic and recursive principles, the assignment of weights for the dimension of sustainable development and, finally, the formulation of contingent scenarios. These scenarios consider: exhaustion of a primary source and change of technology with negative or positive impact. As a result, priority is given to the development of wind technology among renewable sources, followed by hydropower and geothermal. In the field of conventional energy, natural gas remains in the first place, since it also reinforces the security and fairness of the system. It is concluded that the process of formulating energy policies based on economic variables and the incorporation of sustainability, in terms of restrictions and linearity in the study models. This must be complemented with the adaptation of the legal and institutional framework that allows the fulfillment of the objectives that are expected to be achieved. Finally, it is necessary to keep constantly updated on changes and improvements in technology, which can modify the variables under study, in order to adapt strategies to new conditions.},
  archive      = {J_FDATA},
  author       = {Acevedo-Rueda, Rubén Alexander and Ramírez-Pisco, Rodrigo and Vásquez Stanescu, Carmen Luisa and Torres Cruz, Ennodio José and Gómez-Caicedo, Melva Inés and Gaitán-Angulo, Mercedes},
  doi          = {10.3389/fdata.2023.1114796},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1114796},
  shortjournal = {Front. Big Data},
  title        = {Sustainable energy policies from a complexity perspective},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Just preparation for war and AI-enabled weapons.
<em>FDATA</em>, <em>6</em>, 1020107. (<a
href="https://doi.org/10.3389/fdata.2023.1020107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper maintains that the just war tradition provides a useful framework for analyzing ethical issues related to the development of weapons that incorporate artificial intelligence (AI), or “AI-enabled weapons.” While development of any weapon carries the risk of violations of jus ad bellum and jus in bello, AI-enabled weapons can pose distinctive risks of these violations. The article argues that developing AI-enabled weapons in accordance with jus ante bellum principles of just preparation for war can help minimize the risk of these violations. These principles impose two obligations. The first is that before deploying an AI-enabled weapon a state must rigorously test its safety and reliability, and conduct review of its ability to comply with international law. Second, a state must develop AI-enabled weapons in ways that minimize the likelihood that a security dilemma will arise, in which other states feel threatened by this development and hasten to deploy such weapons without sufficient testing and review. Ethical development of weapons that incorporate AI therefore requires that a state focus not only on its own activity, but on how that activity is perceived by other states.},
  archive      = {J_FDATA},
  author       = {Regan, Mitt and Davidovic, Jovana},
  doi          = {10.3389/fdata.2023.1020107},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {1020107},
  shortjournal = {Front. Big Data},
  title        = {Just preparation for war and AI-enabled weapons},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Biomedical heterogeneous data categorization and schema
mapping toward data integration. <em>FDATA</em>, <em>6</em>, 1173038.
(<a href="https://doi.org/10.3389/fdata.2023.1173038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data integration is a well-motivated problem in the clinical data science domain. Availability of patient data, reference clinical cases, and datasets for research have the potential to advance the healthcare industry. However, the unstructured (text, audio, or video data) and heterogeneous nature of the data, the variety of data standards and formats, and patient privacy constraint make data interoperability and integration a challenge. The clinical text is further categorized into different semantic groups and may be stored in different files and formats. Even the same organization may store cases in different data structures, making data integration more challenging. With such inherent complexity, domain experts and domain knowledge are often necessary to perform data integration. However, expert human labor is time and cost prohibitive. To overcome the variability in the structure, format, and content of the different data sources, we map the text into common categories and compute similarity within those. In this paper, we present a method to categorize and merge clinical data by considering the underlying semantics behind the cases and use reference information about the cases to perform data integration. Evaluation shows that we were able to merge 88% of clinical data from five different sources.},
  archive      = {J_FDATA},
  author       = {Deshpande, Priya and Rasin, Alexander and Tchoua, Roselyne and Furst, Jacob and Raicu, Daniela and Schinkel, Michiel and Trivedi, Hari and Antani, Sameer},
  doi          = {10.3389/fdata.2023.1173038},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {1173038},
  shortjournal = {Front. Big Data},
  title        = {Biomedical heterogeneous data categorization and schema mapping toward data integration},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Factors for the implementation of the circular economy in
big data environments in service companies in post pandemic times of
COVID-19: The case of colombia. <em>FDATA</em>, <em>6</em>, 1156780. (<a
href="https://doi.org/10.3389/fdata.2023.1156780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In emerging economies, Big Data (BD) analytics has become increasingly popular, particularly regarding the opportunities and expected benefits. Such analyzes have identified that the production and consumption of goods and services, while unavoidable, have proven to be unsustainable and inefficient. For this reason, the concept of the circular economy (CE) has emerged strongly as a sustainable approach that contributes to the eco-efficient use of resources. However, to develop a circular economy in DB environments, it is necessary to understand what factors influence the intention to accept its implementation. The main objective of this research was to assess the influence of attitudes, subjective norms, and perceived behavioral norms on the intention to adopt CE in BD-mediated environments. The methodology is quantitative, cross-sectional with a descriptive correlational approach, based on the theory of planned behavior and a Partial Least Squares Structural Equation Model (PLS-SEM). A total of 413 Colombian service SMEs participated in the study. The results show that managers&#39; attitudes, subjective norms, and perceived norms of behavior positively influence the intentions of organizations to implement CB best practices. Furthermore, most organizations have positive intentions toward CE and that these intentions positively influence the adoption of DB; however, the lack of government support and cultural barriers are perceived as the main limitation for its adoption. The research leads to the conclusion that BD helps business and government develop strategies to move toward CE, and that there is a clear positive will and intent toward a more restorative and sustainable corporate strategy.},
  archive      = {J_FDATA},
  author       = {Almanza Junco, Carlos Alberto and Pulido Ramirez, Marial del Pilar and Gaitán Angulo, Mercedes and Gómez-Caicedo, Melva Inés and Mercado Suárez, Álvaro Luis},
  doi          = {10.3389/fdata.2023.1156780},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {1156780},
  shortjournal = {Front. Big Data},
  title        = {Factors for the implementation of the circular economy in big data environments in service companies in post pandemic times of COVID-19: The case of colombia},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparative study of minutiae selection methods for digital
fingerprints. <em>FDATA</em>, <em>6</em>, 1146034. (<a
href="https://doi.org/10.3389/fdata.2023.1146034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric systems are more and more used for many applications (physical access control, e-payment, etc.). Digital fingerprint is an interesting biometric modality as it can easily be used for embedded systems (smartcard, smartphone, and smartwatch). A fingerprint template is composed of a set of minutiae used for their comparison. In embedded systems, a secure element is in general used to store and compare fingerprint templates to meet security and privacy requirements. Nevertheless, it is necessary to select a subset of minutiae from a template due to storage and computation constraints. In this study, we present, a comparative study of the main minutiae selection methods from the literature. The considered methods require no further information like the raw image. Experimental results show their relative performance when using different matching algorithms and datasets. We identified that some methods can be used within different contexts (enrollment or verification) with minimal degradation of performance.},
  archive      = {J_FDATA},
  author       = {Vibert, Benoit and Le Bars, Jean-Marie and Charrier, Christophe and Rosenberger, Christophe},
  doi          = {10.3389/fdata.2023.1146034},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {1146034},
  shortjournal = {Front. Big Data},
  title        = {Comparative study of minutiae selection methods for digital fingerprints},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network experiment designs for inferring causal effects
under interference. <em>FDATA</em>, <em>6</em>, 1128649. (<a
href="https://doi.org/10.3389/fdata.2023.1128649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current approaches to A/B testing in networks focus on limiting interference, the concern that treatment effects can “spill over” from treatment nodes to control nodes and lead to biased causal effect estimation. In the presence of interference, two main types of causal effects are direct treatment effects and total treatment effects. In this paper, we propose two network experiment designs that increase the accuracy of direct and total effect estimations in network experiments through minimizing interference between treatment and control units. For direct treatment effect estimation, we present a framework that takes advantage of independent sets and assigns treatment and control only to a set of non-adjacent nodes in a graph, in order to disentangle peer effects from direct treatment effect estimation. For total treatment effect estimation, our framework combines weighted graph clustering and cluster matching approaches to jointly minimize interference and selection bias. Through a series of simulated experiments on synthetic and real-world network datasets, we show that our designs significantly increase the accuracy of direct and total treatment effect estimation in network experiments.},
  archive      = {J_FDATA},
  author       = {Fatemi, Zahra and Zheleva, Elena},
  doi          = {10.3389/fdata.2023.1128649},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {1128649},
  shortjournal = {Front. Big Data},
  title        = {Network experiment designs for inferring causal effects under interference},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AI-based radiodiagnosis using chest x-rays: A review.
<em>FDATA</em>, <em>6</em>, 1120989. (<a
href="https://doi.org/10.3389/fdata.2023.1120989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chest Radiograph or Chest X-ray (CXR) is a common, fast, non-invasive, relatively cheap radiological examination method in medical sciences. CXRs can aid in diagnosing many lung ailments such as Pneumonia, Tuberculosis, Pneumoconiosis, COVID-19, and lung cancer. Apart from other radiological examinations, every year, 2 billion CXRs are performed worldwide. However, the availability of the workforce to handle this amount of workload in hospitals is cumbersome, particularly in developing and low-income nations. Recent advances in AI, particularly in computer vision, have drawn attention to solving challenging medical image analysis problems. Healthcare is one of the areas where AI/ML-based assistive screening/diagnostic aid can play a crucial part in social welfare. However, it faces multiple challenges, such as small sample space, data privacy, poor quality samples, adversarial attacks and most importantly, the model interpretability for reliability on machine intelligence. This paper provides a structured review of the CXR-based analysis for different tasks, lung diseases and, in particular, the challenges faced by AI/ML-based systems for diagnosis. Further, we provide an overview of existing datasets, evaluation metrics for different[][15mm][0mm]Q5 tasks and patents issued. We also present key challenges and open problems in this research domain.},
  archive      = {J_FDATA},
  author       = {Akhter, Yasmeena and Singh, Richa and Vatsa, Mayank},
  doi          = {10.3389/fdata.2023.1120989},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {1120989},
  shortjournal = {Front. Big Data},
  title        = {AI-based radiodiagnosis using chest X-rays: A review},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human behavior in the time of COVID-19: Learning from big
data. <em>FDATA</em>, <em>6</em>, 1099182. (<a
href="https://doi.org/10.3389/fdata.2023.1099182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the World Health Organization (WHO) characterized COVID-19 as a pandemic in March 2020, there have been over 600 million confirmed cases of COVID-19 and more than six million deaths as of October 2022. The relationship between the COVID-19 pandemic and human behavior is complicated. On one hand, human behavior is found to shape the spread of the disease. On the other hand, the pandemic has impacted and even changed human behavior in almost every aspect. To provide a holistic understanding of the complex interplay between human behavior and the COVID-19 pandemic, researchers have been employing big data techniques such as natural language processing, computer vision, audio signal processing, frequent pattern mining, and machine learning. In this study, we present an overview of the existing studies on using big data techniques to study human behavior in the time of the COVID-19 pandemic. In particular, we categorize these studies into three groups—using big data to measure, model, and leverage human behavior, respectively. The related tasks, data, and methods are summarized accordingly. To provide more insights into how to fight the COVID-19 pandemic and future global catastrophes, we further discuss challenges and potential opportunities.},
  archive      = {J_FDATA},
  author       = {Lyu, Hanjia and Imtiaz, Arsal and Zhao, Yufei and Luo, Jiebo},
  doi          = {10.3389/fdata.2023.1099182},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {1099182},
  shortjournal = {Front. Big Data},
  title        = {Human behavior in the time of COVID-19: Learning from big data},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The myth of reproducibility: A review of event tracking
evaluations on twitter. <em>FDATA</em>, <em>6</em>, 1067335. (<a
href="https://doi.org/10.3389/fdata.2023.1067335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event tracking literature based on Twitter does not have a state-of-the-art. What it does have is a plethora of manual evaluation methodologies and inventive automatic alternatives: incomparable and irreproducible studies incongruous with the idea of a state-of-the-art. Many researchers blame Twitter&#39;s data sharing policy for the lack of common datasets and a universal ground truth–for the lack of reproducibility–but many other issues stem from the conscious decisions of those same researchers. In this paper, we present the most comprehensive review yet on event tracking literature&#39;s evaluations on Twitter. We explore the challenges of manual experiments, the insufficiencies of automatic analyses and the misguided notions on reproducibility. Crucially, we discredit the widely-held belief that reusing tweet datasets could induce reproducibility. We reveal how tweet datasets self-sanitize over time; how spam and noise become unavailable at much higher rates than legitimate content, rendering downloaded datasets incomparable with the original. Nevertheless, we argue that Twitter&#39;s policy can be a hindrance without being an insurmountable barrier, and propose how the research community can make its evaluations more reproducible. A state-of-the-art remains attainable for event tracking research.},
  archive      = {J_FDATA},
  author       = {Mamo, Nicholas and Azzopardi, Joel and Layfield, Colin},
  doi          = {10.3389/fdata.2023.1067335},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {1067335},
  shortjournal = {Front. Big Data},
  title        = {The myth of reproducibility: A review of event tracking evaluations on twitter},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum: SemNet: Learning semantic attributes for human
activity recognition with deep belief networks. <em>FDATA</em>,
<em>6</em>, 1170820. (<a
href="https://doi.org/10.3389/fdata.2023.1170820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Venkatachalam, Shanmuga and Nair, Harideep and Zeng, Ming and Tan, Cathy Shunwen and Mengshoel, Ole J. and Shen, John Paul},
  doi          = {10.3389/fdata.2023.1170820},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1170820},
  shortjournal = {Front. Big Data},
  title        = {Corrigendum: SemNet: learning semantic attributes for human activity recognition with deep belief networks},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on multi-objective recommender systems.
<em>FDATA</em>, <em>6</em>, 1157899. (<a
href="https://doi.org/10.3389/fdata.2023.1157899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems can be characterized as software solutions that provide users with convenient access to relevant content. Traditionally, recommender systems research predominantly focuses on developing machine learning algorithms that aim to predict which content is relevant for individual users. In real-world applications, however, optimizing the accuracy of such relevance predictions as a single objective in many cases is not sufficient. Instead, multiple and often competing objectives, e.g., long-term vs. short-term goals, have to be considered, leading to a need for more research in multi-objective recommender systems. We can differentiate between several types of such competing goals, including (i) competing recommendation quality objectives at the individual and aggregate level, (ii) competing objectives of different involved stakeholders, (iii) long-term vs. short-term objectives, (iv) objectives at the user interface level, and (v) engineering related objectives. In this paper, we review these types of multi-objective recommendation settings and outline open challenges in this area.1},
  archive      = {J_FDATA},
  author       = {Jannach, Dietmar and Abdollahpouri, Himan},
  doi          = {10.3389/fdata.2023.1157899},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1157899},
  shortjournal = {Front. Big Data},
  title        = {A survey on multi-objective recommender systems},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Automated data curation and data governance
automation. <em>FDATA</em>, <em>6</em>, 1148331. (<a
href="https://doi.org/10.3389/fdata.2023.1148331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Talburt, John R. and Ehrlinger, Lisa and Magruder, Justin},
  doi          = {10.3389/fdata.2023.1148331},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1148331},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Automated data curation and data governance automation},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised domain adaptation methods for cross-species
transfer of regulatory code signals. <em>FDATA</em>, <em>6</em>,
1140663. (<a href="https://doi.org/10.3389/fdata.2023.1140663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to advances in NGS technologies whole-genome maps of various functional genomic elements were generated for a dozen of species, however experiments are still expensive and are not available for many species of interest. Deep learning methods became the state-of-the-art computational methods to analyze the available data, but the focus is often only on the species studied. Here we take advantage of the progresses in Transfer Learning in the area of Unsupervised Domain Adaption (UDA) and tested nine UDA methods for prediction of regulatory code signals for genomes of other species. We tested each deep learning implementation by training the model on experimental data from one species, then refined the model using the genome sequence of the target species for which we wanted to make predictions. Among nine tested domain adaptation architectures non-adversarial methods Minimum Class Confusion (MCC) and Deep Adaptation Network (DAN) significantly outperformed others. Conditional Domain Adversarial Network (CDAN) appeared as the third best architecture. Here we provide an empirical assessment of each approach using real world data. The different approaches were tested on ChIP-seq data for transcription factor binding sites and histone marks on human and mouse genomes, but is generalizable to any cross-species transfer of interest. We tested the efficiency of each method using species where experimental data was available for both. The results allows us to assess how well each implementation will work for species for which only limited experimental data is available and will inform the design of future experiments in these understudied organisms. Overall, our results proved the validity of UDA methods for generation of missing experimental data for histone marks and transcription factor binding sites in various genomes and highlights how robust the various approaches are to data that is incomplete, noisy and susceptible to analytic bias.},
  archive      = {J_FDATA},
  author       = {Latyshev, Pavel and Pavlov, Fedor and Herbert, Alan and Poptsova, Maria},
  doi          = {10.3389/fdata.2023.1140663},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1140663},
  shortjournal = {Front. Big Data},
  title        = {Unsupervised domain adaptation methods for cross-species transfer of regulatory code signals},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A proposed scenario to improve the ncut algorithm in
segmentation. <em>FDATA</em>, <em>6</em>, 1134946. (<a
href="https://doi.org/10.3389/fdata.2023.1134946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image segmentation, there are many methods to accomplish the result of segmenting an image into k clusters. However, the number of clusters k is always defined before running the process. It is defined by some observation or knowledge based on the application. In this paper, we propose a new scenario in order to define the value k clusters automatically using histogram information. This scenario is applied to Ncut algorithm and speeds up the running time by using CUDA language to parallel computing in GPU. The Ncut is improved in four steps: determination of number of clusters in segmentation, computing the similarity matrix W, computing the similarity matrix&#39;s eigenvalues, and grouping on the Fuzzy C-Means (FCM) clustering algorithm. Some experimental results are shown to prove that our scenario is 20 times faster than the Ncut algorithm while keeping the same accuracy.},
  archive      = {J_FDATA},
  author       = {Tran, Nhu Y. and Hieu, Huynh Trung and Bao, Pham The},
  doi          = {10.3389/fdata.2023.1134946},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1134946},
  shortjournal = {Front. Big Data},
  title        = {A proposed scenario to improve the ncut algorithm in segmentation},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Google trends for health research: Its advantages,
application, methodological considerations, and limitations in
psychiatric and mental health infodemiology. <em>FDATA</em>, <em>6</em>,
1132764. (<a href="https://doi.org/10.3389/fdata.2023.1132764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high utilization of infodemiological tools for psychiatric and mental health topics signals the emergence of a new discipline. Drawing on the definition of infodemiology by Eysenbach, this emerging field can be termed “psychiatric and mental health infodemiology,” defined as the science of distribution and determinants of information in an electronic medium, including the internet, or in a population to inform mental health services and policies. Since Google Trends is one of its popular tools, this minireview describes its advantages, application, methodological considerations, and limitations in psychiatric and mental health research. The advantage of Google Trends is the nature of its data, which may represent the actual behavior rather than their users&#39; stated preferences in real-time through automatic anonymization. As such, it can provide readily available data about sensitive health topics like mental disorders. Therefore, Google Trends has been used to explore public concerns, interests, and behaviors about psychiatric and mental health phenomena, service providers, and specific disciplines. In this regard, several methodological can be considered by studies using Google Trends, including documenting their exact keywords, query category, time range, location, and date of retrieval. Likewise, its limitations should be accounted for in its interpretation, including restricted representation of people who use the Google search engine, limited validity in areas with low internet penetration or freedom of speech, does not provide absolute search volumes, unknown sampled queries, and limited transparency in its algorithm, especially the terms and idioms it subsumes under its “topic” keywords.},
  archive      = {J_FDATA},
  author       = {Alibudbud, Rowalt},
  doi          = {10.3389/fdata.2023.1132764},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1132764},
  shortjournal = {Front. Big Data},
  title        = {Google trends for health research: Its advantages, application, methodological considerations, and limitations in psychiatric and mental health infodemiology},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CRMnet: A deep learning model for predicting gene expression
from large regulatory sequence datasets. <em>FDATA</em>, <em>6</em>,
1113402. (<a href="https://doi.org/10.3389/fdata.2023.1113402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent large datasets measuring the gene expression of millions of possible gene promoter sequences provide a resource to design and train optimized deep neural network architectures to predict expression from sequences. High predictive performance due to the modeling of dependencies within and between regulatory sequences is an enabler for biological discoveries in gene regulation through model interpretation techniques. To understand the regulatory code that delineates gene expression, we have designed a novel deep-learning model (CRMnet) to predict gene expression in Saccharomyces cerevisiae. Our model outperforms the current benchmark models and achieves a Pearson correlation coefficient of 0.971 and a mean squared error of 3.200. Interpretation of informative genomic regions determined from model saliency maps, and overlapping the saliency maps with known yeast motifs, supports that our model can successfully locate the binding sites of transcription factors that actively modulate gene expression. We compare our model&#39;s training times on a large compute cluster with GPUs and Google TPUs to indicate practical training times on similar datasets.},
  archive      = {J_FDATA},
  author       = {Ding, Ke and Dixit, Gunjan and Parker, Brian J. and Wen, Jiayu},
  doi          = {10.3389/fdata.2023.1113402},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1113402},
  shortjournal = {Front. Big Data},
  title        = {CRMnet: A deep learning model for predicting gene expression from large regulatory sequence datasets},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NuSegDA: Domain adaptation for nuclei segmentation.
<em>FDATA</em>, <em>6</em>, 1108659. (<a
href="https://doi.org/10.3389/fdata.2023.1108659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate segmentation of nuclei is crucial for cancer diagnosis and further clinical treatments. To successfully train a nuclei segmentation network in a fully-supervised manner for a particular type of organ or cancer, we need the dataset with ground-truth annotations. However, such well-annotated nuclei segmentation datasets are highly rare, and manually labeling an unannotated dataset is an expensive, time-consuming, and tedious process. Consequently, we require to discover a way for training the nuclei segmentation network with unlabeled dataset. In this paper, we propose a model named NuSegUDA for nuclei segmentation on the unlabeled dataset (target domain). It is achieved by applying Unsupervised Domain Adaptation (UDA) technique with the help of another labeled dataset (source domain) that may come from different type of organ, cancer, or source. We apply UDA technique at both of feature space and output space. We additionally utilize a reconstruction network and incorporate adversarial learning into it so that the source-domain images can be accurately translated to the target-domain for further training of the segmentation network. We validate our proposed NuSegUDA on two public nuclei segmentation datasets, and obtain significant improvement as compared with the baseline methods. Extensive experiments also verify the contribution of newly proposed image reconstruction adversarial loss, and target-translated source supervised loss to the performance boost of NuSegUDA. Finally, considering the scenario when we have a small number of annotations available from the target domain, we extend our work and propose NuSegSSDA, a Semi-Supervised Domain Adaptation (SSDA) based approach.},
  archive      = {J_FDATA},
  author       = {Haq, Mohammad Minhazul and Ma, Hehuan and Huang, Junzhou},
  doi          = {10.3389/fdata.2023.1108659},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1108659},
  shortjournal = {Front. Big Data},
  title        = {NuSegDA: Domain adaptation for nuclei segmentation},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Natural language processing for humanitarian action:
Opportunities, challenges, and the path toward humanitarian NLP.
<em>FDATA</em>, <em>6</em>, 1082787. (<a
href="https://doi.org/10.3389/fdata.2023.1082787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP) is a rapidly evolving field at the intersection of linguistics, computer science, and artificial intelligence, which is concerned with developing methods to process and generate language at scale. Modern NLP tools have the potential to support humanitarian action at multiple stages of the humanitarian response cycle. Both internal reports, secondary text data (e.g., social media data, news media articles, or interviews with affected individuals), and external-facing documents like Humanitarian Needs Overviews (HNOs) encode information relevant to monitoring, anticipating, or responding to humanitarian crises. Yet, lack of awareness of the concrete opportunities offered by state-of-the-art techniques, as well as constraints posed by resource scarcity, limit adoption of NLP tools in the humanitarian sector. This paper provides a pragmatically-minded primer to the emerging field of humanitarian NLP, reviewing existing initiatives in the space of humanitarian NLP, highlighting potentially impactful applications of NLP in the humanitarian sector, and describing criteria, challenges, and potential solutions for large-scale adoption. In addition, as one of the main bottlenecks is the lack of data and standards for this domain, we present recent initiatives (the DEEP and HumSet) which are directly aimed at addressing these gaps. With this work, we hope to motivate humanitarians and NLP experts to create long-term impact-driven synergies and to co-develop an ambitious roadmap for the field.},
  archive      = {J_FDATA},
  author       = {Rocca, Roberta and Tamagnone, Nicolò and Fekih, Selim and Contla, Ximena and Rekabsaz, Navid},
  doi          = {10.3389/fdata.2023.1082787},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1082787},
  shortjournal = {Front. Big Data},
  title        = {Natural language processing for humanitarian action: Opportunities, challenges, and the path toward humanitarian NLP},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The detection and effect of social events on wikipedia
data-set for studying human preferences. <em>FDATA</em>, <em>6</em>,
1077318. (<a href="https://doi.org/10.3389/fdata.2023.1077318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several studies have used Wikipedia (WP) data-set to analyse worldwide human preferences by languages. However, those studies could suffer from bias related to exceptional social circumstances. Any massive event promoting exceptional editions of WP can be defined as a source of bias. In this article, we follow a procedure for detecting outliers. Our study is based on 12 languages and 13 different categories. Our methodology defines a parameter, which is language-dependent instead of being externally fixed. We also study the presence of human cyclic behavior to evaluate apparent outliers. After our analysis, we found that the outliers in our data-set do not significantly affect the analysis of preferences by categories among different WP languages. While investigating the possibility of bias related to exceptional social circumstances is always a safe measure before doing any analysis on Big Data, we found that in the case of the first ten years of the Wikipedia data-set, outliers do not significantly affect using Wikipedia data-set as a digital footprint to analyse worldwide human preferences.},
  archive      = {J_FDATA},
  author       = {Assuied, Julien and Gandica, Yérali},
  doi          = {10.3389/fdata.2023.1077318},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1077318},
  shortjournal = {Front. Big Data},
  title        = {The detection and effect of social events on wikipedia data-set for studying human preferences},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A large-scale machine learning study of sociodemographic
factors contributing to COVID-19 severity. <em>FDATA</em>, <em>6</em>,
1038283. (<a href="https://doi.org/10.3389/fdata.2023.1038283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding sociodemographic factors behind COVID-19 severity relates to significant methodological difficulties, such as differences in testing policies and epidemics phase, as well as a large number of predictors that can potentially contribute to severity. To account for these difficulties, we assemble 115 predictors for more than 3,000 US counties and employ a well-defined COVID-19 severity measure derived from epidemiological dynamics modeling. We then use a number of advanced feature selection techniques from machine learning to determine which of these predictors significantly impact the disease severity. We obtain a surprisingly simple result, where only two variables are clearly and robustly selected—population density and proportion of African Americans. Possible causes behind this result are discussed. We argue that the approach may be useful whenever significant determinants of disease progression over diverse geographic regions should be selected from a large number of potentially important factors.},
  archive      = {J_FDATA},
  author       = {Tumbas, Marko and Markovic, Sofija and Salom, Igor and Djordjevic, Marko},
  doi          = {10.3389/fdata.2023.1038283},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1038283},
  shortjournal = {Front. Big Data},
  title        = {A large-scale machine learning study of sociodemographic factors contributing to COVID-19 severity},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Examining the relationship between big data analytics
capabilities and organizational ambidexterity in the malaysian banking
sector. <em>FDATA</em>, <em>6</em>, 1036174. (<a
href="https://doi.org/10.3389/fdata.2023.1036174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drawing on previous literature on dynamic capability view (DCV), we examine the effects of data analytics capabilities (BDAC) on organizational ambidexterity and the paradoxical tensions between exploration and exploitation in the Malaysian banking sector. Although banks are often considered as mature commercial organizations, they are not free of issues concerning technological advancement and organizational changes for long-term competitiveness. Through statistical analysis by using data from 162 bank managers in Malaysia, it is confirmed that BDAC positively influences the two contradictory aspects of organizational ambidexterity (i.e., explorative dynamic capabilities and exploitative dynamic capabilities), and explorative dynamic capabilities also mediate the positive relationship between BDAC and exploitative marketing capabilities. The findings provide meaningful insights to researchers and bank managers on how to obtain sustainable competitive advances in the current digital era.},
  archive      = {J_FDATA},
  author       = {Aziz, Norzalita Abd and Long, Fei},
  doi          = {10.3389/fdata.2023.1036174},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {1036174},
  shortjournal = {Front. Big Data},
  title        = {Examining the relationship between big data analytics capabilities and organizational ambidexterity in the malaysian banking sector},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personalized diversification of complementary
recommendations with user preference in online grocery. <em>FDATA</em>,
<em>6</em>, 974072. (<a
href="https://doi.org/10.3389/fdata.2023.974072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complementary recommendations play an important role in surfacing the relevant items to the customers. In the cross-selling scenario, some customers might present more exploratory shopping behaviors and prefer more diverse complements, while other customers show less exploratory (or more conventional) shopping behaviors and want to have a deep dive of less diverse types of complements. The existence of two distinct shopping behaviors reflects users&#39; different shopping intents and requires complementary recommendations to be adaptable based on the user&#39;s shopping intent. Although many studies focus on improving the recommendations through post-processing techniques, such as user-item-level personalized ranking and diversification of recommendations, they fail to address such a requirement. First, many user-item-level personalization methods cannot explicitly model the preference of users in two types of shopping behaviors and their intent on the corresponding complementary recommendations. Second, most of the diversification methods increase the heterogeneity of the recommendations. However, users&#39; intent on conventional complementary shopping requires more homogeneity of the recommendations, which is not explicitly modeled. The present study tries attempts to solve these problems by the personalized diversification strategies for complementary recommendations. To address the requirement of modeling heterogenized and homogenized complementary recommendations, we propose two diversification strategies, heterogenization and homogenization, to re-rank complementary recommendations based on the determinantal point process (DPP). We use transaction history to estimate users&#39; intent on more exploratory or more conventional complementary shopping. With the estimated user intent scores and two diversification strategies, we propose an algorithm to personalize the diversification strategies dynamically. We demonstrate the effectiveness of our re-ranking algorithm on the publicly available Instacart dataset.},
  archive      = {J_FDATA},
  author       = {Ma, Luyi and Sinha, Nimesh and Cho, Jason H. D. and Kumar, Sushant and Achan, Kannan},
  doi          = {10.3389/fdata.2023.974072},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {974072},
  shortjournal = {Front. Big Data},
  title        = {Personalized diversification of complementary recommendations with user preference in online grocery},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CURTAINs for your sliding window: Constructing unobserved
regions by transforming adjacent intervals. <em>FDATA</em>, <em>6</em>,
899345. (<a href="https://doi.org/10.3389/fdata.2023.899345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new model independent technique for constructing background data templates for use in searches for new physics processes at the LHC. This method, called Curtains, uses invertible neural networks to parameterise the distribution of side band data as a function of the resonant observable. The network learns a transformation to map any data point from its value of the resonant observable to another chosen value. Using Curtains, a template for the background data in the signal window is constructed by mapping the data from the side-bands into the signal region. We perform anomaly detection using the Curtains background template to enhance the sensitivity to new physics in a bump hunt. We demonstrate its performance in a sliding window search across a wide range of mass values. Using the LHC Olympics dataset, we demonstrate that Curtains matches the performance of other leading approaches which aim to improve the sensitivity of bump hunts, can be trained on a much smaller range of the invariant mass, and is fully data driven.},
  archive      = {J_FDATA},
  author       = {Raine, John Andrew and Klein, Samuel and Sengupta, Debajyoti and Golling, Tobias},
  doi          = {10.3389/fdata.2023.899345},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {899345},
  shortjournal = {Front. Big Data},
  title        = {CURTAINs for your sliding window: Constructing unobserved regions by transforming adjacent intervals},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: AI and data science in drug development and
public health: Highlights from the MCBIOS 2022 conference.
<em>FDATA</em>, <em>6</em>, 1156811. (<a
href="https://doi.org/10.3389/fdata.2023.1156811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Homayouni, Ramin and Manda, Prashanti and Tan, Aik Choon and Qin, Zhaohui S.},
  doi          = {10.3389/fdata.2023.1156811},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {1156811},
  shortjournal = {Front. Big Data},
  title        = {Editorial: AI and data science in drug development and public health: highlights from the MCBIOS 2022 conference},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning-based ozone and PM2.5 forecasting:
Application to multiple AQS sites in the pacific northwest.
<em>FDATA</em>, <em>6</em>, 1124148. (<a
href="https://doi.org/10.3389/fdata.2023.1124148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air quality in the Pacific Northwest (PNW) of the U.S has generally been good in recent years, but unhealthy events were observed due to wildfires in summer or wood burning in winter. The current air quality forecasting system, which uses chemical transport models (CTMs), has had difficulty forecasting these unhealthy air quality events in the PNW. We developed a machine learning (ML) based forecasting system, which consists of two components, ML1 (random forecast classifiers and multiple linear regression models) and ML2 (two-phase random forest regression model). Our previous study showed that the ML system provides reliable forecasts of O3 at a single monitoring site in Kennewick, WA. In this paper, we expand the ML forecasting system to predict both O3 in the wildfire season and PM2.5 in wildfire and cold seasons at all available monitoring sites in the PNW during 2017–2020, and evaluate our ML forecasts against the existing operational CTM-based forecasts. For O3, both ML1 and ML2 are used to achieve the best forecasts, which was the case in our previous study: ML2 performs better overall (R2 = 0.79), especially for low-O3 events, while ML1 correctly captures more high-O3 events. Compared to the CTM-based forecast, our O3 ML forecasts reduce the normalized mean bias (NMB) from 7.6 to 2.6% and normalized mean error (NME) from 18 to 12% when evaluating against the observation. For PM2.5, ML2 performs the best and thus is used for the final forecasts. Compared to the CTM-based PM2.5, ML2 clearly improves PM2.5 forecasts for both wildfire season (May to September) and cold season (November to February): ML2 reduces NMB (−27 to 7.9% for wildfire season; 3.4 to 2.2% for cold season) and NME (59 to 41% for wildfires season; 67 to 28% for cold season) significantly and captures more high-PM2.5 events correctly. Our ML air quality forecast system requires fewer computing resources and fewer input datasets, yet it provides more reliable forecasts than (if not, comparable to) the CTM-based forecast. It demonstrates that our ML system is a low-cost, reliable air quality forecasting system that can support regional/local air quality management.},
  archive      = {J_FDATA},
  author       = {Fan, Kai and Dhammapala, Ranil and Harrington, Kyle and Lamb, Brian and Lee, Yunha},
  doi          = {10.3389/fdata.2023.1124148},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {1124148},
  shortjournal = {Front. Big Data},
  title        = {Machine learning-based ozone and PM2.5 forecasting: Application to multiple AQS sites in the pacific northwest},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining environmental and socioeconomic data to understand
determinants of conflicts in colombia. <em>FDATA</em>, <em>6</em>,
1107785. (<a href="https://doi.org/10.3389/fdata.2023.1107785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conflicts cause immense human suffering, violate human rights, and affect people&#39;s stability. Colombia is affected for decades by a high level of armed conflicts and violence. The political and socio-economic situation, drug trafficking in the Colombian economy, and natural disasters events affect the country and foster general violence. In this work, we aim to evaluate the role of the socioeconomic, political, financial, and environmental determinants of conflicts in the Colombian context. To achieve these aims, we apply a spatial analysis to explore patterns and identify areas that suffer from high levels of conflict. We investigate the role of determinants and their relationship with conflicts through spatial regression models. In this study, we do not consider only the entire Colombian territory, but we extend the analysis to a restricted area (Norte de Santander department) to explore the phenomena locally. Our findings indicate a possible diffusion process of conflicts and the presence of spillover effects among regions by comparing the two most known spatial regression models. As regards possible key drivers of conflicts, our results show that surprisingly socioeconomic variables present very little relationship with conflicts, while natural disasters and cocaine areas show a relevant impact on them. Despite some variables seeming to be the more informative to explain the process globally, they highlight a strong relationship for only a few specific areas while considering a local analysis. This result proves the importance of moving to a local investigation to strengthen our understanding and bring out additional interesting information. Our work emphasizes how the identification of key drivers of violence is crucial to have evidence to inform subnational governments and to support the decision-making policies that could assess targeted policy options.},
  archive      = {J_FDATA},
  author       = {Fiandrino, Stefania and Cattuto, Ciro and Paolotti, Daniela and Schifanella, Rossano},
  doi          = {10.3389/fdata.2023.1107785},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {1107785},
  shortjournal = {Front. Big Data},
  title        = {Combining environmental and socioeconomic data to understand determinants of conflicts in colombia},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fresh takes on five health data sharing domains: Quality,
privacy, equity, incentives, and sustainability. <em>FDATA</em>,
<em>6</em>, 1095119. (<a
href="https://doi.org/10.3389/fdata.2023.1095119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As entities around the world invest in repositories and other infrastructure to facilitate health data sharing, scalable solutions to data sharing challenges are needed. We conducted semi-structured interviews with 24 experts to explore views on potential issues and policy options related to health data sharing. In this Perspective, we describe and contextualize unconventional insights shared by our interviewees relevant to issues in five domains: data quality, privacy, equity, incentives, and sustainability. These insights question a focus on granular quality metrics for gatekeeping; challenge enthusiasm for maximalist risk disclosure practices; call attention to power dynamics that potentially compromise the patient&#39;s voice; encourage faith in the sharing proclivities of new generations of scientists; and endorse accounting for personal disposition in the selection of long-term partners. We consider the merits of each insight with the broad goal of encouraging creative thinking to address data sharing challenges.},
  archive      = {J_FDATA},
  author       = {Guerrini, Christi J. and Majumder, Mary A. and Robinson, Jill O. and Cook-Deegan, Robert and Blank, Matthew and Bollinger, Juli and Geary, Janis and Gutierrez, Amanda M. and Shrikant, Maya and McGuire, Amy L.},
  doi          = {10.3389/fdata.2023.1095119},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {1095119},
  shortjournal = {Front. Big Data},
  title        = {Fresh takes on five health data sharing domains: Quality, privacy, equity, incentives, and sustainability},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An extended approach to appraise electricity distribution
and carbon footprint of bitcoin in a smart city. <em>FDATA</em>,
<em>6</em>, 1082113. (<a
href="https://doi.org/10.3389/fdata.2023.1082113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nation cannot sustain a highly productive and efficient population without smart cities. Due to their significant reliance on digital technologies, these cities require a high level of cybercrime protection. Cryptocurrencies have gained significant attention due to their secure and reliable infrastructure. The decentralised cryptocurrency operates in a trust-less environment known as the blockchain, where each network participant has a ledger copy of all transactions. Blockchain technology employs a proven consensus mechanism without requiring establishment of a central authority. But the consensus mechanism requires miner to solve a cryptographic problem by generating random hashes until one of them matches the desired one. This procedure is energy-intensive, and when thousands of miners repeat it to verify a single transaction, a substantial amount of electricity is consumed. Moreover, electricity produces a significant amount of carbon footprint. Patch methodology utilises the data of all hashes created per year and the efficiency of mining hardware over a 10-year period to calculate the Bitcoins energy consumption. Due to a large number of unknown and uncertain factors involved, it is difficult to precisely calculate a single value for electricity consumption and carbon footprint as reported by Patch methodology. The proposed method extends the Patch methodology by adding a maximum and minimum limit to the hardware efficiency as well as the sources of power generation, which can help refine estimates of electricity consumption and carbon emissions for a more accurate picture. Using the proposed methodology, it was estimated that Bitcoin consumed between 38.495 and 120.72 terawatt hours of electricity in 2021 and released between 2.12 and 45.37 million metric tonnes of carbon dioxide. To address the issue of excessive energy consumption and carbon emissions, a significant number of individual miners and mining pools are relocating to energy-intensive regions, such as aluminium mining sites that rely on hydroelectricity for energy generation.},
  archive      = {J_FDATA},
  author       = {Sharma, Ayushi and Sharma, Pratham and Bamotra, Harsh and Gaur, Vibha},
  doi          = {10.3389/fdata.2023.1082113},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {1082113},
  shortjournal = {Front. Big Data},
  title        = {An extended approach to appraise electricity distribution and carbon footprint of bitcoin in a smart city},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybridized bio-inspired intrusion detection system for
internet of things. <em>FDATA</em>, <em>6</em>, 1081466. (<a
href="https://doi.org/10.3389/fdata.2023.1081466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) consists of several smart devices equipped with computing, sensing, and network capabilities, which enable them to collect and exchange heterogeneous data wirelessly. The increasing usage of IoT devices in daily activities increases the security needs of IoT systems. These IoT devices are an easy target for intruders to perform malicious activities and make the underlying network corrupt. Hence, this paper proposes a hybridized bio-inspired-based intrusion detection system (IDS) for the IoT framework. The hybridized sine-cosine algorithm (SCA) and salp swarm algorithm (SSA) determines the essential features of the network traffic. Selected features are passed to a machine learning (ML) classifier for the detection and classification of intrusive traffic. The IoT network intrusion dataset determines the performance of the proposed system in a python environment. The proposed hybridized system achieves maximum accuracy of 84.75% with minimum selected features i.e., 8 and takes minimum time of 96.42 s in detecting intrusion for the IoT network. The proposed system&#39;s effectiveness is shown by comparing it with other similar approaches for performing multiclass classification.},
  archive      = {J_FDATA},
  author       = {Singh, Richa and Ujjwal, R. L.},
  doi          = {10.3389/fdata.2023.1081466},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {1081466},
  shortjournal = {Front. Big Data},
  title        = {Hybridized bio-inspired intrusion detection system for internet of things},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strengths and limitations of relative wealth indices derived
from big data in indonesia. <em>FDATA</em>, <em>6</em>, 1054156. (<a
href="https://doi.org/10.3389/fdata.2023.1054156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate relative wealth estimates in Low and Middle-Income Countries (LMICS) are crucial to help policymakers address socio-demographic inequalities under the guidance of the Sustainable Development Goals set by the United Nations. Survey-based approaches have traditionally been employed to collect highly granular data about income, consumption, or household material goods to create index-based poverty estimates. However, these methods are only capture persons in households (i.e., in the household sample framework) and they do not include migrant populations or unhoused citizens. Novel approaches combining frontier data, computer vision, and machine learning have been proposed to complement these existing approaches. However, the strengths and limitations of these big-data-derived indices have yet to be sufficiently studied. In this paper, we focus on the case of Indonesia and examine one frontier-data derived Relative Wealth Index (RWI), created by the Facebook Data for Good initiative, that utilizes connectivity data from the Facebook Platform and satellite imagery data to produce a high-resolution estimate of relative wealth for 135 countries. We examine it concerning asset-based relative wealth indices estimated from existing high-quality national-level traditional survey instruments, the USAID-developed Demographic Health Survey (DHS), and the Indonesian National Socio-economic survey (SUSENAS). In this work, we aim to understand how the frontier-data derived index can be used to inform anti-poverty programs in Indonesia and the Asia Pacific region. First, we unveil key features that affect the comparison between the traditional and non-traditional sources, such as the publishing time and authority and the granularity of the spatial aggregation of the data. Second, to provide operational input, we hypothesize how a re-distribution of resources based on the RWI map would impact a current social program, the Social Protection Card (KPS) of Indonesia and assess impact. In this hypothetical scenario, we estimate the percentage of Indonesians eligible for the program, which would have been incorrectly excluded from a social protection payment had the RWI been used in place of the survey-based wealth index. The exclusion error in that case would be 32.82%. Within the context of the KPS program targeting, we noted significant differences between the RWI map&#39;s predictions and the SUSENAS ground truth index estimates.},
  archive      = {J_FDATA},
  author       = {Sartirano, Daniele and Kalimeri, Kyriaki and Cattuto, Ciro and Delamónica, Enrique and Garcia-Herranz, Manuel and Mockler, Anthony and Paolotti, Daniela and Schifanella, Rossano},
  doi          = {10.3389/fdata.2023.1054156},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {1054156},
  shortjournal = {Front. Big Data},
  title        = {Strengths and limitations of relative wealth indices derived from big data in indonesia},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypergraphs with edge-dependent vertex weights: P-laplacians
and spectral clustering. <em>FDATA</em>, <em>6</em>, 1020173. (<a
href="https://doi.org/10.3389/fdata.2023.1020173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study p-Laplacians and spectral clustering for a recently proposed hypergraph model that incorporates edge-dependent vertex weights (EDVW). These weights can reflect different importance of vertices within a hyperedge, thus conferring the hypergraph model higher expressivity and flexibility. By constructing submodular EDVW-based splitting functions, we convert hypergraphs with EDVW into submodular hypergraphs for which the spectral theory is better developed. In this way, existing concepts and theorems such as p-Laplacians and Cheeger inequalities proposed under the submodular hypergraph setting can be directly extended to hypergraphs with EDVW. For submodular hypergraphs with EDVW-based splitting functions, we propose an efficient algorithm to compute the eigenvector associated with the second smallest eigenvalue of the hypergraph 1-Laplacian. We then utilize this eigenvector to cluster the vertices, achieving higher clustering accuracy than traditional spectral clustering based on the 2-Laplacian. More broadly, the proposed algorithm works for all submodular hypergraphs that are graph reducible. Numerical experiments using real-world data demonstrate the effectiveness of combining spectral clustering based on the 1-Laplacian and EDVW.},
  archive      = {J_FDATA},
  author       = {Zhu, Yu and Segarra, Santiago},
  doi          = {10.3389/fdata.2023.1020173},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {1020173},
  shortjournal = {Front. Big Data},
  title        = {Hypergraphs with edge-dependent vertex weights: P-laplacians and spectral clustering},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Big data in corneal diseases and cataract: Current
applications and future directions. <em>FDATA</em>, <em>6</em>, 1017420.
(<a href="https://doi.org/10.3389/fdata.2023.1017420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accelerated growth in electronic health records (EHR), Internet-of-Things, mHealth, telemedicine, and artificial intelligence (AI) in the recent years have significantly fuelled the interest and development in big data research. Big data refer to complex datasets that are characterized by the attributes of “5 Vs”—variety, volume, velocity, veracity, and value. Big data analytics research has so far benefitted many fields of medicine, including ophthalmology. The availability of these big data not only allow for comprehensive and timely examinations of the epidemiology, trends, characteristics, outcomes, and prognostic factors of many diseases, but also enable the development of highly accurate AI algorithms in diagnosing a wide range of medical diseases as well as discovering new patterns or associations of diseases that are previously unknown to clinicians and researchers. Within the field of ophthalmology, there is a rapidly expanding pool of large clinical registries, epidemiological studies, omics studies, and biobanks through which big data can be accessed. National corneal transplant registries, genome-wide association studies, national cataract databases, and large ophthalmology-related EHR-based registries (e.g., AAO IRIS Registry) are some of the key resources. In this review, we aim to provide a succinct overview of the availability and clinical applicability of big data in ophthalmology, particularly from the perspective of corneal diseases and cataract, the synergistic potential of big data, AI technologies, internet of things, mHealth, and wearable smart devices, and the potential barriers for realizing the clinical and research potential of big data in this field.},
  archive      = {J_FDATA},
  author       = {Ting, Darren S. J. and Deshmukh, Rashmi and Ting, Daniel S. W. and Ang, Marcus},
  doi          = {10.3389/fdata.2023.1017420},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {1017420},
  shortjournal = {Front. Big Data},
  title        = {Big data in corneal diseases and cataract: Current applications and future directions},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A 3D mixed reality visualization of network topology and
activity results in better dyadic cyber team communication and cyber
situational awareness. <em>FDATA</em>, <em>6</em>, 1042783. (<a
href="https://doi.org/10.3389/fdata.2023.1042783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundCyber defense decision-making during cyber threat situations is based on human-to-human communication aiming to establish a shared cyber situational awareness. Previous studies suggested that communication inefficiencies were among the biggest problems facing security operation center teams. There is a need for tools that allow for more efficient communication of cyber threat information between individuals both in education and during cyber threat situations.MethodsIn the present study, we compared how the visual representation of network topology and traffic in 3D mixed reality vs. 2D affected team performance in a sample of cyber cadets (N = 22) cooperating in dyads. Performance outcomes included network topology recognition, cyber situational awareness, confidence in judgements, experienced communication demands, observed verbal communication, and forced choice decision-making. The study utilized network data from the NATO CCDCOE 2022 Locked Shields cyber defense exercise.ResultsWe found that participants using the 3D mixed reality visualization had better cyber situational awareness than participants in the 2D group. The 3D mixed reality group was generally more confident in their judgments except when performing worse than the 2D group on the topology recognition task (which favored the 2D condition). Participants in the 3D mixed reality group experienced less communication demands, and performed more verbal communication aimed at establishing a shared mental model and less communications discussing task resolution. Better communication was associated with better cyber situational awareness. There were no differences in decision-making between the groups. This could be due to cohort effects such as formal training or the modest sample size.ConclusionThis is the first study comparing the effect of 3D mixed reality and 2D visualizations of network topology on dyadic cyber team communication and cyber situational awareness. Using 3D mixed reality visualizations resulted in better cyber situational awareness and team communication. The experiment should be repeated in a larger and more diverse sample to determine its potential effect on decision-making.},
  archive      = {J_FDATA},
  author       = {Ask, Torvald F. and Kullman, Kaur and Sütterlin, Stefan and Knox, Benjamin J. and Engel, Don and Lugo, Ricardo G.},
  doi          = {10.3389/fdata.2023.1042783},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1042783},
  shortjournal = {Front. Big Data},
  title        = {A 3D mixed reality visualization of network topology and activity results in better dyadic cyber team communication and cyber situational awareness},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting the distribution of COVID-19 through CGAN—taking
macau as an example. <em>FDATA</em>, <em>6</em>, 1008292. (<a
href="https://doi.org/10.3389/fdata.2023.1008292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) is an innovative method that is widely used in data prediction. Predicting the COVID-19 distribution using ML is essential for urban security risk assessment and governance. This study uses conditional generative adversarial network (CGAN) to construct a method to predict the COVID-19 hotspot distribution through urban texture and business formats and establishes a relationship between urban elements and COVID-19 so that machines can automatically predict the epidemic hotspots in cities. Taking Macau as an example, this method is used to determine the correlation between the urban texture and business hotspots of Macau and the new epidemic hotspot clusters. Different types of samples afforded different epidemic prediction accuracies. The results show the following: (1) CGAN can accurately predict the distribution area of COVID-19, and the accuracy can exceed 70%. (2) The results of predicting the COVID-19 distribution through urban texture and POI data of hospitals and stations are the best, with an accuracy of more than 60% in experiments in different regions of Macau. (3) The proposed method can also predict other areas in the city that may be at risk of COVID-19 and help urban epidemic prevention and control.},
  archive      = {J_FDATA},
  author       = {Zheng, Liang and Chen, Yile and Jiang, Shan and Song, Junxin and Zheng, Jianyi},
  doi          = {10.3389/fdata.2023.1008292},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1008292},
  shortjournal = {Front. Big Data},
  title        = {Predicting the distribution of COVID-19 through CGAN—Taking macau as an example},
  volume       = {6},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tools of war and virtue–institutional structures as a source
of ethical deskilling. <em>FDATA</em>, <em>5</em>, 1019293. (<a
href="https://doi.org/10.3389/fdata.2022.1019293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shannon Vallor has raised the possibility of ethical deskilling as a potential pitfall as AI technology is increasingly being developed for and implemented in military institutions. Bringing the sociological concept of deskilling into the field of virtue ethics, she has questioned if military operators will be able to possess the ethical wherewithal to act as responsible moral agents as they find themselves increasingly removed from the battlefield, their actions ever more mediated by artificial intelligence. The risk, as Vallor sees it, is that if combatants were removed, they would be deprived of the opportunity to develop moral skills crucial for acting as virtuous individuals. This article constitutes a critique of this conception of ethical deskilling and an attempt at a reappraisal of the concept. I argue first that her treatment of moral skills and virtue, as it pertains to professional military ethics, treating military virtue as a sui generis form of ethical cognition, is both normatively problematic as well as implausible from a moral psychological view. I subsequently present an alternative account of ethical deskilling, based on an analysis of military virtues, as a species of moral virtues essentially mediated by institutional and technological structures. According to this view, then, professional virtue is a form of extended cognition, and professional roles and institutional structures are parts of what makes these virtues the virtues that they are, i.e., constitutive parts of the virtues in question. Based on this analysis, I argue that the most likely source of ethical deskilling caused by technological change is not how technology, AI, or otherwise, makes individuals unable to develop appropriate moral–psychological traits but rather how it changes the institution&#39;s capacities to act.},
  archive      = {J_FDATA},
  author       = {Hovd, Sigurd N.},
  doi          = {10.3389/fdata.2022.1019293},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {1019293},
  shortjournal = {Front. Big Data},
  title        = {Tools of war and virtue–Institutional structures as a source of ethical deskilling},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From video summarization to real time video summarization in
smart cities and beyond: A survey. <em>FDATA</em>, <em>5</em>, 1106776.
(<a href="https://doi.org/10.3389/fdata.2022.1106776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the massive expansion of videos on the internet, searching through millions of them has become quite challenging. Smartphones, recording devices, and file sharing are all examples of ways to capture massive amounts of real time video. In smart cities, there are many surveillance cameras, which has created a massive volume of video data whose indexing, retrieval, and administration is a difficult problem. Exploring such results takes time and degrades the user experience. In this case, video summarization is extremely useful. Video summarization allows for the efficient storing, retrieval, and browsing of huge amounts of information from video without sacrificing key features. This article presents a classification and analysis of video summarization approaches, with a focus on real-time video summarization (RVS) domain techniques that can be used to summarize videos. The current study will be useful in integrating essential research findings and data for quick reference, laying the preliminaries, and investigating prospective research directions. A variety of practical uses, including aberrant detection in a video surveillance system, have made successful use of video summarization in smart cities.},
  archive      = {J_FDATA},
  author       = {Shambharkar, Prashant Giridhar and Goel, Ruchi},
  doi          = {10.3389/fdata.2022.1106776},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1106776},
  shortjournal = {Front. Big Data},
  title        = {From video summarization to real time video summarization in smart cities and beyond: A survey},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dependable modulation classifier explainer with measurable
explainability. <em>FDATA</em>, <em>5</em>, 1081872. (<a
href="https://doi.org/10.3389/fdata.2022.1081872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) plays a significant role in building smart cities worldwide. Smart cities use IoT devices to collect and analyze data to provide better services and solutions. These IoT devices are heavily dependent on the network for communication. These new-age networks use artificial intelligence (AI) that plays a crucial role in reducing network roll-out and operation costs, improving entire system performance, enhancing customer services, and generating possibilities to embed a wide range of telecom services and applications. For IoT devices, it is essential to have a robust and trustable network for reliable communication among devices and service points. The signals sent between the devices or service points use modulation to send a password over a bandpass frequency range. Our study focuses on modulation classification performed using deep learning method(s), adaptive modulation classification (AMC), which has now become an integral part of a communication system. We propose a dependable modulation classifier explainer (DMCE) that focuses on the explainability of modulation classification. Our study demonstrates how we can visualize and understand a particular prediction made by seeing highlighted data points crucial for modulation class prediction. We also demonstrate a numeric explainability measurable metric (EMM) to interpret the prediction. In the end, we present a comparative analysis with existing state-of-the-art methods.},
  archive      = {J_FDATA},
  author       = {Duggal, Gaurav and Gaikwad, Tejas and Sinha, Bhupendra},
  doi          = {10.3389/fdata.2022.1081872},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1081872},
  shortjournal = {Front. Big Data},
  title        = {Dependable modulation classifier explainer with measurable explainability},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain-enabled access control to prevent cyber attacks
in IoT: Systematic literature review. <em>FDATA</em>, <em>5</em>,
1081770. (<a href="https://doi.org/10.3389/fdata.2022.1081770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) enables communication among objects to collect information and make decisions to improve the quality of life. There are several unresolved security and privacy concerns in IoT due to multiple resource constrained devices, which lead to various cyber attacks. The conventional access control techniques depend on a central authority that further poses privacy and scalability issues in IoT. Various problems with access control in IoT can be resolved to prevent various cyber attacks using the decentralization and immutability properties of the blockchain. This study explored the current research trends in blockchain-enabled secure access control mechanisms and also identifies their applicability in creating reliable access control solutions for IoT. The basic properties of blockchain, such as decentralization, auditability, transparency, and immutability, act as the propulsion that provides integrity and security, disregarding the participation of an external entity. Initially, the application of blockchain was created only for cryptocurrencies but with the introduction of Ethereum, which allows the writiting and execution of smart contracts, applications other than cryptocurrencies are also being created. As various research articles have been written on the usage of different types of blockchains for creating secure access control solutions for IoT, this study intends to find and examine such primary researches as well as come up with a systematic review of various findings. This study perceives the most frequently utilized blockchain for creating blockchain-based access control solutions to prevent various cyber attacks and also discusses the improvement in access control mechanisms using blockchain along with smart contracts in IoT. The present study also discusses the obstacles in building decentralized access control solutions for IoT systems as well as future research areas. For new researchers, this article is a nice place to start and a strong reference point.},
  archive      = {J_FDATA},
  author       = {Singh, Rinki and Kukreja, Deepika and Sharma, Deepak Kumar},
  doi          = {10.3389/fdata.2022.1081770},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1081770},
  shortjournal = {Front. Big Data},
  title        = {Blockchain-enabled access control to prevent cyber attacks in IoT: Systematic literature review},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D bi-directional transformer u-net for medical image
segmentation. <em>FDATA</em>, <em>5</em>, 1080715. (<a
href="https://doi.org/10.3389/fdata.2022.1080715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the popular deep learning methods, deep convolutional neural networks (DCNNs) have been widely adopted in segmentation tasks and have received positive feedback. However, in segmentation tasks, DCNN-based frameworks are known for their incompetence in dealing with global relations within imaging features. Although several techniques have been proposed to enhance the global reasoning of DCNN, these models are either not able to gain satisfying performances compared with traditional fully-convolutional structures or not capable of utilizing the basic advantages of CNN-based networks (namely the ability of local reasoning). In this study, compared with current attempts to combine FCNs and global reasoning methods, we fully extracted the ability of self-attention by designing a novel attention mechanism for 3D computation and proposed a new segmentation framework (named 3DTU) for three-dimensional medical image segmentation tasks. This new framework processes images in an end-to-end manner and executes 3D computation on both the encoder side (which contains a 3D transformer) and the decoder side (which is based on a 3D DCNN). We tested our framework on two independent datasets that consist of 3D MRI and CT images. Experimental results clearly demonstrate that our method outperforms several state-of-the-art segmentation methods in various metrics.},
  archive      = {J_FDATA},
  author       = {Fu, Xiyao and Sun, Zhexian and Tang, Haoteng and Zou, Eric M. and Huang, Heng and Wang, Yong and Zhan, Liang},
  doi          = {10.3389/fdata.2022.1080715},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1080715},
  shortjournal = {Front. Big Data},
  title        = {3D bi-directional transformer U-net for medical image segmentation},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Socio-technical system analysis of responsible data sharing
in water systems as critical infrastructure. <em>FDATA</em>, <em>5</em>,
1057155. (<a href="https://doi.org/10.3389/fdata.2022.1057155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention is increasingly focused on the protection of water systems as critical infrastructure, including subsystems of supply, sanitation, hygiene, and management. Similarly increasing consideration is paid to the growing role and impact of data on water systems and management. We explore key challenges associated with data-driven water systems as critical infrastructure. First, we describe the status of water infrastructure as a part of national critical infrastructure. Second, as this infrastructure increasingly relies on the constant flow of data from a huge variety, quality, and complexity of sensors, we provide a descriptive framework to map in detail the particular expertise needed across data-driven water management, applied to the UK water infrastructure as our use case. Third, through the framework of Capabilities Approach (CA) we analyze the specific challenges of data-driven water management, and argue that the current predominant narratives in the water infrastructure discourse have difficulties to effectively convey existing and emerging challenges. Fourth, we further demonstrate the widening gap between infrastructure services and consumer goods, arguing for increased convergence of the utilization of consumer data, and developing open data ecosystems.},
  archive      = {J_FDATA},
  author       = {Hazell, Peter and Novitzky, Peter and van den Oord, Steven},
  doi          = {10.3389/fdata.2022.1057155},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1057155},
  shortjournal = {Front. Big Data},
  title        = {Socio-technical system analysis of responsible data sharing in water systems as critical infrastructure},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ProKnow: Process knowledge for safety constrained and
explainable question generation for mental health diagnostic assistance.
<em>FDATA</em>, <em>5</em>, 1056728. (<a
href="https://doi.org/10.3389/fdata.2022.1056728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Mental Health Assistants (VMHAs) are utilized in health care to provide patient services such as counseling and suggestive care. They are not used for patient diagnostic assistance because they cannot adhere to safety constraints and specialized clinical process knowledge (ProKnow) used to obtain clinical diagnoses. In this work, we define ProKnow as an ordered set of information that maps to evidence-based guidelines or categories of conceptual understanding to experts in a domain. We also introduce a new dataset of diagnostic conversations guided by safety constraints and ProKnow that healthcare professionals use (ProKnow-data). We develop a method for natural language question generation (NLG) that collects diagnostic information from the patient interactively (ProKnow-algo). We demonstrate the limitations of using state-of-the-art large-scale language models (LMs) on this dataset. ProKnow-algo incorporates the process knowledge through explicitly modeling safety, knowledge capture, and explainability. As computational metrics for evaluation do not directly translate to clinical settings, we involve expert clinicians in designing evaluation metrics that test four properties: safety, logical coherence, and knowledge capture for explainability while minimizing the standard cross entropy loss to preserve distribution semantics-based similarity to the ground truth. LMs with ProKnow-algo generated 89% safer questions in the depression and anxiety domain (tested property: safety). Further, without ProKnow-algo generations question did not adhere to clinical process knowledge in ProKnow-data (tested property: knowledge capture). In comparison, ProKnow-algo-based generations yield a 96% reduction in our metrics to measure knowledge capture. The explainability of the generated question is assessed by computing similarity with concepts in depression and anxiety knowledge bases. Overall, irrespective of the type of LMs, ProKnow-algo achieved an averaged 82% improvement over simple pre-trained LMs on safety, explainability, and process-guided question generation. For reproducibility, we will make ProKnow-data and the code repository of ProKnow-algo publicly available upon acceptance.},
  archive      = {J_FDATA},
  author       = {Roy, Kaushik and Gaur, Manas and Soltani, Misagh and Rawte, Vipula and Kalyan, Ashwin and Sheth, Amit},
  doi          = {10.3389/fdata.2022.1056728},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1056728},
  shortjournal = {Front. Big Data},
  title        = {ProKnow: Process knowledge for safety constrained and explainable question generation for mental health diagnostic assistance},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair classification via domain adaptation: A dual
adversarial learning approach. <em>FDATA</em>, <em>5</em>, 1049565. (<a
href="https://doi.org/10.3389/fdata.2022.1049565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern machine learning (ML) models are becoming increasingly popular and are widely used in decision-making systems. However, studies have shown critical issues of ML discrimination and unfairness, which hinder their adoption on high-stake applications. Recent research on fair classifiers has drawn significant attention to developing effective algorithms to achieve fairness and good classification performance. Despite the great success of these fairness-aware machine learning models, most of the existing models require sensitive attributes to pre-process the data, regularize the model learning or post-process the prediction to have fair predictions. However, sensitive attributes are often incomplete or even unavailable due to privacy, legal or regulation restrictions. Though we lack the sensitive attribute for training a fair model in the target domain, there might exist a similar domain that has sensitive attributes. Thus, it is important to exploit auxiliary information from a similar domain to help improve fair classification in the target domain. Therefore, in this paper, we study a novel problem of exploring domain adaptation for fair classification. We propose a new framework that can learn to adapt the sensitive attributes from a source domain for fair classification in the target domain. Extensive experiments on real-world datasets illustrate the effectiveness of the proposed model for fair classification, even when no sensitive attributes are available in the target domain.},
  archive      = {J_FDATA},
  author       = {Liang, Yueqing and Chen, Canyu and Tian, Tian and Shu, Kai},
  doi          = {10.3389/fdata.2022.1049565},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1049565},
  shortjournal = {Front. Big Data},
  title        = {Fair classification via domain adaptation: A dual adversarial learning approach},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the purpose of meaningful human control of AI.
<em>FDATA</em>, <em>5</em>, 1017677. (<a
href="https://doi.org/10.3389/fdata.2022.1017677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meaningful human control over AI is exalted as a key tool for assuring safety, dignity, and responsibility for AI and automated decision-systems. It is a central topic especially in fields that deal with the use of AI for decisions that could cause significant harm, like AI-enabled weapons systems. This paper argues that discussions regarding meaningful human control commonly fail to identify the purpose behind the call for meaningful human control and that stating that purpose is a necessary step in deciding how best to institutionalize meaningful human control over AI. The paper identifies 5 common purposes for human control and sketches how different purpose translate into different institutional design.},
  archive      = {J_FDATA},
  author       = {Davidovic, Jovana},
  doi          = {10.3389/fdata.2022.1017677},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1017677},
  shortjournal = {Front. Big Data},
  title        = {On the purpose of meaningful human control of AI},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Changing features of the northern hemisphere 500-hPa
circumpolar vortex. <em>FDATA</em>, <em>5</em>, 1009158. (<a
href="https://doi.org/10.3389/fdata.2022.1009158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tropospheric circumpolar vortex (CPV), an important signature of processes steering the general atmospheric circulation, surrounds each pole and is linked to the surface weather conditions. The CPV can be characterized by its area and circularity ratio (Rc), which both vary temporally. This research advances previous work identifying the daily 500-hPa Northern Hemispheric CPV (NHCPV) area, Rc, and temporal trends in its centroid by examining linear trends and periodic cycles in NHCPV area and Rc (1979–2017). Results suggest that NHCPV area has increased linearly over time. However, a more representative signal of the planetary warming may be the temporally weakening gradient which has blurred NHCPV distinctiveness—perhaps a new indicator of Arctic amplification. Rc displays opposing trends in subperiods and an insignificant overall trend. Distinct annual and semiannual cycles exist for area and Rc over all subperiods. These features of NHCPV change over time may impact surface weather/climate.},
  archive      = {J_FDATA},
  author       = {Bushra, Nazla and Rohli, Robert V. and Li, Chunyan and Miller, Paul W. and Mostafiz, Rubayet Bin},
  doi          = {10.3389/fdata.2022.1009158},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1009158},
  shortjournal = {Front. Big Data},
  title        = {Changing features of the northern hemisphere 500-hPa circumpolar vortex},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Audio deepfakes: A survey. <em>FDATA</em>, <em>5</em>,
1001063. (<a href="https://doi.org/10.3389/fdata.2022.1001063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deepfake is content or material that is synthetically generated or manipulated using artificial intelligence (AI) methods, to be passed off as real and can include audio, video, image, and text synthesis. The key difference between manual editing and deepfakes is that deepfakes are AI generated or AI manipulated and closely resemble authentic artifacts. In some cases, deepfakes can be fabricated using AI-generated content in its entirety. Deepfakes have started to have a major impact on society with more generation mechanisms emerging everyday. This article makes a contribution in understanding the landscape of deepfakes, and their detection and generation methods. We evaluate various categories of deepfakes especially in audio. The purpose of this survey is to provide readers with a deeper understanding of (1) different deepfake categories; (2) how they could be created and detected; (3) more specifically, how audio deepfakes are created and detected in more detail, which is the main focus of this paper. We found that generative adversarial networks (GANs), convolutional neural networks (CNNs), and deep neural networks (DNNs) are common ways of creating and detecting deepfakes. In our evaluation of over 150 methods, we found that the majority of the focus is on video deepfakes, and, in particular, the generation of video deepfakes. We found that for text deepfakes, there are more generation methods but very few robust methods for detection, including fake news detection, which has become a controversial area of research because of the potential heavy overlaps with human generation of fake content. Our study reveals a clear need to research audio deepfakes and particularly detection of audio deepfakes. This survey has been conducted with a different perspective, compared to existing survey papers that mostly focus on just video and image deepfakes. This survey mainly focuses on audio deepfakes that are overlooked in most of the existing surveys. This article&#39;s most important contribution is to critically analyze and provide a unique source of audio deepfake research, mostly ranging from 2016 to 2021. To the best of our knowledge, this is the first survey focusing on audio deepfakes generation and detection in English.},
  archive      = {J_FDATA},
  author       = {Khanjani, Zahra and Watson, Gabrielle and Janeja, Vandana P.},
  doi          = {10.3389/fdata.2022.1001063},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {1001063},
  shortjournal = {Front. Big Data},
  title        = {Audio deepfakes: A survey},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flood risk assessment for residences at the neighborhood
scale by owner/occupant type and first-floor height. <em>FDATA</em>,
<em>5</em>, 997447. (<a
href="https://doi.org/10.3389/fdata.2022.997447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating flood risk is an essential component of understanding and increasing community resilience. A robust approach for quantifying flood risk in terms of average annual loss (AAL) in dollars across multiple homes is needed to provide valuable information for stakeholder decision-making. This research develops a computational framework to evaluate AAL at the neighborhood level by owner/occupant type (i.e., homeowner, landlord, and tenant) for increasing first-floor height (FFH). The AAL values were calculated here by numerically integrating loss-exceedance probability distributions to represent economic annual flood risk to the building, contents, and use. A simple case study for a census block in Jefferson Parish, Louisiana, revealed that homeowners bear a mean AAL of $4,390 at the 100-year flood elevation (E100), compared with $2,960, and $1,590 for landlords and tenants, respectively, because the homeowner incurs losses to building, contents, and use, rather than only two of the three, as for the landlord and tenant. The results of this case study showed that increasing FFH reduces AAL proportionately for each owner/occupant type, and that two feet of additional elevation above E100 may provide the most economically advantageous benefit. The modeled results suggested that Hazus Multi-Hazard (Hazus-MH) output underestimates the AAL by 11% for building and 15% for contents. Application of this technique while partitioning the owner/occupant types will improve planning for improved resilience and assessment of impacts attributable to the costly flood hazard.},
  archive      = {J_FDATA},
  author       = {Al Assi, Ayat and Mostafiz, Rubayet Bin and Friedland, Carol J. and Rahim, Md Adilur and Rohli, Robert V.},
  doi          = {10.3389/fdata.2022.997447},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {997447},
  shortjournal = {Front. Big Data},
  title        = {Flood risk assessment for residences at the neighborhood scale by owner/occupant type and first-floor height},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A machine learning approach to quantify gender bias in
collaboration practices of mathematicians. <em>FDATA</em>, <em>5</em>,
989469. (<a href="https://doi.org/10.3389/fdata.2022.989469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaboration practices have been shown to be crucial determinants of scientific careers. We examine the effect of gender on coauthorship-based collaboration in mathematics, a discipline in which women continue to be underrepresented, especially in higher academic positions. We focus on two key aspects of scientific collaboration—the number of different coauthors and the number of single authorships. A higher number of coauthors has a positive effect on, e.g., the number of citations and productivity, while single authorships, for example, serve as evidence of scientific maturity and help to send a clear signal of one&#39;s proficiency to the community. Using machine learning-based methods, we show that collaboration networks of female mathematicians are slightly larger than those of their male colleagues when potential confounders such as seniority or total number of publications are controlled, while they author significantly fewer papers on their own. This confirms previous descriptive explorations and provides more precise models for the role of gender in collaboration in mathematics.},
  archive      = {J_FDATA},
  author       = {Steinfeldt, Christian and Mihaljević, Helena},
  doi          = {10.3389/fdata.2022.989469},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {989469},
  shortjournal = {Front. Big Data},
  title        = {A machine learning approach to quantify gender bias in collaboration practices of mathematicians},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sharing social media data: The role of past experiences,
attitudes, norms, and perceived behavioral control. <em>FDATA</em>,
<em>5</em>, 971974. (<a
href="https://doi.org/10.3389/fdata.2022.971974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media data (SMD) have become an important data source in the social sciences. The purpose of this paper is to investigate the experiences and practices of researchers working with SMD in their research and gain insights into researchers&#39; sharing behavior and influencing factors for their decisions. To achieve these aims, we conducted a survey study among researchers working with SMD. The questionnaire covered different topics related to accessing, (re)using, and sharing SMD. To examine attitudes toward data sharing, perceived subjective norms, and perceived behavioral control, we used questions based on the Theory of Planned Behavior (TPB). We employed a combination of qualitative and quantitative analyses. The results of the qualitative analysis show that the main reasons for not sharing SMD were that sharing was not considered or needed, as well as legal and ethical challenges. The quantitative analyses reveal that there are differences in the relative importance of past sharing and reuse experiences, experienced challenges, attitudes, subjective norms, and perceived behavioral control as predictors of future SMD sharing intentions, depending on the way the data should be shared (publicly, with restricted access, or upon personal request). Importantly, the TPB variables have predictive power for all types of SMD sharing.},
  archive      = {J_FDATA},
  author       = {Akdeniz, Esra and Borschewski, Kerrin Emilia and Breuer, Johannes and Voronin, Yevhen},
  doi          = {10.3389/fdata.2022.971974},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {971974},
  shortjournal = {Front. Big Data},
  title        = {Sharing social media data: The role of past experiences, attitudes, norms, and perceived behavioral control},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Staying with the trouble of networks. <em>FDATA</em>,
<em>5</em>, 510310. (<a
href="https://doi.org/10.3389/fdata.2022.510310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks have risen to prominence as intellectual technologies and graphical representations, not only in science, but also in journalism, activism, policy, and online visual cultures. Inspired by approaches taking trouble as occasion to (re)consider and reflect on otherwise implicit knowledge practices, in this article we explore how problems with network practices can be taken as invitations to attend to the diverse settings and situations in which network graphs and maps are created and used in society. In doing so, we draw on cases from our research, engagement and teaching activities involving making networks, making sense of networks, making networks public, and making network tools. As a contribution to “critical data practice,” we conclude with some approaches for slowing down and caring for network practices and their associated troubles to elicit a richer picture of what is involved in making networks work as well as reconsidering their role in collective forms of inquiry.},
  archive      = {J_FDATA},
  author       = {van Geenen, Daniela and Gray, Jonathan W. Y. and Bounegru, Liliana and Venturini, Tommaso and Jacomy, Mathieu and Meunier, Axel},
  doi          = {10.3389/fdata.2022.510310},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {510310},
  shortjournal = {Front. Big Data},
  title        = {Staying with the trouble of networks},
  volume       = {5},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
