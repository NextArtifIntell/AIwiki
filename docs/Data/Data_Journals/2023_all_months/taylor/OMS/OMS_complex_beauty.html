<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="oms---45">OMS - 45</h2>
<ul>
<li><details>
<summary>
(2023). Sequential path-equilibration algorithm for highly accurate
traffic flow assignment in an urban road network. <em>OMS</em>,
<em>38</em>(5), 1081‚Äì1104. (<a
href="https://doi.org/10.1080/10556788.2023.2196725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the issue of traffic flow assignment has become an interdisciplinary topic that concerns multiple research areas and branches of science. This work is focussed on the mathematical and computational aspects of the equilibrium traffic assignment problem in the case when route flows are considered to be decision variables. Firstly, we obtain a fixed-point mapping with the explicit contraction operator, which is proven to equilibrate the journey times on feasible routes between a single origin-destination pair of nodes with the quadratic rate. Remarkable that from mathematical perspectives, the developed operator generalizes most path-equilibration operators already exploited by researchers. Secondly, we use the obtained fixed-point procedure to run the sequential path-equilibration algorithm for traffic flow assignment on well-known test urban road networks with arc-additive travel time functions. Our computational results appear to demonstrate higher accuracy of user-equilibrium traffic assignment solutions than the best ones known to us. In other words, developed within this paper sequential path-equilibration algorithm leads to solutions with less goal function values compared to the best solutions for today, according to our knowledge.},
  archive      = {J_OMS},
  author       = {Alexander Y. Krylatov},
  doi          = {10.1080/10556788.2023.2196725},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1081-1104},
  shortjournal = {Optim. Methods Softw.},
  title        = {Sequential path-equilibration algorithm for highly accurate traffic flow assignment in an urban road network},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infinite dimensional tensor variational inequalities with
applications to an economic equilibrium problem. <em>OMS</em>,
<em>38</em>(5), 1058‚Äì1080. (<a
href="https://doi.org/10.1080/10556788.2023.2192494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a general oligopolistic market equilibrium model in which each firm produces several commodities in a time interval. To this aim, we introduce tensor variational inequalities in Hilbert spaces which are a powerful tool to analyse the model. Indeed we characterize the equilibrium condition by means of a suitable time-dependent tensor variational inequality. In addition, we prove some existence and regularity results and a numerical scheme to compute the solution. Finally we provide a numerical example.},
  archive      = {J_OMS},
  author       = {A. Barbagallo and S. Guarino Lo Bianco},
  doi          = {10.1080/10556788.2023.2192494},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1058-1080},
  shortjournal = {Optim. Methods Softw.},
  title        = {Infinite dimensional tensor variational inequalities with applications to an economic equilibrium problem},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A branch and bound method solving the max‚Äìmin linear
discriminant analysis problem. <em>OMS</em>, <em>38</em>(5), 1031‚Äì1057.
(<a href="https://doi.org/10.1080/10556788.2023.2198769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisher linear discriminant analysis (FLDA or LDA) is a well-known technique for dimension reduction and classification. The method was first formulated in 1936 by Fisher in the one-dimensional setting. In this paper, we will examine the LDA problem using a different objective function. Instead of maximizing the sum of all distances between all classes, we will define an objective function that will maximize the minimum separation among all distances between all classes. This leads to a difficult nonconvex optimization problem. We present a branch and bound method for the problem in the case where the reduction is to the one-dimensional space.},
  archive      = {J_OMS},
  author       = {Amir Beck and Raz Sharon},
  doi          = {10.1080/10556788.2023.2198769},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1031-1057},
  shortjournal = {Optim. Methods Softw.},
  title        = {A branch and bound method solving the max‚Äìmin linear discriminant analysis problem},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient model for the multiple allocation hub maximal
covering problem. <em>OMS</em>, <em>38</em>(5), 1009‚Äì1030. (<a
href="https://doi.org/10.1080/10556788.2023.2196726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hub location problem is one of the challenging subjects in location theory. This problem can benefit transportation, telecommunication or delivery systems, in which hub nodes are responsible for receiving, collecting and delivering commodities. This paper discusses the multiple allocation hub maximal covering problem (MAHMCP). In this problem, the flow of an origin‚Äìdestination pair is transferred via multiple hubs such that the total flow covered by located hubs is maximized. We present a new model for the MAHMCP, which has a significantly smaller number of binary variables compared to previous models. The proposed model is theoretically stronger than past models, and an empirical study using the Australia Post (AP) dataset demonstrates its effectiveness. Our experiments show that the new formulation provides high-quality solutions and fast run times for instances up to 100 nodes.},
  archive      = {J_OMS},
  author       = {Mohammad Maleki and Nahid Majlesinasab and Ashesh Kumar Sinha},
  doi          = {10.1080/10556788.2023.2196726},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1009-1030},
  shortjournal = {Optim. Methods Softw.},
  title        = {An efficient model for the multiple allocation hub maximal covering problem},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convergences for robust bilevel polynomial programmes with
applications. <em>OMS</em>, <em>38</em>(5), 975‚Äì1008. (<a
href="https://doi.org/10.1080/10556788.2023.2189719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a bilevel polynomial optimization problem, where the constraint functions of both the upper-level and lower-level¬†problems involve uncertain parameters. We employ the deterministic robust optimization approach to examine the bilevel polynomial optimization problem under data uncertainties by providing lower bound approximations and convergences of sum-of-squares (SOS) relaxations for the robust bilevel polynomial optimization problem. More precisely, we show that under the convexity of the lower-level¬†problem and either the boundedness of the feasible set or the coercivity of the objective function, the global optimal values of SOS relaxation problems are lower bounds of the global optimal value of the robust bilevel polynomial problem and they converge to this global optimal value when the degrees of SOS polynomials in the relaxation problems tend to infinity. Moreover, an application to an electric vehicle charging scheduling problem with renewable energy sources demonstrates that using the proposed SOS relaxation schemes, we obtain more stable optimal values than applying a direct solution approach as the SOS relaxations are capable of solving these models involving data uncertainties in dynamic charging price and weather conditions.},
  archive      = {J_OMS},
  author       = {Thai Doan Chuong and Xinghuo Yu and Andrew Eberhard and Chaojie Li and Chen Liu},
  doi          = {10.1080/10556788.2023.2189719},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {975-1008},
  shortjournal = {Optim. Methods Softw.},
  title        = {Convergences for robust bilevel polynomial programmes with applications},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two families of hybrid conjugate gradient methods with
restart procedures and their applications. <em>OMS</em>, <em>38</em>(5),
947‚Äì974. (<a
href="https://doi.org/10.1080/10556788.2023.2189718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, two families of hybrid conjugate gradient methods with restart procedures are proposed. Their hybrid conjugate parameters are yielded by projection or convex combination of the classical parameters. Moreover, their restart procedures are given uniformly, which are determined by the proposed hybrid conjugate parameters. The search directions of the presented families satisfy the sufficient descent condition. Under usual assumption and the weak Wolfe line search, the proposed families are proved to be globally convergent. Finally, choosing a specific parameter for each family to solve large-scale unconstrained optimization problems, convex constrained nonlinear monotone equations and image restoration problems. All the numerical results are reported and analysed, which show that the proposed families of hybrid conjugate gradient methods are promising.},
  archive      = {J_OMS},
  author       = {Xianzhen Jiang and Huihui Yang and Jinbao Jian and Xiaodi Wu},
  doi          = {10.1080/10556788.2023.2189718},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {947-974},
  shortjournal = {Optim. Methods Softw.},
  title        = {Two families of hybrid conjugate gradient methods with restart procedures and their applications},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bregman stochastic method for nonconvex nonsmooth problem
beyond global lipschitz gradient continuity. <em>OMS</em>,
<em>38</em>(5), 914‚Äì946. (<a
href="https://doi.org/10.1080/10556788.2023.2189717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider solving a broad class of large-scale nonconvex and nonsmooth minimization problems by a Bregman proximal stochastic gradient (BPSG) algorithm. The objective function of the minimization problem is the composition of a differentiable and a nondifferentiable function, and the differentiable part does not admit a global Lipschitz continuous gradient. Under some suitable conditions, the subsequential convergence of the proposed algorithm is established. And under expectation conditions with the Kurdyka-≈Åojasiewicz (KL) property, we also prove that the proposed method converges globally. We also apply the BPSG algorithm to solve sparse nonnegative matrix factorization (NMF), symmetric NMF via non-symmetric relaxation, and matrix completion problems under different kernel generating distances, and numerically compare it with other algorithms. The results demonstrate the robustness and effectiveness of the proposed algorithm.},
  archive      = {J_OMS},
  author       = {Qingsong Wang and Deren Han},
  doi          = {10.1080/10556788.2023.2189717},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {914-946},
  shortjournal = {Optim. Methods Softw.},
  title        = {A bregman stochastic method for nonconvex nonsmooth problem beyond global lipschitz gradient continuity},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Properties of semi-conjugate gradient methods for solving
unsymmetric positive definite linear systems. <em>OMS</em>,
<em>38</em>(5), 887‚Äì913. (<a
href="https://doi.org/10.1080/10556788.2023.2189716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conjugate gradient (CG) method is a classic Krylov subspace method for solving symmetric positive definite linear systems. We analyze an analogous semi-conjugate gradient (SCG) method, a special case of the existing semi-conjugate direction (SCD) methods, for unsymmetric positive definite linear systems. Unlike CG, SCG requires the solution of a lower triangular linear system to produce each semi-conjugate direction. We prove that SCG is theoretically equivalent to the full orthogonalization method (FOM), which is based on the Arnoldi process and converges in a finite number of steps. Because SCG&#39;s triangular system increases in size each iteration, Dai and Yuan [ Study on semi-conjugate direction methods for non-symmetric systems , Int. J. Numer. Meth. Eng. 60(8) (2004), pp. 1383‚Äì1399] proposed a sliding window implementation (SWI) to improve efficiency. We show that the directions produced are still locally semi-conjugate. A counter-example illustrates that SWI is different from the direct incomplete orthogonalization method (DIOM), which is FOM with a sliding window. Numerical experiments from the convection-diffusion equation and other applications show that SCG is robust and that the sliding window implementation SWI allows SCG to solve large systems efficiently.},
  archive      = {J_OMS},
  author       = {Na Huang and Yu-Hong Dai and Dominique Orban and Michael A. Saunders},
  doi          = {10.1080/10556788.2023.2189716},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {887-913},
  shortjournal = {Optim. Methods Softw.},
  title        = {Properties of semi-conjugate gradient methods for solving unsymmetric positive definite linear systems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient second-order optimization with predictions in
differential games. <em>OMS</em>, <em>38</em>(5), 861‚Äì886. (<a
href="https://doi.org/10.1080/10556788.2023.2189715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing number of training methods for generative adversarial networks (GANs) are differential games. Different from convex optimization problems on single functions, gradient descent on multiple objectives may not converge to stable fixed points (SFPs). In order to improve learning dynamics in such games, many recently proposed methods utilize the second-order information of the game, such as the Hessian matrix. Unfortunately, these methods often suffer from the enormous computational cost of Hessian, which hinders their further applications. In this paper, we present efficient second-order optimization (ESO), in which only a part of Hessian is updated in each iteration, and the algorithm is derived. Furthermore, we give the local convergence of the method under reasonable assumptions. In order to further speed up the training process of GANs, we propose efficient second-order optimization with predictions (ESOP) using a novel accelerator. Basic experiments show that the proposed learning methods are faster than some state-of-art methods in GANs, while applicable to many other n-player differential games with local convergence guarantee.},
  archive      = {J_OMS},
  author       = {Deliang Wei and Peng Chen and Fang Li and Xiangyun Zhang},
  doi          = {10.1080/10556788.2023.2189715},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {861-886},
  shortjournal = {Optim. Methods Softw.},
  title        = {Efficient second-order optimization with predictions in differential games},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conic optimization-based algorithms for nonnegative matrix
factorization. <em>OMS</em>, <em>38</em>(4), 837‚Äì859. (<a
href="https://doi.org/10.1080/10556788.2023.2189714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization is the following problem: given a nonnegative input matrix V and a factorization rank K , compute two nonnegative matrices, W with K columns and H with K rows, such that WH approximates V as well as possible. In this paper, we propose two new approaches for computing high-quality NMF solutions using conic optimization. These approaches rely on the same two steps. First, we reformulate NMF as minimizing a concave function over a product of convex cones ‚Äì one approach is based on the exponential cone and the other on the second-order cone. Then, we solve these reformulations iteratively: at each step, we minimize exactly, over the feasible set, a majorization of the objective functions obtained via linearization at the current iterate. Hence these subproblems are convex conic programs and can be solved efficiently using dedicated algorithms. We prove that our approaches reach a stationary point with an accuracy decreasing as O ( 1 i ) , where i denotes the iteration number. To the best of our knowledge, our analysis is the first to provide a convergence rate to stationary points for NMF. Furthermore, in the particular cases of rank-1 factorizations (i.e. K =‚Äâ1), we show that one of our formulations can be expressed as a convex optimization problem, implying that optimal rank-1 approximations can be computed efficiently. Finally, we show on several numerical examples that our approaches are able to frequently compute exact NMFs (i.e. with V = WH ) and compete favourably with the state of the art.},
  archive      = {J_OMS},
  author       = {Valentin Leplat and Yurii Nesterov and Nicolas Gillis and Fran√ßois Glineur},
  doi          = {10.1080/10556788.2023.2189714},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {837-859},
  shortjournal = {Optim. Methods Softw.},
  title        = {Conic optimization-based algorithms for nonnegative matrix factorization},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fenchel dual gradient method enabling regularization for
nonsmooth distributed optimization over time-varying networks.
<em>OMS</em>, <em>38</em>(4), 813‚Äì836. (<a
href="https://doi.org/10.1080/10556788.2023.2189713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a regularized Fenchel dual gradient method (RFDGM), which allows nodes in a time-varying undirected network to find a common decision, in a fully distributed fashion, for minimizing the sum of their local objective functions subject to their local constraints. Different from most existing distributed optimization algorithms that also cope with time-varying networks, RFDGM is able to handle problems with general convex objective functions and distinct local constraints, and still has non-asymptotic convergence results. Specifically, under a standard network connectivity condition, we show that RFDGM is guaranteed to reach œµ -accuracy in both optimality and feasibility within O ( 1 œµ 2 ln ‚Å° 1 œµ ) iterations. Such iteration complexity can be improved to O ( 1 œµ ln ‚Å° 1 œµ ) if the local objective functions are strongly convex but not necessarily differentiable. Finally, simulation results demonstrate the competence of RFDGM in practice.},
  archive      = {J_OMS},
  author       = {Xuyang Wu and Kin Cheong Sou and Jie Lu},
  doi          = {10.1080/10556788.2023.2189713},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {813-836},
  shortjournal = {Optim. Methods Softw.},
  title        = {A fenchel dual gradient method enabling regularization for nonsmooth distributed optimization over time-varying networks},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On a minimal criterion of efficiency in vector variational
control problems. <em>OMS</em>, <em>38</em>(4), 804‚Äì812. (<a
href="https://doi.org/10.1080/10556788.2023.2189712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we establish a minimal criterion of efficiency for a class of multiobjective variational control problems. Thus, some minimal conditions are formulated so that all local efficient solutions are also global. Also, we present an example to illustrate the mathematical developments derived in the paper.},
  archive      = {J_OMS},
  author       = {Savin TreantƒÉ},
  doi          = {10.1080/10556788.2023.2189712},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {804-812},
  shortjournal = {Optim. Methods Softw.},
  title        = {On a minimal criterion of efficiency in vector variational control problems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Worst-case evaluation complexity of a quadratic penalty
method for nonconvex optimization. <em>OMS</em>, <em>38</em>(4),
781‚Äì803. (<a
href="https://doi.org/10.1080/10556788.2023.2189711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the worst-case evaluation complexity of a version of the standard quadratic penalty method for smooth nonconvex optimization problems with constraints. The method analysed allows inexact solution of the subproblems and do not require prior knowledge of the Lipschitz constants related with the problem. When an approximate feasible point is used as starting point, it is shown that the referred method takes at most ùí™ ( log ( œÉ ‚àí 1 0 œµ ‚àí 2 ) ) O ( log ‚Å° ( œÉ 0 ‚àí 1 œµ ‚àí 2 ) ) O(log‚Å°(œÉ0‚àí1œµ‚àí2)) outer iterations to generate an œµ -approximate KKT point, where œÉ 0 is the first penalty parameter. For equality constrained problems, this bound yields to an evaluation complexity bound of O ( œµ ‚àí 4 ) , when œÉ 0 = œµ ‚àí 2 and suitable first-order methods are used as inner solvers. For problems having only linear equality constraints, an evaluation complexity bound of O ( œµ ‚àí ( p + 1 ) / p ) is established when appropriate p -order methods ( p ‚â• 2 ) are used as inner solvers. Illustrative numerical results are also presented and corroborate the theoretical predictions.},
  archive      = {J_OMS},
  author       = {Geovani Nunes Grapiglia},
  doi          = {10.1080/10556788.2023.2189711},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {781-803},
  shortjournal = {Optim. Methods Softw.},
  title        = {Worst-case evaluation complexity of a quadratic penalty method for nonconvex optimization},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributionally robust expected residual minimization for
stochastic variational inequality problems. <em>OMS</em>,
<em>38</em>(4), 756‚Äì780. (<a
href="https://doi.org/10.1080/10556788.2023.2167995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stochastic variational inequality problem (SVIP) is an equilibrium model that includes random variables and has been widely applied in various fields such as economics and engineering. Expected residual minimization (ERM) is an established model for obtaining a reasonable solution for the SVIP, and its objective function is an expected value of a suitable merit function for the SVIP. However, the ERM is restricted to the case where the distribution is known in advance. We extend the ERM to ensure the attainment of robust solutions for the SVIP under the uncertainty distribution (the extended ERM is referred to as distributionally robust expected residual minimization (DRERM), where the worst-case distribution is derived from the set of probability measures in which the expected value and variance take the same sample mean and variance, respectively). Under suitable assumptions, we demonstrate that the DRERM can be reformulated as a deterministic convex nonlinear semidefinite programming to avoid numerical integration.},
  archive      = {J_OMS},
  author       = {Atsushi Hori and Yuya Yamakawa and Nobuo Yamashita},
  doi          = {10.1080/10556788.2023.2167995},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {756-780},
  shortjournal = {Optim. Methods Softw.},
  title        = {Distributionally robust expected residual minimization for stochastic variational inequality problems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Outer approximation algorithms for convex vector
optimization problems. <em>OMS</em>, <em>38</em>(4), 723‚Äì755. (<a
href="https://doi.org/10.1080/10556788.2023.2167994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present a general framework of outer approximation algorithms to solve convex vector optimization problems, in which the Pascoletti-Serafini (PS) scalarization is solved iteratively. This scalarization finds the minimum ‚Äòdistance‚Äô from a reference point, which is usually taken as a vertex of the current outer approximation, to the upper image through a given direction. We propose efficient methods to select the parameters (the reference point and direction vector) of the PS scalarization and analyse the effects of these on the overall performance of the algorithm. Different from the existing vertex selection rules from the literature, the proposed methods do not require solving additional single-objective optimization problems. Using some test problems, we conduct an extensive computational study where three different measures are set as the stopping criteria: the approximation error, the runtime, and the cardinality of the solution set. We observe that the proposed variants have satisfactory results, especially in terms of runtime compared to the existing variants from the literature.},
  archive      = {J_OMS},
  author       = {ƒ∞rem Nur Keskin and Firdevs Ulus},
  doi          = {10.1080/10556788.2023.2167994},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {723-755},
  shortjournal = {Optim. Methods Softw.},
  title        = {Outer approximation algorithms for convex vector optimization problems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new multipoint symmetric secant method with a dense
initial matrix. <em>OMS</em>, <em>38</em>(4), 698‚Äì722. (<a
href="https://doi.org/10.1080/10556788.2023.2167993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale optimization, when either forming or storing Hessian matrices are prohibitively expensive, quasi-Newton methods are often used in lieu of Newton&#39;s method because they only require first-order information to approximate the true Hessian. Multipoint symmetric secant (MSS) methods can be thought of as generalizations of quasi-Newton methods in that they attempt to impose additional requirements on their approximation of the Hessian. Given an initial Hessian approximation, MSS methods generate a sequence of possibly-indefinite matrices using rank-2 updates to solve nonconvex unconstrained optimization problems. For practical reasons, up to now, the initialization has been a constant multiple of the identity matrix. In this paper, we propose a new limited-memory MSS method for large-scale nonconvex optimization that allows for dense initializations. Numerical results on the CUTEst test problems suggest that the MSS method using a dense initialization outperforms the standard initialization. Numerical results also suggest that this approach is competitive with both a basic L-SR1 trust-region method and an L-PSB method.},
  archive      = {J_OMS},
  author       = {Jennifer B. Erway and Mostafa Rezapour},
  doi          = {10.1080/10556788.2023.2167993},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {698-722},
  shortjournal = {Optim. Methods Softw.},
  title        = {A new multipoint symmetric secant method with a dense initial matrix},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Steering exact penalty DCA for nonsmooth DC optimisation
problems with equality and inequality constraints. <em>OMS</em>,
<em>38</em>(4), 668‚Äì697. (<a
href="https://doi.org/10.1080/10556788.2023.2167992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study a version of the DCA (Difference-of-Convex functions Algorithm) using the ‚Ñì 1 ‚Ñì 1 ‚Ñì1 penalty function for solving nonsmooth DC optimisation problems with nonsmooth DC equality and inequality constraints. The method employs an adaptive penalty updating strategy to improve its performance. This strategy is based on the so-called steering exact penalty methodology and relies on solving some auxiliary convex subproblems to determine a suitable value of the penalty parameter. We present a detailed convergence analysis of the method and illustrate its practical performance by applying the method to two nonsmooth discrete optimal control problem.},
  archive      = {J_OMS},
  author       = {M. V. Dolgopolik},
  doi          = {10.1080/10556788.2023.2167992},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {668-697},
  shortjournal = {Optim. Methods Softw.},
  title        = {Steering exact penalty DCA for nonsmooth DC optimisation problems with equality and inequality constraints},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unrestricted douglas-rachford algorithms for solving convex
feasibility problems in hilbert space. <em>OMS</em>, <em>38</em>(4),
655‚Äì667. (<a
href="https://doi.org/10.1080/10556788.2022.2157003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we focus on the convex feasibility problem (CFP) in Hilbert space. A specific method in this area that has gained a lot of interest in recent years is the Douglas-Rachford (DR) algorithm. This algorithm was originally introduced in 1956 for solving stationary and non-stationary heat equations. Then in 1979, Lions and Mercier adjusted and extended the algorithm with the aim of solving CFPs and even more general problems, such as finding zeros of the sum of two maximally monotone operators. Many developments which implement various concepts concerning this algorithm have occurred during the last decade. We introduce an unrestricted DR algorithm, which provides a general framework for such concepts. Using unrestricted products of a finite number of strongly nonexpansive operators, we apply this framework to provide new iterative methods, where, inter alia , such operators may be interlaced between the operators used in the scheme of our unrestricted DR algorithm.},
  archive      = {J_OMS},
  author       = {Kay Barshad and Aviv Gibali and Simeon Reich},
  doi          = {10.1080/10556788.2022.2157003},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {655-667},
  shortjournal = {Optim. Methods Softw.},
  title        = {Unrestricted douglas-rachford algorithms for solving convex feasibility problems in hilbert space},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using general triangle inequalities within quadratic convex
reformulation method. <em>OMS</em>, <em>38</em>(3), 626‚Äì653. (<a
href="https://doi.org/10.1080/10556788.2022.2157002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the exact solution of Problem ( P ) which consists in minimizing a quadratic function subject to quadratic constraints. We start with an explicit description of new general triangle inequalities that are derived from the ranges of the variables of ( P ). We show that they extend the triangle inequalities, introduced for the binary case, to variables that belong to a generic interval. We also prove that these inequalities cut feasible solutions of McCormick envelopes, and we relate them to the literature. We then introduce ( SDP ), a strong semidefinite relaxation of ( P ), that we call ‚ÄòShor&#39;s plus RLT plus Triangle‚Äô, which includes both the McCormick envelopes and the general triangle inequalities. We further show how to compute a convex relaxation ( P ‚àó ) whose optimal value reaches the value of ( SDP ). In order to handle these inequalities in the solution of ( SDP ), we solve it by a heuristic that also serves as a separation algorithm. We then solve ( P ) to global optimality with a branch-and-bound based on ( P ‚àó ) . Finally, we show that our method outperforms the compared solvers.},
  archive      = {J_OMS},
  author       = {Am√©lie Lambert},
  doi          = {10.1080/10556788.2022.2157002},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {626-653},
  shortjournal = {Optim. Methods Softw.},
  title        = {Using general triangle inequalities within quadratic convex reformulation method},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A linear-time algorithm for finding hamiltonian cycles in
rectangular grid graphs with two rectangular holes. <em>OMS</em>,
<em>38</em>(3), 591‚Äì625. (<a
href="https://doi.org/10.1080/10556788.2022.2157001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hamiltonian cycle problem is one of the most important problems in graph theory that has many applications. This problem is NP-complete for general grid graphs. For solid grid graphs, there are polynomial-time algorithms. Existence of polynomial-time algorithms for grid graphs with few holes has been asked in the literature. In this paper, we give a linear-time algorithm for rectangular grid graphs with two rectangular holes. This extends the previous result for rectangular grid graphs with one rectangular hole. We first present the forbidden conditions in which there is no Hamiltonian cycle for any grid graphs, including rectangular grid graphs with rectangular holes. We then show that if these forbidden conditions do not hold, then there exists a Hamiltonian cycle. The proof is constructive, therefore, it gives an algorithm. An application of the problem is in off-line robot exploration.},
  archive      = {J_OMS},
  author       = {Fatemeh Keshavarz-Kohjerdi and Alireza Bagheri},
  doi          = {10.1080/10556788.2022.2157001},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {591-625},
  shortjournal = {Optim. Methods Softw.},
  title        = {A linear-time algorithm for finding hamiltonian cycles in rectangular grid graphs with two rectangular holes},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A newton-type proximal gradient method for nonlinear
multi-objective optimization problems. <em>OMS</em>, <em>38</em>(3),
570‚Äì590. (<a
href="https://doi.org/10.1080/10556788.2022.2157000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a globally convergent Newton-type proximal gradient method is developed for composite multi-objective optimization problems where each objective function can be represented as the sum of a smooth function and a nonsmooth function. The proposed method deals with unconstrained convex multi-objective optimization problems. This method is free from any kind of priori chosen parameters or ordering information of objective functions. At every iteration of the proposed method, a subproblem is solved to find a suitable descent direction. The subproblem uses a quadratic approximation of each smooth function. An Armijo type line search is conducted to find a suitable step length. A sequence is generated using the descent direction and the step length. The global convergence of this method is justified under some mild assumptions. The proposed method is verified and compared with some existing methods using a set of test problems.},
  archive      = {J_OMS},
  author       = {Md Abu Talhamainuddin Ansary},
  doi          = {10.1080/10556788.2022.2157000},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {570-590},
  shortjournal = {Optim. Methods Softw.},
  title        = {A newton-type proximal gradient method for nonlinear multi-objective optimization problems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interior point methods for solving pareto eigenvalue
complementarity problems. <em>OMS</em>, <em>38</em>(3), 543‚Äì569. (<a
href="https://doi.org/10.1080/10556788.2022.2152023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose to solve Pareto eigenvalue complementarity problems by using interior-point methods. Precisely, we focus the study on an adaptation of the Mehrotra Predictor Corrector Method (MPCM) and a Non-Parametric Interior Point Method (NPIPM). We compare these two methods with two alternative methods, namely the Lattice Projection Method (LPM) and the Soft Max Method (SM). On a set of data generated from the Matrix Market, the performance profiles highlight the efficiency of MPCM and NPIPM for solving eigenvalue complementarity problems. We also consider an application to a concrete and large size situation corresponding to a geomechanical fracture problem. Finally, we discuss the extension of MPCM and NPIPM methods to solve quadratic pencil eigenvalue problems under conic constraints.},
  archive      = {J_OMS},
  author       = {Samir Adly and Mounir Haddou and Manh Hung Le},
  doi          = {10.1080/10556788.2022.2152023},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {543-569},
  shortjournal = {Optim. Methods Softw.},
  title        = {Interior point methods for solving pareto eigenvalue complementarity problems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An approximate newton-type proximal method using symmetric
rank-one updating formula for minimizing the nonsmooth composite
functions. <em>OMS</em>, <em>38</em>(3), 529‚Äì542. (<a
href="https://doi.org/10.1080/10556788.2022.2142587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Founded upon the scaled memoryless symmetric rank-one updating formula, we propose an approximation of the Newton-type proximal strategy for minimizing the nonsmooth composite functions. More exactly, to approximate the inverse Hessian of the smooth part of the objective function, a symmetric rank-one matrix is employed to straightly compute the search directions for a special category of well-known functions. Convergence of the given algorithm is argued with a nonmonotone backtracking line search adjusted for the corresponding nonsmooth model. Also, its practical advantages are computationally depicted in the two well-known real-world models.},
  archive      = {J_OMS},
  author       = {Zohre Aminifard and Saman Babaie-Kafaki},
  doi          = {10.1080/10556788.2022.2142587},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {529-542},
  shortjournal = {Optim. Methods Softw.},
  title        = {An approximate newton-type proximal method using symmetric rank-one updating formula for minimizing the nonsmooth composite functions},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feasible newton methods for symmetric tensor z-eigenvalue
problems. <em>OMS</em>, <em>38</em>(3), 510‚Äì528. (<a
href="https://doi.org/10.1080/10556788.2022.2142586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a Z -eigenpair of a symmetric tensor is equivalent to finding a Karush‚ÄìKuhn‚ÄìTucker point of a sphere constrained minimization problem. Based on this equivalency, in this paper, we first propose a class of iterative methods to get a Z-eigenpair of a symmetric tensor. Each method can generate a sequence of feasible points such that the sequence of function evaluations is decreasing. These methods can be regarded as extensions of the descent methods for unconstrained optimization problems. We pay particular attention to the Newton method. We show that under appropriate conditions, the Newton method is globally and quadratically convergent. Moreover, after finitely many iterations, the unit steplength will always be accepted. We also propose a nonlinear equations-based Newton method and establish its global and quadratic convergence. In the end, we do several numerical experiments to test the proposed Newton methods. The results show that both Newton methods are very efficient.},
  archive      = {J_OMS},
  author       = {Jiefeng Xu and Dong-Hui Li and Xueli Bai},
  doi          = {10.1080/10556788.2022.2142586},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {510-528},
  shortjournal = {Optim. Methods Softw.},
  title        = {Feasible newton methods for symmetric tensor Z-eigenvalue problems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A descent family of the spectral hestenes‚Äìstiefel method by
considering the quasi-newton method. <em>OMS</em>, <em>38</em>(3),
495‚Äì509. (<a
href="https://doi.org/10.1080/10556788.2022.2142585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prominent computational features of the Hestenes‚ÄìStiefel parameter as one of the fundamental members of conjugate gradient methods have attracted the attention of many researchers. Yet, as a weak stop, it lacks global convergence for general functions. To overcome this defect, a family of spectral version of Hestenes‚ÄìStiefel conjugate gradient methods is introduced. To compute the spectral parameter, in the account of worthy properties of quasi-Newton methods, we minimize the distance between the search direction matrix of the spectral conjugate gradient method and the BFGS (Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno) update. To achieve sufficient descent property, the search direction is projected in the orthogonal subspace to the gradient of the objective function. The convergence analysis of the proposed method is carried out under standard assumptions for general functions. Finally, the practical merits of the suggested method are investigated by numerical experiments on a set of CUTEr test functions using the Dolan‚ÄìMor√© performance profile. The results show the computational efficiency of the proposed method.},
  archive      = {J_OMS},
  author       = {Maryam Khoshsimaye-Bargard and Ali Ashrafi},
  doi          = {10.1080/10556788.2022.2142585},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {495-509},
  shortjournal = {Optim. Methods Softw.},
  title        = {A descent family of the spectral Hestenes‚ÄìStiefel method by considering the quasi-newton method},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A majorization penalty method for SVM with sparse
constraint. <em>OMS</em>, <em>38</em>(3), 474‚Äì494. (<a
href="https://doi.org/10.1080/10556788.2022.2142584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is an important and fundamental technique in machine learning. Soft-margin SVM models have stronger generalization performance compared with the hard-margin SVM. Most existing works use the hinge-loss function which can be regarded as an upper bound of the 0‚Äì1 loss function. However, it cannot explicitly control the number of misclassified samples. In this paper, we use the idea of soft-margin SVM and propose a new SVM model with a sparse constraint. Our model can strictly limit the number of misclassified samples, expressing the soft-margin constraint as a sparse constraint. By constructing a majorization function, a majorization penalty method can be used to solve the sparse-constrained optimization problem. We apply Conjugate-Gradient (CG) method to solve the resulting subproblem. Extensive numerical results demonstrate the impressive performance of the proposed majorization penalty method.},
  archive      = {J_OMS},
  author       = {Sitong Lu and Qingna Li},
  doi          = {10.1080/10556788.2022.2142584},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {474-494},
  shortjournal = {Optim. Methods Softw.},
  title        = {A majorization penalty method for SVM with sparse constraint},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Projection onto the exponential cone: A univariate
root-finding problem. <em>OMS</em>, <em>38</em>(3), 457‚Äì473. (<a
href="https://doi.org/10.1080/10556788.2021.2022147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential function and its logarithmic counterpart are essential corner stones of nonlinear mathematical modelling. In this paper, we treat their conic extensions, the exponential cone and the relative entropy cone, in primal, dual and polar form, and show that finding the nearest mapping of a point onto these convex sets all reduce to a single univariate root-finding problem. This leads to a fast projection algorithm shown numerically robust over a wide range of inputs.},
  archive      = {J_OMS},
  author       = {Henrik A. Friberg},
  doi          = {10.1080/10556788.2021.2022147},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {457-473},
  shortjournal = {Optim. Methods Softw.},
  title        = {Projection onto the exponential cone: A univariate root-finding problem},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-convex regularization and accelerated gradient algorithm
for sparse portfolio selection. <em>OMS</em>, <em>38</em>(2), 434‚Äì456.
(<a href="https://doi.org/10.1080/10556788.2022.2142580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In portfolio optimization, non-convex regularization has recently been recognized as an important approach to promote sparsity, while countervailing the shortcomings of convex penalty. In this paper, we customize the non-convex piecewise quadratic approximation (PQA) function considering the background of portfolio management and present the PQA regularized mean‚Äìvariance model (PMV). By exposing the feature of PMV, we prove that a KKT point of PMV is a local minimizer if the regularization parameter satisfies a mild condition. Besides, the theoretical sparsity of PMV is analysed, which is associated with the regularization parameter and the weight parameter. To solve this model, we introduce the accelerated proximal gradient (APG) algorithm, whose improved linear convergence rate compared with proximal gradient (PG) algorithm is developed. Moreover, the optimal accelerated parameter of APG algorithm for PMV is attained. These theoretical results are further illustrated with numerical experiments. Finally, empirical analysis demonstrates that the proposed model has a better out-of-sample performance and a lower turnover than many other existing models on the tested datasets.},
  archive      = {J_OMS},
  author       = {Qian Li and Wei Zhang and Guoqiang Wang and Yanqin Bai},
  doi          = {10.1080/10556788.2022.2142580},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {434-456},
  shortjournal = {Optim. Methods Softw.},
  title        = {Non-convex regularization and accelerated gradient algorithm for sparse portfolio selection},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exact penalization for cardinality and rank-constrained
optimization problems via partial regularization. <em>OMS</em>,
<em>38</em>(2), 412‚Äì433. (<a
href="https://doi.org/10.1080/10556788.2022.2142583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a class of constrained optimization problems whose constraints involve a cardinality or rank constraint. The penalty formulation based on a partial regularization has recently been promoted in the literature to approximate these problems, which usually outperforms the penalty formulation based on a full regularization in terms of solution quality. Nevertheless, the relation between the penalty formulation with a partial regularizer and the original problem was not much studied yet. Under some suitable assumptions, we show that the penalty formulation based on a partial regularization is an exact reformulation of the original problem in the sense that they both share the same global minimizers. We also show that a local minimizer of the original problem is that of the penalty reformulation. These results provide some theoretical justification for the often-observed superior performance of the penalty model based on a partial regularizer over a corresponding full regularizer.},
  archive      = {J_OMS},
  author       = {Zhaosong Lu and Xiaorui Li and Shuhuang Xiang},
  doi          = {10.1080/10556788.2022.2142583},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {412-433},
  shortjournal = {Optim. Methods Softw.},
  title        = {Exact penalization for cardinality and rank-constrained optimization problems via partial regularization},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Full-low evaluation methods for derivative-free
optimization. <em>OMS</em>, <em>38</em>(2), 386‚Äì411. (<a
href="https://doi.org/10.1080/10556788.2022.2142582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new class of rigorous methods for derivative-free optimization with the aim of delivering efficient and robust numerical performance for functions of all types, from smooth to non-smooth, and under different noise regimes. To this end, we have developed a class of methods, called Full-Low Evaluation methods, organized around two main types of iterations. The first iteration type (called Full-Eval ) is expensive in function evaluations, but exhibits good performance in the smooth and non-noisy cases. For the theory, we consider a line search based on an approximate gradient, backtracking until a sufficient decrease condition is satisfied. In practice, the gradient was approximated via finite differences, and the direction was calculated by a quasi-Newton step (BFGS). The second iteration type (called Low-Eval ) is cheap in function evaluations, yet more robust in the presence of noise or non-smoothness. For the theory, we consider direct search, and in practice we use probabilistic direct search with one random direction and its negative. A switch condition from Full-Eval to Low-Eval iterations was developed based on the values of the line-search and direct-search stepsizes. If enough Full-Eval steps are taken, we derive a complexity result of gradient-descent type. Under failure of Full-Eval , the Low-Eval iterations become the drivers of convergence yielding non-smooth convergence. Full-Low Evaluation methods are shown to be efficient and robust in practice across problems with different levels of smoothness and noise.},
  archive      = {J_OMS},
  author       = {A. S. Berahas and O. Sohab and L. N. Vicente},
  doi          = {10.1080/10556788.2022.2142582},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {386-411},
  shortjournal = {Optim. Methods Softw.},
  title        = {Full-low evaluation methods for derivative-free optimization},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generic optimization framework for resilient systems.
<em>OMS</em>, <em>38</em>(2), 356‚Äì385. (<a
href="https://doi.org/10.1080/10556788.2022.2142581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the optimal design of resilient systems, in which components can fail. The system can react to failures and its behaviour is described by general mixed integer nonlinear programs, which allows for applications to many (technical) systems. This then leads to a three-level optimization problem. The upper level designs the system minimizing a cost function, the middle level represents worst-case failures of components, i.e. interdicts the system, and the lowest level operates the remaining system. We describe new inequalities that characterize the set of resilient solutions and allow to reformulate the problem. The reformulation can then be solved using a nested branch-and-cut approach. We discuss several improvements, for instance, by taking symmetry into account and strengthening cuts. We demonstrate the effectiveness of our implementation on the optimal design of water networks, robust trusses, and gas networks, in comparison to an approach in which the failure scenarios are directly included into the model.},
  archive      = {J_OMS},
  author       = {Marc E. Pfetsch and Andreas Schmitt},
  doi          = {10.1080/10556788.2022.2142581},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {356-385},
  shortjournal = {Optim. Methods Softw.},
  title        = {A generic optimization framework for resilient systems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HyKKT: A hybrid direct-iterative method for solving KKT
linear systems. <em>OMS</em>, <em>38</em>(2), 332‚Äì355. (<a
href="https://doi.org/10.1080/10556788.2022.2124990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a solution strategy for the large indefinite linear systems arising in interior methods for nonlinear optimization. The method is suitable for implementation on hardware accelerators such as graphical processing units (GPUs). The current gold standard for sparse indefinite systems is the LBLT factorization where L L L is a lower triangular matrix and B B B is 1 √ó 1 1 √ó 1 1√ó1 or 2 √ó 2 2 √ó 2 2√ó2 block diagonal. However, this requires pivoting, which substantially increases communication cost and degrades performance on GPUs. Our approach solves a large indefinite system by solving multiple smaller positive definite systems, using an iterative solver on the Schur complement and an inner direct solve (via Cholesky factorization) within each iteration. Cholesky is stable without pivoting, thereby reducing communication and allowing reuse of the symbolic factorization. We demonstrate the practicality of our approach on large optimal power flow problems and show that it can efficiently utilize GPUs and outperform LBL T factorization of the full system.},
  archive      = {J_OMS},
  author       = {Shaked Regev and Nai-Yuan Chiang and Eric Darve and Cosmin G. Petra and Michael A. Saunders and Kasia ≈öwirydowicz and Slaven Pele≈°},
  doi          = {10.1080/10556788.2022.2124990},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {332-355},
  shortjournal = {Optim. Methods Softw.},
  title        = {HyKKT: A hybrid direct-iterative method for solving KKT linear systems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An incremental descent method for multi-objective
optimization. <em>OMS</em>, <em>38</em>(2), 312‚Äì331. (<a
href="https://doi.org/10.1080/10556788.2022.2124989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective steepest descent, under the assumption of lower-bounded objective functions with L -Lipschitz continuous gradients, requires O ( m / œµ 2 ) O ( m / œµ 2 ) O(m/œµ2) gradient and function computations to produce a measure of proximity to critical conditions akin to ‚à£ ‚à£ ‚à£ ‚à£ ‚àá f ( x ) ‚à£ ‚à£ ‚à£ ‚à£ ‚â§ œµ | | ‚àá f ( x ) | | ‚â§ œµ ||‚àáf(x)||‚â§œµ in the single-objective setting, where m is the number of objectives considered. We reduce this to O ( 1 / œµ 2 ) O ( 1 / œµ 2 ) O(1/œµ2) with a multi-objective incremental approach that has a computational cost that does not grow with the number of objective functions m .},
  archive      = {J_OMS},
  author       = {Ivo F. D. Oliveira and Ricardo H. C. Takahashi},
  doi          = {10.1080/10556788.2022.2124989},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {312-331},
  shortjournal = {Optim. Methods Softw.},
  title        = {An incremental descent method for multi-objective optimization},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the numerical performance of finite-difference-based
methods for derivative-free optimization. <em>OMS</em>, <em>38</em>(2),
289‚Äì311. (<a
href="https://doi.org/10.1080/10556788.2022.2121832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this paper is to investigate an approach for derivative-free optimization that has not received sufficient attention in the literature and is yet one of the simplest to implement and parallelize. In its simplest form, it consists of employing derivative-based methods for unconstrained or constrained optimization and replacing the gradient of the objective (and constraints) by finite-difference approximations. This approach is applicable to problems with or without noise in the functions. The differencing interval is determined by a bound on the second (or third) derivative and by the noise level, which is assumed to be known or to be accessible through difference tables or sampling. The use of finite-difference gradient approximations has been largely dismissed in the derivative-free optimization literature as too expensive in terms of function evaluations or as impractical in the presence of noise. However, the test results presented in this paper suggest that it has much to be recommended. The experiments compare newuoa, dfo-ls and cobyla against finite-difference versions of l-bfgs, lmder and knitro on three classes of problems: general unconstrained problems, nonlinear least squares problems and nonlinear programs with inequality constraints.},
  archive      = {J_OMS},
  author       = {Hao-Jun Michael Shi and Melody Qiming Xuan and Figen Oztoprak and Jorge Nocedal},
  doi          = {10.1080/10556788.2022.2121832},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {289-311},
  shortjournal = {Optim. Methods Softw.},
  title        = {On the numerical performance of finite-difference-based methods for derivative-free optimization},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient semismooth newton method for adaptive sparse
signal recovery problems. <em>OMS</em>, <em>38</em>(2), 262‚Äì288. (<a
href="https://doi.org/10.1080/10556788.2022.2120983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We know that compressive sensing can establish stable sparse recovery results from highly undersampled data under a restricted isometry property condition. In reality, however, numerous problems are coherent, and vast majority conventional methods might work not so well. Recently, it was shown that using the difference between ‚Ñì 1 ‚Ñì 1 ‚Ñì1 - and ‚Ñì 2 ‚Ñì 2 ‚Ñì2 -norm as a regularization always has superior performance. In this paper, we consider an adaptive ‚Ñì p ‚Ñì p ‚Ñìp - ‚Ñì 1 ‚àí 2 ‚Ñì 1 ‚àí 2 ‚Ñì1‚àí2 model where the ‚Ñì p ‚Ñì p ‚Ñìp -norm with p ‚â• 1 p ‚â• 1 p‚â•1 measures the data fidelity and the ‚Ñì 1 ‚àí 2 -term measures the sparsity. This proposed model has the ability to deal with different types of noises and extract the sparse property even under high coherent condition. We use a proximal majorization-minimization technique to handle the non-convex regularization term and then employ a semismooth Newton method to solve the corresponding convex relaxation subproblem. We prove that the sequence generated by the semismooth Newton method admits fast local convergence rate to the subproblem under some technical assumptions. Finally, we do some numerical experiments to demonstrate the superiority of the proposed model and the progressiveness of the proposed algorithm.},
  archive      = {J_OMS},
  author       = {Yanyun Ding and Haibin Zhang and Peili Li and Yunhai Xiao},
  doi          = {10.1080/10556788.2022.2120983},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {262-288},
  shortjournal = {Optim. Methods Softw.},
  title        = {An efficient semismooth newton method for adaptive sparse signal recovery problems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On minty variational principle for quasidifferentiable
vector optimization problems. <em>OMS</em>, <em>38</em>(2), 243‚Äì261. (<a
href="https://doi.org/10.1080/10556788.2022.2119235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with quasidifferentiable vector optimization problems involving invex functions wrt convex compact sets. We present vector variational-like inequalities of Minty type and of¬†Stampacchia type in terms of quasidifferentials denoted by (QMVVLI) and (QSVVLI), respectively. By utilizing these variational inequalities, we infer vital and adequate optimality conditions for an¬†efficient solution of the¬†quasidifferentiable vector optimization problem involving invex functions wrt convex compact sets. We also establish various results for the solutions of the corresponding weak versions of the vector variational-like inequalities in terms of quasidifferentials.},
  archive      = {J_OMS},
  author       = {Harsh Narayan Singh and Vivek Laha},
  doi          = {10.1080/10556788.2022.2119235},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {243-261},
  shortjournal = {Optim. Methods Softw.},
  title        = {On minty variational principle for quasidifferentiable vector optimization problems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implementation of a projection and rescaling algorithm for
second-order conic feasibility problems. <em>OMS</em>, <em>38</em>(1),
218‚Äì241. (<a
href="https://doi.org/10.1080/10556788.2022.2119234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper documents a computational implementation of a projection and rescaling algorithm for solving one of the alternative feasibility problems find x ‚àà L ‚à© Œ© or find x ÃÇ ‚àà L ‚ä• ‚à© Œ© , find x ‚àà L ‚à© Œ© or find x ^ ‚àà L ‚ä• ‚à© Œ© , findx‚ààL‚à©Œ©orfindx^‚ààL‚ä•‚à©Œ©, where L is a linear subspace in ‚Ñù n R n Rn , L ‚ä• L ‚ä• L‚ä• is its orthogonal complement, and Œ© ‚äÜ ‚Ñù n Œ© ‚äÜ R n Œ©‚äÜRn is the interior of a direct product of second order cones. The gist of the projection and rescaling algorithm is to enhance a low-cost first-order method (a basic procedure ) with an adaptive reconditioning transformation (a rescaling step ). We give a full description of a Python implementation of this algorithm and present multiple sets of numerical experiments on synthetic problem instances with varied levels of conditioning. Our computational experiments provide promising evidence of the effectiveness of the projection and rescaling algorithm. Our Python code is publicly available. Furthermore, the simplicity of the algorithm makes a computational implementation in other environments completely straightforward.},
  archive      = {J_OMS},
  author       = {Javier Pe√±a and Negar Soheili},
  doi          = {10.1080/10556788.2022.2119234},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {218-241},
  shortjournal = {Optim. Methods Softw.},
  title        = {Implementation of a projection and rescaling algorithm for second-order conic feasibility problems},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new randomized primal-dual algorithm for convex
optimization with fast last iterate convergence rates. <em>OMS</em>,
<em>38</em>(1), 184‚Äì217. (<a
href="https://doi.org/10.1080/10556788.2022.2119233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel unified randomized block-coordinate primal-dual algorithm to solve a class of nonsmooth constrained convex optimization problems, which covers different existing variants and model settings from the literature. We prove that our algorithm achieves ùí™ ( n / k ) O ( n / k ) O(n/k) and ùí™ ( n 2 / k 2 ) O ( n 2 / k 2 ) O(n2/k2) convergence rates in two cases: merely convexity and strong convexity, respectively, where k is the iteration counter and n is the number of block-coordinates. These rates are known to be optimal (up to a constant factor) when n =‚Äâ1. Our convergence rates are obtained through three criteria: primal objective residual and primal feasibility violation, dual objective residual, and primal-dual expected gap. Moreover, our rates for the primal problem are on the last-iterate sequence. Our dual convergence guarantee requires additionally a Lipschitz continuity assumption. We specify our algorithm to handle two important special cases, where our rates are still applied. Finally, we verify our algorithm on two well-studied numerical examples and compare it with two existing methods. Our results show that the proposed method has encouraging performance on different experiments.},
  archive      = {J_OMS},
  author       = {Quoc Tran-Dinh and Deyi Liu},
  doi          = {10.1080/10556788.2022.2119233},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {184-217},
  shortjournal = {Optim. Methods Softw.},
  title        = {A new randomized primal-dual algorithm for convex optimization with fast last iterate convergence rates},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonconvex equilibrium models for energy markets: Exploiting
price information to determine the existence of an equilibrium.
<em>OMS</em>, <em>38</em>(1), 153‚Äì183. (<a
href="https://doi.org/10.1080/10556788.2022.2117358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by examples from the energy sector, we consider market equilibrium problems (MEPs) involving players with nonconvex strategy spaces or objective functions, where the latter are assumed to be linear in market prices. We propose an algorithm that determines if an equilibrium of such an MEP exists and that computes an equilibrium in case of existence. Three key prerequisites have to be met. First, appropriate bounds on market prices have to be derived from necessary optimality conditions of some players. Second, a technical assumption is required for those prices that are not uniquely determined by the derived bounds. Third, nonconvex optimization problems have to be solved to global optimality. We test the algorithm on well-known instances from the power and gas literature that meet these three prerequisites. There, nonconvexities arise from considering the transmission system operator as an additional player besides producers and consumers who, e.g. switches lines or faces nonlinear physical laws. Our numerical results indicate that equilibria often exist, especially for the case of continuous nonconvexities in the context of gas market problems.},
  archive      = {J_OMS},
  author       = {Julia Gr√ºbel and Olivier Huber and Lukas H√ºmbs and Max Klimm and Martin Schmidt and Alexandra Schwartz},
  doi          = {10.1080/10556788.2022.2117358},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {153-183},
  shortjournal = {Optim. Methods Softw.},
  title        = {Nonconvex equilibrium models for energy markets: Exploiting price information to determine the existence of an equilibrium},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New iterative algorithms with self-adaptive step size for
solving split equality fixed point problem and its applications.
<em>OMS</em>, <em>38</em>(1), 128‚Äì152. (<a
href="https://doi.org/10.1080/10556788.2022.2117357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to propose a new alternative step size algorithm without using projections and without prior knowledge of operator norms to the split equality fixed point problem for a class of quasi-pseudo-contractive mappings. Under appropriate conditions, weak and strong convergence theorems for the presented algorithms are obtained, respectively. Furthermore, the algorithm proposed in this paper is also applied to approximate the solution of the split equality equilibrium and split equality inclusion problems.},
  archive      = {J_OMS},
  author       = {Yan Tang and Haiyun Zhou},
  doi          = {10.1080/10556788.2022.2117357},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {128-152},
  shortjournal = {Optim. Methods Softw.},
  title        = {New iterative algorithms with self-adaptive step size for solving split equality fixed point problem and its applications},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear programming with nonparametric penalty programs and
iterated thresholding. <em>OMS</em>, <em>38</em>(1), 107‚Äì127. (<a
href="https://doi.org/10.1080/10556788.2022.2117356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known¬† [Mangasarian, A Newton method for linear programming , J. Optim. Theory Appl. 121 (2004), pp. 1‚Äì18] that every linear program can be solved exactly by minimizing an unconstrained quadratic penalty program. The penalty program is parameterized by a scalar t &gt;0, and one is able to solve the original linear program in this manner when t is selected larger than a finite, but unknown t 0 &gt; 0 t 0 &gt; 0 t0&amp;gt;0 . In this paper, we show that every linear program can be solved using the solution to a parameter-free penalty program. We also characterize the solutions to the quadratic penalty programs using fixed points of certain nonexpansive maps. This leads to an iterative thresholding algorithm that converges to a desired limit point. We show in numerical experiments that this iterative method can outperform a variety of standard quadratic program solvers. Finally, we show that for every t ‚àà R , the solution one obtains by solving a parameterized penalty program is guaranteed to lie in the feasible set of the original linear program.},
  archive      = {J_OMS},
  author       = {Jeffery Kline and Glenn Martin Fung},
  doi          = {10.1080/10556788.2022.2117356},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {107-127},
  shortjournal = {Optim. Methods Softw.},
  title        = {Linear programming with nonparametric penalty programs and iterated thresholding},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic distributed learning with gradient quantization
and double-variance reduction. <em>OMS</em>, <em>38</em>(1), 91‚Äì106. (<a
href="https://doi.org/10.1080/10556788.2022.2117355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider distributed optimization over several devices, each sending incremental model updates to a central server. This setting is considered, for instance, in federated learning. Various schemes have been designed to compress the model updates in order to reduce the overall communication cost. However, existing methods suffer from a significant slowdown due to additional variance œâ &gt; 0 œâ &gt; 0 œâ&amp;gt;0 coming from the compression operator and as a result, only converge sublinearly. What is needed is a variance reduction technique for taming the variance introduced by compression. We propose the first methods that achieve linear convergence for arbitrary compression operators. For strongly convex functions with condition number Œ∫ , distributed among n machines with a finite-sum structure, each worker having less than m components, we also (i)¬†give analysis for the weakly convex and the non-convex cases and (ii) verify in experiments that our novel variance reduced schemes are more efficient than the baselines. Moreover, we show theoretically that as the number of devices increases, higher compression levels are possible without this affecting the overall number of communications in comparison with methods that do not perform any compression. This leads to a significant reduction in communication cost. Our general analysis allows to pick the most suitable compression for each problem, finding the right balance between additional variance and communication savings. Finally, we also (iii) give analysis for arbitrary quantized updates.},
  archive      = {J_OMS},
  author       = {Samuel Horv√°th and Dmitry Kovalev and Konstantin Mishchenko and Peter Richt√°rik and Sebastian Stich},
  doi          = {10.1080/10556788.2022.2117355},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {91-106},
  shortjournal = {Optim. Methods Softw.},
  title        = {Stochastic distributed learning with gradient quantization and double-variance reduction},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the implementation of a quasi-newton interior-point
method for PDE-constrained optimization using finite element
discretizations. <em>OMS</em>, <em>38</em>(1), 59‚Äì90. (<a
href="https://doi.org/10.1080/10556788.2022.2117354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a quasi-Newton interior-point method appropriate for optimization problems with pointwise inequality constraints in Hilbert function spaces. Among others, our methodology applies to optimization problems constrained by partial differential equations (PDEs) that are posed in a reduced-space formulation and have bounds or inequality constraints on the optimized parameter function. We first introduce the formalization of an infinite-dimensional quasi-Newton interior-point algorithm using secant BFGS updates and then proceed to derive a discretized interior-point method capable of working with a wide range of finite element discretization schemes. We also discuss and address mathematical and software interface issues that are pervasive when existing off-the-shelf PDE solvers are to be used with off-the-shelf nonlinear programming solvers. Finally, we elaborate on the numerical and parallel computing strengths and limitations of the proposed methodology on several classes of PDE-constrained problems.},
  archive      = {J_OMS},
  author       = {Cosmin G. Petra and Miguel Salazar De Troya and Noemi Petra and Youngsoo Choi and Geoffrey M. Oxberry and Daniel Tortorelli},
  doi          = {10.1080/10556788.2022.2117354},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {59-90},
  shortjournal = {Optim. Methods Softw.},
  title        = {On the implementation of a quasi-newton interior-point method for PDE-constrained optimization using finite element discretizations},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A stochastic approximation method for convex programming
with many semidefinite constraints. <em>OMS</em>, <em>38</em>(1), 34‚Äì58.
(<a href="https://doi.org/10.1080/10556788.2022.2091563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a type of semidefinite programming problem (MSDP), which involves many (not necessarily finite) of semidefinite constraints. MSDP can be established in a wide range of applications, including covering ellipsoids problem and truss topology design. We propose a random method based on a stochastic approximation technique for solving MSDP, without calculating and storing the multiplier. Under mild conditions, we establish the almost sure convergence and expected convergence rates of the proposed method. A variety of simulation experiments are carried out to support our theoretical results.},
  archive      = {J_OMS},
  author       = {Pang Li-Ping and Zhang Ming-Kun and Xiao Xian-Tao},
  doi          = {10.1080/10556788.2022.2091563},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {34-58},
  shortjournal = {Optim. Methods Softw.},
  title        = {A stochastic approximation method for convex programming with many semidefinite constraints},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On inexact stochastic splitting methods for a class of
nonconvex composite optimization problems with relative error.
<em>OMS</em>, <em>38</em>(1), 1‚Äì33. (<a
href="https://doi.org/10.1080/10556788.2022.2091562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider minimizing a class of nonconvex composite stochastic optimization problems, and deterministic optimization problems whose objective function consists of an expectation function (or an average of finitely many smooth functions) and a weakly convex but potentially nonsmooth function. And in this paper, we focus on the theoretical properties of two types of stochastic splitting methods for solving these nonconvex optimization problems: stochastic alternating direction method of multipliers and stochastic proximal gradient descent. In particular, several inexact versions of these two types of splitting methods are studied. At each iteration, the proposed schemes inexactly solve their subproblems by using relative error criteria instead of exogenous and diminishing error rules, which allows our approaches to handle some complex regularized problems in statistics and machine learning. Under mild conditions, we obtain the convergence of the schemes and their computational complexity related to the evaluations on the component gradient of the smooth function, and find that some conclusions of their exact counterparts can be recovered.},
  archive      = {J_OMS},
  author       = {Jia Hu and Congying Han and Tiande Guo and Tong Zhao},
  doi          = {10.1080/10556788.2022.2091562},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Optim. Methods Softw.},
  title        = {On inexact stochastic splitting methods for a class of nonconvex composite optimization problems with relative error},
  volume       = {38},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
