<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JASA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jasa---116">JASA - 116</h2>
<ul>
<li><details>
<summary>
(2023). Bayesian conjugacy in probit, tobit, multinomial probit and
extensions: A review and new results. <em>JASA</em>, <em>118</em>(542),
1451–1469. (<a
href="https://doi.org/10.1080/01621459.2023.2169150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A broad class of models that routinely appear in several fields can be expressed as partially or fully discretized Gaussian linear regressions. Besides including classical Gaussian response settings, this class also encompasses probit, multinomial probit and tobit regression, among others, thereby yielding one of the most widely-implemented families of models in routine applications. The relevance of such representations has stimulated decades of research in the Bayesian field, mostly motivated by the fact that, unlike for Gaussian linear regression, the posterior distribution induced by such models does not seem to belong to a known class, under the commonly assumed Gaussian priors for the coefficients. This has motivated several solutions for posterior inference relying either on sampling-based strategies or on deterministic approximations that, however, still experience computational and accuracy issues, especially in high dimensions. The scope of this article is to review, unify and extend recent advances in Bayesian inference and computation for this core class of models. To address such a goal, we prove that the likelihoods induced by these formulations share a common analytical structure implying conjugacy with a broad class of distributions, namely the unified skew-normal (SUN), that generalize Gaussians to include skewness. This result unifies and extends recent conjugacy properties for specific models within the class analyzed, and opens new avenues for improved posterior inference, under a broader class of formulations and priors, via novel closed-form expressions, iid samplers from the exact SUN posteriors, and more accurate and scalable approximations from variational Bayes and expectation-propagation. Such advantages are illustrated in simulations and are expected to facilitate the routine-use of these core Bayesian models, while providing novel frameworks for studying theoretical properties and developing future extensions. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Niccolò Anceschi and Augusto Fasano and Daniele Durante and Giacomo Zanella},
  doi          = {10.1080/01621459.2023.2169150},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1451-1469},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian conjugacy in probit, tobit, multinomial probit and extensions: A review and new results},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Handbook of bayesian variable selection. <em>JASA</em>,
<em>118</em>(542), 1449–1450. (<a
href="https://doi.org/10.1080/01621459.2023.2183128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Yang Ni},
  doi          = {10.1080/01621459.2023.2183128},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1449-1450},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of bayesian variable selection},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical thinking in clinical trials. <em>JASA</em>,
<em>118</em>(542), 1448–1449. (<a
href="https://doi.org/10.1080/01621459.2023.2183001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Susan S. Ellenberg},
  doi          = {10.1080/01621459.2023.2183001},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1448-1449},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical thinking in clinical trials},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernel-based partial permutation test for detecting
heterogeneous functional relationship. <em>JASA</em>, <em>118</em>(542),
1429–1447. (<a
href="https://doi.org/10.1080/01621459.2021.2000867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a kernel-based partial permutation test for checking the equality of functional relationship between response and covariates among different groups. The main idea, which is intuitive and easy to implement, is to keep the projections of the response vector Y on leading principle components of a kernel matrix fixed and permute Y ’s projections on the remaining principle components. The proposed test allows for different choices of kernels, corresponding to different classes of functions under the null hypothesis. First, using linear or polynomial kernels, our partial permutation tests are exactly valid in finite samples for linear or polynomial regression models with Gaussian noise; similar results straightforwardly extend to kernels with finite feature spaces. Second, by allowing the kernel feature space to diverge with the sample size, the test can be large-sample valid for a wider class of functions. Third, for general kernels with possibly infinite-dimensional feature space, the partial permutation test is exactly valid when the covariates are exactly balanced across all groups, or asymptotically valid when the underlying function follows certain regularized Gaussian processes. We further suggest test statistics using likelihood ratio between two (nested) Gaussian process regression models, and propose computationally efficient algorithms utilizing the EM algorithm and Newton’s method, where the latter also involves Fisher scoring and quadratic programming and is particularly useful when EM suffers from slow convergence. Extensions to correlated and non-Gaussian noises have also been investigated theoretically or numerically. Furthermore, the test can be extended to use multiple kernels together and can thus enjoy properties from each kernel. Both simulation study and application illustrate the properties of the proposed test.},
  archive      = {J_JASA},
  author       = {Xinran Li and Bo Jiang and Jun S. Liu},
  doi          = {10.1080/01621459.2021.2000867},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1429-1447},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Kernel-based partial permutation test for detecting heterogeneous functional relationship},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven selection of the number of change-points via
error rate control. <em>JASA</em>, <em>118</em>(542), 1415–1428. (<a
href="https://doi.org/10.1080/01621459.2021.1999820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiple change-point analysis, one of the main difficulties is to determine the number of change-points. Various consistent selection methods, including the use of Schwarz information criterion and cross-validation, have been proposed to balance the model fitting and complexity. However, there is lack of systematic approaches to provide theoretical guarantee of significance in determining the number of changes. In this paper, we introduce a data-adaptive selection procedure via error rate control based on order-preserving sample-splitting, which is applicable to most existing change-point methods. The key idea is to construct a series of statistics with global symmetry property and then utilize the symmetry to derive a data-driven threshold. Under this general framework, we are able to rigorously investigate the false discovery proportion control, and show that the proposed method controls the false discovery rate (FDR) asymptotically under mild conditions while retaining the true change-points. Numerical experiments indicate that our selection procedure works well for many change-detection methods and is able to yield accurate FDR control in finite samples. Keywords: Empirical distribution; False discovery rate; Multiple change-point model; Sample-splitting; Symmetry; Uniform convergence.},
  archive      = {J_JASA},
  author       = {Hui Chen and Haojie Ren and Fang Yao and Changliang Zou},
  doi          = {10.1080/01621459.2021.1999820},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1415-1428},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Data-driven selection of the number of change-points via error rate control},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A likelihood-based approach for multivariate categorical
response regression in high dimensions. <em>JASA</em>,
<em>118</em>(542), 1402–1414. (<a
href="https://doi.org/10.1080/01621459.2021.1999819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a penalized likelihood method to fit the bivariate categorical response regression model. Our method allows practitioners to estimate which predictors are irrelevant, which predictors only affect the marginal distributions of the bivariate response, and which predictors affect both the marginal distributions and log odds ratios. To compute our estimator, we propose an efficient algorithm which we extend to settings where some subjects have only one response variable measured, that is, a semi-supervised setting. We derive an asymptotic error bound which illustrates the performance of our estimator in high-dimensional settings. Generalizations to the multivariate categorical response regression model are proposed. Finally, simulation studies and an application in pan-cancer risk prediction demonstrate the usefulness of our method in terms of interpretability and prediction accuracy. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Aaron J. Molstad and Adam J. Rothman},
  doi          = {10.1080/01621459.2021.1999819},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1402-1414},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A likelihood-based approach for multivariate categorical response regression in high dimensions},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized factor model for ultra-high dimensional
correlated variables with mixed types. <em>JASA</em>, <em>118</em>(542),
1385–1401. (<a
href="https://doi.org/10.1080/01621459.2021.1999818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As high-dimensional data measured with mixed-type variables gradually become prevalent, it is particularly appealing to represent those mixed-type high-dimensional data using a much smaller set of so-called factors. Due to the limitation of the existing methods for factor analysis that deal with only continuous variables, in this article, we develop a generalized factor model, a corresponding algorithm and theory for ultra-high dimensional mixed types of variables where both the sample size n and variable dimension p could diverge to infinity. Specifically, to solve the computational problem arising from the non-linearity and mixed types, we develop a two-step algorithm so that each update can be carried out in parallel across variables and samples by using an existing package. Theoretically, we establish the rate of convergence for the estimators of factors and loadings in the presence of nonlinear structure accompanied with mixed-type variables when both n and p diverge to infinity. Moreover, since the correct specification of the number of factors is crucial to both the theoretical and the empirical validity of factor models, we also develop a criterion based on a penalized loss to consistently estimate the number of factors under the framework of a generalized factor model. To demonstrate the advantages of the proposed method over the existing ones, we conducted extensive simulation studies and also applied it to the analysis of the NFBC1966 dataset and a cardiac arrhythmia dataset, resulting in more predictive and interpretable estimators for loadings and factors than the existing factor model.},
  archive      = {J_JASA},
  author       = {Wei Liu and Huazhen Lin and Shurong Zheng and Jin Liu},
  doi          = {10.1080/01621459.2021.1999818},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1385-1401},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Generalized factor model for ultra-high dimensional correlated variables with mixed types},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling the extremes of bivariate mixture distributions
with application to oceanographic data. <em>JASA</em>,
<em>118</em>(542), 1373–1384. (<a
href="https://doi.org/10.1080/01621459.2021.1996379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There currently exist a variety of statistical methods for modeling bivariate extremes. However, when the dependence between variables is driven by more than one latent process, these methods are likely to fail to give reliable inferences. We consider situations in which the observed dependence at extreme levels is a mixture of a possibly unknown number of much simpler bivariate distributions. For such structures, we demonstrate the limitations of existing methods and propose two new methods: an extension of the Heffernan–Tawn conditional extreme value model to allow for mixtures and an extremal quantile-regression approach. The two methods are examined in a simulation study and then applied to oceanographic data. Finally, we discuss extensions including a subasymptotic version of the proposed model, which has the potential to give more efficient results by incorporating data that are less extreme. Both new methods outperform existing approaches when mixtures are present.},
  archive      = {J_JASA},
  author       = {Stan Tendijck and Emma Eastoe and Jonathan Tawn and David Randell and Philip Jonathan},
  doi          = {10.1080/01621459.2021.1996379},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1373-1384},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling the extremes of bivariate mixture distributions with application to oceanographic data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast network community detection with profile-pseudo
likelihood methods. <em>JASA</em>, <em>118</em>(542), 1359–1372. (<a
href="https://doi.org/10.1080/01621459.2021.1996378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stochastic block model is one of the most studied network models for community detection, and fitting its likelihood function on large-scale networks is known to be challenging. One prominent work that overcomes this computational challenge is the fast pseudo-likelihood approach proposed by Amini et al. for fitting stochastic block models to large sparse networks. However, this approach does not have convergence guarantee, and may not be well suited for small and medium scale networks. In this article, we propose a novel likelihood based approach that decouples row and column labels in the likelihood function, enabling a fast alternating maximization. This new method is computationally efficient, performs well for both small- and large-scale networks, and has provable convergence guarantee. We show that our method provides strongly consistent estimates of communities in a stochastic block model. We further consider extensions of our proposed method to handle networks with degree heterogeneity and bipartite properties. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jiangzhou Wang and Jingfei Zhang and Binghui Liu and Ji Zhu and Jianhua Guo},
  doi          = {10.1080/01621459.2021.1996378},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1359-1372},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fast network community detection with profile-pseudo likelihood methods},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating bayesian structure learning in sparse gaussian
graphical models. <em>JASA</em>, <em>118</em>(542), 1345–1358. (<a
href="https://doi.org/10.1080/01621459.2021.1996377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian structure learning in Gaussian graphical models is often done by search algorithms over the graph space.The conjugate prior for the precision matrix satisfying graphical constraints is the well-known G -Wishart.With this prior, the transition probabilities in the search algorithms necessitate evaluating the ratios of the prior normalizing constants of G -Wishart.In moderate to high-dimensions, this ratio is often approximated by using sampling-based methods as computationally expensive updates in the search algorithm.Calculating this ratio so far has been a major computational bottleneck.We overcome this issue by representing a search algorithm in which the ratio of normalizing constants is carried out by an explicit closed-form approximation.Using this approximation within our search algorithm yields significant improvement in the scalability of structure learning without sacrificing structure learning accuracy.We study the conditions under which the approximation is valid.We also evaluate the efficacy of our method with simulation studies.We show that the new search algorithm with our approximation outperforms state-of-the-art methods in both computational efficiency and accuracy.The implementation of our work is available in the R package BDgraph.},
  archive      = {J_JASA},
  author       = {Reza Mohammadi and Hélène Massam and Gérard Letac},
  doi          = {10.1080/01621459.2021.1996377},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1345-1358},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Accelerating bayesian structure learning in sparse gaussian graphical models},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrepancy between global and local principal component
analysis on large-panel high-frequency data. <em>JASA</em>,
<em>118</em>(542), 1333–1344. (<a
href="https://doi.org/10.1080/01621459.2021.1996376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the discrepancy between the global principal component analysis (GPCA) and local principal component analysis (LPCA) in recovering the common components of a large-panel high-frequency data. We measure the discrepancy by the total sum of squared differences between common components reconstructed from GPCA and LPCA. The asymptotic distribution of the discrepancy measure is provided when the factor space is time invariant as the dimension p and sample size n tend to infinity simultaneously. Alternatively when the factor space changes, the discrepancy measure explodes under some mild signal condition on the magnitude of time-variation of the factor space. We apply the theory to test the invariance in time of the factor space. The test performs well in controlling the Type I error and detecting time-varying factor spaces. This is checked by extensive simulation studies. A real data analysis provides strong evidences that the factor space is always time-varying within a time span longer than one week.},
  archive      = {J_JASA},
  author       = {Xin-Bing Kong and Jin-Guan Lin and Cheng Liu and Guang-Ying Liu},
  doi          = {10.1080/01621459.2021.1996376},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1333-1344},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discrepancy between global and local principal component analysis on large-panel high-frequency data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical inference for high-dimensional generalized
linear models with binary outcomes. <em>JASA</em>, <em>118</em>(542),
1319–1332. (<a
href="https://doi.org/10.1080/01621459.2021.1990769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a unified statistical inference framework for high-dimensional binary generalized linear models (GLMs) with general link functions. Both unknown and known design distribution settings are considered. A two-step weighted bias-correction method is proposed for constructing confidence intervals (CIs) and simultaneous hypothesis tests for individual components of the regression vector. Minimax lower bound for the expected length is established and the proposed CIs are shown to be rate-optimal up to a logarithmic factor. The numerical performance of the proposed procedure is demonstrated through simulation studies and an analysis of a single cell RNA-seq dataset, which yields interesting biological insights that integrate well into the current literature on the cellular immune response mechanisms as characterized by single-cell transcriptomics. The theoretical analysis provides important insights on the adaptivity of optimal CIs with respect to the sparsity of the regression vector. New lower bound techniques are introduced and they can be of independent interest to solve other inference problems in high-dimensional binary GLMs.},
  archive      = {J_JASA},
  author       = {T. Tony Cai and Zijian Guo and Rong Ma},
  doi          = {10.1080/01621459.2021.1990769},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1319-1332},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for high-dimensional generalized linear models with binary outcomes},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Independent nonlinear component analysis. <em>JASA</em>,
<em>118</em>(542), 1305–1318. (<a
href="https://doi.org/10.1080/01621459.2021.1990768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea of summarizing the information contained in a large number of variables by a small number of “factors” or “principal components” has been broadly adopted in statistics. This article introduces a generalization of the widely used principal component analysis (PCA) to nonlinear settings, thus providing a new tool for dimension reduction and exploratory data analysis or representation. The distinguishing features of the method include (i) the ability to always deliver truly independent (instead of merely uncorrelated) factors; (ii) the use of optimal transport theory and Brenier maps to obtain a robust and efficient computational algorithm; (iii) the use of a new multivariate additive entropy decomposition to determine the most informative principal nonlinear components, and (iv) formally nesting PCA as a special case for linear Gaussian factor models. We illustrate the method’s effectiveness in an application to excess bond returns prediction from a large number of macro factors. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Florian Gunsilius and Susanne Schennach},
  doi          = {10.1080/01621459.2021.1990768},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1305-1318},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Independent nonlinear component analysis},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rerandomization in stratified randomized experiments.
<em>JASA</em>, <em>118</em>(542), 1295–1304. (<a
href="https://doi.org/10.1080/01621459.2021.1990767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stratification and rerandomization are two well-known methods used in randomized experiments for balancing the baseline covariates. Renowned scholars in experimental design have recommended combining these two methods; however, limited studies have addressed the statistical properties of this combination. This article proposes two rerandomization methods to be used in stratified randomized experiments, based on the overall and stratum-specific Mahalanobis distances. The first method is applicable for nearly arbitrary numbers of strata, strata sizes, and stratum-specific proportions of the treated units. The second method, which is generally more efficient than the first method, is suitable for situations in which the number of strata is fixed with their sizes tending to infinity. Under the randomization inference framework, we obtain the asymptotic distributions of estimators used in these methods and the formulas of variance reduction when compared to stratified randomization. Our analysis does not require any modeling assumption regarding the potential outcomes. Moreover, we provide asymptotically conservative variance estimators and confidence intervals for the average treatment effect. The advantages of the proposed methods are exhibited through an extensive simulation study and a real-data example.},
  archive      = {J_JASA},
  author       = {Xinhe Wang and Tingyu Wang and Hanzhong Liu},
  doi          = {10.1080/01621459.2021.1990767},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1295-1304},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rerandomization in stratified randomized experiments},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating association between two event times with
observations subject to informative censoring. <em>JASA</em>,
<em>118</em>(542), 1282–1294. (<a
href="https://doi.org/10.1080/01621459.2021.1990766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with evaluating the association between two event times without specifying the joint distribution parametrically. This is particularly challenging when the observations on the event times are subject to informative censoring due to a terminating event such as death. There are few methods suitable for assessing covariate effects on association in this context. We link the joint distribution of the two event times and the informative censoring time using a nested copula function. We use flexible functional forms to specify the covariate effects on both the marginal and joint distributions. In a semiparametric model for the bivariate event time, we estimate simultaneously the association parameters, the marginal survival functions, and the covariate effects. A byproduct of the approach is a consistent estimator for the induced marginal survival function of each event time conditional on the covariates. We develop an easy-to-implement pseudolikelihood-based inference procedure, derive the asymptotic properties of the estimators, and conduct simulation studies to examine the finite-sample performance of the proposed approach. For illustration, we apply our method to analyze data from the breast cancer survivorship study that motivated this research. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Dongdong Li and X. Joan Hu and Rui Wang},
  doi          = {10.1080/01621459.2021.1990766},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1282-1294},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Evaluating association between two event times with observations subject to informative censoring},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification, semiparametric efficiency, and quadruply
robust estimation in mediation analysis with treatment-induced
confounding. <em>JASA</em>, <em>118</em>(542), 1272–1281. (<a
href="https://doi.org/10.1080/01621459.2021.1990765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural mediation effects are often of interest when the goal is to understand a causal mechanism. However, most existing methods and their identification assumptions preclude treatment-induced confounders often present in practice. To address this fundamental limitation, we provide a set of assumptions that identify the natural direct effect in the presence of treatment-induced confounders. Even when some of those assumptions are violated, the estimand still has an interventional direct effect interpretation. We derive the semiparametric efficiency bound for the estimand, which unlike usual expressions, contains conditional densities that are variational dependent. We consider a reparameterization and propose a quadruply robust estimator that remains consistent under four types of possible misspecification and is also locally semiparametric efficient. We use simulation studies to demonstrate the proposed method and study an application to the 2017 Natality data to investigate the effect of prenatal care on preterm birth mediated by preeclampsia with smoking status during pregnancy being a potential treatment-induced confounder.},
  archive      = {J_JASA},
  author       = {Fan Xia and Kwun Chuen Gary Chan},
  doi          = {10.1080/01621459.2021.1990765},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1272-1281},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Identification, semiparametric efficiency, and quadruply robust estimation in mediation analysis with treatment-induced confounding},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal simulator selection. <em>JASA</em>,
<em>118</em>(542), 1264–1271. (<a
href="https://doi.org/10.1080/01621459.2021.1987920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer simulators are widely used for the study of complex systems. In many applications, there are multiple simulators available with different scientific interpretations of the underlying mechanism, and the goal is to identify an optimal simulator based on the observed physical experiments. To achieve the goal, we propose a selection criterion based on leave-one-out cross-validation. This criterion consists of a goodness-of-fit measure and a generalized degrees of freedom penalizing the simulator sensitivity to perturbations in the physical observations. Asymptotic properties of the selected optimal simulator are discussed. It is shown that the proposed procedure includes a conventional calibration method as a special case. The finite sample performance of the proposed procedure is demonstrated through numerical examples. In the application of cell biology, an optimal simulator is selected, which can shed light on the T cell recognition mechanism in the human immune system. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ying Hung and Li-Hsiang Lin and C. F. Jeff Wu},
  doi          = {10.1080/01621459.2021.1987920},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1264-1271},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal simulator selection},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GEE-assisted variable selection for latent variable models
with multivariate binary data. <em>JASA</em>, <em>118</em>(542),
1252–1263. (<a
href="https://doi.org/10.1080/01621459.2021.1987251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate data are commonly analyzed using one of two approaches: a conditional approach based on generalized linear latent variable models (GLLVMs) or some variation thereof, and a marginal approach based on generalized estimating equations (GEEs). With research on mixed models and GEEs having gone down separate paths, there is a common mindset to treat the two approaches as mutually exclusive, with which to use driven by the question of interest. In this article, focusing on multivariate binary responses, we study the connections between the parameters from conditional and marginal models, with the aim of using GEEs for fast variable selection in GLLVMs. This is accomplished through two main contributions. First, we show that GEEs are zero consistent for GLLVMs fitted to multivariate binary data. That is, if the true model is a GLLVM but we misspecify and fit GEEs, then the latter is able to asymptotically differentiate between truly zero versus nonzero coefficients in the former. Building on this result, we propose GEE-assisted variable selection for GLLVMs using score- and Wald-based information criteria to construct a fast forward selection path followed by pruning. We demonstrate GEE-assisted variable selection is selection consistent for the underlying GLLVM, with simulation studies demonstrating its strong finite sample performance and computational efficiency.},
  archive      = {J_JASA},
  author       = {Francis K. C. Hui and Samuel Müller and A. H. Welsh},
  doi          = {10.1080/01621459.2021.1987251},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1252-1263},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {GEE-assisted variable selection for latent variable models with multivariate binary data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-assisted estimation through random forests in finite
population sampling. <em>JASA</em>, <em>118</em>(542), 1234–1251. (<a
href="https://doi.org/10.1080/01621459.2021.1987250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In surveys, the interest lies in estimating finite population parameters such as population totals and means. In most surveys, some auxiliary information is available at the estimation stage. This information may be incorporated in the estimation procedures to increase their precision. In this article, we use random forests (RFs) to estimate the functional relationship between the survey variable and the auxiliary variables. In recent years, RFs have become attractive as National Statistical Offices have now access to a variety of data sources, potentially exhibiting a large number of observations on a large number of variables. We establish the theoretical properties of model-assisted procedures based on RFs and derive corresponding variance estimators. A model-calibration procedure for handling multiple survey variables is also discussed. The results of a simulation study suggest that the proposed point and estimation procedures perform well in terms of bias, efficiency and coverage of normal-based confidence intervals, in a wide variety of settings. Finally, we apply the proposed methods using data on radio audiences collected by Médiamétrie, a French audience company. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Mehdi Dagdoug and Camelia Goga and David Haziza},
  doi          = {10.1080/01621459.2021.1987250},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1234-1251},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-assisted estimation through random forests in finite population sampling},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure of nonregular two-level designs. <em>JASA</em>,
<em>118</em>(542), 1222–1233. (<a
href="https://doi.org/10.1080/01621459.2021.1984927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-level fractional factorial designs are often used in screening scenarios to identify active factors. This article investigates the block diagonal structure of the information matrix of nonregular two-level designs. This structure is appealing since estimates of parameters belonging to different diagonal submatrices are uncorrelated. As such, the covariance matrix of the least squares estimates is simplified and the number of linear dependencies is reduced. We connect the block diagonal information matrix to the parallel flats design (PFD) literature and gain insights into the structure of what is estimable and/or aliased using the concept of minimal dependent sets. We show how to determine the number of parallel flats for any given design, and how to construct a design with a specified number of parallel flats. The usefulness of our construction method is illustrated by producing designs for estimation of the two-factor interaction model with three or more parallel flats. We also provide a fuller understanding of recently proposed group orthogonal supersaturated designs. Benefits of PFDs for analysis, including bias containment, are also discussed.},
  archive      = {J_JASA},
  author       = {David J. Edwards and Robert W. Mee},
  doi          = {10.1080/01621459.2021.1984927},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1222-1233},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Structure of nonregular two-level designs},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rejective sampling, rerandomization, and regression
adjustment in survey experiments. <em>JASA</em>, <em>118</em>(542),
1207–1221. (<a
href="https://doi.org/10.1080/01621459.2021.1984926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical randomized experiments, equipped with randomization-based inference, provide assumption-free inference for treatment effects. They have been the gold standard for drawing causal inference and provide excellent internal validity. However, they have also been criticized for questionable external validity, in the sense that the conclusion may not generalize well to a larger population. The randomized survey experiment is a design tool that can help mitigate this concern, by randomly selecting the experimental units from the target population of interest. However, as pointed out by Morgan and Rubin, chance imbalances often exist in covariate distributions between different treatment groups even under completely randomized experiments. Not surprisingly, such covariate imbalances also occur in randomized survey experiments. Furthermore, the covariate imbalances happen not only between different treatment groups, but also between the sampled experimental units and the overall population of interest. In this article, we propose a two-stage rerandomization design that can actively avoid undesirable covariate imbalances at both the sampling and treatment assignment stages. We further develop asymptotic theory for rerandomized survey experiments, demonstrating that rerandomization provides better covariate balance, more precise treatment effect estimators, and shorter large-sample confidence intervals. We also propose covariate adjustment to deal with remaining covariate imbalances after rerandomization, showing that it can further improve both the sampling and estimated precision. Our work allows general relationship among covariates at the sampling, treatment assignment and analysis stages, and generalizes both rerandomization in classical randomized experiments and rejective sampling in survey sampling.},
  archive      = {J_JASA},
  author       = {Zihao Yang and Tianyi Qu and Xinran Li},
  doi          = {10.1080/01621459.2021.1984926},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1207-1221},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejective sampling, rerandomization, and regression adjustment in survey experiments},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assumption-lean analysis of cluster randomized trials in
infectious diseases for intent-to-treat effects and network effects.
<em>JASA</em>, <em>118</em>(542), 1195–1206. (<a
href="https://doi.org/10.1080/01621459.2021.1983437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials (CRTs) are a popular design to study the effect of interventions in infectious disease settings. However, standard analysis of CRTs primarily relies on strong parametric methods, usually mixed-effect models to account for the clustering structure, and focuses on the overall intent-to-treat (ITT) effect to evaluate effectiveness. The article presents two assumption-lean methods to analyze two types of effects in CRTs, ITT effects and network effects among well-known compliance groups. For the ITT effects, we study the overall and the heterogeneous ITT effects among the observed covariates where we do not impose parametric models or asymptotic restrictions on cluster size. For the network effects among compliance groups, we propose a new bound-based method that uses pretreatment covariates, classification algorithms, and a linear program to obtain sharp bounds. A key feature of our method is that the bounds can become narrower as the classification algorithm improves and the method may also be useful for studies of partial identification with instrumental variables. We conclude by reanalyzing a CRT studying the effect of face masks and hand sanitizers on transmission of 2008 interpandemic influenza in Hong Kong.},
  archive      = {J_JASA},
  author       = {Chan Park and Hyunseung Kang},
  doi          = {10.1080/01621459.2021.1983437},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1195-1206},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Assumption-lean analysis of cluster randomized trials in infectious diseases for intent-to-treat effects and network effects},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hamiltonian-assisted metropolis sampling. <em>JASA</em>,
<em>118</em>(542), 1176–1194. (<a
href="https://doi.org/10.1080/01621459.2021.1982723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various Markov chain Monte Carlo (MCMC) methods are studied to improve upon random walk Metropolis sampling, for simulation from complex distributions. Examples include Metropolis-adjusted Langevin algorithms, Hamiltonian Monte Carlo, and other algorithms related to underdamped Langevin dynamics. We propose a broad class of irreversible sampling algorithms, called Hamiltonian-assisted Metropolis sampling (HAMS), and develop two specific algorithms with appropriate tuning and preconditioning strategies. Our HAMS algorithms are designed to simultaneously achieve two distinctive properties, while using an augmented target density with a momentum as an auxiliary variable. One is generalized detailed balance, which induces an irreversible exploration of the target. The other is a rejection-free property for a Gaussian target with a prespecified variance matrix. This property allows our preconditioned algorithms to perform satisfactorily with relatively large step sizes. Furthermore, we formulate a framework of generalized Metropolis–Hastings sampling, which not only highlights our construction of HAMS at a more abstract level, but also facilitates possible further development of irreversible MCMC algorithms. We present several numerical experiments, where the proposed algorithms consistently yield superior results among existing algorithms using the same preconditioning schemes.},
  archive      = {J_JASA},
  author       = {Zexi Song and Zhiqiang Tan},
  doi          = {10.1080/01621459.2021.1982723},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1176-1194},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Hamiltonian-assisted metropolis sampling},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Saddlepoint approximations for spatial panel data models.
<em>JASA</em>, <em>118</em>(542), 1164–1175. (<a
href="https://doi.org/10.1080/01621459.2021.1981913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop new higher-order asymptotic techniques for the Gaussian maximum likelihood estimator in a spatial panel data model, with fixed effects, time-varying covariates, and spatially correlated errors. Our saddlepoint density and tail area approximation feature relative error of order O ( 1 / ( n ( T − 1 ) ) ) O ( 1 / ( n ( T − 1 ) ) ) O(1/(n(T−1))) with n being the cross-sectional dimension and T the time-series dimension. The main theoretical tool is the tilted-Edgeworth technique in a nonidentically distributed setting. The density approximation is always nonnegative, does not need resampling, and is accurate in the tails. Monte Carlo experiments on density approximation and testing in the presence of nuisance parameters illustrate the good performance of our approximation over first-order asymptotics and Edgeworth expansion. An empirical application to the investment–saving relationship in OECD (Organisation for Economic Co-operation and Development) countries shows disagreement between testing results based on the first-order asymptotics and saddlepoint techniques. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Chaonan Jiang and Davide La Vecchia and Elvezio Ronchetti and Olivier Scaillet},
  doi          = {10.1080/01621459.2021.1981913},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1164-1175},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Saddlepoint approximations for spatial panel data models},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-robust inference for clinical trials that improve
precision by stratified randomization and covariate adjustment.
<em>JASA</em>, <em>118</em>(542), 1152–1163. (<a
href="https://doi.org/10.1080/01621459.2021.1981338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two commonly used methods for improving precision and power in clinical trials are stratified randomization and covariate adjustment. However, many trials do not fully capitalize on the combined precision gains from these two methods, which can lead to wasted resources in terms of sample size and trial duration. We derive consistency and asymptotic normality of model-robust estimators that combine these two methods, and show that these estimators can lead to substantial gains in precision and power. Our theorems cover a class of estimators that handle continuous, binary, and time-to-event outcomes; missing outcomes under the missing at random assumption are handled as well. For each estimator, we give a formula for a consistent variance estimator that is model-robust and that fully captures variance reductions from stratified randomization and covariate adjustment. Also, we give the first proof (to the best of our knowledge) of consistency and asymptotic normality of the Kaplan–Meier estimator under stratified randomization, and we derive its asymptotic variance. The above results also hold for the biased-coin covariate-adaptive design. We demonstrate our results using data from three trials of substance use disorder treatments, where the variance reduction due to stratified randomization and covariate adjustment ranges from 1\% to 36\%. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Bingkai Wang and Ryoko Susukida and Ramin Mojtabai and Masoumeh Amin-Esmaeili and Michael Rosenblum},
  doi          = {10.1080/01621459.2021.1981338},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1152-1163},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-robust inference for clinical trials that improve precision by stratified randomization and covariate adjustment},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Matching one sample according to two criteria in
observational studies. <em>JASA</em>, <em>118</em>(542), 1140–1151. (<a
href="https://doi.org/10.1080/01621459.2021.1981337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate matching has two goals (i) to construct treated and control groups that have similar distributions of observed covariates, and (ii) to produce matched pairs or sets that are homogeneous in a few key covariates. When there are only a few binary covariates, both goals may be achieved by matching exactly for these few covariates. Commonly, however, there are many covariates, so goals (i) and (ii) come apart, and must be achieved by different means. As is also true in a randomized experiment, similar distributions can be achieved for a high-dimensional covariate, but close pairs can be achieved for only a few covariates. We introduce a new polynomial-time method for achieving both goals that substantially generalizes several existing methods; in particular, it can minimize the earthmover distance between two marginal distributions. The method involves minimum cost flow optimization in a network built around a tripartite graph, unlike the usual network built around a bipartite graph. In the tripartite graph, treated subjects appear twice, on the far left and the far right, with controls sandwiched between them, and efforts to balance covariates are represented on the right, while efforts to find close individual pairs are represented on the left. In this way, the two efforts may be pursued simultaneously without conflict. The method is applied to our on-going study in the Medicare population of the relationship between superior nursing and sepsis mortality. The match2C package in R implements the method. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {B. Zhang and D. S. Small and K. B. Lasater and M. McHugh and J. H. Silber and P. R. Rosenbaum},
  doi          = {10.1080/01621459.2021.1981337},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1140-1151},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Matching one sample according to two criteria in observational studies},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online debiasing for adaptively collected high-dimensional
data with applications to time series analysis. <em>JASA</em>,
<em>118</em>(542), 1126–1139. (<a
href="https://doi.org/10.1080/01621459.2021.1979011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive collection of data is commonplace in applications throughout science and engineering. From the point of view of statistical inference, however, adaptive data collection induces memory and correlation in the samples, and poses significant challenge. We consider the high-dimensional linear regression, where the samples are collected adaptively, and the sample size n can be smaller than p , the number of covariates. In this setting, there are two distinct sources of bias: the first due to regularization imposed for consistent estimation, for example, using the LASSO, and the second due to adaptivity in collecting the samples. We propose “online debiasing,” a general procedure for estimators such as the LASSO, which addresses both sources of bias. In two concrete contexts (i) time series analysis and (ii) batched data collection, we demonstrate that online debiasing optimally debiases the LASSO estimate when the underlying parameter θ 0 has sparsity of order o ( n / log p ) . In this regime, the debiased estimator can be used to compute p -values and confidence intervals of optimal size.},
  archive      = {J_JASA},
  author       = {Yash Deshpande and Adel Javanmard and Mohammad Mehrabi},
  doi          = {10.1080/01621459.2021.1979011},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1126-1139},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Online debiasing for adaptively collected high-dimensional data with applications to time series analysis},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Is a classification procedure good enough?—a goodness-of-fit
assessment tool for classification learning. <em>JASA</em>,
<em>118</em>(542), 1115–1125. (<a
href="https://doi.org/10.1080/01621459.2021.1979010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many nontraditional classification methods, such as random forest, boosting, and neural network, have been widely used in applications. Their performance is typically measured in terms of classification accuracy. While the classification error rate and the like are important, they do not address a fundamental question: Is the classification method underfitted? To our best knowledge, there is no existing method that can assess the goodness of fit of a general classification procedure. Indeed, the lack of a parametric assumption makes it challenging to construct proper tests. To overcome this difficulty, we propose a methodology called BAGofT that splits the data into a training set and a validation set. First, the classification procedure to assess is applied to the training set, which is also used to adaptively find a data grouping that reveals the most severe regions of underfitting. Then, based on this grouping, we calculate a test statistic by comparing the estimated success probabilities and the actual observed responses from the validation set. The data splitting guarantees that the size of the test is controlled under the null hypothesis, and the power of the test goes to one as the sample size increases under the alternative hypothesis. For testing parametric classification models, the BAGofT has a broader scope than the existing methods since it is not restricted to specific parametric models (e.g., logistic regression). Extensive simulation studies show the utility of the BAGofT when assessing general classification procedures and its strengths over some existing methods when testing parametric classification models. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jiawei Zhang and Jie Ding and Yuhong Yang},
  doi          = {10.1080/01621459.2021.1979010},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1115-1125},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Is a classification procedure good Enough?—A goodness-of-fit assessment tool for classification learning},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noncompliance and instrumental variables for 2K factorial
experiments. <em>JASA</em>, <em>118</em>(542), 1102–1114. (<a
href="https://doi.org/10.1080/01621459.2021.1978468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factorial experiments are widely used to assess the marginal, joint, and interactive effects of multiple concurrent factors. While a robust literature covers the design and analysis of these experiments, there is less work on how to handle treatment noncompliance in this setting. To fill this gap, we introduce a new methodology that uses the potential outcomes framework for analyzing 2 K 2 K 2K factorial experiments with noncompliance on any number of factors. This framework builds on and extends the literature on both instrumental variables and factorial experiments in several ways. First, we define novel, complier-specific quantities of interest for this setting and show how to generalize key instrumental variables assumptions. Second, we show how partial compliance across factors gives researchers a choice over different types of compliers to target in estimation. Third, we show how to conduct inference for these new estimands from both the finite-population and superpopulation asymptotic perspectives. Finally, we illustrate these techniques by applying them to a field experiment on the effectiveness of different forms of get-out-the-vote canvassing. New easy-to-use, open-source software implements the methodology. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Matthew Blackwell and Nicole E. Pashley},
  doi          = {10.1080/01621459.2021.1978468},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1102-1114},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Noncompliance and instrumental variables for 2K factorial experiments},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforced risk prediction with budget constraint using
irregularly measured data from electronic health records. <em>JASA</em>,
<em>118</em>(542), 1090–1101. (<a
href="https://doi.org/10.1080/01621459.2021.1978467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncontrolled glycated hemoglobin (HbA1c) levels are associated with adverse events among complex diabetic patients. These adverse events present serious health risks to affected patients and are associated with significant financial costs. Thus, a high-quality predictive model that could identify high-risk patients so as to inform preventative treatment has the potential to improve patient outcomes while reducing healthcare costs. Because the biomarker information needed to predict risk is costly and burdensome, it is desirable that such a model collect only as much information as is needed on each patient so as to render an accurate prediction. We propose a sequential predictive model that uses accumulating patient longitudinal data to classify patients as: high-risk, low-risk, or uncertain. Patients classified as high-risk are then recommended to receive preventative treatment and those classified as low-risk are recommended to standard care. Patients classified as uncertain are monitored until a high-risk or low-risk determination is made. We construct the model using claims and enrollment files from Medicare, linked with patient electronic health records (EHR) data. The proposed model uses functional principal components to accommodate noisy longitudinal data and weighting to deal with missingness and sampling bias. The proposed method demonstrates higher predictive accuracy and lower cost than competing methods in a series of simulation experiments and application to data on complex patients with diabetes. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yinghao Pan and Eric B. Laber and Maureen A. Smith and Ying-Qi Zhao},
  doi          = {10.1080/01621459.2021.1978467},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1090-1101},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Reinforced risk prediction with budget constraint using irregularly measured data from electronic health records},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bagged filters for partially observed interacting systems.
<em>JASA</em>, <em>118</em>(542), 1078–1089. (<a
href="https://doi.org/10.1080/01621459.2021.1974867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bagging (i.e., bootstrap aggregating) involves combining an ensemble of bootstrap estimators. We consider bagging for inference from noisy or incomplete measurements on a collection of interacting stochastic dynamic systems. Each system is called a unit, and each unit is associated with a spatial location. A motivating example arises in epidemiology, where each unit is a city: the majority of transmission occurs within a city, with smaller yet epidemiologically important interactions arising from disease transmission between cities. Monte Carlo filtering methods used for inference on nonlinear non-Gaussian systems can suffer from a curse of dimensionality (COD) as the number of units increases. We introduce bagged filter (BF) methodology which combines an ensemble of Monte Carlo filters, using spatiotemporally localized weights to select successful filters at each unit and time. We obtain conditions under which likelihood evaluation using a BF algorithm can beat a COD, and we demonstrate applicability even when these conditions do not hold. BF can out-perform an ensemble Kalman filter on a coupled population dynamics model describing infectious disease transmission. A block particle filter (BPF) also performs well on this task, though the bagged filter respects smoothness and conservation laws that a BPF can violate. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Edward L. Ionides and Kidus Asfaw and Joonha Park and Aaron A. King},
  doi          = {10.1080/01621459.2021.1974867},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1078-1089},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bagged filters for partially observed interacting systems},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Score-driven modeling of spatio-temporal data.
<em>JASA</em>, <em>118</em>(542), 1066–1077. (<a
href="https://doi.org/10.1080/01621459.2021.1970571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A simultaneous autoregressive score-driven model with autoregressive disturbances is developed for spatio-temporal data that may exhibit heavy tails. The model specification rests on a signal plus noise decomposition of a spatially filtered process, where the signal can be approximated by a nonlinear function of the past variables and a set of explanatory variables, while the noise follows a multivariate Student- t distribution. The key feature of the model is that the dynamics of the space-time varying signal are driven by the score of the conditional likelihood function. When the distribution is heavy-tailed, the score provides a robust update of the space-time varying location. Consistency and asymptotic normality of maximum likelihood estimators are derived along with the stochastic properties of the model. The motivating application of the proposed model comes from brain scans recorded through functional magnetic resonance imaging when subjects are at rest and not expected to react to any controlled stimulus. We identify spontaneous activations in brain regions as extreme values of a possibly heavy-tailed distribution, by accounting for spatial and temporal dependence.},
  archive      = {J_JASA},
  author       = {Francesca Gasperoni and Alessandra Luati and Lucia Paci and Enzo D’Innocenzo},
  doi          = {10.1080/01621459.2021.1970571},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1066-1077},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Score-driven modeling of spatio-temporal data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-fitted residual regression for high-dimensional
heteroscedasticity pursuit. <em>JASA</em>, <em>118</em>(542), 1056–1065.
(<a href="https://doi.org/10.1080/01621459.2021.1970570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a vast amount of work on high-dimensional regression. The common starting point for the existing theoretical work is to assume the data generating model is a homoscedastic linear regression model with some sparsity structure. In reality the homoscedasticity assumption is often violated, and hence understanding the heteroscedasticity of the data is of critical importance. In this article we systematically study the estimation of a high-dimensional heteroscedastic regression model. In particular, the emphasis is on how to detect and estimate the heteroscedasticity effects reliably and efficiently. To this end, we propose a cross-fitted residual regression approach and prove the resulting estimator is selection consistent for heteroscedasticity effects and establish its rates of convergence. Our estimator has tuning parameters to be determined by the data in practice. We propose a novel high-dimensional BIC for tuning parameter selection and establish its consistency. This is the first high-dimensional BIC result under heteroscedasticity. The theoretical analysis is more involved in order to handle heteroscedasticity, and we develop a couple of interesting new concentration inequalities that are of independent interests.},
  archive      = {J_JASA},
  author       = {Le Zhou and Hui Zou},
  doi          = {10.1080/01621459.2021.1970570},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1056-1065},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Cross-fitted residual regression for high-dimensional heteroscedasticity pursuit},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical inference for high-dimensional matrix-variate
factor models. <em>JASA</em>, <em>118</em>(542), 1038–1055. (<a
href="https://doi.org/10.1080/01621459.2021.1970569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the estimation and inference of the low-rank components in high-dimensional matrix-variate factor models, where each dimension of the matrix-variates ( p × q ) is comparable to or greater than the number of observations ( T ). We propose an estimation method called α -PCA that preserves the matrix structure and aggregates mean and contemporary covariance through a hyper-parameter α . We develop an inferential theory, establishing consistency, the rate of convergence, and the limiting distributions, under general conditions that allow for correlations across time, rows, or columns of the noise. We show both theoretical and empirical methods of choosing the best α , depending on the use-case criteria. Simulation results demonstrate the adequacy of the asymptotic results in approximating the finite sample properties. The α -PCA compares favorably with the existing ones. Finally, we illustrate its applications with a real numeric dataset and two real image datasets. In all applications, the proposed estimation procedure outperforms previous methods in the power of variance explanation using out-of-sample 10-fold cross-validation. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Elynn Y. Chen and Jianqing Fan},
  doi          = {10.1080/01621459.2021.1970569},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1038-1055},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for high-dimensional matrix-variate factor models},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable selection for global fréchet regression.
<em>JASA</em>, <em>118</em>(542), 1023–1037. (<a
href="https://doi.org/10.1080/01621459.2021.1969240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global Fréchet regression is an extension of linear regression to cover more general types of responses, such as distributions, networks, and manifolds, which are becoming more prevalent. In such models, predictors are Euclidean while responses are metric space valued. Predictor selection is of major relevance for regression modeling in the presence of multiple predictors but has not yet been addressed for Fréchet regression. Due to the metric space-valued nature of the responses, Fréchet regression models do not feature model parameters, and this lack of parameters makes it a major challenge to extend existing variable selection methods for linear regression to global Fréchet regression. In this work, we address this challenge and propose a novel variable selection method that overcomes it and has good practical performance. We provide theoretical support and demonstrate that the proposed variable selection method achieves selection consistency. We also explore the finite sample performance of the proposed method with numerical examples and data illustrations.},
  archive      = {J_JASA},
  author       = {Danielle C. Tucker and Yichao Wu and Hans-Georg Müller},
  doi          = {10.1080/01621459.2021.1969240},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1023-1037},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Variable selection for global fréchet regression},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional estimation and change detection for nonstationary
time series. <em>JASA</em>, <em>118</em>(542), 1011–1022. (<a
href="https://doi.org/10.1080/01621459.2021.1969239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tests for structural breaks in time series should ideally be sensitive to breaks in the parameter of interest, while being robust to nuisance changes. Statistical analysis thus needs to allow for some form of nonstationarity under the null hypothesis of no change. In this article, estimators for integrated parameters of locally stationary time series are constructed and a corresponding functional central limit theorem is established, enabling change-point inference for a broad class of parameters under mild assumptions. The proposed framework covers all parameters which may be expressed as nonlinear functions of moments, for example kurtosis, autocorrelation, and coefficients in a linear regression model. To perform feasible inference based on the derived limit distribution, a bootstrap variant is proposed and its consistency is established. The methodology is illustrated by means of a simulation study and by an application to high-frequency asset prices.},
  archive      = {J_JASA},
  author       = {Fabian Mies},
  doi          = {10.1080/01621459.2021.1969239},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1011-1022},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Functional estimation and change detection for nonstationary time series},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communication-efficient accurate statistical estimation.
<em>JASA</em>, <em>118</em>(542), 1000–1010. (<a
href="https://doi.org/10.1080/01621459.2021.1969238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the data are stored in a distributed manner, direct applications of traditional statistical inference procedures are often prohibitive due to communication costs and privacy concerns. This article develops and investigates two communication-efficient accurate statistical estimators (CEASE), implemented through iterative algorithms for distributed optimization. In each iteration, node machines carry out computation in parallel and communicate with the central processor, which then broadcasts aggregated information to node machines for new updates. The algorithms adapt to the similarity among loss functions on node machines, and converge rapidly when each node machine has large enough sample size. Moreover, they do not require good initialization and enjoy linear converge guarantees under general conditions. The contraction rate of optimization errors is presented explicitly, with dependence on the local sample size unveiled. In addition, the improved statistical accuracy per iteration is derived. By regarding the proposed method as a multistep statistical estimator, we show that statistical efficiency can be achieved in finite steps in typical statistical applications. In addition, we give the conditions under which the one-step CEASE estimator is statistically efficient. Extensive numerical experiments on both synthetic and real data validate the theoretical results and demonstrate the superior performance of our algorithms.},
  archive      = {J_JASA},
  author       = {Jianqing Fan and Yongyi Guo and Kaizheng Wang},
  doi          = {10.1080/01621459.2021.1969238},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {1000-1010},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Communication-efficient accurate statistical estimation},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empirical bayes mean estimation with nonparametric errors
via order statistic regression on replicated data. <em>JASA</em>,
<em>118</em>(542), 987–999. (<a
href="https://doi.org/10.1080/01621459.2021.1967164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study empirical Bayes estimation of the effect sizes of N units from K noisy observations on each unit. We show that it is possible to achieve near-Bayes optimal mean squared error, without any assumptions or knowledge about the effect size distribution or the noise. The noise distribution can be heteroscedastic and vary arbitrarily from unit to unit. Our proposal, which we call Aurora, leverages the replication inherent in the K observations per unit and recasts the effect size estimation problem as a general regression problem. Aurora with linear regression provably matches the performance of a wide array of estimators including the sample mean, the trimmed mean, the sample median, as well as James-Stein shrunk versions thereof. Aurora automates effect size estimation for Internet-scale datasets, as we demonstrate on data from a large technology firm.},
  archive      = {J_JASA},
  author       = {Nikolaos Ignatiadis and Sujayam Saha and Dennis L. Sun and Omkar Muralidharan},
  doi          = {10.1080/01621459.2021.1967164},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {987-999},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Empirical bayes mean estimation with nonparametric errors via order statistic regression on replicated data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bootstrap prediction bands for functional time series.
<em>JASA</em>, <em>118</em>(542), 972–986. (<a
href="https://doi.org/10.1080/01621459.2021.1963262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A bootstrap procedure for constructing prediction bands for a stationary functional time series is proposed. The procedure exploits a general vector autoregressive representation of the time-reversed series of Fourier coefficients appearing in the Karhunen–Loève representation of the functional process. It generates backward-in-time functional replicates that adequately mimic the dependence structure of the underlying process in a model-free way and have the same conditionally fixed curves at the end of each functional pseudo-time series. The bootstrap prediction error distribution is then calculated as the difference between the model-free, bootstrap-generated future functional observations and the functional forecasts obtained from the model used for prediction. This allows the estimated prediction error distribution to account for the innovation and estimation errors associated with prediction and the possible errors due to model misspecification. We establish the asymptotic validity of the bootstrap procedure in estimating the conditional prediction error distribution of interest, and we also show that the procedure enables the construction of prediction bands that achieve (asymptotically) the desired coverage. Prediction bands based on a consistent estimation of the conditional distribution of the studentized prediction error process also are introduced. Such bands allow for taking more appropriately into account the local uncertainty of the prediction. Through a simulation study and the analysis of two datasets, we demonstrate the capabilities and the good finite-sample performance of the proposed method.},
  archive      = {J_JASA},
  author       = {Efstathios Paparoditis and Han Lin Shang},
  doi          = {10.1080/01621459.2021.1963262},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {972-986},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bootstrap prediction bands for functional time series},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Greedy segmentation for a functional data sequence.
<em>JASA</em>, <em>118</em>(542), 959–971. (<a
href="https://doi.org/10.1080/01621459.2021.1963261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach known as greedy segmentation (GS) to identify multiple changepoints for a functional data sequence. The proposed multiple changepoint detection criterion links detectability with the projection onto a suitably chosen subspace and the changepoint locations. The changepoint estimator identifies the true changepoints for any predetermined number of changepoint candidates, either over-reporting or under-reporting. This theoretical finding supports the proposed GS estimator, which can be efficiently obtained in a greedy manner. The GS estimator’s consistency holds without being restricted to the conventional at most one changepoint condition, and it is robust to the relative positions of the changepoints. Based on the GS estimator, the test statistic’s asymptotic distribution leads to the novel GS algorithm, which identifies the number and locations of changepoints. Using intensive simulation studies, we compare the finite sample performance of the GS approach with other competing methods. We also apply our method to temporal changepoint detection in weather datasets.},
  archive      = {J_JASA},
  author       = {Yu-Ting Chen and Jeng-Min Chiou and Tzee-Ming Huang},
  doi          = {10.1080/01621459.2021.1963261},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {959-971},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Greedy segmentation for a functional data sequence},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Derandomizing knockoffs. <em>JASA</em>, <em>118</em>(542),
948–958. (<a
href="https://doi.org/10.1080/01621459.2021.1962720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-X knockoffs is a general procedure that can leverage any feature importance measure to produce a variable selection algorithm, which discovers true effects while rigorously controlling the number or fraction of false positives. Model-X knockoffs is a randomized procedure which relies on the one-time construction of synthetic (random) variables. This article introduces a derandomization method by aggregating the selection results across multiple runs of the knockoffs algorithm. The derandomization step is designed to be flexible and can be adapted to any variable selection base procedure to yield stable decisions without compromising statistical power. When applied to the base procedure of Janson and Su, we prove that derandomized knockoffs controls both the per family error rate (PFER) and the k family-wise error rate ( k -FWER). Furthermore, we carry out extensive numerical studies demonstrating tight Type I error control and markedly enhanced power when compared with alternative variable selection algorithms. Finally, we apply our approach to multistage genome-wide association studies of prostate cancer and report locations on the genome that are significantly associated with the disease. When cross-referenced with other studies, we find that the reported associations have been replicated. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Zhimei Ren and Yuting Wei and Emmanuel Candès},
  doi          = {10.1080/01621459.2021.1962720},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {948-958},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Derandomizing knockoffs},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A particle method for solving fredholm equations of the
first kind. <em>JASA</em>, <em>118</em>(542), 937–947. (<a
href="https://doi.org/10.1080/01621459.2021.1962328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fredholm integral equations of the first kind are the prototypical example of ill-posed linear inverse problems. They model, among other things, reconstruction of distorted noisy observations and indirect density estimation and also appear in instrumental variable regression. However, their numerical solution remains a challenging problem. Many techniques currently available require a preliminary discretization of the domain of the solution and make strong assumptions about its regularity. For example, the popular expectation maximization smoothing (EMS) scheme requires the assumption of piecewise constant solutions which is inappropriate for most applications. We propose here a novel particle method that circumvents these two issues. This algorithm can be thought of as a Monte Carlo approximation of the EMS scheme which not only performs an adaptive stochastic discretization of the domain but also results in smooth approximate solutions. We analyze the theoretical properties of the EMS iteration and of the corresponding particle algorithm. Compared to standard EMS, we show experimentally that our novel particle method provides state-of-the-art performance for realistic systems, including motion deblurring and reconstruction of cross-section images of the brain from positron emission tomography.},
  archive      = {J_JASA},
  author       = {Francesca R. Crucinio and Arnaud Doucet and Adam M. Johansen},
  doi          = {10.1080/01621459.2021.1962328},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {937-947},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A particle method for solving fredholm equations of the first kind},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A kernel log-rank test of independence for right-censored
data. <em>JASA</em>, <em>118</em>(542), 925–936. (<a
href="https://doi.org/10.1080/01621459.2021.1961784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a general nonparametric independence test between right-censored survival times and covariates, which may be multivariate. Our test statistic has a dual interpretation, first in terms of the supremum of a potentially infinite collection of weight-indexed log-rank tests, with weight functions belonging to a reproducing kernel Hilbert space (RKHS) of functions; and second, as the norm of the difference of embeddings of certain finite measures into the RKHS, similar to the Hilbert–Schmidt Independence Criterion (HSIC) test-statistic. We study the asymptotic properties of the test, finding sufficient conditions to ensure our test correctly rejects the null hypothesis under any alternative. The test statistic can be computed straightforwardly, and the rejection threshold is obtained via an asymptotically consistent Wild Bootstrap procedure. Extensive investigations on both simulated and real data suggest that our testing procedure generally performs better than competing approaches in detecting complex nonlinear dependence.},
  archive      = {J_JASA},
  author       = {Tamara Fernández and Arthur Gretton and David Rindt and Dino Sejdinovic},
  doi          = {10.1080/01621459.2021.1961784},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {925-936},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A kernel log-rank test of independence for right-censored data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative multilabel classification. <em>JASA</em>,
<em>118</em>(542), 913–924. (<a
href="https://doi.org/10.1080/01621459.2021.1961783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multilabel classification, strong label dependence is present for exploiting, particularly for word-to-word dependence defined by semantic labels. In such a situation, we develop a collaborative-learning framework to predict class labels based on label-predictor pairs and label-only data. For example, in image categorization and recognition, language expressions describe the content of an image together with a large number of words and phrases without associated images. This article proposes a new loss quantifying partial correctness for false positive and negative misclassifications due to label similarities. Given this loss, we develop the Bayes rule to capture label dependence by nonlinear classification. On this ground, we introduce a weighted random forest classifier for complete data and a stacking scheme for leveraging additional labels to enhance the performance of supervised learning based on label-predictor pairs. Importantly, we decompose multilabel classification into a sequence of independent learning tasks, based on which the computational complexity of our classifier becomes linear in the size of labels. Compared to existing classifiers without label-only data, the proposed classifier enjoys the computational benefit while enabling the detection of novel labels absent from training by exploring label dependence and leveraging label-only data for higher accuracy. Theoretically, we show that the proposed method reconstructs the Bayes performance consistently, achieving the desired learning accuracy. Numerically, we demonstrate that the proposed method compares favorably in terms of the proposed and Hamming losses against binary relevance and a regularized Ising classifier modeling conditional label dependence. Indeed, leveraging additional labels tends to improve the supervised performance, especially when the training sample is not very large, as in semisupervised learning. Finally, we demonstrate the utility of the proposed approach on the Microsoft COCO object detection challenge, PASCAL visual object classes challenge 2007, and Mediamill benchmark.},
  archive      = {J_JASA},
  author       = {Yunzhang Zhu and Xiaotong Shen and Hui Jiang and Wing Hung Wong},
  doi          = {10.1080/01621459.2021.1961783},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {913-924},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Collaborative multilabel classification},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference for high-dimensional censored quantile regression.
<em>JASA</em>, <em>118</em>(542), 898–912. (<a
href="https://doi.org/10.1080/01621459.2021.1957900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the availability of high-dimensional genetic biomarkers, it is of interest to identify heterogeneous effects of these predictors on patients’ survival, along with proper statistical inference. Censored quantile regression has emerged as a powerful tool for detecting heterogeneous effects of covariates on survival outcomes. To our knowledge, there is little work available to draw inferences on the effects of high-dimensional predictors for censored quantile regression (CQR). This article proposes a novel procedure to draw inference on all predictors within the framework of global CQR, which investigates covariate-response associations over an interval of quantile levels, instead of a few discrete values. The proposed estimator combines a sequence of low-dimensional model estimates that are based on multi-sample splittings and variable selection. We show that, under some regularity conditions, the estimator is consistent and asymptotically follows a Gaussian process indexed by the quantile level. Simulation studies indicate that our procedure can properly quantify the uncertainty of the estimates in high-dimensional settings. We apply our method to analyze the heterogeneous effects of SNPs residing in lung cancer pathways on patients’ survival, using the Boston Lung Cancer Survival Cohort, a cancer epidemiology study on the molecular mechanism of lung cancer.},
  archive      = {J_JASA},
  author       = {Zhe Fei and Qi Zheng and Hyokyoung G. Hong and Yi Li},
  doi          = {10.1080/01621459.2021.1957900},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {898-912},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference for high-dimensional censored quantile regression},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reproducing kernel hilbert space approach to functional
calibration of computer models. <em>JASA</em>, <em>118</em>(542),
883–897. (<a
href="https://doi.org/10.1080/01621459.2021.1956938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a frequentist solution to the functional calibration problem, where the value of a calibration parameter in a computer model is allowed to vary with the value of control variables in the physical system. The need of functional calibration is motivated by engineering applications where using a constant calibration parameter results in a significant mismatch between outputs from the computer model and the physical experiment. Reproducing kernel Hilbert spaces (RKHS) are used to model the optimal calibration function, defined as the functional relationship between the calibration parameter and control variables that gives the best prediction. This optimal calibration function is estimated through penalized least squares with an RKHS-norm penalty and using physical data. An uncertainty quantification procedure is also developed for such estimates. Theoretical guarantees of the proposed method are provided in terms of prediction consistency and consitency of estimating the optimal calibration function. The proposed method is tested using both real and synthetic data and exhibits more robust performance in prediction and uncertainty quantification than the existing parametric functional calibration method and a state-of-art Bayesian method.},
  archive      = {J_JASA},
  author       = {Rui Tuo and Shiyuan He and Arash Pourhabib and Yu Ding and Jianhua Z. Huang},
  doi          = {10.1080/01621459.2021.1956938},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {883-897},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A reproducing kernel hilbert space approach to functional calibration of computer models},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wasserstein regression. <em>JASA</em>, <em>118</em>(542),
869–882. (<a
href="https://doi.org/10.1080/01621459.2021.1956937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of samples of random objects that do not lie in a vector space is gaining increasing attention in statistics. An important class of such object data is univariate probability measures defined on the real line. Adopting the Wasserstein metric, we develop a class of regression models for such data, where random distributions serve as predictors and the responses are either also distributions or scalars. To define this regression model, we use the geometry of tangent bundles of the space of random measures endowed with the Wasserstein metric for mapping distributions to tangent spaces. The proposed distribution-to-distribution regression model provides an extension of multivariate linear regression for Euclidean data and function-to-function regression for Hilbert space-valued data in functional data analysis. In simulations, it performs better than an alternative transformation approach where one maps distributions to a Hilbert space through the log quantile density transformation and then applies traditional functional regression. We derive asymptotic rates of convergence for the estimator of the regression operator and for predicted distributions and also study an extension to autoregressive models for distribution-valued time series. The proposed methods are illustrated with data on human mortality and distributional time series of house prices.},
  archive      = {J_JASA},
  author       = {Yaqing Chen and Zhenhua Lin and Hans-Georg Müller},
  doi          = {10.1080/01621459.2021.1956937},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {869-882},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Wasserstein regression},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convex and nonconvex optimization are both minimax-optimal
for noisy blind deconvolution under random designs. <em>JASA</em>,
<em>118</em>(542), 858–868. (<a
href="https://doi.org/10.1080/01621459.2021.1956501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the effectiveness of convex relaxation and nonconvex optimization in solving bilinear systems of equations under two different designs (i.e., a sort of random Fourier design and Gaussian design). Despite the wide applicability, the theoretical understanding about these two paradigms remains largely inadequate in the presence of random noise. The current article makes two contributions by demonstrating that (i) a two-stage nonconvex algorithm attains minimax-optimal accuracy within a logarithmic number of iterations, and (ii) convex relaxation also achieves minimax-optimal statistical accuracy vis-à-vis random noise. Both results significantly improve upon the state-of-the-art theoretical guarantees. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yuxin Chen and Jianqing Fan and Bingyan Wang and Yuling Yan},
  doi          = {10.1080/01621459.2021.1956501},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {858-868},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Convex and nonconvex optimization are both minimax-optimal for noisy blind deconvolution under random designs},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric causal effects based on longitudinal modified
treatment policies. <em>JASA</em>, <em>118</em>(542), 846–857. (<a
href="https://doi.org/10.1080/01621459.2021.1955691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most causal inference methods consider counterfactual variables under interventions that set the exposure to a fixed value. With continuous or multi-valued treatments or exposures, such counterfactuals may be of little practical interest because no feasible intervention can be implemented that would bring them about. Longitudinal modified treatment policies (LMTPs) are a recently developed nonparametric alternative that yield effects of immediate practical relevance with an interpretation in terms of meaningful interventions such as reducing or increasing the exposure by a given amount. LMTPs also have the advantage that they can be designed to satisfy the positivity assumption required for causal inference. We present a novel sequential regression formula that identifies the LMTP causal effect, study properties of the LMTP statistical estimand such as the efficient influence function and the efficiency bound, and propose four different estimators. Two of our estimators are efficient, and one is sequentially doubly robust in the sense that it is consistent if, for each time point, either an outcome regression or a treatment mechanism is consistently estimated. We perform numerical studies of the estimators, and present the results of our motivating study on hypoxemia and mortality in intubated Intensive Care Unit (ICU) patients. Software implementing our methods is provided in the form of the open source R package lmtp freely available on GitHub ( https://github.com/nt-williams/lmtp ) and CRAN.},
  archive      = {J_JASA},
  author       = {Iván Díaz and Nicholas Williams and Katherine L. Hoffman and Edward J. Schenck},
  doi          = {10.1080/01621459.2021.1955691},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {846-857},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric causal effects based on longitudinal modified treatment policies},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate temporal point process regression.
<em>JASA</em>, <em>118</em>(542), 830–845. (<a
href="https://doi.org/10.1080/01621459.2021.1955690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point process modeling is gaining increasing attention, as point process type data are emerging in a large variety of scientific applications. In this article, motivated by a neuronal spike trains study, we propose a novel point process regression model, where both the response and the predictor can be a high-dimensional point process. We model the predictor effects through the conditional intensities using a set of basis transferring functions in a convolutional fashion. We organize the corresponding transferring coefficients in the form of a three-way tensor, then impose the low-rank, sparsity, and subgroup structures on this coefficient tensor. These structures help reduce the dimensionality, integrate information across different individual processes, and facilitate the interpretation. We develop a highly scalable optimization algorithm for parameter estimation. We derive the large sample error bound for the recovered coefficient tensor, and establish the subgroup identification consistency, while allowing the dimension of the multivariate point process to diverge. We demonstrate the efficacy of our method through both simulations and a cross-area neuronal spike trains analysis in a sensory cortex study.},
  archive      = {J_JASA},
  author       = {Xiwei Tang and Lexin Li},
  doi          = {10.1080/01621459.2021.1955690},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {830-845},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multivariate temporal point process regression},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crime in philadelphia: Bayesian clustering with particle
optimization. <em>JASA</em>, <em>118</em>(542), 818–829. (<a
href="https://doi.org/10.1080/01621459.2022.2156348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate estimation of the change in crime over time is a critical first step toward better understanding of public safety in large urban environments. Bayesian hierarchical modeling is a natural way to study spatial variation in urban crime dynamics at the neighborhood level, since it facilitates principled “sharing of information” between spatially adjacent neighborhoods. Typically, however, cities contain many physical and social boundaries that may manifest as spatial discontinuities in crime patterns. In this situation, standard prior choices often yield overly smooth parameter estimates, which can ultimately produce mis-calibrated forecasts. To prevent potential over-smoothing, we introduce a prior that partitions the set of neighborhoods into several clusters and encourages spatial smoothness within each cluster. In terms of model implementation, conventional stochastic search techniques are computationally prohibitive, as they must traverse a combinatorially vast space of partitions. We introduce an ensemble optimization procedure that simultaneously identifies several high probability partitions by solving one optimization problem using a new local search strategy. We then use the identified partitions to estimate crime trends in Philadelphia between 2006 and 2017. On simulated and real data, our proposed method demonstrates good estimation and partition selection performance. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Cecilia Balocchi and Sameer K. Deshpande and Edward I. George and Shane T. Jensen},
  doi          = {10.1080/01621459.2022.2156348},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {818-829},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Crime in philadelphia: Bayesian clustering with particle optimization},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature screening for interval-valued response with
application to study association between posted salary and required
skills. <em>JASA</em>, <em>118</em>(542), 805–817. (<a
href="https://doi.org/10.1080/01621459.2022.2152342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important to quantify the differences in returns to skills using the online job advertisements data, which have attracted great interest in both labor economics and statistics fields. In this article, we study the relationship between the posted salary and the job requirements in online labor markets. There are two challenges to deal with. First, the posted salary is always presented in an interval-valued form, for example, 5k–10k yuan per month. Simply taking the mid-point or the lower bound as the alternative for salary may result in biased estimators. Second, the number of the potential skill words as predictors generated from the job advertisements by word segmentation is very large and many of them may not contribute to the salary. To this end, we propose a new feature screening method, Absolute Distribution Difference Sure Independence Screening (ADD-SIS), to select important skill words for the interval-valued response. The marginal utility for feature screening is based on the difference of estimated distribution functions via nonparametric maximum likelihood estimation, which sufficiently uses the interval information. It is model-free and robust to outliers. Numerical simulations show that the new method using the interval information is more efficient to select important predictors than the methods only based on the single points of the intervals. In the real data application, we study the text data of job advertisements for data scientists and data analysts in a major China’s online job posting website, and explore the important skill words for the salary. We find that the skill words like optimization, long short-term memory (LSTM), convolutional neural networks (CNN), collaborative filtering, are positively correlated with the salary while the words like Excel, Office, data collection, may negatively contribute to the salary. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wei Zhong and Chen Qian and Wanjun Liu and Liping Zhu and Runze Li},
  doi          = {10.1080/01621459.2022.2152342},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {805-817},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Feature screening for interval-valued response with application to study association between posted salary and required skills},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A flexible zero-inflated poisson-gamma model with
application to microbiome sequence count data. <em>JASA</em>,
<em>118</em>(542), 792–804. (<a
href="https://doi.org/10.1080/01621459.2022.2151447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In microbiome studies, it is of interest to use a sample from a population of microbes, such as the gut microbiota community, to estimate the population proportion of these taxa. However, due to biases introduced in sampling and preprocessing steps, these observed taxa abundances may not reflect true taxa abundance patterns in the ecosystem. Repeated measures, including longitudinal study designs, may be potential solutions to mitigate the discrepancy between observed abundances and true underlying abundances. Yet, widely observed zero-inflation and over-dispersion issues can distort downstream statistical analyses aiming to associate taxa abundances with covariates of interest. To this end, we propose a Zero-Inflated Poisson Gamma (ZIPG) model framework to address these aforementioned challenges. From a perspective of measurement errors, we accommodate the discrepancy between observations and truths by decomposing the mean parameter in Poisson regression into a true abundance level and a multiplicative measurement of sampling variability from the microbial ecosystem. Then, we provide a flexible ZIPG model framework by connecting both the mean abundance and the variability of abundances to different covariates, and build valid statistical inference procedures for both parameter estimation and hypothesis testing. Through comprehensive simulation studies and real data applications, the proposed ZIPG method provides significant insights into distinguished differential variability and mean abundance. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Roulan Jiang and Xiang Zhan and Tianying Wang},
  doi          = {10.1080/01621459.2022.2151447},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {792-804},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A flexible zero-inflated poisson-gamma model with application to microbiome sequence count data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-dimensional portfolio selection with cardinality
constraints. <em>JASA</em>, <em>118</em>(542), 779–791. (<a
href="https://doi.org/10.1080/01621459.2022.2133718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expanding number of assets offers more opportunities for investors but poses new challenges for modern portfolio management (PM). As a central plank of PM, portfolio selection by expected utility maximization (EUM) faces uncontrollable estimation and optimization errors in ultrahigh-dimensional scenarios. Past strategies for high-dimensional PM mainly concern only large-cap companies and select many stocks, making PM impractical. We propose a sample-average-approximation-based portfolio strategy to tackle the difficulties above with cardinality constraints. Our strategy bypasses the estimation of mean and covariance, the Chinese walls in high-dimensional scenarios. Empirical results on S&amp;P 500 and Russell 2000 show that an appropriate number of carefully chosen assets leads to better out-of-sample mean-variance efficiency. On Russell 2000, our best portfolio profits as much as the equally weighted portfolio but reduces the maximum drawdown and the average number of assets by 10\% and 90\%, respectively. The flexibility and the stability of incorporating factor signals for augmenting out-of-sample performances are also demonstrated. Our strategy balances the tradeoff among the return, the risk, and the number of assets with cardinality constraints. Therefore, we provide a theoretically sound and computationally efficient strategy to make PM practical in the growing global financial market. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jin-Hong Du and Yifeng Guo and Xueqin Wang},
  doi          = {10.1080/01621459.2022.2133718},
  journal      = {Journal of the American Statistical Association},
  number       = {542},
  pages        = {779-791},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {High-dimensional portfolio selection with cardinality constraints},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correction to “modeling time-varying random objects and
dynamic networks.” <em>JASA</em>, <em>118</em>(541), 778. (<a
href="https://doi.org/10.1080/01621459.2023.2173603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2023.2173603},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {778},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correction to “Modeling time-varying random objects and dynamic networks”},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Handbook of measurement error models. <em>JASA</em>,
<em>118</em>(541), 776–777. (<a
href="https://doi.org/10.1080/01621459.2023.2174869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Kwun Chuen Gary Chan},
  doi          = {10.1080/01621459.2023.2174869},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {776-777},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of measurement error models},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data science ethics: Concepts, techniques and cautionary
tales. <em>JASA</em>, <em>118</em>(541), 774–776. (<a
href="https://doi.org/10.1080/01621459.2022.2163898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Sabrina Giordano},
  doi          = {10.1080/01621459.2022.2163898},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {774-776},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Data science ethics: Concepts, techniques and cautionary tales},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differential privacy for government agencies—are we there
yet? <em>JASA</em>, <em>118</em>(541), 761–773. (<a
href="https://doi.org/10.1080/01621459.2022.2161385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Government agencies typically need to take potential risks of disclosure into account whenever they publish statistics based on their data or give external researchers access to collected data. In this context, the promise of formal privacy guarantees offered by concepts such as differential privacy seems to be the panacea enabling the agencies to quantify and control the privacy loss incurred by any data release exactly. Nevertheless, despite the excitement in academia and industry, most agencies—with the prominent exception of the U.S. Census Bureau—have been reluctant to even consider the concept for their data release strategy. This article discusses potential reasons for this. We argue that the requirements for implementing differential privacy approaches at government agencies are often fundamentally different from the requirements in industry. This raises many challenges and questions that still need to be addressed before the concept can be used as an overarching principle when sharing data with the public. The article does not offer any solutions to these challenges. Instead, we hope to stimulate some collaborative research efforts, as we believe that many of the problems can only be addressed by interdisciplinary collaborations.},
  archive      = {J_JASA},
  author       = {Jörg Drechsler},
  doi          = {10.1080/01621459.2022.2161385},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {761-773},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Differential privacy for government Agencies—Are we there yet?},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A joint MLE approach to large-scale structured latent
attribute analysis. <em>JASA</em>, <em>118</em>(541), 746–760. (<a
href="https://doi.org/10.1080/01621459.2021.1955689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured latent attribute models (SLAMs) are a family of discrete latent variable models widely used in education, psychology, and epidemiology to model multivariate categorical data. A SLAM assumes that multiple discrete latent attributes explain the dependence of observed variables in a highly structured fashion. Usually, the maximum marginal likelihood estimation approach is adopted for SLAMs, treating the latent attributes as random effects. The increasing scope of modern assessment data involves large numbers of observed variables and high-dimensional latent attributes. This poses challenges to classical estimation methods and requires new methodology and understanding of latent variable modeling. Motivated by this, we consider the joint maximum likelihood estimation (MLE) approach to SLAMs, treating latent attributes as fixed unknown parameters. We investigate estimability, consistency, and computation in the regime where sample size, number of variables, and number of latent attributes all can diverge. We establish the statistical consistency of the joint MLE and propose efficient algorithms that scale well to large-scale data for several popular SLAMs. Simulation studies demonstrate the superior empirical performance of the proposed methods. An application to real data from an international educational assessment gives interpretable findings of cognitive diagnosis.},
  archive      = {J_JASA},
  author       = {Yuqi Gu and Gongjun Xu},
  doi          = {10.1080/01621459.2021.1955689},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {746-760},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A joint MLE approach to large-scale structured latent attribute analysis},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure–adaptive sequential testing for online false
discovery rate control. <em>JASA</em>, <em>118</em>(541), 732–745. (<a
href="https://doi.org/10.1080/01621459.2021.1955688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the online testing of a stream of hypotheses where a real-time decision must be made before the next data point arrives. The error rate is required to be controlled at all decision points. Conventional simultaneous testing rules are no longer applicable due to the more stringent error constraints and absence of future data. Moreover, the online decision-making process may come to a halt when the total error budget, or alpha-wealth, is exhausted. This work develops a new class of structure-adaptive sequential testing (SAST) rules for online false discovery rate (FDR) control. A key element in our proposal is a new alpha-investing algorithm that precisely characterizes the gains and losses in sequential decision making. SAST captures time varying structures of the data stream, learns the optimal threshold adaptively in an ongoing manner and optimizes the alpha-wealth allocation across different time periods. We present theory and numerical results to show that SAST is asymptotically valid for online FDR control and achieves substantial power gain over existing online testing rules.},
  archive      = {J_JASA},
  author       = {Bowen Gang and Wenguang Sun and Weinan Wang},
  doi          = {10.1080/01621459.2021.1955688},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {732-745},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Structure–Adaptive sequential testing for online false discovery rate control},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A normality test for high-dimensional data based on the
nearest neighbor approach. <em>JASA</em>, <em>118</em>(541), 719–731.
(<a href="https://doi.org/10.1080/01621459.2021.1953507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many statistical methodologies for high-dimensional data assume the population is normal. Although a few multivariate normality tests have been proposed, to the best of our knowledge, none of them can properly control the Type I error when the dimension is larger than the number of observations. In this work, we propose a novel nonparametric test that uses the nearest neighbor information. The proposed method guarantees the asymptotic Type I error control under the high-dimensional setting. Simulation studies verify the empirical size performance of the proposed test when the dimension grows with the sample size and at the same time exhibit a superior power performance of the new test compared with alternative methods. We also illustrate our approach through two popularly used datasets in high-dimensional classification and clustering literatures where deviation from the normality assumption may lead to invalid conclusions.},
  archive      = {J_JASA},
  author       = {Hao Chen and Yin Xia},
  doi          = {10.1080/01621459.2021.1953507},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {719-731},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A normality test for high-dimensional data based on the nearest neighbor approach},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating causal peer influence in homophilous social
networks by inferring latent locations. <em>JASA</em>,
<em>118</em>(541), 707–718. (<a
href="https://doi.org/10.1080/01621459.2021.1953506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social influence cannot be identified from purely observational data on social networks, because such influence is generically confounded with latent homophily, that is, with a node’s network partners being informative about the node’s attributes and therefore its behavior. If the network grows according to either a latent community (stochastic block) model, or a continuous latent space model, then latent homophilous attributes can be consistently estimated from the global pattern of social ties. We show that, for common versions of those two network models, these estimates are so informative that controlling for estimated attributes allows for asymptotically unbiased and consistent estimation of social-influence effects in linear models. In particular, the bias shrinks at a rate that directly reflects how much information the network provides about the latent attributes. These are the first results on the consistent nonexperimental estimation of social-influence effects in the presence of latent homophily, and we discuss the prospects for generalizing them.},
  archive      = {J_JASA},
  author       = {Edward McFowland III and Cosma Rohilla Shalizi},
  doi          = {10.1080/01621459.2021.1953506},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {707-718},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating causal peer influence in homophilous social networks by inferring latent locations},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recovering latent variables by matching. <em>JASA</em>,
<em>118</em>(541), 693–706. (<a
href="https://doi.org/10.1080/01621459.2021.1952877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an optimal-transport-based matching method to nonparametrically estimate linear models with independent latent variables. The method consists in generating pseudo-observations from the latent variables, so that the Euclidean distance between the model’s predictions and their matched counterparts in the data is minimized. We show that our nonparametric estimator is consistent, and we document that it performs well in simulated data. We apply this method to study the cyclicality of permanent and transitory income shocks in the Panel Study of Income Dynamics. We find that the dispersion of income shocks is approximately acyclical, whereas the skewness of permanent shocks is procyclical. By comparison, we find that the dispersion and skewness of shocks to hourly wages vary little with the business cycle. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Manuel Arellano and Stéphane Bonhomme},
  doi          = {10.1080/01621459.2021.1952877},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {693-706},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Recovering latent variables by matching},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric bounds for causal effects in imperfect
randomized experiments. <em>JASA</em>, <em>118</em>(541), 684–692. (<a
href="https://doi.org/10.1080/01621459.2021.1950734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonignorable missingness and noncompliance can occur even in well-designed randomized experiments, making the intervention effect that the experiment was designed to estimate nonidentifiable. Nonparametric causal bounds provide a way to narrow the range of possible values for a nonidentifiable causal effect with minimal assumptions. We derive novel bounds for the causal risk difference for a binary outcome and intervention in randomized experiments with nonignorable missingness that is caused by a variety of mechanisms, with both perfect and imperfect compliance. We show that the so-called worst-case imputation, whereby all missing subjects on the intervention arm are assumed to have events and all missing subjects on the control or placebo arm are assumed to be event-free, can be too pessimistic in blinded studies with perfect compliance, and is not bounding the correct estimand with imperfect compliance. We illustrate the use of the proposed bounds in our motivating data example of peanut consumption on the development of peanut allergies in infants. We find that, even accounting for potentially nonignorable missingness and noncompliance, our derived bounds confirm that regular exposure to peanuts reduces the risk of development of peanut allergies, making the results of this study much more compelling.},
  archive      = {J_JASA},
  author       = {Erin E. Gabriel and Arvid Sjölander and Michael C. Sachs},
  doi          = {10.1080/01621459.2021.1950734},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {684-692},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric bounds for causal effects in imperfect randomized experiments},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random forests for spatially dependent data. <em>JASA</em>,
<em>118</em>(541), 665–683. (<a
href="https://doi.org/10.1080/01621459.2021.1950003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial linear mixed-models, consisting of a linear covariate effect and a Gaussian process (GP) distributed spatial random effect, are widely used for analyses of geospatial data. We consider the setting where the covariate effect is nonlinear. Random forests (RF) are popular for estimating nonlinear functions but applications of RF for spatial data have often ignored the spatial correlation. We show that this impacts the performance of RF adversely. We propose RF-GLS, a novel and well-principled extension of RF, for estimating nonlinear covariate effects in spatial mixed models where the spatial correlation is modeled using GP. RF-GLS extends RF in the same way generalized least squares (GLS) fundamentally extends ordinary least squares (OLS) to accommodate for dependence in linear models. RF becomes a special case of RF-GLS, and is substantially outperformed by RF-GLS for both estimation and prediction across extensive numerical experiments with spatially correlated data. RF-GLS can be used for functional estimation in other types of dependent data like time series. We prove consistency of RF-GLS for β -mixing dependent error processes that include the popular spatial Matérn GP. As a byproduct, we also establish, to our knowledge, the first consistency result for RF under dependence. We establish results of independent importance, including a general consistency result of GLS optimizers of data-driven function classes, and a uniform law of large number under β -mixing dependence with weaker assumptions. These new tools can be potentially useful for asymptotic analysis of other GLS-style estimators in nonparametric regression with dependent data.},
  archive      = {J_JASA},
  author       = {Arkajyoti Saha and Sumanta Basu and Abhirup Datta},
  doi          = {10.1080/01621459.2021.1950003},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {665-683},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Random forests for spatially dependent data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient estimation for random dot product graphs via a
one-step procedure. <em>JASA</em>, <em>118</em>(541), 651–664. (<a
href="https://doi.org/10.1080/01621459.2021.1948419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a one-step procedure to estimate the latent positions in random dot product graphs efficiently. Unlike the classical spectral-based methods, the proposed one-step procedure takes advantage of both the low-rank structure of the expected adjacency matrix and the Bernoulli likelihood information of the sampling model simultaneously. We show that for each vertex, the corresponding row of the one-step estimator (OSE) converges to a multivariate normal distribution after proper scaling and centering up to an orthogonal transformation, with an efficient covariance matrix. The initial estimator for the one-step procedure needs to satisfy the so-called approximate linearization property. The OSE improves the commonly adopted spectral embedding methods in the following sense: Globally for all vertices, it yields an asymptotic sum of squares error no greater than those of the spectral methods, and locally for each vertex, the asymptotic covariance matrix of the corresponding row of the OSE dominates those of the spectral embeddings in spectra. The usefulness of the proposed one-step procedure is demonstrated via numerical examples and the analysis of a real-world Wikipedia graph dataset.},
  archive      = {J_JASA},
  author       = {Fangzheng Xie and Yanxun Xu},
  doi          = {10.1080/01621459.2021.1948419},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {651-664},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficient estimation for random dot product graphs via a one-step procedure},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of knots in linear spline models. <em>JASA</em>,
<em>118</em>(541), 639–650. (<a
href="https://doi.org/10.1080/01621459.2021.1947307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The linear spline model is able to accommodate nonlinear effects while allowing for an easy interpretation. It has significant applications in studying threshold effects and change-points. However, its application in practice has been limited by the lack of both rigorously studied and computationally convenient method for estimating knots. A key difficulty in estimating knots lies in the nondifferentiability. In this article, we study influence functions of regular and asymptotically linear estimators for linear spline models using the semiparametric theory. Based on the theoretical development, we propose a simple semismooth estimating equation approach to circumvent the nondifferentiability issue using modified derivatives, in contrast to the previous smoothing-based methods. Without relying on any smoothing parameters, the proposed method is computationally convenient. To further improve numerical stability, a two-step algorithm taking advantage of the analytic solution available when knots are known is developed to solve the proposed estimating equation. Consistency and asymptotic normality are rigorously derived using the empirical process theory. Simulation studies have shown that the two-step algorithm performs well in terms of both statistical and computational properties and improves over existing methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Guangyu Yang and Baqun Zhang and Min Zhang},
  doi          = {10.1080/01621459.2021.1947307},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {639-650},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation of knots in linear spline models},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individualized group learning. <em>JASA</em>,
<em>118</em>(541), 622–638. (<a
href="https://doi.org/10.1080/01621459.2021.1947306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many massive data sets are assembled through collections of information of a large number of individuals in a population. The analysis of such data, especially in the aspect of individualized inferences and solutions, has the potential to create significant value for practical applications. Traditionally, inference for an individual in the dataset is either solely relying on the information of the individual or from summarizing the information about the whole population. However, with the availability of big data, we have the opportunity, as well as a unique challenge, to make a more effective individualized inference that takes into consideration of both the population information and the individual discrepancy. To deal with the possible heterogeneity within the population while providing effective and credible inferences for individuals in a dataset, this article develops a new approach called the individualized group learning (iGroup). The iGroup approach uses local nonparametric techniques to generate an individualized group by pooling other entities in the population which share similar characteristics with the target individual, even when individual estimates are biased due to limited number of observations. Three general cases of iGroup are discussed, and their asymptotic performances are investigated. Both theoretical results and empirical simulations reveal that, by applying iGroup, the performance of statistical inference on the individual level are ensured and can be substantially improved from inference based on either solely individual information or entire population information. The method has a broad range of applications. An example in financial statistics is presented.},
  archive      = {J_JASA},
  author       = {Chencheng Cai and Rong Chen and Min-ge Xie},
  doi          = {10.1080/01621459.2021.1947306},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {622-638},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Individualized group learning},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). False discovery rate control under general dependence by
symmetrized data aggregation. <em>JASA</em>, <em>118</em>(541), 607–621.
(<a href="https://doi.org/10.1080/01621459.2021.1945459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new class of distribution-free multiple testing rules for false discovery rate (FDR) control under general dependence. A key element in our proposal is a symmetrized data aggregation (SDA) approach to incorporating the dependence structure via sample splitting, data screening, and information pooling. The proposed SDA filter first constructs a sequence of ranking statistics that fulfill global symmetry properties, and then chooses a data-driven threshold along the ranking to control the FDR. The SDA filter substantially outperforms the Knockoff method in power under moderate to strong dependence, and is more robust than existing methods based on asymptotic p -values. We first develop finite-sample theories to provide an upper bound for the actual FDR under general dependence, and then establish the asymptotic validity of SDA for both the FDR and false discovery proportion control under mild regularity conditions. The procedure is implemented in the R package sdafilter. Numerical results confirm the effectiveness and robustness of SDA in FDR control and show that it achieves substantial power gain over existing methods in many settings.},
  archive      = {J_JASA},
  author       = {Lilun Du and Xu Guo and Wenguang Sun and Changliang Zou},
  doi          = {10.1080/01621459.2021.1945459},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {607-621},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {False discovery rate control under general dependence by symmetrized data aggregation},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent gaussian count time series. <em>JASA</em>,
<em>118</em>(541), 596–606. (<a
href="https://doi.org/10.1080/01621459.2021.1944874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops the theory and methods for modeling a stationary count time series via Gaussian transformations. The techniques use a latent Gaussian process and a distributional transformation to construct stationary series with very flexible correlation features that can have any prespecified marginal distribution, including the classical Poisson, generalized Poisson, negative binomial, and binomial structures. Gaussian pseudo-likelihood and implied Yule–Walker estimation paradigms, based on the autocovariance function of the count series, are developed via a new Hermite expansion. Particle filtering and sequential Monte Carlo methods are used to conduct likelihood estimation. Connections to state space models are made. Our estimation approaches are evaluated in a simulation study and the methods are used to analyze a count series of weekly retail sales. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yisu Jia and Stefanos Kechagias and James Livsey and Robert Lund and Vladas Pipiras},
  doi          = {10.1080/01621459.2021.1944874},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {596-606},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Latent gaussian count time series},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous inference for empirical best predictors with a
poverty study in small areas. <em>JASA</em>, <em>118</em>(541), 583–595.
(<a href="https://doi.org/10.1080/01621459.2021.1942014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, generalized linear mixed models (GLMM) are broadly used in many fields. However, the development of tools for performing simultaneous inference has been largely neglected in this domain. A framework for joint inference is indispensable to carry out statistically valid multiple comparisons of parameters of interest between all or several clusters. We therefore develop simultaneous confidence intervals and multiple testing procedures for empirical best predictors under GLMM. In addition, we implement our methodology to study widely employed examples of mixed models, that is, the unit-level binomial, the area-level Poisson-gamma and the area-level Poisson-lognormal mixed models. The asymptotic results are accompanied by extensive simulations. A case study on predicting poverty rates illustrates applicability and advantages of our simultaneous inference tools.},
  archive      = {J_JASA},
  author       = {Katarzyna Reluga and María-José Lombardía and Stefan Sperlich},
  doi          = {10.1080/01621459.2021.1942014},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {583-595},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Simultaneous inference for empirical best predictors with a poverty study in small areas},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse identification and estimation of large-scale vector
AutoRegressive moving averages. <em>JASA</em>, <em>118</em>(541),
571–582. (<a
href="https://doi.org/10.1080/01621459.2021.1942013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vector autoregressive moving average (VARMA) model is fundamental to the theory of multivariate time series; however, identifiability issues have led practitioners to abandon it in favor of the simpler but more restrictive vector autoregressive (VAR) model. We narrow this gap with a new optimization-based approach to VARMA identification built upon the principle of parsimony. Among all equivalent data-generating models, we use convex optimization to seek the parameterization that is simplest in a certain sense. A user-specified strongly convex penalty is used to measure model simplicity, and that same penalty is then used to define an estimator that can be efficiently computed. We establish consistency of our estimators in a double-asymptotic regime. Our nonasymptotic error bound analysis accommodates both model specification and parameter estimation steps, a feature that is crucial for studying large-scale VARMA algorithms. Our analysis also provides new results on penalized estimation of infinite-order VAR, and elastic net regression under a singular covariance structure of regressors, which may be of independent interest. We illustrate the advantage of our method over VAR alternatives on three real data examples.},
  archive      = {J_JASA},
  author       = {Ines Wilms and Sumanta Basu and Jacob Bien and David S. Matteson},
  doi          = {10.1080/01621459.2021.1942013},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {571-582},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sparse identification and estimation of large-scale vector AutoRegressive moving averages},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic tree ensembles for regularized nonlinear
regression. <em>JASA</em>, <em>118</em>(541), 551–570. (<a
href="https://doi.org/10.1080/01621459.2021.1942012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a novel stochastic tree ensemble method for nonlinear regression, referred to as accelerated Bayesian additive regression trees, or XBART. By combining regularization and stochastic search strategies from Bayesian modeling with computationally efficient techniques from recursive partitioning algorithms, XBART attains state-of-the-art performance at prediction and function estimation. Simulation studies demonstrate that XBART provides accurate point-wise estimates of the mean function and does so faster than popular alternatives, such as BART, XGBoost, and neural networks (using Keras) on a variety of test functions. Additionally, it is demonstrated that using XBART to initialize the standard BART MCMC algorithm considerably improves credible interval coverage and reduces total run-time. Finally, two basic theoretical results are established: the single tree version of the model is asymptotically consistent and the Markov chain produced by the ensemble version of the algorithm has a unique stationary distribution.},
  archive      = {J_JASA},
  author       = {Jingyu He and P. Richard Hahn},
  doi          = {10.1080/01621459.2021.1942012},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {551-570},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Stochastic tree ensembles for regularized nonlinear regression},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stick-breaking processes with exchangeable length variables.
<em>JASA</em>, <em>118</em>(541), 537–550. (<a
href="https://doi.org/10.1080/01621459.2021.1941054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our object of study is the general class of stick-breaking processes with exchangeable length variables. These generalize well-known Bayesian nonparametric priors in an unexplored direction. We give conditions to assure the respective species sampling process is proper and the corresponding prior has full support. For a rich subclass we explain how, by tuning a single [ 0 , 1 ] [ 0 , 1 ] [0,1] -valued parameter, the stochastic ordering of the weights can be modulated, and Dirichlet and Geometric priors can be recovered. A general formula for the distribution of the latent allocation variables is derived and an MCMC algorithm is proposed for density estimation purposes.},
  archive      = {J_JASA},
  author       = {María F. Gil–Leyva and Ramsés H. Mena},
  doi          = {10.1080/01621459.2021.1941054},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {537-550},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Stick-breaking processes with exchangeable length variables},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The generalized oaxaca-blinder estimator. <em>JASA</em>,
<em>118</em>(541), 524–536. (<a
href="https://doi.org/10.1080/01621459.2021.1941053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After performing a randomized experiment, researchers often use ordinary least-square (OLS) regression to adjust for baseline covariates when estimating the average treatment effect. It is widely known that the resulting confidence interval is valid even if the linear model is misspecified. In this article, we generalize that conclusion to covariate adjustment with nonlinear models. We introduce an intuitive way to use any “simple” nonlinear model to construct a covariate-adjusted confidence interval for the average treatment effect. The confidence interval derives its validity from randomization alone, and when nonlinear models fit the data better than linear models, it is narrower than the usual interval from OLS adjustment.},
  archive      = {J_JASA},
  author       = {Kevin Guo and Guillaume Basse},
  doi          = {10.1080/01621459.2021.1941053},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {524-536},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The generalized oaxaca-blinder estimator},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive-to-model hybrid of tests for regressions.
<em>JASA</em>, <em>118</em>(541), 514–523. (<a
href="https://doi.org/10.1080/01621459.2021.1941052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In model checking for regressions, nonparametric estimation-based tests usually have tractable limiting null distributions and are sensitive to oscillating alternative models, but suffer from the curse of dimensionality. In contrast, empirical process-based tests can, at the fastest possible rate, detect local alternatives distinct from the null model, yet are less sensitive to oscillating alternatives and rely on Monte Carlo approximation for critical value determination, which is costly in computation. We propose an adaptive-to-model hybrid of moment and conditional moment-based tests to fully inherit the merits of these two types of tests and avoid the shortcomings. Further, such a hybrid makes nonparametric estimation-based tests, under the alternatives, also share the merits of existing empirical process-based tests. The methodology can be readily applied to other kinds of data and construction of other hybrids. As a by-product in sufficient dimension reduction field, a study on residual-related central mean subspace and central subspace for model adaptation is devoted to showing when alternative models can be indicated and when cannot. Numerical studies are conducted to verify the powerfulness of the proposed test.},
  archive      = {J_JASA},
  author       = {Lingzhu Li and Xuehu Zhu and Lixing Zhu},
  doi          = {10.1080/01621459.2021.1941052},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {514-523},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Adaptive-to-model hybrid of tests for regressions},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving predictions when interest focuses on extreme
random effects. <em>JASA</em>, <em>118</em>(541), 504–513. (<a
href="https://doi.org/10.1080/01621459.2021.1938583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical models that generate predicted random effects are widely used to evaluate the performance of and rank patients, physicians, hospitals and health plans from longitudinal and clustered data. Predicted random effects have been proven to outperform treating clusters as fixed effects (essentially a categorical predictor variable) and using standard regression models, on average. These predicted random effects are often used to identify extreme or outlying values, such as poorly performing hospitals or patients with rapid declines in their health. When interest focuses on the extremes rather than performance on average, there has been no systematic investigation of best approaches. We develop novel methods for prediction of extreme values, evaluate their performance, and illustrate their application using data from the Osteoarthritis Initiative to predict walking speed in older adults. The new methods substantially outperform standard random and fixed-effects approaches for extreme values.},
  archive      = {J_JASA},
  author       = {Charles E. McCulloch and John M. Neuhaus},
  doi          = {10.1080/01621459.2021.1938583},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {504-513},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Improving predictions when interest focuses on extreme random effects},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributional (single) index models. <em>JASA</em>,
<em>118</em>(541), 489–503. (<a
href="https://doi.org/10.1080/01621459.2021.1938582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Distributional (Single) Index Model (DIM) is a semiparametric model for distributional regression, that is, estimation of conditional distributions given covariates. The method is a combination of classical single-index models for the estimation of the conditional mean of a response given covariates, and isotonic distributional regression. The model for the index is parametric, whereas the conditional distributions are estimated nonparametrically under a stochastic ordering constraint. We show consistency of our estimators and apply them to a highly challenging dataset on the length of stay (LoS) of patients in intensive care units. We use the model to provide skillful and calibrated probabilistic predictions for the LoS of individual patients, which outperform the available methods in the literature.},
  archive      = {J_JASA},
  author       = {Alexander Henzi and Gian-Reto Kleger and Johanna F. Ziegel},
  doi          = {10.1080/01621459.2021.1938582},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {489-503},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distributional (Single) index models},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Honest confidence sets for high-dimensional regression by
projection and shrinkage. <em>JASA</em>, <em>118</em>(541), 469–488. (<a
href="https://doi.org/10.1080/01621459.2021.1938581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of honesty in constructing confidence sets arises in nonparametric regression. While optimal rate in nonparametric estimation can be achieved and utilized to construct sharp confidence sets, severe degradation of confidence level often happens after estimating the degree of smoothness. Similarly, for high-dimensional regression, oracle inequalities for sparse estimators could be utilized to construct sharp confidence sets. Yet, the degree of sparsity itself is unknown and needs to be estimated, which causes the honesty problem. To resolve this issue, we develop a novel method to construct honest confidence sets for sparse high-dimensional linear regression. The key idea in our construction is to separate signals into a strong and a weak group, and then construct confidence sets for each group separately. This is achieved by a projection and shrinkage approach, the latter implemented via Stein estimation and the associated Stein unbiased risk estimate. Our confidence set is honest over the full parameter space without any sparsity constraints, while its size adapts to the optimal rate of n − 1 / 4 when the true parameter is indeed sparse. Moreover, under some form of a separation assumption between the strong and weak signals, the diameter of our confidence set can achieve a faster rate than existing methods. Through extensive numerical comparisons on both simulated and real data, we demonstrate that our method outperforms other competitors with big margins for finite samples, including oracle methods built upon the true sparsity of the underlying model.},
  archive      = {J_JASA},
  author       = {Kun Zhou and Ker-Chau Li and Qing Zhou},
  doi          = {10.1080/01621459.2021.1938581},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {469-488},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Honest confidence sets for high-dimensional regression by projection and shrinkage},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RaSE: A variable screening framework via random subspace
ensembles. <em>JASA</em>, <em>118</em>(541), 457–468. (<a
href="https://doi.org/10.1080/01621459.2021.1938084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable screening methods have been shown to be effective in dimension reduction under the ultra-high dimensional setting. Most existing screening methods are designed to rank the predictors according to their individual contributions to the response. As a result, variables that are marginally independent but jointly dependent with the response could be missed. In this work, we propose a new framework for variable screening, random subspace ensemble (RaSE) , which works by evaluating the quality of random subspaces that may cover multiple predictors. This new screening framework can be naturally combined with any subspace evaluation criterion, which leads to an array of screening methods. The framework is capable to identify signals with no marginal effect or with high-order interaction effects. It is shown to enjoy the sure screening property and rank consistency. We also develop an iterative version of RaSE screening with theoretical support. Extensive simulation studies and real-data analysis show the effectiveness of the new screening framework.},
  archive      = {J_JASA},
  author       = {Ye Tian and Yang Feng},
  doi          = {10.1080/01621459.2021.1938084},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {457-468},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {RaSE: A variable screening framework via random subspace ensembles},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correlation tensor decomposition and its application in
spatial imaging data. <em>JASA</em>, <em>118</em>(541), 440–456. (<a
href="https://doi.org/10.1080/01621459.2021.1938083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional tensor data have gained increasing attention in the recent years, especially in biomedical imaging analyses. However, the most existing tensor models are only based on the mean information of imaging pixels. Motivated by multimodal optical imaging data in a breast cancer study, we develop a new tensor learning approach to use pixel-wise correlation information, which is represented through the higher order correlation tensor. We proposed a novel semi-symmetric correlation tensor decomposition method which effectively captures the informative spatial patterns of pixel-wise correlations to facilitate cancer diagnosis. We establish the theoretical properties for recovering structure and for classification consistency. In addition, we develop an efficient algorithm to achieve computational scalability. Our simulation studies and an application on breast cancer imaging data all indicate that the proposed method outperforms other competing methods in terms of pattern recognition and prediction accuracy.},
  archive      = {J_JASA},
  author       = {Yujia Deng and Xiwei Tang and Annie Qu},
  doi          = {10.1080/01621459.2021.1938083},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {440-456},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correlation tensor decomposition and its application in spatial imaging data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partially observed dynamic tensor response regression.
<em>JASA</em>, <em>118</em>(541), 424–439. (<a
href="https://doi.org/10.1080/01621459.2021.1938082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern data science, dynamic tensor data prevail in numerous applications. An important task is to characterize the relationship between dynamic tensor datasets and external covariates. However, the tensor data are often only partially observed, rendering many existing methods inapplicable. In this article, we develop a regression model with a partially observed dynamic tensor as the response and external covariates as the predictor. We introduce the low-rankness, sparsity, and fusion structures on the regression coefficient tensor, and consider a loss function projected over the observed entries. We develop an efficient nonconvex alternating updating algorithm, and derive the finite-sample error bound of the actual estimator from each step of our optimization algorithm. Unobserved entries in the tensor response have imposed serious challenges. As a result, our proposal differs considerably in terms of estimation algorithm, regularity conditions, as well as theoretical properties, compared to the existing tensor completion or tensor response regression solutions. We illustrate the efficacy of our proposed method using simulations and two real applications, including a neuroimaging dementia study and a digital advertising study.},
  archive      = {J_JASA},
  author       = {Jie Zhou and Will Wei Sun and Jingfei Zhang and Lexin Li},
  doi          = {10.1080/01621459.2021.1938082},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {424-439},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Partially observed dynamic tensor response regression},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uniform projection designs and strong orthogonal arrays.
<em>JASA</em>, <em>118</em>(541), 417–423. (<a
href="https://doi.org/10.1080/01621459.2021.1935268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the connections between uniform projection designs and strong orthogonal arrays of strength 2 + 2 + 2+ in this article. Both of these classes of designs are suitable designs for computer experiments and space-filling in two-dimensional margins, but they are motivated by different considerations. Uniform projection designs are introduced by Sun, Wang, and Xu to capture two-dimensional uniformity using the centered L 2 -discrepancy whereas strong orthogonal arrays of strength 2 + are brought forth by He, Cheng, and Tang as they achieve stratifications in two-dimensions on finer grids than ordinary orthogonal arrays. We first derive a new expression for the centered L 2 -discrepancy, which gives a decomposition of the criterion into a sum of squares where each square measures one aspect of design uniformity. This result is not only insightful in itself but also allows us to study strong orthogonal arrays in terms of the discrepancy criterion. More specifically, we show that strong orthogonal arrays of strength 2 + are optimal or nearly optimal under the uniform projection criterion.},
  archive      = {J_JASA},
  author       = {Cheng-Yu Sun and Boxin Tang},
  doi          = {10.1080/01621459.2021.1935268},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {417-423},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Uniform projection designs and strong orthogonal arrays},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A common atoms model for the bayesian nonparametric analysis
of nested data. <em>JASA</em>, <em>118</em>(541), 405–416. (<a
href="https://doi.org/10.1080/01621459.2021.1933499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of large datasets for targeted therapeutic interventions requires new ways to characterize the heterogeneity observed across subgroups of a specific population. In particular, models for partially exchangeable data are needed for inference on nested datasets, where the observations are assumed to be organized in different units and some sharing of information is required to learn distinctive features of the units. In this manuscript, we propose a nested common atoms model (CAM) that is particularly suited for the analysis of nested datasets where the distributions of the units are expected to differ only over a small fraction of the observations sampled from each unit. The proposed CAM allows a two-layered clustering at the distributional and observational level and is amenable to scalable posterior inference through the use of a computationally efficient nested slice sampler algorithm. We further discuss how to extend the proposed modeling framework to handle discrete measurements, and we conduct posterior inference on a real microbiome dataset from a diet swap study to investigate how the alterations in intestinal microbiota composition are associated with different eating habits. We further investigate the performance of our model in capturing true distributional structures in the population by means of a simulation study.},
  archive      = {J_JASA},
  author       = {Francesco Denti and Federico Camerlenghi and Michele Guindani and Antonietta Mira},
  doi          = {10.1080/01621459.2021.1933499},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {405-416},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A common atoms model for the bayesian nonparametric analysis of nested data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online covariance matrix estimation in stochastic gradient
descent. <em>JASA</em>, <em>118</em>(541), 393–404. (<a
href="https://doi.org/10.1080/01621459.2021.1933498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stochastic gradient descent (SGD) algorithm is widely used for parameter estimation, especially for huge datasets and online learning. While this recursive algorithm is popular for computation and memory efficiency, quantifying variability and randomness of the solutions has been rarely studied. This article aims at conducting statistical inference of SGD-based estimates in an online setting. In particular, we propose a fully online estimator for the covariance matrix of averaged SGD (ASGD) iterates only using the iterates from SGD. We formally establish our online estimator’s consistency and show that the convergence rate is comparable to offline counterparts. Based on the classic asymptotic normality results of ASGD, we construct asymptotically valid confidence intervals for model parameters. Upon receiving new observations, we can quickly update the covariance matrix estimate and the confidence intervals. This approach fits in an online setting and takes full advantage of SGD: efficiency in computation and memory.},
  archive      = {J_JASA},
  author       = {Wanrong Zhu and Xi Chen and Wei Biao Wu},
  doi          = {10.1080/01621459.2021.1933498},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {393-404},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Online covariance matrix estimation in stochastic gradient descent},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of the number of spiked eigenvalues in a
covariance matrix by bulk eigenvalue matching analysis. <em>JASA</em>,
<em>118</em>(541), 374–392. (<a
href="https://doi.org/10.1080/01621459.2021.1933497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spiked covariance model has gained increasing popularity in high-dimensional data analysis. A fundamental problem is determination of the number of spiked eigenvalues, K . For estimation of K , most attention has focused on the use of top eigenvalues of sample covariance matrix, and there is little investigation into proper ways of using bulk eigenvalues to estimate K . We propose a principled approach to incorporating bulk eigenvalues in the estimation of K . Our method imposes a working model on the residual covariance matrix, which is assumed to be a diagonal matrix whose entries are drawn from a gamma distribution. Under this model, the bulk eigenvalues are asymptotically close to the quantiles of a fixed parametric distribution. This motivates us to propose a two-step method: the first step uses bulk eigenvalues to estimate parameters of this distribution, and the second step leverages these parameters to assist the estimation of K . The resulting estimator K ̂ aggregates information in a large number of bulk eigenvalues. We show the consistency of K ̂ under a standard spiked covariance model. We also propose a confidence interval estimate for K . Our extensive simulation studies show that the proposed method is robust and outperforms the existing methods in a range of scenarios. We apply the proposed method to analysis of a lung cancer microarray dataset and the 1000 Genomes dataset.},
  archive      = {J_JASA},
  author       = {Zheng Tracy Ke and Yucong Ma and Xihong Lin},
  doi          = {10.1080/01621459.2021.1933497},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {374-392},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation of the number of spiked eigenvalues in a covariance matrix by bulk eigenvalue matching analysis},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic interaction semiparametric function-on-scalar
model. <em>JASA</em>, <em>118</em>(541), 360–373. (<a
href="https://doi.org/10.1080/01621459.2021.1933496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by recent work studying massive functional data, such as the COVID-19 data, we propose a new dynamic interaction semiparametric function-on-scalar (DISeF) model. The proposed model is useful to explore the dynamic interaction among a set of covariates and their effects on the functional response. The proposed model includes many important models investigated recently as special cases. By tensor product B-spline approximating the unknown bivariate coefficient functions, a three-step efficient estimation procedure is developed to iteratively estimate bivariate varying-coefficient functions, the vector of index parameters, and the covariance functions of random effects. We also establish the asymptotic properties of the estimators including the convergence rate and their asymptotic distributions. In addition, we develop a test statistic to check whether the dynamic interaction varies with time/spatial locations, and we prove the asymptotic normality of the test statistic. The finite sample performance of our proposed method and of the test statistic are investigated with several simulation studies. Our proposed DISeF model is also used to analyze the COVID-19 data and the ADNI data. In both applications, hypothesis testing shows that the bivariate varying-coefficient functions significantly vary with the index and the time/spatial locations. For instance, we find that the interaction effect of the population aging and the socio-economic covariates, such as the number of hospital beds, physicians, nurses per 1000 people and GDP per capita, on the COVID-19 mortality rate varies in different periods of the COVID-19 pandemic. The healthcare infrastructure index related to the COVID-19 mortality rate is also obtained for 141 countries estimated based on the proposed DISeF model.},
  archive      = {J_JASA},
  author       = {Hua Liu and Jinhong You and Jiguo Cao},
  doi          = {10.1080/01621459.2021.1933496},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {360-373},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A dynamic interaction semiparametric function-on-scalar model},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partition–mallows model and its inference for rank
aggregation. <em>JASA</em>, <em>118</em>(541), 343–359. (<a
href="https://doi.org/10.1080/01621459.2021.1930547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning how to aggregate ranking lists has been an active research area for many years and its advances have played a vital role in many applications ranging from bioinformatics to internet commerce. The problem of discerning reliability of rankers based only on the rank data is of great interest to many practitioners, but has received less attention from researchers. By dividing the ranked entities into two disjoint groups, that is, relevant and irrelevant/background ones, and incorporating the Mallows model for the relative ranking of relevant entities, we propose a framework for rank aggregation that can not only distinguish quality differences among the rankers but also provide the detailed ranking information for relevant entities. Theoretical properties of the proposed approach are established, and its advantages over existing approaches are demonstrated via simulation studies and real-data applications. Extensions of the proposed method to handle partial ranking lists and conduct covariate-assisted rank aggregation are also discussed.},
  archive      = {J_JASA},
  author       = {Wanchuang Zhu and Yingkai Jiang and Jun S. Liu and Ke Deng},
  doi          = {10.1080/01621459.2021.1930547},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {343-359},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Partition–Mallows model and its inference for rank aggregation},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Treatment effect estimation under additive hazards models
with high-dimensional confounding. <em>JASA</em>, <em>118</em>(541),
327–342. (<a
href="https://doi.org/10.1080/01621459.2021.1930546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating treatment effects for survival outcomes in the high-dimensional setting is critical for many biomedical applications and any application with censored observations. This article establishes an “orthogonal” score for learning treatment effects, using observational data with a potentially large number of confounders. The estimator allows for root- n , asymptotically valid confidence intervals, despite the bias induced by the regularization. Moreover, we develop a novel hazard difference (HDi), estimator. We establish rate double robustness through the cross-fitting formulation. Numerical experiments illustrate the finite sample performance, where we observe that the cross-fitted HDi estimator has the best performance. We study the radical prostatectomy’s effect on conservative prostate cancer management through the SEER-Medicare linked data. Last, we provide an extension to machine learning both approaches and heterogeneous treatment effects. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jue Hou and Jelena Bradic and Ronghui Xu},
  doi          = {10.1080/01621459.2021.1930546},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {327-342},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Treatment effect estimation under additive hazards models with high-dimensional confounding},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional time series prediction under partial observation
of the future curve. <em>JASA</em>, <em>118</em>(541), 315–326. (<a
href="https://doi.org/10.1080/01621459.2021.1929248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– This article tackles one of the most fundamental goals in functional time series analysis which is to provide reliable predictions for future functions. Existing methods for predicting a complete future functional observation use only completely observed trajectories. We develop a new method, called partial functional prediction (PFP), which uses both completely observed trajectories and partial information (available partial data) on the trajectory to be predicted. The PFP method includes an automatic selection criterion for tuning parameters based on minimizing the prediction error, and the convergence rate of the PFP prediction is established. Simulation studies demonstrate that incorporating partially observed trajectory in the prediction outperforms existing methods with respect to mean squared prediction error. The PFP method is illustrated to be superior in the analysis of environmental data and traffic flow data.},
  archive      = {J_JASA},
  author       = {Shuhao Jiao and Alexander Aue and Hernando Ombao},
  doi          = {10.1080/01621459.2021.1929248},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {315-326},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Functional time series prediction under partial observation of the future curve},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-assisted uniformly honest inference for optimal
treatment regimes in high dimension. <em>JASA</em>, <em>118</em>(541),
305–314. (<a
href="https://doi.org/10.1080/01621459.2021.1929246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops new tools to quantify uncertainty in optimal decision making and to gain insight into which variables one should collect information about given the potential cost of measuring a large number of variables. We investigate simultaneous inference to determine if a group of variables is relevant for estimating an optimal decision rule in a high-dimensional semiparametric framework. The unknown link function permits flexible modeling of the interactions between the treatment and the covariates, but leads to nonconvex estimation in high dimension and imposes significant challenges for inference. We first establish that a local restricted strong convexity condition holds with high probability and that any feasible local sparse solution of the estimation problem can achieve the near-oracle estimation error bound. We further rigorously verify that a wild bootstrap procedure based on a debiased version of the local solution can provide asymptotically honest uniform inference for the effect of a group of variables on optimal decision making. The advantage of honest inference is that it does not require the initial estimator to achieve perfect model selection and does not require the zero and nonzero effects to be well-separated. We also propose an efficient algorithm for estimation. Our simulations suggest satisfactory performance. An example from a diabetes study illustrates the real application. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yunan Wu and Lan Wang and Haoda Fu},
  doi          = {10.1080/01621459.2021.1929246},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {305-314},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-assisted uniformly honest inference for optimal treatment regimes in high dimension},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable selection via thompson sampling. <em>JASA</em>,
<em>118</em>(541), 287–304. (<a
href="https://doi.org/10.1080/01621459.2021.1928514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Thompson sampling is a heuristic algorithm for the multi-armed bandit problem which has a long tradition in machine learning. The algorithm has a Bayesian spirit in the sense that it selects arms based on posterior samples of reward probabilities of each arm. By forging a connection between combinatorial binary bandits and spike-and-slab variable selection, we propose a stochastic optimization approach to subset selection called Thompson variable selection (TVS). TVS is a framework for interpretable machine learning which does not rely on the underlying model to be linear. TVS brings together Bayesian reinforcement and machine learning in order to extend the reach of Bayesian subset selection to nonparametric models and large datasets with very many predictors and/or very many observations. Depending on the choice of a reward, TVS can be deployed in offline as well as online setups with streaming data batches. Tailoring multiplay bandits to variable selection, we provide regret bounds without necessarily assuming that the arm mean rewards be unrelated. We show a very strong empirical performance on both simulated and real data. Unlike deterministic optimization methods for spike-and-slab variable selection, the stochastic nature makes TVS less prone to local convergence and thereby more robust.},
  archive      = {J_JASA},
  author       = {Yi Liu and Veronika Ročková},
  doi          = {10.1080/01621459.2021.1928514},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {287-304},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Variable selection via thompson sampling},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric efficiency in convexity constrained
single-index model. <em>JASA</em>, <em>118</em>(541), 272–286. (<a
href="https://doi.org/10.1080/01621459.2021.1927741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider estimation and inference in a single-index regression model with an unknown convex link function. We introduce a convex and Lipschitz constrained least-square estimator (CLSE) for both the parametric and the nonparametric components given independent and identically distributed observations. We prove the consistency and find the rates of convergence of the CLSE when the errors are assumed to have only q ≥ 2 q ≥ 2 q≥2 moments and are allowed to depend on the covariates. When q ≥ 5 , we establish n − 1 / 2 -rate of convergence and asymptotic normality of the estimator of the parametric component. Moreover, the CLSE is proved to be semiparametrically efficient if the errors happen to be homoscedastic. We develop and implement a numerically stable and computationally fast algorithm to compute our proposed estimator in the R package simest. We illustrate our methodology through extensive simulations and data analysis. Finally, our proof of efficiency is geometric and provides a general framework that can be used to prove efficiency of estimators in a wide variety of semiparametric models even when they do not satisfy the efficient score equation directly. Supplementary files for this article are available online.},
  archive      = {J_JASA},
  author       = {Arun K. Kuchibhotla and Rohit K. Patra and Bodhisattva Sen},
  doi          = {10.1080/01621459.2021.1927741},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {272-286},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semiparametric efficiency in convexity constrained single-index model},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional functional graphical models. <em>JASA</em>,
<em>118</em>(541), 257–271. (<a
href="https://doi.org/10.1080/01621459.2021.1924178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical modeling of multivariate functional data is becoming increasingly important in a wide variety of applications. The changes of graph structure can often be attributed to external variables, such as the diagnosis status or time, the latter of which gives rise to the problem of dynamic graphical modeling. Most existing methods focus on estimating the graph by aggregating samples, but largely ignore the subject-level heterogeneity due to the external variables. In this article, we introduce a conditional graphical model for multivariate random functions, where we treat the external variables as conditioning set, and allow the graph structure to vary with the external variables. Our method is built on two new linear operators, the conditional precision operator and the conditional partial correlation operator, which extend the precision matrix and the partial correlation matrix to both the conditional and functional settings. We show that their nonzero elements can be used to characterize the conditional graphs, and develop the corresponding estimators. We establish the uniform convergence of the proposed estimators and the consistency of the estimated graph, while allowing the graph size to grow with the sample size, and accommodating both completely and partially observed data. We demonstrate the efficacy of the method through both simulations and a study of brain functional connectivity network.},
  archive      = {J_JASA},
  author       = {Kuang-Yao Lee and Dingjue Ji and Lexin Li and Todd Constable and Hongyu Zhao},
  doi          = {10.1080/01621459.2021.1924178},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {257-271},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Conditional functional graphical models},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental evaluation of individualized treatment rules.
<em>JASA</em>, <em>118</em>(541), 242–256. (<a
href="https://doi.org/10.1080/01621459.2021.1923511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing availability of individual-level data has led to numerous applications of individualized (or personalized) treatment rules (ITRs). Policy makers often wish to empirically evaluate ITRs and compare their relative performance before implementing them in a target population. We propose a new evaluation metric, the population average prescriptive effect (PAPE). The PAPE compares the performance of ITR with that of non-individualized treatment rule, which randomly treats the same proportion of units. Averaging the PAPE over a range of budget constraints yields our second evaluation metric, the area under the prescriptive effect curve (AUPEC). The AUPEC represents an overall performance measure for evaluation, like the area under the receiver and operating characteristic curve (AUROC) does for classification, and is a generalization of the QINI coefficient used in uplift modeling. We use Neyman’s repeated sampling framework to estimate the PAPE and AUPEC and derive their exact finite-sample variances based on random sampling of units and random assignment of treatment. We extend our methodology to a common setting, in which the same experimental data are used to both estimate and evaluate ITRs. In this case, our variance calculation incorporates the additional uncertainty due to random splits of data used for cross-validation. The proposed evaluation metrics can be estimated without requiring modeling assumptions, asymptotic approximation, or resampling methods. As a result, it is applicable to any ITR including those based on complex machine learning algorithms. The open-source software package is available for implementing the proposed methodology. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kosuke Imai and Michael Lingzhi Li},
  doi          = {10.1080/01621459.2021.1923511},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {242-256},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Experimental evaluation of individualized treatment rules},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Controlling false discovery rate using gaussian mirrors.
<em>JASA</em>, <em>118</em>(541), 222–241. (<a
href="https://doi.org/10.1080/01621459.2021.1923510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneously, finding multiple influential variables and controlling the false discovery rate (FDR) for linear regression models is a fundamental problem. We here propose the Gaussian Mirror (GM) method, which creates for each predictor variable a pair of mirror variables by adding and subtracting a randomly generated Gaussian perturbation, and proceeds with a certain regression method, such as the ordinary least-square or the Lasso (the mirror variables can also be created after selection). The mirror variables naturally lead to test statistics effective for controlling the FDR. Under a mild assumption on the dependence among the covariates, we show that the FDR can be controlled at any designated level asymptotically. We also demonstrate through extensive numerical studies that the GM method is more powerful than many existing methods for selecting relevant variables subject to FDR control, especially for cases when the covariates are highly correlated and the influential variables are not overly sparse.},
  archive      = {J_JASA},
  author       = {Xin Xing and Zhigen Zhao and Jun S. Liu},
  doi          = {10.1080/01621459.2021.1923510},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {222-241},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Controlling false discovery rate using gaussian mirrors},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regression discontinuity designs with a continuous
treatment. <em>JASA</em>, <em>118</em>(541), 208–221. (<a
href="https://doi.org/10.1080/01621459.2021.1923509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard regression discontinuity (RD) design deals with a binary treatment. Many empirical applications of RD designs involve continuous treatments. This article establishes identification and robust bias-corrected inference for such RD designs. Causal identification is achieved by using any changes in the distribution of the continuous treatment at the RD threshold (including the usual mean change as a special case). We discuss a double-robust identification approach and propose an estimand that incorporates the standard fuzzy RD estimand as a special case. Applying the proposed approach, we estimate the impacts of bank capital on bank failure in the pre-Great Depression era in the United States. Our RD design takes advantage of the minimum capital requirements, which change discontinuously with town size.},
  archive      = {J_JASA},
  author       = {Yingying Dong and Ying-Ying Lee and Michael Gou},
  doi          = {10.1080/01621459.2021.1923509},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {208-221},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Regression discontinuity designs with a continuous treatment},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate rank-based distribution-free nonparametric
testing using measure transportation. <em>JASA</em>, <em>118</em>(541),
192–207. (<a
href="https://doi.org/10.1080/01621459.2021.1923508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a general framework for distribution-free nonparametric testing in multi-dimensions, based on a notion of multivariate ranks defined using the theory of measure transportation. Unlike other existing proposals in the literature, these multivariate ranks share a number of useful properties with the usual one-dimensional ranks; most importantly, these ranks are distribution-free. This crucial observation allows us to design nonparametric tests that are exactly distribution-free under the null hypothesis. We demonstrate the applicability of this approach by constructing exact distribution-free tests for two classical nonparametric problems: (I) testing for mutual independence between random vectors, and (II) testing for the equality of multivariate distributions. In particular, we propose (multivariate) rank versions of distance covariance and energy statistic for testing scenarios (I) and (II), respectively. In both these problems, we derive the asymptotic null distribution of the proposed test statistics. We further show that our tests are consistent against all fixed alternatives. Moreover, the proposed tests are computationally feasible and are well-defined under minimal assumptions on the underlying distributions (e.g., they do not need any moment assumptions). We also demonstrate the efficacy of these procedures via extensive simulations. In the process of analyzing the theoretical properties of our procedures, we end up proving some new results in the theory of measure transportation and in the limit theory of permutation statistics using Stein’s method for exchangeable pairs, which may be of independent interest.},
  archive      = {J_JASA},
  author       = {Nabarun Deb and Bodhisattva Sen},
  doi          = {10.1080/01621459.2021.1923508},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {192-207},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multivariate rank-based distribution-free nonparametric testing using measure transportation},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-dimensional MANOVA via bootstrapping and its
application to functional and sparse count data. <em>JASA</em>,
<em>118</em>(541), 177–191. (<a
href="https://doi.org/10.1080/01621459.2021.1920959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach to the problem of high-dimensional multivariate ANOVA via bootstrapping max statistics that involve the differences of sample mean vectors. The proposed method proceeds via the construction of simultaneous confidence regions for the differences of population mean vectors. It is suited to simultaneously test the equality of several pairs of mean vectors of potentially more than two populations. By exploiting the variance decay property that is a natural feature in relevant applications, we are able to provide dimension-free and nearly parametric convergence rates for Gaussian approximation, bootstrap approximation, and the size of the test. We demonstrate the proposed approach with ANOVA problems for functional data and sparse count data. The proposed methodology is shown to work well in simulations and several real data applications.},
  archive      = {J_JASA},
  author       = {Zhenhua Lin and Miles E. Lopes and Hans-Georg Müller},
  doi          = {10.1080/01621459.2021.1920959},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {177-191},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {High-dimensional MANOVA via bootstrapping and its application to functional and sparse count data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Filtering the rejection set while preserving false discovery
rate control. <em>JASA</em>, <em>118</em>(541), 165–176. (<a
href="https://doi.org/10.1080/01621459.2021.1920958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific hypotheses in a variety of applications have domain-specific structures, such as the tree structure of the international classification of diseases (ICD), the directed acyclic graph structure of the gene ontology (GO), or the spatial structure in genome-wide association studies. In the context of multiple testing, the resulting relationships among hypotheses can create redundancies among rejections that hinder interpretability. This leads to the practice of filtering rejection sets obtained from multiple testing procedures, which may in turn invalidate their inferential guarantees. We propose Focused BH, a simple, flexible, and principled methodology to adjust for the application of any prespecified filter. We prove that Focused BH controls the false discovery rate under various conditions, including when the filter satisfies an intuitive monotonicity property and the p -values are positively dependent. We demonstrate in simulations that Focused BH performs well across a variety of settings, and illustrate this method’s practical utility via analyses of real datasets based on ICD and GO.},
  archive      = {J_JASA},
  author       = {Eugene Katsevich and Chiara Sabatti and Marina Bogomolov},
  doi          = {10.1080/01621459.2021.1920958},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {165-176},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Filtering the rejection set while preserving false discovery rate control},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear-cost covariance functions for gaussian random fields.
<em>JASA</em>, <em>118</em>(541), 147–164. (<a
href="https://doi.org/10.1080/01621459.2021.1919122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian random fields (GRF) are a fundamental stochastic model for spatiotemporal data analysis. An essential ingredient of GRF is the covariance function that characterizes the joint Gaussian distribution of the field. Commonly used covariance functions give rise to fully dense and unstructured covariance matrices, for which required calculations are notoriously expensive to carry out for large data. In this work, we propose a construction of covariance functions that result in matrices with a hierarchical structure. Empowered by matrix algorithms that scale linearly with the matrix dimension, the hierarchical structure is proved to be efficient for a variety of random field computations, including sampling, kriging, and likelihood evaluation. Specifically, with n scattered sites, sampling and likelihood evaluation has an O ( n ) cost and kriging has an O ( log n ) cost after preprocessing, particularly favorable for the kriging of an extremely large number of sites (e.g., predicting on more sites than observed). We demonstrate comprehensive numerical experiments to show the use of the constructed covariance functions and their appealing computation time. Numerical examples on a laptop include simulated data of size up to one million, as well as a climate data product with over two million observations.},
  archive      = {J_JASA},
  author       = {Jie Chen and Michael L. Stein},
  doi          = {10.1080/01621459.2021.1919122},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {147-164},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Linear-cost covariance functions for gaussian random fields},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A model-free variable screening method based on leverage
score. <em>JASA</em>, <em>118</em>(541), 135–146. (<a
href="https://doi.org/10.1080/01621459.2021.1918554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid advances in information technology, massive datasets are collected in all fields of science, such as biology, chemistry, and social science. Useful or meaningful information is extracted from these data often through statistical learning or model fitting. In massive datasets, both sample size and number of predictors can be large, in which case conventional methods face computational challenges. Recently, an innovative and effective sampling scheme based on leverage scores via singular value decompositions has been proposed to select rows of a design matrix as a surrogate of the full data in linear regression. Analogously, variable screening can be viewed as selecting rows of the design matrix. However, effective variable selection along this line of thinking remains elusive. In this article, we bridge this gap to propose a weighted leverage variable screening method by using both the left and right singular vectors of the design matrix. We show theoretically and empirically that the predictors selected using our method can consistently include true predictors not only for linear models but also for complicated general index models. Extensive simulation studies show that the weighted leverage screening method is highly computationally efficient and effective. We also demonstrate its success in identifying carcinoma related genes using spatial transcriptome data.},
  archive      = {J_JASA},
  author       = {Wenxuan Zhong and Yiwen Liu and Peng Zeng},
  doi          = {10.1080/01621459.2021.1918554},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {135-146},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A model-free variable screening method based on leverage score},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bootstrap inference for quantile-based modal regression.
<em>JASA</em>, <em>118</em>(541), 122–134. (<a
href="https://doi.org/10.1080/01621459.2021.1918130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop uniform inference methods for the conditional mode based on quantile regression. Specifically, we propose to estimate the conditional mode by minimizing the derivative of the estimated conditional quantile function defined by smoothing the linear quantile regression estimator, and develop two bootstrap methods, a novel pivotal bootstrap and the nonparametric bootstrap, for our conditional mode estimator. Building on high-dimensional Gaussian approximation techniques, we establish the validity of simultaneous confidence rectangles constructed from the two bootstrap methods for the conditional mode. We also extend the preceding analysis to the case where the dimension of the covariate vector is increasing with the sample size. Finally, we conduct simulation experiments and a real data analysis using the U.S. wage data to demonstrate the finite sample performance of our inference method. The supplemental materials include the wage dataset, R codes and an appendix containing proofs of the main results, additional simulation results, discussion of model misspecification and quantile crossing, and additional details of the numerical implementation.},
  archive      = {J_JASA},
  author       = {Tao Zhang and Kengo Kato and David Ruppert},
  doi          = {10.1080/01621459.2021.1918130},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {122-134},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bootstrap inference for quantile-based modal regression},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Eigen selection in spectral clustering: A theory-guided
practice. <em>JASA</em>, <em>118</em>(541), 109–121. (<a
href="https://doi.org/10.1080/01621459.2021.1917418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on a Gaussian mixture type model of K components, we derive eigen selection procedures that improve the usual spectral clustering algorithms in high-dimensional settings, which typically act on the top few eigenvectors of an affinity matrix (e.g., X ⊤ X X ⊤ X X⊤X ) derived from the data matrix X X X . Our selection principle formalizes two intuitions: (i) eigenvectors should be dropped when they have no clustering power; (ii) some eigenvectors corresponding to smaller spiked eigenvalues should be dropped due to estimation inaccuracy. Our selection procedures lead to new spectral clustering algorithms: ESSC for K = 2 and GESSC for K &gt; 2. The newly proposed algorithms enjoy better stability and compare favorably against canonical alternatives, as demonstrated in extensive simulation and multiple real data studies. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiao Han and Xin Tong and Yingying Fan},
  doi          = {10.1080/01621459.2021.1917418},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {109-121},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Eigen selection in spectral clustering: A theory-guided practice},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ultra-high dimensional quantile regression for longitudinal
data: An application to blood pressure analysis. <em>JASA</em>,
<em>118</em>(541), 97–108. (<a
href="https://doi.org/10.1080/01621459.2022.2128806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite major advances in research and treatment, identifying important genotype risk factors for high blood pressure remains challenging. Traditional genome-wide association studies (GWAS) focus on one single nucleotide polymorphism (SNP) at a time. We aim to select among over half a million SNPs along with time-varying phenotype variables via simultaneous modeling and variable selection, focusing on the most dangerous blood pressure levels at high quantiles. Taking advantage of rich data from a large-scale public health study, we develop and apply a novel quantile penalized generalized estimating equations (GEE) approach, incorporating several key aspects including ultra-high dimensional genetic SNPs, the longitudinal nature of blood pressure measurements, time-varying covariates, and conditional high quantiles of blood pressure. Importantly, we identify interesting new SNPs for high blood pressure. Besides, we find blood pressure levels are likely heterogeneous, where the important risk factors identified differ among quantiles. This comprehensive picture of conditional quantiles of blood pressure can allow more insights and targeted treatments. We provide an efficient computational algorithm and prove consistency, asymptotic normality, and the oracle property for the quantile penalized GEE estimators with ultra-high dimensional predictors. Moreover, we establish model-selection consistency for high-dimensional BIC. Simulation studies show the promise of the proposed approach. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Tianhai Zu and Heng Lian and Brittany Green and Yan Yu},
  doi          = {10.1080/01621459.2022.2128806},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {97-108},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Ultra-high dimensional quantile regression for longitudinal data: An application to blood pressure analysis},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpolating population distributions using public-use
data: An application to income segregation using american community
survey data. <em>JASA</em>, <em>118</em>(541), 84–96. (<a
href="https://doi.org/10.1080/01621459.2022.2126779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of income inequality is an important problem to demographers, policy makers, economists, and social scientists. A causal link has been hypothesized between income inequality and income segregation, which measures how much households with similar incomes cluster. The information theory index is used to measure income segregation, however, critics have suggested the divergence index instead. Motivated by this, we construct both indices using American Community Survey (ACS) estimates of features of the income distribution. Since the elimination of the decennial census long form, methods of computing these indices must be updated to interpolate ACS estimates and account for survey error. We propose a novel model-based method to do this which improves on previous approaches by using more types of estimates, and by providing uncertainty quantification. We apply this method to estimate U.S. census tract-level income distributions, and in turn use these to construct both income segregation indices. We find major differences between the two indices and find evidence that the information index underestimates the relationship between income inequality and income segregation. The literature suggests interventions designed to reduce income inequality by reducing income segregation, or vice versa, so using the information index implicitly understates the value of these interventions. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Matthew Simpson and Scott H. Holan and Christopher K. Wikle and Jonathan R. Bradley},
  doi          = {10.1080/01621459.2022.2126779},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {84-96},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Interpolating population distributions using public-use data: An application to income segregation using american community survey data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crop yield prediction using bayesian spatially varying
coefficient models with functional predictors. <em>JASA</em>,
<em>118</em>(541), 70–83. (<a
href="https://doi.org/10.1080/01621459.2022.2123333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable prediction for crop yield is crucial for economic planning, food security monitoring, and agricultural risk management. This study aims to develop a crop yield forecasting model at large spatial scales using meteorological variables closely related to crop growth. The influence of climate patterns on agricultural productivity can be spatially inhomogeneous due to local soil and environmental conditions. We propose a Bayesian spatially varying functional model (BSVFM) to predict county-level corn yield for five Midwestern states, based on annual precipitation and daily maximum and minimum temperature trajectories modeled as multivariate functional predictors. The proposed model accommodates spatial correlation and measurement errors of functional predictors, and respects the spatially heterogeneous relationship between the response and associated predictors by allowing the functional coefficients to vary over space. The model also incorporates a Bayesian variable selection device to further expand its capacity to accommodate spatial heterogeneity. The proposed method is demonstrated to outperform other highly competitive methods in corn yield prediction, owing to the flexibility of allowing spatial heterogeneity with spatially varying coefficients in our model. Our study provides further insights into understanding the impact of climate change on crop yield. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yeonjoo Park and Bo Li and Yehua Li},
  doi          = {10.1080/01621459.2022.2123333},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {70-83},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Crop yield prediction using bayesian spatially varying coefficient models with functional predictors},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capture-recapture models with heterogeneous temporary
emigration. <em>JASA</em>, <em>118</em>(541), 56–69. (<a
href="https://doi.org/10.1080/01621459.2022.2123332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach for modeling capture-recapture (CR) data on open populations that exhibit temporary emigration, while also accounting for individual heterogeneity to allow for differences in visit patterns and capture probabilities between individuals. Our modeling approach combines changepoint processes—fitted using an adaptive approach—for inferring individual visits, with Bayesian mixture modeling—fitted using a nonparametric approach—for identifying clusters of individuals with similar visit patterns or capture probabilities. The proposed method is extremely flexible as it can be applied to any CR dataset and is not reliant upon specialized sampling schemes, such as Pollock’s robust design. We fit the new model to motivating data on salmon anglers collected annually at the Gaula river in Norway. Our results when analyzing data from the 2017, 2018, and 2019 seasons reveal two clusters of anglers—consistent across years—with substantially different visit patterns. Most anglers are allocated to the “occasional visitors” cluster, making infrequent and shorter visits with mean total length of stay at the river of around seven days, whereas there also exists a small cluster of “super visitors,” with regular and longer visits, with mean total length of stay of around 30 days in a season. Our estimate of the probability of catching salmon whilst at the river is more than three times higher than that obtained when using a model that does not account for temporary emigration, giving us a better understanding of the impact of fishing at the river. Finally, we discuss the effect of the COVID-19 pandemic on the angling population by modeling data from the 2020 season. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Eleni Matechou and Raffaele Argiento},
  doi          = {10.1080/01621459.2022.2123332},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {56-69},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Capture-recapture models with heterogeneous temporary emigration},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IProMix: A mixture model for studying the function of ACE2
based on bulk proteogenomic data. <em>JASA</em>, <em>118</em>(541),
43–55. (<a href="https://doi.org/10.1080/01621459.2022.2110876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused over six million deaths in the ongoing COVID-19 pandemic. SARS-CoV-2 uses ACE2 protein to enter human cells, raising a pressing need to characterize proteins/pathways interacted with ACE2. Large-scale proteomic profiling technology is not mature at single-cell resolution to examine the protein activities in disease-relevant cell types. We propose iProMix , a novel statistical framework to identify epithelial-cell specific associations between ACE2 and other proteins/pathways with bulk proteomic data. iProMix decomposes the data and models cell type-specific conditional joint distribution of proteins through a mixture model. It improves cell-type composition estimation from prior input, and uses a nonparametric inference framework to account for uncertainty of cell-type proportion estimates in hypothesis test. Simulations demonstrate iProMix has well-controlled false discovery rates and favorable powers in nonasymptotic settings. We apply iProMix to the proteomic data of 110 (tumor-adjacent) normal lung tissue samples from the Clinical Proteomic Tumor Analysis Consortium lung adenocarcinoma study, and identify interferon α / γ response pathways as the most significant pathways associated with ACE2 protein abundances in epithelial cells. Strikingly, the association direction is sex-specific. This result casts light on the sex difference of COVID-19 incidences and outcomes, and motivates sex-specific evaluation for interferon therapies. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiaoyu Song and Jiayi Ji and Pei Wang},
  doi          = {10.1080/01621459.2022.2110876},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {43-55},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {IProMix: A mixture model for studying the function of ACE2 based on bulk proteogenomic data},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalized integration approach to association analysis
with multi-category outcome: An application to a tumor sequencing study
of colorectal cancer and smoking. <em>JASA</em>, <em>118</em>(541),
29–42. (<a href="https://doi.org/10.1080/01621459.2022.2105703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is a heterogeneous disease, and rapid progress in sequencing and -omics technologies has enabled researchers to characterize tumors comprehensively. This has stimulated an intensive interest in studying how risk factors are associated with various tumor heterogeneous features. The Cancer Prevention Study-II (CPS-II) cohort is one of the largest prospective studies, particularly valuable for elucidating associations between cancer and risk factors. In this article, we investigate the association of smoking with novel colorectal tumor markers obtained from targeted sequencing. However, due to cost and logistic difficulties, only a limited number of tumors can be assayed, which limits our capability for studying these associations. Meanwhile, there are extensive studies for assessing the association of smoking with overall cancer risk and established colorectal tumor markers. Importantly, such summary information is readily available from the literature. By linking this summary information to parameters of interest with proper constraints, we develop a generalized integration approach for polytomous logistic regression model with outcome characterized by tumor features. The proposed approach gains the efficiency through maximizing the joint likelihood of individual-level tumor data and external summary information under the constraints that narrow the parameter searching space. We apply the proposed method to the CPS-II data and identify the association of smoking with colorectal cancer risk differing by the mutational status of APC and RNF43 genes, neither of which is identified by the conventional analysis of CPS-II individual data only. These results help better understand the role of smoking in the etiology of colorectal cancer. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jiayin Zheng and Xinyuan Dong and Christina C. Newton and Li Hsu},
  doi          = {10.1080/01621459.2022.2105703},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {29-42},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A generalized integration approach to association analysis with multi-category outcome: An application to a tumor sequencing study of colorectal cancer and smoking},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Rejoinder: LESA: Longitudinal elastic shape analysis of
brain subcortical structures. <em>JASA</em>, <em>118</em>(541), 25–28.
(<a href="https://doi.org/10.1080/01621459.2022.2139264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Zhengwu Zhang and Yuexuan Wu and Di Xiong and Joseph G. Ibrahim and Anuj Srivastava and Hongtu Zhu},
  doi          = {10.1080/01621459.2022.2139264},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {25-28},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder: LESA: longitudinal elastic shape analysis of brain subcortical structures},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion of “LESA: Longitudinal elastic shape analysis of
brain subcortical structures.” <em>JASA</em>, <em>118</em>(541), 22–24.
(<a href="https://doi.org/10.1080/01621459.2022.2123334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Daiwei Zhang and Jian Kang},
  doi          = {10.1080/01621459.2022.2123334},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {22-24},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “LESA: Longitudinal elastic shape analysis of brain subcortical structures”},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion of “LESA: Longitudinal elastic shape analysis of
brain subcortical structures.” <em>JASA</em>, <em>118</em>(541), 20–21.
(<a href="https://doi.org/10.1080/01621459.2022.2115916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Moo K. Chung and Jamie L. Hanson and Richard J. Davidson and Seth D. Pollak},
  doi          = {10.1080/01621459.2022.2115916},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {20-21},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “LESA: Longitudinal elastic shape analysis of brain subcortical structures”},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discussion of LESA: Longitudinal elastic shape analysis of
brain subcortical structures. <em>JASA</em>, <em>118</em>(541), 18–19.
(<a href="https://doi.org/10.1080/01621459.2022.2120399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {John A. D. Aston and Eardi Lila},
  doi          = {10.1080/01621459.2022.2120399},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {18-19},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of LESA: Longitudinal elastic shape analysis of brain subcortical structures},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). LESA: Longitudinal elastic shape analysis of brain
subcortical structures. <em>JASA</em>, <em>118</em>(541), 3–17. (<a
href="https://doi.org/10.1080/01621459.2022.2102984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past 30 years, magnetic resonance imaging has become a ubiquitous tool for accurately visualizing the change and development of the brain’s subcortical structures (e.g., hippocampus). Although subcortical structures act as information hubs of the nervous system, their quantification is still in its infancy due to many challenges in shape extraction, representation, and modeling. Here, we develop a simple and efficient framework of longitudinal elastic shape analysis (LESA) for subcortical structures. Integrating ideas from elastic shape analysis of static surfaces and statistical modeling of sparse longitudinal data, LESA provides a set of tools for systematically quantifying changes of longitudinal subcortical surface shapes from raw structure MRI data. The key novelties of LESA include: (i) it can efficiently represent complex subcortical structures using a small number of basis functions and (ii) it can accurately delineate the spatiotemporal shape changes of the human subcortical structures. We applied LESA to analyze three longitudinal neuroimaging datasets and showcase its wide applications in estimating continuous shape trajectories, building life-span growth patterns, and comparing shape differences among different groups. In particular, with the Alzheimer’s Disease Neuroimaging Initiative (ADNI) data, we found that Alzheimer’s Disease (AD) can significantly speed the shape change of the lateral ventricle and the hippocampus from 60 to 75 years olds compared with normal aging. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhengwu Zhang and Yuexuan Wu and Di Xiong and Joseph G. Ibrahim and Anuj Srivastava and Hongtu Zhu},
  doi          = {10.1080/01621459.2022.2102984},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {3-17},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {LESA: Longitudinal elastic shape analysis of brain subcortical structures},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: What makes for a great applications and case
studies paper? <em>JASA</em>, <em>118</em>(541), 1–2. (<a
href="https://doi.org/10.1080/01621459.2023.2173458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Michael L. Stein},
  doi          = {10.1080/01621459.2023.2173458},
  journal      = {Journal of the American Statistical Association},
  number       = {541},
  pages        = {1-2},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Editorial: What makes for a great applications and case studies paper?},
  volume       = {118},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
