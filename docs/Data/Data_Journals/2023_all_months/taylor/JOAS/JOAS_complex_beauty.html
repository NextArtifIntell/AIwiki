<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JOAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="joas---150">JOAS - 150</h2>
<ul>
<li><details>
<summary>
(2023). Hot-spots detection in count data by poisson assisted smooth
sparse tensor decomposition. <em>JOAS</em>, <em>50</em>(14), 2999–3029.
(<a href="https://doi.org/10.1080/02664763.2022.2112557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data occur widely in many bio-surveillance and healthcare applications, e.g. the numbers of new patients of different types of infectious diseases from different cities/counties/states repeatedly over time, say, daily/weekly/monthly. For this type of count data, one important task is the quick detection and localization of hot-spots in terms of unusual infectious rates so that we can respond appropriately. In this paper, we develop a method called Poisson assisted Smooth Sparse Tensor Decomposition (PoSSTenD) , which not only detect when hot-spots occur but also localize where hot-spots occur. The main idea of our proposed PoSSTenD method is articulated as follows. First, we represent the observed count data as a three-dimensional tensor including (1) a spatial dimension for location patterns, e.g. different cities/countries/states; (2) a temporal domain for time patterns, e.g. daily/weekly/monthly; (3) a categorical dimension for different types of data sources, e.g. different types of diseases. Second, we fit this tensor into a Poisson regression model, and then we further decompose the infectious rate into two components: smooth global trend and local hot-spots. Third, we detect when hot-spots occur by building a cumulative sum (CUSUM) control chart and localize where hot-spots occur by their LASSO-type sparse estimation. The usefulness of our proposed methodology is validated through numerical simulation studies and a real-world dataset, which records the annual number of 10 different infectious diseases from 1993 to 2018 for 49 mainland states in the United States.},
  archive      = {J_JOAS},
  author       = {Yujie Zhao and Xiaoming Huo and Yajun Mei},
  doi          = {10.1080/02664763.2022.2112557},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {2999-3029},
  shortjournal = {J. Appl. Stat.},
  title        = {Hot-spots detection in count data by poisson assisted smooth sparse tensor decomposition},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rapid online plant leaf area change detection with
high-throughput plant image data. <em>JOAS</em>, <em>50</em>(14),
2984–2998. (<a
href="https://doi.org/10.1080/02664763.2022.2150753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput plant phenotyping (HTPP) has become an emerging technique to study plant traits due to its fast, labor-saving, accurate and non-destructive nature. It has wide applications in plant breeding and crop management. However, the resulting massive image data has raised a challenge associated with efficient plant traits prediction and anomaly detection. In this paper, we propose a two-step image-based online detection framework for monitoring and quick change detection of the individual plant leaf area via real-time imaging data. Our proposed method is able to achieve a smaller detection delay compared with some baseline methods under some predefined false alarm rate constraint. Moreover, it does not need to store all past image information and can be implemented in real time. The efficiency of the proposed framework is validated by a real data analysis.},
  archive      = {J_JOAS},
  author       = {Yinglun Zhan and Ruizhi Zhang and Yuzhen Zhou and Vincent Stoerger and Jeremy Hiller and Tala Awada and Yufeng Ge},
  doi          = {10.1080/02664763.2022.2150753},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {2984-2998},
  shortjournal = {J. Appl. Stat.},
  title        = {Rapid online plant leaf area change detection with high-throughput plant image data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knockoff procedure for false discovery rate control in
high-dimensional data streams. <em>JOAS</em>, <em>50</em>(14),
2970–2983. (<a
href="https://doi.org/10.1080/02664763.2023.2200496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by applications to root-cause identification of faults in high-dimensional data streams that may have very limited samples after faults are detected, we consider multiple testing in models for multivariate statistical process control (SPC). With quick fault detection, only small portion of data streams being out-of-control (OC) can be assumed. It is a long standing problem to identify those OC data streams while controlling the number of false discoveries. It is challenging due to the limited number of OC samples after the termination of the process when faults are detected. Although several false discovery rate (FDR) controlling methods have been proposed, people may prefer other methods for quick detection. With a recently developed method called Knockoff filtering, we propose a knockoff procedure that can combine with other fault detection methods in the sense that the knockoff procedure does not change the stopping time, but may identify another set of faults to control FDR. A theorem for the FDR control of the proposed procedure is provided. Simulation studies show that the proposed procedure can control FDR while maintaining high power. We also illustrate the performance in an application to semiconductor manufacturing processes that motivated this development.},
  archive      = {J_JOAS},
  author       = {Ka Wai Tsang and Fugee Tsung and Zhihao Xu},
  doi          = {10.1080/02664763.2023.2200496},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {2970-2983},
  shortjournal = {J. Appl. Stat.},
  title        = {Knockoff procedure for false discovery rate control in high-dimensional data streams},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active learning-based multistage sequential decision-making
model with application on common bile duct stone evaluation.
<em>JOAS</em>, <em>50</em>(14), 2951–2969. (<a
href="https://doi.org/10.1080/02664763.2023.2164885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistage sequential decision-making occurs in many real-world applications such as healthcare diagnosis and treatment. One concrete example is when the doctors need to decide to collect which kind of information from subjects so as to make the good medical decision cost-effectively. In this paper, an active learning-based method is developed to model the doctors&#39; decision-making process that actively collects necessary information from each subject in a sequential manner. The effectiveness of the proposed model, especially its two-stage version, is validated on both simulation studies and a case study of common bile duct stone evaluation for pediatric patients.},
  archive      = {J_JOAS},
  author       = {Hongzhen Tian and Reuven Zev Cohen and Chuck Zhang and Yajun Mei},
  doi          = {10.1080/02664763.2023.2164885},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {2951-2969},
  shortjournal = {J. Appl. Stat.},
  title        = {Active learning-based multistage sequential decision-making model with application on common bile duct stone evaluation},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Significance testing of rank cross-correlations between
autocorrelated time series with short-range dependence. <em>JOAS</em>,
<em>50</em>(14), 2934–2950. (<a
href="https://doi.org/10.1080/02664763.2022.2137115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical dependency measures such as Kendall’s Tau or Spearman’s Rho are frequently used to analyse the coherence between time series in environmental data analyses. Autocorrelation of the data can, however, result in spurious cross correlations if not accounted for. Here, we present the asymptotic distribution of the estimators of Spearman’s Rho and Kendall’s Tau, which can be used for statistical hypothesis testing of cross-correlations between autocorrelated observations. The results are derived using U-statistics under the assumption of absolutely regular (or β -mixing) processes. These comprise many short-range dependent processes, such as ARMA-, GARCH- and some copula-based models relevant in the environmental sciences. We show that while the assumption of absolute regularity is required, the specific type of model does not have to be specified for the hypothesis test. Simulations show the improved performance of the modified hypothesis test for some common stochastic models and small to moderate sample sizes under autocorrelation. The methodology is applied to observed climatological time series of flood discharges and temperatures in Europe. While the standard test results in spurious correlations between floods and temperatures, this is not the case for the proposed test, which is more consistent with the literature on flood regime changes in Europe.},
  archive      = {J_JOAS},
  author       = {David Lun and Svenja Fischer and Alberto Viglione and Günter Blöschl},
  doi          = {10.1080/02664763.2022.2137115},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {2934-2950},
  shortjournal = {J. Appl. Stat.},
  title        = {Significance testing of rank cross-correlations between autocorrelated time series with short-range dependence},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable, predictive spatio-temporal models via
enhanced pairwise directions estimation. <em>JOAS</em>, <em>50</em>(14),
2914–2933. (<a
href="https://doi.org/10.1080/02664763.2022.2147150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns predictive modeling for spatio-temporal data as well as model interpretation using data information in space and time. We develop a novel approach based on supervised dimension reduction for such data in order to capture nonlinear mean structures without requiring a prespecified parametric model. In addition to prediction as a common interest, this approach emphasizes the exploration of geometric information from the data. The method of Pairwise Directions Estimation (PDE) is implemented in our approach as a data-driven function searching for spatial patterns and temporal trends. The benefit of using geometric information from the method of PDE is highlighted, which aids effectively in exploring data structures. We further enhance PDE, referring to it as PDE+, by incorporating kriging to estimate the random effects not explained in the mean functions. Our proposal can not only increase prediction accuracy but also improve the interpretation for modeling. Two simulation examples are conducted and comparisons are made with several existing methods. The results demonstrate that the proposed PDE+ method is very useful for exploring and interpreting the patterns and trends for spatio-temporal data. Illustrative applications to two real datasets are also presented.},
  archive      = {J_JOAS},
  author       = {Heng-Hui Lue and ShengLi Tzeng},
  doi          = {10.1080/02664763.2022.2147150},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {2914-2933},
  shortjournal = {J. Appl. Stat.},
  title        = {Interpretable, predictive spatio-temporal models via enhanced pairwise directions estimation},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive resources allocation CUSUM for binomial count data
monitoring with application to COVID-19 hotspot detection.
<em>JOAS</em>, <em>50</em>(14), 2889–2913. (<a
href="https://doi.org/10.1080/02664763.2022.2117288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an efficient statistical method (denoted as ‘Adaptive Resources Allocation CUSUM’) to robustly and efficiently detect the hotspot with limited sampling resources. Our main idea is to combine the multi-arm bandit (MAB) and change-point detection methods to balance the exploration and exploitation of resource allocation for hotspot detection. Further, a Bayesian weighted update is used to update the posterior distribution of the infection rate. Then, the upper confidence bound (UCB) is used for resource allocation and planning. Finally, CUSUM monitoring statistics to detect the change point as well as the change location. For performance evaluation, we compare the performance of the proposed method with several benchmark methods in the literature and showed the proposed algorithm is able to achieve a lower detection delay and higher detection precision. Finally, this method is applied to hotspot detection in a real case study of county-level daily positive COVID-19 cases in Washington State WA) and demonstrates the effectiveness with very limited distributed samples.},
  archive      = {J_JOAS},
  author       = {Jiuyun Hu and Yajun Mei and Sarah Holte and Hao Yan},
  doi          = {10.1080/02664763.2022.2117288},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {2889-2913},
  shortjournal = {J. Appl. Stat.},
  title        = {Adaptive resources allocation CUSUM for binomial count data monitoring with application to COVID-19 hotspot detection},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection and estimation of multiple transient changes.
<em>JOAS</em>, <em>50</em>(14), 2862–2888. (<a
href="https://doi.org/10.1080/02664763.2023.2174257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change-point detection methods are proposed for the case of temporary failures, or transient changes, when an unexpected disorder is ultimately followed by a re-adjustment and return to the initial state. A base distribution of the ‘in-control’ state changes to an ‘out-of-control’ distribution for unknown periods of time. Likelihood based sequential and retrospective tools are proposed for the detection and estimation of each pair of change-points. The accuracy of the obtained change-point estimates is assessed. Proposed methods offer simultaneous control of the familywise false alarm and false re-adjustment rates at the pre-chosen levels.},
  archive      = {J_JOAS},
  author       = {Michael Baron and Sergey V. Malov},
  doi          = {10.1080/02664763.2023.2174257},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {2862-2888},
  shortjournal = {J. Appl. Stat.},
  title        = {Detection and estimation of multiple transient changes},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial to the special issue: Modern streaming data
analytics. <em>JOAS</em>, <em>50</em>(14), 2857–2861. (<a
href="https://doi.org/10.1080/02664763.2023.2247646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {Yajun Mei and Jay Bartroff and Jie Chen and Georgios Fellouris and Ruizhi Zhang},
  doi          = {10.1080/02664763.2023.2247646},
  journal      = {Journal of Applied Statistics},
  number       = {14},
  pages        = {2857-2861},
  shortjournal = {J. Appl. Stat.},
  title        = {Editorial to the special issue: Modern streaming data analytics},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forward variable selection for random forest models.
<em>JOAS</em>, <em>50</em>(13), 2836–2856. (<a
href="https://doi.org/10.1080/02664763.2022.2095362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random forest is a popular prediction approach for handling high dimensional covariates. However, it often becomes infeasible to interpret the obtained high dimensional and non-parametric model. Aiming for an interpretable predictive model, we develop a forward variable selection method using the continuous ranked probability score (CRPS) as the loss function. eOur stepwise procedure selects at each step a variable that minimizes the CRPS risk and a stopping criterion for selection is designed based on an estimation of the CRPS risk difference of two consecutive steps. We provide mathematical motivation for our method by proving that in a population sense, the method attains the optimal set. In a simulation study, we compare the performance of our method with an existing variable selection method, for different sample sizes and correlation strength of covariates. Our method is observed to have a much lower false positive rate. We also demonstrate an application of our method to statistical post-processing of daily maximum temperature forecasts in the Netherlands. Our method selects about 10\% covariates while retaining the same predictive power.},
  archive      = {J_JOAS},
  author       = {Jasper Velthoen and Juan-Juan Cai and Geurt Jongbloed},
  doi          = {10.1080/02664763.2022.2095362},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {2836-2856},
  shortjournal = {J. Appl. Stat.},
  title        = {Forward variable selection for random forest models},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimum regularized covariance determinant and principal
component analysis-based method for the identification of high leverage
points in high dimensional sparse data. <em>JOAS</em>, <em>50</em>(13),
2817–2835. (<a
href="https://doi.org/10.1080/02664763.2022.2093842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main aim of this paper is to propose a novel method (RMD-MRCD-PCA) of identification of High Leverage Points (HLPs) in high-dimensional sparse data. It is to address the weakness of the Robust Mahalanobis Distance (RMD) method which is based on the Minimum Regularized Covariance Determinant (RMD-MRCD), which indicates a decrease in its performance as the number of independent variables ( p ) increases. The RMD-MRCD-PCA is developed by incorporating the Principal Component Analysis (PCA) in the MRCD algorithm whereby this robust approach shrinks the covariance matrix to make it invertible and thus, can be employed to compute the RMD for high dimensional data. A simulation study and two real data sets are used to illustrate the merit of our proposed method compared to the RMD-MRCD and Robust PCA (ROBPCA) methods. Findings show that the performance of the RMD-MRCD is similar to the performance of the RMD-MRCD-PCA for p close to 200. However, its performance tends to decrease when the number of p is more than 200 and worsens at p equals 700 and larger. On the other hand, the ROBPCA is not effective for less than 20\% contamination as it suffers from serious swamping problems.},
  archive      = {J_JOAS},
  author       = {Siti Zahariah and Habshah Midi},
  doi          = {10.1080/02664763.2022.2093842},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {2817-2835},
  shortjournal = {J. Appl. Stat.},
  title        = {Minimum regularized covariance determinant and principal component analysis-based method for the identification of high leverage points in high dimensional sparse data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Determination of new multiple deferred state sampling plan
with economic perspective under weibull distribution. <em>JOAS</em>,
<em>50</em>(13), 2796–2816. (<a
href="https://doi.org/10.1080/02664763.2022.2091526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on designing a new multiple deferred state sampling plan to ensure products’ mean lifetime that complies with Weibull distribution. The parameters that characterize the proposed plan are determined by considering two specified points on the operating characteristic curve. Practical applications of the proposed plan for assuring mean lifetimes of electrical appliances as well as Lithium-ion batteries are explained by using real-time data and simulated data respectively. Sensitivity analysis on testing time of the life test is done and theoretical average sample number is compared with the same obtained by simulation. By comparing the proposed plan with other existing sampling plans based on discriminating power, the number of units required for lot sentencing, it is observed that the new multiple deferred state sampling plan provides quality assurance for the products with low inspection costs compared to the other existing sampling plans. Besides, this study investigates the economic design of a new multiple deferred state sampling plan and compares the total cost needed in the proposed plan with the same required for some other existing sampling plans.},
  archive      = {J_JOAS},
  author       = {Jeyadurga Periyasamypandian and Saminathan Balamurali},
  doi          = {10.1080/02664763.2022.2091526},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {2796-2816},
  shortjournal = {J. Appl. Stat.},
  title        = {Determination of new multiple deferred state sampling plan with economic perspective under weibull distribution},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Assessing adult physical activity and compliance with 2008
CDC guidelines using a bayesian two-part measurement error model.
<em>JOAS</em>, <em>50</em>(13), 2777–2795. (<a
href="https://doi.org/10.1080/02664763.2022.2088706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While there is wide agreement that physical activity is an important component of a healthy lifestyle, it is unclear how many people adhere to public health recommendations on physical activity. The Physical Activity Guidelines (PAG) , published by the CDC, provides guidelines to American adults, but it is difficult to assess compliance with these guidelines. The PAG further complicates adherence assessment by recommending activity to occur in at least 10 min bouts . To better understand the measurement capabilities of various instruments to quantify activity, and to propose an approach to evaluate activity relative to the PAG, researchers at Iowa State University administered the Physical Activity Measurement Survey (PAMS) to over 1000 participants in four different Iowa counties. In this paper, we develop a two-part Bayesian measurement error model and apply it to the PAMS data in order to assess compliance with the PAG in the Iowa adult population. The model accurately accounts for the 10 min bout requirement put forth in the PAG. The measurement error model corrects biased estimates and accounts for day-to-day variation in activity. The model is also applied to the nationally representative National Health and Nutrition Examination Survey.},
  archive      = {J_JOAS},
  author       = {Daniel Ries and Alicia Carriquiry},
  doi          = {10.1080/02664763.2022.2088706},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {2777-2795},
  shortjournal = {J. Appl. Stat.},
  title        = {Assessing adult physical activity and compliance with 2008 CDC guidelines using a bayesian two-part measurement error model},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian heterogeneity in a meta–analysis with two studies
and binary data. <em>JOAS</em>, <em>50</em>(13), 2760–2776. (<a
href="https://doi.org/10.1080/02664763.2022.2084719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The meta–analysis of two trials is valuable in many practical situations, such as studies of rare and/or orphan diseases focussed on a single intervention. In this context, additional concerns, like small sample size and/or heterogeneity in the results obtained, might make standard frequentist and Bayesian techniques inappropriate. In a meta–analysis, moreover, the presence of between–sample heterogeneity adds model uncertainty, which must be taken into consideration when drawing inferences. We suggest that the most appropriate way to measure this heterogeneity is by clustering the samples and then determining the posterior probability of the cluster models. The meta–inference is obtained as a mixture of all the meta–inferences for the cluster models, where the mixing distribution is the posterior model probability. We present a simple two–component form of Bayesian model averaging that is unaffected by characteristics such as small study size or zero–cell counts, and which is capable of incorporating uncertainties into the estimation process. Illustrative examples are given and analysed, using real sparse binomial data.},
  archive      = {J_JOAS},
  author       = {M. Martel and M. A. Negrín and F. J. Vázquez–Polo},
  doi          = {10.1080/02664763.2022.2084719},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {2760-2776},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian heterogeneity in a meta–analysis with two studies and binary data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint modelling of longitudinal measurements and survival
times via a multivariate copula approach. <em>JOAS</em>,
<em>50</em>(13), 2739–2759. (<a
href="https://doi.org/10.1080/02664763.2022.2081965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint modelling of longitudinal and time-to-event data is usually described by a joint model which uses shared or correlated latent effects to capture associations between the two processes. Under this framework, the joint distribution of the two processes can be derived straightforwardly by assuming conditional independence given the random effects. Alternative approaches to induce interdependency into sub-models have also been considered in the literature and one such approach is using copulas to introduce non-linear correlation between the marginal distributions of the longitudinal and time-to-event processes. The multivariate Gaussian copula joint model has been proposed in the literature to fit joint data by applying a Monte Carlo expectation-maximisation algorithm. In this paper, we propose an exact likelihood estimation approach to replace the more computationally expensive Monte Carlo expectation-maximisation algorithm and we consider results based on using both the multivariate Gaussian and t copula functions. We also provide a straightforward way to compute dynamic predictions of survival probabilities, showing that our proposed model is comparable in prediction performance to the shared random effects joint model.},
  archive      = {J_JOAS},
  author       = {Zili Zhang and Christiana Charalambous and Peter Foster},
  doi          = {10.1080/02664763.2022.2081965},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {2739-2759},
  shortjournal = {J. Appl. Stat.},
  title        = {Joint modelling of longitudinal measurements and survival times via a multivariate copula approach},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of medians under two-way model with and without
interaction for birnbaum–saunders distributed response. <em>JOAS</em>,
<em>50</em>(13), 2717–2738. (<a
href="https://doi.org/10.1080/02664763.2022.2078798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Birnbaum–Saunders (BS) distribution, well-known as the fatigue-life distribution, has been used in numerous disciplines ranging from engineering to medical sciences. In this article, we develop a test for analysis of medians for BS distributed response to assess the impact of two interacting factors on the median, where no test is presently available. The proposed integrated likelihood ratio test (ILRT) eliminates the nuisance shape parameters by integrating them out. The second-order accurate asymptotic chi-square distribution of ILRT is derived. An in-depth simulation study strongly supports its excellent performance even under small group sizes. Furthermore, ILRT developed under the one-way model is found uniformly superior over its peers, is straightway extendable under general multiway setup, and has potential to be extended to other non-normal response variables. Its genuine need in industry, where non-normal responses are commonly encountered, is highlighted through analysis of three real data sets: ILRT strongly picked out the deposition time as influential factor in epitaxial layer experiment, revealed significant impact of spools on fiber life for the failure times of Kevlar 49 fiber data, and gave more accurate parameter estimates in delivery time data experiment, as assessed by various model adequacy tools, where its competitors failed to deliver desired results.},
  archive      = {J_JOAS},
  author       = {S. M. Patil and H. V. Kulkarni},
  doi          = {10.1080/02664763.2022.2078798},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {2717-2738},
  shortjournal = {J. Appl. Stat.},
  title        = {Analysis of medians under two-way model with and without interaction for Birnbaum–Saunders distributed response},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian models for spatial count data with informative
finite populations with application to the american community survey.
<em>JOAS</em>, <em>50</em>(13), 2701–2716. (<a
href="https://doi.org/10.1080/02664763.2022.2078289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The American Community Survey (ACS) is an ongoing program conducted by the US Census Bureau that publishes estimates of important demographic statistics over pre-specified administrative areas. ACS provides spatially referenced count-valued outcomes that are paired with finite populations. For example, the number of people below the poverty line and the total population for each county are estimated by ACS. One common assumption is that the spatially referenced count-valued outcome given the finite population is binomial distributed. This conditionally specified (CS) model does not define the joint relationship between the count-valued outcome and the finite population. Thus, we consider a joint model for the count-valued outcome and the finite population. When cross-dependence in our joint model can be leveraged to ‘improve spatial prediction’ we say that the finite population is ‘informative.’ We model the count given the finite population as binomial and the finite population as negative binomial and use multivariate logit-beta prior distributions. This leads to closed-form expressions of the full-conditional distributions for an efficient Gibbs sampler. We illustrate our model through simulations and our motivating application of ACS poverty estimates. These empirical analyses show the benefits of using our proposed model over the more traditional CS binomial model.},
  archive      = {J_JOAS},
  author       = {Kai Qu and Jonathan R. Bradley},
  doi          = {10.1080/02664763.2022.2078289},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {2701-2716},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian models for spatial count data with informative finite populations with application to the american community survey},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A stationary weibull process and its applications.
<em>JOAS</em>, <em>50</em>(13), 2681–2700. (<a
href="https://doi.org/10.1080/02664763.2022.2073585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce a discrete-time and continuous state-space Markov stationary process { X n ; n = 1 , 2 , … } { X n ; n = 1 , 2 , … } {Xn;n=1,2,…} , where X n X n Xn has a two-parameter Weibull distribution, X n X n Xn &#39;s are dependent and there is a positive probability that X n = X n + 1 X n = X n + 1 Xn=Xn+1 . The motivation came from the gold price data where there are several instances for which X n = X n + 1 X n = X n + 1 Xn=Xn+1 . Hence, the existing methods cannot be used to analyze this data. We derive different properties of the proposed Weibull process. It is observed that the joint cumulative distribution function of X n X n Xn and X n + 1 X n + 1 Xn+1 has a very convenient copula structure. Hence, different dependence properties and dependence measures can be obtained. The maximum likelihood estimators cannot be obtained in explicit forms, we have proposed a simple profile likelihood method to compute these estimators. We have used this model to analyze two synthetic data sets and one gold price data set of the Indian market, and it is observed that the proposed model fits quite well with the data set.},
  archive      = {J_JOAS},
  author       = {Debasis Kundu},
  doi          = {10.1080/02664763.2022.2073585},
  journal      = {Journal of Applied Statistics},
  number       = {13},
  pages        = {2681-2700},
  shortjournal = {J. Appl. Stat.},
  title        = {A stationary weibull process and its applications},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical bayesian spatio-temporal modeling of COVID-19
in the united states. <em>JOAS</em>, <em>50</em>(11-12), 2663–2680. (<a
href="https://doi.org/10.1080/02664763.2022.2069232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the impact of economic, demographic, and mobility-related factors have had on the transmission of COVID-19 in 2020. While many models in the academic literature employ linear/generalized linear models, few contributions exist that incorporate spatial analysis, which is useful for understanding factors influencing the proliferation of the disease before the introduction of vaccines. We utilize a Poisson generalized linear model coupled with a spatial autoregressive structure to do so. Our analysis yields a number of insights including that, in some areas of the country, the counterintuitive result that staying at home can lead to increased disease proliferation. Additionally, we find some positive effects from increased gathering at grocery stores, negative effects of visiting retail stores and workplaces, and even small effects on visiting parks highlighting the complexities travel and migration have on the transmission of diseases.},
  archive      = {J_JOAS},
  author       = {Kevin D. Dayaratna and Drew Gonshorowski and Mary Kolesar},
  doi          = {10.1080/02664763.2022.2069232},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2663-2680},
  shortjournal = {J. Appl. Stat.},
  title        = {Hierarchical bayesian spatio-temporal modeling of COVID-19 in the united states},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust clustering of COVID-19 cases across u.s. Counties
using mixtures of asymmetric time series models with time varying and
freely indexed covariates. <em>JOAS</em>, <em>50</em>(11-12), 2648–2662.
(<a href="https://doi.org/10.1080/02664763.2021.2019688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a mixture of autoregressive (MoAR) process model with time varying and freely indexed covariates under the flexible class of two–piece distributions using the scale mixtures of normal (TP-SMN) family. This novel family of time series (TP-SMN-MoAR) models was used to examine flexible and robust clustering of reported cases of Covid-19 across 313 counties in the U.S. The TP-SMN distributions allow for symmetrical/ asymmetrical distributions as well as heavy-tailed distributions providing for flexibility to handle outliers and complex data. Developing a suitable hierarchical representation of the TP-SMN family enabled the construction of a pseudo-likelihood function to derive the maximum pseudo-likelihood estimates via an EM-type algorithm.},
  archive      = {J_JOAS},
  author       = {Mohsen Maleki and Hamid Bidram and Darren Wraith},
  doi          = {10.1080/02664763.2021.2019688},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2648-2662},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust clustering of COVID-19 cases across U.S. counties using mixtures of asymmetric time series models with time varying and freely indexed covariates},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification of factors impacting on the transmission and
mortality of COVID-19. <em>JOAS</em>, <em>50</em>(11-12), 2624–2647. (<a
href="https://doi.org/10.1080/02664763.2021.1953449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a dynamic infectious disease model for COVID-19 daily counts data and estimate the model using the Langevinized EnKF algorithm, which is scalable for large-scale spatio-temporal data, converges to the right filtering distribution, and is thus suitable for performing statistical inference and quantifying uncertainty for the underlying dynamic system. Under the framework of the proposed dynamic infectious disease model, we tested the impact of temperature, precipitation, state emergency order and stay home order on the spread of COVID-19 based on the United States county-wise daily counts data. Our numerical results show that warm and humid weather can significantly slow the spread of COVID-19, and the state emergency and stay home orders also help to slow it. This finding provides guidance and support to future policies or acts for mitigating the community transmission and lowering the mortality rate of COVID-19.},
  archive      = {J_JOAS},
  author       = {Peiyi Zhang and Tianning Dong and Ninghui Li and Faming Liang},
  doi          = {10.1080/02664763.2021.1953449},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2624-2647},
  shortjournal = {J. Appl. Stat.},
  title        = {Identification of factors impacting on the transmission and mortality of COVID-19},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exact inference for disease prevalence based on a test with
unknown specificity and sensitivity. <em>JOAS</em>, <em>50</em>(11-12),
2599–2623. (<a
href="https://doi.org/10.1080/02664763.2021.2019687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To make informative public policy decisions in battling the ongoing COVID-19 pandemic, it is important to know the disease prevalence in a population. There are two intertwined difficulties in estimating this prevalence based on testing results from a group of subjects. First, the test is prone to measurement error with unknown sensitivity and specificity. Second, the prevalence tends to be low at the initial stage of the pandemic and we may not be able to determine if a positive test result is a false positive due to the imperfect test specificity. The statistical inference based on a large sample approximation or conventional bootstrap may not be valid in such cases. In this paper, we have proposed a set of confidence intervals, whose validity doesn&#39;t depend on the sample size in the unweighted setting. For the weighted setting, the proposed inference is equivalent to hybrid bootstrap methods, whose performance is also more robust than those based on asymptotic approximations. The methods are used to reanalyze data from a study investigating the antibody prevalence in Santa Clara County, California in addition to several other seroprevalence studies. Simulation studies have been conducted to examine the finite-sample performance of the proposed method.},
  archive      = {J_JOAS},
  author       = {Bryan Cai and John P. A. Ioannidis and Eran Bendavid and Lu Tian},
  doi          = {10.1080/02664763.2021.2019687},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2599-2623},
  shortjournal = {J. Appl. Stat.},
  title        = {Exact inference for disease prevalence based on a test with unknown specificity and sensitivity},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent trait models for perceived risk assessment using a
covid-19 data survey. <em>JOAS</em>, <em>50</em>(11-12), 2575–2598. (<a
href="https://doi.org/10.1080/02664763.2021.1937584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aim of the contribution is analyzing potential events that may negatively impact individuals, assets, and/or the environment, and making judgments about the perceived personal and social riskiness of Covid-19 compared to other hazards belonging to health (AIDS, cancer, infarction), environmental (climate change), behavioral (serious car accidents), and technological (nuclear weapons) domains. The comparative risk analysis has been performed on a survey data collected during the first Italian Covid-19 lockdown. An item response theory model for polytomously scored items has been implemented for the analysis of the positioning of Covid-19 with respect to the other hazards in terms of perceived risk. Among the attributes determining the hazard&#39;s perceived risk, Covid-19 distinguishes for the knowledge of risks from the hazard, media attention, and fear caused by the hazard in the peers. Besides, through a latent regression analysis, the role of some individual characteristics on the perceived risk for Covid-19 has been examined. Our contribution allows us to disentangle among several aspects of hazards and describe the main factors affecting the perceived risk. It also contributes to determine if existing control measures are perceived as adequate and the interest for new media with related impact on a person&#39;s reaction.},
  archive      = {J_JOAS},
  author       = {S. Bacci and R. Fabbricatore and Maria Iannario},
  doi          = {10.1080/02664763.2021.1937584},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2575-2598},
  shortjournal = {J. Appl. Stat.},
  title        = {Latent trait models for perceived risk assessment using a covid-19 data survey},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting waved daily COVID-19 death count series with a
novel combination of segmented poisson model and ARIMA models.
<em>JOAS</em>, <em>50</em>(11-12), 2561–2574. (<a
href="https://doi.org/10.1080/02664763.2021.1976119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoregressive Integrated Moving Average (ARIMA) models have been widely used to forecast and model the development of various infectious diseases including COVID-19 outbreaks; however, such use of ARIMA models does not respect the count nature of the pandemic development data. For example, the daily COVID-19 death count series data for Canada and the United States (USA) are generally skewed with lots of low counts. In addition, there are generally waved patterns with turning points influenced by government major interventions against the spread of COVID-19 during different periods and seasons. In this study, we propose a novel combination of the segmented Poisson model and ARIMA models to handle these features and correlation structures in a two-stage process. The first stage of this process is a generalization of trend analysis of time series data. Our approach is illustrated with forecasting and modeling of daily COVID-19 death count series data for Canada and the USA.},
  archive      = {J_JOAS},
  author       = {Xiaolei Zhang and Renjun Ma},
  doi          = {10.1080/02664763.2021.1976119},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2561-2574},
  shortjournal = {J. Appl. Stat.},
  title        = {Forecasting waved daily COVID-19 death count series with a novel combination of segmented poisson model and ARIMA models},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven analysis of the simulations of the spread of
COVID-19 under different interventions of china. <em>JOAS</em>,
<em>50</em>(11-12), 2547–2560. (<a
href="https://doi.org/10.1080/02664763.2021.1895089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since February 2020, COVID-19 has spread rapidly to more than 200 countries in the world. During the pandemic, local governments in China have implemented different interventions to efficiently control the spread of the epidemic. Characterizing transmission of COVID-19 under some typical interventions is essential to help countries develop appropriate interventions. Based on the pre-symptomatic transmission patterns of COVID-19, we established a novel compartmental model: Susceptible-Infectious-Confirmed-Removed (SICR) model, which allowed the effective reproduction number to change over time, thus the effects of policies could be reasonably estimated. Using the epidemic data of Wuhan, Wenzhou, and Shenzhen, we migrated the corresponding estimated policy modes to South Korea, Italy, and the United States and simulated the potential outcomes for these countries when they adopted similar policy strategies to China. We found that the mild interventions implemented in Shenzhen were effective in controlling the epidemic in the early stage, while more stringent policies which were implemented in Wuhan and Wenzhou were necessary if the epidemic became severe and needed to be controlled in a short time.},
  archive      = {J_JOAS},
  author       = {Ting Tian and Jingwen Zhang and Shiyun Lin and Yukang Jiang and Jianbin Tan and Zhongfei Li and Xueqin Wang},
  doi          = {10.1080/02664763.2021.1895089},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2547-2560},
  shortjournal = {J. Appl. Stat.},
  title        = {Data-driven analysis of the simulations of the spread of COVID-19 under different interventions of china},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling the heterogeneity in COVID-19’s reproductive number
and its impact on predictive scenarios. <em>JOAS</em>,
<em>50</em>(11-12), 2518–2546. (<a
href="https://doi.org/10.1080/02664763.2021.1941806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correct evaluation of the reproductive number R for COVID-19 is central in the quantification of the potential scope of the pandemic and the selection of an appropriate course of action. In most models, R is modeled as a constant - effectively averaging out the inherent variability of the transmission process due to varying individual contact rates, population densities, or temporal factors amongst many. Yet, due to the exponential nature of epidemic growth, the error due to this simplification can be rapidly amplified, and its extent remains unknown. How can this intrinsic variability be percolated into epidemic models, and its impact, better quantified? We study this question here through a Bayesian perspective that captures at scale the heterogeneity of a population and environmental conditions, creating a bridge between the traditional agent-based and compartmental approaches. We use our model to simulate the spread as well as the impact of different social distancing strategies on real COVID-19 data, and highlight the significant impact of the heterogeneity. We emphasize that the contribution of this paper focuses on discussing the importance of the impact of R&#39;s heterogeneity on uncertainty quantification from a statistical viewpoint, rather than developing new predictive models.},
  archive      = {J_JOAS},
  author       = {Claire Donnat and Susan Holmes},
  doi          = {10.1080/02664763.2021.1941806},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2518-2546},
  shortjournal = {J. Appl. Stat.},
  title        = {Modeling the heterogeneity in COVID-19&#39;s reproductive number and its impact on predictive scenarios},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A continuous age-specific standardized mortality ratio for
estimating the unascertained rates in the early epidemic of COVID-19 in
different regions. <em>JOAS</em>, <em>50</em>(11-12), 2504–2517. (<a
href="https://doi.org/10.1080/02664763.2021.1947995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difference in age structure and aging population level was an important factor that caused the difference in COVID-19’s case fatality rate (CFR) in various regions. To eliminate the age effect on estimating the CFR of COVID-19, our study applied nonlinear logistic model and maximum likelihood method to fit the age-fatality curves of COVID-19 in different countries and regions. We further computed the standardized mortality ratio from the age-fatality curves of COVID-19 in the above regions and found that the risk of COVID-19 death in Wuhan was of a moderate level, while the non-Hubei region was even lower, compared with other regions. Regarding the disparity of CFRs among different regions in the country, we believed that there might be an unascertained phenomenon in high-endemic regions. Based on age-fatality rate curves, we estimated unascertained rates in cities with severe epidemics such as Wuhan and New York, and it was found that the total unascertained rates in Wuhan and New York were 81.6\% and 81.2\%, respectively. Meanwhile, we also found that the unascertained rates varied greatly with age.},
  archive      = {J_JOAS},
  author       = {Peipei Du and Peihua Cao and Xiaodong Yan and Daihai He and Xiaotong Zhang and Weixiang Chen and Jiawei Luo and Ziqian Zeng and Yaolong Chen and Lin Yang and Shu Yang and Xixi Feng},
  doi          = {10.1080/02664763.2021.1947995},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2504-2517},
  shortjournal = {J. Appl. Stat.},
  title        = {A continuous age-specific standardized mortality ratio for estimating the unascertained rates in the early epidemic of COVID-19 in different regions},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification and severity progression measure of COVID-19
patients using pairs of multi-omic factors. <em>JOAS</em>,
<em>50</em>(11-12), 2473–2503. (<a
href="https://doi.org/10.1080/02664763.2022.2064975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection and effective treatment of severe COVID-19 patients remain two major challenges during the current pandemic. Analysis of molecular changes in blood samples of severe patients is one of the promising approaches to this problem. From thousands of proteomic, metabolomic, lipidomic, and transcriptomic biomarkers selected in other research, we identify several pairs of biomarkers that after additional nonlinear spline transformation are highly effective in classifying and predicting severe COVID-19 cases. The performance of these pairs is evaluated in-sample, in a cross-validation exercise, and in an out-of-sample analysis on two independent datasets. We further improve our classifier by identifying complementary pairs using hierarchical clustering. In a result, we achieve 96–98\% AUC on the validation data. Our findings can help medical experts to identify small groups of biomarkers that after nonlinear transformation can be used to construct a cost-effective test for patient screening and prediction of severity progression.},
  archive      = {J_JOAS},
  author       = {Teng Chen and Paweł Polak and Stanislav Uryasev},
  doi          = {10.1080/02664763.2022.2064975},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2473-2503},
  shortjournal = {J. Appl. Stat.},
  title        = {Classification and severity progression measure of COVID-19 patients using pairs of multi-omic factors},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A latent space model and hotelling’s t2 control chart to
monitor the networks of covid-19 symptoms. <em>JOAS</em>,
<em>50</em>(11-12), 2450–2472. (<a
href="https://doi.org/10.1080/02664763.2022.2145459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the COVID-19 coronavirus pandemic, potential patients that suffer from different symptoms can be diagnosed with COVID-19. At the early stages of the pandemic, patients were mainly diagnosed with fever and respiratory symptoms. Recently, patients with new symptoms, such as gastrointestinal or loss of senses, are also diagnosed with COVID-19. Monitoring these symptoms can help the healthcare system to be aware of new symptoms that can be related to the COVID-19 coronavirus. This article focuses on monitoring the behavior of COVID-19 symptoms over time. In this regard, a Latent space model (LSM) and a Generalized linear model (GLM) are introduced to model the networks of symptoms. We apply Hotelling&#39;s T2 control chart to the estimated parameters of the LSM and GLM, to identify significant changes and detect anomalies in the networks. The performance of the proposed methods is evaluated using simulation and calculating average run length (ARL). Then, dynamic networks are generated from a COVID-19 epidemic survey dataset.},
  archive      = {J_JOAS},
  author       = {Fatemeh Elhambakhsh and Kamyar Sabri-Laghaie and Rassoul Noorossana},
  doi          = {10.1080/02664763.2022.2145459},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2450-2472},
  shortjournal = {J. Appl. Stat.},
  title        = {A latent space model and hotelling&#39;s t2 control chart to monitor the networks of covid-19 symptoms},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On evolution model for SARS-cov-2-infected population: The
case of new zealand. <em>JOAS</em>, <em>50</em>(11-12), 2435–2449. (<a
href="https://doi.org/10.1080/02664763.2021.2006153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The work proposes a mathematical model of the process of COVID-19 epidemic as it evolved in New Zealand. The model uses a system of differential equations which emanate from natural assumptions on some probability measure and evolution of this measure on evolving family of simplexes. The authors tried to create the model which, at one hand, is simple and easy to follow. and, at the other hand, reflects the observed epidemic process correctly. The practical aim was to come to justifiable estimations of important parameters like the rate of infection as function of time, thus quantifying effectiveness of the Government measures. Another parameters estimated were the probability distribution of detection times and recovery times.},
  archive      = {J_JOAS},
  author       = {Estate Khmaladze and Giorgi Kvizhinadze},
  doi          = {10.1080/02664763.2021.2006153},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2435-2449},
  shortjournal = {J. Appl. Stat.},
  title        = {On evolution model for SARS-cov-2-infected population: The case of new zealand},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparing and integrating US COVID-19 data from multiple
sources with anomaly detection and repairing. <em>JOAS</em>,
<em>50</em>(11-12), 2408–2434. (<a
href="https://doi.org/10.1080/02664763.2021.1928016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few months, the outbreak of Coronavirus disease (COVID-19) has been expanding over the world. A reliable and accurate dataset of the cases is vital for scientists to conduct related research and policy-makers to make better decisions. We collect the United States COVID-19 daily reported data from four open sources: the New York Times, the COVID-19 Data Repository by Johns Hopkins University, the COVID Tracking Project at the Atlantic, and the USAFacts, then compare the similarities and differences among them. To obtain reliable data for further analysis, we first examine the cyclical pattern and the following anomalies, which frequently occur in the reported cases: (1) the order dependencies violation, (2) the point or period anomalies, and (3) the issue of reporting delay. To address these detected issues, we propose the corresponding repairing methods and procedures if corrections are necessary. In addition, we integrate the COVID-19 reported cases with the county-level auxiliary information of the local features from official sources, such as health infrastructure, demographic, socioeconomic, and environmental information, which are also essential for understanding the spread of the virus.},
  archive      = {J_JOAS},
  author       = {Guannan Wang and Zhiling Gu and Xinyi Li and Shan Yu and Myungjin Kim and Yueying Wang and Lei Gao and Li Wang},
  doi          = {10.1080/02664763.2021.1928016},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2408-2434},
  shortjournal = {J. Appl. Stat.},
  title        = {Comparing and integrating US COVID-19 data from multiple sources with anomaly detection and repairing},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hot spot identification method based on andrews curves: An
application on the COVID-19 crisis effects on caregiver distress in
neurocognitive disorder. <em>JOAS</em>, <em>50</em>(11-12), 2388–2407.
(<a href="https://doi.org/10.1080/02664763.2021.2022607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying and locating areas – hot spots – that present high concentration of observations in a high-dimensional data set is crucial in many data processing and analysis methods and techniques, since observations that belong to the same hot spot share information and behave in a similar way. A useful tool towards that aim is the reduction of the data dimensionality and the graphical representation of them. In the present paper, a new method to identify and locate hot spots is proposed, based on the Andrews curves. Simulations results demonstrate the performance of the proposed method, which is also applied to a high-dimensional data set, regarding caregiver distress related to symptoms of people with neurocognitive disorder and to the mental effects of the recent outbreak of the COVID-19 pandemic.},
  archive      = {J_JOAS},
  author       = {E. Skamnia and P. Economou and S. Bersimis and M. Frouda and A. Politis and P. Alexopoulos},
  doi          = {10.1080/02664763.2021.2022607},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2388-2407},
  shortjournal = {J. Appl. Stat.},
  title        = {Hot spot identification method based on andrews curves: An application on the COVID-19 crisis effects on caregiver distress in neurocognitive disorder},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time fused coefficient SIR model with application to
COVID-19 epidemic in the united states. <em>JOAS</em>,
<em>50</em>(11-12), 2373–2387. (<a
href="https://doi.org/10.1080/02664763.2021.1936467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Susceptible–Infected–Removal (SIR) model with time fused coefficients. In particular, our proposed model discovers the underlying time homogeneity pattern for the SIR model&#39;s transmission rate and removal rate via Bayesian shrinkage priors. MCMC sampling for the proposed method is facilitated by the nimble package in R. Extensive simulation studies are carried out to examine the empirical performance of the proposed methods. We further apply the proposed methodology to analyze different levels of COVID-19 data in the United States.},
  archive      = {J_JOAS},
  author       = {Hou-Cheng Yang and Yishu Xue and Yuqing Pan and Qingyang Liu and Guanyu Hu},
  doi          = {10.1080/02664763.2021.1936467},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2373-2387},
  shortjournal = {J. Appl. Stat.},
  title        = {Time fused coefficient SIR model with application to COVID-19 epidemic in the united states},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying the cycles in COVID-19 infection: The case of
turkey. <em>JOAS</em>, <em>50</em>(11-12), 2360–2372. (<a
href="https://doi.org/10.1080/02664763.2022.2028744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new coronavirus disease, called COVID-19, has spread extremely quickly to more than 200 countries since its detection in December 2019 in China. COVID-19 marks the return of a very old and familiar enemy. Throughout human history, disasters such as earthquakes, volcanic eruptions and even wars have not caused more human losses than lethal diseases, which are caused by viruses, bacteria and parasites. The first COVID-19 case was detected in Turkey on 12 March 2020 and researchers have since then attempted to examine periodicity in the number of daily new cases. One of the most curious questions in the pandemic process that affects the whole world is whether there will be a second wave. Such questions can be answered by examining any periodicities in the series of daily cases. Periodic series are frequently seen in many disciplines. An important method based on harmonic regression is the focus of the study. The main aim of this study is to identify the hidden periodic structure of the daily infected cases. Infected case of Turkey is analyzed by using periodogram-based methodology. Our results revealed that there are 4, 5 and 62 days cycles in the daily new cases of Turkey.},
  archive      = {J_JOAS},
  author       = {Yılmaz Akdi and Yunus Emre Karamanoğlu and Kamil Demirberk Ünlü and Cem Baş},
  doi          = {10.1080/02664763.2022.2028744},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2360-2372},
  shortjournal = {J. Appl. Stat.},
  title        = {Identifying the cycles in COVID-19 infection: The case of turkey},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On regime changes of COVID-19 outbreak. <em>JOAS</em>,
<em>50</em>(11-12), 2343–2359. (<a
href="https://doi.org/10.1080/02664763.2023.2177625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has had a very serious impact on societies and caused large-scale economic changes and death toll worldwide. The first cases were detected in China, but soon the virus spread quickly worldwide and the intensity of newly reported infections grew high during this initial period almost everywhere. Later, despite all imposed measures, the intensity shifted abruptly multiple times during the two-year period between 2020 and 2022 causing waves of too high infection rates in almost every part of the world. To target this problem, we assume the data heterogeneity as multiple consecutive regime changes. The research study includes the development of a model based on automatic regime change detection and their combination with the linear birth-death process for long-run data fits. The results are empirically verified on data for 38 countries and US states for the period from February 2020 to April 2022. Finally, the initial phase (conditions) properties of infection development are studied.},
  archive      = {J_JOAS},
  author       = {A. Tchorbadjieff and L. P. Tomov and V. Velev and G. Dezhov and V. Manev and P. Mayster},
  doi          = {10.1080/02664763.2023.2177625},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2343-2359},
  shortjournal = {J. Appl. Stat.},
  title        = {On regime changes of COVID-19 outbreak},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical modelling of COVID-19 pandemic development
applying branching processes. <em>JOAS</em>, <em>50</em>(11-12),
2330–2342. (<a
href="https://doi.org/10.1080/02664763.2021.2006154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a statistical model for COVID-19 infection dynamics is described, using only the observed daily statistics of infected individuals. For this purpose, two special classes of branching processes without or with an immigration component are considered. These models are intended to estimate the main parameter of the infection and to give a prediction of the mean value of the non-observed population of the infected individuals. This is a serious advantage in comparison with other more complicated models where the officially reported data are not sufficient for estimation of the model parameters. The model is applied for different regions in the world and the corresponding parameters of the infection dynamics are estimated.},
  archive      = {J_JOAS},
  author       = {D. Atanasov and Vessela Stoimenova and Nikolay M. Yanev},
  doi          = {10.1080/02664763.2021.2006154},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2330-2342},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical modelling of COVID-19 pandemic development applying branching processes},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spatio-temporal statistical model to analyze COVID-19
spread in the USA. <em>JOAS</em>, <em>50</em>(11-12), 2310–2329. (<a
href="https://doi.org/10.1080/02664763.2021.1970122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus pandemic has affected the whole world extensively and it is of immense importance to understand how the disease is spreading. In this work, we provide evidence of spatial dependence in the pandemic data and accordingly develop a new statistical technique that captures the spatio-temporal dependence pattern of the COVID-19 spread appropriately. The proposed model uses a separable Gaussian spatio-temporal process, in conjunction with an additive mean structure and a random error process. The model is implemented through a Bayesian framework, thereby providing a computational advantage over the classical way. We use state-level data from the United States of America in this study. We show that a quadratic trend pattern is most appropriate in this context. Interestingly, the population is found not to affect the numbers significantly, whereas the number of deaths in the previous week positively affects the spread of the disease. Residual diagnostics establish that the model is adequate enough to understand the spatio-temporal dependence pattern in the data. It is also shown to have superior predictive power than other spatial and temporal models. In fact, we show that the proposed approach can predict well for both short term (1 week) and long term (up to three months).},
  archive      = {J_JOAS},
  author       = {Siddharth Rawat and Soudeep Deb},
  doi          = {10.1080/02664763.2021.1970122},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2310-2329},
  shortjournal = {J. Appl. Stat.},
  title        = {A spatio-temporal statistical model to analyze COVID-19 spread in the USA},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Point process models for COVID-19 cases and deaths.
<em>JOAS</em>, <em>50</em>(11-12), 2294–2309. (<a
href="https://doi.org/10.1080/02664763.2021.1907839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of events distributed over time which can be quantified as point processes has attracted much interest over the years due to its wide range of applications. It has recently gained new relevance due to the COVID-19 case and death processes associated with SARS-CoV-2 that characterize the COVID-19 pandemic and are observed across different countries. It is of interest to study the behavior of these point processes and how they may be related to covariates such as mobility restrictions, gross domestic product per capita, and fraction of population of older age. As infections and deaths in a region are intrinsically events that arrive at random times, a point process approach is natural for this setting. We adopt techniques for conditional functional point processes that target point processes as responses with vector covariates as predictors, to study the interaction and optimal transport between case and death processes and doubling times conditional on covariates.},
  archive      = {J_JOAS},
  author       = {Álvaro Gajardo and Hans-Georg Müller},
  doi          = {10.1080/02664763.2021.1907839},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2294-2309},
  shortjournal = {J. Appl. Stat.},
  title        = {Point process models for COVID-19 cases and deaths},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial to the special issue: Statistical perspectives on
analytics for COVID-19 data. <em>JOAS</em>, <em>50</em>(11-12),
2287–2293. (<a
href="https://doi.org/10.1080/02664763.2023.2228597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {Arnold Stromberg and Jie Chen and Teresa Paula Costa Azinheira Oliveira and Yichuan Zhao and Ramin Moghaddass and Milan Stehlik},
  doi          = {10.1080/02664763.2023.2228597},
  journal      = {Journal of Applied Statistics},
  number       = {11-12},
  pages        = {2287-2293},
  shortjournal = {J. Appl. Stat.},
  title        = {Editorial to the special issue: Statistical perspectives on analytics for COVID-19 data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional data analysis of the relationship between
electricity consumption and climate change drivers. <em>JOAS</em>,
<em>50</em>(10), 2267–2285. (<a
href="https://doi.org/10.1080/02664763.2022.2108773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate change has become increasingly important in recent years. It is the outcome of the burning of fossil fuels that increased the concentration of atmospheric carbon dioxide (CO 2 2 2 ), over the last century. Mitigating the impacts of climate change requires a better understanding and assessment of the countries&#39; economic decisions on the amount of CO 2 2 2 emissions. This paper assesses the variability between the different countries in the trends of CO 2 emissions and electricity consumption from 1975 to 2014, while identifying clusters of countries of similar trends over time. The novel methodology applied in this paper enables us to assess long-debated issues in climate literature. The temporal dynamic effects of electricity consumption and economic growth on CO 2 emissions across countries are studied using functional data analysis (FDA) methods. The latter have proven to be useful tools for visualising similarities and differences in the non-linear trends of CO 2 emissions without forcing linear trends and stationary relationships which can be unrealistic and misleading. The results indicate the possibility of identifying changes in the trends of CO 2 emissions and electricity consumption for a wide range of heterogeneous countries over the study period. The findings also reveal that economic growth puts a strain on the environment, where many high-income countries are still away from attaining economic-energy sustainability.},
  archive      = {J_JOAS},
  author       = {A. Elayouty and H. Abou-Ali},
  doi          = {10.1080/02664763.2022.2108773},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2267-2285},
  shortjournal = {J. Appl. Stat.},
  title        = {Functional data analysis of the relationship between electricity consumption and climate change drivers},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). The relationship between moderate to vigorous physical
activity and metabolic syndrome: A bayesian measurement error approach.
<em>JOAS</em>, <em>50</em>(10), 2246–2266. (<a
href="https://doi.org/10.1080/02664763.2022.2073336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metabolic Syndrome (MetS) is a serious condition that can be an early warning sign of heart disease and Type 2 diabetes. MetS is characterized by having elevated levels of blood pressure, cholesterol, waist circumference, and fasting glucose. There are many articles in the literature exploring the relationship between physical activity and MetS, but most do not consider the measurement error in the physical activity measurements nor the correlations among the MetS risk factors. Furthermore, previous work has generally treated MetS as binary, rather than directly modeling the risk factors on their measured, continuous space. Using data from the National Health and Nutrition Examination Survey (NHANES), we explore the relationship between minutes of moderate to vigorous physical activity (MVPA) and MetS risk factors. We construct a measurement error model for the accelerometry data, and then model its relationship between MetS risk factors with nonlinear seemingly unrelated regressions, incorporating dependence among MetS risk factors. The novel features of this model give the medical research community a new way to understand relationships between MVPA and MetS. The results of this approach present the field with a different modeling perspective than previously taken and suggest future avenues of scientific discovery.},
  archive      = {J_JOAS},
  author       = {Daniel Ries and Alicia Carriquiry},
  doi          = {10.1080/02664763.2022.2073336},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2246-2266},
  shortjournal = {J. Appl. Stat.},
  title        = {The relationship between moderate to vigorous physical activity and metabolic syndrome: A bayesian measurement error approach},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The efficient design of nested group testing algorithms for
disease identification in clustered data. <em>JOAS</em>,
<em>50</em>(10), 2228–2245. (<a
href="https://doi.org/10.1080/02664763.2022.2071419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group testing study designs have been used since the 1940s to reduce screening costs for uncommon diseases; for rare diseases, all cases are identifiable with substantially fewer tests than the population size. Substantial research has identified efficient designs under this paradigm. However, little work has focused on the important problem of disease screening among clustered data, such as geographic heterogeneity in HIV prevalence. We evaluated designs where we first estimate disease prevalence and then apply efficient group testing algorithms using these estimates. Specifically, we evaluate prevalence using individual testing on a fixed-size subset of each cluster and use these prevalence estimates to choose group sizes that minimize the corresponding estimated average number of tests per subject. We compare designs where we estimate cluster-specific prevalences as well as a common prevalence across clusters, use different group testing algorithms, construct groups from individuals within and in different clusters, and consider misclassification. For diseases with low prevalence, our results suggest that accounting for clustering is unnecessary. However, for diseases with higher prevalence and sizeable between-cluster heterogeneity, accounting for clustering in study design and implementation improves efficiency. We consider the practical aspects of our design recommendations with two examples with strong clustering effects: (1) Identification of HIV carriers in the US population and (2) Laboratory screening of anti-cancer compounds using cell lines.},
  archive      = {J_JOAS},
  author       = {Ana F. Best and Yaakov Malinovsky and Paul S. Albert},
  doi          = {10.1080/02664763.2022.2071419},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2228-2245},
  shortjournal = {J. Appl. Stat.},
  title        = {The efficient design of nested group testing algorithms for disease identification in clustered data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The fraud loss for selecting the model complexity in fraud
detection. <em>JOAS</em>, <em>50</em>(10), 2209–2227. (<a
href="https://doi.org/10.1080/02664763.2022.2070137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical fraud detection consists in making a system that automatically selects a subset of all cases (insurance claims, financial transactions, etc.) that are the most interesting for further investigation. The reason why such a system is needed is that the total number of cases typically is much higher than one realistically could investigate manually and that fraud tends to be quite rare. Further, the investigator is typically limited to controlling a restricted number k of cases, due to limited resources. The most efficient manner of allocating these resources is then to try selecting the k cases with the highest probability of being fraudulent. The prediction model used for this purpose must normally be regularised to avoid overfitting and consequently bad prediction performance. A loss function, denoted the fraud loss, is proposed for selecting the model complexity via a tuning parameter. A simulation study is performed to find the optimal settings for validation. Further, the performance of the proposed procedure is compared to the most relevant competing procedure, based on the area under the receiver operating characteristic curve (AUC), in a set of simulations, as well as on a credit card default dataset. Choosing the complexity of the model by the fraud loss resulted in either comparable or better results in terms of the fraud loss than choosing it according to the AUC.},
  archive      = {J_JOAS},
  author       = {Simon Boge Brant and Ingrid Hobæk Haff},
  doi          = {10.1080/02664763.2022.2070137},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2209-2227},
  shortjournal = {J. Appl. Stat.},
  title        = {The fraud loss for selecting the model complexity in fraud detection},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hierarchical bayesian approach for modeling the evolution
of the 7-day moving average of the number of deaths by COVID-19.
<em>JOAS</em>, <em>50</em>(10), 2194–2208. (<a
href="https://doi.org/10.1080/02664763.2022.2070136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a hierarchical Bayesian approach for modeling the evolution of the 7-day moving average for the number of deaths due to COVID-19 in a country, state or city. The proposed approach is based on a Gaussian process regression model. The main advantage of this model is that it assumes that a nonlinear function f used for modeling the observed data is an unknown random parameter in opposite to usual approaches that set up f as being a known mathematical function. This assumption allows the development of a Bayesian approach with a Gaussian process prior over f . In order to estimate the parameters of interest, we develop an MCMC algorithm based on the Metropolis-within-Gibbs sampling algorithm. We also present a procedure for making predictions. The proposed method is illustrated in a case study, in which, we model the 7-day moving average for the number of deaths recorded in the state of São Paulo, Brazil. Results obtained show that the proposed method is very effective in modeling and predicting the values of the 7-day moving average.},
  archive      = {J_JOAS},
  author       = {E. F. Saraiva and L. Sauer and C. A. B. Pereira},
  doi          = {10.1080/02664763.2022.2070136},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2194-2208},
  shortjournal = {J. Appl. Stat.},
  title        = {A hierarchical bayesian approach for modeling the evolution of the 7-day moving average of the number of deaths by COVID-19},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling dragonfly population data with a bayesian bivariate
geometric mixed-effects model. <em>JOAS</em>, <em>50</em>(10),
2171–2193. (<a
href="https://doi.org/10.1080/02664763.2022.2068513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a generalized linear mixed model (GLMM) for bivariate count responses for statistically analyzing dragonfly population data from the Northern Netherlands. The populations of the threatened dragonfly species Aeshna viridis were counted in the years 2015–2018 at 17 different locations (ponds and ditches). Two different widely applied population size measures were used to quantify the population sizes, namely the number of found exoskeletons (‘exuviae’) and the number of spotted egg-laying females were counted. Since both measures (responses) led to many zero counts but also feature very large counts, our GLMM model builds on a zero-inflated bivariate geometric (ZIBGe) distribution, for which we show that it can be easily parameterized in terms of a correlation parameter and its two marginal medians. We model the medians with linear combinations of fixed (environmental covariates) and random (location-specific intercepts) effects. Modeling the medians yields a decreased sensitivity to overly large counts; in particular, in light of growing marginal zero inflation rates. Because of the relatively small sample size ( n = 114) we follow a Bayesian modeling approach and use Metropolis-Hastings Markov Chain Monte Carlo (MCMC) simulations for generating posterior samples.},
  archive      = {J_JOAS},
  author       = {Yulan B. van Oppen and Gabi Milder-Mulderij and Christophe Brochard and Rink Wiggers and Saskia de Vries and Wim P. Krijnen and Marco A. Grzegorczyk},
  doi          = {10.1080/02664763.2022.2068513},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2171-2193},
  shortjournal = {J. Appl. Stat.},
  title        = {Modeling dragonfly population data with a bayesian bivariate geometric mixed-effects model},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient estimation of a cox model when integrating the
subgroup incidence rate information. <em>JOAS</em>, <em>50</em>(10),
2151–2170. (<a
href="https://doi.org/10.1080/02664763.2022.2068512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incidence rates for diseases are widely used in the field of medical research because they lead to clear and simple physical and clinical interpretations. In this study, we propose an efficient estimation method that incorporates auxiliary subgroup information related to the incidence rate into the estimation of the Cox proportional hazard model. The results show that utilizing the incidence rate information improves the efficiency of the estimation of regression parameters based on the double empirical likelihood method compared to that for conventional models that do not incorporation such information. We show that estimators of regression parameters asymptotically follow a multivariate normal distribution with a variance-covariance matrix that can be consistently estimated. Simulation results indicate that the proposed estimators significantly increase efficiency. Finally, an example of the effects of type 2 diabetes on stroke is applied to demonstrate the proposed method.},
  archive      = {J_JOAS},
  author       = {Pei-Fang Su and Junjiang Zhong and Yi-Chia Liu and Tzu-Hsuan Lin and Huang-Tz Ou},
  doi          = {10.1080/02664763.2022.2068512},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2151-2170},
  shortjournal = {J. Appl. Stat.},
  title        = {Efficient estimation of a cox model when integrating the subgroup incidence rate information},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Some properties of stop-loss moments under biased sampling.
<em>JOAS</em>, <em>50</em>(10), 2127–2150. (<a
href="https://doi.org/10.1080/02664763.2022.2065468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stop-loss moments have generally been used as useful summary measures for analyzing the data which exceeds specific threshold levels. In many scientific studies, the investigator cannot record the sampling units with equal probability, and in such a scenario, the selected sample units appear with unequal probability, in other words with different weights, which leads to a biased or weighted sampling. In the present study, we examine the usefulness of stop-loss moments in biased sampling. The application of the weighted stop-loss moments in analyzing biased data has been investigated and compared using different empirical estimators through simulated and real data sets.},
  archive      = {J_JOAS},
  author       = {N Vipin and Indranil Ghosh and S. M. Sunoj},
  doi          = {10.1080/02664763.2022.2065468},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2127-2150},
  shortjournal = {J. Appl. Stat.},
  title        = {Some properties of stop-loss moments under biased sampling},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatiotemporal nonhomogeneous poisson model with a seasonal
component applied to the analysis of extreme rainfall. <em>JOAS</em>,
<em>50</em>(10), 2108–2126. (<a
href="https://doi.org/10.1080/02664763.2022.2064978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops an extension of spatiotemporal models that handle count data using nonhomogeneous Poisson processes. In this new proposal, we incorporate a seasonal cycle component in the definition of the intensity function to control possible effects produced by the occurrence of the event of interest in regular periods. The seasonal cycle can cause problems in estimating the shape parameter of the Weibull and generalized Goel intensity functions. This shape parameter serves to confront the research hypothesis that seeks to identify a trend in the occurrence rate of an event of interest. In the case of the Weibull intensity function, a value significantly equal to one of the shape parameters indicates a constant rate of occurrence, less than one indicates a decreasing rate, and greater than one indicates an increasing rate. In the case of the Goel intensity function, parameter values less than or equal to one indicate a decreasing occurrence rate, and values greater than one indicate the presence of a change point. We also built a spatial model using the Musa-Okumoto intensity function as an alternative to approximate counting processes for which there is a decreasing trend in the occurrence rate of the event of interest. We estimated the parameters of the proposed method from a Bayesian perspective. Finally, we fitted the proposed model and compared it with other approximations to analyze the frequency of extreme rainfall in the northern region of the states of Maranhão and Piauí in northeastern Brazil over ten years. Among the main results, we found that (1) the proposed method has proven superior in terms of fit and prediction performance than the other models, and (2) unlike other approximations, the proposed model does not detect changes in the rate of extreme rainfall occurrences.},
  archive      = {J_JOAS},
  author       = {Fidel Ernesto Castro Morales and Daniele Torres Rodrigues},
  doi          = {10.1080/02664763.2022.2064978},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2108-2126},
  shortjournal = {J. Appl. Stat.},
  title        = {Spatiotemporal nonhomogeneous poisson model with a seasonal component applied to the analysis of extreme rainfall},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A double generally weighted moving average control chart for
monitoring the process variability. <em>JOAS</em>, <em>50</em>(10),
2079–2107. (<a
href="https://doi.org/10.1080/02664763.2022.2064977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present article, a double generally weighted moving average (DGWMA) control chart based on a three-parameter logarithmic transformation is proposed for monitoring the process variability, namely the S 2 S 2 S2 -DGWMA chart. Monte-Carlo simulations are utilized in order to evaluate the run-length performance of the S 2 S 2 S2 -DGWMA chart. In addition, a detailed comparative study is conducted to compare the performance of the S 2 -DGWMA chart with several well-known memory-type control charts in the literature. The comparisons indicate that the proposed one is more efficient in detecting small shifts, while it is more sensitive in identifying upward shifts in the process variability. A real data example is given to present the implementation of the new S 2 -DGWMA chart.},
  archive      = {J_JOAS},
  author       = {Vasileios Alevizakos and Kashinath Chatterjee and Christos Koukouvinos and Angeliki Lappa},
  doi          = {10.1080/02664763.2022.2064977},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {2079-2107},
  shortjournal = {J. Appl. Stat.},
  title        = {A double generally weighted moving average control chart for monitoring the process variability},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of phase i clinical trial designs for
combinational agents along with guidance based on simulation studies.
<em>JOAS</em>, <em>50</em>(9), 2055–2078. (<a
href="https://doi.org/10.1080/02664763.2022.2105827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinational therapy that combines two or more therapeutic agents is very common in cancer treatment. Currently, many clinical trials aim to assess feasibility, safety and activity of combinational therapeutics to achieve synergistic response. Dose-finding for combinational agents is considerably more complex than single agent, because only partial order of dose combinations&#39; toxicity is known. Prototypical phase I designs may not adequately capture this complexity thus limiting identification of the maximum tolerated dose (MTD) of combinational agents. In response, novel phase I clinical trial designs for combinational agents have been extensively proposed. However, with so many available designs, studies that compare their performances and explore the impact of design parameters, along with providing recommendations are limited. We are evaluating available phase I designs that identify a single MTD for combinational agents using simulation studies under various conditions. We are also exploring the influences of different design parameters and summarizing the risks/benefits of each design to provide general guidance in design selection.},
  archive      = {J_JOAS},
  author       = {Shu Wang and Elias Sayour and Ji-Hyun Lee},
  doi          = {10.1080/02664763.2022.2105827},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2055-2078},
  shortjournal = {J. Appl. Stat.},
  title        = {Evaluation of phase i clinical trial designs for combinational agents along with guidance based on simulation studies},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatio-temporal forecasting using wavelet transform-based
decision trees with application to air quality and covid-19 forecasting.
<em>JOAS</em>, <em>50</em>(9), 2036–2054. (<a
href="https://doi.org/10.1080/02664763.2022.2064976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new method that combines a decision tree with a wavelet transform to forecast time series data with spatial spillover effects. The method can not only improve prediction but also give good interpretability of the time series mechanism. As a feature exploration method, the wavelet transform represents information at different resolution levels, which may improve the performance of decision trees. The method is applied to simulated data, air pollution and COVID time series data sets. In the simulation, Haar, LA8, D4 and D6 wavelets are compared, with the Haar wavelet having the best performance. In the air pollution application, by using wavelet transform-based decision trees, the temporal effect of air quality index including autoregressive and seasonal effects can be described as well as the spatial correlation effect. To describe the spillover spatial effect in contiguous regions, a spatial weight is constructed to improve the modeling performance. The results show that air quality index has autoregressive, seasonal and spatial spillover effects. The wavelet transformed variables have a better forecasting performance and enhanced interpretability than the original variables. For the COVID time series of cumulative cases, spatial weighted variables are not selected which shows the lock-down policies are truly effective.},
  archive      = {J_JOAS},
  author       = {Xin Zhao and Stuart Barber and Charles C Taylor and Xiaokai Nie and Wenqian Shen},
  doi          = {10.1080/02664763.2022.2064976},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2036-2054},
  shortjournal = {J. Appl. Stat.},
  title        = {Spatio-temporal forecasting using wavelet transform-based decision trees with application to air quality and covid-19 forecasting},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian negative binomial regression model with unobserved
covariates for predicting the frequency of north atlantic tropical
storms. <em>JOAS</em>, <em>50</em>(9), 2014–2035. (<a
href="https://doi.org/10.1080/02664763.2022.2063266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the annual frequency of tropical storms is of interest because it can provide basic information towards improved preparation against these storms. Sea surface temperatures (SSTs) averaged over the hurricane season can predict annual tropical cyclone activity well. But predictions need to be made before the hurricane season when the predictors are not yet observed. Several climate models issue forecasts of the SSTs, which can be used instead. Such models use the forecasts of SSTs as surrogates for the true SSTs. We develop a Bayesian negative binomial regression model, which makes a distinction between the true SSTs and their forecasts, both of which are included in the model. For prediction, the true SSTs may be regarded as unobserved predictors and sampled from their posterior predictive distribution. We also have a small fraction of missing data for the SST forecasts from the climate models. Thus, we propose a model that can simultaneously handle missing predictors and variable selection uncertainty. If the main goal is prediction, an interesting question is should we include predictors in the model that are missing at the time of prediction? We attempt to answer this question and demonstrate that our model can provide gains in prediction.},
  archive      = {J_JOAS},
  author       = {Xun Li and Joyee Ghosh and Gabriele Villarini},
  doi          = {10.1080/02664763.2022.2063266},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {2014-2035},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian negative binomial regression model with unobserved covariates for predicting the frequency of north atlantic tropical storms},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven choice of a model selection method in joinpoint
regression. <em>JOAS</em>, <em>50</em>(9), 1992–2013. (<a
href="https://doi.org/10.1080/02664763.2022.2063265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting the number of change points in segmented line regression is an important problem in trend analysis, and there have been various approaches proposed in the literature. We first study the empirical properties of several model selection procedures and propose a new method based on two Schwarz type criteria, a classical Bayes Information Criterion (BIC) and the one with a harsher penalty than BIC ( BIC 3 ). The proposed rule is designed to use the former when effect sizes are small and the latter when the effect sizes are large and employs the partial R 2 to determine the weight between BIC and BIC 3 . The proposed method is computationally much more efficient than the permutation test procedure that has been the default method of Joinpoint software developed for cancer trend analysis, and its satisfactory performance is observed in our simulation study. Simulations indicate that the proposed method performs well in keeping the probability of correct selection at least as large as that of BIC 3 , whose performance is comparable to that of the permutation test procedure, and improves BIC 3 when it performs worse than BIC . The proposed method is applied to the U.S. prostate cancer incidence and mortality rates.},
  archive      = {J_JOAS},
  author       = {Hyune-Ju Kim and Huann-Sheng Chen and Douglas Midthune and Bill Wheeler and Dennis W. Buckman and Donald Green and Jeffrey Byrne and Jun Luo and Eric J. Feuer},
  doi          = {10.1080/02664763.2022.2063265},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1992-2013},
  shortjournal = {J. Appl. Stat.},
  title        = {Data-driven choice of a model selection method in joinpoint regression},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On detecting the effect of exposure mixture. <em>JOAS</em>,
<em>50</em>(9), 1980–1991. (<a
href="https://doi.org/10.1080/02664763.2022.2061430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To study the effect of exposure mixture on the continuous health outcomes, one can use the linear model with a weighted sum of multiple standardized exposure variables as an index predictor and its coefficient for the overall effect. The unknown weights typically range between zero and one, indicating contributions of individual exposures to the overall effect. Because the weight parameters present only when the parameter for overall effect is non-zero, testing hypotheses on the overall effect can be challenging, especially when the number of exposure variables is above two. This paper presents a working model based approach to estimate the parameter for overall effect and to test specific hypotheses, including two tests for detecting the overall effect and one test for detecting unequal weights when the overall effect is evident. The statistics are computationally easy and one can apply existing statistical software to perform the analysis. A simulation study shows that the proposed estimators for the parameters of interest may have better finite sample performance than some other estimators.},
  archive      = {J_JOAS},
  author       = {Xinhua Liu and Zhezhen Jin},
  doi          = {10.1080/02664763.2022.2061430},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1980-1991},
  shortjournal = {J. Appl. Stat.},
  title        = {On detecting the effect of exposure mixture},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incomplete clustering analysis via multiple imputation.
<em>JOAS</em>, <em>50</em>(9), 1962–1979. (<a
href="https://doi.org/10.1080/02664763.2022.2060952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering analysis is a prevalent statistical method which divides populations into several subgroups of similar units. However, most existing clustering methods require complete data. One general method that addresses incomplete data is multiple imputation (MI) which avoids many limitations found in other single imputation-based methods and complete case analyses. Nevertheless, adopting MI framework to clustering analysis can be challenging since each imputed data might consist of a different number of clusters and there is not a unique parameter for clustering analysis. In response to this problem, we have developed MICA: Multiply Imputed Cluster Analysis. MICA is a framework for clustering incomplete data consisting of two clustering stages. We assess the properties of MICA and its superiority over other existing incomplete clustering strategies based on a simulation study under various data structures. In addition, we demonstrate the usage of MICA by applying it to the Youth Risk Behavior Surveillance System (YRBSS) 2019 data.},
  archive      = {J_JOAS},
  author       = {Jung Wun Lee and Ofer Harel},
  doi          = {10.1080/02664763.2022.2060952},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1962-1979},
  shortjournal = {J. Appl. Stat.},
  title        = {Incomplete clustering analysis via multiple imputation},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intrinsic spherical smoothing method based on generalized
bézier curves and sparsity inducing penalization. <em>JOAS</em>,
<em>50</em>(9), 1942–1961. (<a
href="https://doi.org/10.1080/02664763.2022.2054962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines an intrinsic penalized smoothing method on the 2-sphere. We propose a method based on the spherical Bézier curves obtained using a generalized de Casteljau algorithm to provide a degree-based regularity constraint to the spherical smoothing problem. A smooth Bézier curve is found by minimizing the least squares criterion under the regularization constraint. The de Casteljau algorithm constructs higher-order Bézier curves in a recursive manner using linear Bézier curves. We introduce a local penalization scheme based on a penalty function that regularizes the velocity differences in consecutive linear Bézier curves. The imposed penalty induces sparsity on the control points so that the proposed method determines the number of control points, or equivalently the order of the Bézier curve, in a data-adaptive way. An efficient Riemannian block coordinate descent algorithm is devised to implement the proposed method. Numerical studies based on real and simulated data are provided to illustrate the performance and properties of the proposed method. The results show that the penalized Bézier curve adapts well to local data trends without compromising overall smoothness.},
  archive      = {J_JOAS},
  author       = {Kwan-Young Bak and Jae-Kyung Shin and Ja-Yong Koo},
  doi          = {10.1080/02664763.2022.2054962},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1942-1961},
  shortjournal = {J. Appl. Stat.},
  title        = {Intrinsic spherical smoothing method based on generalized bézier curves and sparsity inducing penalization},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goodness-of-fit inference for the additive hazards
regression model with clustered current status data. <em>JOAS</em>,
<em>50</em>(9), 1921–1941. (<a
href="https://doi.org/10.1080/02664763.2022.2053950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered current status data are frequently encountered in biomedical research and other areas that require survival analysis. This paper proposes graphical and formal model assessment procedures to evaluate the goodness of fit of the additive hazards model to clustered current status data. The test statistics proposed are based on sums of martingale-based residuals. Relevant asymptotic properties are established, and empirical distributions of the test statistics can be simulated utilizing Gaussian multipliers. Extensive simulation studies confirmed that the proposed test procedures work well for practical scenarios. This proposed method applies when failure times within the same cluster are correlated, and in particular, when cluster sizes can be informative about intra-cluster correlations. The method is applied to analyze clustered current status data from a lung tumorigenicity study.},
  archive      = {J_JOAS},
  author       = {Yanqin Feng and Jie Wang and Yang Li},
  doi          = {10.1080/02664763.2022.2053950},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1921-1941},
  shortjournal = {J. Appl. Stat.},
  title        = {Goodness-of-fit inference for the additive hazards regression model with clustered current status data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LIC criterion for optimal subset selection in distributed
interval estimation. <em>JOAS</em>, <em>50</em>(9), 1900–1920. (<a
href="https://doi.org/10.1080/02664763.2022.2053949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed interval estimation in linear regression may be computationally infeasible in the presence of big data that are normally stored in different computer servers or in cloud. The existing challenge represents the results from the distributed estimation may still contain redundant information about the population characteristics of the data. To tackle this computing challenge, we develop an optimization procedure to select the best subset from the collection of data subsets, based on which we perform interval estimation in the context of linear regression. The procedure is derived based on minimizing the length of the final interval estimator and maximizing the information remained in the selected data subset, thus is named as the LIC criterion. Theoretical performance of the LIC criterion is studied in this paper together with a simulation study and real data analysis.},
  archive      = {J_JOAS},
  author       = {Guangbao Guo and Yue Sun and Guoqi Qian and Qian Wang},
  doi          = {10.1080/02664763.2022.2053949},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1900-1920},
  shortjournal = {J. Appl. Stat.},
  title        = {LIC criterion for optimal subset selection in distributed interval estimation},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A competing risk study of menarcheal age distribution based
on non-recall current status data. <em>JOAS</em>, <em>50</em>(9),
1877–1899. (<a
href="https://doi.org/10.1080/02664763.2022.2052821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many cross-sectional studies, the chances that an individual will be able to exactly recall the event are very low. The possibility of recalling the exact time as well as the cause of occurrence of an event usually decreases as the gap between event and monitoring time increases. This gives rise to non-recall current status data. In this article, an efficient approach to deal with such non-recall current status data is established in a competing risk set up. In the classical method, a nested Expectation–Maximization technique is worked out for the estimation purpose and the information matrix is evaluated using the missing information principle. In the Bayesian paradigm, point and interval estimates are obtained using the Gibbs sampling algorithm. A recent anthropometric study data containing the menarcheal status of girls and age at menarche is analyzed using the considered methodology.},
  archive      = {J_JOAS},
  author       = {C. P. Yadav and Sanjeev K. Tomer and M. S. Panwar},
  doi          = {10.1080/02664763.2022.2052821},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1877-1899},
  shortjournal = {J. Appl. Stat.},
  title        = {A competing risk study of menarcheal age distribution based on non-recall current status data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian markov chain monte carlo for reparameterized
stochastic volatility models using asian FX rates during covid-19.
<em>JOAS</em>, <em>50</em>(8), 1853–1875. (<a
href="https://doi.org/10.1080/02664763.2022.2064440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, reparameterization and student-t are applied to Stochastic Volatility (SV) model. We aim to reduce the amount of autocorrelation of the SV parameters and to introduce heavy-tailed model via the Bayesian computation of the Markov Chain Monte Carlo (MCMC) samplers. This research paper helps support better MCMC estimation of the SV model for volatile Asian FX series during Covid-19.},
  archive      = {J_JOAS},
  author       = {Wantanee Poonvoralak},
  doi          = {10.1080/02664763.2022.2064440},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1853-1875},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian markov chain monte carlo for reparameterized stochastic volatility models using asian FX rates during covid-19},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel outlier statistic in multivariate survival models
and its application to identify unusual under-five mortality
sub-districts in malawi. <em>JOAS</em>, <em>50</em>(8), 1836–1852. (<a
href="https://doi.org/10.1080/02664763.2022.2043255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although under-five mortality (U5M) rates have declined worldwide, many countries in sub-Saharan Africa still have much higher rates. Detection of subnational areas with unusually higher U5M rates could support targeted high impact child health interventions. We propose a novel group outlier detection statistic for identifying areas with extreme U5M rates under a multivariate survival data model. The performance of the proposed statistic was evaluated through a simulation study. We applied the proposed method to an analysis of child survival data in Malawi to identify sub-districts with unusually higher or lower U5M rates. The simulation study showed that the proposed outlier statistic can detect unusual high or low mortality groups with a high accuracy of at least 90\%, for datasets with at least 50 clusters of size 80 or more. In the application, at most 7 U5M outlier sub-districts were identified, based on the best fitting model as measured by the Akaike information criterion (AIC).},
  archive      = {J_JOAS},
  author       = {Tsirizani M. Kaombe and Samuel O. M. Manda},
  doi          = {10.1080/02664763.2022.2043255},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1836-1852},
  shortjournal = {J. Appl. Stat.},
  title        = {A novel outlier statistic in multivariate survival models and its application to identify unusual under-five mortality sub-districts in malawi},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multivariate spatiotemporal model for tracking COVID-19
incidence and death rates in socially vulnerable populations.
<em>JOAS</em>, <em>50</em>(8), 1812–1835. (<a
href="https://doi.org/10.1080/02664763.2022.2046713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have produced inconsistent findings regarding the association between community social vulnerability and COVID-19 incidence and death rates. This inconsistency may be due, in part, to the fact that these studies modeled cases and deaths separately, ignoring their inherent association and thus yielding imprecise estimates. To improve inferences, we develop a Bayesian multivariate negative binomial model for exploring joint spatial and temporal trends in COVID-19 infections and deaths. The model introduces smooth functions that capture long-term temporal trends, while maintaining enough flexibility to detect local outbreaks in areas with vulnerable populations. Using multivariate autoregressive priors, we jointly model COVID-19 cases and deaths over time, taking advantage of convenient conditional representations to improve posterior computation. As such, the proposed model provides a general framework for multivariate spatiotemporal modeling of counts and rates. We adopt a fully Bayesian approach and develop an efficient posterior Markov chain Monte Carlo algorithm that relies on easily sampled Gibbs steps. We use the model to examine incidence and death rates among counties with high and low social vulnerability in the state of Georgia, USA, from 15 March to 15 December 2020.},
  archive      = {J_JOAS},
  author       = {Brian Neelon and Chun-Che Wen and Sara E. Benjamin-Neelon},
  doi          = {10.1080/02664763.2022.2046713},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1812-1835},
  shortjournal = {J. Appl. Stat.},
  title        = {A multivariate spatiotemporal model for tracking COVID-19 incidence and death rates in socially vulnerable populations},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adjusted partial least squares regression framework to
utilize additional exposure information in environmental mixture data
analysis. <em>JOAS</em>, <em>50</em>(8), 1790–1811. (<a
href="https://doi.org/10.1080/02664763.2022.2043254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a large-scale environmental health population study that is composed of subprojects, often different fractions of participants out of the total enrolled have measures of specific outcomes. It’s conceptually reasonable to assume the association study would benefit from utilizing additional exposure information from those with a specific outcome not measured. Partial least squares regression is a practical approach to determine the exposure-outcome associations for mixture data. Like a typical regression approach, however, the partial least squares regression requires that each data observation must have both complete covariate and outcome for model fitting. In this paper, we propose novel adjustments to the general partial least squares regression to estimate and examine the association effects of individual environmental exposure to an outcome within a more complete context of the study population’s environmental mixture exposures. The proposed framework takes advantage of the bilinear model structure. It allows information from all participants, with or without the outcome values, to contribute to the model fitting and the assessment of association effects. Using this proposed framework, incorporation of additional information will lead to smaller root mean square errors in the estimation of association effects, and improve the ability to assess the significance of the effects.},
  archive      = {J_JOAS},
  author       = {Ruofei Du and Li Luo and Laurie G. Hudson and Sara Nozadi and Johnnye Lewis},
  doi          = {10.1080/02664763.2022.2043254},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1790-1811},
  shortjournal = {J. Appl. Stat.},
  title        = {An adjusted partial least squares regression framework to utilize additional exposure information in environmental mixture data analysis},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On diagnostic accuracy measure with cut-points criterion for
ordinal disease classification based on concordance and discordance.
<em>JOAS</em>, <em>50</em>(8), 1772–1789. (<a
href="https://doi.org/10.1080/02664763.2022.2041567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of a diagnostic test has always been essential in detecting disease staging. Many diagnostic tests of accuracy measures are used in binary diagnosis tests. Some measures apply to multi-stage diagnosis. Yet, there are limitations to the implementation, and the performance highly depends on the distribution of diagnostic outcomes. Another essential aspect of medical diagnostic testing using biomarkers is to find an optimal cut-point that categorizes a patient as diseased or healthy. This aspect was extended to the diseases with more than two stages. We propose a diagnostic accuracy measure and optimal cut-points selection (CD), using concordance and discordance for k-stages diseases. The CD measure uses the classification agreement and disagreement between tests outcomes and diseases stages. Simulations for power studies suggest that CD can detect the differences between the null and alternative hypotheses that other methods cannot for some scenarios. Simulation results indicate that using CD measures to select optimal cut-points can provide relatively high correct classification rates than the existing measures and more balanced accurate classification rates than the generalized Youden Index (GYI). An illustration is provided using the ANDI data to choose biomarkers for diagnosing Alzheimer&#39;s Disease (AD) and select optimal cut-points for the chosen biomarkers.},
  archive      = {J_JOAS},
  author       = {Jing Kersey and Hani Samawi and Jingjing Yin and Haresh Rochani and Xinyan Zhang},
  doi          = {10.1080/02664763.2022.2041567},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1772-1789},
  shortjournal = {J. Appl. Stat.},
  title        = {On diagnostic accuracy measure with cut-points criterion for ordinal disease classification based on concordance and discordance},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Logarithmic confidence estimation of a ratio of binomial
proportions for dependent populations. <em>JOAS</em>, <em>50</em>(8),
1750–1771. (<a
href="https://doi.org/10.1080/02664763.2022.2041566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the logarithmic interval estimation of a ratio of two binomial proportions in dependent samples. Previous studies suggest that the confidence intervals of the difference between two correlated proportions and their ratio typically do not possess closed-form solutions. Moreover, the computation process is complex and often based on a maximum likelihood estimator, which is a biased estimator of the ratio. We look at the data from two dependent samples and explore the general problem of estimating the ratio of two proportions. Each sample is obtained in the framework of direct binomial sampling. Our goal is to demonstrate that the normal approximation for the estimation of the ratio is reliable for the construction of a confidence interval. The main characteristics of confidence estimators will be investigated by a Monte Carlo simulation. We also provide recommendations for applying the asymptotic logarithmic interval. The estimations of the coverage probability, average width, standard deviation of interval width, and index H are presented as the criteria of our judgment. The simulation studies indicate that the proposed interval performs well based on the aforementioned criteria. Finally, the confidence intervals are illustrated with three real data examples.},
  archive      = {J_JOAS},
  author       = {Angkana Kokaew and Winai Bodhisuwan and Su-Fen Yang and Andrei Volodin},
  doi          = {10.1080/02664763.2022.2041566},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1750-1771},
  shortjournal = {J. Appl. Stat.},
  title        = {Logarithmic confidence estimation of a ratio of binomial proportions for dependent populations},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A statistical testing procedure for validating class labels.
<em>JOAS</em>, <em>50</em>(8), 1725–1749. (<a
href="https://doi.org/10.1080/02664763.2022.2038546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by an open problem of validating protein identities in label-free shotgun proteomics work-flows, we present a testing procedure to validate class (protein) labels using available measurements across N instances (peptides). More generally, we present a non-parametric solution to the problem of identifying instances that are deemed as outliers relative to the subset of instances assigned to the same class. The primary assumption is that measured distances between instances within the same class are stochastically smaller than measured distances between instances from different classes. We show that the overall type I error probability across all instances within a class can be controlled by some fixed value (say α ). We also demonstrate conditions where similar results on type II error probability hold. The theoretical results are supplemented by an extensive numerical study illustrating the applicability and viability of our method. Even with up to 25\% of instances initially mislabeled, our testing procedure maintains a high specificity and greatly reduces the proportion of mislabeled instances. The applicability and effectiveness of our testing procedure is further illustrated by a detailed example on a proteomics data set from children with sickle cell disease where five spike-in proteins acted as contrasting controls.},
  archive      = {J_JOAS},
  author       = {Melissa C. Key and Susanne Ragg and Benzion Boukai},
  doi          = {10.1080/02664763.2022.2038546},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1725-1749},
  shortjournal = {J. Appl. Stat.},
  title        = {A statistical testing procedure for validating class labels},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of the skew exponential power distribution to
ROC curves. <em>JOAS</em>, <em>50</em>(8), 1709–1724. (<a
href="https://doi.org/10.1080/02664763.2022.2037528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bi-Normal ROC model and corresponding metrics are commonly used in medical studies to evaluate the discriminatory ability of a biomarker. However, in practice, many clinical biomarkers tend to have skewed or other non-Normal distributions. And while the bi-Normal ROC model’s AUC tends to be unbiased in this setting, providing a reasonable measure of global performance, the corresponding decision thresholds tend to be biased. To correct this bias, we propose using an ROC model based on the skew exponential power (SEP) distribution, whose additional parameters can accommodate skewed, heavy tailed, or other non-Normal distributions. Additionally, the SEP distribution can be used to evaluate whether the bi-Normal model would be appropriate. The performance of these ROC models and the non-parametric approach are evaluated via a simulation study and applied to a real data set involving infections from Klebsiella pneumoniae. The SEP based ROC-model provides some efficiency gains with respect to estimation of the AUC and provides cut-points with improved classification rates. As such, in the presence non-Normal data, we suggest using the proposed SEP ROC model.},
  archive      = {J_JOAS},
  author       = {Kristopher Attwood and Surui Hou and Alan Hutson},
  doi          = {10.1080/02664763.2022.2037528},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1709-1724},
  shortjournal = {J. Appl. Stat.},
  title        = {Application of the skew exponential power distribution to ROC curves},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fusion learning method to subgroup analysis of alzheimer’s
disease. <em>JOAS</em>, <em>50</em>(8), 1686–1708. (<a
href="https://doi.org/10.1080/02664763.2022.2036953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncovering the heterogeneity in the disease progression of Alzheimer&#39;s is a key factor to disease understanding and treatment development, so that interventions can be tailored to target the subgroups that will benefit most from the treatment, which is an important goal of precision medicine. However, in practice, one top methodological challenge hindering the heterogeneity investigation is that the true subgroup membership of each individual is often unknown. In this article, we aim to identify latent subgroups of individuals who share a common disorder progress over time, to predict latent subgroup memberships, and to estimate and infer the heterogeneous trajectories among the subgroups. To achieve these goals, we apply a concave fusion learning method to conduct subgroup analysis for longitudinal trajectories of the Alzheimer&#39;s disease data. The heterogeneous trajectories are represented by subject-specific unknown functions which are approximated by B-splines. The concave fusion method can simultaneously estimate the spline coefficients and merge them together for the subjects belonging to the same subgroup to automatically identify subgroups and recover the heterogeneous trajectories. The resulting estimator of the disease trajectory of each subgroup is supported by an asymptotic distribution. It provides a sound theoretical basis for further conducting statistical inference in subgroup analysis.},
  archive      = {J_JOAS},
  author       = {Mingming Liu and Jing Yang and Yushi Liu and Bochao Jia and Yun-Fei Chen and Luna Sun and Shujie Ma},
  doi          = {10.1080/02664763.2022.2036953},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1686-1708},
  shortjournal = {J. Appl. Stat.},
  title        = {A fusion learning method to subgroup analysis of alzheimer&#39;s disease},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The re-parameterized inverse gaussian regression to model
length of stay of COVID-19 patients in the public health care system of
piracicaba, brazil. <em>JOAS</em>, <em>50</em>(8), 1665–1685. (<a
href="https://doi.org/10.1080/02664763.2022.2036707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the models applied to analyze survival data, a standout is the inverse Gaussian distribution, which belongs to the class of models to analyze positive asymmetric data. However, the variance of this distribution depends on two parameters, which prevents establishing a functional relation with a linear predictor when the assumption of constant variance does not hold. In this context, the aim of this paper is to re-parameterize the inverse Gaussian distribution to enable establishing an association between a linear predictor and the variance. We propose deviance residuals to verify the model assumptions. Some simulations indicate that the distribution of these residuals approaches the standard normal distribution and the mean squared errors of the estimators are small for large samples. Further, we fit the new model to hospitalization times of COVID-19 patients in Piracicaba (Brazil) which indicates that men spend more time hospitalized than women, and this pattern is more pronounced for individuals older than 60 years. The re-parameterized inverse Gaussian model proved to be a good alternative to analyze censored data with non-constant variance.},
  archive      = {J_JOAS},
  author       = {E. M. Hashimoto and E. M. M. Ortega and G. M. Cordeiro and V. G. Cancho and I. Silva},
  doi          = {10.1080/02664763.2022.2036707},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1665-1685},
  shortjournal = {J. Appl. Stat.},
  title        = {The re-parameterized inverse gaussian regression to model length of stay of COVID-19 patients in the public health care system of piracicaba, brazil},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beta-negative binomial nonlinear spatio-temporal random
effects modeling of COVID-19 case counts in japan. <em>JOAS</em>,
<em>50</em>(7), 1650–1663. (<a
href="https://doi.org/10.1080/02664763.2022.2064439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19) caused by the SARS-CoV-2 virus has spread seriously throughout the world. Predicting the spread, or the number of cases, in the future can facilitate preparation for, and prevention of, a worst-case scenario. To achieve these purposes, statistical modeling using past data is one feasible approach. This paper describes spatio-temporal modeling of COVID-19 case counts in 47 prefectures of Japan using a nonlinear random effects model, where random effects are introduced to capture the heterogeneity of a number of model parameters associated with the prefectures. The negative binomial distribution is frequently used with the Paul-Held random effects model to account for overdispersion in count data; however, the negative binomial distribution is known to be incapable of accommodating extreme observations such as those found in the COVID-19 case count data. We therefore propose use of the beta-negative binomial distribution with the Paul-Held model. This distribution is a generalization of the negative binomial distribution that has attracted much attention in recent years because it can model extreme observations with analytical tractability. The proposed beta-negative binomial model was applied to multivariate count time series data of COVID-19 cases in the 47 prefectures of Japan. Evaluation by one-step-ahead prediction showed that the proposed model can accommodate extreme observations without sacrificing predictive performance.},
  archive      = {J_JOAS},
  author       = {Masao Ueki},
  doi          = {10.1080/02664763.2022.2064439},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1650-1663},
  shortjournal = {J. Appl. Stat.},
  title        = {Beta-negative binomial nonlinear spatio-temporal random effects modeling of COVID-19 case counts in japan},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Income and democracy: A bivariate copula approach.
<em>JOAS</em>, <em>50</em>(7), 1635–1649. (<a
href="https://doi.org/10.1080/02664763.2022.2055749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach for exploring the relationship between income and democracy by modeling the two most popular discrete democracy indexes, Polity IV and Freedom House, as a joint random variable by means of a copula function. Joint modeling is crucial for eliciting complementarity and/or substitutability amongst these indexes claiming to measure similar things, i.e. a country’s degree of democratization. We find strong evidence supporting both the existence of the relationship and the positive dependence between the two democracy indexes, suggesting that they are complements to each other. Our findings are robust to different samples and model specifications.},
  archive      = {J_JOAS},
  author       = {Suzanna-Maria Paleologou},
  doi          = {10.1080/02664763.2022.2055749},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1635-1649},
  shortjournal = {J. Appl. Stat.},
  title        = {Income and democracy: A bivariate copula approach},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensitivity analysis of error-contaminated time series data
under autoregressive models with the application of COVID-19 data.
<em>JOAS</em>, <em>50</em>(7), 1611–1634. (<a
href="https://doi.org/10.1080/02664763.2022.2034760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoregressive (AR) models are useful in time series analysis. Inferences under such models are distorted in the presence of measurement error, a common feature in applications. In this article, we establish analytical results for quantifying the biases of the parameter estimation in AR models if the measurement error effects are neglected. We consider two measurement error models to describe different data contamination scenarios. We propose an estimating equation approach to estimate the AR model parameters with measurement error effects accounted for. We further discuss forecasting using the proposed method. Our work is inspired by COVID-19 data, which are error-contaminated due to multiple reasons including those related to asymptomatic cases and varying incubation periods. We implement the proposed method by conducting sensitivity analyses and forecasting the fatality rate of COVID-19 over time for the four most populated provinces in Canada. The results suggest that incorporating or not incorporating measurement error effects may yield rather different results for parameter estimation and forecasting.},
  archive      = {J_JOAS},
  author       = {Qihuang Zhang and Grace Y. Yi},
  doi          = {10.1080/02664763.2022.2034760},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1611-1634},
  shortjournal = {J. Appl. Stat.},
  title        = {Sensitivity analysis of error-contaminated time series data under autoregressive models with the application of COVID-19 data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Confidence intervals for ratios of means applied to
corpus-based word frequency classes. <em>JOAS</em>, <em>50</em>(7),
1592–1610. (<a
href="https://doi.org/10.1080/02664763.2022.2034759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The words we choose when we communicate with one another convey meaning and information. In written or spoken language, we tend to employ a relatively small number of words repeatedly whereas a large number of words in the lexicon are seldom used. By considering a ratio of means of the most prevalent word in a body of texts (or corpus) compared to that of the word in question, one can quantify the prevalence of the word in question. Furthermore, the concept of word classes or grouping words having similar measures of prevalence enables the investigator to compare the words. Using a sample of texts having varying lengths from a corpus, the sample mean relative frequency of a word and the maximum likelihood estimator using the zero-inflated beta distribution serve as two measures of the prevalence of a word. We construct and then compare asymptotic confidence intervals involving ratios of means for a number of words in the British National Corpus, a 100 million-word collection of written and spoken language of a wide range of British English. We also examine the sample sizes required to meet specific objectives regarding word classes and ratios of means.},
  archive      = {J_JOAS},
  author       = {Brent Burch and Jesse Egbert},
  doi          = {10.1080/02664763.2022.2034759},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1592-1610},
  shortjournal = {J. Appl. Stat.},
  title        = {Confidence intervals for ratios of means applied to corpus-based word frequency classes},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast inference for robust nonlinear mixed-effects models.
<em>JOAS</em>, <em>50</em>(7), 1568–1591. (<a
href="https://doi.org/10.1080/02664763.2022.2034141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interest for nonlinear mixed-effects models comes from application areas as pharmacokinetics, growth curves and HIV viral dynamics. However, the modeling procedure usually leads to many difficulties, as the inclusion of random effects, the estimation process and the model sensitivity to atypical or nonnormal data. The scale mixture of normal distributions include heavy-tailed models, as the Student- t , slash and contaminated normal distributions, and provide competitive alternatives to the usual models, enabling the obtention of robust estimates against outlying observations. Our proposal is to compare two estimation methods in nonlinear mixed-effects models where the random components follow a multivariate scale mixture of normal distributions. For this purpose, a Monte Carlo expectation-maximization algorithm (MCEM) and an efficient likelihood-based approximate method are developed. Results show that the approximate method is much faster and enables a fairly efficient likelihood maximization, although a slightly larger bias may be produced, especially for the fixed-effects parameters. A discussion on the robustness aspects of the proposed models are also provided. Two real nonlinear applications are discussed and a brief simulation study is presented.},
  archive      = {J_JOAS},
  author       = {José Clelto Barros Gomes and Reiko Aoki and Victor Hugo Lachos and Gilberto Alvarenga Paula and Cibele Maria Russo},
  doi          = {10.1080/02664763.2022.2034141},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1568-1591},
  shortjournal = {J. Appl. Stat.},
  title        = {Fast inference for robust nonlinear mixed-effects models},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference of multicomponent stress-strength reliability
following topp-leone distribution using progressively censored data.
<em>JOAS</em>, <em>50</em>(7), 1538–1567. (<a
href="https://doi.org/10.1080/02664763.2022.2032621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the inference of multicomponent stress-strength reliability has been derived using progressively censored samples from Topp-Leone distribution. Both stress and strength variables are assumed to follow Topp-Leone distributions with different shape parameters. The maximum likelihood estimate along with the asymptotic confidence interval are developed. Boot-p and Boot-t confidence intervals are also constructed. The Bayes estimates under generalized entropy loss function based on gamma priors using Lindley&#39;s, Tierney-Kadane&#39;s approximation and Markov chain Monte Carlo methods are derived. A simulation study is considered to check the performance of various estimation methods and different censoring schemes. A real data study shows the applicability of the proposed estimation methods.},
  archive      = {J_JOAS},
  author       = {Shubham Saini and Sachin Tomer and Renu Garg},
  doi          = {10.1080/02664763.2022.2032621},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1538-1567},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference of multicomponent stress-strength reliability following topp-leone distribution using progressively censored data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable targeting and reduction in large vector
autoregressions with applications to workforce indicators.
<em>JOAS</em>, <em>50</em>(7), 1515–1537. (<a
href="https://doi.org/10.1080/02664763.2022.2032619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop statistical tools for time series analysis of large multivariate datasets, when a few core series are of principal interest and there are many potential auxiliary predictive variables. The methodology, based on Vector Autoregressions (VAR), handles the case where unrestricted fitting is precluded by a large number of series and a huge parameter space. In particular, we adopt a forecast error criterion and use Granger-causality tests in a sequential manner to build a VAR model that targets the main variables. This approach affects variable reduction (or equivalently, sparsity restrictions) in a computationally fast way that remains feasible for large dimensions. The search for the best model results in a VAR, fitted with a selection of supporting series, that has the best possible forecast performance with respect to the core variables. We apply the statistical methodology to model real Gross Domestic Product and the national Unemployment Rate, two time series widely monitored by economists and policy-makers, based on a large set of Quarterly Workforce Indicators comprising various major sectors of the economy and different measures of labor market conditions.},
  archive      = {J_JOAS},
  author       = {T. S. McElroy and Thomas Trimbur},
  doi          = {10.1080/02664763.2022.2032619},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1515-1537},
  shortjournal = {J. Appl. Stat.},
  title        = {Variable targeting and reduction in large vector autoregressions with applications to workforce indicators},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The kendall interaction filter for variable interaction
screening in high dimensional classification problems. <em>JOAS</em>,
<em>50</em>(7), 1496–1514. (<a
href="https://doi.org/10.1080/02664763.2022.2031125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accounting for important interaction effects can improve the prediction of many statistical learning models. Identification of relevant interactions, however, is a challenging issue owing to their ultrahigh-dimensional nature. Interaction screening strategies can alleviate such issues. However, due to heavier tail distribution and complex dependence structure of interaction effects, innovative robust and/or model-free methods for screening interactions are required to better scale analysis of complex and high-throughput data. In this work, we develop a new model-free interaction screening method, termed Kendall Interaction Filter (KIF), for the classification in high-dimensional settings. KIF method suggests a weighted-sum measure, which compares the overall to the within-cluster Kendall&#39;s τ of pairs of predictors, to select interactive couples of features. The proposed KIF measure captures relevant interactions for the clusters response-variable, handles continuous, categorical or a mixture of continuous-categorical features, and is invariant under monotonic transformations. The tKIF measure enjoys the sure screening property in the high-dimensional setting under mild conditions, without imposing sub-exponential moment assumptions on the features&#39; distribution. We illustrate the favorable behavior of the proposed methodology compared to the methods in the same category using simulation studies, and we conduct real data analyses to demonstrate its utility.},
  archive      = {J_JOAS},
  author       = {Youssef Anzarmou and Abdallah Mkhadri and Karim Oualkacha},
  doi          = {10.1080/02664763.2022.2031125},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1496-1514},
  shortjournal = {J. Appl. Stat.},
  title        = {The kendall interaction filter for variable interaction screening in high dimensional classification problems},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust latent CUSUM chart for monitoring customer
attrition. <em>JOAS</em>, <em>50</em>(7), 1477–1495. (<a
href="https://doi.org/10.1080/02664763.2022.2031123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In competitive business, such as insurance and telecommunications, customers can easily replace one provider for another, which leads to customer attrition. Keeping customer attrition rate low is crucial for companies, since retaining a customer is more profitable than recruiting a new one. As a main statistical process control (SPC) method, the CUSUM scheme is able to detect small and persistent shifts in customer attrition. However, customer attrition summaries are typically available on an uneven time scale (e.g. 4-week and 5-week ‘business month’), which may not satisfy the assumptions of traditional CUSUM designs. This paper mainly develops a latent CUSUM chart based on an exponential model for monitoring ‘monthly’ customer attrition, under varying time scales. Both maximum likelihood and least squares methods are studied, where the former mostly performs better and the latter is advantageous for quite small shifts. We apply a Markov chain algorithm to obtain the average run length (ARL), make calibrations for different combinations of parameters, and present reference tables of cutoffs. Three more complicated models are considered to test the robustness of deviations from the initial model. Furthermore, a real example of monitoring monthly customer attrition from a Chinese insurance company is used to illustrate the scheme.},
  archive      = {J_JOAS},
  author       = {Chunjie Wu and Zhijun Wang and Steven MacEachern and Jingjing Schneider},
  doi          = {10.1080/02664763.2022.2031123},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1477-1495},
  shortjournal = {J. Appl. Stat.},
  title        = {A robust latent CUSUM chart for monitoring customer attrition},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting the analysis pipeline for overdispersed poisson
and binomial data. <em>JOAS</em>, <em>50</em>(7), 1455–1476. (<a
href="https://doi.org/10.1080/02664763.2022.2026897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overdispersion is a common feature in categorical data analysis and several methods have been developed for detecting and handling it in generalized linear models. The first aim of this study is to clarify the relationships among various score statistics for testing overdispersion and to compare their performances. In addition, we investigate a principled way to correct finite sample bias in the score statistic caused by estimating regression parameters with restricted likelihood. The second aim is to reconsider the current practice for handling overdispersed categorical data. Although the conventional models are based on substantially different mechanisms for generating overdispersion, model selection in practice has not been well studied. We perform an intensive numerical study for determining which method is more robust to various overdispersion mechanisms. In addition, we provide some graphical tools for identifying the better model. The last aim is to reconsider the key assumption for deriving the score statistics. We study the meaning of testing overdispersion when this assumption is violated, and we analytically show the conditions for which it is not appropriate to employ the current statistical practices for analyzing overdispersed data.},
  archive      = {J_JOAS},
  author       = {Woojoo Lee and Jeonghwan Kim and Donghwan Lee},
  doi          = {10.1080/02664763.2022.2026897},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1455-1476},
  shortjournal = {J. Appl. Stat.},
  title        = {Revisiting the analysis pipeline for overdispersed poisson and binomial data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spot it! And balanced block designs: Keys to better debate
architecture for a plethora of candidates in presidential primaries?
<em>JOAS</em>, <em>50</em>(6), 1435–1454. (<a
href="https://doi.org/10.1080/02664763.2022.2041568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {U.S. presidential primary debates are influential but under-researched. Before 2015, all of these debates, both Democratic and Republican, had 10 candidates or fewer. The first Republican debate in 2015, however, abided 17 candidates. They were split into two segments, with the 10 best-polling candidates in the main (prime-time) segment and the others in an ‘undercard’ session. A comparable pattern applied for the next six Republican debates. Concern arose not only because many candidates were crowded into a session but also because the undercard candidates were seen as receiving inferior exposure. The Democratic presidential primary debates that started four years later encountered similar difficulty. An authorized policy caused their candidates in each of the first two debates to be limited to 20, randomly divided into two groups of 10 appearing on successive nights. For remedy, this paper examines innovative debate plans, for different numbers of candidates, that feature symmetry among all candidates and entail many short segments with relatively few candidates in each. We apply combinatorial designs—balanced incomplete block designs and regular pairwise balanced designs, which are analogous to the games Spot It Jr.! Animals and (full-fledged) Spot It! , respectively.},
  archive      = {J_JOAS},
  author       = {Richard F. Potthoff},
  doi          = {10.1080/02664763.2022.2041568},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1435-1454},
  shortjournal = {J. Appl. Stat.},
  title        = {Spot it! and balanced block designs: Keys to better debate architecture for a plethora of candidates in presidential primaries?},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On summary ROC curve for dichotomous diagnostic studies: An
application to meta-analysis of COVID-19. <em>JOAS</em>, <em>50</em>(6),
1418–1434. (<a
href="https://doi.org/10.1080/02664763.2022.2041565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a systematic review of a diagnostic performance, summarizing performance metrics is crucial. There are various summary models in the literature, and hence model selection becomes inevitable. However, most existing large-sample-based model selection approaches may not fit in a meta-analysis of diagnostic studies, typically having a rather small sample size. Researchers need to effectively determine the final model for further inference, which motivates this article to investigate existing methods and to suggest a more robust method for this need. We considered models covering several widely-used methods for bivariate summary of sensitivity and specificity. Simulation studies were conducted based on different number of studies and different population sensitivity and specificity. Then final models were selected using several existing criteria, and we compared the summary receiver operating characteristic (sROC) curves to the theoretical ROC curve given the generating model. Even though parametric likelihood-based criteria are often applied in practice for their asymptotic property, they fail to consistently choose appropriate models under the limited number of studies. When the number of studies is as small as 10 or 5, our suggestion is best in different scenarios. An example for summary ROC curves for chemiluminescence immunoassay (CLIA) used in COVID-19 diagnosis is also illustrated.},
  archive      = {J_JOAS},
  author       = {ShengLi Tzeng and Chun-Shu Chen and Yu-Fen Li and Jin-Hua Chen},
  doi          = {10.1080/02664763.2022.2041565},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1418-1434},
  shortjournal = {J. Appl. Stat.},
  title        = {On summary ROC curve for dichotomous diagnostic studies: An application to meta-analysis of COVID-19},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction models with graph kernel regularization for
network data. <em>JOAS</em>, <em>50</em>(6), 1400–1417. (<a
href="https://doi.org/10.1080/02664763.2022.2028745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional regression methods typically consider only covariate information and assume that the observations are mutually independent samples. However, samples usually come from individuals connected by a network in many modern applications. We present a risk minimization formulation for learning from both covariates and network structure in the context of graph kernel regularization. The formulation involves a loss function with a penalty term. This penalty can be used not only to encourage similarity between linked nodes but also lead to improvement over traditional regression models. Furthermore, the penalty can be used with many loss-based predictive methods, such as linear regression with squared loss and logistic regression with log-likelihood loss. Simulations to evaluate the performance of this model in the cases of low dimensions and high dimensions show that our proposed approach outperforms all other benchmarks. We verify this for uniform graph, nonuniform graph, balanced-sample, and unbalanced-sample datasets. The approach was applied to predicting the response values on a ‘follow’ social network of Tencent Weibo users and on two citation networks (Cora and CiteSeer). Each instance verifies that the proposed method combining covariate information and link structure with the graph kernel regularization can improve predictive performance.},
  archive      = {J_JOAS},
  author       = {Jie Liu and Haojie Chen and Yang Yang},
  doi          = {10.1080/02664763.2022.2028745},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1400-1417},
  shortjournal = {J. Appl. Stat.},
  title        = {Prediction models with graph kernel regularization for network data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse regression for low-dimensional time-dynamic varying
coefficient models with application to air quality data. <em>JOAS</em>,
<em>50</em>(6), 1378–1399. (<a
href="https://doi.org/10.1080/02664763.2022.2028131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time dynamic varying coefficient models play an important role in applications of biology, medicine, environment, finance, etc. Traditional methods, such as kernel smoothing and spline smoothing, are popular. But explicit expressions are unavailable using these methods, and the convergence rate of coefficient function estimators is slow. To address these problems, we expand the varying component with appropriate basis functions. And then we solve a sparse regression problem via a sequential thresholded least-squares estimator. The “parameterization” leads to explicit expressions and fast computation speed. Convergence of the sequential thresholded least squares algorithm is guaranteed. The asymptotic distribution of the coefficient function estimator is derived under certain assumptions. Our simulation shows the proposed method has higher precision and computing speed. Finally, our proposed method is applied to the study of PM 2.5 concentration in Beijing. We analyze the relationship between PM 2.5 and other impact factors.},
  archive      = {J_JOAS},
  author       = {Jinwen Liang and Maozai Tian},
  doi          = {10.1080/02664763.2022.2028131},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1378-1399},
  shortjournal = {J. Appl. Stat.},
  title        = {Sparse regression for low-dimensional time-dynamic varying coefficient models with application to air quality data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compound truncated poisson gamma distribution for
understanding multimodal SAR intensities. <em>JOAS</em>, <em>50</em>(6),
1358–1377. (<a
href="https://doi.org/10.1080/02664763.2022.2028130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many works have addressed the proposal of new probability models by theoretic and applied reasons. Specifically, mixture models have been indicated to describe phenomena whose resulting data impose high flexibility. One drawback of these tools is the high number of parameters involved, which implies hard inference procedures. To outperform this gap, we propose a new model that is able to describe multimodal behaviors with only three parameters, called compound truncated Poisson gamma (CTrPGa) distribution. Some properties of the CTrPGa law are derived and discussed: hazard, characteristic and cumulative functions and ordinary moments. Beyond, moment estimation, maximum likelihood estimation (via the expectation maximization algorithm) and empirical characteristic function methods for CTrPGa parameters are furnished. The first of them may be reduced to solve one nonlinear equation, which facilitates its use. We perform a simulation analysis to compare the performance of the three estimation methods studied. Moreover, since the gamma distribution and its mixture versions are commonly used to characterize synthetic aperture radar (SAR) intensities, we perform some real experiments with SAR imagery. The results present evidence that our model is a reasonable assumption that can be taken into account in the pre-processing step of such images.},
  archive      = {J_JOAS},
  author       = {A. D. C. Nascimento and Leandro C. Rêgo and Jonas W. A. Silva},
  doi          = {10.1080/02664763.2022.2028130},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1358-1377},
  shortjournal = {J. Appl. Stat.},
  title        = {Compound truncated poisson gamma distribution for understanding multimodal SAR intensities},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tests and classification methods in adaptive designs with
applications. <em>JOAS</em>, <em>50</em>(6), 1334–1357. (<a
href="https://doi.org/10.1080/02664763.2022.2026898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical tests for biomarker identification and classification methods for patient grouping are two important topics in adaptive designs of clinical trials related to genomic studies. In this article, we evaluate four test methods for biomarker identification in the first stage of an adaptive design: a model-based identification method, the popular two-sided t -test, the nonparametric Wilcoxon Rank-Sum test (two-sided), and the Regularized Generalized Linear Models. For patients grouping in the second stage, we examine classification methods such as Random Forest, Elastic-net Regularized Generalized Linear Models, Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Extreme Gradient Boosting (XGBoost). Simulation studies are carried out to assess the performance of the different methods. The best identification methods are chosen based on the well-known F 1 score, while the best classification techniques are selected based on the area under a receiver operating characteristic curve (AUC). The chosen methods are then applied to the Adaptive Signature Design (ASD) with a real data set from breast cancer patients for the purpose of evaluating the performance of ASD in different situations.},
  archive      = {J_JOAS},
  author       = {Diana Q. Chen and Si-Qi Mao and Xu-Feng Niu},
  doi          = {10.1080/02664763.2022.2026898},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1334-1357},
  shortjournal = {J. Appl. Stat.},
  title        = {Tests and classification methods in adaptive designs with applications},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian hierarchical models for the prediction of the
driver flow and passenger waiting times in a stochastic carpooling
service. <em>JOAS</em>, <em>50</em>(6), 1310–1333. (<a
href="https://doi.org/10.1080/02664763.2022.2026896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carpooling is an integral component in smart carbon-neutral cities, in particular to facilitate home-work commuting. We study an innovative carpooling service which offers stochastic passenger-driver matching. Stochastic matching is when a passenger makes a carpooling request, and then waits for the first driver from a population of drivers who are already en route. Crucially a designated driver is not assigned as in a traditional carpooling service. For this new form of stochastic carpooling, we propose a two-stage Bayesian hierarchical model to predict the driver flow and the passenger waiting times. The first stage focuses on prediction of the aggregated daily driver flows, and the second stage processes these daily driver flow into hourly predictions of the passenger waiting times. We demonstrate, for an operational carpooling service, that the predictions from our Bayesian hierarchical model outperform the predictions from a frequentist model and a Bayesian non-hierarchical model. The inferences from our proposed model provide insights for the service operator in their evidence-based decision making.},
  archive      = {J_JOAS},
  author       = {Panayotis Papoutsis and Tarn Duong and Bertrand Michel and Anne Philippe},
  doi          = {10.1080/02664763.2022.2026896},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1310-1333},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian hierarchical models for the prediction of the driver flow and passenger waiting times in a stochastic carpooling service},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local linear regression and the problem of dimensionality: A
remedial strategy via a new locally adaptive bandwidths selector.
<em>JOAS</em>, <em>50</em>(6), 1283–1309. (<a
href="https://doi.org/10.1080/02664763.2022.2026895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local Linear Regression (LLR) is a nonparametric regression model applied in the modeling phase of Response Surface Methodology (RSM). LLR does not make reference to any fixed parametric model. Hence, LLR is flexible and can capture local trends in the data that might be too complicated for the OLS. However, besides the small sample size and sparse data which characterizes RSM, the performance of the LLR model nosedives as the number of explanatory variables considered in the study increases. This phenomenon, popularly referred to as curse of dimensionality, results in the scanty application of LLR in RSM. In this paper, we propose a novel locally adaptive bandwidths selector, unlike the fixed bandwidths and existing locally adaptive bandwidths selectors, takes into account both the number of the explanatory variables in the study and their individual values at each data point. Single and multiple response problems from the literature and simulated data were used to compare the performance of the L L R P A B with those of the OLS, L L R F B and L L R A B . Neural network activation functions such ReLU, Leaky-ReLU, SELU and SPOCU was considered and give a remarkable improvement on the loss function (Mean Squared Error) over the regression models utilized in the three data.},
  archive      = {J_JOAS},
  author       = {O. Eguasa and E Edionwe and J. I. Mbegbu},
  doi          = {10.1080/02664763.2022.2026895},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1283-1309},
  shortjournal = {J. Appl. Stat.},
  title        = {Local linear regression and the problem of dimensionality: A remedial strategy via a new locally adaptive bandwidths selector},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heteroscedastic partially linear model under skew-normal
distribution with application in ragweed pollen concentration.
<em>JOAS</em>, <em>50</em>(6), 1255–1282. (<a
href="https://doi.org/10.1080/02664763.2021.2024798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new class of heteroscedastic partially linear model (PLM) with skew-normal distribution. Maximum likelihood estimation of the model parameters by the ECM algorithm (Expectation/Conditional Maximization) as well as influence diagnostics for the new model are investigated. In addition, a Likelihood Ratio test for assessing the homogeneity of the scale parameter is presented. Simulation studies for assessing the performance of the ECM algorithm and the Likelihood Ratio test statistics for homogeneity of variance are developed. Also, a study for misspecification of the structure function is considered. Finally, an application of the new heteroscedastic PLM to a real data set on ragweed pollen concentration is presented to show that it provides a better fit than the classic homocedastic PLM. We hope that the proposed model may attract applications in different areas of knowledge.},
  archive      = {J_JOAS},
  author       = {Clécio S. Ferreira and Camila Borelli Zeller and Rafael R. de Oliveira Garcia},
  doi          = {10.1080/02664763.2021.2024798},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1255-1282},
  shortjournal = {J. Appl. Stat.},
  title        = {Heteroscedastic partially linear model under skew-normal distribution with application in ragweed pollen concentration},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference for a discretized stochastic logistic differential
equation and its application to biological growth. <em>JOAS</em>,
<em>50</em>(6), 1231–1254. (<a
href="https://doi.org/10.1080/02664763.2021.2024154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a method to adjust a stochastic logistic differential equation (SLDE) to a set of highly sparse real data. We assume that the SLDE have two unknown parameters to be estimated. We calculate the Maximum Likelihood Estimator (MLE) to estimate the intrinsic growth rate. We prove that the MLE is strongly consistent and asymptotically normal. For estimating the diffusion parameter, the quadratic variation of the data is used. We validate our method with several types of simulated data. For more realistic cases in which we observe discretizations of the solution, we use diffusion bridges and the stochastic expectation-maximization algorithm to estimate the parameters. Furthermore, when we observe only one point for each path for a given number of trajectories we were still able to estimate the parameters of the SLDE. As far as we know, this is the first attempt to fit stochastic differential equations (SDEs) to these types of data. Finally, we apply our method to real data coming from fishery. The proposed adjustment method can be applied to other examples of SDEs and is highly applicable in several areas of science, especially in situations of sparse data.},
  archive      = {J_JOAS},
  author       = {F. Delgado-Vences and F. Baltazar-Larios and A. Ornelas Vargas and E. Morales-Bojórquez and V. H. Cruz-Escalona and C. Salomón Aguilar},
  doi          = {10.1080/02664763.2021.2024154},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1231-1254},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference for a discretized stochastic logistic differential equation and its application to biological growth},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully nonparametric survival analysis in the presence of
time-dependent covariates and dependent censoring. <em>JOAS</em>,
<em>50</em>(5), 1215–1229. (<a
href="https://doi.org/10.1080/02664763.2022.2031128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the presence of informative right censoring and time-dependent covariates, we estimate the survival function in a fully nonparametric fashion. We introduce a novel method for incorporating multiple observations per subject when estimating the survival function at different covariate values and compare several competing methods via simulation. The proposed method is applied to survival data from people awaiting liver transplant.},
  archive      = {J_JOAS},
  author       = {David M. Ruth and Nicholas L. Wood and Douglas N. VanDerwerken},
  doi          = {10.1080/02664763.2022.2031128},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1215-1229},
  shortjournal = {J. Appl. Stat.},
  title        = {Fully nonparametric survival analysis in the presence of time-dependent covariates and dependent censoring},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A random effect regression based on the odd log-logistic
generalized inverse gaussian distribution. <em>JOAS</em>,
<em>50</em>(5), 1199–1214. (<a
href="https://doi.org/10.1080/02664763.2021.2024515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, the use of regression models with random effects has made great progress. Among these models&#39; attractions is the flexibility to analyze correlated data. In various situations, the distribution of the response variable presents asymmetry or bimodality. In these cases, it is possible to use the normal regression with random effect at the intercept. In light of these contexts, i.e. the desire to analyze correlated data in the presence of bimodality or asymmetry, in this paper we propose a regression model with random effect at the intercept based onthe generalized inverse Gaussian distribution model with correlated data. The maximum likelihood is adopted to estimate the parameters and various simulations are performed for correlated data. A type of residuals for the new regression is proposed whose empirical distribution is close to normal. The versatility of the new regression is demonstrated by estimating the average price per hectare of bare land in 10 municipalities in the state of São Paulo (Brazil). In this context, various databases are constantly emerging, requiring flexible modeling. Thus, it is likely to be of interest to data analysts, and can make a good contribution to the statistical literature.},
  archive      = {J_JOAS},
  author       = {J. C. S. Vasconcelos and G. M. Cordeiro and E. M. M. Ortega and G. O. Silva},
  doi          = {10.1080/02664763.2021.2024515},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1199-1214},
  shortjournal = {J. Appl. Stat.},
  title        = {A random effect regression based on the odd log-logistic generalized inverse gaussian distribution},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based joint curve registration and classification.
<em>JOAS</em>, <em>50</em>(5), 1178–1198. (<a
href="https://doi.org/10.1080/02664763.2021.2023118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of classification of misaligned multivariate functional data. We propose to use a model-based approach for the joint registration and classification of such data. The observed functional inputs are modeled as a functional nonlinear mixed effects model containing a nonlinear functional fixed effect constructed upon warping functions to account for curve alignment, and a nonlinear functional random effects component to address the variability among subjects. The warping functions are also modeled to accommodate common effect within groups and the variability between subjects. Then, a functional logistic regression model defined upon the representation of the aligned curves and scalar inputs is used to account for curve classification. EM-based algorithms are developed to perform maximum likelihood inference of the proposed models. The identifiability of the registration model and the asymptotical properties of the proposed method are established. The performance of the proposed procedure is illustrated via simulation studies and an analysis of a hyoid bone movement data application. The statistical developments proposed in this paper were motivated by the hyoid bone movement study, the methodology is designed and presented generality and can be applied to numerous areas of scientific research.},
  archive      = {J_JOAS},
  author       = {Lin Tang and Pengcheng Zeng and Jian Qing Shi and Won-Seok Kim},
  doi          = {10.1080/02664763.2021.2023118},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1178-1198},
  shortjournal = {J. Appl. Stat.},
  title        = {Model-based joint curve registration and classification},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Log-mean distribution: Applications to medical data,
survival regression, bayesian and non-bayesian discussion with MCMC
algorithm. <em>JOAS</em>, <em>50</em>(5), 1152–1177. (<a
href="https://doi.org/10.1080/02664763.2021.2023117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new family via the log mean of an underlying distribution and as baseline the proportional hazards model and derive some important properties. A special model is proposed by taking the Weibull for the baseline. We derive several properties of the sub-model such as moments, order statistics, hazard function, survival regression and certain characterization results. We estimate the parameters using frequentist and Bayesian approaches. Further, Bayes estimators, posterior risks, credible intervals and highest posterior density intervals are obtained under different symmetric and asymmetric loss functions. A Monte Carlo simulation study examines the biases and mean square errors of the maximum likelihood estimators. For the illustrative purposes, we consider heart transplant and bladder cancer data sets and investigate the efficiency of proposed model.},
  archive      = {J_JOAS},
  author       = {O. Kharazmi and G. G. Hamedani and G. M. Cordeiro},
  doi          = {10.1080/02664763.2021.2023117},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1152-1177},
  shortjournal = {J. Appl. Stat.},
  title        = {Log-mean distribution: Applications to medical data, survival regression, bayesian and non-bayesian discussion with MCMC algorithm},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The integrated nested laplace approximation applied to
spatial log-gaussian cox process models. <em>JOAS</em>, <em>50</em>(5),
1128–1151. (<a
href="https://doi.org/10.1080/02664763.2021.2023116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial point process models are theoretically useful for mapping discrete events, such as plant or animal presence, across space; however, the computational complexity of fitting these models is often a barrier to their practical use. The log-Gaussian Cox process (LGCP) is a point process driven by a latent Gaussian field, and recent advances have made it possible to fit Bayesian LGCP models using approximate methods that facilitate rapid computation. These advances include the integrated nested Laplace approximation (INLA) with a stochastic partial differential equations (SPDE) approach to sparsely approximate the Gaussian field and an extension using pseudodata with a Poisson response. To help link the theoretical results to statistical practice, we provide an overview of INLA for point process data and then illustrate their implementation using freely available data. The analyzed datasets include both a completely observed spatial field and an incomplete data situation. Our well-commented R code is shared in the online supplement. Our intent is to make these methods accessible to the practitioner of spatial statistics without requiring deep knowledge of point process theory.},
  archive      = {J_JOAS},
  author       = {Kenneth Flagg and Andrew Hoegh},
  doi          = {10.1080/02664763.2021.2023116},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1128-1151},
  shortjournal = {J. Appl. Stat.},
  title        = {The integrated nested laplace approximation applied to spatial log-gaussian cox process models},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personalized treatment selection using observational data.
<em>JOAS</em>, <em>50</em>(5), 1115–1127. (<a
href="https://doi.org/10.1080/02664763.2021.2019689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the optimal treatment regime based on individual patient characteristics has been a topic of discussion in many forums. Advanced computational power has added momentum to this discussion over the last two decades and practitioners have been advocating the use of new methods in determining the best treatment. Treatments that are geared toward the ‘best’ outcome for a patient based on his/her genetic markers and characteristics are of high importance. In this article, we develop an approach to predict the optimal personalized treatment based on observational data. We have used inverse probability of treatment weighted machine learning methods to obtain score functions to predict the optimal treatment. Extensive simulation studies showed that our proposed method has desirable performance in selecting the optimal treatment. We provided a case study to examine the Statin use on cognitive function to illustrate the use of our proposed method.},
  archive      = {J_JOAS},
  author       = {K. B. Kulasekera and Sudaraka Tholkage and Maiying Kong},
  doi          = {10.1080/02664763.2021.2019689},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1115-1127},
  shortjournal = {J. Appl. Stat.},
  title        = {Personalized treatment selection using observational data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized fiducial methods for testing the homogeneity of
a three-sample problem with a mixture structure. <em>JOAS</em>,
<em>50</em>(5), 1094–1114. (<a
href="https://doi.org/10.1080/02664763.2021.2017414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the likelihood ratio (LR) test was proposed to test the homogeneity of a three-sample model with a mixture structure. Because of the presence of the mixture structure, the null limiting distribution of the LR test has a complicated supremum form, which leads to challenges in determining p -values. In addition, the LR test cannot control type-I errors well under small to moderate sample size. In this paper, we propose seven generalized fiducial methods to test the homogeneity of the three-sample model. Via simulation studies, we find that our methods perform significantly better than the LR test method in controlling the type-I errors under small to moderate sample size, while they have comparable powers in most cases. A halibut data example is used to illustrate the proposed methods.},
  archive      = {J_JOAS},
  author       = {Pengcheng Ren and Guanfu Liu and Xiaolong Pu},
  doi          = {10.1080/02664763.2021.2017414},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1094-1114},
  shortjournal = {J. Appl. Stat.},
  title        = {Generalized fiducial methods for testing the homogeneity of a three-sample problem with a mixture structure},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pearson’s goodness-of-fit tests for sparse distributions.
<em>JOAS</em>, <em>50</em>(5), 1078–1093. (<a
href="https://doi.org/10.1080/02664763.2021.2017413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pearson&#39;s chi-squared test is widely used to test the goodness of fit between categorical data and a given discrete distribution function. When the number of sets of the categorical data, say k , is a fixed integer, Pearson&#39;s chi-squared test statistic converges in distribution to a chi-squared distribution with k −1 degrees of freedom when the sample size n goes to infinity. In real applications, the number k often changes with n and may be even much larger than n . By using the martingale techniques, we prove that Pearson&#39;s chi-squared test statistic converges to the normal under quite general conditions. We also propose a new test statistic which is more powerful than chi-squared test statistic based on our simulation study. A real application to lottery data is provided to illustrate our methodology.},
  archive      = {J_JOAS},
  author       = {Shuhua Chang and Deli Li and Yongcheng Qi},
  doi          = {10.1080/02664763.2021.2017413},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1078-1093},
  shortjournal = {J. Appl. Stat.},
  title        = {Pearson&#39;s goodness-of-fit tests for sparse distributions},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Homogeneity test of relative risk ratios for stratified
bilateral data under different algorithms. <em>JOAS</em>,
<em>50</em>(5), 1060–1077. (<a
href="https://doi.org/10.1080/02664763.2021.2017412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical clinical studies about paired body parts often involve stratified bilateral data. The correlation between responses from paired parts should be taken into account to avoid biased or misleading results. This paper aims to test if the relative risk ratios across strata are equal under the optimal algorithms. Based on different algorithms, we obtain the desired global and constrained maximum likelihood estimations (MLEs). Three asymptotic test statistics (i.e. T L , T S C and T W ) are proposed. Monte Carlo simulations are conducted to evaluate the performance of these algorithms with respect to mean square errors of MLEs and convergence rate. The empirical results show Fisher scoring algorithm is usually better than other methods since it has effective convergence rate for global MLEs, and makes mean-square error lower for constrained MLEs. Three test statistics are compared in terms of type I error rate (TIE) and power. Among these statistics, T S C is recommended according to its robust TIEs and satisfactory power.},
  archive      = {J_JOAS},
  author       = {Ke-Yi Mou and Chang-Xing Ma and Zhi-Ming Li},
  doi          = {10.1080/02664763.2021.2017412},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1060-1077},
  shortjournal = {J. Appl. Stat.},
  title        = {Homogeneity test of relative risk ratios for stratified bilateral data under different algorithms},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ultrastructural calibration model for proficiency testing.
<em>JOAS</em>, <em>50</em>(5), 1037–1059. (<a
href="https://doi.org/10.1080/02664763.2021.2012563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proficiency testing (PT) determines the performance of individual laboratories for specific tests or measurements and it is used to monitor the reliability of laboratories measurements. PT plays a highly valuable role as it provides objective evidence of the competence of the participant laboratories. In this paper, we propose a multivariate calibration model to assess equivalence among laboratories measurements in PT. Our method allows to deal with multivariate data, where the item under test is measured at different levels. Although intuitive, the proposed model is nonergodic, which means that the asymptotic Fisher information matrix is random. As a consequence, a detailed asymptotic analysis was carried out to establish the strategy for comparing the results of the participating laboratories. To illustrate, we apply our method to analyze the data from the Brazilian engine test group, PT program, where the power of an engine was measured by eight laboratories at several levels of rotation.},
  archive      = {J_JOAS},
  author       = {Reiko Aoki and Dorival Leão and Juan P. Mamani Bustamante and Filidor Vilca},
  doi          = {10.1080/02664763.2021.2012563},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {1037-1059},
  shortjournal = {J. Appl. Stat.},
  title        = {Ultrastructural calibration model for proficiency testing},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Applications of monitoring and tracing the evolution of
clustering solutions in dynamic datasets. <em>JOAS</em>, <em>50</em>(4),
1017–1035. (<a
href="https://doi.org/10.1080/02664763.2021.2008882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The clustering approach is widely accepted as the most prominent unsupervised learning problem in data mining techniques. This procedure deals with the identification of notable structures in unlabeled datasets. In modern days clustering of dynamic data, streams play a vital role in policy-making, and researchers are paying particular attention to monitoring the evolution of clustering solutions over time. The data streams evolve continually, and different sources generate data items over time. The clustering solution over this stream is not stationary and changes with the influx of new data items. This paper presents a comprehensive study of algorithms related to tracing the evolution of clusters over time in cumulative datasets. To demonstrate the applications and significance of the tracing cluster evolution, we implement the MONIC algorithm in R-software. This article illustrates how the data segmentation of dynamic streams is done and shows the applications of monitoring changes in clustering solutions with the help of real-life published datasets.},
  archive      = {J_JOAS},
  author       = {Muhammad Atif and Muhammad Shafiq and Friedrich Leisch},
  doi          = {10.1080/02664763.2021.2008882},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {1017-1035},
  shortjournal = {J. Appl. Stat.},
  title        = {Applications of monitoring and tracing the evolution of clustering solutions in dynamic datasets},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating the parameters of a dependent model and applying
it to environmental data set. <em>JOAS</em>, <em>50</em>(4), 984–1016.
(<a href="https://doi.org/10.1080/02664763.2021.2006613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new dependent model is introduced. The model is motivated using the structure of series-parallel systems consisting of two series-parallel systems with a random number of parallel sub-systems that have fixed components connected in series. The dependence properties of the proposed model are studied. Two estimation methods, namely the moment method, and the maximum likelihood method are applied to estimate the parameters of the distributions of the components based on observing the system&#39;s lifetime data. A Monte Carlo simulation study is used to evaluate the performance of the estimators. Two real data sets are used to illustrate the proposed method. The results are useful for researchers and practitioners interested in analyzing bivariate data related to extreme events.},
  archive      = {J_JOAS},
  author       = {V. Mohtashami-Borzadaran and M. Amini and J. Ahmadi},
  doi          = {10.1080/02664763.2021.2006613},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {984-1016},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating the parameters of a dependent model and applying it to environmental data set},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curve fitting and jump detection on nonparametric regression
with missing data. <em>JOAS</em>, <em>50</em>(4), 963–983. (<a
href="https://doi.org/10.1080/02664763.2021.2004580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, by virtual of the inverse probability weighted technique, we considered the jump-preserving estimation on the nonparametric regression models with missing data on response variable. First, we used local piecewise-linear expansion respectively with left and right kernel to approximate the unknown regression function. Second, we obtained the left- and right-limit estimation of regression function at each observed points and then determinated the final estimators by residual sums of squares. Third, we presented the convergence rate of estimators and the residual sums of squares. Finally, we illustrated the performance of our proposed method through some simulation studies and a conjunctivitis example from The Affiliated Hospital of Hangzhou Normal University.},
  archive      = {J_JOAS},
  author       = {Qianyi Li and Jianbo Li and Yongran Cheng and Riquan Zhang},
  doi          = {10.1080/02664763.2021.2004580},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {963-983},
  shortjournal = {J. Appl. Stat.},
  title        = {Curve fitting and jump detection on nonparametric regression with missing data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monitoring the weibull shape parameter under progressive
censoring in presence of independent competing risks. <em>JOAS</em>,
<em>50</em>(4), 945–962. (<a
href="https://doi.org/10.1080/02664763.2021.2003760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, monitoring the Weibull shape parameter arising from progressively censored competing risks data is investigated. The competing risks are assumed to be independent and not identically distributed from the Weibull distributions with different shape and scale parameters. Both the shape parameters can be monitored separately by the proposed control charts using censored and predicted observations. We also introduced a control chart for monitoring both shape parameters simultaneously to detect possible shifts in both opposite and the same directions. In addition, the problem of mask data is discussed and an efficient prediction method is proposed. The behavior of the average run length with and without mask data is investigated through extensive simulations. Furthermore, the effects of sample size, number of failures due to each risk, and censoring scheme on the charts&#39; performance are also studied. Finally, an illustrative example is presented to demonstrate the application of the proposed control charts by investigating a real data set of the failure times of two-component ARC-1 VHF communication transmitter receivers of a single commercial airline. Although this data set has been widely investigated in reliability analysis studies, this is the first time it has been analyzed in a statistical process monitoring setting.},
  archive      = {J_JOAS},
  author       = {Rusul Mohsin Moharib Alsarray and Jaber Kazempoor and Adel Ahmadi Nadi},
  doi          = {10.1080/02664763.2021.2003760},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {945-962},
  shortjournal = {J. Appl. Stat.},
  title        = {Monitoring the weibull shape parameter under progressive censoring in presence of independent competing risks},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing quantitative trait locus effects in genetic
backcross studies with double recombination occurring. <em>JOAS</em>,
<em>50</em>(4), 927–944. (<a
href="https://doi.org/10.1080/02664763.2021.2001444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing the existence of quantitative trait locus (QTL) effects is an important task in QTL mapping studies. In this paper, we assume the phenotype distributions from a location-scale distribution family, and consider to test the QTL effects in both location and scale in the backcross studies with double recombination occurring. Without equal scale assumption, the log-likelihood function is unbounded, which leads to the traditional likelihood ratio test being invalid. To deal with this problem, we propose a penalized likelihood ratio test (PLRT) for testing the QTL effects. The null limiting distribution of the PLRT is shown to be a supremum of a chi-square process. As a complement, we also investigate the null limiting distribution of the likelihood ratio test for the case with equal scale assumption. The limiting distributions of the two tests under local alternatives are also studied. Simulation studies are performed to evaluate the asymptotic results and a real-data example is given for illustration.},
  archive      = {J_JOAS},
  author       = {Guanfu Liu and Zongliang Hu},
  doi          = {10.1080/02664763.2021.2001444},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {927-944},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing quantitative trait locus effects in genetic backcross studies with double recombination occurring},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional distributional clustering using spatio-temporal
data. <em>JOAS</em>, <em>50</em>(4), 909–926. (<a
href="https://doi.org/10.1080/02664763.2021.2001443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new method called the functional distributional clustering algorithm (FDCA) that seeks to identify spatially contiguous clusters and incorporate changes in temporal patterns across overcrowded networks. This method is motivated by a graph-based network composed of sensors arranged over space where recorded observations for each sensor represent a multi-modal distribution. The proposed method is fully non-parametric and generates clusters within an agglomerative hierarchical clustering approach based on a measure of distance that defines a cumulative distribution function over temporal changes for different locations in space. Traditional hierarchical clustering algorithms that are spatially adapted do not typically accommodate the temporal characteristics of the underlying data. The effectiveness of the FDCA is illustrated using an application to both empirical and simulated data from about 400 sensors in a 2.5 square miles network area in downtown San Francisco, California. The results demonstrate the superior ability of the the FDCA in identifying true clusters compared to functional only and distributional only algorithms and similar performance to a model-based clustering algorithm.},
  archive      = {J_JOAS},
  author       = {A. Venkatasubramaniam and L. Evers and P. Thakuriah and K. Ampountolas},
  doi          = {10.1080/02664763.2021.2001443},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {909-926},
  shortjournal = {J. Appl. Stat.},
  title        = {Functional distributional clustering using spatio-temporal data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The unit log–log distribution: A new unit distribution with
alternative quantile regression modeling and educational measurements
applications. <em>JOAS</em>, <em>50</em>(4), 889–908. (<a
href="https://doi.org/10.1080/02664763.2021.2001442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new distribution, named unit log–log distribution , defined on the bounded (0,1) interval. Basic distributional properties such as model shapes, stochastic ordering, quantile function, moments, and order statistics of the newly defined unit distribution are studied. The maximum likelihood estimation method has been pointed out to estimate its model parameters. The new quantile regression model based on the proposed distribution is introduced and it has been derived estimations of its model parameters also. The Monte Carlo simulation studies have been given to see the performance of the estimation method based on the new unit distribution and its regression modeling. Applications of the newly defined distribution and its quantile regression model to real data sets show that the proposed models have better modeling abilities than competitive models. The proposed unit quantile regression model has targeted to explain linear relation between educational measurements of both OECD (Organization for Economic Co-operation and Development) countries and some non-members of OECD countries, and their Better Life Index. The existence of the significant covariates has been seen on the real data applications for the unit median response.},
  archive      = {J_JOAS},
  author       = {Mustafa Ç. Korkmaz and Zehra Sedef Korkmaz},
  doi          = {10.1080/02664763.2021.2001442},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {889-908},
  shortjournal = {J. Appl. Stat.},
  title        = {The unit log–log distribution: A new unit distribution with alternative quantile regression modeling and educational measurements applications},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A finite mixture mixed proportion regression model for
classification problems in longitudinal voting data. <em>JOAS</em>,
<em>50</em>(4), 871–888. (<a
href="https://doi.org/10.1080/02664763.2021.1998392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous clustered proportion data often arise in various areas of the social and political sciences where the response variable of interest is a proportion (or percentage). An example is the behavior of the proportion of voters favorable to a political party in municipalities (or cities) of a country over time. This behavior can be different depending on the region of the country, giving rise to groups (or clusters) with similar profiles. For this kind of data, we propose a finite mixture of a random effects regression model based on the L-Logistic distribution. A Markov chain Monte Carlo algorithm is tailored to obtain posterior distributions of the unknown quantities of interest through a Bayesian approach. To illustrate the proposed method, with emphasis on analysis of clusters, we analyze the proportion of votes for a political party in presidential elections in different municipalities observed over time, and then identify groups according to electoral behavior at different levels of favorable votes.},
  archive      = {J_JOAS},
  author       = {Rosineide da Paz and Jorge Luis Bazán and Victor Hugo Lachos and Dipak Dey},
  doi          = {10.1080/02664763.2021.1998392},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {871-888},
  shortjournal = {J. Appl. Stat.},
  title        = {A finite mixture mixed proportion regression model for classification problems in longitudinal voting data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A more powerful test for three-arm non-inferiority via risk
difference: Frequentist and bayesian approaches. <em>JOAS</em>,
<em>50</em>(4), 848–870. (<a
href="https://doi.org/10.1080/02664763.2021.1998391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Necessity for finding improved intervention in many legacy therapeutic areas are of high priority. This has the potential to decrease the expense of medical care and poor outcomes for many patients. Typically, clinical efficacy is the primary evaluating criteria to measure any beneficial effect of a treatment. Albeit, there could be situations when several other factors (e.g. side-effects, cost-burden, less debilitating, less intensive, etc.) which can permit some slightly less efficacious treatment options favorable to a subgroup of patients. This often leads to non-inferiority (NI) testing. NI trials may or may not include a placebo arm due to ethical reasons. However, when included, the resulting three-arm trial is more prudent since it requires less stringent assumptions compared to a two-arm placebo-free trial. In this article, we consider both Frequentist and Bayesian procedures for testing NI in the three-arm trial with binary outcomes when the functional of interest is risk difference. An improved Frequentist approach is proposed first, which is then followed by a Bayesian counterpart. Bayesian methods have a natural advantage in many active-control trials, including NI trial, as it can seamlessly integrate substantial prior information. In addition, we discuss sample size calculation and draw an interesting connection between the two paradigms.},
  archive      = {J_JOAS},
  author       = {Erina Paul and Ram C. Tiwari and Shrabanti Chowdhury and Samiran Ghosh},
  doi          = {10.1080/02664763.2021.1998391},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {848-870},
  shortjournal = {J. Appl. Stat.},
  title        = {A more powerful test for three-arm non-inferiority via risk difference: Frequentist and bayesian approaches},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distribution-free phase-i scheme for location, scale and
skewness shifts with an application in monitoring customers’ waiting
time. <em>JOAS</em>, <em>50</em>(4), 827–847. (<a
href="https://doi.org/10.1080/02664763.2021.1994530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase-I analysis of historical data from a statistical process is a strategic problem in Statistical Process Monitoring and control. Before the establishment of process stability, it is challenging to model historical data. Consequently, a distribution-free approach is a natural choice in Phase-I monitoring. Existing distribution-free Phase-I control charts are suitable for detecting instability in location and scale parameters only and are often insensitive in complex processes involving skewness or shape parameters. A new Phase-I control chart is proposed to identify more general shifts, including location, scale and skewness. The proposed Phase-I scheme is efficient in such a situation. The proposed Phase-I scheme uses subsamples, and the plotting statistic is based on the omnibus multi-sample linear rank statistic corresponding to the location, scale and skewness shifts. The new scheme can identify subsamples that are not in control, and it can also indicate one or more process parameters where a deviation has occurred. The encouraging performance of the proposed scheme is established with a large-scale numerical study based on Monte-Carlo in detecting shifts of various nature in a comprehensive class of situations. An illustration based on monitoring the waiting time data from a customer service centre is given. Some concluding remarks and some future research problems are also offered.},
  archive      = {J_JOAS},
  author       = {Akira Suzuki and Hidetoshi Murakami and Amitava Mukherjee},
  doi          = {10.1080/02664763.2021.1994530},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {827-847},
  shortjournal = {J. Appl. Stat.},
  title        = {Distribution-free phase-I scheme for location, scale and skewness shifts with an application in monitoring customers&#39; waiting time},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-resolution super learner for voxel-wise classification
of prostate cancer using multi-parametric MRI. <em>JOAS</em>,
<em>50</em>(3), 805–826. (<a
href="https://doi.org/10.1080/02664763.2021.2017411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-parametric MRI (mpMRI) is a critical tool in prostate cancer (PCa) diagnosis and management. To further advance the use of mpMRI in patient care, computer aided diagnostic methods are under continuous development for supporting/supplanting standard radiological interpretation. While voxel-wise PCa classification models are the gold standard, few if any approaches have incorporated the inherent structure of the mpMRI data, such as spatial heterogeneity and between-voxel correlation, into PCa classification. We propose a machine learning-based method to fill in this gap. Our method uses an ensemble learning approach to capture regional heterogeneity in the data, where classifiers are developed at multiple resolutions and combined using the super learner algorithm, and further account for between-voxel correlation through a Gaussian kernel smoother. It allows any type of classifier to be the base learner and can be extended to further classify PCa sub-categories. We introduce the algorithms for binary PCa classification, as well as for classifying the ordinal clinical significance of PCa for which a weighted likelihood approach is implemented to improve the detection of less prevalent cancer categories. The proposed method has shown important advantages over conventional modeling and machine learning approaches in simulations and application to our motivating patient data.},
  archive      = {J_JOAS},
  author       = {Jin Jin and Lin Zhang and Ethan Leng and Gregory J. Metzger and Joseph S. Koopmeiners},
  doi          = {10.1080/02664763.2021.2017411},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {805-826},
  shortjournal = {J. Appl. Stat.},
  title        = {Multi-resolution super learner for voxel-wise classification of prostate cancer using multi-parametric MRI},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Handling high-dimensional data with missing values by modern
machine learning techniques. <em>JOAS</em>, <em>50</em>(3), 786–804. (<a
href="https://doi.org/10.1080/02664763.2022.2068514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data have been regarded as one of the most important types of big data in practice. It happens frequently in practice including genetic study, financial study, and geographical study. Missing data in high dimensional data analysis should be handled properly to reduce nonresponse bias. We discuss some modern machine learning techniques including penalized regression approaches, tree-based approaches, and deep learning (DL) for handling missing data with high dimensionality. Specifically, our proposed methods can be used for estimating general parameters of interest including population means and percentiles with imputation-based estimators, propensity score estimators, and doubly robust estimators. We compare those methods through some limited simulation studies and a real application. Both simulation studies and real application show the benefits of DL and XGboost approaches compared with other methods in terms of balancing bias and variance.},
  archive      = {J_JOAS},
  author       = {Sixia Chen and Chao Xu},
  doi          = {10.1080/02664763.2022.2068514},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {786-804},
  shortjournal = {J. Appl. Stat.},
  title        = {Handling high-dimensional data with missing values by modern machine learning techniques},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-assisted estimation in high-dimensional settings for
survey data. <em>JOAS</em>, <em>50</em>(3), 761–785. (<a
href="https://doi.org/10.1080/02664763.2022.2047905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-assisted estimators have attracted a lot of attention in the last three decades. These estimators attempt to make an efficient use of auxiliary information available at the estimation stage. A working model linking the survey variable to the auxiliary variables is specified and fitted on the sample data to obtain a set of predictions, which are then incorporated in the estimation procedures. A nice feature of model-assisted procedures is that they maintain important design properties such as consistency and asymptotic unbiasedness irrespective of whether or not the working model is correctly specified. In this article, we examine several model-assisted estimators from a design-based point of view and in a high-dimensional setting, including linear regression and penalized estimators. We conduct an extensive simulation study using data from the Irish Commission for Energy Regulation Smart Metering Project, to assess the performance of several model-assisted estimators in terms of bias and efficiency in this high-dimensional data set.},
  archive      = {J_JOAS},
  author       = {Mehdi Dagdoug and Camelia Goga and David Haziza},
  doi          = {10.1080/02664763.2022.2047905},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {761-785},
  shortjournal = {J. Appl. Stat.},
  title        = {Model-assisted estimation in high-dimensional settings for survey data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensitivity analysis of unmeasured confounding in causal
inference based on exponential tilting and super learner. <em>JOAS</em>,
<em>50</em>(3), 744–760. (<a
href="https://doi.org/10.1080/02664763.2021.1999398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference under the potential outcome framework relies on the strongly ignorable treatment assumption. This assumption is usually questionable in observational studies, and the unmeasured confounding is one of the fundamental challenges in causal inference. To this end, we propose a new sensitivity analysis method to evaluate the impact of the unmeasured confounder by leveraging ideas of doubly robust estimators, the exponential tilt method, and the super learner algorithm. Compared to other existing methods of sensitivity analysis that parameterize the unmeasured confounder as a latent variable in the working models, the exponential tilting method does not impose any restrictions on the structure or models of the unmeasured confounders. In addition, in order to reduce the modeling bias of traditional parametric methods, we propose incorporating the super learner machine learning algorithm to perform nonparametric model estimation and the corresponding sensitivity analysis. Furthermore, most existing sensitivity analysis methods require multivariate sensitivity parameters, which make its choice difficult and subjective in practice. In comparison, the new method has a univariate sensitivity parameter with a nice and simple interpretation of log-odds ratios for binary outcomes, which makes its choice and the application of the new sensitivity analysis method very easy for practitioners.},
  archive      = {J_JOAS},
  author       = {Mi Zhou and Weixin Yao},
  doi          = {10.1080/02664763.2021.1999398},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {744-760},
  shortjournal = {J. Appl. Stat.},
  title        = {Sensitivity analysis of unmeasured confounding in causal inference based on exponential tilting and super learner},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing evidence-based falls prevention data with
significant missing information using variable selection after multiple
imputation. <em>JOAS</em>, <em>50</em>(3), 724–743. (<a
href="https://doi.org/10.1080/02664763.2021.1985090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Falls are the leading cause of fatal and non-fatal injuries among older adults. Evidence-based fall prevention programs are delivered nationwide, largely supported by funding from the Administration for Community Living (ACL), to mitigate fall-related risk. This study utilizes data from 39 ACL grantees in 22 states from 2014 to 2017. The large amount of missing values for falls efficacy in this national database may lead to potentially biased statistical results and make it challenging to implement reliable variable selection. Multiple imputation is used to deal with missing values. To obtain a consistent result of variable selection in multiply-imputed datasets, multiple imputation-stepwise regression (MI-stepwise) and multiple imputation-least absolute shrinkage and selection operator (MI-LASSO) methods are used. To compare the performances of MI-stepwise and MI-LASSO, simulation studies were conducted. In particular, we extended prior work by considering several circumstances not covered in previous studies, including an extensive investigation of data with different signal-to-noise ratios and various missing data patterns across predictors, as well as a data structure that allowed the missingness mechanism to be missing not at random (MNAR). In addition, we evaluated the performance of MI-LASSO method with varying tuning parameters to address the overselection issue in cross-validation (CV)-based LASSO.},
  archive      = {J_JOAS},
  author       = {Yujia Cheng and Yang Li and Matthew Lee Smith and Changwei Li and Ye Shen},
  doi          = {10.1080/02664763.2021.1985090},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {724-743},
  shortjournal = {J. Appl. Stat.},
  title        = {Analyzing evidence-based falls prevention data with significant missing information using variable selection after multiple imputation},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalized l2,p-norm regression based feature selection
algorithm. <em>JOAS</em>, <em>50</em>(3), 703–723. (<a
href="https://doi.org/10.1080/02664763.2021.1975662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important data dimension reduction method, and it has been used widely in applications involving high-dimensional data such as genetic data analysis and image processing. In order to achieve robust feature selection, the latest works apply the l 2 , 1 l 2 , 1 l2,1 or l 2 , p l 2 , p l2,p -norm of matrix to the loss function and regularization terms in regression, and have achieved encouraging results. However, these existing works rigidly set the matrix norms used in the loss function and the regularization terms to the same l 2 , 1 or l 2 , p -norm, which limit their applications. In addition, the algorithms for solutions they present either have high computational complexity and are not suitable for large data sets, or cannot provide satisfying performance due to the approximate calculation. To address these problems, we present a generalized l 2 , p -norm regression based feature selection ( l 2 , p -RFS) method based on a new optimization criterion. The criterion extends the optimization criterion of ( l 2 , p -RFS) when the loss function and the regularization terms in regression use different matrix norms. We cast the new optimization criterion in a regression framework without regularization. In this framework, the new optimization criterion can be solved using an iterative re-weighted least squares (IRLS) procedure in which the least squares problem can be solved efficiently by using the least square QR decomposition (LSQR) algorithm. We have conducted extensive experiments to evaluate the proposed algorithm on various well-known data sets of both gene expression and image data sets, and compare it with other related feature selection methods.},
  archive      = {J_JOAS},
  author       = {X. Zhi and J. Liu and S. Wu and C. Niu},
  doi          = {10.1080/02664763.2021.1975662},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {703-723},
  shortjournal = {J. Appl. Stat.},
  title        = {A generalized l2,p-norm regression based feature selection algorithm},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification of novel genes for triple-negative breast
cancer with semiparametric gene-based analysis. <em>JOAS</em>,
<em>50</em>(3), 691–702. (<a
href="https://doi.org/10.1080/02664763.2021.1973387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triple-negative breast cancer (TNBC) is generally considered an aggressive breast cancer subtype associated with poor prognostic outcomes. Up to now, the molecular and cellular mechanisms underlying TNBC pathology have not been fully understood. In this manuscript, we propose a novel semiparametric model with kernel for gene-based analysis with a breast cancer GWAS data. The software of SPMGBA (semiparametric method for gene-based analysis) in MATLAB is available at GitHub (https://github.com/zliu3/SPMGBA). Genetic signatures associated with breast cancer are discovered. We further validate the prognostic power of the identified genes with a large cohort of expression data from the European Genome-Phenome Archive, and discover that SEL1L is associated with the overall survival of TNBC with the p -value of .0002. We conclude that gene SEL1L is down-regulated in TNBC and the expression of SEL1L is positively associated with patient survival.},
  archive      = {J_JOAS},
  author       = {Xiaotong Liu and Guoliang Tian and Zhenqiu Liu},
  doi          = {10.1080/02664763.2021.1973387},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {691-702},
  shortjournal = {J. Appl. Stat.},
  title        = {Identification of novel genes for triple-negative breast cancer with semiparametric gene-based analysis},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification of histogram-valued data with support
histogram machines. <em>JOAS</em>, <em>50</em>(3), 675–690. (<a
href="https://doi.org/10.1080/02664763.2021.1947996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current large amounts of data and advanced technologies have produced new types of complex data, such as histogram-valued data. The paper focuses on classification problems when predictors are observed as or aggregated into histograms. Because conventional classification methods take vectors as input, a natural approach converts histograms into vector-valued data using summary values, such as the mean or median. However, this approach forgoes the distributional information available in histograms. To address this issue, we propose a margin-based classifier called support histogram machine (SHM) for histogram-valued data. We adopt the support vector machine framework and the Wasserstein-Kantorovich metric to measure distances between histograms. The proposed optimization problem is solved by a dual approach. We then test the proposed SHM via simulated and real examples and demonstrate its superior performance to summary-value-based methods.},
  archive      = {J_JOAS},
  author       = {Ilsuk Kang and Cheolwoo Park and Young Joo Yoon and Changyi Park and Soon-Sun Kwon and Hosik Choi},
  doi          = {10.1080/02664763.2021.1947996},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {675-690},
  shortjournal = {J. Appl. Stat.},
  title        = {Classification of histogram-valued data with support histogram machines},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noise-insensitive discriminative subspace fuzzy clustering.
<em>JOAS</em>, <em>50</em>(3), 659–674. (<a
href="https://doi.org/10.1080/02664763.2021.1937583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative subspace clustering (DSC) can make full use of linear discriminant analysis (LDA) to reduce the dimension of data and achieve effective clustering high-dimension data by clustering low-dimension data in discriminant subspace. However, most existing DSC algorithms do not consider the noise and outliers that may be contained in data sets, and when they are applied to the data sets with noise or outliers, and they often obtain poor performance due to the influence of noise and outliers. In this paper, we address the problem of the sensitivity of DSC to noise and outlier. Replacing the Euclidean distance in the objective function of LDA by an exponential non-Euclidean distance, we first develop a noise-insensitive LDA (NILDA) algorithm. Then, combining the proposed NILDA and a noise-insensitive fuzzy clustering algorithm: AFKM, we propose a noise-insensitive discriminative subspace fuzzy clustering (NIDSFC) algorithm. Experiments on some benchmark data sets show the effectiveness of the proposed NIDSFC algorithm.},
  archive      = {J_JOAS},
  author       = {Xiaobin Zhi and Tongjun Yu and Longtao Bi and Yalan Li},
  doi          = {10.1080/02664763.2021.1937583},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {659-674},
  shortjournal = {J. Appl. Stat.},
  title        = {Noise-insensitive discriminative subspace fuzzy clustering},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of multivariate longitudinal data using dynamic
lasso-regularized copula models with application to large pediatric
cardiovascular studies. <em>JOAS</em>, <em>50</em>(3), 631–658. (<a
href="https://doi.org/10.1080/02664763.2021.1937581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The National Heart, Lung and Blood Institute Growth and Health Study (NGHS) is a large longitudinal study of childhood health. A main objective of the study is to estimate the joint distributions of cardiovascular risk outcomes at any two time points conditioning on a large number of covariates. Existing multivariate longitudinal methods are not suitable for outcomes at multiple time points. We present a dynamic copula approach for estimating an outcome&#39;s joint distributions at two time points given a large number of time-varying covariates. Our models depend on the outcome&#39;s time-varying distributions at one time point, the bivariate copula densities and the functional copula parameters. We develop a three-step procedure for variable selection and estimation, which selects the influential covariates using a machine learning procedure based on spline Lasso-regularized least squares, computes the outcome&#39;s single-time distribution using splines, and estimates the functional copula parameter of the dynamic copula models. Pointwise confidence intervals are constructed through the resampling-subject bootstrap. We apply our procedure to the NGHS cardiovascular risk data and illustrate the clinical interpretations of the conditional distributions of a set of risk outcomes. We demonstrate the statistical properties of the dynamic models and estimation procedure through a simulation study.},
  archive      = {J_JOAS},
  author       = {Wei Zhang and Colin O. Wu and Xiaoyang Ma and Xin Tian and Qizhai Li},
  doi          = {10.1080/02664763.2021.1937581},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {631-658},
  shortjournal = {J. Appl. Stat.},
  title        = {Analysis of multivariate longitudinal data using dynamic lasso-regularized copula models with application to large pediatric cardiovascular studies},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling multivariate cyber risks: Deep learning dating
extreme value theory. <em>JOAS</em>, <em>50</em>(3), 610–630. (<a
href="https://doi.org/10.1080/02664763.2021.1936468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling cyber risks has been an important but challenging task in the domain of cyber security, which is mainly caused by the high dimensionality and heavy tails of risk patterns. Those obstacles have hindered the development of statistical modeling of the multivariate cyber risks. In this work, we propose a novel approach for modeling the multivariate cyber risks which relies on the deep learning and extreme value theory. The proposed model not only enjoys the high accurate point predictions via deep learning but also can provide the satisfactory high quantile predictions via extreme value theory. Both the simulation and empirical studies show that the proposed approach can model the multivariate cyber risks very well and provide satisfactory prediction performances.},
  archive      = {J_JOAS},
  author       = {Mingyue Zhang Wu and Jinzhu Luo and Xing Fang and Maochao Xu and Peng Zhao},
  doi          = {10.1080/02664763.2021.1936468},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {610-630},
  shortjournal = {J. Appl. Stat.},
  title        = {Modeling multivariate cyber risks: Deep learning dating extreme value theory},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimized machine learning technology scheme and its
application in fault detection in wireless sensor networks.
<em>JOAS</em>, <em>50</em>(3), 592–609. (<a
href="https://doi.org/10.1080/02664763.2021.1929089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of fault detection in data collection in wireless sensor networks, this paper combines evolutionary computing and machine learning to propose a productive technical solution. We choose the classical particle swarm optimization (PSO) and improve it, including the introduction of a biological population model to control the population size, and the addition of a parallel mechanism for further tuning. The proposed RS-PPSO algorithm was successfully used to optimize the initial weights and biases of back propagation neural network (BPNN), shortening the training time and raising the prediction accuracy. Wireless sensor networks (WSN) has become the key supporting platform of Internet of Things (IoT). The correctness of the data collected by the sensor nodes has a great influence on the reliability, real-time performance and energy saving of the entire network. The optimized machine learning technology scheme given in this paper can effectively identify the fault data, so as to ensure the effective operation of WSN.},
  archive      = {J_JOAS},
  author       = {Fang Fan and Shu-Chuan Chu and Jeng-Shyang Pan and Chuang Lin and Huiqi Zhao},
  doi          = {10.1080/02664763.2021.1929089},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {592-609},
  shortjournal = {J. Appl. Stat.},
  title        = {An optimized machine learning technology scheme and its application in fault detection in wireless sensor networks},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised document classification integrating web
scraping, one-class SVM and LDA topic modelling. <em>JOAS</em>,
<em>50</em>(3), 574–591. (<a
href="https://doi.org/10.1080/02664763.2021.1919063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised document classification for imbalanced data sets poses a major challenge. To obtain accurate classification results, training data sets are often created manually by humans which requires expert knowledge, time and money. Depending on the imbalance of the data set, this approach also either requires human labelling of all of the data or it fails to adequately recognize underrepresented categories. We propose an integration of web scraping, one-class Support Vector Machines (SVM) and Latent Dirichlet Allocation (LDA) topic modelling as a multi-step classification rule that circumvents manual labelling. Unsupervised one-class document classification with the integration of out-of-domain training data is achieved and &gt;80\% of the target data is correctly classified. The proposed method thus even outperforms common machine learning classifiers and is validated on multiple data sets.},
  archive      = {J_JOAS},
  author       = {Anton Thielmann and Christoph Weisser and Astrid Krenz and Benjamin Säfken},
  doi          = {10.1080/02664763.2021.1919063},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {574-591},
  shortjournal = {J. Appl. Stat.},
  title        = {Unsupervised document classification integrating web scraping, one-class SVM and LDA topic modelling},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A distributed multiple sample testing for massive data.
<em>JOAS</em>, <em>50</em>(3), 555–573. (<a
href="https://doi.org/10.1080/02664763.2021.1911967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the data are stored in a distributed manner, direct application of traditional hypothesis testing procedures is often prohibitive due to communication costs and privacy concerns. This paper mainly develops and investigates a distributed two-node Kolmogorov–Smirnov hypothesis testing scheme, implemented by the divide-and-conquer strategy. In addition, this paper also provides a distributed fraud detection and a distribution-based classification for multi-node machines based on the proposed hypothesis testing scheme. The distributed fraud detection is to detect which node stores fraud data in multi-node machines and the distribution-based classification is to determine whether the multi-node distributions differ and classify different distributions. These methods can improve the accuracy of statistical inference in a distributed storage architecture. Furthermore, this paper verifies the feasibility of the proposed methods by simulation and real example studies.},
  archive      = {J_JOAS},
  author       = {Xie Xiaoyue and Jian Shi and Kai Song},
  doi          = {10.1080/02664763.2021.1911967},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {555-573},
  shortjournal = {J. Appl. Stat.},
  title        = {A distributed multiple sample testing for massive data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regularized regression when covariates are linked on a
network: The 3CoSE algorithm. <em>JOAS</em>, <em>50</em>(3), 535–554.
(<a href="https://doi.org/10.1080/02664763.2021.1982878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariates in regressions may be linked to each other on a network. Knowledge of the network structure can be incorporated into regularized regression settings via a network penalty term. However, when it is unknown whether the connection signs in the network are positive (connected covariates reinforce each other) or negative (connected covariates repress each other), the connection signs have to be estimated jointly with the covariate coefficients. This can be done with an algorithm iterating a connection sign estimation step and a covariate coefficient estimation step. We develop such an algorithm, called 3CoSE, and show detailed simulation results and an application forecasting event times. The algorithm performs well in a variety of settings. We also briefly describe the publicly available R-package developed for this purpose.},
  archive      = {J_JOAS},
  author       = {Matthias Weber and Jonas Striaukas and Martin Schumacher and Harald Binder},
  doi          = {10.1080/02664763.2021.1982878},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {535-554},
  shortjournal = {J. Appl. Stat.},
  title        = {Regularized regression when covariates are linked on a network: The 3CoSE algorithm},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model estimation and selection for partial linear varying
coefficient EV models with longitudinal data. <em>JOAS</em>,
<em>50</em>(3), 512–534. (<a
href="https://doi.org/10.1080/02664763.2021.1904847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the estimation and model selection for longitudinal partial linear varying coefficient errors-in-variables (EV) models when the covariates are measured with some additive errors. Bias-corrected penalized quadratic inference functions method is proposed based on quadratic inference functions with two penalty function terms. The proposed method can not only handle the measurement errors of covariates and within-subject correlations but also estimate and select significant non-zero parametric and nonparametric components simultaneously. With some regularization conditions, the resulting estimators of parameters are asymptotically normal and the estimators of nonparametric varying coefficient achieves the optimal convergence rate. Furthermore, we present simulation studies and a real example analysis to evaluate the finite sample performance of the proposed method.},
  archive      = {J_JOAS},
  author       = {Mingtao Zhao and Xiaoli Xu and Yanling Zhu and Kongsheng Zhang and Yan Zhou},
  doi          = {10.1080/02664763.2021.1904847},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {512-534},
  shortjournal = {J. Appl. Stat.},
  title        = {Model estimation and selection for partial linear varying coefficient EV models with longitudinal data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A practical two-sample test for weighted random graphs.
<em>JOAS</em>, <em>50</em>(3), 495–511. (<a
href="https://doi.org/10.1080/02664763.2021.1884847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network (graph) data analysis is a popular research topic in statistics and machine learning. In application, one is frequently confronted with graph two-sample hypothesis testing where the goal is to test the difference between two graph populations. Several statistical tests have been devised for this purpose in the context of binary graphs. However, many of the practical networks are weighted and existing procedures cannot be directly applied to weighted graphs. In this paper, we study the weighted graph two-sample hypothesis testing problem and propose a practical test statistic. We prove that the proposed test statistic converges in distribution to the standard normal distribution under the null hypothesis and analyze its power theoretically. The simulation study shows that the proposed test has satisfactory performance and it substantially outperforms the existing counterpart in the binary graph case. A real data application is provided to illustrate the method.},
  archive      = {J_JOAS},
  author       = {Mingao Yuan and Qian Wen},
  doi          = {10.1080/02664763.2021.1884847},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {495-511},
  shortjournal = {J. Appl. Stat.},
  title        = {A practical two-sample test for weighted random graphs},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective deep residual network based class attention
layer with bidirectional LSTM for diagnosis and classification of
COVID-19. <em>JOAS</em>, <em>50</em>(3), 477–494. (<a
href="https://doi.org/10.1080/02664763.2020.1849057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent days, COVID-19 pandemic has affected several people&#39;s lives globally and necessitates a massive number of screening tests to detect the existence of the coronavirus. At the same time, the rise of deep learning (DL) concepts helps to effectively develop a COVID-19 diagnosis model to attain maximum detection rate with minimum computation time. This paper presents a new Residual Network (ResNet) based Class Attention Layer with Bidirectional LSTM called RCAL-BiLSTM for COVID-19 Diagnosis. The proposed RCAL-BiLSTM model involves a series of processes namely bilateral filtering (BF) based preprocessing, RCAL-BiLSTM based feature extraction, and softmax (SM) based classification. Once the BF technique produces the preprocessed image, RCAL-BiLSTM based feature extraction process takes place using three modules, namely ResNet based feature extraction, CAL, and Bi-LSTM modules. Finally, the SM layer is applied to categorize the feature vectors into corresponding feature maps. The experimental validation of the presented RCAL-BiLSTM model is tested against Chest-X-Ray dataset and the results are determined under several aspects. The experimental outcome pointed out the superior nature of the RCAL-BiLSTM model by attaining maximum sensitivity of 93.28\%, specificity of 94.61\%, precision of 94.90\%, accuracy of 94.88\%, F-score of 93.10\% and kappa value of 91.40\%.},
  archive      = {J_JOAS},
  author       = {Denis A. Pustokhin and Irina V. Pustokhina and Phuoc Nguyen Dinh and Son Van Phan and Gia Nhu Nguyen and Gyanendra Prasad Joshi and Shankar K.},
  doi          = {10.1080/02664763.2020.1849057},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {477-494},
  shortjournal = {J. Appl. Stat.},
  title        = {An effective deep residual network based class attention layer with bidirectional LSTM for diagnosis and classification of COVID-19},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-sample behrens–fisher problems for high-dimensional
data: A normal reference scale-invariant test. <em>JOAS</em>,
<em>50</em>(3), 456–476. (<a
href="https://doi.org/10.1080/02664763.2020.1834516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For high-dimensional two-sample Behrens–Fisher problems, several non-scale-invariant and scale-invariant tests have been proposed. Most of them impose strong assumptions on the underlying group covariance matrices so that their test statistics are asymptotically normal. However, in practice, these assumptions may not be satisfied or hardly be checked so that these tests may not be able to maintain the nominal size well in practice. To overcome this difficulty, in this paper, a normal reference scale-invariant test is proposed and studied. It works well by neither imposing strong assumptions on the underlying group covariance matrices nor assuming their equality. It is shown that under some regularity conditions and the null hypothesis, the proposed test and a chi-square-type mixture have the same normal and non-normal limiting distributions. It is then justifiable to approximate the null distribution of the proposed test using that of the chi-square-type mixture. The distribution of the chi-square type mixture can be well approximated by the Welch–Satterthwaite chi-square-approximation with the approximation parameter consistently estimated from the data. The asymptotic power of the proposed test is established. Numerical results demonstrate that the proposed test has much better size control and power than several well-known non-scale-invariant and scale-invariant tests.},
  archive      = {J_JOAS},
  author       = {Liang Zhang and Tianming Zhu and Jin-Ting Zhang},
  doi          = {10.1080/02664763.2020.1834516},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {456-476},
  shortjournal = {J. Appl. Stat.},
  title        = {Two-sample Behrens–Fisher problems for high-dimensional data: A normal reference scale-invariant test},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial to the special issue: Statistical approaches for
big data and machine learning. <em>JOAS</em>, <em>50</em>(3), 451–455.
(<a href="https://doi.org/10.1080/02664763.2023.2162471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {Yichuan Zhao and Chi-Hua Chen and Feng Feng and Dragan Pamucar},
  doi          = {10.1080/02664763.2023.2162471},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {451-455},
  shortjournal = {J. Appl. Stat.},
  title        = {Editorial to the special issue: Statistical approaches for big data and machine learning},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bimodality in reentry latitude predictions for spacecraft in
prograde orbits. <em>JOAS</em>, <em>50</em>(2), 434–450. (<a
href="https://doi.org/10.1080/02664763.2021.2008328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probability distribution functions (PDFs) of atmospheric reentry latitude predictions are shown to be bimodal for spacecraft in low-eccentricity, prograde low Earth orbits at altitudes of 300 km and lower. Using two-line element (TLE) data for initial orbit conditions, coupled with coarse estimates for spacecraft aerodynamic characteristics, parametric simulations produce bimodal distributions that suggest a greater likelihood of reentry near the latitudinal maxima of a given spacecraft&#39;s ground track. Various computational measures are used to test for and quantify bimodality in the reentry latitude data sets. Also, a method for approximating bandwidth is introduced for the kernel estimation of reentry latitude probability density. Overall, statistical analysis indicates that actual reentry latitudes are generally within 1-σ of observed hemisphere means as demonstrated by six historical reentry cases.},
  archive      = {J_JOAS},
  author       = {Robert A. Bettinger},
  doi          = {10.1080/02664763.2021.2008328},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {434-450},
  shortjournal = {J. Appl. Stat.},
  title        = {Bimodality in reentry latitude predictions for spacecraft in prograde orbits},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Addressing overdispersion and zero-inflation for clustered
count data via new multilevel heterogenous hurdle models. <em>JOAS</em>,
<em>50</em>(2), 408–433. (<a
href="https://doi.org/10.1080/02664763.2022.2096875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unobserved heterogeneity causing overdispersion and the excessive number of zeros take a prominent place in the methodological development on count modeling. An insight into the mechanisms that induce heterogeneity is required for better understanding of the phenomenon of overdispersion. When the heterogeneity is sourced by the stochastic component of the model, the use of a heterogenous Poisson distribution for this part encounters as an elegant solution. Hierarchical design of the study is also responsible for the heterogeneity as the unobservable effects at various levels also contribute to the overdispersion. Zero-inflation, heterogeneity and multilevel nature in the count data present special challenges in their own respect, however the presence of all in one study adds more challenges to the modeling strategies. This study therefore is designed to merge the attractive features of the separate strand of the solutions in order to face such a comprehensive challenge. This study differs from the previous attempts by the choice of two recently developed heterogeneous distributions, namely Poisson–Lindley (PL) and Poisson–Ailamujia (PA) for the truncated part. Using generalized linear mixed modeling settings, predictive performances of the multilevel PL and PA models and their hurdle counterparts were assessed within a comprehensive simulation study in terms of bias, precision and accuracy measures. Multilevel models were applied to two separate real world examples for the assessment of practical implications of the new models proposed in this study.},
  archive      = {J_JOAS},
  author       = {Yasin Altinisik},
  doi          = {10.1080/02664763.2022.2096875},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {408-433},
  shortjournal = {J. Appl. Stat.},
  title        = {Addressing overdispersion and zero-inflation for clustered count data via new multilevel heterogenous hurdle models},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian approaches to variable selection in mixture models
with application to disease clustering. <em>JOAS</em>, <em>50</em>(2),
387–407. (<a
href="https://doi.org/10.1080/02664763.2021.1994529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical research, cluster analysis is often performed to identify patient subgroups based on patients&#39; characteristics or traits. In the model-based clustering for identifying patient subgroups, mixture models have played a fundamental role in modeling. While there is an increasing interest in using mixture modeling for identifying patient subgroups, little work has been done in selecting the predictors that are associated with the class assignment. In this study, we develop and compare two approaches to perform variable selection in the context of a mixture model to identify important predictors that are associated with the class assignment. These two approaches are the one-step approach and the stepwise approach. The former refers to an approach in which clustering and variable selection are performed simultaneously in one overall model, whereas the latter refers to an approach in which clustering and variable selection are performed in two sequential steps. We considered both shrinkage prior and spike-and-slab prior to select the importance of variables. Markov chain Monte Carlo algorithms are developed to estimate the posterior distribution of the model parameters. Practical applications and simulation studies are carried out to evaluate the clustering and variable selection performance of the proposed models.},
  archive      = {J_JOAS},
  author       = {Zihang Lu and Wendy Lou},
  doi          = {10.1080/02664763.2021.1994529},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {387-407},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian approaches to variable selection in mixture models with application to disease clustering},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification of outlying observations for
large-dimensional data. <em>JOAS</em>, <em>50</em>(2), 370–386. (<a
href="https://doi.org/10.1080/02664763.2021.1993799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a two-stage procedure for identifying outlying observations in a large-dimensional data set. In the first stage, an outlier identification measure is defined by using a max-normal statistic and a clean subset that contains non-outliers is obtained. The identification of outliers can be deemed as a multiple hypothesis testing problem, then, in the second stage, we explore the asymptotic distribution of the proposed measure, and obtain the threshold of the outlying observations. Furthermore, in order to improve the identification power and better control the misjudgment rate, a one-step refined algorithm is proposed. Simulation results and two real data analysis examples show that, compared with other methods, the proposed procedure has great advantages in identifying outliers in various data situations.},
  archive      = {J_JOAS},
  author       = {Tao Wang and Xiaona Yang and Yunfei Guo and Zhonghua Li},
  doi          = {10.1080/02664763.2021.1993799},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {370-386},
  shortjournal = {J. Appl. Stat.},
  title        = {Identification of outlying observations for large-dimensional data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A first-order binomial-mixed poisson integer-valued
autoregressive model with serially dependent innovations. <em>JOAS</em>,
<em>50</em>(2), 352–369. (<a
href="https://doi.org/10.1080/02664763.2021.1993798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the extended Poisson INAR(1), which allows innovations to be serially dependent, we develop a new family of binomial-mixed Poisson INAR(1) (BMP INAR(1)) processes by adding a mixed Poisson component to the innovations of the classical Poisson INAR(1) process. Due to the flexibility of the mixed Poisson component, the model includes a large class of INAR(1) processes with different transition probabilities. Moreover, it can capture some overdispersion features coming from the data while keeping the innovations serially dependent. We discuss its statistical properties, stationarity conditions and transition probabilities for different mixing densities (Exponential, Lindley). Then, we derive the maximum likelihood estimation method and its asymptotic properties for this model. Finally, we demonstrate our approach using a real data example of iceberg count data from a financial system.},
  archive      = {J_JOAS},
  author       = {Zezhun Chen and Angelos Dassios and George Tzougas},
  doi          = {10.1080/02664763.2021.1993798},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {352-369},
  shortjournal = {J. Appl. Stat.},
  title        = {A first-order binomial-mixed poisson integer-valued autoregressive model with serially dependent innovations},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayes factor testing of equality and order constraints on
measures of association in social research. <em>JOAS</em>,
<em>50</em>(2), 315–351. (<a
href="https://doi.org/10.1080/02664763.2021.1992360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measures of association play a central role in the social sciences to quantify the strength of a linear relationship between the variables of interest. In many applications researchers can translate scientific expectations to hypotheses with equality and/or order constraints on these measures of association. In this paper a Bayes factor test is proposed for testing multiple hypotheses with constraints on the measures of association between ordinal and/or continuous variables, possibly after correcting for certain covariates. This test can be used to obtain a direct answer to the research question how much evidence there is in the data for a social science theory relative to competing theories. The stand-alone software package ‘ BCT ’ allows users to apply the methodology in an easy manner. The methodology will also be available in the R package ‘ BFpack ’. An empirical application from leisure studies about the associations between life, leisure and relationship satisfaction and an application about the differences about egalitarian justice beliefs across countries are used to illustrate the methodology.},
  archive      = {J_JOAS},
  author       = {Joris Mulder and John P. T. M. Gelissen},
  doi          = {10.1080/02664763.2021.1992360},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {315-351},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayes factor testing of equality and order constraints on measures of association in social research},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Component selection for exponential power mixture models.
<em>JOAS</em>, <em>50</em>(2), 291–314. (<a
href="https://doi.org/10.1080/02664763.2021.1990225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exponential Power (EP) family is a much flexible distribution family including Gaussian family as a sub-family. In this article, we study component selection and estimation for EP mixture models and regressions. The assumption on zero component mean in [X. Cao, Q. Zhao, D. Meng, Y. Chen, and Z. Xu, Robust low-rank matrix factorization under general mixture noise distributions, IEEE. Trans. Image. Process. 25 (2016), pp. 4677–4690.] is relaxed. To select components and estimate parameters simultaneously, we propose a penalized likelihood method, which can shrink mixing proportions to zero to achieve components selection. Modified EM algorithms are proposed, and the consistency of estimated component number is obtained. Simulation studies show the advantages of the proposed methods on accuracies of component number selection, parameter estimation, and density estimation. Analysis of value at risk of SHIBOR and a climate change data are given as illustration.},
  archive      = {J_JOAS},
  author       = {Xinyi Wang and Zhenghui Feng},
  doi          = {10.1080/02664763.2021.1990225},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {291-314},
  shortjournal = {J. Appl. Stat.},
  title        = {Component selection for exponential power mixture models},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survival tree based on stabilized score tests for
high-dimensional covariates. <em>JOAS</em>, <em>50</em>(2), 264–290. (<a
href="https://doi.org/10.1080/02664763.2021.1990224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A survival tree can classify subjects into different survival prognostic groups. However, when data contains high-dimensional covariates, the two popular classification trees exhibit fatal drawbacks. The logrank tree is unstable and tends to have false nodes; the conditional inference tree is difficult to interpret the adjusted P -value for high-dimensional tests. Motivated by these problems, we propose a new survival tree based on the stabilized score tests. We propose a novel matrix-based algorithm in order to tests a number of nodes simultaneously via stabilized score tests. We propose a recursive partitioning algorithm to construct a survival tree and develop our original R package uni.survival.tree ( https://cran.r-project.org/package=uni.survival.tree ) for implementation. Simulations are performed to demonstrate the superiority of the proposed method over the existing methods. The lung cancer data analysis demonstrates the usefulness of the proposed method.},
  archive      = {J_JOAS},
  author       = {Takeshi Emura and Wei-Chern Hsu and Wen-Chi Chou},
  doi          = {10.1080/02664763.2021.1990224},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {264-290},
  shortjournal = {J. Appl. Stat.},
  title        = {A survival tree based on stabilized score tests for high-dimensional covariates},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel group VIF regression for group variable selection
with application to multiple change-point detection. <em>JOAS</em>,
<em>50</em>(2), 247–263. (<a
href="https://doi.org/10.1080/02664763.2021.1987400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel group variance inflation factor (VIF) regression model for tackling large data sets where data follows a grouped structure. Unlike classical penalized methods, this approach can perform group variable selection in a sparse model, which is quite different from the classical penalized methods. We further adapt the proposed method associated with a two-stage procedure for detecting multiple change-point in linear models. We carry out extensive simulation studies to show that the proposed group variable selection and change-point detection methods are stable and efficient. Finally, we provide two real data examples, including a body fat data set and an air pollution data set, to illustrate the performance of our algorithms in group selection and change-point detection.},
  archive      = {J_JOAS},
  author       = {Hao Ding and Yan Zhang and Yuehua Wu},
  doi          = {10.1080/02664763.2021.1987400},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {247-263},
  shortjournal = {J. Appl. Stat.},
  title        = {A novel group VIF regression for group variable selection with application to multiple change-point detection},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monitoring SEIRD model parameters using MEWMA for the
COVID-19 pandemic with application to the state of qatar. <em>JOAS</em>,
<em>50</em>(2), 231–246. (<a
href="https://doi.org/10.1080/02664763.2021.1985091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the current COVID-19 pandemic, decision-makers are tasked with implementing and evaluating strategies for both treatment and disease prevention. In order to make effective decisions, they need to simultaneously monitor various attributes of the pandemic such as transmission rate and infection rate for disease prevention, recovery rate which indicates treatment effectiveness as well as the mortality rate and others. This work presents a technique for monitoring the pandemic by employing an Susceptible, Exposed, Infected, Recovered, Death model regularly estimated by an augmented particle Markov chain Monte Carlo scheme in which the posterior distribution samples are monitored via Multivariate Exponentially Weighted Average process monitoring. This is illustrated on the COVID-19 data for the State of Qatar.},
  archive      = {J_JOAS},
  author       = {Edward L. Boone and Abdel-Salam G. Abdel-Salam and Indranil Sahoo and Ryad Ghanam and Xi Chen and Aiman Hanif},
  doi          = {10.1080/02664763.2021.1985091},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {231-246},
  shortjournal = {J. Appl. Stat.},
  title        = {Monitoring SEIRD model parameters using MEWMA for the COVID-19 pandemic with application to the state of qatar},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semi-parametric bayesian approach for detection of gene
expression heterosis with RNA-seq data. <em>JOAS</em>, <em>50</em>(1),
214–230. (<a
href="https://doi.org/10.1080/02664763.2021.2004581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterosis refers to the superior performance of a hybrid offspring over its two inbred parents. Although heterosis has been widely observed in agriculture, its molecular mechanism is not well studied. Recent advances in high-throughput genomic technologies such as RNA sequencing (RNA-seq) facilitate the investigation of heterosis at the gene expression level. However, it is challenging to identify genes exhibiting heterosis using RNA-seq data because high-dimension of hypotheses tests are conducted with limited sample size. Furthermore, detecting heterosis genes requires testing composite null hypotheses involving multiple mean expression levels instead of testing simple null hypotheses as in differential expression analysis. In this manuscript, we formulate a statistical model with parameters directly reflecting heterosis status, and develop a powerful test to detect heterosis genes. We employ a Bayesian framework where the RNA-seq count data are modeled through a Poisson-Gamma mixture with Dirichlet processes as priors for the distributions of the parameters of interest, the fold changes between each parent and the hybrid. Markov Chain Monte Carlo sampling with Gibbs algorithm is utilized to provide posterior inference to detect heterosis genes while controlling false discovery rate. Simulation results demonstrate that our proposed method outperformed other methods utilized to detect gene expression heterosis.},
  archive      = {J_JOAS},
  author       = {Ran Bi and Peng Liu},
  doi          = {10.1080/02664763.2021.2004581},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {214-230},
  shortjournal = {J. Appl. Stat.},
  title        = {A semi-parametric bayesian approach for detection of gene expression heterosis with RNA-seq data},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cubic rank transmuted generalized gompertz distribution:
Properties and applications. <em>JOAS</em>, <em>50</em>(1), 195–213. (<a
href="https://doi.org/10.1080/02664763.2022.2025585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new lifetime distribution as an alternative to generalized Gompertz, Gompertz distribution and its modified ones. This new distribution is a special case of the family of distributions introduced by Granzotto et al. [D.C.T. Granzotto, F. Louzada and N. Balakrishnan, Cubic rank transmuted distributions: inferential issues and applications., J. Stat. Comput. Simul. 87 (2017), pp. 2760–2778]. We obtain some characteristic properties of suggested distribution such as hazard function, ordinary moments, coefficient of skewness, coefficient of kurtosis, moment generating function, quantile function and median. We discuss three different methods of estimation to estimate the parameters of proposed distribution. A comprehensive Monte Carlo simulation study is performed in order to compare the performances of estimators according to mean square errors and biases. Finally, three real data applications are performed to illustrate usefulness of suggested distribution.},
  archive      = {J_JOAS},
  author       = {Caner Taniş and Buğra Saraçoğlu},
  doi          = {10.1080/02664763.2022.2025585},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {195-213},
  shortjournal = {J. Appl. Stat.},
  title        = {Cubic rank transmuted generalized gompertz distribution: Properties and applications},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The EWMA sign chart revisited: Performance and alternatives
without and with ties. <em>JOAS</em>, <em>50</em>(1), 170–194. (<a
href="https://doi.org/10.1080/02664763.2021.1982879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The EWMA Sign control chart is an efficient tool for monitoring shifts in a process regardless the observations&#39; underlying distribution. Recent studies have shown that, for nonparametric control charts, due to the discrete nature of the statistics being used (such as the Sign statistic), it is impossible to accurately compute their Run Length properties using Markov chain or integral equation methods. In this work, a modified nonparametric Phase II EWMA chart based on the Sign statistic is proposed and its exact Run Length properties are discussed. A continuous transformation of the Sign statistic, combined with the classical Markov Chain method, is used for the determination of the chart&#39;s in- and out-of-control Run Length properties. Additionally, we show that when ties occur due to measurement rounding-off errors, the EWMA Sign control chart is no longer distribution-free and a Bernoulli trial approach is discussed to handle the occurrence of ties and makes the proposed chart almost distribution-free. Finally, an illustrative example is provided to show the practical implementation of our proposed chart.},
  archive      = {J_JOAS},
  author       = {Theodoros Perdikis and Stelios Psarakis and Philippe Castagliola and Athanasios C. Rakitzis and Petros E. Maravelakis},
  doi          = {10.1080/02664763.2021.1982879},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {170-194},
  shortjournal = {J. Appl. Stat.},
  title        = {The EWMA sign chart revisited: Performance and alternatives without and with ties},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Misspecification of a binary dependent variable in the
logistic model controlling for the repeated longitudinal measures.
<em>JOAS</em>, <em>50</em>(1), 155–169. (<a
href="https://doi.org/10.1080/02664763.2021.1982877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many medical applications are interested to know the disease status. The disease status can be related to multiple serial measurements. Nevertheless, owing to various reasons, the binary outcome can be measured incorrectly. The estimators derived from the misspecified outcome can be biased. This paper derives the complete data likelihood function to incorporate both the multiple serial measurements and the misspecified outcome. Owing to the latent variables, EM algorithm is used to derive the maximum-likelihood estimators. Monte Carlo simulations are conducted to compare the impact of misspecification on the estimates. A retrospective data for the recurrence of atrial fibrillation is used to illustrate the usage of the proposed model.},
  archive      = {J_JOAS},
  author       = {Chun-Chao Wang and Yi-Ting Hwang and Chung-Chuan Chou and Hui-Ling Lee},
  doi          = {10.1080/02664763.2021.1982877},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {155-169},
  shortjournal = {J. Appl. Stat.},
  title        = {Misspecification of a binary dependent variable in the logistic model controlling for the repeated longitudinal measures},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new alternative quantile regression model for the bounded
response with educational measurements applications of OECD countries.
<em>JOAS</em>, <em>50</em>(1), 131–154. (<a
href="https://doi.org/10.1080/02664763.2021.1981834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a new distribution with two tuning parameters specified on the unit interval. It follows from a ‘hyperbolic secant transformation’ of a random variable following the Weibull distribution. The lack of research on the prospect of hyperbolic transformations providing flexible distributions over the unit interval is a motivation for the study. The main distributional structural properties of the new distribution are established. The different estimation methods and two simulation works have been derived for model parameters. Subsequently, we develop a related quantile regression model for further statistical perspectives. We consider two real data applications based on the educational measurements of both OECD and some non-members of OECD countries. Our regression model aims to relate the desire to get top grades on certain young students in the OECD countries with some of their Education and School Life Index such as reading performance, work environment at home, and paid work experience. It is shown that the elaborated quantile regression model has a better fitting power than famous regression models when the unit response variable possesses skewed distribution as well as two independent variables are significant in the statistical sense at any standard significance level for the median response.},
  archive      = {J_JOAS},
  author       = {Mustafa Ç. Korkmaz and Christophe Chesneau and Zehra Sedef Korkmaz},
  doi          = {10.1080/02664763.2021.1981834},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {131-154},
  shortjournal = {J. Appl. Stat.},
  title        = {A new alternative quantile regression model for the bounded response with educational measurements applications of OECD countries},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing the reliability of forecasting systems.
<em>JOAS</em>, <em>50</em>(1), 106–130. (<a
href="https://doi.org/10.1080/02664763.2021.1981833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of statistically evaluating forecasting systems is revisited. The forecaster claims the forecasts to exhibit a certain nominal statistical behaviour; for instance, the forecasts provide the expected value (or certain quantiles) of the verification, conditional on the information available at forecast time. Forecasting systems that indeed exhibit the nominal behaviour are referred to as reliable . Statistical tests for reliability are presented (based on an archive of verification–forecast pairs). As noted previously, devising such tests is encumbered by the fact that the dependence structure of the verification–forecast pairs is not known in general. Ignoring this dependence though might lead to incorrect tests and too-frequent rejection of forecasting systems that are actually reliable. On the other hand, reliability typically implies that the forecast provides information about the dependence structure, and using this in conjunction with judicious choices of the test statistic, rigorous results on the asymptotic distribution of the test statistic are obtained. These results are used to test for reliability under minimal additional assumptions on the statistical properties of the verification–forecast pairs. Applications to environmental forecasts are discussed. A python implementation of the discussed methods is available online.},
  archive      = {J_JOAS},
  author       = {J. Bröcker},
  doi          = {10.1080/02664763.2021.1981833},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {106-130},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing the reliability of forecasting systems},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inferences for multiple interval type-i censoring scheme.
<em>JOAS</em>, <em>50</em>(1), 86–105. (<a
href="https://doi.org/10.1080/02664763.2021.1981832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we have introduced a new type of censoring scheme named the multiple interval type-I censoring scheme. Further, We have assumed that the test units are drawn from the Weibull population. We have also proposed the maximum product of spacing estimators for unknown parameters under the multiple interval type-I censoring scheme and compare them with the existing maximum likelihood estimators. In addition to this, the Bayes estimators for shape and scale parameters are also obtained under the squared error loss function. Their corresponding asymptotic confidence/credible intervals are also discussed. A real data set containing the breakdown time of insulating fluids are used to demonstrate the appropriateness of the proposed methodology.},
  archive      = {J_JOAS},
  author       = {Shubham Agnihotri and Sanjay Kumar Singh and Umesh Singh},
  doi          = {10.1080/02664763.2021.1981832},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {86-105},
  shortjournal = {J. Appl. Stat.},
  title        = {Inferences for multiple interval type-I censoring scheme},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elastic statistical analysis of interval-valued time series.
<em>JOAS</em>, <em>50</em>(1), 60–85. (<a
href="https://doi.org/10.1080/02664763.2021.1981257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the problem of statistical analysis of interval-valued time series data – two nonintersecting real-valued functions, representing lower and upper limits, over a period of time. Specifically, we pay attention to the two concepts of phase (or horizontal) variability and amplitude (or vertical) variability, and propose a phase-amplitude separation method. We view interval-valued time series as elements of a function (Hilbert) space and impose a Riemannian structure on it. We separate phase and amplitude variability in observed interval functions using a metric-based alignment solution. The key idea is to map an interval to a point in R 2 , view interval-valued time series as parameterized curves in R 2 , and borrow ideas from elastic shape analysis of planar curves, including PCA, to perform registration, summarization, analysis, and modeling of multiple series. The proposed phase-amplitude separation provides a new way of PCA and modeling for interval-valued time series, and enables shape clustering of interval-valued time series. We apply this framework to three different applications, including finance, meteorology and physiology, proves the effectiveness of proposed methods, and discovers some underlying patterns in the data. Experimental results on simulated data show that our method applies to the point-valued time series.},
  archive      = {J_JOAS},
  author       = {Honggang Zhang and Jingyong Su and Linlin Tang and Anuj Srivastava},
  doi          = {10.1080/02664763.2021.1981257},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {60-85},
  shortjournal = {J. Appl. Stat.},
  title        = {Elastic statistical analysis of interval-valued time series},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Jointly modelling multiple transplant outcomes by a
competing risk model via functional principal component analysis.
<em>JOAS</em>, <em>50</em>(1), 43–59. (<a
href="https://doi.org/10.1080/02664763.2021.1981256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many clinical studies, longitudinal biomarkers are often used to monitor the progression of a disease. For example, in a kidney transplant study, the glomerular filtration rate (GFR) is used as a longitudinal biomarker to monitor the progression of the kidney function and the patient&#39;s state of survival that is characterized by multiple time-to-event outcomes, such as kidney transplant failure and death. It is known that the joint modelling of longitudinal and survival data leads to a more accurate and comprehensive estimation of the covariates&#39; effect. While most joint models use the longitudinal outcome as a covariate for predicting survival, very few models consider the further decomposition of the variation within the longitudinal trajectories and its effect on survival. We develop a joint model that uses functional principal component analysis (FPCA) to extract useful features from the longitudinal trajectories and adopt the competing risk model to handle multiple time-to-event outcomes. The longitudinal trajectories and the multiple time-to-event outcomes are linked via the shared functional features. The application of our model on a real kidney transplant data set reveals the significance of these functional features, and a simulation study is carried out to validate the accurateness of the estimation method.},
  archive      = {J_JOAS},
  author       = {Jianghu (James) Dong and Haolun Shi and Liangliang Wang and Ying Zhang and Jiguo Cao},
  doi          = {10.1080/02664763.2021.1981256},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {43-59},
  shortjournal = {J. Appl. Stat.},
  title        = {Jointly modelling multiple transplant outcomes by a competing risk model via functional principal component analysis},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monitoring process mean and dispersion with one double
generally weighted moving average control chart. <em>JOAS</em>,
<em>50</em>(1), 19–42. (<a
href="https://doi.org/10.1080/02664763.2021.1980506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control charts are widely known quality tools used to detect and control industrial process deviations in Statistical Process Control. In the current paper, we propose a new single memory-type control chart, called the maximum double generally weighted moving average chart (referred as Max-DGWMA), that simultaneously detects shifts in the process mean and/or process dispersion. The run length performance of the proposed Max-DGWMA chart is compared with that of the Max-EWMA, Max-DEWMA, Max-GWMA and SS-DGWMA charts, using time-varying control limits, through Monte–Carlo simulations. The comparisons reveal that the proposed chart is more efficient than the Max-EWMA, Max-DEWMA and Max-GWMA charts, while it is comparable with the SS-DGWMA chart. An automotive industry application is presented in order to implement the Max-DGWMA chart. The goal is to establish statistical control of the manufacturing process of the automobile engine piston rings. The source of the out-of-control signals is interpreted and the efficiency of the proposed chart in detecting shifts faster is evident.},
  archive      = {J_JOAS},
  author       = {Kashinath Chatterjee and Christos Koukouvinos and Angeliki Lappa},
  doi          = {10.1080/02664763.2021.1980506},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {19-42},
  shortjournal = {J. Appl. Stat.},
  title        = {Monitoring process mean and dispersion with one double generally weighted moving average control chart},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing for genetic mutation of seasonal influenza virus.
<em>JOAS</em>, <em>50</em>(1), 1–18. (<a
href="https://doi.org/10.1080/02664763.2021.1978955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influenza virus strains undergo genetic mutations every year and these changes in genetic makeup pose difficulties for effective vaccine selection. To better understand the problem it is important to statistically quantify the amount of genetic change between circulating strains from different years. In this paper, we propose the nonparametric crossmatch test applied to phylogenetic trees to assess the level of discrepancy between circulating flu virus strains between two years; the viruses being represented by a phylogenetic tree. The crossmatch test has advantages compared to parametric tests in that it preserves more information in the data. The outcome of the test would indicate whether the circulating influenza virus has mutated sufficiently in the past year to be considered as a new population of virus, suggesting the need to consider a new vaccine. We validate the test on simulated phylogenetic tree samples with varying branch lengths, as well as with publicly available virus sequence data from the ‘Global Initiative on Sharing All Influenza Data’ (GISAID: https://www.gisaid.org/ )},
  archive      = {J_JOAS},
  author       = {Vera Liu and Stephen Walker},
  doi          = {10.1080/02664763.2021.1978955},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing for genetic mutation of seasonal influenza virus},
  volume       = {50},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
