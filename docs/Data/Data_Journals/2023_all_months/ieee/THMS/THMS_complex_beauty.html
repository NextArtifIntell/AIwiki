<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms---102">THMS - 102</h2>
<ul>
<li><details>
<summary>
(2023). Mouth cavity visual analysis based on deep learning for
oropharyngeal swab robot sampling. <em>THMS</em>, <em>53</em>(6),
1083–1092. (<a href="https://doi.org/10.1109/THMS.2023.3309256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual analysis of the mouth cavity plays a significant role in the pathogen specimen sampling and disease diagnosis of the mouth cavity. Aiming at performance defects of general detectors based on deep learning in detecting mouth cavity components, this article proposes a mouth cavity analysis network (MCNet), which is an instance segmentation method with spatial features, and a mouth cavity dataset (MCData), which is the first available dataset for mouth cavity detecting and segmentation. First, given the lack of a mouth cavity image dataset, the MCData for detecting and segmenting key parts in the mouth cavity was developed for model training and testing. Second, the MCNet was designed based on the mask region-based convolutional neural network. To improve the performance of feature extraction, a parallel multiattention module was designed. Besides, to solve low detection accuracy of small-sized objects, a multiscale region proposal network structure was designed. Then, the mouth cavity spatial structure features were introduced, and the detection confidence could be refined to increase the detection accuracy. The MCNet achieved 81.5% detection accuracy and 78.1% segmentation accuracy (intersection over union = 0.50:0.95) on the MCData. Comparative experiments with the MCData showed that the proposed MCNet outperformed state-of-the-art approaches with the task of mouth cavity instance segmentation. In addition, the MCNet has been used in an oropharyngeal swab robot for COVID-19 oropharyngeal sampling.},
  archive      = {J_THMS},
  author       = {Qing Gao and Zhaojie Ju and Yongquan Chen and Tianwei Zhang and Yuquan Leng},
  doi          = {10.1109/THMS.2023.3309256},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1083-1092},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Mouth cavity visual analysis based on deep learning for oropharyngeal swab robot sampling},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Oropharynx visual detection by using a multi-attention
single-shot multibox detector for human–robot collaborative oropharynx
sampling. <em>THMS</em>, <em>53</em>(6), 1073–1082. (<a
href="https://doi.org/10.1109/THMS.2023.3324664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pandemic of COVID-19 has increased the demand for the oropharynx sampling robots. For an automatic oropharynx sampling, detection and localization of the oropharynx objects are essential. First, in response to the small-object and real-time needs of visual oropharynx detection, a lightweight multi-attention single-shot multibox detector (MASSD) method is designed. This method can effectively improve the detection accuracy of oropharynx sampling regions, especially small regions, while ensuring sufficient speed by introducing spatial attention, channel attention, and feature fusion mechanisms into the single-shot multibox detector. Second, the proposed MASSD is applied to an oropharyngeal swab (OP-swab) robot system to detect oropharynx sampling regions and conduct autonomous sampling. In the experiment, training and validation based on a custom oropharynx dataset verify the effectiveness and efficiency of the proposed MASSD. The detection accuracy can reach 81.3% of mean average precision@0.5:0.95 at 104 frames per second and the application experiment on the OP-swab robot system performs oropharynx sampling with 100% success accuracy in human–robot collaboration strategy.},
  archive      = {J_THMS},
  author       = {Qing Gao and Yongquan Chen and Zhaojie Ju},
  doi          = {10.1109/THMS.2023.3324664},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1073-1082},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Oropharynx visual detection by using a multi-attention single-shot multibox detector for Human–Robot collaborative oropharynx sampling},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast, accurate, but sometimes too-compelling support: The
impact of imperfectly automated cues in an augmented-reality
head-mounted display on visual search performance. <em>THMS</em>,
<em>53</em>(6), 1061–1072. (<a
href="https://doi.org/10.1109/THMS.2023.3302152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the visual search for targets in a complex scene might benefit from using augmented-reality (AR) head-mounted display (HMD) technologies by helping to efficiently direct human attention, imperfectly reliable automation support could manifest in occasional errors. The current study examined the effectiveness of different HMD cues that might support visual search performance and their respective consequences following automation errors. A total of 56 participants searched a three-dimensional environment containing 48 objects in a room, in order to locate a target object that was viewed prior to each trial. They searched either unaided or assisted by one of the three HMD types of cues: an arrow pointing to the target, a plan-view minimap highlighting the target, and a constantly visible icon depicting the appearance of the target object. The cue was incorrect in 17% of the trials for one group of participants and 100% correct for the second group. Through both analysis and modeling of both search speed and accuracy, the results indicated that the arrow and minimap cues depicting location information were more effective than the icon cue depicting visual appearance, both overall, and when the cue was correct. However, there was a tradeoff on the infrequent occasions when the cue erred. The most effective AR-based cue led to a greater automation bias in which the cue was more often blindly followed without careful examination of the raw images. The results speak to the benefits of AR and the need to examine potential costs when AR-conveyed information may be incorrect because of imperfectly reliable systems.},
  archive      = {J_THMS},
  author       = {Amelia C. Warden and Christopher D. Wickens and Daniel Rehberg and Francisco R. Ortega and Benjamin A. Clegg},
  doi          = {10.1109/THMS.2023.3302152},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1061-1072},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Fast, accurate, but sometimes too-compelling support: The impact of imperfectly automated cues in an augmented-reality head-mounted display on visual search performance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Augmented-reality-based human memory enhancement using
artificial intelligence. <em>THMS</em>, <em>53</em>(6), 1048–1060. (<a
href="https://doi.org/10.1109/THMS.2023.3307397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a human memory augmentation system that uses augmented reality (AR), computer vision (CV), and artificial intelligence to replace the internal mental representation of objects in the environment with an external augmented representation. The system consists of two components: 1) an AR headset; and 2) a computing station. The AR headset runs an application that senses the indoor environment, sends data to the computing station for processing, receives the processed data, and updates the external representation of objects using a virtual 3-D object projected into the real environment in front of the user&#39;s eyes. The computing station performs CV-based indoor environment self-localization, object detection, and object-to-location binding using first-person view data received from the AR headset. We designed a behavioral study to evaluate the usability of the system. In a pilot study with 26 participants (12 females and 14 males), we investigated human performance in an experimental task that involved remembering the positions of objects in a physical space and displaying the positions of the learned objects on the 2-D map of the space. We conducted the studies under two conditions—that is, with and without using the AR system. We investigated the usability of the system, subjective workload, and performance variables under both conditions. The results showed that the AR-based augmentation of the mental representation of objects indoors reduced cognitive load and increased performance accuracy.},
  archive      = {J_THMS},
  author       = {Zhanat Makhataeva and Tolegen Akhmetov and Huseyin Atakan Varol},
  doi          = {10.1109/THMS.2023.3307397},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1048-1060},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Augmented-reality-based human memory enhancement using artificial intelligence},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human–machine interactive learning method based on active
learning for smart workshop dynamic scheduling. <em>THMS</em>,
<em>53</em>(6), 1038–1047. (<a
href="https://doi.org/10.1109/THMS.2023.3308614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of dynamic scheduling, workers and scheduling models (SMs) play a crucial role in decision-making. Workers are able to help SM training by sample labeling, thereby enhancing the decision-making ability of SMs. However, existing supervised learning methods require a large number of labeled samples to train SMs, which limits the learning efficiency between workers and SMs. In this article, a human-machine interactive learning method based on active learning (HMILM/AL) is proposed. The method introduces active learning (AL) techniques to reduce labeling costs and improve learning efficiency. Referring to the AL framework, only a small subset of samples are selected from an unlabeled dataset and are labeled by workers, to train SMs. To further reduce labeling costs, sample selection, the key to the HMILM/AL, is improved by two strategies. First, a novel hybrid selection strategy (NHSS) is developed. By identifying and selecting more useful samples in an unlabeled dataset, the NHSS promotes efficient use of workers, and reduces labeling costs. Second, an enhanced NHSS (E-NHSS) is proposed, which considers both the difficulty of labeling samples and the usefulness of the samples. It reduces labeling costs by selecting easily labeled samples as much as possible. Finally, the proposed method is evaluated through experiments conducted in a real smart workshop. The results demonstrate that the HMILM/AL is very competitive compared with existing supervised learning methods. Moreover, both the NHSS and the E-NHSS can reduce labeling costs efficiently.},
  archive      = {J_THMS},
  author       = {Dongyuan Wang and Liuen Guan and Juan Liu and Chen Ding and Fei Qiao},
  doi          = {10.1109/THMS.2023.3308614},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1038-1047},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human–Machine interactive learning method based on active learning for smart workshop dynamic scheduling},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised human activity recognition with localized
time-frequency contrastive representation learning. <em>THMS</em>,
<em>53</em>(6), 1027–1037. (<a
href="https://doi.org/10.1109/THMS.2023.3303438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a self-supervised learning solution for human activity recognition with smartphone accelerometer data. We aim to develop a model that learns strong representations from accelerometer signals, in order to perform robust human activity classification, while reducing the model&#39;s reliance on class labels. Specifically, we intend to enable cross-dataset transfer learning such that our network pretrained on a particular dataset can perform effective activity classification on other datasets (successive to a small amount of fine-tuning). To tackle this problem, we design our solution with the intention of learning as much information from the accelerometer signals as possible. As a result, we design two separate pipelines, one that learns the data in time-frequency domain, and the other in time-domain alone. In order to address the issues mentioned above in regards to cross-dataset transfer learning, we use self-supervised contrastive learning to train each of these streams. Next, each stream is fine-tuned for final classification, and eventually the two are fused to provide the final results. We evaluate the performance of the proposed solution on three datasets, namely, MotionSense, human activities and postural transitions data (HAPT), and heterogeneity human activity recognition (HHAR), and demonstrate that our solution outperforms prior works in this field achieving F1 scores 0.934, 0.911, and 0.826, respectively. We further evaluate the performance of the method in learning generalized features, by using MobiAct dataset for pretraining and the remaining three datasets for the downstream classification task, and show that the proposed solution achieves better performance in comparison with other self-supervised methods in cross-dataset transfer learning.},
  archive      = {J_THMS},
  author       = {Setareh Rahimi Taghanaki and Michael Rainbow and Ali Etemad},
  doi          = {10.1109/THMS.2023.3303438},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1027-1037},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Self-supervised human activity recognition with localized time-frequency contrastive representation learning},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NLP-crowdsourcing hybrid framework for inter-researcher
similarity detection. <em>THMS</em>, <em>53</em>(6), 1017–1026. (<a
href="https://doi.org/10.1109/THMS.2023.3319290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizing and examining the intellectual landscape and evolution of scientific communities to support collaboration is crucial for multiple research purposes. In some cases, measuring similarities and matching patterns between research publication document sets can help to identify people with similar interests for building research collaboration networks and university–industry linkages. The premise of this work is assessing feasibility for resolving ambiguous cases in similarity detection to determine authorship with natural language processing (NLP) techniques so that crowdsourcing is applied only in instances that require human judgment. Using an NLP-crowdsourcing convergence strategy, we can reduce the costs of microtask crowdsourcing while saving time and maintaining disambiguation accuracy over large datasets. This article contributes a next-gen crowd-artificial intelligence framework that used an ensemble of term frequency-inverse document frequency and bidirectional encoder representation from transformers to obtain similarity rankings for pairs of scientific documents. A sequence of content-based similarity tasks was created using a crowd-powered interface for solving disambiguation problems. Our experimental results suggest that an adaptive NLP-crowdsourcing hybrid framework has advantages for inter-researcher similarity detection tasks where fully automatic algorithms provide unsatisfactory results, with the goal of helping researchers discover potential collaborators using data-driven approaches.},
  archive      = {J_THMS},
  author       = {António Correia and Diogo Guimarães and Hugo Paredes and Benjamim Fonseca and Dennis Paulino and Luís Trigo and Pavel Brazdil and Daniel Schneider and Andrea Grover and Shoaib Jameel},
  doi          = {10.1109/THMS.2023.3319290},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1017-1026},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {NLP-crowdsourcing hybrid framework for inter-researcher similarity detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-belt versus split-belt: Intelligent treadmill control
via microphase gait capture for poststroke rehabilitation.
<em>THMS</em>, <em>53</em>(6), 1006–1016. (<a
href="https://doi.org/10.1109/THMS.2023.3327661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke is the leading long-term disability and causes a significant financial burden associated with rehabilitation. In poststroke rehabilitation, individuals with hemiparesis have a specialized demand for coordinated movement between the paretic and the nonparetic legs. The split-belt treadmill can effectively facilitate the paretic leg by slowing down the belt speed for that leg while the patient is walking on a split-belt treadmill. Although studies have found that split-belt treadmills can produce better gait recovery outcomes than traditional single-belt treadmills, the high cost of split-belt treadmills is a significant barrier to stroke rehabilitation in clinics. In this article, we design an AI-based system for the single-belt treadmill to make it act like a split-belt by adjusting the belt speed instantaneously according to the patient&#39;s microgait phases. This system only requires a low-cost RGB camera to capture human gait patterns. A novel microgait classification pipeline model is used to detect gait phases in real time. The pipeline is based on self-supervised learning that can calibrate the anchor video with the real-time video. We then use a ResNet-LSTM module to handle temporal information and increase accuracy. A real-time filtering algorithm is used to smoothen the treadmill control. We have tested the developed system with 34 healthy individuals and four stroke patients. The results show that our system is able to detect the gait microphase accurately and requires less human annotation in training, compared to the ResNet50 classifier. Our system “Splicer” is boosted by AI modules and performs comparably as a split-belt system, in terms of timely varying left/right foot speed, creating a hemiparetic gait in healthy individuals, and promoting paretic side symmetry in force exertion for stroke patients. This innovative design can potentially provide cost-effective rehabilitation treatment for hemiparetic patients.},
  archive      = {J_THMS},
  author       = {Shengting Cao and Mansoo Ko and Chih-Ying Li and David Brown and Xuefeng Wang and Fei Hu and Yu Gan},
  doi          = {10.1109/THMS.2023.3327661},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {1006-1016},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Single-belt versus split-belt: Intelligent treadmill control via microphase gait capture for poststroke rehabilitation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A state-space control approach for tracking isometric grip
force during BMI enabled neuromuscular stimulation. <em>THMS</em>,
<em>53</em>(6), 996–1005. (<a
href="https://doi.org/10.1109/THMS.2023.3316185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sixty percent of elderly hand movements involve grasping, which is unarguably why grasp restoration is a major component of upper-limb rehabilitation therapy. Neuromuscular electrical stimulation is effective in assisting grasping, but challenges around patient engagement and control, as well as poor movement regulation due to fatigue and muscle nonlinearity continue to hinder its adoption for clinical applications. In this study, we integrate an electroencephalography-based brain–machine interface (BMI) with closed-loop neuromuscular stimulation to restore grasping and evaluate its performance using an isometric force tracking task. After three sessions, it was concluded that the normalized tracking error during closed-loop stimulation using a state-space feedback controller (25 ± 15%), was significantly smaller than conventional open-loop stimulation (31 ± 24%), ( F (748.03, 1) = 23.22, p &lt; 0.001). Also, the impaired study participants were able to achieve a BMI classification accuracy of 65 ± 10% while able-bodied participants achieved 57 ± 18% accuracy, which suggests the proposed closed-loop system is more capable of engaging patients for rehabilitation. These findings demonstrate the multisession performance of model-based feedback-controlled stimulation, without requiring frequent reconfiguration.},
  archive      = {J_THMS},
  author       = {Nikunj A. Bhagat and Gerard E. Francisco and Jose L. Contreras-Vidal},
  doi          = {10.1109/THMS.2023.3316185},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {996-1005},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A state-space control approach for tracking isometric grip force during BMI enabled neuromuscular stimulation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessment of upper-body movement quality in the
cartesian-space is feasible in the harmony exoskeleton. <em>THMS</em>,
<em>53</em>(6), 985–995. (<a
href="https://doi.org/10.1109/THMS.2023.3305391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To determine the most effective interventions for poststroke patients, it is imperative to monitor the recovery process. Robotic exoskeletons&#39; built-in sensing capabilities enable accurate kinematic measurement with no additional setup time. Although position sensors used in exoskeletons are accurate, a mismatch between the robot&#39;s and the human&#39;s joints can lead to inaccurate measurements. In addition, the robot&#39;s residual dynamics can interfere with human&#39;s natural movements and the kinematic metrics assessed in the robot would not be representative of the human&#39;s movement in free-motion. So far, the accuracy of robotic exoskeletons in assessing upper-body kinematics has not been verified. The bilateral upper-body Harmony exoskeleton has features favorable to minimize joint misalignments and the robot&#39;s residual dynamics. In this study, we examined Harmony&#39;s ability to accurately assess Cartesian-space kinematic parameters associated with the wearer&#39;s movement quality. We analyzed data collected from eight healthy participants that executed point-to-point movements with and without the presence of the robot and at fast and slow speeds. Ground truth was acquired with an optical motion capture, and we extracted the kinematic parameters from the measured data. The results suggest that Harmony can accurately measure kinematic parameters associated with movement quality, and these parameters could appropriately reflect wearer&#39;s natural movements at a slow speed. Therefore, Harmony could aid the evaluation of the effectiveness of different interventions, which is more sensitive and efficient than currently adopted clinical outcomes. This allows for individualization of a treatment plan and a detailed follow-up.},
  archive      = {J_THMS},
  author       = {Ana C. De Oliveira and Ashish D. Deshpande},
  doi          = {10.1109/THMS.2023.3305391},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {985-995},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Assessment of upper-body movement quality in the cartesian-space is feasible in the harmony exoskeleton},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human–robot interaction video sequencing task (HRIVST) for
robot’s behavior legibility. <em>THMS</em>, <em>53</em>(6), 975–984. (<a
href="https://doi.org/10.1109/THMS.2023.3327132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People&#39;s acceptance and trust in robots are a direct consequence of people&#39;s ability to infer and predict the robot&#39;s behavior. However, there is no clear consensus on how the legibility of a robot&#39;s behavior and explanations should be assessed. In this work, the construct of the Theory of Mind (i.e., the ability to attribute mental states to others) is taken into account and a computerized version of the theory of mind picture sequencing task is presented. Our tool, called the human–robot interaction (HRI) video sequencing task (HRIVST), evaluates the legibility of a robot&#39;s behavior toward humans by asking them to order short videos to form a logical sequence of the robot&#39;s actions. To validate the proposed metrics, we recruited a sample of 86 healthy subjects. Results showed that the HRIVST has good psychometric properties and is a valuable tool for assessing the legibility of robot behaviors. We also evaluated the effects of symbolic explanations, the presence of a person during the interaction, and the humanoid appearance. Results showed that the interaction condition had no effect on the legibility of the robot&#39;s behavior. In contrast, the combination of humanoid robots and explanations seems to result in a better performance of the task.},
  archive      = {J_THMS},
  author       = {Silvia Rossi and Alessia Coppola and Mariachiara Gaita and Alessandra Rossi},
  doi          = {10.1109/THMS.2023.3327132},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {975-984},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human–Robot interaction video sequencing task (HRIVST) for robot&#39;s behavior legibility},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the impact of time-to-collision constraint and
head gaze on usability for robot navigation in a corridor.
<em>THMS</em>, <em>53</em>(6), 965–974. (<a
href="https://doi.org/10.1109/THMS.2023.3314894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Navigation of robots among humans is still an open problem, especially in confined locations (e.g. narrow corridors, doors). This article aims at finding how an anthropomorphic robot, like a PR2 robot with a height of 1.33 m, should behave when crossing a human in a narrow corridor in order to increase its usability. Two experiments studied how a combination of robot head behavior and navigation strategy can enhance robot legibility. Experiment 1 aimed to measure where a pedestrian looks when crossing another pedestrian, comparing the nature of the pedestrian: human or a robot. Based on the results of this experiment and the literature, we then designed a robot behavior exhibiting mutual manifestness by both modifying its trajectory to be more legible, and using its head to glance at the human. Experiment 2 evaluated this behavior in real situations of pedestrians crossing a robot. The visual behavior and user experience of pedestrians were assessed. The first experiment revealed that humans primarily look at the robot&#39;s head just before crossing. The second experiment showed that when crossing a human in a narrow corridor, both modifying the robot trajectory and glancing at the human is necessary to significantly increase the usability of the robot. We suggest using mutual manifestness is crucial for an anthropomorphic robot when crossing a human in a corridor. It should be conveyed both by altering the trajectory and by showing the robot awareness of the human presence through the robot head motion. Small changes in robot trajectory and manifesting robot perception of the human via a user identified robot head can avoid users&#39; hesitation and feeling of threat.},
  archive      = {J_THMS},
  author       = {Guilhem Buisan and Nathan Compan and Loïc Caroux and Aurélie Clodic and Ophélie Carreras and Camille Vrignaud and Rachid Alami},
  doi          = {10.1109/THMS.2023.3314894},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {965-974},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Evaluating the impact of time-to-collision constraint and head gaze on usability for robot navigation in a corridor},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel interval type-2 fuzzy classifier based on
explainable neural network for surface electromyogram gesture
recognition. <em>THMS</em>, <em>53</em>(6), 955–964. (<a
href="https://doi.org/10.1109/THMS.2023.3310524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing hand gesture classification research based on surface electromyogram (sEMG) faces the challenges of low classification accuracy, weak real-time ability, weak robustness, few categories, and lack of explainability. In this article, we investigate how to classify sEMG signals for grasp recognition and human–robot interaction to consider these issues. A novel interval type-2 (IT2) fuzzy classifier based on explainable neural network is proposed for sEMG gesture recognition. Based on fully connected neural network, the adaptive moment estimation is applied to tune the antecedent parameters. The Ninapro data is adopted to test the performance of the proposed model, which realizes recognition of 52 gestures and achieves 95.04% categorization accuracy. Moreover, grasping experiments are conducted on computer, communication, and consumer electronics (3C) experiment platform to test the ability of the classifier in real scenarios. The experiment recognizes six gestures. The results of the 3C grasping experiment show that the proposed method achieves 99.4% offline training accuracy as well as 96.07% online test accuracy. Meanwhile, 89.4% of the classification results can be obtained within 0.5 s. The overall results demonstrate great potential for real-world applications, such as human intent detection and manipulator control.},
  archive      = {J_THMS},
  author       = {Shuai Lv and Zhijun Li and Jin Huang and Peng Shi},
  doi          = {10.1109/THMS.2023.3310524},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {955-964},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A novel interval type-2 fuzzy classifier based on explainable neural network for surface electromyogram gesture recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PC-GNN: Pearson correlation-based graph neural network for
recognition of human lower limb activity using sEMG signal.
<em>THMS</em>, <em>53</em>(6), 945–954. (<a
href="https://doi.org/10.1109/THMS.2023.3319356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence has a plethora of applications in the realm of biomedical sciences, such as pattern recognition, diagnosis of disease, human–machine interaction, medical image processing, robotic limbs, or exoskeletons. Robotic limbs, or exoskeletons, are widely employed to assist with lower limb movement. To increase the exoskeleton&#39;s flexibility in the lower extremities, it is critical to recognize the diverse motion intents of the lower limbs of the human body. In this investigation, sEMG signals from lower limb muscles are used for a computer-aided recognition system to correctly identify the lower limb activities because these signals can identify movement ahead of time and enable faster detection of signal fluctuation than other wearable sensors. Several types of noise are introduced into the signal during collection. A multistage classification strategy is proposed to overcome the processing challenges associated with these sEMG signals. Initially, nine time-domain handcrafted features are retrieved using a hybrid of wavelet denoising and ensemble empirical mode decomposition approach with a sliding window of 256 ms and a 25% overlap. Next, a Pearson correlation-based graph is formed from the extracted features and applied to a graph neural network (GNN). GNN not only captures individual information but also makes use of information from other samples to form a graph. The combination of a Pearson correlation-based graph with a GNN is referred to as Pearson correlation-based GNN. The observation states that the approach proposed in the research achieved an accuracy of 99.19%, 99.02%, 96.21% for the walking, sitting, and standing of healthy subjects, while 99.29%, 97.97%, 99.36% for the subjects comprising knee abnormalities, respectively.},
  archive      = {J_THMS},
  author       = {Ankit Vijayvargiya and Rajesh Kumar and Parul Sharma},
  doi          = {10.1109/THMS.2023.3319356},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {945-954},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {PC-GNN: Pearson correlation-based graph neural network for recognition of human lower limb activity using sEMG signal},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A layered sEMG–FMG hybrid sensor for hand motion recognition
from forearm muscle activities. <em>THMS</em>, <em>53</em>(5), 935–944.
(<a href="https://doi.org/10.1109/THMS.2023.3287594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The activities of muscles in the forearm have been widely investigated to develop human interfaces involving hand motions, especially in the fields of prosthetic hands and teleoperation. Although surface electromyography (sEMG) is considered as an effective biological signal from which hand motions can be recognized, the availability and quality of sEMG data can limit the usability and intuitiveness of human interfaces. This article introduces force myography (FMG) as a supplementary signal and proposes a layered sEMG–FMG hybrid sensor that can measure both sEMG and FMG at the same skin surface location. Meanwhile, a layer fusion convolution neural network (LFC) is designed to extract multiscale features from sEMG and FMG. To evaluate the effectiveness of the hybrid sEMG–FMG sensor and LFC, a 22-hand motion classification experiment was conducted on nine able-bodied subjects. The recognition results indicated a significantly improved classification accuracy (p &lt; 0.001) of the hybrid sEMG–FMG modality with respect to single sEMG or FMG modality. The classification accuracies (CAs) of LFC were compared with conventional machine learning methods, including support vector machine, random forest classifier, xgboost, and k-nearest neighbor. Compared with the single-modality sEMG, the CAs of the dual-modality sEMG–FMG using conventional methods, and LFC were improved by 21.31% and 16.71%, respectively. These results suggest that the layered sEMG–FMG sensing approach can effectively enhance the performance of human interfaces, which offers great potential in the clinical applications of sophisticated prosthetic hands and teleoperation.},
  archive      = {J_THMS},
  author       = {Peiji Chen and Ziye Li and Shunta Togo and Hiroshi Yokoi and Yinlai Jiang},
  doi          = {10.1109/THMS.2023.3287594},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {935-944},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A layered sEMG–FMG hybrid sensor for hand motion recognition from forearm muscle activities},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mapping intrinsic and extrinsic muscle myoelectric activity
during natural dynamic movements into finger and wrist kinematics using
deep learning prediction models. <em>THMS</em>, <em>53</em>(5), 924–934.
(<a href="https://doi.org/10.1109/THMS.2023.3302613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: We investigate the use of high-density surface electromyographic (HDsEMG) recordings of intrinsic hand muscles, along with those from extrinsic muscles, on finger and wrist kinematic prediction performance. We incorporate these HDsEMG signals using a framework based on a custom hybrid convolutional-recurrent deep learning model. Methods: Five healthy subjects performed a wide variety of motion tasks activating multiple degrees of freedom of the wrist and fingers. During the tasks, HDsEMG signals were recorded from extrinsic and intrinsic muscles of the hand while motion capture technology tracked the hand/wrist kinematics. A convolutional-recurrent model architecture was designed and trained on the recorded dataset, incorporating both residual connections as well as inception convolutional structures. Results: The proposed model led to greater regression accuracy over the simultaneous prediction of 12 joint angles (correlation coefficient, mean absolute error, and root mean squared error of 0.850, 4.84°, and 11.2°, respectively) than previously proposed mapping models, when incorporating both intrinsic and extrinsic muscle signals. The inclusion of both sets of hand muscles also led to statistically greater performance than the same model trained on only extrinsic muscle data. Conclusion: We show accurate predictions of hand/wrist kinematics from combined extrinsic and intrinsic hand muscle myoelectric activity, using a convolutional-recurrent hybrid deep learning model. This greater performance is replicated over several subjects and across multidegree of freedom motion tasks. Significance: Our developed system (electrode setup and deep neural networks) can be translated into a compact wearable interface in the future for medical as well as consumer applications.},
  archive      = {J_THMS},
  author       = {Marcus Panchal and Simone Tanzarella and Moon Ki Jung and Dario Farina},
  doi          = {10.1109/THMS.2023.3302613},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {924-934},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Mapping intrinsic and extrinsic muscle myoelectric activity during natural dynamic movements into finger and wrist kinematics using deep learning prediction models},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-brain coding expands the instruction set in
SSVEP-based brain-computer interfaces. <em>THMS</em>, <em>53</em>(5),
915–923. (<a href="https://doi.org/10.1109/THMS.2023.3273538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies have made great efforts to expand the instruction set in steady-state visual evoked potential (SSVEP)-based brain-computer interfaces. However, most systems are limited to single persons and expand the instruction set by increasing the flicker stimulation frequency range or via multiple frequencies sequential coding or joint frequency/phase coding. In this article, we propose a multibrain coding SSVEP paradigm that encodes the SSVEP instructions generated by N independent subjects with M flicker stimuli, thus increasing the instruction set to MN instructions. A total of 40 subjects participated in online experiments in this article. The results show that there is no significant difference in accuracy ( p &gt; 0.05, paired t test) between the multibrain and single-brain coding systems, while the information transfer rate (ITR) increases significantly ( p &lt; 0.0001, paired t test), with the ITR increasing from 37.66 ± 3.60 bits/min for the single-brain coding system to 111.65 ± 10.84 bits/min for multibrain coding system when N = 3 and M = 5. In summary, the proposed multibrain coding paradigm exponentially increases the number of instructions without increasing the output instruction time, which is of great significance to the application and promotion of BCIs.},
  archive      = {J_THMS},
  author       = {Xingxing Chu and Yang Yu and Kaixuan Liu and Zeqi Ye and Dewen Hu and Ling-Li Zeng},
  doi          = {10.1109/THMS.2023.3273538},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {915-923},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Multi-brain coding expands the instruction set in SSVEP-based brain-computer interfaces},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of smart clothing with automatic cardiovascular
diseases detection. <em>THMS</em>, <em>53</em>(5), 905–914. (<a
href="https://doi.org/10.1109/THMS.2023.3297603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiogram (ECG) is one of the most important information for cardiovascular diseases (CVDs) diagnosis. In recent year, several dry electrode-based smart clothes have been widely developed to improve the skin allergic reaction and gel-drying issue from conventional Ag/AgCl electrode under long-term measurement. However, most of these dry electrodes still have to contact with skin and may encounter the risk of skin irritation, and many smart clothing systems lack of automatic CVDs detection. In this article, a novel smart clothing was designed to automatically detect CVDs in daily life. Based on the technique of capacitive electrodes, the proposed smart clothing could access the bio-potential across the clothes to prevent the skin from irritation and discomfort, and could adapt to different body sizes by the specific belt mechanical design. Moreover, the CVDs detection algorithm was also designed and implemented in the field programmable gate array (FPGA) based ECG analysis module. The experiment results show that the proposed smart clothing could effectively real-time extract ECG features (P-, R-, and T-waves) and detect CVDs state via the front-end circuit, including bradycardia, tachycardia, atrial fibrillation, left ventricular hypertrophy, first-degree atrioventricular block, and hyperkalemia. The proposed FPGA architecture is also beneficial for future revisions or additions of CVD algorithms to improve more accurate diagnosis and monitoring of heart disease. It might reduce huge ECG data collected in daily life via only transmitting the abnormal ECG segment, and improve the diagnostic efficiency of CVDs in the future.},
  archive      = {J_THMS},
  author       = {Wei-Ting Chang and Bor-Shing Lin and Yung-Lin Chen and Heng-Yin Chen and Chengyu Liu and Yi-Ting Hwang and Bor-Shyh Lin},
  doi          = {10.1109/THMS.2023.3297603},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {905-914},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Design of smart clothing with automatic cardiovascular diseases detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of foot motions for interaction with exergames
using shoe-mounted inertial sensors. <em>THMS</em>, <em>53</em>(5),
895–904. (<a href="https://doi.org/10.1109/THMS.2023.3292902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inertial sensors are widely used to measure human movement. Although inertial sensors have been successfully applied to exergaming in the past, the problem of detecting foot motions to interact with stepping exergames is still largely understudied. In this work, we developed a new method to detect and classify step directions relying on inertial sensor data captured by two shoe-mounted inertial sensors. Drawing on previous results, we developed a single multiclass classifier to distinguish front, back, side, and center steps originating from any of these positions. Since some of these steps exhibit similar displacement patterns, the previous step position was also considered as an input to the classifier. The method was tested on a group of young and older adults, achieving an accuracy of 93.1%. Performance remained consistent throughout the acquisition time due to the introduction of a novel calibration approach designed to handle sensor orientation drift over time. This study provided the first insights into the potential of inertial sensors to detect the foot motions required to interact with stepping exergames. Experimental results support their application in a real scenario.},
  archive      = {J_THMS},
  author       = {Vânia Guimarães and Inês Sousa and Miguel Velhote Correia},
  doi          = {10.1109/THMS.2023.3292902},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {895-904},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Detection of foot motions for interaction with exergames using shoe-mounted inertial sensors},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A user-driven sampling model for large-scale geographical
point data visualization via convolutional neural networks.
<em>THMS</em>, <em>53</em>(5), 885–894. (<a
href="https://doi.org/10.1109/THMS.2023.3296692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous sampling strategies have been proposed to reduce the visual clutter of large-scale geographical point data visualization, which focus on the preservation of original data features, such as randomness, spatial distribution, and associated relationship. However, user preferences and demands are not taken into account in the course of sampling, which will lead to the sampled results deviating from user requirements and impede personalized geospatial analysis in specific application scenarios. In this article, we propose a user-driven sampling model for the visual abstraction of the large-scale geographical point data based on convolutional neural networks (CNN). First, a blue noise sampling model is applied to partition the geographical space into local areas, and a set of visual interfaces are designed to present the data features of those points in the local areas, enabling users to visually select representative points according to their requirements. Then, user preferences are quantified with a CNN model based on the eigenvectors of the representative points, which are further utilized to guide the sampling courses of the other local areas. Thus, all the sampled points will retain the spatial distribution of original data points and fulfill the user preference as far as possible. In addition, we implement a visualization framework to integrate manual point selection, CNN training, automatic point sampling, and visual comparison, allowing users to easily obtain and evaluate the sampled points from the perspectives of data analysis and user requirements. Quantitative comparisons and case studies based on real-world datasets are conducted to demonstrate the effectiveness of our sampling model in the preservation of user preferences and visual exploration of large-scale geospatial point data.},
  archive      = {J_THMS},
  author       = {Zhiguang Zhou and Fengling Zheng and Jin Wen and Yuanyuan Chen and Xinyu Li and Yuhua Liu and Yigang Wang and Wei Chen},
  doi          = {10.1109/THMS.2023.3296692},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {885-894},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A user-driven sampling model for large-scale geographical point data visualization via convolutional neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate analysis of gaze behavior and task performance
within interface design evaluation. <em>THMS</em>, <em>53</em>(5),
875–884. (<a href="https://doi.org/10.1109/THMS.2023.3305715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye tracking technologies have frequently been used in sport research to understand the interrelations between gaze behavior and performance, using a paradigm known as vision-for-action. This methodology has not been robustly applied within the field of interface design. The present work demonstrates the benefit of employing a vision-for-action paradigm for interface evaluation. This is demonstrated through the evaluation of a novel task-specific symbology set presented on a head-up-display (HUD), developed to support pilots conduct ground operations in low-visibility conditions. HUD gaze behavior was correlated with task performance to determine whether certain combinations of gaze behavior could produce effective predictive performance models. A human-in-the-loop experiment was conducted with 11 professional pilots who were required to taxi in a fixed-base flight simulator using the HUD symbology, while gaze data toward the different HUD symbology elements was collected. Performance was measured as centerline deviation error and taxiing speed. Results revealed that appropriately timed gaze behavior toward task-specific elements of the HUD were associated with superior performance. During turns, attention toward an undercarriage lateral position indicator was associated with reduced centerline deviation ( p &lt; 0.05). The findings are interpreted alongside detailed posttrial user-feedback of the HUD symbology to illustrate how eye tracking methodologies can be incorporated into interface usability evaluations. The joint interpretation of these data demonstrates these novel procedures, the findings contribute to enhancing the wider domain of interface design evaluation.},
  archive      = {J_THMS},
  author       = {James Blundell and Charlotte Collins and Rod Sears and Tassos Plioutsias and John Huddlestone and Don Harris and James Harrison and Anthony Kershaw and Paul Harrison and Phil Lamb},
  doi          = {10.1109/THMS.2023.3305715},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {875-884},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Multivariate analysis of gaze behavior and task performance within interface design evaluation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MobiEye: An efficient shopping-assistance system for the
visually impaired with mobile phone sensing. <em>THMS</em>,
<em>53</em>(5), 865–874. (<a
href="https://doi.org/10.1109/THMS.2023.3305566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of rich visual information affects the shopping experience of the visually impaired (VI), including identifying and selecting commodities. Recent studies on VI assistance have focused on commodity identification but neglected to provide fine-grained and intuitive pick-up guidance, which is not user-friendly enough. Therefore, we propose a user-driven shopping assistance system to improve the shopping experience for VI users. We first conduct an in-depth interview with VI, then implement a prototype shopping assistance system—MobiEye, with real-time video analysis. Further, we evaluate the prototype system and identify two directions to optimizing the existing system: (1) Improving the pick-up accuracy in dense placement; and (2) reducing the latency and communication overhead. To address these two problems, we design a new guidance strategy for dense placements and propose a mobile-edge cocomputing strategy with a motion predictor and a communication gate to filter the transmitted images. Finally, we invited VI participants to evaluate the effectiveness and efficiency of MobiEye. The experimental results show that MobiEye achieved a 13% improvement in pick-up success rate and a 12 s reduction in average pick-up time compared with other shopping assistance systems.},
  archive      = {J_THMS},
  author       = {Ziqi Wang and Bin Guo and Qianru Wang and Daqing Zhang and Zhiwen Yu},
  doi          = {10.1109/THMS.2023.3305566},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {865-874},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {MobiEye: An efficient shopping-assistance system for the visually impaired with mobile phone sensing},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stiffness-switchable hydrostatic transmission toward safe
physical human–robot interaction. <em>THMS</em>, <em>53</em>(5),
855–864. (<a href="https://doi.org/10.1109/THMS.2023.3294517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A lightweight and compliant manipulator design has been considered crucial in safe physical human–robot interaction. Remote actuation relocating the massive parts to the robot base and transmitting power to the distal joint minimizes the actuator inertia and provides series elasticity to the actuator. Rolling diaphragm hydrostatic transmission (RDHT), one of the remote actuation, has recently been studied in physically interacting robots, which can tackle the remaining issues in hydraulic actuation, such as low backdrivability and fluid leakage. However, existing RDHTs are challenging to achieve the desired safety and control performance simultaneously due to their fixed stiffness. This article presents a stiffness-switchable hydrostatic transmission (SwHST) consisting of an RDHT and valve-controlled pneumatic springs. The SwHST has a wide stiffness range of 15–290 N $\cdot$ m/rad and a fast response in stiffness transition of less than 50 ms without any complex stiffness tuning mechanism. It is one of the most efficient transmissions in remote actuation and stiffness adjustment. Its static friction is less than 0.4% of full-range torque, and the stiffness-switching module consumes only 6 W of power when valves are open. The dynamic characteristics of the SwHST are experimentally scrutinized under various operational conditions. Safety performance is verified in unconstrained and constrained collision tests, demonstrating that the SwHST can effectively mitigate the clamping force of more than 50% for both the tests. Control performance is evaluated on position tracking tests. We foresee the proposed SwHST being utilized in human–robot collaboration without jeopardizing control performance through a rapid and efficient stiffness-switching mechanism.},
  archive      = {J_THMS},
  author       = {Sungbin Park and Kyungseo Park and Wonseok Shin and Jung Kim},
  doi          = {10.1109/THMS.2023.3294517},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {855-864},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Stiffness-switchable hydrostatic transmission toward safe physical Human–Robot interaction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance and usability evaluation scheme for mobile
manipulator teleoperation. <em>THMS</em>, <em>53</em>(5), 844–854. (<a
href="https://doi.org/10.1109/THMS.2023.3289628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a standardized human–robot teleoperation interface (HRTI) evaluation scheme for mobile manipulators. Teleoperation remains the predominant control type for mobile manipulators in open environments, particularly for quadruped manipulators. However, mobile manipulators, especially quadruped manipulators, are relatively novel systems to be implemented in the industry compared to traditional machinery. Consequently, no standardized interface evaluation method has been established for them. The proposed scheme is the first of its kind in evaluating mobile manipulator teleoperation. It comprises a set of robot motion tests, objective measures, subjective measures, and a prediction model to provide a comprehensive evaluation. The motion tests encompass locomotion, manipulation, and a combined test. The duration for each trial is collected as the response variable in the objective measure. Statistical tools, including mean value, standard deviation, and T-test, are utilized to cross-compare between different predictor variables. Based on an extended Fitts&#39; law, the prediction model employs the time and mission difficulty index to forecast system performance in future missions. The subjective measures utilize the NASA-task load index and the system usability scale to assess workload and usability. Finally, the proposed scheme is implemented on a real-world quadruped manipulator with two widely-used HRTIs, the gamepad and the wearable motion capture system.},
  archive      = {J_THMS},
  author       = {Yuhui Wan and Jingcheng Sun and Christopher Peers and Joseph Humphreys and Dimitrios Kanoulas and Chengxu Zhou},
  doi          = {10.1109/THMS.2023.3289628},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {844-854},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Performance and usability evaluation scheme for mobile manipulator teleoperation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated in situ image acquisition and annotation
scheme for instance segmentation models in open scenes with a
human–robot interaction approach. <em>THMS</em>, <em>53</em>(5),
834–843. (<a href="https://doi.org/10.1109/THMS.2022.3222021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large amount of data acquisition and annotation work is required to train a supervised machine learning model for open scenes. However, traditional manual approaches are inefficient. Here, a method is proposed for on-site image acquisition and semiautomatic annotation based on eye-tracking. This method uses the recognition capabilities and computational advantages of humans and machines to improve annotation efficiency, overcoming the bottleneck of AI-based approaches to the natural scenery understanding of field robots. The proposed method contains three advancements. First, we designed a head-mounted display with a built-in pose measurement module to achieve first-person teleoperation data acquisition, where a pseudoframe interpolation algorithm is designed to overcome the latency problem in immersive remote data transmission and to achieve efficient field data acquisition. Second, we propose an adaptive superpixel segmentation algorithm to reduce human–machine interactions based on eye-tracking. Third, since traditionally, the annotation process cannot provide feedback to the acquisition process and results in a low conversion rate. We proposed a new conversion rate index denoting the rate of transforming collected data into valid data to quantify the acquisition quality in real time. While achieving an annotation quality of 0.964 per the Dice index, which is approximately equal to that of the manual method, the proposed method improves the annotation efficiency by more than 3 times. Finally, the agricultural field experiments containing a real-life scene of robotic tomato-picking verified that the proposed method based on human–computer interaction can make full use of human perception and recognition intelligence.},
  archive      = {J_THMS},
  author       = {Liang Gong and Zhiyu Yang and Yihang Yao and Binhao Chen and Wenjie Wang and Xiaofeng Du and Yidong He and Chengliang Liu},
  doi          = {10.1109/THMS.2022.3222021},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {834-843},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An integrated in situ image acquisition and annotation scheme for instance segmentation models in open scenes with a Human–Robot interaction approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Driver response to take-over requests in real traffic.
<em>THMS</em>, <em>53</em>(5), 823–833. (<a
href="https://doi.org/10.1109/THMS.2023.3304003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing research on control-transitions from automated driving (AD) to manual driving mainly stems from studies in virtual settings. There is a need for studies conducted in real settings to better understand the impacts of increasing vehicle automation on traffic safety. This study aims specifically to understand how drivers respond to take-over requests (TORs) in real traffic by investigating the associations between 1) where drivers look when receiving the TOR, 2) repeated exposure to TORs, and 3) the drivers’ response process. In total, thirty participants were exposed to four TORs after about 5–6 min of driving with AD on public roads. While in AD, participants could choose to engage in nondriving-related tasks (NDRTs). When they received the TOR, for 38% of TORs, participants were already looking on path. For those TORs where drivers looked off path at the time of the TOR, the off-path glance was most commonly towards an NDRT item. Then, for 72% of TORs (independent on gaze direction), drivers started their response process to the TOR by looking towards the instrument cluster before placing their hands on the steering wheel and their foot on the accelerator pedal, and deactivating automation. Both timing and order of these actions varied among participants, but all participants deactivated AD within 10 s from the TOR. The drivers’ gaze direction at the TOR had a stronger association with the response process than the repeated exposure to TORs did. Drivers can respond to TORs in real traffic. However, the response should be considered as a sequence of actions that requires a certain amount of time.},
  archive      = {J_THMS},
  author       = {Linda Pipkorn and Emma Tivesten and Carol Flannagan and Marco Dozza},
  doi          = {10.1109/THMS.2023.3304003},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {823-833},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Driver response to take-over requests in real traffic},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gamification of driver distraction feedback: A simulator
study with younger drivers. <em>THMS</em>, <em>53</em>(5), 813–822. (<a
href="https://doi.org/10.1109/THMS.2023.3298309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing personalized behavioral information as feedback to drivers can lead to safer practices. However, feedback efficacy is likely moderated by the driver&#39;s level of motivation towards behavioral change. Gamification of feedback, which is the incorporation of game design elements intended to motivate drivers toward safe behaviors, could potentially reduce unsafe behaviors in the long term. This article assesses a gamified driver feedback design in mitigating driver distraction and enhancing driving performance among younger drivers. A driving simulator study was conducted with 42 drivers, 21–30 years old, comparing: 1) no feedback; 2) real-time feedback; 3) real-time feedback + postdrive feedback; and 4) real-time feedback + postdrive feedback + game design elements to examine their impact on distraction engagement (manual-visual interactions with an in-vehicle display) and driving performance. Groups that received postdrive feedback, both with and without gamification elements, showed reduced distraction engagement and enhanced driving performance compared to no feedback. Between the two types of postdrive feedback, the nongamified feedback provided more benefits in reducing the 95th percentile glance duration to the in-vehicle display, and the one with gamification provided more benefits in reducing the rate of manual interactions with the in-vehicle display. Meanwhile, no benefits were observed with the real-time feedback only condition over no feedback. Despite minor differences in efficacy, both postdrive and gamification feedback appear to be effective countermeasures for distracted driving in the short term. Future research should investigate other game designs for driver feedback and assess the impact of feedback gamification over longer-term exposure.},
  archive      = {J_THMS},
  author       = {Huei-Yen Winnie Chen and Jeanne Y. Xie and Birsen Donmez},
  doi          = {10.1109/THMS.2023.3298309},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {813-822},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Gamification of driver distraction feedback: A simulator study with younger drivers},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inertial measurement units and partial least square
regression to predict perceived exertion during repetitive fatiguing
piano tasks. <em>THMS</em>, <em>53</em>(4), 802–810. (<a
href="https://doi.org/10.1109/THMS.2023.3278874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aim. Predict the rate of perceived exertion (RPE) of pianists using inertial measurement units (IMUs)-based kinematic descriptors. Method. Fifty expert pianists played Digital (right-hand 16-tone sequence) and Chord (right-hand chord sequence) excerpts in a continuous loop for 12 min or until exhaustion. Partial least square regression was used to predict RPE with IMUs-based kinematic descriptors. The mean error of prediction over 50 iterations with five-folds cross-validation was used to assess the quality of the model. Variable importance in projection was calculated to determine the most relevant features for predicting the RPE and reduce the number of input predictors. Results. Thirty and twenty-six participants showed signs of fatigue before 12 min of the Digital and Chord tasks, respectively, and were included in the analysis. The reduced model of 275 and 227 input variables including four-latent variables explained 86.95 ± 0.46 and 83.91 ± 0.54 of the variance of the RPE on the training set with an absolute error of 0.976 ± 0.033 and 1.189 ± 0.068 on the testing set for both Digital and Chord tasks, respectively. The best features, variables, and sensor positions to predict RPE were different between both Digital and Chord tasks suggesting a task-dependency in the prediction of effort exertion during piano performance. Conclusion. These results highlight the feasibility of continuously monitoring RPE in pianists using kinematic descriptors. These results are promising for developing methods to prevent high levels of fatigue and injuries in musicians.},
  archive      = {J_THMS},
  author       = {Etienne Goubault and Felipe Verdugo and François Bailly and Mickaël Begon and Fabien Dal Maso},
  doi          = {10.1109/THMS.2023.3278874},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {802-810},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Inertial measurement units and partial least square regression to predict perceived exertion during repetitive fatiguing piano tasks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topological nonlinear analysis of dynamical systems in
wearable sensor-based human physical activity inference. <em>THMS</em>,
<em>53</em>(4), 792–801. (<a
href="https://doi.org/10.1109/THMS.2023.3275774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a topological nonlinear analysis approach for dynamical system measurements, frequently appearing in sensor-based inference tasks in human physical activity analysis. Traditional approaches to dynamical modeling included linear and nonlinear methods with specific representational abilities and some drawbacks. A novel approach we investigate is using topological descriptors of the shape of the dynamical attractor to represent the nature of dynamics. The proposed framework has three essential advantages compared to previous approaches: 1) with nonlinear phase space reconstruction, the dynamics descriptor is derived from the observation time series without any statistical assumption; 2) with the topological data analysis technique, the phase space topological properties are described in an intrinsic multiresolution analytical way, which brings novel information compared to traditional phase-space modeling techniques; 3) with different types of measurement sensing signals, the proposed approach shows stability in activities state inference. We illustrate our idea with the physical activity recognition tasks with wearable sensors, where the topological characteristics of reconstructed phase state space show strong representational ability for activity type inference.},
  archive      = {J_THMS},
  author       = {Yan Yan and Yi-Chun Huang and Jinjin Zhao and Yu-Shi Liu and Liang Ma and Jing Yang and Xu-Dong Yan and Jing Xiong and Lei Wang},
  doi          = {10.1109/THMS.2023.3275774},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {792-801},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Topological nonlinear analysis of dynamical systems in wearable sensor-based human physical activity inference},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Choice, uncertainty, and decision superiority: Is less
AI-enabled decision support more? <em>THMS</em>, <em>53</em>(4),
781–791. (<a href="https://doi.org/10.1109/THMS.2023.3279036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing decision makers with more information is often expected to result in more informed and superior decisions. This is especially true when leveraging artificial intelligence (AI) to explore and find complex patterns in vast amounts of data. Although AI can enable an “information advantage,” truly intelligent systems should buffer scarce human cognitive resources from information overload and be well adapted to the environment in which they are deployed. Paradoxically, some practitioners have conflated AI&#39;s information processing superiority with a contradictory decision-support goal: to provide human decision makers with more , higher quality, or more novel courses of action, regardless of context, than they could generate without AI. In this article, I review the evidence examining the costs and benefits of providing decision makers with more or less choice and identify the factors that moderate the relationship between the amount of choice and decision effectiveness. Although providing more information and choice increases confidence and certainty in one&#39;s decision, it can make decision making more difficult, decrease satisfaction, and result in poorer decision outcomes. The research indicates that such negative effects are influenced by the level of entropy and variety provided and can be reduced with increased familiarity but are further compounded when decisions are increasingly effortful, difficult, or complex. The review concludes with guidance on how designers might leverage knowledge of choice overload and associated moderator effects to create more adaptive and effective decision support systems.},
  archive      = {J_THMS},
  author       = {Paul Ward},
  doi          = {10.1109/THMS.2023.3279036},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {781-791},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Choice, uncertainty, and decision superiority: Is less AI-enabled decision support more?},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing screen reader intelligibility in noisy
environments. <em>THMS</em>, <em>53</em>(4), 771–780. (<a
href="https://doi.org/10.1109/THMS.2023.3280030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People with blindness or severe low vision access mobile devices using screen readers. However, noisy environments can impair screen reader intelligibility. During mobility, this could disorient or even endanger the user. To address this issue, we propose three screen reader speech compensation techniques based on environmental noise: speech rate slowing, adaptive volume increase, and adaptive equalization. Through a study with 12 participants in three simulated noise scenarios, we evaluate screen reader intelligibility and the perceived distraction from the soundscape, with and without compensations. Four of the proposed compensations, in particular those that pair speech rate reduction with volume or equalization adaptation, significantly improve screen reader&#39;s speech intelligibility in all the considered scenarios, and the compensations do not have a significant impact on the distraction from the soundscape.},
  archive      = {J_THMS},
  author       = {Dragan Ahmetovic and Gabriele Galimberti and Federico Avanzini and Cristian Bernareggi and Luca Andrea Ludovico and Giorgio Presti and Gianluca Vasco and Sergio Mascetti},
  doi          = {10.1109/THMS.2023.3280030},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {771-780},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Enhancing screen reader intelligibility in noisy environments},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human–machine cooperative decision making outperforms
individualism and autonomy. <em>THMS</em>, <em>53</em>(4), 761–770. (<a
href="https://doi.org/10.1109/THMS.2023.3274916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The experiment reported in this article provides a first experimental evaluation of human–machine cooperation on decision level: It explicitly focuses on the interaction of human and machine in cooperative decision-making situations for which a suitable experimental design is introduced. Furthermore, it challenges conventional leader–follower approaches by comparing them to newly proposed automation designs based on cooperative decision-making models. These models originate from negotiation theory and game theory and allow for an investigation of cooperative decision making between equal partners. This equality is motivated by similar approaches on the action level of human–machine cooperation. The experiment&#39;s results indicate an added value of the proposed automation designs in terms of objective cooperative performance as well as human trust in and satisfaction with the cooperation. Hence, the experiment yields the same insight on decision level as already observed on action level: It may be beneficial to design machines as equal cooperation partners and in accordance to models of emancipated human–machine cooperation.},
  archive      = {J_THMS},
  author       = {Simon Rothfuß and Maximilian Wörner and Jairo Inga and Andrea Kiesel and Sören Hohmann},
  doi          = {10.1109/THMS.2023.3274916},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {761-770},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human–Machine cooperative decision making outperforms individualism and autonomy},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How humans comply with a (potentially) faulty robot: Effects
of multidimensional transparency. <em>THMS</em>, <em>53</em>(4),
751–760. (<a href="https://doi.org/10.1109/THMS.2023.3273773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with how a human operator follows a request made by a robot in an industrial context. This robot is potentially myopic, i.e., its request could be based on partial and limited information. Therefore, it could possibly be faulty. This exploratory study aims to analyze whether multidimensional agent transparency may have an effect on two drivers of compliance identified in the literature: trust in signal and risk perception. In this experiment, we manipulated different agent transparency levels combined with two dimensions of agent transparency: robot-TO-human transparency (rTOh) and robot-OF-human transparency (rOFh). Results mainly show that adding rOFh to rTOh transparency changes human compliance with the agent and moderates both trust and risk perception. Moreover, task performance and completion time are shown to vary according to the different transparency levels. Our results show that transparency has no effect on mental workload. From a methodological perspective, this article shows the importance of distinguishing the different types of information about which a robot can be transparent, especially the combination of rTOh and rOFh transparencies. From a practical point of view, the article shows that the agent transparency framework needs to be considered carefully when designing human-robot teaming in the context of Industry 4.0 in the case of robots that may be unreliable due to their myopia.},
  archive      = {J_THMS},
  author       = {Loïck Simon and Clément Guérin and Philippe Rauffet and Christine Chauvin and Éric Martin},
  doi          = {10.1109/THMS.2023.3273773},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {751-760},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {How humans comply with a (Potentially) faulty robot: Effects of multidimensional transparency},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human performance in highly automated, cyber vulnerable
unmanned aerial systems: Effects of operators’ background and task load.
<em>THMS</em>, <em>53</em>(4), 743–750. (<a
href="https://doi.org/10.1109/THMS.2023.3283796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article evaluated the effects of operator background and task load on operator ability to detect and respond to cyber events while controlling multiple unmanned aerial systems (UAS). Cyber physical systems (CPS) are susceptible to cyber threats and associated events via connections critical to their operations. The effects of cyber intrusions on UAS operator performance as a function of background and task load have been understudied. Fifty-one operators with different backgrounds (Aviators, Gamers and Non-Gamers; n = 17 per group) were evaluated across two levels of task load (in-the-loop (ITL) and out-of-the-loop (OTL)) while controlling a simulation of multiple UAS involving cyber events. Cyber events represented spoofed information that could not be corroborated with available secondary information/displays. Multivariate analysis of variance was used to analyze the effect of background and task load on operator performance, subjective perception of workload (assessed using the NASA Task Load Index), and trust. Nongamers responded to cyber events 62.8% and 67.7% slower than aviators and gamers, respectively. Mean response times across task load conditions were substantially longer for cyber events (128.9 s) than for noncyber, “real” events (42.8 s). Scenario type (ITL/OTL) was statistically significant for workload, cyber classification, and cyber action response. The findings may inform recruitment criteria for future UAS operators. Individuals with gaming experience may be as adept at responding to cyber events while operating multiple UAS as aviators. Cyber events are likely to affect the performance of the overall CPS due to their impact on the human operator.},
  archive      = {J_THMS},
  author       = {Ike R. Stutts and Mark C. Schall and Jennifer Matthews and Sean Gallagher and Grayson H. Phillips and David Umphress},
  doi          = {10.1109/THMS.2023.3283796},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {743-750},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human performance in highly automated, cyber vulnerable unmanned aerial systems: Effects of operators’ background and task load},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asynchronous remote usability tests using web-based tools
versus laboratory usability tests: An experimental study. <em>THMS</em>,
<em>53</em>(4), 731–742. (<a
href="https://doi.org/10.1109/THMS.2023.3282225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote usability testing is performed by evaluators who are in different physical locations from the participants (synchronous remote testing) and possibly operating at different times (asynchronous remote testing). The tools developed in recent years to support remote tests exploit web technology based on HTML5 and JavaScript ES6 and thus enable previously unexplored scenarios. However, studies providing evidence on the benefits or drawbacks of utilizing recent web-based tools have not yet been reported in the literature. This article sheds some light on the impact of such tools on asynchronous remote usability testing of websites by reporting an experimental study with 100 participants and 15 evaluators to compare real-time laboratory tests with asynchronous remote tests. The study investigates: 1) how the metrics results of asynchronous remote usability tests performed through a web-based tool differ from those of usability tests conducted in real-time laboratory settings; and 2) how the experience of participants differs in the two types of tests. The lessons learned in the study are instrumental in informing the design of future tools. Some results of particular interest indicate that the web technology used by the tool for asynchronous remote testing affects task execution times and participants’ satisfaction. Another indication is that slow internet connections must be managed in asynchronous remote testing; slow connections introduce delays when transferring large amounts of collected data, which, together with the lack of human support, make participants of asynchronous remote tests more prone to feel negative emotions.},
  archive      = {J_THMS},
  author       = {Giuseppe Desolda and Rosa Lanzilotti and Danilo Caivano and Maria Francesca Costabile and Paolo Buono},
  doi          = {10.1109/THMS.2023.3282225},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {731-742},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Asynchronous remote usability tests using web-based tools versus laboratory usability tests: An experimental study},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling team interaction and decision-making in agile
human–machine teams: Quantum and dynamical systems perspective.
<em>THMS</em>, <em>53</em>(4), 720–730. (<a
href="https://doi.org/10.1109/THMS.2023.3276744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we define team agility as a function of exploration and exploitation of team coordination. Based on these two coordination concepts, we examined interactive decision-making in a dynamic task environment by applying: first, the principles of quantum cognition for the decision-making processes at the confluence of teamwork and taskwork (to discern the effects of ontic uncertainty for each human team member in the case of having incomplete teamwork) and second, nonlinear dynamical systems modeling for the teamwork (to capture epistemic uncertainty). In this study, there were the following three conditions based on manipulation of the pilot role: first, synthetic condition—the pilot role was played by a synthetic agent, second, control condition—it was a randomly assigned participant, and third, experimenter condition—it was an expert who used a role-specific coordination script. Overall findings indicate that when teams in the experimenter condition come across the targets, they tend to explore alternatives by coordinating as a team rather than exploiting existing team strategies, which may not work best for the situation at hand. In contrast, teams in control and synthetic conditions tended more toward exploitation than exploration in coordination. We consider this a sign of agility in teamwork.},
  archive      = {J_THMS},
  author       = {Mustafa Demir and Mustafa Canan and Myke C. Cohen},
  doi          = {10.1109/THMS.2023.3276744},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {720-730},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Modeling team interaction and decision-making in agile Human–Machine teams: Quantum and dynamical systems perspective},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pursuing energy saving and thermal comfort with a
human-driven DRL approach. <em>THMS</em>, <em>53</em>(4), 707–719. (<a
href="https://doi.org/10.1109/THMS.2022.3216365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The management of thermal comfort in a building is a challenging and multifaced problem, because the use of objective parameters, for example, the energy consumption, should be combined with subjective requirements, related to human profile and preferences. This article exploits cognitive technologies, based on deep reinforcement learning (DRL), for the automatic control of the heating, ventilation, and air conditioning system in an office. The learning process is driven by a reward that includes multiple components, related to energy consumption, indoor temperature, and user perceptions, which are inferred by the human interactions with the system. This approach is inspired by the human-in-the-loop paradigm, which in our case helps the DRL controller to learn the requirements of users and readily adapt to them. Experimental results show that the appropriate balance of the reward components can be efficiently exploited to give the desired importance to the different objectives.},
  archive      = {J_THMS},
  author       = {Luigi Scarcello and Franco Cicirelli and Antonio Guerrieri and Carlo Mastroianni and Giandomenico Spezzano and Andrea Vinci},
  doi          = {10.1109/THMS.2022.3216365},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {707-719},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Pursuing energy saving and thermal comfort with a human-driven DRL approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion recognition through combining EEG and EOG over
relevant channels with optimal windowing. <em>THMS</em>, <em>53</em>(4),
697–706. (<a href="https://doi.org/10.1109/THMS.2023.3275626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For dimensional emotion recognition, electroencephalography (EEG) signals and electrooculogram (EOG) signals are often combined to improve the performance of classifiers, as each of them provides complementary features to the other. In this article, we combine the EEG signal on the relevant channels with the EOG signal to boost the recognition accuracy. We first explore the mutual information (MI) of all EEG channels and only select emotion-related channels, i.e., channels with more MI are retained, since the emotion recognition performance can be degraded by the interference between uncorrelated channels, while the computational complexity is significant if all EEG channels are used for recognition. While the optimal lengths of EEG and EOG signals for emotion recognition are still uncertain, we systematically investigate the effects of time-window size on emotion recognition. This strategy not only increases the number of training samples, but also reduces the feature redundancy. At this stage, we not only extract multiple statistical features but also employ the increment entropy to find abrupt changes in EEG signals. The experimental results show that 13 out of 32 EEG channels were selected by the proposed channel selection algorithm, and these selected channels can already produce accurate emotion predictions. We found that using optimal time-windows to split EEG and EOG signals into several thin slices and then combine them can further enhance the emotion recognition performance, where the time-windows of 4, 5, 6, and 10 s allow the combined signals to achieve very high accuracy.},
  archive      = {J_THMS},
  author       = {Huili Cai and Xiaofeng Liu and Rongrong Ni and Siyang Song and Angelo Cangelosi},
  doi          = {10.1109/THMS.2023.3275626},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {697-706},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Emotion recognition through combining EEG and EOG over relevant channels with optimal windowing},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using beta rhythm from EEG to assess physicians’ operative
skills in virtual surgical training. <em>THMS</em>, <em>53</em>(4),
688–696. (<a href="https://doi.org/10.1109/THMS.2022.3228214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of virtual reality technology has ushered in new developments in the medical field. The use of virtual surgery training simulators alleviates the paucity of training resources and high training expenses associated with traditional surgical capabilities. Regardless of the type of schooling, doctors must continue to educate themselves. The postoperative evaluation mechanism is incomplete. Traditional objective evaluation indicators are unable to meet surgeons&#39; stringent expectations. The electroencephalograph (EEG) rhythm index is proposed in this article as a new tool for evaluating and distinguishing between novice and expert doctors. The experiment uses a cutting training module from neurosurgery training and compares it with established assessment metrics to determine the correct rate of classification of new evaluation metrics, classifying testers by both metrics and finding a 20% increase in correctness. Additionally, this article compares the energy topographic maps of different EEG rhythms of novices and experts. For classification, two-machine learning algorithms, SVM and random forest, are utilized at the same time. The findings reveal that the accuracy of distinguishing indicators based on EEG cycles is 10% higher than that of typical objective evaluation indicators, regardless of the categorization method. ROC curve analysis was also used to compare the two classification models. The AUC value for the EEG rhythm evaluation index model was 0.971, whereas the AUC value for the classic objective evaluation index model was 0.761, which explains the EEG rhythm evaluation index. The model demonstrates a categorization standard that is reliable.},
  archive      = {J_THMS},
  author       = {Junzhen Du and Yonghang Tai and Fei Li and Zaiqing Chen and Xuqing Ren and Chengli Li},
  doi          = {10.1109/THMS.2022.3228214},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {688-696},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Using beta rhythm from EEG to assess physicians&#39; operative skills in virtual surgical training},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). To inform or to instruct? An evaluation of meaningful
vibrotactile patterns to support automated vehicle takeover performance.
<em>THMS</em>, <em>53</em>(4), 678–687. (<a
href="https://doi.org/10.1109/THMS.2022.3205880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated vehicles may occasionally require drivers to take over. The complexity of the takeover process warrants the design of effective human–machine interfaces that assist drivers in regaining control, especially when the visual and auditory sensory modalities are occupied. Vibrotactile displays, which can represent information about the status, direction, and position of driving environment elements, have been suggested as one promising approach, but their effectiveness to aid in takeover transitions has not been fully evaluated. This study investigated the effects of meaningful tactile signal patterns, used as takeover requests, on automated vehicle takeover performance. Forty participants rode in a simulated SAE Level 3 automated vehicle and completed a series of takeover tasks with two tactile pattern formats, i.e., informative (which displayed status information of surrounding vehicles) and instructional (that displayed the appropriate takeover maneuver), and three in-vehicle locations (seat back, seat pan, and a seat back and seat pan combination). Takeover response options included lane changes only or brake applications followed by changing lanes, depending on the locations of surrounding vehicles. Results indicate that only meaningful instructional tactile signals, in either the seat back or seat pan, were associated with worse takeover response time and maximum resulting acceleration compared to signals without any patterns. Additionally, tactile information presented on the seat back was perceived as the most useful and satisfying. Findings from this study can inform the development of next-generation human–machine interfaces that utilize tactile stimulation in a wide range of environments with automation.},
  archive      = {J_THMS},
  author       = {Gaojian Huang and Brandon J. Pitts},
  doi          = {10.1109/THMS.2022.3205880},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {678-687},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {To inform or to instruct? an evaluation of meaningful vibrotactile patterns to support automated vehicle takeover performance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A compact and portable exoskeleton for shoulder and elbow
assistance for workers and prospective use in space. <em>THMS</em>,
<em>53</em>(4), 668–677. (<a
href="https://doi.org/10.1109/THMS.2022.3186874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exoskeletons are wearable robotic devices that surround the anatomy of the user to work in tandem with it. Depending on their structure, exoskeletons can be classified as rigid or flexible. The structure of flexible exoskeletons is made of soft materials, such as fabrics, which adapt to user motion. Therefore, these devices are prone to becoming misaligned with the user, due to improper fitting and slipping of the exoskeleton components on the user body. This article describes a cable-driven exosuit, called LUXBIT , that favors its anatomical adaption to the user by arranging the fabric fibers and sewing patterns to transfer the mobilizing forces. This prototype integrates a novel deformable mechanism that promotes the natural lifting of the arm. LUXBIT is intended for bimanual assistance in daily living, being equipped with a backpack to this end. The results analyzed in this article show that LUXBIT reduces muscle activity in the upper limbs’ flexion by ratios of up to 13.17% and allows the user to hold tiring postures for 62.91% longer.},
  archive      = {J_THMS},
  author       = {José Luis Samper-Escudero and Sofía Coloma and Miguel Angel Olivares-Mendez and Miguel Ángel Sanchez-Urán González and Manuel Ferre},
  doi          = {10.1109/THMS.2022.3186874},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {668-677},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A compact and portable exoskeleton for shoulder and elbow assistance for workers and prospective use in space},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An online toolkit for applications featuring collaborative
robots across different domains. <em>THMS</em>, <em>53</em>(4), 657–667.
(<a href="https://doi.org/10.1109/THMS.2022.3213416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative robots (cobots) are being applied in areas such as healthcare, rehabilitation, agriculture and logistics, beyond the typical manufacturing setting. This is leading to a marked increase in the number of cobot stakeholders with little or no experience in traditional safety engineering. Considering the importance of human safety in collaborative robotic applications, this is currently proving to be a barrier to more widespread cobot usage. A web-based Toolkit that targets cobot end-users and manufacturers with varying levels of safety expertise was developed, helping them to understand how to consider the safety of their cobot applications. In this work, we will provide an overview of the state of the art for ensuring cobot safety, highlight the support provided by the “COVR Toolkit” and introduce three examples where third parties applied the Toolkit for their collaborative robotics application.},
  archive      = {J_THMS},
  author       = {José Saenz and Jule Bessler-Etten and Marcello Valori and Gerdienke B. Prange-Lasonder and Irene Fassi and Catherine Bidard and Aske Bach Lassen and Imre Paniti and Andras Toth and Tobias Stuke and Sebastian Wrede and Kurt Nielsen},
  doi          = {10.1109/THMS.2022.3213416},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {657-667},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An online toolkit for applications featuring collaborative robots across different domains},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial special section on featured research from the 2nd
international conference on human-machine systems. <em>THMS</em>,
<em>53</em>(4), 653–656. (<a
href="https://doi.org/10.1109/THMS.2023.3295512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_THMS},
  author       = {David B. Kaber and Andreas Nürnberger and Giancarlo Fortino and David Mendonça and Antonio Guerrieri},
  doi          = {10.1109/THMS.2023.3295512},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {653-656},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Editorial special section on featured research from the 2nd international conference on human-machine systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Respiratory feature extraction for contactless breathing
pattern recognition using a single digital camera. <em>THMS</em>,
<em>53</em>(3), 642–651. (<a
href="https://doi.org/10.1109/THMS.2023.3254895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-contact respiratory monitoring is gaining attention due to its unobtrusive form of measurement. Among all other non-contact sensing modalities, optical sensors integrated with commercial video cameras have received more attention due to their ease of use and low cost, allowing non-experts to employ these methods without any prior knowledge. Prior research mostly focused on extracting breathing rate and heart rate-related information for sedentary breathing patterns using camera to determine health status. However, for abnormal breathing patterns, a range of respiratory-pattern-related signatures need to be monitored to investigate the associated primary cause of pattern change. In this paper, we propose a novel feature extraction technique to recognize respiratory patterns such as eupnea, tachypnea, bradypnea, and apnea retrieved from videos captured using a single digital camera working in the visible range. At this aim, a hyper-feature algorithm has been implemented to extract distinguishable air-flow-related from videos collected on twenty-four participants having four different respiratory patterns. After the training of the algorithm using the reference respiratory signal obtained from a medical-grade chest strap, the performance of the system has been evaluated on additional five participants whose measurement was not used for algorithm development and on seven patients. Results demonstrated an accuracy of 96.07% in the recognition of all the respiratory patterns, with minimum performances (81.81%) in detecting bradypnea and 100% in detecting apnea events. Additionally, including the patients dataset with the apnea and the normal breathing pattern, the accuracy of the proposed algorithm becomes 97.39%, demonstrating its robustness on the patients dataset.},
  archive      = {J_THMS},
  author       = {Shekh Md Mahmudul Islam and Nunzia Molinaro and Sergio Silvestri and Emiliano Schena and Carlo Massaroni},
  doi          = {10.1109/THMS.2023.3254895},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {642-651},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Respiratory feature extraction for contactless breathing pattern recognition using a single digital camera},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wital: A COTS WiFi devices based vital signs monitoring
system using NLOS sensing model. <em>THMS</em>, <em>53</em>(3), 629–641.
(<a href="https://doi.org/10.1109/THMS.2023.3264247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vital sign (breathing and heartbeat) monitoring is essential for patient care and sleep disease prevention. Most current solutions are based on wearable sensors or cameras; however, the former could affect sleep quality, while the latter often present privacy concerns. To address these shortcomings, we propose Wital, a contactless vital sign monitoring system based on low-cost and widespread commercial off-the-shelf (COTS) Wi-Fi devices. There are two challenges that need to be overcome. First, the torso deformations caused by breathing/heartbeats are weak. How can such deformations be effectively captured? Second, movements such as turning over affect the accuracy of vital sign monitoring. How can such detrimental effects be avoided? For the former, we propose a non-line-of-sight (NLOS) sensing model for modeling the relationship between the energy ratio of line-of-sight (LOS) to NLOS signals and the vital sign monitoring capability using Ricean K theory and use this model to guide the system construction to better capture the deformations caused by breathing/heartbeats. For the latter, we propose a motion segmentation method based on motion regularity detection that accurately distinguishes respiration from other motions, and we remove periods that include movements such as turning over to eliminate detrimental effects. We have implemented and validated Wital on low-cost COTS devices. The experimental results demonstrate the effectiveness of Wital in monitoring vital signs.},
  archive      = {J_THMS},
  author       = {Xiang Zhang and Yu Gu and Huan Yan and Yantong Wang and Mianxiong Dong and Kaoru Ota and Fuji Ren and Yusheng Ji},
  doi          = {10.1109/THMS.2023.3264247},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {629-641},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Wital: A COTS WiFi devices based vital signs monitoring system using NLOS sensing model},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multisensor interface to improve the learning experience
in arc welding training tasks. <em>THMS</em>, <em>53</em>(3), 619–628.
(<a href="https://doi.org/10.1109/THMS.2023.3251955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the development of a multisensor user interface to facilitate the instruction of arc welding tasks. Traditional methods to acquire hand-eye coordination skills are typically conducted through one-to-one instruction, where trainees must wear protective helmets and conduct several tests. These approaches are inefficient as the harmful light emitted from the electric arc impedes the close monitoring of the process. Practitioners can only observe a small bright spot. To tackle these problems, recent training approaches have leveraged virtual reality to safely simulate the process and visualize the geometry of the workpieces. However, the synthetic nature of these types of simulation platforms reduces their effectiveness as they fail to comprise actual welding interactions with the environment, which hinders the trainees&#39; learning process. To provide users with a real welding experience, we have developed a new multisensor extended reality platform for arc welding training. Our system is composed of: 1) An HDR camera, monitoring the real welding spot in real time. 2) A depth sensor, capturing the 3-D geometry of the scene; and 3) A head-mounted VR display, visualizing the process safely. Our innovative platform provides users with a “bot trainer,” virtual cues of the seam geometry, automatic spot tracking, and performance scores. To validate the platform&#39;s feasibility, we conduct extensive experiments with several welding training tasks. We show that compared with the traditional training practice and recent virtual reality approaches, our automated multisensor method achieves better performances in terms of accuracy, learning curve, and effectiveness.},
  archive      = {J_THMS},
  author       = {Hoi-Yin Lee and Peng Zhou and Anqing Duan and Jiangliu Wang and Victor Wu and David Navarro-Alarcon},
  doi          = {10.1109/THMS.2023.3251955},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {619-628},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A multisensor interface to improve the learning experience in arc welding training tasks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Drivers’ attention to preview and its momentary persistence.
<em>THMS</em>, <em>53</em>(3), 610–618. (<a
href="https://doi.org/10.1109/THMS.2023.3266152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Participants used a position control system to track the center of a simulated winding roadway with preview that ranged from 0.3 to 1.0 s. Participants’ spatial distributions of attention were measured by perturbing the roadway with different frequency sinusoids at different roadway positions and then measuring the degree to which those frequencies were present in their tracking movements. Consistent with Miller&#39;s optimal control theory analysis of tracking with preview, participants exhibited a continuous range of attention, and it lengthened with the amount of displayed preview. When the displayed preview disappeared for 5 s, the time to reach a steady-state level of tracking error based only on feedback control was measured. More displayed preview was strongly correlated with longer times to regress to feedback control. One interpretation of this finding is that when preview is withdrawn, visual sensory memory of the previewed roadway can be used for a fraction of a second to prolong the period of feedforward control. Attention can be shifted to relevant positions of the sensory memory image to anticipate the roadway curvature. However, the quality and usefulness of sensory memory gradually diminishes until the participant can only perform feedback control. When preview was restored, the time to reach steady-state feedforward control was correlated with the range of attention. These findings characterize some of the dynamic aspects of attention when drivers are briefly interrupted from viewing the upcoming roadway. This measurement technique may be useful for characterizing spatially distributed attention in other active control contexts.},
  archive      = {J_THMS},
  author       = {Tyler N. Morrison and Richard J. Jagacinski and Jordan Petrov},
  doi          = {10.1109/THMS.2023.3266152},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {610-618},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Drivers’ attention to preview and its momentary persistence},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How resource demands of nondriving-related tasks and
engagement time affect drivers’ physiological response and takeover
performance in conditional automated driving. <em>THMS</em>,
<em>53</em>(3), 600–609. (<a
href="https://doi.org/10.1109/THMS.2023.3268095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drivers are allowed to perform a nondriving-related task (NDRT) in the Level-3 (L3) automated driving, which inevitably promotes a variety of complex NDRTs. Hence, investigating the effect of NDRTs on drivers’ behavior is necessary to ensure a safe transition from automated to manual driving. The main aim of this study is to examine the effect of resource demands of NDRTs and its engagement time on drivers’ physiological responses, takeover performance, and subjective ratings. A total of 42 participants were recruited to conduct a driving simulator study. Results showed that visual and physical resource demands significantly increased participants’ response time and deteriorated their takeover quality. Engagement time significantly affected participants’ drowsiness and response time to the takeover request. In addition, participants rated the physical–visual–cognitive task as the most challenging task and the cognitive task as the least demanding task, which matches the takeover performance. Moreover, pupil diameter responds significantly to visual resource demands rather than physical resource demands. Physical resource demands had a significant tendency to elevate heart rate. This study suggested that tasks with a certain level of cognitive resource demand may be more appropriate for L3 automated driving. The findings of this study make it possible to use physiological responses to identify NDRT types or even predict drivers’ takeover performance in automated driving. Based on this, the time budget could be dynamically adjusted to ensure a safe transition.},
  archive      = {J_THMS},
  author       = {Lie Guo and Linli Xu and Pingshu Ge and Xu Wang},
  doi          = {10.1109/THMS.2023.3268095},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {600-609},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {How resource demands of nondriving-related tasks and engagement time affect drivers’ physiological response and takeover performance in conditional automated driving},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The mathematical meaninglessness of the NASA task load
index: A level of measurement analysis. <em>THMS</em>, <em>53</em>(3),
590–599. (<a href="https://doi.org/10.1109/THMS.2023.3263482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human mental workload can profoundly impact human performance and is thus an important consideration in the design and operation of many systems. The standard method for assessing human mental workload is the NASA Task Load Index (NASA-TLX). This involves a human operator subjectively rating a task based on six dimensions. These dimensions are combined into a single workload score using one of two methods: scaling and summing the dimensions (where scales are derived from a paired comparisons procedure) or averaging dimensions together. Despite its widespread use, the level of measurement of NASA-TLX&#39;s dimensions and its computed workload score has not been investigated. Additionally, nobody has researched whether NASA-TLX&#39;s two approaches for computing overall workload are mathematically meaningful with respect to the constituent dimensions&#39; levels of measurement. This is a serious deficiency. Knowing what the level of measurement is for NASA-TLX scores will determine what mathematics can be meaningfully applied to them. Furthermore, if NASA-TLX workload syntheses are mathematically meaningless, then the measure lacks construct validity. The research presented in this article used a previously developed method to evaluate the level of measurement of NASA-TLX workload and its dimensions. Results show that the dimensions can, in most situations, be treated as interval in population analyses and ordinal for individuals. Our results also suggest that the methods for combining dimensions into workload scores are meaningless. We recommend that analysts evaluate the dimensions of NASA-TLX without combining them.},
  archive      = {J_THMS},
  author       = {Matthew L. Bolton and Elliot Biltekoff and Laura Humphrey},
  doi          = {10.1109/THMS.2023.3263482},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {590-599},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {The mathematical meaninglessness of the NASA task load index: A level of measurement analysis},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical active learning with qualitative feedback on
regions. <em>THMS</em>, <em>53</em>(3), 581–589. (<a
href="https://doi.org/10.1109/THMS.2023.3252815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning classification models in practice usually requires numerous labeled data for training. However, instance-based annotation can be inefficient for humans to perform. In this article, we propose and study a new type of human supervision that is fast to perform and useful for model learning. Instead of labeling individual instances, humans provide supervision to data regions , which are subspaces of the input data space, representing subpopulations of data. Since labeling now is performed on a region level, 0/1 labeling becomes imprecise. Thus, we design the region label to be a qualitative assessment of the class proportion, which coarsely preserves the labeling precision but is also easy for humans to do. To identify informative regions for labeling and learning, we further devise a hierarchical active learning process that recursively constructs a region hierarchy. This process is semisupervised in the sense that it is driven by both active learning strategies and human expertise, where humans can provide discriminative features. To evaluate our framework, we conducted extensive experiments on nine datasets as well as a real user study on a survival analysis of colorectal cancer patients. The results have clearly demonstrated the superiority of our region-based active learning framework against many instance-based active learning methods.},
  archive      = {J_THMS},
  author       = {Zhipeng Luo and Yazhou He and Yanbing Xue and Hongjun Wang and Milos Hauskrecht and Tianrui Li},
  doi          = {10.1109/THMS.2023.3252815},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {581-589},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Hierarchical active learning with qualitative feedback on regions},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multitemporal scale and spatial–temporal transformer
network for temporal action localization. <em>THMS</em>, <em>53</em>(3),
569–580. (<a href="https://doi.org/10.1109/THMS.2023.3266037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization plays an important role in video analysis, which aims to localize and classify actions in untrimmed videos. Previous methods often predict actions on a feature space of a single temporal scale. However, the temporal features of a low-level scale lack sufficient semantics for action classification, while a high-level scale cannot provide the rich details of the action boundaries. In addition, the long-range dependencies of video frames are often ignored. To address these issues, a novel multitemporal-scale spatial–temporal transformer (MSST) network is proposed for temporal action localization, which predicts actions on a feature space of multiple temporal scales. Specifically, we first use refined feature pyramids of different scales to pass semantics from high-level scales to low-level scales. Second, to establish the long temporal scale of the entire video, we use a spatial–temporal transformer encoder to capture the long-range dependencies of video frames. Then, the refined features with long-range dependencies are fed into a classifier for coarse action prediction. Finally, to further improve the prediction accuracy, we propose a frame-level self-attention module to refine the classification and boundaries of each action instance. Most importantly, these three modules are jointly explored in a unified framework, and MSST has an anchor-free and end-to-end architecture. Extensive experiments show that the proposed method can outperform state-of-the-art approaches on the THUMOS14 dataset and achieve comparable performance on the ActivityNet1.3 dataset. Compared with A2Net (TIP20, Avg{0.3:0.7}), Sub-Action (CSVT2022, Avg{0.1:0.5}), and AFSD (CVPR21, Avg{0.3:0.7}) on the THUMOS14 dataset, the proposed method can achieve improvements of 12.6%, 17.4%, and 2.2%, respectively.},
  archive      = {J_THMS},
  author       = {Zan Gao and Xinglei Cui and Tao Zhuo and Zhiyong Cheng and An-An Liu and Meng Wang and Shenyong Chen},
  doi          = {10.1109/THMS.2023.3266037},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {569-580},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A multitemporal scale and Spatial–Temporal transformer network for temporal action localization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised contrastive learned deep model for question
continuation evaluation. <em>THMS</em>, <em>53</em>(3), 560–568. (<a
href="https://doi.org/10.1109/THMS.2023.3271625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question continuation evaluation (QCE) is a branch task of dialogue act prediction (DAP) in the natural language processing area, which is aimed at predicting whether each question in a dialogue is worthy of being followed-up under a specific context. QCE is important for communication, education, and even entertainment. Regrettably, QCE has always been disregarded as an auxiliary task for conversational machine reading comprehension. QCE involves more information and relationships than the original DAP task, making it more complex. Moreover, the classification of QCE inherently renders the samples confusing. In this article, a transformer long short-term memory (LSTM)-based supervised contrastive learned model for QCE is proposed to automatically distribute QCE labels. This model is mainly constructed with transformer encoder blocks and LSTM modules, and supervised contrastive learning (SCL) is innovatively introduced to the training process. This model is good at extracting both information about corpora and the relationships among corpora, and SCL alleviates any confusion. With the only applicable dataset, i.e., Question Answering in Context (QuAC), experiments are conducted. This model is proven to perform well and is robust to missing data. The performance is 2.3% (accuracy) and 12.2% (macro-F1 score) higher than baselines from QuAC and only decreases by approximately 2.3% when 10% data remain.},
  archive      = {J_THMS},
  author       = {Bo Sun and Hang Li and Jun He and Yinghui Zhang},
  doi          = {10.1109/THMS.2023.3271625},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {560-568},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Supervised contrastive learned deep model for question continuation evaluation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robot-person tracking in uniform appearance scenarios: A new
dataset and challenges. <em>THMS</em>, <em>53</em>(3), 549–559. (<a
href="https://doi.org/10.1109/THMS.2023.3247000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person-tracking robots have many applications including security, surveillance, and autonomous driving. Despite the abundance of uniform appearance in many contexts and the challenges they exhibit, there is a lack of video datasets dedicated to benchmarking tracking algorithms in such contexts. In this article, we propose a new high-quality RGB-D benchmark called PTUA for robot–person tracking in uniform appearance scenarios. PTUA is recorded using an RGB-D sensor on top of a moving robot and consists of 45 sequences containing more than 85 K frames. Each frame is manually annotated with a bounding box and attributes, making PTUA the largest and the most challenging person tracking RGB-D dataset. To the best of our knowledge, such a densely annotated and properly synchronized RGB-D tracking benchmark does not exist in the literature. Each sequence comprises various challenges deriving from real-life scenarios where the target person appears highly similar to the background or distractors. By releasing PTUA, we expect to provide the community with a large-scale challenging RGB-D benchmark with high quality for the robust evaluation of trackers on uniform appearance scenarios for autonomous robots. We also present a rigorous experimental evaluation of the state-of-the-art trackers on the PTUA dataset with a comprehensive analysis. The findings evidence the challenges of person tracking in a uniform appearance scenario for both target tracking and robot–person tracking, and the need to bridge the performance gap. In addition, we propose a new RGB-D tracker that extracts features from RGB-D frames and it achieves the best performance on each challenging scenario of PTUA.},
  archive      = {J_THMS},
  author       = {Xiaoxiong Zhang and Adarsh Ghimire and Sajid Javed and Jorge Dias and Naoufel Werghi},
  doi          = {10.1109/THMS.2023.3247000},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {549-559},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Robot-person tracking in uniform appearance scenarios: A new dataset and challenges},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel skeleton-based human activity discovery using
particle swarm optimization with gaussian mutation. <em>THMS</em>,
<em>53</em>(3), 538–548. (<a
href="https://doi.org/10.1109/THMS.2023.3269047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity discovery aims to cluster human activities without any prior knowledge of what defines each activity. However, most existing methods for human activity recognition are supervised, relying on labeled inputs for training. In reality, it is challenging to label human activity data due to its large volume and the diversity of human activities. To address this issue, this article proposes an unsupervised framework for human activity discovery in 3-D skeleton sequences. The framework includes a data preprocessing step that selects important frames based on kinetic energy and extracts relevant features, such as joint displacement, statistical displacement, angles, and orientation. To reduce the dimensionality of the extracted features, the framework uses principle component analysis. Unlike many other methods for human activity discovery, the proposed framework is fully unsupervised and does not rely on presegmented videos. To segment the time series of activities, the framework uses a sliding time window with some overlapping. The hybrid particle swarm optimization (PSO) with Gaussian mutation and K-means algorithm is then proposed to discover the activities. PSO is chosen for its powerful global search capability and simple implementation. To further improve the convergence rate of PSO, K-means is applied to the outcome centroids from each iteration of PSO. The experimental results on five datasets demonstrate that the proposed framework has superior performance in discovering activities compared to other state-of-the-art methods. The framework achieves an average increase in accuracy of at least 4%.},
  archive      = {J_THMS},
  author       = {Parham Hadikhani and Daphne Teck Ching Lai and Wee-Hong Ong},
  doi          = {10.1109/THMS.2023.3269047},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {538-548},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A novel skeleton-based human activity discovery using particle swarm optimization with gaussian mutation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unified framework for identity and imagined action
recognition from EEG patterns. <em>THMS</em>, <em>53</em>(3), 529–537.
(<a href="https://doi.org/10.1109/THMS.2023.3267898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a unified deep learning framework for the recognition of user identity and the recognition of imagined actions, based on electroencephalography (EEG) signals, for application as a brain–computer interface. Our solution exploits a novel shifted subsampling preprocessing step as a form of data augmentation, and a matrix representation to encode the inherent local spatial relationships of multielectrode EEG signals. The resulting image-like data are then fed to a convolutional neural network to process the local spatial dependencies, and eventually analyzed through a bidirectional long-short term memory module to focus on temporal relationships. Our solution is compared against several methods in the state of the art, showing comparable or superior performance on different tasks. Specifically, we achieve accuracy levels above 90% both for action and user classification tasks. In terms of user identification, we reach 0.39% equal error rate in the case of known users and gestures, and 6.16% in the more challenging case of unknown users and gestures. Preliminary experiments are also conducted in order to direct future works toward everyday applications relying on a reduced set of EEG electrodes.},
  archive      = {J_THMS},
  author       = {Marco Buzzelli and Simone Bianco and Paolo Napoletano},
  doi          = {10.1109/THMS.2023.3267898},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {529-537},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Unified framework for identity and imagined action recognition from EEG patterns},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An incremental learning approach to detect muscular fatigue
in human– robot collaboration. <em>THMS</em>, <em>53</em>(3), 520–528.
(<a href="https://doi.org/10.1109/THMS.2023.3259139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–robot collaboration aims to join the distinctive strengths of humans and robots to compensate for the weaknesses associated with each party and, thus, to enable synergetic effects. Robots are characteristically considered fatigue-proof. Hence, they are utilized to assist human operators during heavy pushing and pulling activities. To detect physical fatigue or high payloads held by a human operator, wearable sensors, such as electromyographys (EMGs), are deployed. The EMG data are typically processed via machine learning, which includes training models offline before an application in an online system. However, these approaches often demonstrate varying performances between offline and online applications due to subject-specific characteristics within the data. An opportunity to tackle this challenge can be found in incremental learning, as these models purely learn online and constantly fine-tune the model&#39;s structure. In this article, a Mondrian Forest is applied to predict payloads and physical fatigue of human operators during an assistance scenario with a collaborative robot. An experiment was conducted with a total of 12 participants, where the payload was increased until participants initiated an assistance request from a Universal Robots model 10 cobot. This allowed for testing whether the Mondrian Forest can accurately predict the payload and fatigue levels from the acquired EMG signals. Overall, the approach demonstrates a promising potential toward higher awareness when an operator might require assistance from a robot and ultimately toward a more effective human–robot collaboration.},
  archive      = {J_THMS},
  author       = {Achim Buerkle and Ali Al-Yacoub and William Eaton and Melanie Zimmer and Thomas Bamber and Pedro Ferreira and Ella-Mae Hubbard and Niels Lohse},
  doi          = {10.1109/THMS.2023.3259139},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {520-528},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An incremental learning approach to detect muscular fatigue in human– robot collaboration},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-attention feature fusion network for accurate
estimation of finger kinematics from surface electromyographic signals.
<em>THMS</em>, <em>53</em>(3), 512–519. (<a
href="https://doi.org/10.1109/THMS.2023.3252817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous and proportional control (SPC) based on surface electromyographic (sEMG) signals has led to a broad range of applications. However, due to the limitation in the generalization and stability of current machine learning algorithms, these methods can only estimate less than 15 simultaneuous and proportional (SP) categories of finger movement. In this article, a novel deep learning algorithm, named multiattention feature fusion network (MAFN), is proposed to estimate comprehensive finger movement (up to 28 categories SP movements) from sEMG signals. MAFN is based on the multihead attention mechanism, which adaptively extracts essential features for analyzing the joint angles from the extracted sEMG features. Furthermore, a real-time exponential smoothing algorithm is designed for further improvement of the prediction stability. MAFN was evaluated on 28 finger movements of 38 subjects in the Ninapro_db2 dataset, and benchmarked with the state-of-the-art methods, such as temporal convolutional network (TCN) and long short term memory network (LSTM). The results demonstrated that the average Pearson correlation coefficient, root mean squared error of MAFN (0.84 ± 0.03,0.09 ± 0.01) were significantly higher than those of TCN (0.52 ± 0.06, p &lt; 0.001;0.16 ± 0.02, p &lt; 0.001) and LSTM (0.62 ± 0.06, p &lt; 0.001;0.13 ± 0.01, p &lt; 0.001). These improvements led to more stable and accurate movement predictions. Additionally, the time delay and power consumption of MAFN when applied to sEMG signals on a portable device are only 83.4 ms and 3 W, which implies prospective commercial applications.},
  archive      = {J_THMS},
  author       = {Weiyu Guo and Ning Jiang and Dario Farina and Jingyong Su and Zheng Wang and Chuang Lin and Hui Xiong},
  doi          = {10.1109/THMS.2023.3252817},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {512-519},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Multi-attention feature fusion network for accurate estimation of finger kinematics from surface electromyographic signals},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Playing with others using headphones: Musicians prefer
binaural audio with head tracking over stereo. <em>THMS</em>,
<em>53</em>(3), 501–511. (<a
href="https://doi.org/10.1109/THMS.2023.3270703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive listening systems have grown significantly over the past decade and are now an established area of scientific, artistic, and industrial research. However, scarce research has been conducted on musicians&#39; preferences for playing through headphones over binaural spatialization systems with the addition of head tracking, as opposed to classical stereophonic systems. This comparison is essential to optimally support the playing experience with others for cases of remote collaborative playing, individual instrumental practice, individual recreational music-making using backing tracks, and studio recording sessions. In this article, we study the preferences of playing musicians for a stereophonic system versus a binaural head-tracking system composed of Ambisonics technology and binaural synthesis with generalized head-related transfer functions. We conducted two experiments, each with 30 expert musicians, where participants were asked to rate and compare the 2 listening conditions while playing their instrument either seated or standing. Overall, the quantitative and qualitative results indicated a generalized preference for the binaural system with head tracking over the stereophonic system, with higher ratings for localization, immersion, social presence, realism, and connection with other musicians. Moreover, participants moved their heads significantly more in the binaural conditions. This phenomenon may be explained by the higher engagement and arousal due to the improved auditory experience, or alternatively by the presence of embodied music cognition mechanisms that cause a higher degree of exploration to better understand the action–perception loop. These findings highlight the need for progressing current commercial hardware and software systems used by musicians while playing over headphones.},
  archive      = {J_THMS},
  author       = {Matteo Tomasetti and Luca Turchet},
  doi          = {10.1109/THMS.2023.3270703},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {501-511},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Playing with others using headphones: Musicians prefer binaural audio with head tracking over stereo},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Underwater attentional generative adversarial networks for
image enhancement. <em>THMS</em>, <em>53</em>(3), 490–500. (<a
href="https://doi.org/10.1109/THMS.2023.3261341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, to exclusively suppress unuseful underwater noise feature and effectively avoid overenhancement, simultaneously, an underwater attentional generative adversarial network (UAGAN) is innovatively established. Main contributions are as follows: combining dense concatenation with global maximum and average pooling techniques, a cascade dense-channel attention (CDCA) module is devised to adaptively distinguish noise feature and recalibrate channel weight, simultaneously, such that low-contribution feature map can be effectively suppressed; to sufficiently capture long-range dependence between any two nonlocal spatial patches, the position attention (PA) module is created such that the deviation among independent patches can be sufficiently eliminated, thereby avoiding overenhancement; and in conjunction with CDCA and PA modules, the entire UAGAN framework is eventually developed in an end-to-end manner. Comprehensive experiments conducted on underwater image enhancement benchmark (UIEB) and underwater robot professional contest (URPC) datasets demonstrate remarkable effectiveness and superiority of the proposed UAGAN scheme by comparing with typical underwater image enhancement approaches including unsupervised color correction method, image blurriness and light absorption, underwater dark channel prior, underwater generative adversarial network, underwater convolutional neural network, and WaterNet in terms of peak signal-to-noise ratio, underwater color image quality evaluation, underwater image quality measures, etc.},
  archive      = {J_THMS},
  author       = {Ning Wang and Tingkai Chen and Xiangjun Kong and Yanzheng Chen and Rongfeng Wang and Yongjun Gong and Shiji Song},
  doi          = {10.1109/THMS.2023.3261341},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {490-500},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Underwater attentional generative adversarial networks for image enhancement},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Evaluation of short-range depth sonifications for
visual-to-auditory sensory substitution. <em>THMS</em>, <em>53</em>(3),
479–489. (<a href="https://doi.org/10.1109/THMS.2023.3265972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual-to-auditory sensory substitution devices convert visual information into sound and can provide valuable assistance for blind people. Recent iterations of these devices rely on depth sensors. Rules for converting depth into sound (i.e., the sonifications) are often designed arbitrarily, with no strong evidence for choosing one over another. The purpose of this article is to compare and understand the effectiveness of five depth sonifications in order to assist the design process of future visual-to-auditory systems for blind people, which rely on depth sensors. The frequency, amplitude, and reverberation of the sound as well as the repetition rate of short high-pitched sounds and the signal-to-noise ratio of a mixture between pure sound and noise are studied. We conducted positioning experiments with 28 sighted blindfolded participants. Stage 1 incorporates learning phases followed by depth estimation tasks. Stage 2 adds the additional challenge of azimuth estimation to the first stage&#39;s protocol. Stage 3 tests learning retention by incorporating a 10-min break before retesting depth estimation. The best depth estimates in stage 1 were obtained with the sound frequency and the repetition rate of beeps. In stage 2, the beep repetition rate yielded the best depth estimation, and no significant difference was observed for the azimuth estimation. Results of stage 3 showed that the beep repetition rate was the easiest sonification to memorize. Based on the statistical analysis of the results, we discuss the effectiveness of each sonification and compare with other studies that encode depth into sounds. Finally, we provide recommendations for the design of depth encoding.},
  archive      = {J_THMS},
  author       = {Louis Commère and Jean Rouat},
  doi          = {10.1109/THMS.2023.3265972},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {479-489},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Evaluation of short-range depth sonifications for visual-to-auditory sensory substitution},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wheelchair-centered omnidirectional gaze-point estimation in
the wild. <em>THMS</em>, <em>53</em>(3), 466–478. (<a
href="https://doi.org/10.1109/THMS.2023.3263541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a dataset named Wheelchair-OmniGaze and a method of estimating the gaze point of a wheelchair user that can cope with large head motion as the first step for understanding the intention of a wheelchair user. In the proposed method, the wheelchair user&#39;s face is observed by two remote cameras, called face cameras, mounted on the two front sides of the wheelchair, while a spherical camera with a 360° field of view, called a spherical scene camera, is mounted to the top rear to observe the surrounding scenes. A user&#39;s gaze point at the environment is determined by combining the facial information acquired by the face cameras with the spherical image acquired by the spherical scene camera. Concretely, a convolutional neural network–long short-term memory network is used to learn gaze points of the spherical image from the two face image sequences. Using two face cameras enables a user&#39;s face to be observed even for a large head motion. Using a spherical scene camera mounted on a wheelchair, the representation of a user&#39;s gaze points at a wheelchair-centered spherical camera coordinate can be obtained, which is independent of a user&#39;s head motion. The experimental results show that our method achieved competitive performance in comparison with the state-of-the-art method. Since our problem definition is different from the existing research, the experimental results of this article can be seen as a baseline for our built dataset.},
  archive      = {J_THMS},
  author       = {Le Wang and Shigang Li},
  doi          = {10.1109/THMS.2023.3263541},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {466-478},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Wheelchair-centered omnidirectional gaze-point estimation in the wild},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intent prediction in human–human interactions.
<em>THMS</em>, <em>53</em>(2), 458–463. (<a
href="https://doi.org/10.1109/THMS.2023.3239648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human ability to infer others&#39; intent is innate and crucial to development. Machines ought to acquire this ability for seamless interaction with humans. In this article, we propose an agent model for predicting the intent of actors in human–human interactions. This requires simultaneous generation and recognition of an interaction at any time, for which end-to-end models are scarce. The proposed agent actively samples its environment via a sequence of glimpses. At each sampling instant, the model infers the observation class and completes the partially observed body motion. It learns the sequence of body locations to sample by jointly minimizing the classification and generation errors. The model is evaluated on videos of two-skeleton interactions under two settings: (first person) one skeleton is the modeled agent and the other skeleton&#39;s joint movements constitute its visual observation, and (third person) an audience is the modeled agent and the two interacting skeletons&#39; joint movements constitute its visual observation. Three methods for implementing the attention mechanism are analyzed using benchmark datasets. One of them, where attention is driven by sensory prediction error, achieves the highest classification accuracy in both settings by sampling less than 50% of the skeleton joints, while also being the most efficient in terms of model size. This is the first known attention-based agent to learn end-to-end from two-person interactions for intent prediction, with high accuracy and efficiency.},
  archive      = {J_THMS},
  author       = {Murchana Baruah and Bonny Banerjee and Atulya K. Nagar},
  doi          = {10.1109/THMS.2023.3239648},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {458-463},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Intent prediction in Human–Human interactions},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BGcsSENet: Bidirectional GRU with spatial and channel
squeeze-excitation network for bundle branch block detection.
<em>THMS</em>, <em>53</em>(2), 449–457. (<a
href="https://doi.org/10.1109/THMS.2023.3244938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Left bundle branch block (LBBB) and right bundle branch block (RBBB) are the common but neglected arrhythmias clinically. However, the conventional LBBB and RBBB diagnosis methods are strenuous and time-consuming because this process relies heavily on the physician&#39;s visual observation of electrocardiogram. In this article, we developed a novel network module based on the existing bidirectional gated recurrent unit (B-GRU) network for automated LBBB, RBBB, and normal sinus rhythm detection. Inspired from spatial and channel squeeze-excitation network (scSENet), the proposed module is capable of effectively recalibrating feature representation in B-GRU using both channel squeeze-excitation and spatial squeeze-excitation modules along channel and space to emphasize useful features while suppressing weak ones, namely, BGcsSENet. To further verify its effectiveness, we leverage the standard GRU and B-GRU modules as control models across the two public databases. Extensive experiments confirm the effectiveness of BGcsSENet, while yielding better performance than existing GRU, B-GRU modules, and many published approaches. Particularly, our BGcsSENet model is the first research to refine the network architecture of B-GRU, thus demonstrating considerable application potential on lightweight devices.},
  archive      = {J_THMS},
  author       = {Jibin Wang},
  doi          = {10.1109/THMS.2023.3244938},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {449-457},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {BGcsSENet: Bidirectional GRU with spatial and channel squeeze-excitation network for bundle branch block detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-aware faster RCNN for CSI-based human action
perception. <em>THMS</em>, <em>53</em>(2), 438–448. (<a
href="https://doi.org/10.1109/THMS.2022.3225828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread deployment of commercial wireless devices, researchers begin to focus on device-free sensing tasks. In the field of action perception, existing WiFi-based sensing works mostly follow the framework in which action instances of channel state information (CSI) are first extracted and then classified. As for the part of human action detection, a majority of works adopt threshold based sliding window or frame-by-frame detection methods. However, it is hard for the former approach to set a reasonable threshold for all samples. As for the latter, it costs a relatively substantial amount of labor to label each moment of the time sequences. In order to overcome the above problems, we design an end-to-end context-aware faster region-based convolutional neural networks (RCNN) framework named Wisense to simultaneously detect the temporal boundaries as well as classify the actions. More specifically, Wisense consists of backbone net, region proposal net (RPN), pooling layer, and the prediction net, which directly regresses the action location along the time axis and classifies the action types. For the sake of wireless signal temporal detection, we transform the input into 1-D feature map and extract multiscale 1-D anchors. Besides, in order to sufficiently mine the context information, we extend the boundaries of region proposals and further establish the temporal pyramid features. Experimental results conducted in three indoor scenes validate the effectiveness of our proposed Wisense.},
  archive      = {J_THMS},
  author       = {Biyun Sheng and Fu Xiao and Linqing Gui and Zhengxin Guo},
  doi          = {10.1109/THMS.2022.3225828},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {438-448},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Context-aware faster RCNN for CSI-based human action perception},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IMGC: Interactive multiple graph clustering with constrained
laplacian rank. <em>THMS</em>, <em>53</em>(2), 427–437. (<a
href="https://doi.org/10.1109/THMS.2022.3227181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous graph clustering methods have been proposed to explore aggregation structures across multiple graphs. In these methods, single-graph features are merely considered or multigraph features are simply weighted, which are insufficient for the construction of reasonable multiple graph clustering features, since the association information between pairwise graphs is ignored and the varied local correlations might influence the clustering preference. Thus, we propose an interactive multiple graph clustering model, iMGC, in this article, to achieve reasonable multiple graph clustering features, which cannot only express multiple relationships, but also preserve associations of nodes across multiple graphs. First, a unified graph matrix is constructed with the combination of structural differences quantified by graph representation learning, which is further optimized by minimizing the difference of structural characteristics between it and each single graph matrix. Thus, multiple relationships are well integrated and expressed, while the varied local correlations within different graphs are also balanced in the unified graph matrix. Then, a constrained Laplacian rank is applied on the unified graph matrix to generate the unified clustering result directly, which is able to preserve association features across multiple graphs. Furthermore, we provide a set of visualization and interaction interfaces, enabling users to intuitively optimize and evaluate the multiple graph clustering features, and interactively explore the multiple graphs. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of iMGC in the clustering performance from various perspectives and exploration of multiple graphs.},
  archive      = {J_THMS},
  author       = {Zhiguang Zhou and Ling Sun and Haoxuan Wang and Wanghao Yu and Yuhua Liu and Xiang Zhang and Yigang Wang and Wei Chen},
  doi          = {10.1109/THMS.2022.3227181},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {427-437},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IMGC: Interactive multiple graph clustering with constrained laplacian rank},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel dual-hand detection by using hand and body features
for robot teleoperation. <em>THMS</em>, <em>53</em>(2), 417–426. (<a
href="https://doi.org/10.1109/THMS.2023.3243774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual hand-based robot teleoperation provides a powerful guarantee for robots to complete complex tasks. However, detection and distinction of dual hands on images are difficult because of the small differences between left and right hands. To solve this problem, a parallel dual-hand detection and distinction method that combines the features of hands with the relationship features between the dual hands and body pose is proposed to achieve robust and accurate dual-hand detection. This parallel dual-hand detection method includes a hand detection module, a body pose estimation module, and a fusion module. In the hand detection module, a hand detector that realizes fast and accurate hand detection by detecting the center and corner points of hands is designed. In the body pose estimation module, a body pose estimator with dual-hand positions is proposed. The fusion module is designed to fuse hand detection and dual-hand estimation results to achieve distinction between left and right hands. Finally, the parallel dual-hand detection method is applied to a bimanual robot teleoperation system by using a designed dual-hand teleoperation framework. The proposed parallel dual-hand detection method can achieve 98.54% mAP of hand detection with 18 frames per second on a custom dual-hand detection dataset, and the bimanual robot teleoperation method can achieve 95.4% average accuracy for teleoperation tasks. Experimental results show the high accuracy and speed of our proposed parallel dual-hand detection method and its practicability in bimanual robot teleoperation.},
  archive      = {J_THMS},
  author       = {Qing Gao and Zhaojie Ju and Yongquan Chen and Qiwen Wang and Yinan Zhao and Shiwu Lai},
  doi          = {10.1109/THMS.2023.3243774},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {417-426},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Parallel dual-hand detection by using hand and body features for robot teleoperation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). After a decade of teleimpedance: A survey. <em>THMS</em>,
<em>53</em>(2), 401–416. (<a
href="https://doi.org/10.1109/THMS.2022.3231703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant progress made in making robots more intelligent and autonomous, today, teleoperation remains a dominant robot control paradigm for the execution of complex and highly unpredictable tasks. Attempts have been made to make teleoperation systems stable, easy to use, and efficient in terms of physical interactions between the follower remote robot and the environment. In particular, the emergence of torque-controlled robots has permitted to regulate the interaction forces from a distance through direct force or impedance control, enabling them to engage in complex interaction tasks. Exploiting this feature, the concept of teleimpedance control was introduced as an alternative method to bilateral force-reflecting teleoperation. The aim was to create a feed-froward yet contact-efficient teleoperation by enriching the leader commands with desired impedance profiles while executing a task. Since then, the teleimpedance concept has found its way into a wide range of interface and controller designs, as well as application domains. Accordingly, after a decade of research progress, this survey aims to provide: first, a convenient introduction of the concept to new researchers in the field, second, consolidate the existing state-of-the-art for active researchers, third, and discuss the pros and cons of different methods in terms of interface and force feedback to provide guidelines for different applications and future developments.},
  archive      = {J_THMS},
  author       = {Luka Peternel and Arash Ajoudani},
  doi          = {10.1109/THMS.2022.3231703},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {401-416},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {After a decade of teleimpedance: A survey},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of traditional cultural experiences using augmented
reality based on environmental presence. <em>THMS</em>, <em>53</em>(2),
390–400. (<a href="https://doi.org/10.1109/THMS.2022.3232133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional cultural experiences are valuable for tourist destinations. Previous studies have developed systems for experiencing traditional culture using augmented reality (AR). However, few works have considered the effectiveness of different types of AR experiences of traditional cultural elements. In this article, we focus on equipment elements and environment elements , and propose design recommendations for an AR traditional cultural experience system that considers differences in the environmental presence of traditional cultural experiences. We conducted a comparative study with 49 users to understand the impact of these differences. We compared three options for handling environmental presence, including nonpresence , AR-presence , and real-presence . The results show that presenting environmental elements in a state of real-presence improved an AR experience of traditional culture. Additionally, we found that this setting worked positively for Japanese participants, whereas Chinese participants preferred environmental elements in the state of nonpresence .},
  archive      = {J_THMS},
  author       = {Yuya Ieiri and Hung-Ya Tsai and Kenta Mizobuchi and Shao Tengfei and Reiko Hishiyama},
  doi          = {10.1109/THMS.2022.3232133},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {390-400},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Design of traditional cultural experiences using augmented reality based on environmental presence},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring the suitability of using virtual reality and
augmented reality for anatomy training. <em>THMS</em>, <em>53</em>(2),
378–389. (<a href="https://doi.org/10.1109/THMS.2023.3235250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on alternative ways to provide anatomy learning and training has increased over the past few years, especially since the COVID-19 pandemic. Virtual reality (VR) and augmented reality (AR) represent two promising alternatives in this regard. For this reason, in this work, we analyze the suitability of applying VR and AR for anatomy training, comparing an optical-based AR setup and a semi-immersive setup based on a VR table, using the same anatomy training software and the same interaction system. The AR-based setup uses a Magic Leap One, whereas the VR table is configured through the use of stereoscopic TV displays and a motion-capture system. This experiment builds on a previous one (Vergel et al., 2020) on which we have improved the AR-based setup and increased the complexity of one of the two tasks. The goal of this new experiment is to confirm whether the changes made in the setups modify the previous conclusions. Our hypothesis is that the improved AR-based setup will be more suitable, for anatomy training, than the VR-based setup. For this reason, we conducted an experimental research with 45 participants, comparing the use of an anatomy training software. Objective and subjective data were collected. The results show that the AR-based setup is the preferred choice. The differences in measurable performance were small but also favorable to the AR setup. In addition, participants provided better subjective ratings for the AR-based setup, confirming our initial hypothesis. Nevertheless, both setups offer a similar overall performance and provide excellent results in the subjective measures, with both systems approaching the highest possible values.},
  archive      = {J_THMS},
  author       = {Ramiro Serrano-Vergel and Pedro Morillo and Sergio Casas-Yrurzum and Carolina Cruz-Neira},
  doi          = {10.1109/THMS.2023.3235250},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {378-389},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Exploring the suitability of using virtual reality and augmented reality for anatomy training},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Augmenting conversations with comic-style word balloons.
<em>THMS</em>, <em>53</em>(2), 367–377. (<a
href="https://doi.org/10.1109/THMS.2022.3224767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach for enabling comic-style conversation in mixed reality to assist face-to-face conversation on-site or remotely. Our approach brings word balloons of comic-style conversation to the real world. The word balloons can adapt to mixed reality scenes, such as the 3-D head motion of the speaker, the comic styles, and the speech. During the conversation, our approach updates the word balloons continuously in the object space and discretely in the image space, guided by a field learned from comics. Quantitative experiments and perceptual studies were conducted to evaluate and compare our approach with alternatives. The results from the user study and ablation study demonstrated that our approach turns out to be practical for assisting face-to-face conversation.},
  archive      = {J_THMS},
  author       = {Heng Zhang and Lifeng Zhu and Qingdi Chen and Aiguo Song and Lap-Fai Yu},
  doi          = {10.1109/THMS.2022.3224767},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {367-377},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Augmenting conversations with comic-style word balloons},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fusion of spatial, temporal, and spectral EEG signatures
improves multilevel cognitive load prediction. <em>THMS</em>,
<em>53</em>(2), 357–366. (<a
href="https://doi.org/10.1109/THMS.2023.3235003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive load prediction is one of the most important issues in the nascent field of neuroergonomics, and it has significant value in real-world applications. Most of the previous studies of cognitive load prediction only utilized electroencephalography (EEG)-based spectral signatures or interchannel connectivity, ignoring abundant temporal microstate features, which may represent the transient topologies of EEG signals. Furthermore, previous studies have mostly focused on the binary-level classification of cognitive load for single-type cognitive tasks. To date, there are few studies on the multilevel prediction of cognitive load during mixed cognitive tasks. Here, we first designed a new paradigm termed the “finding fault game,” mixing multiple tasks of memory, counting, and visual search, and then developed a multidimensional analysis framework to improve cognitive load prediction using a fusion of spatial, temporal, and spectral EEG features. Specifically, EEG-based functional connectivity, microstates and power spectral densities (PSD) were calculated for three cognitive load levels. Twelve adult subjects participated in the study. The experimental results show that increased cognitive load was associated with elevated theta and degraded alpha power and significant changes in interchannel connectivity and microstates, and that fusing the three types of EEG features improved the performance of three-level cognitive load prediction, achieving the accuracies of greater than 80% in the cross-validation, real-time, and over-time prediction. The findings suggest that all three types of EEG features can serve as signatures of cognitive load and that their fusion can improve multilevel prediction.},
  archive      = {J_THMS},
  author       = {Yingxin Liu and Yang Yu and Zeqi Ye and Ming Li and Yifan Zhang and Zongtan Zhou and Dewen Hu and Ling-Li Zeng},
  doi          = {10.1109/THMS.2023.3235003},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {357-366},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Fusion of spatial, temporal, and spectral EEG signatures improves multilevel cognitive load prediction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated eye movement classification based on EMG of EOM
signals using FBSE-EWT technique. <em>THMS</em>, <em>53</em>(2),
346–356. (<a href="https://doi.org/10.1109/THMS.2023.3238113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate automated eye movement classification is gaining importance in the field of human–computer interaction (HCI). The present article aims at the classification of six types of eye movements from electromyogram (EMG) of extraocular muscles (EOM) signals using the Fourier–Bessel series expansion-based empirical wavelet transform (FBSE-EWT) with time and frequency-domain (TAFD) features. The FBSE-EWT of EMG signals results in Fourier–Bessel intrinsic mode functions (FBIMFs), which correspond to the frequency contents in the signal. A hybrid approach is used to select the prominent FBIMFs followed by the statistical and signal complexity-based feature extraction. Furthermore, metaheuristic optimization algorithms are employed to reduce the feature space dimension. The discrimination ability of the reduced feature set is verified by Kruskal–Wallis statistical test. Multiclass support vector machine (MSVM) has been employed for classification. First, the classification has been performed with TAFD features followed by the combination of TAFD and FBSE-EWT-based reduced feature set. The combination of TAFD and FBSE-EWT-based feature set has provided good classification performance. This study demonstrates the efficacy of FBSE-EWT and subsequent metaheuristic feature selection algorithms in classifying the eye movements from EMG of EOM signals. The combination of TAFD and the selected features through salp swarm optimization algorithm has provided maximum classification accuracy of 98.91% with MSVM employing Gaussian and radial basis function kernels. Thus, the proposed approach has the potential to be used in HCI applications involving biomedical signals.},
  archive      = {J_THMS},
  author       = {Sibghatullah Inayatullah Khan and Ram Bilas Pachori},
  doi          = {10.1109/THMS.2023.3238113},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {346-356},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Automated eye movement classification based on EMG of EOM signals using FBSE-EWT technique},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Silent speech recognition based on high-density surface
electromyogram using hybrid neural networks. <em>THMS</em>,
<em>53</em>(2), 335–345. (<a
href="https://doi.org/10.1109/THMS.2022.3226197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a silent speech recognition approach based on high-density (HD) surface electromyogram (sEMG) using hybrid neural networks that support anomaly detection. In the hybrid networks, both a convolutional long short-term memory module and an autoencoder module were designed to extract discriminative spatio-temporal features and potentially identify any anomaly patterns, respectively. To verify the effectiveness of the proposed method, experimental data were recorded using HD-sEMG arrays with 64 channels from 11 subjects subvocalizing 33 Chinese words and articulating 9 anomaly patterns. The proposed method significantly outperformed other comparison methods ( p &lt; 0.05) and achieved the highest anomaly detection rate of 90.61% while maintaining a high level of target word-pattern classification accuracy of 82.30%. These findings demonstrate the effectiveness of the proposed method for improving the robustness of the SSR approach based on HD-sEMG recordings against anomaly muscular activities. This article also provides a novel solution for building practical and robust sEMG-based SSR systems with broad applications, such as instant messaging and human-computer interaction.},
  archive      = {J_THMS},
  author       = {Xi Chen and Yuanfei Xia and Yong Sun and Le Wu and Xiang Chen and Xun Chen and Xu Zhang},
  doi          = {10.1109/THMS.2022.3226197},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {335-345},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Silent speech recognition based on high-density surface electromyogram using hybrid neural networks},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RFPad: Enabling device-free handwriting recognition with a
tag square. <em>THMS</em>, <em>53</em>(2), 325–334. (<a
href="https://doi.org/10.1109/THMS.2023.3236605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a natural interaction approach, handwriting acts as an essential role in human–computer interaction. In order to achieve ubiquitous and reliable applications, a handwriting recognition system should be easy to use without any intrusion and robust to environment changes, which cannot be satisfied in most of existing approaches. In this article, we present RFPad , a device-free handwriting recognition system with a tag square consisting of four low-cost passive radio frequency identification (RFID) tags. With such an ingenious tag square, we transform the flat surface into a handwriting platform, and build a geometry-based theoretical model between the finger positions and the tags&#39; phase variations so that the finger trajectory can be accurately tracked and handwriting can be recognized with the phases segmented from continuous signals. We implement a prototype of RFPad using commercial off-the-shelf RFID devices and conduct extensive experiments to evaluate its performance. Experiment results show that our RFPad can track finger movement with an average error of 1 cm and achieve average recognition accuracy of 94.08 $\%$ for all 26 handwriting capital letters.},
  archive      = {J_THMS},
  author       = {Wenyuan Liu and Huan Du and Binbin Li and Jincheng Li and Zhuo Chang and Lin Wang},
  doi          = {10.1109/THMS.2023.3236605},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {325-334},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {RFPad: Enabling device-free handwriting recognition with a tag square},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general pipeline for online gesture recognition in
human–robot interaction. <em>THMS</em>, <em>53</em>(2), 315–324. (<a
href="https://doi.org/10.1109/THMS.2022.3227309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in robotics have allowed the introduction of robots assisting and working together with human subjects. To promote their use and diffusion, intuitive and user-friendly interaction means should be adopted. In particular, gestures have become an established way to interact with robots since they allow to command them in an intuitive manner. In this article, we focus on the problem of gesture recognition in human–robot interaction (HRI). While this problem has been largely studied in the literature, it poses specific constraints when applied to HRI. We propose a framework consisting in a pipeline devised to take into account these specific constraints. We implement the proposed pipeline considering, as an example, an evaluation use case. To this end, we consider standard machine learning algorithms for the classification stage and evaluate their performance considering different performance metrics for a thorough assessment.},
  archive      = {J_THMS},
  author       = {Valeria Villani and Cristian Secchi and Marco Lippi and Lorenzo Sabattini},
  doi          = {10.1109/THMS.2022.3227309},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {315-324},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A general pipeline for online gesture recognition in Human–Robot interaction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of a touch-gesture- and a keystroke-based
password method: Toward shoulder-surfing resistant mobile user
authentication. <em>THMS</em>, <em>53</em>(2), 303–314. (<a
href="https://doi.org/10.1109/THMS.2023.3236328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive use of mobile devices exposes users to an elevated risk of shoulder-surfing attacks. Despite the prior work on shoulder-surfing resistance of mobile user authentication methods, there is a lack of empirical studies on textual password authentication methods, particularly the hybrid passwords that integrate textual passwords with biometrics. To fill the literature gap, this research compares two hybrid password methods, touch-gesture- and keystroke-based passwords, with respect to their shoulder-surfing resistance performance. We select a touch-gesture-based password method that deploys multiple shoulder-surfing resistance strategies and a keystroke-based password method that leverages keystroke dynamics. To gain a holistic understanding of these password methods, we examine them under a variety of shoulder-surfing settings by varying interaction mode, observation angle, entry error, and observation effort. Going beyond effectiveness metrics, we also introduce efficiency metrics to assess shoulder-surfing resistance performance more comprehensively. We hypothesize and test the effects of shoulder-surfing settings by conducting both a longitudinal lab experiment and an online experiment with diversified participants. The results of both studies demonstrate the superior performance of the touch-gesture-based password method to the keystroke-based counterpart. The results also provide evidence for the effects of interaction mode, observation angle, and observation effort on shoulder-surfing resistance of hybrid passwords. Our findings offer suggestions for the design and strategies for strengthening the security of password authentication methods.},
  archive      = {J_THMS},
  author       = {Lina Zhou and Kanlun Wang and Jianwei Lai and Dongsong Zhang},
  doi          = {10.1109/THMS.2023.3236328},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {303-314},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A comparison of a touch-gesture- and a keystroke-based password method: Toward shoulder-surfing resistant mobile user authentication},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empirical study on workers’ preferences in human–robot
task assignment in industrial assembly systems. <em>THMS</em>,
<em>53</em>(2), 293–302. (<a
href="https://doi.org/10.1109/THMS.2022.3230667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative industrial robotic arms (cobots) are integrated industrial assembly systems relieving their human coworkers from monotonous tasks and achieving productivity gains. The question of task allocation arises in the organization of these human–robot interactions. State of the art shows static, compensatory task allocation approaches in current assembly systems and flexible, adaptive task sharing (ATS) approaches in human factors research. The latter should exploit the economic and ergonomic advantages of cobot usage. Previous research results did not provide a clear insight into whether industrial workers prefer static or adaptive task allocation and which tasks workers do prefer to assign to cobots. Therefore, we set up a cobot demonstrator with a realistic industrial assembly use case and did a user study with experienced workers from the shop floor (n = 25). The aim of the user study is to provide a systematic understanding and evaluation of workers&#39; preferences in a practical context of human–robot interaction (HRI) in assembly. Our main findings are that participants preferred the ATS concept to a predetermined task allocation and reported increased satisfaction with the allocation. Results show that participants are more likely to give manual tasks to the cobot in contrast to cognitive tasks. It shows that workers do not entrust all tasks to robots, but like to take over cognitive tasks by themselves. This work contributes to the design of human-centered HRI in industrial assembly systems.},
  archive      = {J_THMS},
  author       = {Christina Schmidbauer and Setareh Zafari and Bernd Hader and Sebastian Schlund},
  doi          = {10.1109/THMS.2022.3230667},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {293-302},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An empirical study on workers&#39; preferences in Human–Robot task assignment in industrial assembly systems},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Limited information shared control: A potential game
approach. <em>THMS</em>, <em>53</em>(2), 282–292. (<a
href="https://doi.org/10.1109/THMS.2022.3216789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a systematic method for the design of a limited information shared control (LISC). LISC is used in applications where not all system states or references trajectories are measurable by the automation. Typical examples are partially human controlled systems, in which some subsystems are fully controlled by the automation, whereas others are controlled by a human. The proposed systematic design uses a novel class of games to model human–machine interaction: the near potential differential games (NPDG). We provide a necessary and sufficient condition for the existence of an NPDG and derive an algorithm for finding a NPDG, which completely describes a given differential game. The proposed design method is applied to the control of a large vehicle manipulator system, in which the manipulator is controlled by the human operator and the vehicle is fully automated. The suitability of the NPDG modeling differential games is verified in simulations leading to a faster and more accurate controller design compared with manual tuning. Furthermore, the overall design process is validated in a study with 16 test subjects indicating the applicability of the proposed concept in real applications.},
  archive      = {J_THMS},
  author       = {Balint Varga and Jairo Inga and Sören Hohmann},
  doi          = {10.1109/THMS.2022.3216789},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {282-292},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Limited information shared control: A potential game approach},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal sensing and computational intelligence for
situation awareness classification in autonomous driving. <em>THMS</em>,
<em>53</em>(2), 270–281. (<a
href="https://doi.org/10.1109/THMS.2023.3234429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining situation awareness (SA) is essential for drivers to deal with the situations that Society of Automotive Engineers (SAE) Level 3 automated vehicle systems are not designed to handle. Although advanced physiological sensors can enable continuous SA assessments, previous single-modality approaches may not be sufficient to capture SA. To address this limitation, the current study demonstrates a multimodal sensing approach for objective SA monitoring. Physiological sensor data from electroencephalogram and eye-tracking were recorded for 30 participants as they performed three secondary tasks during automated driving scenarios that consisted of a pre-takeover (pre-TOR) request segment and a post-TOR segment. The tasks varied in terms of how visual attention was allocated in the pre-TOR segment. In the post-TOR segment, drivers were expected to gather information from the driving environment in preparation for a vehicle-to-driver transition. Participants&#39; ground-truth SA level was measured using the Situation Awareness Global Assessment Techniques (SAGAT) after the post-TOR segment. A total of 23 physiological features were extracted from the post-TOR segment to train computational intelligence models. Results compared the performance of five different classifiers, the ground-truth labeling strategies, and the features included in the model. Overall, the proposed neural network model outperformed other machine learning models and achieved the best classification accuracy (90.6%). A model with 11 features was optimal. In addition, the multi-physiological sensor-model outperformed the single sensing model by comparing prediction performance. Our results suggest that multimodal sensing model can objectively predict SA. The results of this study provide new insight into how physiological features contribute to the SA assessment.},
  archive      = {J_THMS},
  author       = {Jing Yang and Nade Liang and Brandon J. Pitts and Kwaku O. Prakah-Asante and Reates Curry and Mike Blommer and Radhakrishnan Swaminathan and Denny Yu},
  doi          = {10.1109/THMS.2023.3234429},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {270-281},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Multimodal sensing and computational intelligence for situation awareness classification in autonomous driving},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vision-based human pose estimation via deep learning: A
survey. <em>THMS</em>, <em>53</em>(1), 253–268. (<a
href="https://doi.org/10.1109/THMS.2022.3219242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) has attracted a significant amount of attention from the computer vision community in the past decades. Moreover, HPE has been applied to various domains, such as human–computer interaction, sports analysis, and human tracking via images and videos. Recently, deep learning-based approaches have shown state-of-the-art performance in HPE-based applications. Although deep learning-based approaches have achieved remarkable performance in HPE, a comprehensive review of deep learning-based HPE methods remains lacking in literature. In this article, we provide an up-to-date and in-depth overview of the deep learning approaches in vision-based HPE. We summarize these methods of 2-D and 3-D HPE, and their applications, discuss the challenges and the research trends through bibliometrics, and provide insightful recommendations for future research. This article provides a meaningful overview as introductory material for beginners to deep learning-based HPE, as well as supplementary material for advanced researchers.},
  archive      = {J_THMS},
  author       = {Gongjin Lan and Yu Wu and Fei Hu and Qi Hao},
  doi          = {10.1109/THMS.2022.3219242},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {253-268},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Vision-based human pose estimation via deep learning: A survey},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LMFNet: A lightweight multiscale fusion network with
hierarchical structure for low-quality 3-d face recognition.
<em>THMS</em>, <em>53</em>(1), 239–252. (<a
href="https://doi.org/10.1109/THMS.2022.3199777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3-D) face recognition (FR) can improve the usability and user-friendliness of human–machine interaction. In general, 3-D FR can be divided into high-quality and low-quality 3-D FR according to different interaction scenarios. The low-quality data can be easily obtained, so its application prospect is more extensive. However, the challenge is how to balance the trade-offs between data accuracy and real-time performance. To solve this problem, we propose a lightweight multiscale fusion network (LMFNet) with a hierarchical structure based on single-mode data for low-quality 3-D FR. First, we design a backbone network with only five feature extraction blocks to reduce computational complexity and improve the inference speed. Second, we devise a mid-low adjacent layer with a multiscale feature fusion (ML-MSFF) module to extract the facial texture and contour information, and a mid-high adjacent layer with a multiscale feature fusion (MH-MSFF) module to obtain the discriminative information in high-level features. Then, a hierarchical multiscale feature fusion (HMSFF) module is formed by combining these two modules mentioned above to acquire the local information of different scales. Finally, we enhance the expression of features by integrating HMSFF with a global convolutional neural network for improving recognition accuracy. Experiments on Lock3DFace, KinectFaceDB, and IIIT-D datasets demonstrate that our proposed LMFNet can achieve superior performance on low-quality datasets. Furthermore, experiments on the cross-quality database based on Bosphorus and the different intensity noise low-quality datasets based on UMB-DB and Bosphorus show that our network is robust and has a high generalization ability. It satisfies the real-time requirement, which lays a foundation for a smooth and user-friendly interactive experience.},
  archive      = {J_THMS},
  author       = {Panzi Zhao and Yue Ming and Xuyang Meng and Hui Yu},
  doi          = {10.1109/THMS.2022.3199777},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {239-252},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {LMFNet: A lightweight multiscale fusion network with hierarchical structure for low-quality 3-D face recognition},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting activities of daily living for the coming time
period in smart homes. <em>THMS</em>, <em>53</em>(1), 228–238. (<a
href="https://doi.org/10.1109/THMS.2022.3176213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Activity prediction aims to predict what activities will occur in the future. In smart homes, to facilitate the daily living of the residents, automated or assistive services are provided. To provide these services, the ability of activity prediction is necessary. When we make a prediction, most of the existing works focus on predicting information about the next activity. However, in a smart home environment, compared with just predicting information about the next activity, another type of activity prediction problem has more practical value: predicting what activities will occur in the coming time period of a certain length. The necessity of this type of prediction is due to the purpose of the smart homes and the character of the activities. Many activities in smart homes need preparation time before being performed. Through this type of prediction, activities could be predicted sufficient time before being performed, and there will be adequate time for the smart home system to prepare corresponding automated or assistive services. As more than one activity could occur within the time period in which the prediction is made, this problem is a multilabel classification problem. In this article, we first give a formulation of the problem. Then, we propose a deep learning model to address it. The proposed model consists of the convolutional part, the long short-term memory layer, and the multilabel output layer. Experiments on real-world datasets show the effectiveness of our model.},
  archive      = {J_THMS},
  author       = {Wei Wang and Jianbo Li and Ying Li and Xueshi Dong},
  doi          = {10.1109/THMS.2022.3176213},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {228-238},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Predicting activities of daily living for the coming time period in smart homes},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weighting-based deep ensemble learning for recognition of
interventionalists’ hand motions during robot-assisted intravascular
catheterization. <em>THMS</em>, <em>53</em>(1), 215–227. (<a
href="https://doi.org/10.1109/THMS.2022.3226038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot-assisted intravascular interventions have evolved as unique treatments approach for cardiovascular diseases. However, the technology currently has low potentials for catheterization skill evaluation, slow learning curve, and inability to transfer experience gained from manual interventions. This study proposes a new weighting-based deep ensemble model for recognizing interventionalists&#39; hand motions in manual and robotic intravascular catheterization. The model has a module of neural layers for extracting features in electromyography data, and an ensemble of machine learning methods for classifying interventionalists&#39; hand gestures as one of the six hand motions used during catheterization. A soft-weighting technique is applied to guide the contributions of each base learners. The model is validated with electromyography data recorded during in-vitro and in-vivo trials and labeled as many-to-one sequences. Results obtained show the proposed model could achieve 97.52% and 47.80% recognition performances on test samples in the in-vitro and in-vivo data, respectively. For the latter, transfer learning was applied to update weights from the in-vitro data, and the retrained model was used for recognizing the hand motions in the in-vivo data. The weighting-based ensemble was evaluated against the base learners and the results obtained shows it has a more stable performance across the six hand motion classes. Also, the proposed model was compared with four existing methods used for hand motion recognition in intravascular catheterization. The results obtained show our model has the best recognition performances for both the in-vitro and in-vivo catheterization datasets. This study is developed toward increasing interventionalists&#39; skills in robot-assisted catheterization.},
  archive      = {J_THMS},
  author       = {Olatunji Mumini Omisore and Toluwanimi Oluwadara Akinyemi and Wenjing Du and Wenke Duan and Rita Orji and Thanh Nho Do and Lei Wang},
  doi          = {10.1109/THMS.2022.3226038},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {215-227},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Weighting-based deep ensemble learning for recognition of interventionalists’ hand motions during robot-assisted intravascular catheterization},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent decision-making and human language communication
based on deep reinforcement learning in a wargame environment.
<em>THMS</em>, <em>53</em>(1), 201–214. (<a
href="https://doi.org/10.1109/THMS.2022.3225867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of artificial intelligence (AI) in games has been significantly developed and attracted much attention over the past few years. This article not only leverages the reinforcement learning multiagent deep deterministic policy gradient algorithm to realize the dynamic decision-making of game AI but also creatively incorporates deep learning and natural language processing technologies in the wargame field to transform game context situation maps into textual suggestions in wargame confrontation. In this article, we effectively integrate reinforcement learning technologies, deep learning technologies, and natural language processing technologies to generalize the semantic text output at state-of-the-art accuracy, which plays an important role in human understanding of game AI behavior. The experimental results are promising and can be used to verify the feasibility, accuracy, and performance of our proposed model in extensive simulations against benchmarking methods.},
  archive      = {J_THMS},
  author       = {Yuxiang Sun and Bo Yuan and Qi Xiang and Jiawei Zhou and Jiahui Yu and Di Dai and Xianzhong Zhou},
  doi          = {10.1109/THMS.2022.3225867},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {201-214},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Intelligent decision-making and human language communication based on deep reinforcement learning in a wargame environment},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). State-of-the-art in smart contact lenses for human–machine
interaction. <em>THMS</em>, <em>53</em>(1), 187–200. (<a
href="https://doi.org/10.1109/THMS.2022.3224683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contact lenses have traditionally been used for vision correction applications. Recent advances in microelectronics and nanofabrication on flexible substrates have now enabled sensors, circuits, and other essential components to be integrated on a small contact lens platform. This has opened up the possibility of using contact lenses for a range of human–machine interaction (HMI) applications, including vision assistance, eye tracking, displays, and healthcare. In this article, we systematically review the range of smart contact lens materials, device architectures, and components that facilitate this interaction for different applications. In fact, evidence from our systematic review demonstrates that these lenses can be used to display information, detect eye movements, restore vision, and detect certain biomarkers in tear fluid. Consequently, whereas previous state-of the-art reviews in contact lenses focused exclusively on biosensing, our systematic review covers a wider range of smart contact lens applications in HMI. Moreover, we present a new method of classifying the literature on smart contact lenses according to their six constituent building blocks, which are the sensing, energy management, driver electronics, communications, substrate, and the input/output interfacing modules. Based on recent developments in each of these categories, we speculate the challenges and opportunities of smart contact lenses for HMI. Moreover, based on our analysis of the state-of-the-art, we develop guidelines for the future design of a self-powered smart contact lens concept with integrated energy harvesters, sensors, and communications modules. Therefore, our review is a critical evaluation of current data and is presented with the aim of guiding researchers to new research directions in smart contact lenses.},
  archive      = {J_THMS},
  author       = {Yuanjie Xia and Mohamed Khamis and F. Anibal Fernandez and Hadi Heidari and Haider Butt and Zubair Ahmed and Tim Wilkinson and Rami Ghannam},
  doi          = {10.1109/THMS.2022.3224683},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {187-200},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {State-of-the-art in smart contact lenses for Human–Machine interaction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Threshold-free phase segmentation and zero velocity
detection for gait analysis using foot-mounted inertial sensors.
<em>THMS</em>, <em>53</em>(1), 176–186. (<a
href="https://doi.org/10.1109/THMS.2022.3228515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait analysis is a prosperous tool for the clinical evaluation and diagnosis. In this article, a portable gait analysis system based on foot-mounted inertial sensors is established. A threshold-free method using a long short-term memory recurrent neural network is constructed to segment four typical gait phases in a gait sequence for the temporal parameters analysis. Segmentation accuracy reaches over 95% across recruited subjects with distinct gait patterns, which is significantly superior when compared with traditional machine learning methods. The zero-velocity indicator is generated successively according to the segmented sequence to accomplish zero velocity update for the spatial parameter calculation. The accuracy of the proposed system is also validated through the OptiTrack in the lab. The comparison result of the stride length shows that the error between the two systems is less than 2%, which demonstrates that our system can satisfy the demand in the clinical.},
  archive      = {J_THMS},
  author       = {Xin Shi and Zhelong Wang and Hongyu Zhao and Sen Qiu and Ruichen Liu and Fang Lin and Kai Tang},
  doi          = {10.1109/THMS.2022.3228515},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {176-186},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Threshold-free phase segmentation and zero velocity detection for gait analysis using foot-mounted inertial sensors},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving, thermal vision with human in the loop
fall detection alert system. <em>THMS</em>, <em>53</em>(1), 164–175. (<a
href="https://doi.org/10.1109/THMS.2022.3203021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support the independent living of older adults in their own homes, it is essential to identify their abnormal behaviors before triggering an automated alert system. Existing normal vision sensing approaches to detect human falls in the activities of daily living (ADL) experienced acceptability issues due to outstanding privacy concerns when they are deployed in personal environments. Besides, false alerts (false-positive) fall detection has not been addressed thoroughly in systems that report abnormal human behaviors as emergency alerts to the information support. This article proposes a novel human-in-the-loop fall detection approach in the ADLs using a low-resolution thermal sensor array. The motivation for enabling a human interactive model, fall detection confirmation, is to influence resource efficiency by reducing false-positive alerts while keeping the false-negative fall predictions as low as possible. The proposed approach is based on the motion sequence classification of human movements using a recurrent neural network. The proposed approach is evaluated with comprehensive experiments using different learning techniques, users, and domestic environment conditions. This article shows a performance accuracy of 99.7% to detect human falls from various typical ADLs.},
  archive      = {J_THMS},
  author       = {Abdallah Naser and Ahmad Lotfi and Maria Drolence Mwanje and Junpei Zhong},
  doi          = {10.1109/THMS.2022.3203021},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {164-175},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Privacy-preserving, thermal vision with human in the loop fall detection alert system},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Sonified distance in sensory substitution does not always
improve localization: Comparison with a 2-d and 3-d handheld device.
<em>THMS</em>, <em>53</em>(1), 154–163. (<a
href="https://doi.org/10.1109/THMS.2022.3194715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early visual to auditory substitution devices encode 2-D monocular images into sounds while more recent devices use distance information from 3-D sensors. This study assesses whether the addition of sound-encoded distance in recent systems helps to convey the “where” information. This is important to the design of new sensory substitution devices. We conducted experiments for object localization and navigation tasks with a handheld visual to audio substitution system. It comprises 2-D and 3-D modes. Both encode in real-time the position of objects in images captured by a camera. The 3-D mode encodes in addition the distance between the system and the object. Experiments have been conducted with 16 blindfolded sighted participants. For the localization, participants were quicker to understand the scene with the 3-D mode that encodes distances. On the other hand, with the 2-D only mode, they were able to compensate for the lack of distance encoding after a small training. For the navigation, participants were as good with the 2-D only mode than with the 3-D mode encoding distance.},
  archive      = {J_THMS},
  author       = {Louis Commère and Jean Rouat},
  doi          = {10.1109/THMS.2022.3194715},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {154-163},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Sonified distance in sensory substitution does not always improve localization: Comparison with a 2-D and 3-D handheld device},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human activity recognition using smartphones with WiFi
signals. <em>THMS</em>, <em>53</em>(1), 142–153. (<a
href="https://doi.org/10.1109/THMS.2022.3188726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a work using a smartphone with an off-the-shelf WiFi router for human activity recognition with various scales. The router serves as a hotspot for transmitting WiFi packets. The smartphone is configured with customized firmware and developed software for capturing WiFi channel state information (CSI) data. We extract the features from the CSI data associated with specific human activities, and utilize the features to classify the activities using machine learning models. To evaluate the system performance, we test 20 types of human activities with different scales including seven small motions, four medium motions, and nine big motions. We recruit 60 participants and spend 140 hours for data collection at various experimental settings, and have 36 000 data points collected in total. Furthermore, for comparison, we adopt three distinct machine learning models, including convolutional neural networks (CNNs), decision tree, and long short-term memory. The results demonstrate that our system can predict these human activities with an overall accuracy of 97.25%. Specifically, our system achieves a mean accuracy of 97.57% for recognizing small-scale motions that are particularly useful for gesture recognition. We then consider the adaptability of the machine learning algorithms in classifying the motions, where CNN achieves the best predicting accuracy. As a result, our system enables human activity recognition in a more ubiquitous and mobile fashion that can potentially enhance a wide range of applications such as gesture control, sign language recognition, etc.},
  archive      = {J_THMS},
  author       = {Guiping Lin and Weiwei Jiang and Sicong Xu and Xiaobo Zhou and Xing Guo and Yujun Zhu and Xin He},
  doi          = {10.1109/THMS.2022.3188726},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {142-153},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human activity recognition using smartphones with WiFi signals},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving intention detection in single-trial classification
through fusion of EEG and eye-tracker data. <em>THMS</em>,
<em>53</em>(1), 132–141. (<a
href="https://doi.org/10.1109/THMS.2022.3225633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intention decoding is an indispensable procedure in hands-free human–computer interaction (HCI). A conventional eye-tracker system using a single-model fixation duration may issue commands that ignore users&#39; real expectations. Here, an eye-brain hybrid brain–computer interface (BCI) interaction system was introduced for intention detection through the fusion of multimodal eye-tracker and event-related potential (ERP) [a measurement derived from electroencephalography (EEG)] features. Eye-tracking and EEG data were recorded from 64 healthy participants as they performed a 40-min customized free search task of a fixed target icon among 25 icons. The corresponding fixation duration of eye tracking and ERP were extracted. Five previously-validated linear discriminant analysis (LDA)-based classifiers [including regularized LDA, stepwise LDA, Bayesian LDA, shrinkage linear discriminant analysis (SKLDA), and spatial-temporal discriminant analysis] and the widely-used convolutional neural network (CNN) method were adopted to verify the efficacy of feature fusion from both offline and pseudo-online analysis, and the optimal approach was evaluated by modulating the training set and system response duration. Our study demonstrated that the input of multimodal eye tracking and ERP features achieved a superior performance of intention detection in the single-trial classification of active search tasks. Compared with the single-model ERP feature, this new strategy also induced congruent accuracy across classifiers. Moreover, in comparison with other classification methods, we found that SKLDA exhibited a superior performance when fusing features in offline tests (ACC = 0.8783, AUC = 0.9004) and online simulations with various sample amounts and duration lengths. In summary, this study revealed a novel and effective approach for intention classification using an eye-brain hybrid BCI and further supported the real-life application of hands-free HCI in a more precise and stable manner.},
  archive      = {J_THMS},
  author       = {Xianliang Ge and Yunxian Pan and Sujie Wang and Linze Qian and Jingjia Yuan and Jie Xu and Nitish Thakor and Yu Sun},
  doi          = {10.1109/THMS.2022.3225633},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {132-141},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Improving intention detection in single-trial classification through fusion of EEG and eye-tracker data},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of dyslexic children using machine learning and
multimodal hindi language eye-gaze-assisted learning system.
<em>THMS</em>, <em>53</em>(1), 122–131. (<a
href="https://doi.org/10.1109/THMS.2022.3221848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children with dyslexia need specific instructions for spelling and word analysis from an early age. It is important to provide appropriate tools using technology for writing aids to such children that can help them to input text, while providing multiple feedback. However, it is unclear how children with dyslexia can efficiently use a gaze-based virtual keyboard (VK). In this study, we propose to use the typing performance of a multimodal Hindi language eye-gaze-assisted learning system based on a VK to help in the reduction of tracking errors for people with writing and reading deficiencies and to detect children with dyslexia. Performance was assessed at three levels: eye tracker, eye tracker with soft switch, and touchscreen as a baseline modality using a predefined copy-typing task. The system was validated through a series of experiments with 32 children (16 dyslexic and 16 control). The results show that the workload and the usability of the system are substantially different for children with dyslexia. Children with dyslexia have a lower typing performance when using the touchscreen modality or the eye tracker only. The detection of children with dyslexia from others was assessed with seven different types of classifiers using the typing speed on different words (AUC $&amp;gt;$ 0.9). These results highlight the need to have fully inclusive VKs. This work demonstrates the superior use of a multimodal system with participants having unique neuropsychological conditions and that the proposed system can be used to detect children with dyslexia.},
  archive      = {J_THMS},
  author       = {Yogesh Kumar Meena and Hubert Cecotti and Braj Bhushan and Ashish Dutta and Girijesh Prasad},
  doi          = {10.1109/THMS.2022.3221848},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {122-131},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Detection of dyslexic children using machine learning and multimodal hindi language eye-gaze-assisted learning system},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Glints detection in noisy images using graph matching.
<em>THMS</em>, <em>53</em>(1), 113–121. (<a
href="https://doi.org/10.1109/THMS.2022.3213656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glints detection is important in gaze estimation with virtual reality equipment. In some complex scenarios, the glints detection may fail due to multiple reasons, such as the ambiguous correspondence between the multiple light sources or their reflected glints in the captured image, as well as the distinction between the good glints and the noisy glints caused by the lights reflect on the glasses that the users may wear. In this article, the glints detection is formulated as a graph matching problem between the two graphs constructed from the candidate glints and the rough predicted ones from the information given by the last frame. In addition, the optimal parameters for the matching are estimated using an Expectation Maximum algorithm-like algorithm. Finally, with the optimal parameters, the Kuhn–Munkras algorithm is used to get the optimal matching, and the successfully matched true glints are considered the correctly detected glints. Experiments are performed to validate the effectiveness of the proposed method.},
  archive      = {J_THMS},
  author       = {Wei Liu and Qiong Liu and Jing Yang and Yafan Chen},
  doi          = {10.1109/THMS.2022.3213656},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {113-121},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Glints detection in noisy images using graph matching},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lower extremity posture assistive wearable devices: A
review. <em>THMS</em>, <em>53</em>(1), 98–112. (<a
href="https://doi.org/10.1109/THMS.2022.3216761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many work-related tasks demand able-bodied employees to assume uncomfortable postures for long periods. This can lead to work-related musculoskeletal disorders. The situation demands the employees to be physically empowered while ensuring their agility and flexibility. In that context, wearable bodyweight support systems have come to the forefront as a futuristic solution to reduce the stress on the human body when assuming sitting and/or crouched postures. Recently, heightened attention on these posture assistive devices was evident with an increasing number of products, patents, and research articles. However, reviews of such wearable posture assistive devices for the lower limbs are lacking in the literature. Thus, this article specifically focuses on reviewing existing methods and technologies to assist workers in the industrial or service sectors to work in uncomfortable or crouched postures. Systematic reviews and meta-analyses extension for scoping reviews method was used to identify and analyze 43 unique designs from research and patent databases. The critical review of the technologies was performed after classifying these devices into commercial phase, lab/research phase, and conceptual design phase. This review has comprehensively covered the trends and patterns of the modern developments in posture assistance and includes future directions for the research community. Holistically, this article serves as a reference to set up a benchmarking system for wearable posture assistive devices.},
  archive      = {J_THMS},
  author       = {Isira Wijegunawardana and R.K.P.S. Ranaweera and R.A.R.C. Gopura},
  doi          = {10.1109/THMS.2022.3216761},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {98-112},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Lower extremity posture assistive wearable devices: A review},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and experimental verification of a hip exoskeleton
based on human–machine dynamics for walking assistance. <em>THMS</em>,
<em>53</em>(1), 85–97. (<a
href="https://doi.org/10.1109/THMS.2022.3217971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High motion compatibility with the human body is essential for lower limb exoskeletons. However, in most exoskeletons, internal/external rotational degrees of freedom are not provided, which makes accurate alignment between the biological and mechanical joints difficult to achieve. To solve this problem, a novel hip exoskeleton with a parallel structure is developed in this article. The unique parallel structure eliminates the misalignment problem and enables walking free of restrictions. On the other hand, this requires a coordinated control among actuations within the parallel exoskeleton structure. In this light, a model-based controller is proposed in this article. The controller is based on a human–machine integrated dynamic model and can generate coordinated force control references that could increase the closed-loop system&#39;s sensitivity to its wearer&#39;s movements. The controller requires only kinematic information from the wearer, but not interaction force data that most existing exoskeletons require in their control design, which saves spaces and makes the system compact for use. Experiments were conducted to demonstrate the kinematic compatibility and assistive performance of the proposed hip exoskeleton.},
  archive      = {J_THMS},
  author       = {Xiangyang Wang and Sheng Guo and Bojian Qu and Shaoping Bai},
  doi          = {10.1109/THMS.2022.3217971},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {85-97},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Design and experimental verification of a hip exoskeleton based on Human–Machine dynamics for walking assistance},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nested densely atrous spatial pyramid pooling and deep dense
short connection for skeleton detection. <em>THMS</em>, <em>53</em>(1),
75–84. (<a href="https://doi.org/10.1109/THMS.2022.3224552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The skeleton shows the local symmetry and the shape/topology of the object, and it is utilized for human pose recognition, road detection, text detection, and the representation of industrial parts. However, the size of the skeleton is variable, which makes high-level feature representation difficult. Existing methods only attempt to integrate multilevel features but ignore the extraction of high-level features and multiscales of contextual information that are helpful for the skeleton detection task. Thus, the contributions of this article include two aspects. The first contribution is to propose a nested densely atrous spatial pyramid pooling method that connects the atrous convolutions with different dilation rates in a nested cascade mode, which can provide the multiscale denser contextual information, a larger receptive field, and more local features. The second one is to propose a deep dense short connection (DDSC) that explores the role of features at different levels in the task of skeleton detection. DDSC adopts concatenation to fuse high-level semantic information with shape information. The proposed method is evaluated on four common datasets, and the experimental results show the effectiveness of the proposed method.},
  archive      = {J_THMS},
  author       = {Daoyong Fu and Xiaofei Zeng and Songchen Han and Hanren Lin and Wei Li},
  doi          = {10.1109/THMS.2022.3224552},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {75-84},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Nested densely atrous spatial pyramid pooling and deep dense short connection for skeleton detection},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stiffness-observer-based adaptive control of an
intrinsically compliant parallel wrist rehabilitation robot.
<em>THMS</em>, <em>53</em>(1), 65–74. (<a
href="https://doi.org/10.1109/THMS.2022.3211164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disability from injuries and diseases is a global problem affecting a large population; however, due to a lack of therapists and labor-intensive procedures, only a few benefits from rehabilitation. Robots can assist therapists in treating many patients simultaneously, but the existing solutions need improvements in their mechanism, actuation, and control. This article presents a four-link parallel end-effector robot for wrist joint rehabilitation. The proposed robot employs biomimetic muscle actuators (BMA) that provide intrinsic compliance to the robotic system. A fuzzy-based model is developed to identify the nonlinear nature of BMAs. The stiffness-observer learns subject-specific stiffness, which is used to modify the robot reference trajectories. An adaptive controller uses the fuzzy model and stiffness-observer and simultaneously controls the four BMAs to provide three degrees of rotational freedom to the robot end-effector. The feasibility of the robot mechanism and the controller was evaluated through proof of concept experiments conducted with three unimpaired human subjects. It was found that the controller was able to guide the robot–human system on the commanded trajectories in the presence of parallel actuation of compliant and nonlinear BMAs. Furthermore, the controller was also able to modify the commanded trajectories in the higher stiffness regions of the wrist workspace.},
  archive      = {J_THMS},
  author       = {Tanishka Goyal and Shahid Hussain and Elisa Martinez-Marroquin and Nicholas A. T. Brown and Prashant K. Jamwal},
  doi          = {10.1109/THMS.2022.3211164},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {65-74},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Stiffness-observer-based adaptive control of an intrinsically compliant parallel wrist rehabilitation robot},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subject-specific human modeling for human pose estimation.
<em>THMS</em>, <em>53</em>(1), 54–64. (<a
href="https://doi.org/10.1109/THMS.2022.3195952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3-D human pose estimation or human tracking has always been the focus of research in the human–computer interaction community. As the calibration step of human pose estimation, subject-specific modeling is crucially important to the subsequent pose estimation process. It not only provides a priori knowledge but also clearly defines the tracking target. This article presents a fully automatic subject modeling framework to reconstruct human pose, shape, as well as the body texture in a challenging optimization scenario. By integrating powerful differentiable rendering into the subject-specific modeling pipeline, the proposed method transforms the texture reconstruction problem into analysis by synthesis minimization and solves it efficiently by a gradient-based method. Furthermore, a novel covariance matrix adaptation annealing algorithm is proposed to attack the high-dimensional multimodal optimization problem in an adaptive manner. The domain knowledge of hierarchical human anatomy is seamlessly injected to the annealing optimization process by using a soft covariance matrix mask. All together contributes to the novel algorithm robust to the temptation of local minima. Experiments on the Human3.6 M dataset and the People-Snapshot dataset demonstrate the competitive results to the state of the art both qualitatively and quantitatively.},
  archive      = {J_THMS},
  author       = {Yifan Lu and Genlang Chen and Chaoyi Pang and Haolan Zhang and Bailing Zhang},
  doi          = {10.1109/THMS.2022.3195952},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {54-64},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Subject-specific human modeling for human pose estimation},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Influence of the physical interface on the quality of
human–exoskeleton interaction. <em>THMS</em>, <em>53</em>(1), 44–53. (<a
href="https://doi.org/10.1109/THMS.2022.3175415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite exoskeletons becoming widespread tools in industrial applications, the impact of the design of human–exoskeleton physical interfaces has received little attention. This study aims at thoroughly quantifying the influence of different physical human–exoskeleton interfaces on subjective and objective biomechanical parameters. To this aim, 18 participants performed elbow flexion/extension movements while wearing an active exoskeleton with three different physical interfaces: a strap without any degree of freedom, a thermoformed orthosis with one (translation) and three degrees of freedom (translation and rotations). Interaction efforts, kinematic parameters, electromyographic activities, and subjective feelings were collected and examined during the experiment. Results showed that increasing the interaction area is necessary to improve the interaction quality at a subjective level. The addition of passive degrees of freedom allows significant improvements on both subjective and objective measurements. Outcomes of this study may provide fundamental insights to select physical interfaces when designing future exoskeletons.},
  archive      = {J_THMS},
  author       = {Dorian Verdel and Guillaume Sahm and Simon Bastide and Olivier Bruneau and Bastien Berret and Nicolas Vignais},
  doi          = {10.1109/THMS.2022.3175415},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {44-53},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Influence of the physical interface on the quality of Human–Exoskeleton interaction},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hand tracking and gesture recognition by multiple
contactless sensors: A survey. <em>THMS</em>, <em>53</em>(1), 35–43. (<a
href="https://doi.org/10.1109/THMS.2022.3188840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand tracking and gesture recognition are fundamental in a multitude of applications. Various sensors have been used for this purpose, however, all monocular vision systems face limitations caused by occlusions. Wearable equipment overcome said limitations, although deemed impractical in some cases. Using more than one sensor provides a way to overcome this problem, but necessitates more complicated designs. In this work, we aim to highlight contemporary methods used for hand tracking and gesture recognition by collecting publications of systems developed in the last decade, that employ contactless devices as RGB cameras, IR, and depth sensors, along with some preceding pillar works. Additionally, we briefly present common steps, techniques, and basic algorithms used during the process of developing modern hand tracking and gesture recognition systems and, finally, we derive the trend for the next future.},
  archive      = {J_THMS},
  author       = {Eleni Theodoridou and Luigi Cinque and Filippo Mignosi and Giuseppe Placidi and Matteo Polsinelli and João Manuel R. S. Tavares and Matteo Spezialetti},
  doi          = {10.1109/THMS.2022.3188840},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {35-43},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Hand tracking and gesture recognition by multiple contactless sensors: A survey},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving teleoperation through human-aware haptic feedback:
A distinguishable and interpretable physical interaction based on the
contact state. <em>THMS</em>, <em>53</em>(1), 24–34. (<a
href="https://doi.org/10.1109/THMS.2022.3227935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Teleoperation with haptic feedback is especially useful for contact-rich manipulation. Humans can easily perceive physical interaction events and effects of hands-on manipulation. However, current measurement-based haptic rendering methods with distorting and confusing transmission limit teleoperation performance. The major problem is a mismatch between low-level modulated sensor signals and human-aware haptic stimuli. We explore a human-aware haptic feedback pipeline that renders a distinguishable and interpretable physical interaction based on the contact states. Manipulation tasks are modeled as time-series sequences of four contact states: 1) noncontact , 2) contact , 3) stick , and 4) slip . The temporal convolutional network model fuses force/torque sensor and velocity signals in real time to identify the four contact states with a 91.3% accuracy. Meanwhile, state-dependent haptic feedback, which combines transient and continuous feedback, brings more cues for physical interaction events and effects, corresponding to human fast- and slow-adapting receptors. We formulate a two-peak waveform for transient feedback based on a second-order superimposed exponentially decaying sinusoid model and adopt the orthogonal decomposition filter method for “inequable” continuous feedback. We demonstrate the effectiveness of our method through contact state and teleoperation experiments under different haptic conditions. The results indicate that the proposed method helps the operator to perceive and understand physical interaction and significantly improves teleoperation performance.},
  archive      = {J_THMS},
  author       = {Hechao Ji and Shiqi Li and Jia Wang and Zhao Ruan},
  doi          = {10.1109/THMS.2022.3227935},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {24-34},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Improving teleoperation through human-aware haptic feedback: A distinguishable and interpretable physical interaction based on the contact state},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient RGB-d hand gesture detection framework for
dexterous robot hand-arm teleoperation system. <em>THMS</em>,
<em>53</em>(1), 13–23. (<a
href="https://doi.org/10.1109/THMS.2022.3206663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of accurate and fast hand gesture detection and teleoperation mapping in the hand-based visual teleoperation of dexterous robots, an efficient hand gesture detection framework based on deep learning is proposed in this article. It can achieve an accurate and fast hand gesture detection and teleoperation of dexterous robots based on an anchor-free network architecture by using an RGB-D camera. First, an RGB-D early-fusion method based on the HSV space is proposed, effectively reducing background interference and enhancing hand information. Second, a hand gesture classification network (HandClasNet) is proposed to realize hand detection and localization by detecting the center and corner points of hands, and a HandClasNet is proposed to realize gesture recognition by using a parallel EfficientNet structure. Then, a dexterous robot hand-arm teleoperation system based on the hand gesture detection framework is designed to realize the hand-based teleoperation of a dexterous robot. Our method achieves high accuracy with fast speed on public and custom hand datasets and outperforms some state-of-the-art methods. In addition, the application of the proposed method in the hand-based teleoperation system can control the grasping of various objects by a dexterous hand-arm system in real time and accurately, which verifies the efficiency of our method.},
  archive      = {J_THMS},
  author       = {Qing Gao and Zhaojie Ju and Yongquan Chen and Qiwen Wang and Chuliang Chi},
  doi          = {10.1109/THMS.2022.3206663},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {13-23},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An efficient RGB-D hand gesture detection framework for dexterous robot hand-arm teleoperation system},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Camera frame misalignment in a teleoperated eye-in-hand
robot: Effects and a simple correction method. <em>THMS</em>,
<em>53</em>(1), 2–12. (<a
href="https://doi.org/10.1109/THMS.2022.3217453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Misalignment between the camera frame and the operator frame is commonly seen in a teleoperated system and usually degrades the operation performance. The effects of such misalignment have not been fully investigated for eye-in-hand systems–systems that have the camera ( eye ) mounted to the end-effector ( hand ) to gain compactness in confined spaces such as in endoscopic surgery. This article provides a systematic study on the effects of the camera frame misalignment in a teleoperated eye-in-hand robot and proposes a simple correction method in the view display. A simulation is designed to compare the effects of the misalignment under different conditions. Users are asked to move a rigid body from its initial position to the specified target position via teleoperation, with different levels of misalignment simulated. It is found that misalignment between the input motion and the output view is much more difficult to compensate by the operators when it is in the orthogonal direction ( $\sim$ 40 s) compared with the opposite direction ( $\sim$ 20 s). An experiment on a real concentric tube robot with an eye-in-hand configuration is also conducted. Users are asked to telemanipulate the robot to complete a pick-and-place task. Results show that with the correction enabled, there is a significant improvement in the operation performance in terms of completion time (mean 40.6%, median 38.6%), trajectory length (mean 34.3%, median 28.1%), difficulty (50.5%), unsteadiness (49.4%), and mental stress (60.9%).},
  archive      = {J_THMS},
  author       = {Liao Wu and Fangwen Yu and Thanh Nho Do and Jiaole Wang},
  doi          = {10.1109/THMS.2022.3217453},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {2-12},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Camera frame misalignment in a teleoperated eye-in-hand robot: Effects and a simple correction method},
  volume       = {53},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
