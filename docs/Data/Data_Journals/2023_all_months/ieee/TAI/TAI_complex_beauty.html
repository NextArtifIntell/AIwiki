<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tai---147">TAI - 147</h2>
<ul>
<li><details>
<summary>
(2023). Learning deep asymmetric tolerant part representation.
<em>TAI</em>, <em>4</em>(6), 1789–1801. (<a
href="https://doi.org/10.1109/TAI.2022.3222644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Categorization objects at a subordinate level inevitably pose a significant challenge, i.e., interclass difference is very subtle and only exists in a few key parts. Therefore, how to localize these key parts for discriminative visual categorization without requiring expensive pixel-level annotations becomes a core question. To that end, this article introduces a novel asymmetric tolerant part segmentation network (ATP-Net). The ATP-Net simultaneously learns to segment parts and identify objects in an end-to-end manner using only image-level category labels. Given the intrinsic asymmetry property of part alignment, a desirable learning of part segmentation should be capable of incorporating such property. Despite the efforts toward regularizing weakly supervised part segmentation, none of them consider this vital and intrinsic property, i.e., the spatial asymmetry of part alignment. Our work, for the first time, proposes to explicitly characterize the spatial asymmetry of part alignment for visual tasks. We propose a novel asymmetry loss function to guide the part segmentation by encoding the spatial asymmetry of part alignment, i.e., restricting the upper bound of how asymmetric those self-similar parts are to each other in the network learning. Via a comprehensive ablation study, we verify the effectiveness of the proposed ATP-Net in driving the network learning toward semantically meaningful part segmentation and discriminative visual categorization. Consistently, superior/competitive performance is reported on 12 datasets covering crop cultivar classification, plant disease classification, bird/butterfly species classification, large-scale natural image classification, attribute recognition, and landmark localization.},
  archive      = {J_TAI},
  author       = {Xiaohan Yu and Yang Zhao and Yongsheng Gao and Shengwu Xiong},
  doi          = {10.1109/TAI.2022.3222644},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1789-1801},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning deep asymmetric tolerant part representation},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optical flow estimation in dense foggy scenes with
domain-adaptive networks. <em>TAI</em>, <em>4</em>(6), 1777–1788. (<a
href="https://doi.org/10.1109/TAI.2022.3221064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating optical flow in dense foggy scenes is a challenging task. The basic assumptions for computing flow such as brightness and gradient constancy become invalid. To address the problem, we introduce a semisupervised deep learning method that can learn from real fog images without requiring the corresponding optical flow ground-truths. Our method is a multitask network, integrating the domain transformation and optical flow networks in one framework. The domain transformation is performed between foggy and clean domains. Under our semisupervised training strategy, we train our method in a supervised manner with a pair of synthetic fog images, their corresponding clean images and optical flow ground-truths. Subsequently, given a pair of real fog images and a pair of clean images that are not corresponding to each other (unpaired), in the next training batch, we train our network in an unsupervised manner. The supervised and unsupervised training processes are alternated iteratively. Since our method relies on unsupervised learning for real data, we show that it can be used for test-time training. We show in our experiments that performing test-time training improves the results further on our test data. Extensive experiments show that our method outperforms the state-of-the-art methods in estimating optical flow in dense foggy scenes.},
  archive      = {J_TAI},
  author       = {Wending Yan and Aashish Sharma and Robby T. Tan},
  doi          = {10.1109/TAI.2022.3221064},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1777-1788},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optical flow estimation in dense foggy scenes with domain-adaptive networks},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memory-augmented lyapunov-based safe reinforcement learning:
End-to-end safety under uncertainty. <em>TAI</em>, <em>4</em>(6),
1767–1776. (<a href="https://doi.org/10.1109/TAI.2023.3238700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent advances in safe reinforcement learning (RL), safety constraints are often violated at deployment, especially under extreme uncertainty in memory-based partially observable environments. To address these limitations, we propose a memory-augmented Lyapunov-based safe RL model. The primary contributions of our method include the following: 1) an explicit memory module based on transformers to process long time horizons of information feedback from the environment; 2) a memory-augmented Lyapunov function to determine a safe set of policies; and 3) an exploration module that identifies highly rewarding safe actions by characterizing the uncertainty in the environment. We evaluate the proposed model in reactive OpenAI Safety Gym and memory-based partially observable DMLab-30 environments. The results of these experiments show that the proposed method significantly outperforms state-of-the-art baselines. Specifically, our proposed method achieves the lowest constraint costs among the tested benchmarks, while delivering high returns. Moreover, we perform ablation studies that show significant contributions of the introduced transformer-based encoder, memory-augmented Lyapunov functions, and the uncertainty-aware exploration module.},
  archive      = {J_TAI},
  author       = {Ashkan B. Jeddi and Nariman L. Dehghani and Abdollah Shafieezadeh},
  doi          = {10.1109/TAI.2023.3238700},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1767-1776},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Memory-augmented lyapunov-based safe reinforcement learning: End-to-end safety under uncertainty},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward symptom assessment guided symptom investigation and
disease diagnosis. <em>TAI</em>, <em>4</em>(6), 1752–1766. (<a
href="https://doi.org/10.1109/TAI.2023.3236897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic disease diagnosis has gained immense popularity and demand over the past few years, and it is emerging as an effective diagnostic assistant to doctors. Diagnosis assistants assist clinicians in conducting a thorough symptom investigation and identifying possible diseases. Doctors correctly diagnose patients by observing only a few symptoms in most cases, even though the diagnosed disease has numerous symptoms. Also, some common symptoms, such as fever and headache, usually emerge due to other symptoms, which do not play a major role in identifying suffering diseases. In this work, we investigate the role of symptom importance in disease diagnosis through several feature engineering techniques and propose a novel symptom assessment guided symptom investigation and disease diagnosis (SA-SIDD) assistant using hierarchical reinforcement learning (HRL). The proposed SA-SIDD assistant first collects an adequate set of symptoms/sign information through conversing with users and then diagnoses a disease based on the extracted symptoms. We incorporated a symptom assessment module with the diagnosis framework that evaluates the relevance of current inspected symptom at each turn and reinforces the assistant to investigate distinctive and context-aligned symptoms using an assessment critic. The proposed methodology outperforms the state-of-the-art method, HRL, on two publicly available datasets, which firmly establishes the crucial role of symptom importance in disease diagnosis and the need for the proposed symptom assessment incorporated disease diagnosis framework. Furthermore, we have also conducted a human evaluation, revealing that the diagnosis method greatly enhances end-user satisfaction because of context-aligned relevant and minimal symptom investigation.},
  archive      = {J_TAI},
  author       = {Abhisek Tiwari and Rishav Raj and Sriparna Saha and Pushpak Bhattacharyya and Sarbajeet Tiwari and Minakshi Dhar},
  doi          = {10.1109/TAI.2023.3236897},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1752-1766},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Toward symptom assessment guided symptom investigation and disease diagnosis},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature-guided perturbation for facial attribute
classification. <em>TAI</em>, <em>4</em>(6), 1739–1751. (<a
href="https://doi.org/10.1109/TAI.2022.3228830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pretrained deep models are widely used in computer vision tasks ranging from face recognition to object classification and attribute prediction. The performance of these models depends heavily on the pretraining dataset, and a shift in the input data adversely affects the model performance. Existing techniques address the problem by updating the pretrained model on the new dataset, most popularly via model fine-tuning. However, this requires updating millions of parameters in the model and is computationally expensive. Therefore, in this research, an algorithm is proposed to address the problem of data shift via perturbation learning without updating the pretrained model parameters. The proposed algorithm shifts the data in the input space to obtain feature-guided perturbed (FGP) data such that when FGP data are given as input to the pretrained model, it results in optimized feature space. Perturbation is learned by minimizing the intraclass distance and maximizing the interclass separation among the classes in the feature space. The proposed algorithm is evaluated on three publicly available datasets, namely, LFW, CelebA, and MUCT. Different experiments and comparisons with existing algorithms, including fine-tuning the model, show the efficacy of the proposed approach. Most importantly, FGP requires fewer parameters to be learned compared to the traditional approaches.},
  archive      = {J_TAI},
  author       = {Saheb Chhabra and Puspita Majumdar and Mayank Vatsa and Richa Singh},
  doi          = {10.1109/TAI.2022.3228830},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1739-1751},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Feature-guided perturbation for facial attribute classification},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ultralight-weight three-prior convolutional neural network
for single image super resolution. <em>TAI</em>, <em>4</em>(6),
1724–1738. (<a href="https://doi.org/10.1109/TAI.2022.3224417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of image super resolution is crucial in many applications, such as computer vision and medical imaging. Conventionally, the task of image super resolution was carried out by formulating it as a constrained optimization problem and then solving it using suitable numerical techniques. However, after the emergence of deep neural networks, the focus of the researchers in this area has been almost entirely on designing deep convolutional neural network architectures that indeed have provided remarkable performance for the task of image super resolution. Even though unified methods of combining the two approaches has a greater potential of providing a superior performance for the task of image super resolution, with the exception of very few works, not much attention has been paid to develop such a unified method for this task. In this article, we propose a three-prior formulation of the optimization problem for image super resolution and develop an ultralight-weight convolutional neural network for its solution. The effectiveness of the proposed formulation of the optimization problem and ultralight-weight convolution neural network architecture for its solution is demonstrated through extensive experimentations of the proposed scheme on benchmark datasets and comparisons of the results with that of the other state-of-the-art ultralight-weight image super resolution networks.},
  archive      = {J_TAI},
  author       = {Alireza Esmaeilzehi and M. Omair Ahmad and M.N.S. Swamy},
  doi          = {10.1109/TAI.2022.3224417},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1724-1738},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ultralight-weight three-prior convolutional neural network for single image super resolution},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variance-guided structured sparsity in deep neural networks.
<em>TAI</em>, <em>4</em>(6), 1714–1723. (<a
href="https://doi.org/10.1109/TAI.2022.3221688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of deep neural networks, especially convolutional neural networks in various applications, has greatly been possible by the presence of an enormous number of learnable parameters. These parameters increase the learning capacity of the model, but at the same time, it also significantly increases the computational and memory costs. This severely hinders the scalability of these models to limited resource environments, such as IoT devices. The majority of the network weights are known to be redundant and can be removed from the network. This article introduces a regularization scheme, which is the combination of structured sparsity regularization and variance regularization. It simultaneously helps to obtain computationally sparse models by making the majority of parameter groups zero and increasing the variance of nonzero groups to compensate for the accuracy drop. We use sparse group lasso, group sparsity variant of &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell _{1}$&lt;/tex-math&gt;&lt;/inline-formula&gt; (lasso) regularization, which removes redundant connections and unnecessary neurons from the network. For variance regularization, the KL divergence of the current parameter distribution and the target distribution is minimized, which aims to have the concentration of weights toward zero and a high variance of nonzero weights (skewed distribution). To check the effectiveness of the proposed regularizer, the experiments are performed on various benchmark datasets and it is noticed that variance regularization helps to reduce the accuracy drop caused by sparsity regularization. On MNIST, the trainable parameters are reduced from 331 984 (baseline model) to 57 327 and managed to obtain better accuracy than the baseline (99.6%). Also, on Fashion-MNIST, Cifar-10, and ImageNet the proposed scheme achieved state-of-the-art sparsity with almost no drop in accuracy.},
  archive      = {J_TAI},
  author       = {Mohammad Khalid Pandit and Mahroosh Banday},
  doi          = {10.1109/TAI.2022.3221688},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1714-1723},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Variance-guided structured sparsity in deep neural networks},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based holistic speaker independent visual
speech recognition. <em>TAI</em>, <em>4</em>(6), 1705–1713. (<a
href="https://doi.org/10.1109/TAI.2022.3220190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From a broader perspective, the objective of visual speech recognition (VSR) is to comprehend the speech spoken by an individual using visual deformations. However, some of the significant limitations of existing solutions include the dearth of training data, improper end-to-end deployed solutions, lack of holistic feature representation, and less accuracy. To resolve these limitations, this study proposes a novel, scalable, and robust VSR system that uses the videotape of the user to determine the word which is being spoken. In this regard, a customized 3-D convolutional neural network (3-D CNN) architecture is proposed by extracting the spatio-temporal features and eventually mapping the prediction probabilities of the elements in the corpus. We have created a customized dataset resembling the metadata contained in the MIRACL-VC1 dataset to validate the concept of person-independence. While being robust to a broad spectrum of lighting conditions across multiple devices, our model achieves a training accuracy of 80.2% and a testing accuracy of 77.9% in predicting the word spoken by the user.},
  archive      = {J_TAI},
  author       = {Praneeth Nemani and Ghanta Sai Krishna and Nikhil Ramisetty and B Digvijay Sri Sai and Santosh Kumar},
  doi          = {10.1109/TAI.2022.3220190},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1705-1713},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep learning-based holistic speaker independent visual speech recognition},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scaling the inference of digital pathology deep learning
models using CPU-based high-performance computing. <em>TAI</em>,
<em>4</em>(6), 1691–1704. (<a
href="https://doi.org/10.1109/TAI.2023.3246032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital pathology whole-slide images (WSIs) are large-size gigapixel images, and image analysis based on deep learning artificial intelligence technology often involves pixelwise testing of a trained deep learning neural network (DLNN) on hundreds of WSI images, which is time-consuming. We take advantage of high-performance computing (HPC) facilities to parallelize this procedure into multiple independent (and hence delightfully parallel) tasks. However, traditional software parallelization techniques and regular file formats can have significant scaling problems on HPC clusters. In this work, a useful computational strategy is designed to localize and extract relevant patches in WSI files and group them in Hierarchical Data Format version 5 files well suited for parallel I/O. HPC&#39;s array job facilities are adapted for hierarchical scaling and parallelization of WSI preprocessing and testing of trained algorithms. Applying these techniques to testing a trained DLNN on the CAMELYON datasets with 399 WSIs reduced the theoretical processing time of 18 years on a single central processing unit (CPU) or 30 days on a single graphics processing unit to less than 45 h on an HPC cluster of 4000 CPU cores. The efficiency–accuracy tradeoff we demonstrated on this dataset further reinforced the importance of efficient computation techniques, without which accuracy may be sacrificed. The framework developed here for testing DLNNs does not rely on any specific neural network architecture and HPC cluster setup and can be utilized for any large-scale image processing and big-data analysis.},
  archive      = {J_TAI},
  author       = {Weizhe Li and Mike Mikailov and Weijie Chen},
  doi          = {10.1109/TAI.2023.3246032},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1691-1704},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Scaling the inference of digital pathology deep learning models using CPU-based high-performance computing},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid intelligent dynamic optimization of switched systems.
<em>TAI</em>, <em>4</em>(6), 1679–1690. (<a
href="https://doi.org/10.1109/TAI.2022.3225366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a hybrid intelligent dynamic optimization method integrating of particle swarm optimization (PSO) and gradient-based optimization (GBO) is proposed for optimal control of switched systems. The method both improves the solution accuracy of PSO and avoids falling into local minima generated by the deterministic optimization. First, the PSO algorithm with ring topology is used to explore the whole search space to detect the global optimum area. Second, the GBO algorithm is deployed in the detected global optimum area to achieve faster convergence rate and higher precision solution than those of pure PSO. Finally, the simulation results show that the algorithm outperforms both PSO and GBO in terms of solution accuracy and computational cost.},
  archive      = {J_TAI},
  author       = {Huan Li and Jun Fu and Tianyou Chai},
  doi          = {10.1109/TAI.2022.3225366},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1679-1690},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Hybrid intelligent dynamic optimization of switched systems},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed adaptive inverse differential game approach to
leader’s behavior learning for multiple autonomous followers.
<em>TAI</em>, <em>4</em>(6), 1666–1678. (<a
href="https://doi.org/10.1109/TAI.2022.3217210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the leader&#39;s behavior learning problem is investigated for a class of leader-follower systems, where the leader&#39;s behavior is assumed not a priori known to all followers that are autonomous. A multiplayer nonzero-sum differential game is introduced to indicate the control interaction between agents, where the leader&#39;s behavior is modeled by an unknown cost function. The autonomous followers aim to retrieve the weighting matrix in the leader&#39;s cost function collaboratively. A distributed online adaptive inverse differential game (IDG) approach to the leader&#39;s behavior learning is proposed for the autonomous followers. Specifically, a concurrent learning based adaptive law and an interactive game controller are first developed for each autonomous follower to learn the leader&#39;s feedback gain matrix online, while the feedback Nash equilibrium of the game can be achieved. Then, a linear matrix inequality optimization problem is formulated to determine the weighting matrix of the leader&#39;s cost function for each autonomous follower. The proposed method simply requires that all followers share their interactive feedback gain matrices, not their private intents, and use only the system state data, not both the system state data and the leader&#39;s control input data. The main advantages of the proposed method are that it can be implemented online without requiring the persistent excitation condition and needs less computational power for all followers. Finally, numerical simulations are provided to demonstrate the effectiveness and feasibility of the developed method.},
  archive      = {J_TAI},
  author       = {Huai-Ning Wu and Mi Wang},
  doi          = {10.1109/TAI.2022.3217210},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1666-1678},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Distributed adaptive inverse differential game approach to leader&#39;s behavior learning for multiple autonomous followers},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive-critic-based event-triggered intelligent
cooperative control for a class of second-order constrained multiagent
systems. <em>TAI</em>, <em>4</em>(6), 1654–1665. (<a
href="https://doi.org/10.1109/TAI.2022.3217978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An event-based intelligent cooperative control policy is developed in this article for a class of second-order multiagent systems (MASs). The followers are subject to external disturbances and output constraints, in which the constraint ranges are asymmetric and time-varying. By using one-to-one nonlinear mapping technique to obtain equivalent unconstrained systems and designing a distributed observer to estimate the leader&#39;s states, a simplified unconstrained tracking control task is established for each follower. After that, event-driven distributed optimal &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$H_\infty$&lt;/tex-math&gt;&lt;/inline-formula&gt; controllers are designed to mitigate the effects of external disturbances, which are obtained from a set of decoupled Hamilton–Jacobi–Isaacs (HJI) equations, where the proposed event-triggered strategy is dynamic. Aiming at solving the HJI equations more efficiently, an event-based adaptive dynamic programming (ADP) learning algorithm applying only critic neural networks (NNs) is developed. And the critic NN weights are convergent by using stored data under finite excitation condition. Finally, the validity of the developed control scheme is demonstrated by multiple single-link robot arm systems.},
  archive      = {J_TAI},
  author       = {Zijie Guo and Hongru Ren and Hongyi Li and Qi Zhou},
  doi          = {10.1109/TAI.2022.3217978},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1654-1665},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive-critic-based event-triggered intelligent cooperative control for a class of second-order constrained multiagent systems},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving actor–critic reinforcement learning via
hamiltonian monte carlo method. <em>TAI</em>, <em>4</em>(6), 1642–1653.
(<a href="https://doi.org/10.1109/TAI.2022.3215614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The actor–critic reinforcement learning (RL) is widely used in various robotic control tasks. By viewing the actor–critic RL from the perspective of variational inference (VI), the policy network is trained to obtain the approximate posterior of actions, given the optimality criteria. However, in practice, the actor–critic RL may yield suboptimal policy estimates due to the amortization gap and insufficient exploration. In this work, inspired by the previous use of Hamiltonian Monte Carlo (HMC) in VI, we propose to integrate the policy network of actor–critic RL with HMC, which is termed as Hamiltonian policy. As such we propose to evolve actions from the base policy according to HMC, and our proposed method has many benefits. First, HMC can improve the policy distribution to better approximate the posterior and hence reduce the amortization gap. Second, HMC can also guide the exploration more to the regions of action spaces with higher Q values, enhancing the exploration efficiency. Further, instead of directly applying HMC into RL, we propose a new leapfrog operator to simulate the Hamiltonian dynamics. Finally, in safe RL problems, we find that the proposed method can not only improve the achieved return, but also reduce safety constraint violations by discarding potentially unsafe actions. With comprehensive empirical experiments on continuous control baselines, including MuJoCo and PyBullet Roboschool, we show that the proposed approach is a data-efficient and easy-to-implement improvement over previous actor–critic methods.},
  archive      = {J_TAI},
  author       = {Duo Xu and Faramarz Fekri},
  doi          = {10.1109/TAI.2022.3215614},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1642-1653},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Improving Actor–Critic reinforcement learning via hamiltonian monte carlo method},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive neuroevolution with genetic operator control and
two-way complexity variation. <em>TAI</em>, <em>4</em>(6), 1627–1641.
(<a href="https://doi.org/10.1109/TAI.2022.3214181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topology and weight evolving artificial neural network algorithms optimize the structure and weights of artificial neural networks (ANNs) simultaneously. The resulting networks are typically used as policy models for solving control and reinforcement learning (RL) type problems. This article presents a neuroevolution algorithm that aims to address the typical stagnation and sluggish convergence issues present in other neuroevolution algorithms. These issues are often caused by inadequacies in population diversity preservation, exploration/exploitation balance, and search flexibility. This new algorithm, called the adaptive genomic evolution of neural-network topologies (AGENT), builds on the neuroevolution of augmenting topologies (NEAT) concept. Novel mechanisms for adapting the selection and mutation operations are proposed to favorably control population diversity and exploration/exploitation balance. The former is founded on a fundamentally new way of quantifying diversity by taking a graph-theoretic perspective of the population of genomes and intergenomic differences. Further advancements to the NEAT paradigm occur through the incorporation of variable neuronal properties and new mutation operations that uniquely allow both the growth and pruning of ANN topologies during evolution. Numerical experiments with benchmark control problems adopted from the OpenAI Gym illustrate the competitive performance of AGENT against standard RL methods and adaptive HyperNEAT, and superiority over the original NEAT algorithm. Further parametric analysis provides key insights into the impact of the new features in AGENT. This is followed by evaluation on an unmanned aerial vehicle collision avoidance problem where maneuver planning models are learnt by AGENT with 33% reward improvement over 15 generations.},
  archive      = {J_TAI},
  author       = {Amir Behjat and Nathan Maurer and Sharat Chidambaran and Souma Chowdhury},
  doi          = {10.1109/TAI.2022.3214181},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1627-1641},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive neuroevolution with genetic operator control and two-way complexity variation},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered adaptive bipartite asymptotic tracking
control using intelligent technique for stochastic nonlinear multiagent
systems. <em>TAI</em>, <em>4</em>(6), 1616–1626. (<a
href="https://doi.org/10.1109/TAI.2022.3214486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of event-triggered-based adaptive bipartite asymptotic tracking control for multiagent stochastic pure-feedback nonlinear systems over sign digraph is considered. The agents classification optimization strategy (ACOS) is presented, which transforms a structurally unbalanced multiagent topology graph into a structurally balanced multiagent topology graph. Furthermore, how to deal with the nonaffine structure and unknown control gains for multiagent stochastic pure-feedback nonlinear systems is a challenging issue for designing controllers. As a result, the mean value theorem is used to transfer the nonaffine systems into the affine systems, and the Nussbaum functions are used to eliminate the influence caused by the unknown control gains. In addition, the intelligent control technique is used to approximate the unknown nonlinearities, the event-triggered control strategy following the switching thresholds is introduced to save unnecessary communication resources, and the computation burden is eliminated by means of the dynamic surface technique. It is proved that all signals of the closed-loop systems are bounded in probability and the tracking errors asymptotically converge to zero in probability. Finally, the simulation results illustrate the validity of the proposed scheme.},
  archive      = {J_TAI},
  author       = {Chaoda Liu and Ben Niu and Lei Liu and Xudong Zhao and Huanqing Wang and Huichuan Duan},
  doi          = {10.1109/TAI.2022.3214486},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1616-1626},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Event-triggered adaptive bipartite asymptotic tracking control using intelligent technique for stochastic nonlinear multiagent systems},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surrogate-assisted multiobjective neural architecture search
for real-time semantic segmentation. <em>TAI</em>, <em>4</em>(6),
1602–1615. (<a href="https://doi.org/10.1109/TAI.2022.3213532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The architectural advancements in deep neural networks have led to remarkable leap-forwards across a broad array of computer vision tasks. Instead of relying on human expertise, neural architecture search (NAS) has emerged as a promising avenue toward automating the design of architectures. While recent achievements on image classification have suggested opportunities, the promises of NAS have yet to be thoroughly assessed on more challenging tasks of semantic segmentation. The main challenges of applying NAS to semantic segmentation arise from two aspects: 1) high-resolution images to be processed; 2) additional requirement of real-time inference speed (i.e., real-time semantic segmentation) for applications such as autonomous driving. To meet such challenges, we propose a surrogate-assisted multiobjective method in this article. Through a series of customized prediction models, our method effectively transforms the original NAS task to an ordinary multiobjective optimization problem. Followed by a hierarchical prescreening criterion for in-fill selection, our method progressively achieves a set of efficient architectures trading-off between segmentation accuracy and inference speed. Empirical evaluations on three benchmark datasets together with an application using Huawei Atlas 200 DK suggest that our method can identify architectures significantly outperforming existing state-of-the-art architectures designed both manually by human experts and automatically by other NAS methods. Code is available from here.},
  archive      = {J_TAI},
  author       = {Zhichao Lu and Ran Cheng and Shihua Huang and Haoming Zhang and Changxiao Qiu and Fan Yang},
  doi          = {10.1109/TAI.2022.3213532},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1602-1615},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Surrogate-assisted multiobjective neural architecture search for real-time semantic segmentation},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid lane detection model for wild road conditions.
<em>TAI</em>, <em>4</em>(6), 1592–1601. (<a
href="https://doi.org/10.1109/TAI.2022.3212347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a lane detection system in unstructured, complex environments in real time. We formulated lane detection as a binary segmentation problem and proposed a novel hybrid convolutional neural network architecture by fusing encoder–decoder network with a sequence of dilated convolution network. We computed weighted average of outputs of these branches for lane detection and introduced a new loss function for binary semantic segmentation to counter the imbalance between lane and nonlane pixels. The proposed model achieved 95.19% accuracy on TuSimple dataset. To evaluate the system with respect to unstructured road scenarios, we created an Indian lane dataset with 6149 labeled images from India driving dataset (IDD). We reported intersection over union of 0.31 on IDD, which was higher than other state-of-the art lane detection models. Finally, we undertook an ablation study to understand effectiveness of two parallel branches, i.e., encoder–decoder and dilated convolution branches and the proposed loss function.},
  archive      = {J_TAI},
  author       = {Abhishek Mukhopadhyay and L. R. D. Murthy and Imon Mukherjee and Pradipta Biswas},
  doi          = {10.1109/TAI.2022.3212347},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1592-1601},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A hybrid lane detection model for wild road conditions},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot learning network for out-of-distribution image
classification. <em>TAI</em>, <em>4</em>(6), 1579–1591. (<a
href="https://doi.org/10.1109/TAI.2022.3212346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification in real-world applications is a challenging task due to the lack of labeled data. Many few-shot learning techniques have been developed to tackle this problem. However, existing few-shot learning techniques will fail if new classes are added to the model without any labeled data (out-of-distribution). The existing techniques will classify the new classes as one of the existing classes and will not be able to detect them as new classes. Also, if all data are unlabeled, existing few-shot learning techniques will not work (because existing techniques are supervised learning). This article proposes a novel few-shot learning network [knowledge transfer network (KTNet)] that can learn from unlabeled data and assigns a pseudolabel to these data. These pseudolabeled data will either be added to the existing labeled data (in-distribution) to increase the number of shots or added as new classes if the data are out-of-distribution. The proposed KTNet technique can work in all cases, such as a small amount of labeled data exist for all classes, labeled data exist for a subset of all classes, and if existing data for all classes are unlabeled. KTNet is evaluated using two benchmark datasets (mini-ImageNet and fewshot-CIFAR). The results show that the proposed network outperforms state-of-the-art models in both datasets with respect to classification accuracy. Also, KTNet is better than existing techniques at detecting and clustering the out-of-distribution classes.},
  archive      = {J_TAI},
  author       = {Islam I. Osman and Mohamed S. Shehata},
  doi          = {10.1109/TAI.2022.3212346},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1579-1591},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Few-shot learning network for out-of-distribution image classification},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoKE: An automatic knowledge embedding framework for
scientific machine learning. <em>TAI</em>, <em>4</em>(6), 1564–1578. (<a
href="https://doi.org/10.1109/TAI.2022.3209167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imposing physical constraints on neural networks as a method of knowledge embedding has achieved great progress in solving physical problems described by governing equations. However, for many engineering problems, governing equations often have complex forms, including complex partial derivatives or stochastic physical fields, which results in significant inconveniences from the perspective of implementation. In this article, to effectively automate the process of embedding physical knowledge, a scientific machine learning framework, called automatic machine learning framework (AutoKE), is proposed, and a reservoir flow problem with a complex governing equation is taken as an instance. In AutoKE, an emulator comprised of deep neural networks is built for predicting the physical variables of interest. An arbitrarily complex equation can be parsed and automatically converted into a computational graph through the equation parser module, and the fitness of the emulator to the governing equation is evaluated via automatic differentiation. Furthermore, the fixed weights in the loss function are substituted with adaptive weights by incorporating the Lagrangian dual method. Neural architecture search is also introduced into the AutoKE to select an optimal network architecture of the emulator according to the specific problem. Finally, we apply transfer learning to enhance the scalability of the emulator. In experiments, the framework is verified by a series of physical problems in which it can automatically embed physical knowledge into an emulator without heavy hand-coding. The results demonstrate that the emulator can not only make accurate predictions, but also be applied to similar problems with high efficiency via transfer learning.},
  archive      = {J_TAI},
  author       = {Mengge Du and Yuntian Chen and Dongxiao Zhang},
  doi          = {10.1109/TAI.2022.3209167},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1564-1578},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {AutoKE: An automatic knowledge embedding framework for scientific machine learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RAGAN: Regression attention generative adversarial networks.
<em>TAI</em>, <em>4</em>(6), 1549–1563. (<a
href="https://doi.org/10.1109/TAI.2022.3209956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite surrounding by Big Data, we still need to learn from insufficient data in many scenarios. Building an accurate regression model for a small amount of data is a pretty tricky and exciting problem. At present, it is a promising solution to augment limited real data by generating data through generative adversarial networks (GANs). However, when GAN is used to generate labeled data in regression modeling, it lacks attention to the relationship between independent and dependent variables, resulting in poor performance of regression modeling. This article proposes a novel regression attention GAN (RA-GAN) for augmented regression modeling. Regression attention mechanisms are introduced into network parameters learning of both generator and discriminator in RA-GAN to establish a known relationship between variables. This makes RA-GAN restore the regression information during data generation. In addition, an indicator called cross regression score is designed to describe the quality of the generated data before augmented regression modeling, effectively evaluating data augmentation performance in advance. The effectiveness and superiority of the proposed methods are verified in an actual industrial soft-sensing case and a diabetes prediction case through data augmentation regression applications.},
  archive      = {J_TAI},
  author       = {Xiaoyu Jiang and Zhiqiang Ge},
  doi          = {10.1109/TAI.2022.3209956},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1549-1563},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {RAGAN: Regression attention generative adversarial networks},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdaInject: Injection-based adaptive gradient descent
optimizers for convolutional neural networks. <em>TAI</em>,
<em>4</em>(6), 1540–1548. (<a
href="https://doi.org/10.1109/TAI.2022.3208223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convolutional neural networks (CNNs) are generally trained using stochastic gradient descent (SGD)-based optimization techniques. The existing SGD optimizers generally suffer with the overshooting of the minimum and oscillation near minimum. In this article, we propose a new approach, hereafter referred as AdaInject, for the gradient descent optimizers by injecting the second-order moment into the first-order moment. Specifically, the short-term change in parameter is used as a weight to inject the second-order moment in the update rule. The AdaInject optimizer controls the parameter update, avoids the overshooting of the minimum, and reduces the oscillation near minimum. The proposed approach is generic in nature and can be integrated with any existing SGD optimizer. The effectiveness of the AdaInject optimizer is explained intuitively as well as through some toy examples. We also show the convergence property of the proposed injection-based optimizer. Furthermore, we depict the efficacy of the AdaInject approach through extensive experiments in conjunction with the state-of-the-art optimizers, namely AdamInject, diffGradInject, RadamInject, and AdaBeliefInject, on four benchmark datasets. Different CNN models are used in the experiments. A highest improvement in the top-1 classification error rate of 16.54% is observed using diffGradInject optimizer with ResNeXt29 model over the CIFAR10 dataset. Overall, we observe very promising performance improvement of existing optimizers with the proposed AdaInject approach.},
  archive      = {J_TAI},
  author       = {Shiv Ram Dubey and S. H. Shabbeer Basha and Satish Kumar Singh and Bidyut Baran Chaudhuri},
  doi          = {10.1109/TAI.2022.3208223},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1540-1548},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {AdaInject: Injection-based adaptive gradient descent optimizers for convolutional neural networks},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FaceTopoNet: Facial expression recognition using face
topology learning. <em>TAI</em>, <em>4</em>(6), 1526–1539. (<a
href="https://doi.org/10.1109/TAI.2022.3207450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior work has shown that the order in which different components of the face are learned using a sequential learner can play an important role in the performance of facial expression recognition systems. We propose FaceTopoNet, an end-to-end deep model for facial expression recognition, which is capable of learning an effective tree topology of the face. Our model then traverses the learned tree to generate a sequence, which is then used to form an embedding to feed a sequential learner. The devised model adopts one stream for learning structure and one stream for learning texture. The structure stream focuses on the positions of the facial landmarks, whereas the main focus of the texture stream is on the patches around the landmarks to learn textural information. We then fuse the outputs of the two streams by utilizing an effective attention-based fusion strategy. We perform extensive experiments on four large-scale in-the-wild facial expression datasets—namely AffectNet, FER2013, ExpW, and real-world affective face database—and one lab-controlled dataset (Cohn–Kanade) to evaluate our approach. FaceTopoNet achieves state-of-the-art performance on three of the five datasets and obtains competitive results on the other two datasets. We also perform rigorous ablation and sensitivity experiments to evaluate the impact of different components and parameters in our model. Finally, we perform robustness experiments and demonstrate that FaceTopoNet is more robust against occlusions in comparison to other leading methods in the area.},
  archive      = {J_TAI},
  author       = {Mojtaba Kolahdouzi and Alireza Sepas-Moghaddam and Ali Etemad},
  doi          = {10.1109/TAI.2022.3207450},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1526-1539},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FaceTopoNet: Facial expression recognition using face topology learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concurrent learning-based neuroadaptive robust tracking
control of wheeled mobile robot: An event-triggered design.
<em>TAI</em>, <em>4</em>(6), 1514–1525. (<a
href="https://doi.org/10.1109/TAI.2022.3207133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an event-based neuroadaptive robust tracking controller for a perturbed and networked differential drive mobile robot (DMR) is designed with concurrent learning. A radial basis function neural network (RBFNN), which approximates an unknown perturbation, is used to design an adaptive sliding mode controller. The RBFNN weights and sliding mode controller parameters are estimated online using an adaptive tuning law to ensure performance with reduced chattering. To improve the convergence of RBFNN weight estimation error, a concurrent learning-based adaptive law is derived, which uses measured online and recorded data. Furthermore, a suitable triggering condition is designed to achieve a reduced number of control computations while minimizing network resources without sacrificing the stability of the sampled data closed-loop control system. A finite sampling frequency is guaranteed for the designed triggering condition by establishing a positive lower bound on the inter-event execution time, which is equivalent to the Zeno-free behavior of the system. Finally, the proposed event-based neuroadaptive robust controller is implemented on a practical system (Q-bot 2e) to show the effectiveness of the proposed design.},
  archive      = {J_TAI},
  author       = {Krishanu Nath and Manas Kumar Bera and Sarangapani Jagannathan},
  doi          = {10.1109/TAI.2022.3207133},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1514-1525},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Concurrent learning-based neuroadaptive robust tracking control of wheeled mobile robot: An event-triggered design},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boundary-aware network with topological consistency
constraint for optic chiasm segmentation. <em>TAI</em>, <em>4</em>(6),
1504–1513. (<a href="https://doi.org/10.1109/TAI.2022.3205870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optic chiasm is a structure that is easily compressed by the tumors leading to different degrees of visual field defect and visual disturbance. When the optic chiasm is compressed, the segmentation of the optic chiasm from magnetic resonance imaging images is helpful for prognosis prediction and radiotherapy planning. However, owing to the ambiguity of the optic chiasm boundary and the neglect of the anatomical structure consistency, the performance of the existing methods is limited. In this article, a boundary-aware network (BANet) with a topological consistency constraint is proposed for the automated segmentation of the optic chiasm. The BANet constrained by topology loss leverages the complementary information between the boundary feature and the segmentation feature to effectively improve segmentation performance and topological consistency. To evaluate the effectiveness of the proposed method, a real-world specialized optic chiasm segmentation dataset is constructed. The experimental results demonstrate that the proposed method achieves higher segmentation accuracy compared with the state-of-the-art method.},
  archive      = {J_TAI},
  author       = {Wei Huang and Xin Shu and Zizhou Wang and Chaoyue Chen and Yang Zhang and Lei Zhang and Jianguo Xu},
  doi          = {10.1109/TAI.2022.3205870},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1504-1513},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Boundary-aware network with topological consistency constraint for optic chiasm segmentation},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DReD–a descriptive relation dataset for expanding relation
extraction. <em>TAI</em>, <em>4</em>(6), 1494–1503. (<a
href="https://doi.org/10.1109/TAI.2022.3205567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction is a fundamental topic in document information extraction. Traditionally, datasets for relation extraction have been annotated with named entities and classified with a subset of relation categories. Models then predict either the entities and relations (end-to-end) or assume the entities are given and only classify the relations. However, current approaches are limited by datasets with a narrow definition of entities and relations. We seek to remedy this by introducing our Descriptive Relation Dataset (DReD), which contains 3286 annotations for descriptions of relations between more general noun phrases inspired by linguistic theory. We benchmark our dataset using several seq2seq models and find that T5 achieves the best results with a ROUGE-1 score of 75.5. We verify the usefulness of DreD by collecting feedback on 100 predictions and comparing human judgment to automated scoring methods. Finally, we verify that relations can be described accurately by transforming the CoNLL04 and Re-TACRED datasets and mapping sentence templates to relation categories. T5 achieves competitive accuracy on CoNLL-04 and Re-TACRED with an F1 score of 78.6 and 90.4, respectively. With this article, we prove that relations can be described, therefore overcoming the limitations set by previous datasets and approaches. We publicly provide our dataset and training code at https://github.com/logan-markewich/DReD . Relation extraction is a powerful task, providing a method to extract labeled connections between words in a document. Existing datasets focus on relations between important named entities, with relations sourced from a list of predefined categories. These categories create limitations for trained models, missing important context that a category name cannot capture alone. Our new Descriptive Relation Dataset, DReD, overcomes these limitations by providing a dataset that allows models to learn how to describe relations in a sentence. DReD contains 3286 annotations of descriptions of relations between general noun phrases, removing the previously stated limitations and providing a way to uncover previously unseen relation types while providing meaningful context. Furthermore, any sequence-to-sequence model can be easily trained on DReD, allowing for flexible and future-proof applications.},
  archive      = {J_TAI},
  author       = {Logan Markewich and Yubin Xing and Roy Ka-Wei Lee and Zhi Li and Seokbum Ko},
  doi          = {10.1109/TAI.2022.3205567},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1494-1503},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {DReD–A descriptive relation dataset for expanding relation extraction},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IBAttack: Being cautious about data labels. <em>TAI</em>,
<em>4</em>(6), 1484–1493. (<a
href="https://doi.org/10.1109/TAI.2022.3206259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional backdoor attacks insert a trigger patch in the training images and associate the trigger with the targeted class label. Backdoor attacks are one of the rapidly evolving types of attack which can have a significant impact. On the other hand, adversarial perturbations have a significantly different attack mechanism from the traditional backdoor corruptions, where an imperceptible noise is learned to fool the deep learning models. In this research, we amalgamate these two concepts and propose a novel imperceptible backdoor attack, termed as the IBAttack , where the adversarial images are associated with the desired target classes. A significant advantage of the adversarial-based proposed backdoor attack is the imperceptibility as compared to the traditional trigger-based mechanism. The proposed adversarial dynamic attack, in contrast to existing attacks, is agnostic to classifiers and trigger patterns. The extensive evaluation using multiple databases and networks illustrates the effectiveness of the proposed attack.},
  archive      = {J_TAI},
  author       = {Akshay Agarwal and Richa Singh and Mayank Vatsa and Nalini Ratha},
  doi          = {10.1109/TAI.2022.3206259},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1484-1493},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {IBAttack: Being cautious about data labels},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). JSSE: Joint sequential semantic encoder for zero-shot event
recognition. <em>TAI</em>, <em>4</em>(6), 1472–1483. (<a
href="https://doi.org/10.1109/TAI.2022.3208860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) is a paradigm in transfer learning that aims to recognize unknown categories by having a mere description of them. The problem of ZSL has been thoroughly studied in the domain of static object recognition; however, ZSL for dynamic events (zero-shot event recognition, ZSER) such as activities and gestures has hardly been investigated. In this context, this article addresses ZSER by relying on semantic attributes of events to transfer the learned knowledge from seen classes to unseen ones. First, we utilized the Amazon Mechanical Turk platform to create the first attribute-based gesture dataset, referred to as zero shot gestural learning (ZSGL), comprising the categories present in MSRC and Italian gesture datasets. Overall, our ZSGL dataset consisted of 26 categories, 65 discriminative attributes, and 16 attribute annotations and 400 examples per category. We used trainable recurrent networks and 3-D convolutional neural networks (CNNs) to learn the spatiotemporal features. Next, we propose a simple yet effective end-to-end approach for ZSER, referred to as joint sequential semantic encoder (JSSE), to explore temporal patterns, to efficiently represent events in the latent space, and to simultaneously optimize for both the semantic and classification tasks. We evaluate our model on ZSGL and two action datasets (UCF and HMDB), and compared the performance of JSSE against several existing baselines under four experimental conditions: 1) within-category , 2) across-category , 3) closed-set , and 4) open-set . Results show that JSSE considerably outperforms ( &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$p&amp;lt; 0.05$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) other approaches and performs favorably for both the datasets under all experimental conditions.},
  archive      = {J_TAI},
  author       = {Naveen Madapana and Juan P. Wachs},
  doi          = {10.1109/TAI.2022.3208860},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1472-1483},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {JSSE: Joint sequential semantic encoder for zero-shot event recognition},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Action recognition in dark videos using spatio-temporal
features and bidirectional encoder representations from transformers.
<em>TAI</em>, <em>4</em>(6), 1461–1471. (<a
href="https://doi.org/10.1109/TAI.2022.3221912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several research works have been developed in the area of action recognition. Unfortunately, when these algorithms are applied to low-light or dark videos, their performances are highly affected and found to be very poor or fall rapidly. To address the issue of improving the performance of action recognition in dark or low-light videos; in this article, we have developed an efficient deep 3-D convolutional neural network based action recognition model. The proposed algorithm follows two-stages for action recognition. In the first stage, the low-light videos are enhanced using zero-reference deep curve estimation, followed by the min–max sampling algorithm. In the latter stage, we propose an action classification network to recognize the actions in the enhanced videos. In the proposed action classification network, we explored the capabilities of the &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$R(2+1)D$&lt;/tex-math&gt;&lt;/inline-formula&gt; for spatio-temporal feature extraction. The model&#39;s overall generalization performance depends on how well it can capture long-range temporal structure in videos, which is essential for action recognition. So we have used a graph convolutional network on the top of &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$R(2+1)D$&lt;/tex-math&gt;&lt;/inline-formula&gt; as our video feature encoder, which captures long-term temporal dependencies of the extracted features. Finally, a bidirectional encoder representations from transformers is adhered to classify the actions from the 3-D features extracted from the enhanced video scenes. The effectiveness of the proposed action recognition scheme is verified on ARID V1.0 and ARID V1.5 datasets. It is observed that the proposed algorithm is able to achieve 96.60% and 99.88% as Top-1 and Top-5 accuracy, respectively, on ARID V1.0 dataset. Similarly, on ARID V1.5, the proposed algorithm is able to achieve 86.93% and 99.35% as Top-1 and Top-5 accuracies, respectively. To corroborate our findings, we have compared the results obtained by the proposed scheme with those of 15 state-of-the-art action recognition techniques.},
  archive      = {J_TAI},
  author       = {Himanshu Singh and Saurabh Suman and Badri Narayan Subudhi and Vinit Jakhetiya and Ashish Ghosh},
  doi          = {10.1109/TAI.2022.3221912},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1461-1471},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Action recognition in dark videos using spatio-temporal features and bidirectional encoder representations from transformers},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to play koi-koi hanafuda card games with
transformers. <em>TAI</em>, <em>4</em>(6), 1449–1460. (<a
href="https://doi.org/10.1109/TAI.2023.3240674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Card games are regarded as an idealized model for many real-world problems for their rich hidden information and strategic decision-making process. It provides a fertile environment for artificial intelligence (AI), especially reinforcement learning (RL) algorithms. With the boom of deep neural networks, increasing breakthroughs have been made in this challenging domain. Koi-Koi is a traditional two-player imperfect-information playing card game. However, due to its unique deck and complex rules, related researches are mostly based on handcrafted features and the custom network architecture. In this article, we design a more general AI framework, relying a transformer encoder as the network backbone with tokenized card state input, which is trained by the Monte-Carlo RL with phased round reward. Experimental results show that our AI achieves a winning rate of 53% and +2.02 average difference point versus experienced human players in multiround Koi-Koi games. Moreover, with the aid of attention mechanism, we provide a novel view for analyzing the playing strategy. Such framework design can be applied to various card games.},
  archive      = {J_TAI},
  author       = {Sanghai Guan and Jingjing Wang and Ruijie Zhu and Junhui Qian and Zhongxiang Wei},
  doi          = {10.1109/TAI.2023.3240674},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1449-1460},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning to play koi-koi hanafuda card games with transformers},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recording behaviors of artificial intelligence in
blockchains. <em>TAI</em>, <em>4</em>(6), 1437–1448. (<a
href="https://doi.org/10.1109/TAI.2022.3213531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key challenges of current blockchain, such as low throughput, high latency, and poor scalability, hinder its future development. The recent advances in artificial intelligence (AI) can well overcome these challenges, thus spurring an increasing number of organizations and individuals to equip blockchain with AI, making blockchain become more intelligent. However, AI behaviors need to be supervised for ex-post forensics in case of dispute and accountability. This motivates us to envision a model to record the AI behaviors in intelligent blockchain. In this article, we propose such a model called AI-Tracer, which leverages proxy re-encryption based on Schnorr signature as well as InterPlanetary File System (IPFS) to be integrated into intelligent blockchain. AI-Tracer generates AI-digest during the AI learning process and encrypts it with proxy re-encryption. IPFS is responsible to store the encrypted AI-digest and returns a hash pointer related to AI-digest for further verification. The authorized user can verify the validity of AI-digest by leveraging advanced features of blockchain technology and only the valid AI-digest can be uploaded to intelligent blockchain, which implements trusted tracking for AI behaviors. AI-Tracer achieves fine-grained access control of AI-digest via proxy re-encryption. A user can retrieve the plaintext of AI-digest from intelligent blockchain with specific authorization. We conduct theoretical analysis to indicate the privacy and security of AI-Tracer. Moreover, we deploy AI-Tracer on the Hyperledger Fabric and demonstrate the efficiency and effectiveness of AI-Tracer through extensive experiments.},
  archive      = {J_TAI},
  author       = {Yushu Zhang and Jiahao Zhao and Jiajia Jiang and Youwen Zhu and Liangmin Wang and Yong Xiang},
  doi          = {10.1109/TAI.2022.3213531},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1437-1448},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Recording behaviors of artificial intelligence in blockchains},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Answering binary causal questions using role-oriented
concept embedding. <em>TAI</em>, <em>4</em>(6), 1426–1436. (<a
href="https://doi.org/10.1109/TAI.2022.3204245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answering binary causal questions is a challenging task, and it requires rich background knowledge to answer such questions. Extracting useful causal features from the background knowledge base and applying them effectively in a model is a crucial step to answering binary causal questions. The state-of-the-art approaches apply deep learning techniques to answer binary causal questions. In these approaches, candidate concepts are often embedded into vectors to model causal relationships among them. However, a concept may play the role of a cause in one question, but it could be an effect in another question. This aspect has not been extensively explored in existing approaches. Role-oriented causal concept embeddings are proposed in this article to model causality between concepts. We also propose leveraging semantic concept similarity to extract causal information from concepts. Finally, we develop a deep learning framework to answer binary causal questions. Our approach yields accuracy that is comparable to or better than the benchmark approaches.},
  archive      = {J_TAI},
  author       = {Humayun Kayesh and Md. Saiful Islam and Junhu Wang},
  doi          = {10.1109/TAI.2022.3204245},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1426-1436},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Answering binary causal questions using role-oriented concept embedding},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine-grained early frequency attention for deep speaker
representation learning. <em>TAI</em>, <em>4</em>(6), 1413–1425. (<a
href="https://doi.org/10.1109/TAI.2023.3240113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have considerably improved speech processing in recent years. Speaker representations extracted by deep learning models are being used in a wide range of tasks, such as speaker recognition and speech emotion recognition. Attention mechanisms have started to play an important role in improving deep learning models in the field of speech processing. Nonetheless, despite the fact that important speaker-related information can be embedded in individual frequency-bins of the input spectral representations, current attention models are unable to attend to fine-grained information items in spectral representations. In this article, we propose Fine-grained Early Frequency Attention (FEFA) for speaker representation learning. Our model is a simple and lightweight model that can be integrated into various convolutional neural networks (CNN) pipelines and is capable of focusing on information items as small as frequency-bins. We evaluate the proposed model on three tasks of speaker recognition, speech emotion recognition, and spoken digit recognition. We use three widely used public datasets, namely VoxCeleb, IEMOCAP, and free spoken digit dataset for our experiments. We attach FEFA to several prominent deep learning models and evaluate its impact on the final performance. We also compare our work with other related works in the area. Our experiments show that by adding FEFA to different CNN architectures, performance is consistently improved by substantial margins, and the models equipped with FEFA outperform all the other attentive models. We also test our model against different levels of the added noise showing improvements in robustness and less sensitivity compared to the backbone networks.},
  archive      = {J_TAI},
  author       = {Amirhossein Hajavi and Ali Etemad},
  doi          = {10.1109/TAI.2023.3240113},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1413-1425},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fine-grained early frequency attention for deep speaker representation learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). View invariant spatio-temporal descriptor for action
recognition from skeleton sequences. <em>TAI</em>, <em>4</em>(6),
1399–1412. (<a href="https://doi.org/10.1109/TAI.2022.3213234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, human action recognition based on 3-D skeleton sequences has gained significant interest in the computer vision field. However, skeleton sequences are sensitive to noises, viewpoint variations, and similar movements. To sort out these problems, this article proposes a view invariant spatio-temporal descriptor (VISTD) that encodes the positions of skeleton joints in each frame of an action sequence. VISTD is a combination of view invariant skeleton joint descriptor and spatio-temporal skeleton joint descriptor. Further, we employ a standard convolutional neural network (CNN) architecture for classification. To analyze the impact of fusion methods, the results of CNN model are fused with different fusion rules. Extensive simulation experiments were carried out over the proposed model through UWA3DII dataset and NTU-RGB+D dataset. The comparison of experimental results with existing methods explores the superiority.},
  archive      = {J_TAI},
  author       = {Venkata Subbareddy K and Nirmala Devi L},
  doi          = {10.1109/TAI.2022.3213234},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1399-1412},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {View invariant spatio-temporal descriptor for action recognition from skeleton sequences},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Writing in the air: Unconstrained text recognition from
finger movement using spatio-temporal convolution. <em>TAI</em>,
<em>4</em>(6), 1386–1398. (<a
href="https://doi.org/10.1109/TAI.2022.3212981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a new benchmark dataset for the challenging writing in the air (WiTA) task—an elaborate task bridging vision and natural language processing (NLP). WiTA implements an intuitive and natural writing method with finger movement for human–computer interaction (HCI). Our WiTA dataset will facilitate the development of data-driven WiTA systems, which, thus, far have displayed unsatisfactory performance—due to lack of dataset as well as traditional statistical models they have adopted. Our dataset consists of five subdatasets in two languages (Korean and English) and amounts to 209 926 video instances from 122 participants. We capture finger movement for WiTA with red-green-blue (RGB) cameras to ensure wide accessibility and cost-efficiency. Next, we propose spatio-temporal residual network architectures inspired by 3-D ResNet. These models perform unconstrained text recognition from finger movement, guarantee a real-time operation [ &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$&amp;gt;$&lt;/tex-math&gt;&lt;/inline-formula&gt; 100 frames per second (FPS)], and will serve as an evaluation standard.},
  archive      = {J_TAI},
  author       = {Ue-Hwan Kim and Yewon Hwang and Sun-Kyung Lee and Jong-Hwan Kim},
  doi          = {10.1109/TAI.2022.3212981},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1386-1398},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Writing in the air: Unconstrained text recognition from finger movement using spatio-temporal convolution},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical reinforcement learning for air combat at
DARPA’s AlphaDogfight trials. <em>TAI</em>, <em>4</em>(6), 1371–1385.
(<a href="https://doi.org/10.1109/TAI.2022.3222143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous control in high-dimensional, continuous state spaces is a persistent and important challenge in the fields of robotics and artificial intelligence. Because of high risk and complexity, the adoption of AI for autonomous combat systems has been a long-standing difficulty. In order to address these issues, DARPA&#39;s AlphaDogfight Trials (ADT) program sought to vet the feasibility of and increase trust in AI for autonomously piloting an F-16 in simulated air-to-air combat. Our submission to ADT solves the high-dimensional, continuous control problem using a novel hierarchical deep reinforcement learning approach consisting of a high-level policy selector and a set of separately trained low-level policies specialized for excelling in specific regions of the state space. Both levels of the hierarchy are trained using off-policy, maximum entropy methods with expert knowledge integrated through reward shaping. Our approach outperformed human expert pilots and achieved a second-place rank in the ADT championship event.},
  archive      = {J_TAI},
  author       = {Adrian P. Pope and Jaime S. Ide and Daria Mićović and Henry Diaz and Jason C. Twedt and Kevin Alcedo and Thayne T. Walker and David Rosenbluth and Lee Ritholtz and Daniel Javorsek},
  doi          = {10.1109/TAI.2022.3222143},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1371-1385},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Hierarchical reinforcement learning for air combat at DARPA&#39;s AlphaDogfight trials},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-order deep learning for generalized synthesis of
radiation patterns for antenna arrays. <em>TAI</em>, <em>4</em>(5),
1359–1368. (<a href="https://doi.org/10.1109/TAI.2022.3192505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article tackles the generalized synthesis of antenna arrays using two-order deep learning. Existing deep learning-assisted antenna synthesis approaches mainly rely on model training using electromagnetic (EM) simulation data, and hence feature limited generalization ability and the need for a huge amount of EM simulations. The proposed two-order deep learning method uses the first-order model to learn the generic features of radiation patterns from the data efficiently generated by applying conventional array factors. After that, the second-order model learns from EM simulations to capture the detailed pattern variations due to concrete coupling effects in the case of a specific array arrangement with different operating frequencies, radiation structures, and feeding schemes. Therefore, the two-order DL model can predict the radiation patterns of a series of antenna arrays while reducing the needed amount of EM simulation data. Implementation is carried out on a series of patch antenna arrays to verify the feasibility and robustness of the proposed approach. The validation includes conditions for the array operating at arbitrary new frequencies, with modified radiation structures or new feeding schemes. The results show that the proposed two-order model provides a prediction accuracy of about 84 &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; for a series of 1 × 4 antenna arrays, clearly outperforming the existing regular one-order DL model, which obtains around 65 &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; with the same EM simulation data. The proposed method reveals a promising direction for applying deep learning to assist the design and analysis of antenna arrays.},
  archive      = {J_TAI},
  author       = {Zhao Zhou and Zhaohui Wei and Jian Ren and Yingzeng Yin and Gert Frølund Pedersen and Ming Shen},
  doi          = {10.1109/TAI.2022.3192505},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1359-1368},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Two-order deep learning for generalized synthesis of radiation patterns for antenna arrays},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot learning with class imbalance. <em>TAI</em>,
<em>4</em>(5), 1348–1358. (<a
href="https://doi.org/10.1109/TAI.2023.3298303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) algorithms are commonly trained through meta-learning (ML), which exposes models to batches of tasks sampled from a meta-dataset to mimic tasks seen during evaluation. However, the standard training procedures overlook the real-world dynamics where classes commonly occur at different frequencies. While it is generally understood that class imbalance harms the performance of supervised methods, limited research examines the impact of imbalance on the FSL evaluation task. Our analysis compares ten state-of-the-art ML and FSL methods on different imbalance distributions and rebalancing techniques. Our results reveal that: 1) some FSL methods display a natural disposition against imbalance while most other approaches produce a performance drop by up to 17% compared to the balanced task without the appropriate mitigation; 2) many ML algorithms will not automatically learn to balance from exposure to imbalanced training tasks; 3) classical rebalancing strategies, such as random oversampling, can still be very effective, leading to state-of-the-art performances and should not be overlooked.},
  archive      = {J_TAI},
  author       = {Mateusz Ochal and Massimiliano Patacchiola and Jose Vazquez and Amos Storkey and Sen Wang},
  doi          = {10.1109/TAI.2023.3298303},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1348-1358},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Few-shot learning with class imbalance},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural architecture search for image dehazing. <em>TAI</em>,
<em>4</em>(5), 1337–1347. (<a
href="https://doi.org/10.1109/TAI.2022.3204732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manual design of deep networks require numerous trials and parameter tuning, resulting in inefficient utilization of time, energy, and resources. In this article, we present a neural architecture search (NAS) algorithm—AutoDehaze, to automatically discover effective neural network for single image dehazing. The proposed AutoDehaze algorithm is built on the gradient-based search strategy and hierarchical network-level optimization. We construct a set of search space layouts to reduce memory consumption, avoid the NAS collapse issue, and considerably accelerate the search speed. We propose four search spaces &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\text{AutoDehaze}_{B}$&lt;/tex-math&gt;&lt;/inline-formula&gt; , &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\text{AutoDehaze}_{U1}$&lt;/tex-math&gt;&lt;/inline-formula&gt; , &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\text{AutoDehaze}_{U2}$&lt;/tex-math&gt;&lt;/inline-formula&gt; , and &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\text{AutoDehaze}_{L}$&lt;/tex-math&gt;&lt;/inline-formula&gt; , which are inspired by the boat-shaped, U-shaped, and lateral connection-based designs. To the best of authors knowledge, this is a first attempt to present an NAS method for dehazing with a variety of network search strategies. We conduct a comprehensive set of experiments on Reside-Standard (SOTS), Reside- &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\beta$&lt;/tex-math&gt;&lt;/inline-formula&gt; (SOTS) and Reside- &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\beta$&lt;/tex-math&gt;&lt;/inline-formula&gt; (HSTS), D-Hazy, and HazeRD datasets. The architectures discovered by the proposed AutoDehaze quantitatively and qualitatively outperform the existing state-of-the-art approaches. The experiments also show that our models have considerably fewer parameters and runs at a faster inference speed in both CPU and GPU devices.},
  archive      = {J_TAI},
  author       = {Murari Mandal and Yashwanth Reddy Meedimale and M. Satish Kumar Reddy and Santosh Kumar Vipparthi},
  doi          = {10.1109/TAI.2022.3204732},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1337-1347},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Neural architecture search for image dehazing},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multistate load state identification model based on time
convolutional networks and conditional random fields. <em>TAI</em>,
<em>4</em>(5), 1328–1336. (<a
href="https://doi.org/10.1109/TAI.2022.3203685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing concern for carbon emissions requires smarter monitoring solutions for industrial load operating conditions. Nonintrusive load monitoring is one possible monitoring solution that can provide users with detailed information on the operating status of the load, thus improving energy management capabilities. We note that industrial loads have two outstanding characteristics compared to domestic loads: 1) long single-state operation times; and 2) highly directional state transfer probabilities. Considering these two characteristics, we propose the temporal convolutional network-conditional random field architecture to construct a deep learning architecture with long-term dependencies capability and probabilistic transfer modeling. The results show that the proposed architecture can achieve over 97% recognition accuracy on the industrial load dataset used, and can suppress the problem of frequent switching of recognition state results.},
  archive      = {J_TAI},
  author       = {Zhenyu Zhang and Yong Li and Jing Duan and Yixiu Guo and Zeyu Hou and Yilong Duan and Yijia Cao and Christian Rehtanz},
  doi          = {10.1109/TAI.2022.3203685},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1328-1336},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A multistate load state identification model based on time convolutional networks and conditional random fields},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partial point cloud registration with deep local feature.
<em>TAI</em>, <em>4</em>(5), 1317–1327. (<a
href="https://doi.org/10.1109/TAI.2022.3201505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to accurately register partial point cloud still remains a challenging task, because of its irregular and unordered structure in a non-Euclidean space, noise, outliers, and other unfavorable factors. In this article, an effective partial point cloud registration network is proposed by devising a two-stage deep local feature extraction process and an outlier filtering strategy. To be specific, on the one hand, to effectively capture geometric interdependency in the low-level space, a local attention feature extraction module is explored to extract local contextual attention features by highlighting different attention weights on neighborhoods. On the other hand, in the local feature aggregation module, two position encoding blocks are applied to increase the receptive field of each point in the high-level space. Of these, an attentive pooling can automatically learn important local features to alleviate the possible information loss. Furthermore, to derive the weight of the putative correspondence, an outlier filtering module is designed by consisting of point context normalization block, differentiable pooling layer, and differentiable unpooling layer. Moreover, in order to enhance robustness, a weighting point cloud registration model is formulated to alleviate outliers by considering the contribution of each correspondence. Experiments on multiple datasets demonstrate that the proposed approach is competitive to several state-of-the-art algorithms.},
  archive      = {J_TAI},
  author       = {Yu-Xin Zhang and Zhan-Li Sun and Zhigang Zeng and Kin-Man Lam},
  doi          = {10.1109/TAI.2022.3201505},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1317-1327},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Partial point cloud registration with deep local feature},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). M2R2: Missing-modality robust emotion recognition framework
with iterative data augmentation. <em>TAI</em>, <em>4</em>(5),
1305–1316. (<a href="https://doi.org/10.1109/TAI.2022.3201809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with the utterance-level modalities missing problem with uncertain patterns on emotion recognition in conversation (ERC) task. Present models generally predict the speaker&#39;s emotions by its current utterance and context, which is degraded by modality missing considerably. Our work proposes a framework missing-modality robust emotion recognition (M2R2), which trains emotion recognition model with iterative data augmentation by learned common representation. First, a network called party attentive network (PANet) is designed to classify emotions, which tracks all the speakers&#39; states and context. Attention mechanism between speaker with other participants and dialogue topic is used to decentralize dependence on multitime and multiparty utterances instead of the possible incomplete one. Moreover, the common representation learning (CRL) problem is defined for modality-missing problem. Data imputation methods improved by the adversarial strategy are used here to construct extra features to augment data. Extensive experiments and case studies validate the effectiveness of our methods over baselines for modality-missing emotion recognition on two different datasets.},
  archive      = {J_TAI},
  author       = {Ning Wang and Hui Cao and Jun Zhao and Ruilin Chen and Dapeng Yan and Jie Zhang},
  doi          = {10.1109/TAI.2022.3201809},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1305-1316},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {M2R2: Missing-modality robust emotion recognition framework with iterative data augmentation},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-adaptive imbalanced domain adaptation with deep sparse
autoencoder. <em>TAI</em>, <em>4</em>(5), 1293–1304. (<a
href="https://doi.org/10.1109/TAI.2022.3196813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to transfer knowledge between different domains to develop an effective hypothesis in the target domain with scarce labeled data, which is an effective method for remedying the problem of labeled data requirement in deep learning. In reality, it is unavoidable that the dataset has a large gap in the number of positive and negative instances across different categories in source and target domains, which is the imbalanced domain adaptation problem. However, since the imbalanced degree always varies greatly in different source- and target-domain datasets, most of the existing imbalanced domain adaptation models fix the imbalanced parameters, which cannot adapt to the change of the proportion between positive and negative instances in different domains. To address this problem, in this article, we propose a self-adaptive imbalanced domain adaptation method via a deep sparse autoencoder, which can adjust the model automatically according to the imbalanced extent for bridging the chasm of domains. More specifically, the self-adaptive imbalanced cross-entropy loss is designed for emphasizing more on minority categories and compensating the bias of training loss automatically. In addition, to alleviate the deficient problem of labeled data, we further propose the unlabeled information incorporating method by minimizing the distribution discrepancy of high-level representation space between the source and target domains. Experiments on several real-world datasets demonstrate the effectiveness of our method compared to other state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Yi Zhu and Xindong Wu and Yun Li and Jipeng Qiang and Yunhao Yuan},
  doi          = {10.1109/TAI.2022.3196813},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1293-1304},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Self-adaptive imbalanced domain adaptation with deep sparse autoencoder},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An online unsupervised streaming features selection through
dynamic feature clustering. <em>TAI</em>, <em>4</em>(5), 1281–1292. (<a
href="https://doi.org/10.1109/TAI.2022.3196637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming feature selection (SFS) is emerging as a key research direction that addresses the nonstationary property of feature streams when the sample size is fixed. Most existing SFS techniques are supervised methods and ignore the label scarcity. Real-world datasets are typically unlabeled and the labeling costs are expensive. Although some unsupervised SFS approaches are proposed, these approaches are either limited to the homogeneous feature types or require substantial computational complexity. To address these problems, we propose an online unsupervised feature selection framework using dynamic feature clustering in this article. We derived a recursive density lower bound to estimate the density distribution of feature streams and developed a density-based dynamic clustering method to perform the online feature stream clustering for exploring feature redundancy. An unsupervised online feature relevance maximization and redundancy minimization strategy is introduced to extract a subset of important features with low redundancy from the feature stream. Experimental results on 13 well-known benchmark datasets and comparison studies with 7 state-of-the-art supervised SFS methods demonstrate that the proposed unsupervised method provides statistically comparable performance with the supervised SFS techniques while the label information is unknown.},
  archive      = {J_TAI},
  author       = {Xuyang Yan and Abdollah Homaifar and Mrinmoy Sarkar and Benjamin Lartey and Kishor Datta Gupta},
  doi          = {10.1109/TAI.2022.3196637},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1281-1292},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An online unsupervised streaming features selection through dynamic feature clustering},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization for interval type-2 polynomial fuzzy systems: A
deep reinforcement learning approach. <em>TAI</em>, <em>4</em>(5),
1269–1280. (<a href="https://doi.org/10.1109/TAI.2022.3187951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that the interval type-2 (IT2) fuzzy controllers are superior compared to their type-1 counterparts in terms of robustness, flexibility, etc. However, how to conduct the type reduction optimally with the consideration of system stability under the fuzzy-model-based (FMB) control framework is still an open problem. To address this issue, we present a new approach through the membership-function-dependent (MFD) and deep reinforcement learning approaches. In the proposed approach, the reduction of IT2 membership functions of the fuzzy controller is completing during optimizing the control performance. Another fundamental issue is that the stability conditions must hold subject to different type-reduction methods. It is tedious and impractical to resolve the stability conditions according to different type-reduction methods, which could lead to infinite possibility. It is more practical to guarantee the holding of stability conditions during type-reduction rather than resolving the stability conditions, the MFD approach is proposed with the imperfect premise matching concept. Thanks to the unique merit of the MFD approach, the stability conditions according to all the different embedded type-1 membership functions within the footprint of uncertainty are guaranteed to be valid. During the control processes, the state transitions associated with properly engineered cost/reward function can be used to approximately calculate the deterministic policy gradient to optimize the acting policy and then to improve the control performance through determining the grade of IT2 membership functions of the fuzzy controller. The detailed simulation example is provided to verify the merits of the proposed approach.},
  archive      = {J_TAI},
  author       = {Bo Xiao and Hak-Keung Lam and Chengbin Xuan and Ziwei Wang and Eric M. Yeatman},
  doi          = {10.1109/TAI.2022.3187951},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1269-1280},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optimization for interval type-2 polynomial fuzzy systems: A deep reinforcement learning approach},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ResNet-based parkinson’s disease classification.
<em>TAI</em>, <em>4</em>(5), 1258–1268. (<a
href="https://doi.org/10.1109/TAI.2022.3193651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson&#39;s disease (PD) is a brain disorder that leads to shaking, stiffness, and difficulty with walking, balance, and coordination. The symptoms usually begin gradually and get worse over time. Early diagnosis is very important because treatments are more effective and easier to perform during the early stages of PD. However, early diagnosis is challenging because the symptoms start gradually, and at the early stages, they are not very noticeable. In this article, we propose a method that uses ResNet50, a residual network that has 50 layers, to help diagnosis PD. The data used are a collection of frequency features acquired by applying spectral analysis strategies to the speech recordings of the patient. We then convert the frequency features into a 2-D heat map. This heat map is passed to ResNet50, which predicts whether the patient has PD or not. We have conducted experiments and compared the accuracy with several state-of-the-art methods. The results have demonstrated the feasibility and robustness of the proposed method.},
  archive      = {J_TAI},
  author       = {Omar El Ariss and Kaoning Hu},
  doi          = {10.1109/TAI.2022.3193651},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1258-1268},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ResNet-based parkinson&#39;s disease classification},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-task feedback fusion GAN for joint MR-CT synthesis and
segmentation of target and organs-at-risk. <em>TAI</em>, <em>4</em>(5),
1246–1257. (<a href="https://doi.org/10.1109/TAI.2022.3187388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synthesis of computed tomography (CT) images from magnetic resonance imaging (MR) images and segmentation of target and organs-at-risk (OARs) are two important tasks in MR-only radiotherapy treatment planning (RTP). Some methods have been proposed to utilize the paired MR and CT images for MR-CT synthesis or target and OARs segmentation. However, these methods usually handle synthesis and segmentation as two separate tasks, and ignore the inevitable registration errors in paired images after standard registration. In this article, we propose a cross-task feedback fusion generative adversarial network (CTFF-GAN) for joint MR-CT synthesis and segmentation of target and OARs to enhance each task’s performance. Specifically, we propose a cross-task feedback fusion (CTFF) module to feedback the semantic information from the segmentation task to the synthesis task for the anatomical structure correction in synthetic CT images. Besides, we use CT images synthesized from MR images for multimodal segmentation to eliminate the registration errors. Moreover, we develop a multitask discriminator to urge the generator to devote more attention to the organ boundaries. Experiments on our nasopharyngeal carcinoma dataset show that CTFF-GAN achieves impressive performance with MAE of 70.69 &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\pm$&lt;/tex-math&gt;&lt;/inline-formula&gt; 10.50 HU, SSIM of 0.755 &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\pm$&lt;/tex-math&gt;&lt;/inline-formula&gt; 0.03, and PSNR of 27.44 &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\pm$&lt;/tex-math&gt;&lt;/inline-formula&gt; 1.20 dB in synthetic CT, and the mean dice of 0.783 &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\pm$&lt;/tex-math&gt;&lt;/inline-formula&gt; 0.075 in target and OARs segmentation. Our CTFF-GAN outperforms state-of-the-art methods in both the synthesis and segmentation tasks.},
  archive      = {J_TAI},
  author       = {Yiwen Zhang and Liming Zhong and Hai Shu and Zhenhui Dai and Kaiyi Zheng and Zefeiyun Chen and Qianjin Feng and Xuetao Wang and Wei Yang},
  doi          = {10.1109/TAI.2022.3187388},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1246-1257},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Cross-task feedback fusion GAN for joint MR-CT synthesis and segmentation of target and organs-at-risk},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification of DNA n4-methylcytosine sites via multiview
kernel sparse representation model. <em>TAI</em>, <em>4</em>(5),
1236–1245. (<a href="https://doi.org/10.1109/TAI.2022.3187060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying DNA N4-methylcytosine (4mC) sites is of great significance in biological research, such as chromatin structure, DNA stability, DNA–protein interaction, and controlling gene expression. However, the traditional sequencing technology to identify 4mC sites is very time-consuming. In order to detect 4mC sites, we develop a multiview learning method for achieving more effectively via merging multiple feature spaces. Furthermore, we think about whether the multiview learning method can improve the across species classification ability by fusing data of multiple species. In our study, we propose a multiview Laplacian kernel sparse representation-based classifier, called MvLapKSRC-HSIC. First, we make use of three feature extraction methods [position-specific trinucleotide propensity, nucleotide chemical property, and DNA physicochemical properties) to extract the DNA sequence features. MvLapKSRC-HSIC uses a kernel sparse representation-based classifier with graph regularization. In order to maintain the independence between various views, we add a multiview regularization term constructed by Hilbert–Schmidt independence criterion (HSIC). In the experiments, MvLapKSRC-HSIC is applied on six datasets, so as to compare with other popular methods in single-species and cross-species experiments. All experimental results show that MvLapKSRC-HSIC is superior to other outstanding methods on both single species and cross species. Importantly, MvLapKSRC-HSIC can identify a series of potential DNA 4mC sites, which have not yet been experimentally evaluate on multiple species and merit further research.},
  archive      = {J_TAI},
  author       = {Chengwei Ai and Prayag Tiwari and Hongpeng Yang and Yijie Ding and Jijun Tang and Fei Guo},
  doi          = {10.1109/TAI.2022.3187060},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1236-1245},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Identification of DNA n4-methylcytosine sites via multiview kernel sparse representation model},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive multidimensional dual attentive DCNN for detecting
cardiac morbidities using fused ECG-PPG signals. <em>TAI</em>,
<em>4</em>(5), 1225–1235. (<a
href="https://doi.org/10.1109/TAI.2022.3184656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinicians refer to standard physiological signals (ECG and PPG) to diagnose conduction disorders and coronary arterial malfunctions. The exact disease is identified from the results of confirmatory medical tests. This procedure creates a time lag in treating patients and causes difficulty in attending emergency cases. In this study, a novel method is proposed to detect and classify diseases related to electrical impulses and coronary arteries from other cardiac abnormalities efficiently. The ECG and PPG signals are collected simultaneously from 300 cardiovascular diseased patients, following a predecided inclusion and exclusion criteria. A fused signal is generated using an algorithm from the PPG and ECG signals. The deep neural network is constructed involving self and cross attention properties, multidimensional convolution, and skip connections. Reinforcement learning is utilized to induce an adaptive property in the model. The model’s ability in distinguishing the targeted diseases is achieved with an accuracy of 0.80, recall of 0.75, precision of 0.73, micro avg F-score of 0.78, and specificity of 0.85. Ablation studies are performed to get an efficient network. Comparison with the state-of-art methods and other established networks is made to denote the superiority of the proposed network. The fused signal is more efficient in comparison to the individual signals. Analysis of the ROC and PR curves produced an AUC and AP of 0.79 and 0.76, respectively. This technique could be used to identify the type of CVD at a preliminary stage of clinical diagnosis effectively involving less time, effort, and resources.},
  archive      = {J_TAI},
  author       = {Poulomi Pal and Manjunatha Mahadevappa},
  doi          = {10.1109/TAI.2022.3184656},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1225-1235},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive multidimensional dual attentive DCNN for detecting cardiac morbidities using fused ECG-PPG signals},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A very-short-term online PV power prediction model based on
RAN with secondary dynamic adjustment. <em>TAI</em>, <em>4</em>(5),
1214–1224. (<a href="https://doi.org/10.1109/TAI.2022.3179353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photovoltaic (PV) power is progressively being subsumed into power grids. As a consequence, reliable PV power forecasting has become essential in order to ensure the optimal functioning of the power grid. Neural networks remain the dominant prediction model utilized. Conventional neural network forecasting models are wholly dependent upon offline data. Subsequent to offline training, no further structural adjustments can be made during the forecasting process, which therefore they fail to cater for PV power supply fluctuations that are fundamentally dynamic. To address this failing, this article proposes a very-short-term online prediction model based on a resource-allocating network (RAN) incorporating a secondary dynamic adjustment. The RAN is initially trained offline to obtain a basic forecasting model. Thereafter, in an online prediction process, those samples with large prediction errors that exceed a preset value are recorded in a specific buffer. When the set conditions are triggered, the secondary dynamic adjustment strategy is employed, which enables the online prediction model to effectively relearn previously unmodeled samples while shielding external interference. Experimental results obtained from actual testing demonstrate the validity of the secondary dynamic adjustment strategy for online learning while also providing higher accuracy levels from the prediction model.},
  archive      = {J_TAI},
  author       = {Tengfei Zhang and Fumin Ma and Chen Peng and Yang Yu and Dong Yue and Chunxia Dou and Gregory M. P. O&#39;Hare},
  doi          = {10.1109/TAI.2022.3179353},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1214-1224},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A very-short-term online PV power prediction model based on RAN with secondary dynamic adjustment},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GraphClusNet: A hierarchical graph neural network for
recovered circuit netlist partitioning. <em>TAI</em>, <em>4</em>(5),
1199–1213. (<a href="https://doi.org/10.1109/TAI.2022.3198930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware assurance (HA) is imperative to ensure the integrity of integrated circuits (ICs) after manufacturing. To verify the integrity of an IC would require the extraction of relevant functional block of interest through circuit partitioning. This is usually done by converting a recovered circuit netlist into a circuit graph and subsequently performing unsupervised clustering on the graph. However, circuit graphs are difficult to cluster due to the existence of inherent hierarchies and clusters of imbalanced sizes. In this article, we propose a novel hierarchical graph neural network (GNN), termed as GraphClusNet, to perform circuit graph clustering in a multistage process to achieve near-optimal results. We analytically derive a normalized-cut-based loss function, which allows for clusters of imbalanced sizes. We train our GraphClusNet in a multistage hierarchical fashion to regularize intermediate node embedding, which greatly alleviates the oversmoothing effect in deep GNNs. We further propose a multiscale location-based node feature and feature concatenation to provide a good initialization to stabilize the training of our GNN. Based on the experiments of synthetic graphs, we show that our proposed GraphClusNet obtained solutions with n-cut values up to &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathbf {65\times }$&lt;/tex-math&gt;&lt;/inline-formula&gt; closer to the global optimum. On FPGA circuit graphs, our proposed GraphClusNet outperformed the reported methods with up to 30% improvement in normalized mutual information for circuit graph clustering.},
  archive      = {J_TAI},
  author       = {Xuenong Hong and Tong Lin and Yiqiong Shi and Bah Hwee Gwee},
  doi          = {10.1109/TAI.2022.3198930},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1199-1213},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {GraphClusNet: A hierarchical graph neural network for recovered circuit netlist partitioning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Phrase-based affordance detection via cyclic bilateral
interaction. <em>TAI</em>, <em>4</em>(5), 1186–1198. (<a
href="https://doi.org/10.1109/TAI.2022.3199190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affordance detection, which refers to perceiving objects with potential action possibilities in images, is a challenging task since the possible affordance depends on the person&#39;s purpose in real-world application scenarios. The existing works mainly extract the inherent human–object dependencies from image/video to accommodate affordance properties that change dynamically. In this article, we explore to perceive affordances from a vision-language perspective, and consider the challenging phrase-based affordance detection task, i.e., given a set of phrases describing the potential actions, all the object regions in a scene with the same affordance should be detected. To this end, we propose a cyclic bilateral c onsistency enhancement network (CBCE-Net) to align language and vision features in a progressive manner. Specifically, the presented CBCE-Net consists of a mutual guided vision-language module that updates the common features of vision and language in a progressive manner, and a cyclic interaction module that facilitates the perception of possible interaction with objects in a cyclic manner. In addition, we extend the public purpose-driven affordance dataset (PAD) by annotating affordance categories with short phrases. The extensive contrastive experimental results demonstrate the superior performance of our method over nine typical methods from four relevant fields in terms of both objective metrics and visual quality.},
  archive      = {J_TAI},
  author       = {Liangsheng Lu and Wei Zhai and Hongchen Luo and Yu Kang and Yang Cao},
  doi          = {10.1109/TAI.2022.3199190},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1186-1198},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Phrase-based affordance detection via cyclic bilateral interaction},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-LSTM sequence modeling for on-the-fly fine-grained
sketch-based image retrieval. <em>TAI</em>, <em>4</em>(5), 1178–1185.
(<a href="https://doi.org/10.1109/TAI.2022.3188959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained sketch-based image retrieval (FG-SBIR) addresses the problem of retrieving a particular photo for a given query sketch. However, its widespread applicability is limited by the fact that it is difficult to draw a complete sketch, and the drawing process often takes more time than the text/tag method. On-the-fly FG-SBIR was proposed to address this problem, in which image retrieval is performed after each stroke. The aim is to retrieve the target photo using the least number of strokes. Each photo corresponds to a sketch drawing episode, in which a significant correlation exists between these incomplete sketches. This correlation will allow a more efficient learning embedding space for incomplete sketches, which is considered in this study. First, a triplet network, as used in the classical FG-SBIR framework, was designed to learn the joint embedding space shared between the photo and its corresponding complete sketch. Second, assuming strong time correlation, each sketch drawing episode is considered a sequence, and each incomplete sketch in the drawing episode is extracted as a feature vector. A learnable Bi-LSTM module and triplet loss function map the feature space of incomplete sketches obtained from the base model for efficient representation. In the experiments, we proposed more realistic challenges, and our method achieved superior early retrieval efficiency over the state-of-the-art baseline methods on two publicly available fine-grained sketch retrieval datasets.},
  archive      = {J_TAI},
  author       = {Yingge Liu and Dawei Dai and Xiaoyu Tang and Shuyin Xia and Guoyin Wang},
  doi          = {10.1109/TAI.2022.3188959},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1178-1185},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bi-LSTM sequence modeling for on-the-fly fine-grained sketch-based image retrieval},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SpinalNet: Deep neural network with gradual input.
<em>TAI</em>, <em>4</em>(5), 1165–1177. (<a
href="https://doi.org/10.1109/TAI.2022.3185179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved the state-of-the-art (SOTA) performance in numerous fields. However, DNNs need high computation times, and people always expect better performance in a lower computation. Therefore, we study the human somatosensory system and design a neural network (SpinalNet) to achieve higher accuracy with fewer computations. Hidden layers (HLs) in traditional NNs receive inputs in the previous layer, apply activation function, and then transfer the outcomes to the next layer. In the proposed SpinalNet, each layer is split into three splits: 1) input split, 2) intermediate split, and 3) output split. Input split of each layer receives a part of the inputs. The intermediate split of each layer receives outputs of the intermediate split of the previous layer and outputs of the input split of the current layer. The number of incoming weights becomes significantly lower than traditional DNNs. The SpinalNet can also be used as the fully connected or classification layer of DNN and supports both traditional learning and transfer learning. We observe significant error reductions with lower computational costs in most of the DNNs. Traditional learning on the VGG-5 network with SpinalNet classification layers provided the SOTA performance on QMNIST, Kuzushiji-MNIST, and EMNIST (Letters, Digits, and Balanced) datasets. Traditional learning with ImageNet pretrained initial weights and SpinalNet classification layers provided the SOTA performance on STL-10, Fruits 360, Bird225, and Caltech-101 datasets. The scripts of the proposed SpinalNet training are available at the following link: https://github.com/dipuk0506/SpinalNet .},
  archive      = {J_TAI},
  author       = {H M Dipu Kabir and Moloud Abdar and Abbas Khosravi and Seyed Mohammad Jafar Jalali and Amir F. Atiya and Saeid Nahavandi and Dipti Srinivasan},
  doi          = {10.1109/TAI.2022.3185179},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1165-1177},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {SpinalNet: Deep neural network with gradual input},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rényi state entropy maximization for exploration
acceleration in reinforcement learning. <em>TAI</em>, <em>4</em>(5),
1154–1164. (<a href="https://doi.org/10.1109/TAI.2022.3185180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most critical challenges in deep reinforcement learning is to maintain the long-term exploration capability of the agent. To tackle this problem, it has been recently proposed to provide intrinsic rewards for the agent to encourage exploration. However, most existing intrinsic reward-based methods proposed in the literature fail to provide sustainable exploration incentives, a problem known as vanishing rewards. In addition, these conventional methods incur complex models and additional memory in their learning procedures, resulting in high computational complexity and low robustness. In this work, a novel intrinsic reward module based on the Rényi entropy is proposed to provide high-quality intrinsic rewards. It is shown that the proposed method actually generalizes the existing state entropy maximization methods. In particular, a &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -nearest neighbor estimator is introduced for entropy estimation while a &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -value search method is designed to guarantee the estimation accuracy. Extensive simulation results demonstrate that the proposed Rényi entropy-based method can achieve higher performance as compared to existing schemes.},
  archive      = {J_TAI},
  author       = {Mingqi Yuan and Man-On Pun and Dong Wang},
  doi          = {10.1109/TAI.2022.3185180},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1154-1164},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Rényi state entropy maximization for exploration acceleration in reinforcement learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning multiagent options for tabular reinforcement
learning using factor graphs. <em>TAI</em>, <em>4</em>(5), 1141–1153.
(<a href="https://doi.org/10.1109/TAI.2022.3195818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covering option discovery has been developed to improve the exploration of reinforcement learning in single-agent scenarios, where only sparse reward signals are available. It aims to connect the most distant states identified through the Fiedler vector of the state transition graph. However, the approach cannot be directly extended to multiagent scenarios, since the joint state space grows exponentially with the number of agents, thus prohibiting efficient option computation. Existing research adopting options in multiagent scenarios still relies on single-agent algorithms and fails to directly discover joint options that can improve the connectivity of the joint state space. In this article, we propose a new algorithm to directly compute multiagent options with collaborative exploratory behaviors while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as the Kronecker product of individual agents&#39; state spaces, based on which we can directly estimate the Fiedler vector of the joint state space using the Laplacian spectrum of individual agents&#39; transition graphs. This decomposition enables us to efficiently construct multiagent joint options by encouraging agents to connect the subgoal joint states, which are corresponding to the minimum or maximum of the estimated joint Fiedler vector. Evaluation on multiagent collaborative tasks shows that our algorithm can successfully identify multiagent options and significantly outperforms prior works using single-agent options or no options, in terms of both faster exploration and higher cumulative rewards.},
  archive      = {J_TAI},
  author       = {Jiayu Chen and Jingdi Chen and Tian Lan and Vaneet Aggarwal},
  doi          = {10.1109/TAI.2022.3195818},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1141-1153},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning multiagent options for tabular reinforcement learning using factor graphs},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking logic minimization for tabular machine learning.
<em>TAI</em>, <em>4</em>(5), 1129–1140. (<a
href="https://doi.org/10.1109/TAI.2022.3224415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tabular datasets can be viewed as logic functions that can be simplified using two-level logic minimization to produce minimal logic formulas in disjunctive normal form, which in turn can be readily viewed as an explainable decision rule set for binary classification. However, there are two problems with using logic minimization for tabular machine learning. First, tabular datasets often contain overlapping examples that have different class labels, which have to be resolved before logic minimization can be applied since logic minimization assumes consistent logic functions. Second, even without inconsistencies, logic minimization alone generally produces complex models with poor generalization because it exactly fits all data points, which leads to detrimental overfitting . How best to remove training instances to eliminate inconsistencies and overfitting is highly nontrivial. In this article, we propose a novel statistical framework for removing these training samples so that logic minimization can become an effective approach to tabular machine learning. Using the proposed approach, we are able to obtain comparable performance as gradient boosted and ensemble decision trees, which have been the winning hypothesis classes in tabular learning competitions, but with human-understandable explanations in the form of decision rules. To the best of authors&#39; knowledge, neither logic minimization nor explainable decision rule methods have been able to achieve the state-of-the-art performance before in tabular learning problems.},
  archive      = {J_TAI},
  author       = {Litao Qiao and Weijia Wang and Sanjoy Dasgupta and Bill Lin},
  doi          = {10.1109/TAI.2022.3224415},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1129-1140},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Rethinking logic minimization for tabular machine learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Replication robust payoff allocation in submodular
cooperative games. <em>TAI</em>, <em>4</em>(5), 1114–1128. (<a
href="https://doi.org/10.1109/TAI.2022.3195686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Submodular functions have been a powerful mathematical model for a wide range of real-world applications. Recently, submodular functions are becoming increasingly important in machine learning (ML) for modeling notions such as information and redundancy among entities such as data and features. Among these applications, a key question is payoff allocation, i.e., how to evaluate the importance of each entity toward a collective objective? To this end, classic solution concepts from cooperative game theory offer principled approaches to payoff allocation. However, despite the extensive body of game-theoretic literature, payoff allocation in submodular games is relatively under-researched. In particular, an important notion that arises in the emerging submodular applications is redundancy, which may occur from various sources such as abundant data or malicious manipulations, where a player replicates its resource and acts under multiple identities. Though many game-theoretic solution concepts can be directly used in submodular games, naively applying them for payoff allocation in these settings may incur robustness issues against replication. In this article, we systematically study the replication manipulation in submodular games and investigate replication robustness , a metric that quantitatively measures the robustness of solution concepts against replication. Using this metric, we present conditions which theoretically characterise the robustness of semivalues, a wide family of solution concepts including the Shapley and Banzhaf value. Moreover, we empirically validate our theoretical results on an emerging submodular ML application—ML data markets.},
  archive      = {J_TAI},
  author       = {Dongge Han and Michael Wooldridge and Alex Rogers and Olga Ohrimenko and Sebastian Tschiatschek},
  doi          = {10.1109/TAI.2022.3195686},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1114-1128},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Replication robust payoff allocation in submodular cooperative games},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Residual tensor train: A quantum-inspired approach for
learning multiple multilinear correlations. <em>TAI</em>, <em>4</em>(5),
1101–1113. (<a href="https://doi.org/10.1109/TAI.2022.3194132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {States of quantum many-body systems are defined in a high-dimensional Hilbert space, where rich and complex interactions among subsystems can be modeled. In machine learning, complex multiple multilinear correlations may also exist within input features. In this article, we present a quantum-inspired multilinear model, named residual tensor train (ResTT), to capture the multiple multilinear correlations of features, from low to high orders, within a single model. ResTT is able to build a robust decision boundary in a high-dimensional space for solving fitting and classification tasks. In particular, we prove that the fully connected layer and the Volterra series can be taken as special cases of ResTT. Furthermore, we derive the rule for weight initialization that stabilizes the training of ResTT based on a mean-field analysis. We prove that such a rule is much more relaxed than that of tensor train (TT), which means that ResTT can easily address the vanishing and exploding gradient problem that exists in the existing TT models. Numerical experiments demonstrate that ResTT outperforms the state-of-the-art tensor network and benchmark deep learning models on MNIST and Fashion-MNIST datasets. Moreover, ResTT achieves better performance than other statistical methods on two practical examples with limited data, which are known to have complex feature interactions.},
  archive      = {J_TAI},
  author       = {Yiwei Chen and Yu Pan and Daoyi Dong},
  doi          = {10.1109/TAI.2022.3194132},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1101-1113},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Residual tensor train: A quantum-inspired approach for learning multiple multilinear correlations},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A mutual information-based assessment of reverse engineering
on rewards of reinforcement learning. <em>TAI</em>, <em>4</em>(5),
1089–1100. (<a href="https://doi.org/10.1109/TAI.2022.3190811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rewards are critical hyperparameters in reinforcement learning (RL), since in most cases different reward values will lead to greatly different performance. Due to their commercial value, RL rewards become the target of reverse engineering by the inverse reinforcement learning (IRL) algorithm family. Existing efforts typically utilize two metrics to measure the IRL performance: the expected value difference (EVD) and the mean reward loss (MRL). Unfortunately, in some cases, EVD and MRL can give completely opposite results, due to MRL focusing on whole state-space rewards, while EVD only considering partly sampled rewards. Such situation naturally rises to one fundamental question: whether current metrics and assessment are sufficient and accurate for more general use. Thus, in this article, based on the metric called normalized mutual information of reward clusters (C-NMI), we propose a novel IRL assessment; we aim to fill this research gap by considering a middle-granularity state space between the entire state space and the specific sampling space. We utilize the agglomerative nesting algorithm (AGNES) to control dynamical C-NMI computing via a fourth-order tensor model with injected manipulated trajectories. With such a model, we can uniformly capture different-dimension values of MRL, EVD, and C-NMI, and perform more comprehensive and accurate assessment and analyses. Extensive experiments on several mainstream IRLs are experimented in object world, hence revealing that the assessing accuracy of our method increases 110.13% and 116.59%, respectively, when compared with the EVD and MRL. Meanwhile, C-NMI is more robust than EVD and MRL under different demonstrations.},
  archive      = {J_TAI},
  author       = {Tong Chen and Jiqiang Liu and Thar Baker and Yalun Wu and Yingxiao Xiang and Yike Li and Wenjia Niu and Endong Tong and Albert Y. Zomaya},
  doi          = {10.1109/TAI.2022.3190811},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1089-1100},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A mutual information-based assessment of reverse engineering on rewards of reinforcement learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evasion generative adversarial network for low data regimes.
<em>TAI</em>, <em>4</em>(5), 1076–1088. (<a
href="https://doi.org/10.1109/TAI.2022.3196283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A myriad of recent literary works have leveraged generative adversarial networks (GANs) to generate unseen evasion samples. The purpose is to annex the generated data with the original train set for adversarial training to improve the detection performance of machine learning (ML) classifiers. The quality of generated adversarial samples relies on the adequacy of training data samples. However, in low data regimes like medical diagnostic imaging and cybersecurity, the anomaly samples are scarce in number. This paper proposes a novel GAN design called evasion generative adversarial network (EVAGAN) that is more suitable for low data regime problems that use oversampling for detection improvement of ML classifiers. EVAGAN not only can generate evasion samples but its discriminator can act as an evasion-aware classifier. We have considered auxiliary classifier GAN (ACGAN) as a benchmark to evaluate the performance of EVAGAN on cybersecurity (ISCX-2014, CIC-2017, and CIC2018) botnet and computer vision (MNIST) datasets. We demonstrate that EVAGAN outperforms ACGAN for unbalanced datasets with respect to detection performance, training stability, and time complexity. EVAGAN&#39;s generator quickly learns to generate the low sample class and hardens its discriminator simultaneously. In contrast to ML classifiers that require security hardening after being adversarially trained by GAN-generated data, EVAGAN renders it needless. The experimental analysis proves that EVAGAN is an efficient evasion hardened model for low data regimes for the selected cybersecurity and computer vision datasets.},
  archive      = {J_TAI},
  author       = {Rizwan Hamid Randhawa and Nauman Aslam and Mohammad Alauthman and Husnain Rafiq},
  doi          = {10.1109/TAI.2022.3196283},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1076-1088},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Evasion generative adversarial network for low data regimes},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel unsupervised learning architecture for exposure-based
classification and enhancement. <em>TAI</em>, <em>4</em>(5), 1064–1075.
(<a href="https://doi.org/10.1109/TAI.2022.3190240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous imaging applications are affected by the poor quality of images caused by poor illuminating conditions, contrast degradation, and unwanted noise. These effects create noticeable artifacts in an indeterministic selective manner, where some parts of the image are modified, and some parts of the image are uninfluenced. Thus, the classification of an image into various sections and, then, segmentwise application of imaging algorithms are a preferable solution. This article focuses on classifying an image into three categories as under–well–over exposed regions. This article introduces the concept of multilevel superpixel-based classification. Superpixel stores the local integrity and color similarity of an image; hence, an image is initially classified into an experimentally predetermined number of superpixels. Then, a novel algorithm depending upon the superpixel contrast, entropy, and statistical distribution of illumination classifies it into an under–well–over exposed region. Then, with an increased number of superpixels, we reiterate the whole process. The regions classified into the same category in both iterations perform as the training datasets for the support-vector-machine (SVM) classifier. Finally, the trained SVM classifies the ambiguous regions obtained from multilevel superpixel classification. Both qualitative and visual results show the superior performance of the proposed method over the state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Mohit Kumar and Ashish Kumar Bhandari},
  doi          = {10.1109/TAI.2022.3190240},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1064-1075},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Novel unsupervised learning architecture for exposure-based classification and enhancement},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A secure and efficient biometric template protection scheme
for palmprint recognition system. <em>TAI</em>, <em>4</em>(5),
1051–1063. (<a href="https://doi.org/10.1109/TAI.2022.3188596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy and security of a biometric system are the two sides of a coin. A biometric system must be simple, flexible, efficient, and secure enough from unauthorized access. Concerning these requirements, this article presents an enhanced palmprint recognition system with superior performance and improved biometric template protection schemes. This method comprises five components: image preprocessing, feature extraction, cancelable biometrics, classification, and Bio-Cryptosystem. The palm texture region is extracted from the input hand image during preprocessing. A feature representation technique is then used to extract discriminant features from the palmprint texture patterns. The extracted features are then analyzed using an information coding scheme to compute a user-specific token. Here, the user-specific tokens along with the original palmprint features are used to calculate the feature vectors for the cancelable biometric system. A novel Bio-Cryptosystem has been proposed to protect templates with high security where there is no need to remember the users’ tokens. Cancelable feature vectors are kept online in encrypted form using the proposed Bio-Cryptosystem, which is used to identify or verify the subjects after decryption. The original feature vectors are kept offline to prevent these from external attacks and misuse. Finally, the multiclass linear support vector machine has been used for user classification. For experimental purposes, two benchmark hand image databases: BOSPHORUS and CASIA-Palmprint, are employed, and the proposed system has achieved nearly 100% correct recognition rate in cancelable domain for both the employed databases. Finally, the performance comparison in terms of both verification and identification for the palmprint and the cancelable palmprint recognition systems with the existing state-of-the-art methods shows the superiority of the proposed system.},
  archive      = {J_TAI},
  author       = {Alamgir Sardar and Saiyed Umer and Ranjeet Kumar Rout and Muhammad Khurram Khan},
  doi          = {10.1109/TAI.2022.3188596},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1051-1063},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A secure and efficient biometric template protection scheme for palmprint recognition system},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FastSlowMo: Federated learning with combined worker and
aggregator momenta. <em>TAI</em>, <em>4</em>(5), 1041–1050. (<a
href="https://doi.org/10.1109/TAI.2022.3187678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose and analyze FastSlowMo, a combined Nesterov accelerated gradient (NAG) style worker momentum and aggregator momentum algorithm for federated learning (FL). Existing NAG momentum-based FL algorithms depend on either worker momentum only or aggregator momentum only, which may be inefficient due to infrequent usage of momentum, data heterogeneity, and out-of-date momentum. FastSlowMo combines the advantages of worker momentum and aggregator momentum to address these issues. We then provide mathematical proof for the convergence of FastSlowMo. Finally, extensive experiments based on real-world datasets and trace-driven simulation are conducted, verifying that FastSlowMo decreases the total training time by 3–61%, and outperforms existing mainstream benchmarks under a wide range of settings.},
  archive      = {J_TAI},
  author       = {Zhengjie Yang and Sen Fu and Wei Bao and Dong Yuan and Albert Y. Zomaya},
  doi          = {10.1109/TAI.2022.3187678},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1041-1050},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FastSlowMo: Federated learning with combined worker and aggregator momenta},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature fusion based adversarial example detection against
second-round adversarial attacks. <em>TAI</em>, <em>4</em>(5),
1029–1040. (<a href="https://doi.org/10.1109/TAI.2022.3190816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) achieve remarkable performances in various areas. However, adversarial examples threaten their security. They are designed to mislead CNNs to output incorrect results. Many methods are proposed to detect adversarial examples. Unfortunately, most detection-based defense methods are vulnerable to second-round adversarial attacks, which can simultaneously deceive the base model and the detector. To resist such second-round adversarial attacks, handcrafted steganalysis features are introduced to detect adversarial examples, while such a method receives low accuracy at detecting sparse perturbations. In this article, we propose to combine handcrafted features with deep features via a fusion scheme to increase the detection accuracy and defend against second-round adversarial attacks. To avoid deep features being overwhelmed by high-dimensional handcrafted features, we propose an expansion-then-reduction process to compress the dimensionality of handcrafted features. Experimental results show that the proposed model outperforms the state-of-the-art adversarial example detection methods and remains robust under various second-round adversarial attacks.},
  archive      = {J_TAI},
  author       = {Chuan Qin and Yuefeng Chen and Kejiang Chen and Xiaoyi Dong and Weiming Zhang and Xiaofeng Mao and Yuan He and Nenghai Yu},
  doi          = {10.1109/TAI.2022.3190816},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1029-1040},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Feature fusion based adversarial example detection against second-round adversarial attacks},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A federated dictionary learning method for process
monitoring with industrial applications. <em>TAI</em>, <em>4</em>(5),
1017–1028. (<a href="https://doi.org/10.1109/TAI.2022.3152458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is an indispensable technology in the Industry 4.0 era. The data are stored at the edge and transmission is prohibited due to concerns about data privacy. Thus, the traditional centralized data-driven modeling methods will face many difficulties and can hardly be used in practice. Federated learning aims to let local nodes learn a global model cooperatively while keeping their data localized. Motivated by the pioneering framework of federated learning, a federated dictionary learning method is proposed for process monitoring. In detail, local nodes train local data by the K-SVD method and learn local dictionaries. Then, local dictionaries, instead of local data, are transferred to the fusion center for calculation of the global dictionary. In order to guarantee an optimal global dictionary, a novel federated dictionary average strategy is introduced. Finally, the reconstruction errors of local data and the control limit are calculated based on the global dictionary for process monitoring. The performance of the proposed method is verified on a numerical simulation, the continuous stirred tank heater benchmark process, and the industrial aluminum electrolysis process. Process monitoring can help industrial enterprises monitor the operating status of the industrial process in real time based on the collected process data. However, industrial data are stored at the edge and enterprises pay more attention to the privacy of industrial data. The prohibition of external transmission of raw data leads to challenges for traditional centralized process monitoring methods. The federated dictionary learning method proposed in this article can have the good process monitoring performance under the premise of protecting data privacy, which is superior to some state-of-the-art methods. For example, in the aluminum electrolysis industrial process, the false alarm rate is only 1.91%, and the false discovery rate is 86.23%. The method has high data security and low model complexity, which can be quickly and conveniently promoted in other industrial processes, helping enterprises to ensure the safe and stable operation of industrial processes.},
  archive      = {J_TAI},
  author       = {Keke Huang and Xinyi Liu and Fanbiao Li and Chunhua Yang and Okyay Kaynak and Tingwen Huang},
  doi          = {10.1109/TAI.2022.3152458},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1017-1028},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A federated dictionary learning method for process monitoring with industrial applications},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiparticipant federated feature selection algorithm with
particle swarm optimization for imbalanced data under privacy
protection. <em>TAI</em>, <em>4</em>(5), 1002–1016. (<a
href="https://doi.org/10.1109/TAI.2022.3145333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, data describing the same learning task may be distributed in different institutions (called participants), and these participants cannot share their own data due to the need of privacy protection. How to select an optimal feature subset on the basis of ensuring the data privacy of each participant is an important and challenging topic. At the same time, data on part participants show imbalanced or even miss some classes, further increasing the difficulty of solving this kind of problem. In view of this, by introducing a trusted third party to process and integrate optimal information from participants, this article proposes a multiparticipant federated evolutionary feature selection algorithm for imbalanced data under privacy protection. First, a multilevel joint sample filling strategy with multiple participants (called sampling-rough selection-fine tuning strategy) is proposed to fill imbalanced or empty classes on each participant while ensuring data privacy. Then, a federated evolutionary feature selection method based on particle swarm optimization (PSO) is proposed by periodically sharing the optimal feature subsets obtained by PSOs on participants. Finally, the proposed algorithm is applied to 15 test datasets, and compared with several typical imbalanced feature selection algorithms and two kinds of ensemble feature selection algorithms. The experimental results show that the proposed algorithm can significantly promote the ability of each participant on processing imbalanced datasets, and improve the classification accuracy of obtained feature subsets while protecting the participants’ data privacy.},
  archive      = {J_TAI},
  author       = {Ying Hu and Yong Zhang and Dunwei Gong and Xiaoyan Sun},
  doi          = {10.1109/TAI.2022.3145333},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1002-1016},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiparticipant federated feature selection algorithm with particle swarm optimization for imbalanced data under privacy protection},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PIVODL: Privacy-preserving vertical federated learning over
distributed labels. <em>TAI</em>, <em>4</em>(5), 988–1001. (<a
href="https://doi.org/10.1109/TAI.2021.3139055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging privacy preserving machine learning protocol that allows multiple devices to collaboratively train a shared global model without revealing their private local data. Nonparametric models like gradient boosting decision trees (GBDTs) have been commonly used in FL for vertically partitioned data. However, all these studies assume that all the data labels are stored on only one client, which may be unrealistic for real-world applications. Therefore, in this article, we propose a secure vertical FL framework, named privacy-preserving vertical federated learning system over distributed labels (PIVODL), to train GBDTs with data labels distributed on multiple devices. Both homomorphic encryption and differential privacy are adopted to prevent label information from being leaked through transmitted gradients and leaf values. Our experimental results show that both information leakage and model performance degradation of the proposed PIVODL are negligible. Impact Statement—Federated learning is a distributed machine learning framework proposed for privacy preservation. Most federated learning algorithms work on horizontally partitioned data, with only a few exceptions considering vertically partitioned data that is widely seen in the real world. However, existing vertical federated learning makes an unrealistic assumption that data labels are distributed on only one device and no research has been reported so far that considers data labels distributed on multiple client devices. The PIVODL framework reported in this article allows us to build a secure vertical federated XGBoost system, in which the labels may distributed either on one device or on multiple devices, making it possible to apply federated learning to a wider range of real-world problems.},
  archive      = {J_TAI},
  author       = {Hangyu Zhu and Rui Wang and Yaochu Jin and Kaitai Liang},
  doi          = {10.1109/TAI.2021.3139055},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {988-1001},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {PIVODL: Privacy-preserving vertical federated learning over distributed labels},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Special issue on security and privacy in
machine learning. <em>TAI</em>, <em>4</em>(5), 986–987. (<a
href="https://doi.org/10.1109/TAI.2022.3223125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning plays an increasingly important role in the field of artificial intelligence, and obtains fantastic performance in various real-world applications, including image classification, computer vision, natural language processing, and recommendation systems, among many others. Meanwhile, in the era of Big Data, both security and privacy are of paramount importance. Machine learning vulnerabilities and privacy-preserving machine learning have attracted growing interest in the fields of artificial intelligence, information security, and data privacy.},
  archive      = {J_TAI},
  author       = {Wenjian Luo and Yaochu Jin and Catherine Huang},
  doi          = {10.1109/TAI.2022.3223125},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {986-987},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Guest editorial: Special issue on security and privacy in machine learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Directed acyclic graphs with tears. <em>TAI</em>,
<em>4</em>(4), 972–983. (<a
href="https://doi.org/10.1109/TAI.2022.3181115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian network is a frequently used method for fault detection and diagnosis in industrial processes. The basis of Bayesian network is structure learning which learns a directed acyclic graph (DAG) from data. However, the search space will scale super-exponentially with the increase of process variables, which makes the data-driven structure learning a challenging problem. To this end, the DAGs with NOTEAR methods are being well studied not only for their conversion of the discrete optimization into continuous optimization problem but also their compatibility with deep learning framework. Nevertheless, there still remain challenges for NOTEAR-based methods: 1) the infeasible solution results from the gradient descent based optimization paradigm; 2) the truncation operation to promise the learned graph acyclic. In this work, the reason for challenge 1) is analyzed theoretically, and a novel method named DAGs with tears method is proposed based on mix-integer programming to alleviate challenge 2). In addition, prior knowledge is able to incorporate into the new proposed method, making structure learning more practical and useful in industrial processes. Finally, a numerical example and an industrial example are adopted as case studies to demonstrate the superiority of the developed method.},
  archive      = {J_TAI},
  author       = {Zhichao Chen and Zhiqiang Ge},
  doi          = {10.1109/TAI.2022.3181115},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {972-983},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Directed acyclic graphs with tears},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Delve into neural activations: Toward understanding dying
neurons. <em>TAI</em>, <em>4</em>(4), 959–971. (<a
href="https://doi.org/10.1109/TAI.2022.3180272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theoretically, a deep neuron network with nonlinear activation is able to approximate any function, while empirically the performance of the model with different activations varies widely. In this work, we investigate the expressivity of the network from an activation perspective. In particular, we introduce a generalized activation region/pattern to describe the functional relationship of the model with an arbitrary activation function and illustrate its fundamental properties. We then propose a metric named pattern similarity to evaluate the practical expressivity of neuron networks regarding datasets based on the neuron level reaction toward the input. We find an undocumented dying neuron issue that the postactivation value of most neurons remain in the same region for data with different labels, implying that the expressivity of the network with certain activations is greatly constrained. For instance, around 80% of postactivation values of a well-trained Sigmoid net or Tanh net are clustered in the same region given any test sample. This means most of the neurons fail to provide any useful information in distinguishing the data with different labels, suggesting that the practical expressivity of those networks is far below the theoretical. By evaluating our metrics and the test accuracy of the model, we show that the seriousness of the dying neuron issue is highly related to the model performance. At last, we also discussed the cause of the dying neuron issue, providing an explanation of the model performance gap caused by the choice of activation.},
  archive      = {J_TAI},
  author       = {Ziping Jiang and Yunpeng Wang and Chang-Tsun Li and Plamen Angelov and Richard Jiang},
  doi          = {10.1109/TAI.2022.3180272},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {959-971},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Delve into neural activations: Toward understanding dying neurons},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards disturbance rejection in feature pyramid network.
<em>TAI</em>, <em>4</em>(4), 946–958. (<a
href="https://doi.org/10.1109/TAI.2022.3178062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiscale problem is one of the challenging topics in the object detection field. Among the current studies toward this problem, feature pyramid network (FPN) has been shown a superior performance. The core principle of FPN is a mapping for the detectable objects into hierarchy features. These features are designed with different size of receptive fields, which brings good scale invariance in the classification and localization subtasks. However, a disturbance of information dilution is generated by the assignment for the detectable objects into different features. This dilution disturbance is associated with the convolution or pooling operation during the hierarchy feature extraction process. Both of the imprecise localization information in the top features and the fuzzy classification information in the bottom ones limit the performance of FPN. Aiming at the disturbance rejection problem of information dilution, this article presents a feature compensation method. A cross compensation between the semantic information and the localization information in hierarchy features is used to enhance the vanilla FPN. Then, the evaluation metrics in an object detection task are examined. Not only the overall performance, but also the respective metrics for multiscale objects validate a considerable improvement of the FPN with a disturbance compensation mechanism. The experimental results demonstrate an effectiveness of the proposed method in this article.},
  archive      = {J_TAI},
  author       = {Xiaobo Hu and Wei Xu and Yi Gan and Jianbo Su and Jun Zhang},
  doi          = {10.1109/TAI.2022.3178062},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {946-958},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Towards disturbance rejection in feature pyramid network},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonoverlapping feature projection convolutional neural
network with differentiable loss function. <em>TAI</em>, <em>4</em>(4),
933–945. (<a href="https://doi.org/10.1109/TAI.2022.3177394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose NullSpaceNet, a novel network that maps from the pixel-level image to a joint-nullspace, as opposed to the traditional feature space. The features in the proposed learned joint-nullspace have clearer interpretation and are more separable. NullSpaceNet ensures that all input images that belong to the same class are collapsed into one point in this new joint-nullspace, and the input images of different classes are collapsed into different points with high separation margins. Moreover, a novel differentiable loss function is proposed that has a closed-form solution with no free parameters. NullSpaceNet architecture consists of two components; 1) a feature extractor backbone (i.e., the convolution and pooling layers), which is used to extract features from the input, and 2) a nullspace layer, which maps from the pixel-level image to the joint-nullspace. This novel architecture and formulation results in a significant reduction in the number of learnable parameters in the network. NullSpaceNet is architecture-agnostic, which means it can use any feature extractor as a backbone in its first component. NullSpaceNet exhibits superior performance when tested over four different datasets against VGG16, MobileNet-224, and MNASNET1-0. In general, NullSpaceNet needs only &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\text{1}\hbox{--}\hbox{30}\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; of the time it takes a traditional CNN to classify a batch of images, and with better accuracy of up to &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$+2.57\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; . Impact Statement– Convolutional neural networks (CNNs) have achieved excellent performance in most computer vision tasks. However, the current formulation of CNN lacks a clear interpretation of the learned features in the feature space. Moreover, most learnable parameters are located in the classifier component (i.e., the fully connected layers), which require extensive computations during training and inference. We propose a novel feature space called NullSpaceNet. We provide theoretical and experimental evidence in NullSpaceNet that the learned nullspace features are more discriminative than the feature space. Moreover, the new formulation significantly decreases the number of parameters up to 86% and with better accuracy by up to &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$+2.57\%$&lt;/tex-math&gt;&lt;/inline-formula&gt; . NullspaceNet sets a new line of architecture formulation research by improving the performance while decreasing the number of learnable parameters. Computer vision and deep learning communities will benefit from NullSpaceNet formulation, as it is architecture-agnostic.},
  archive      = {J_TAI},
  author       = {Mohamed H. Abdelpakey and Mohamed S. Shehata},
  doi          = {10.1109/TAI.2022.3177394},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {933-945},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Nonoverlapping feature projection convolutional neural network with differentiable loss function},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph dual adversarial network for hyperspectral image
classification. <em>TAI</em>, <em>4</em>(4), 922–932. (<a
href="https://doi.org/10.1109/TAI.2022.3177168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An end-to-end unsupervised domain adaptation method for hyperspectral image (HSI) classification based on a graph dual adversarial network is proposed in this article. First, in order to extract the domain-invariant features of the source and target domains, the rich spectral information and spatial position of HSI are used to construct a spectral–spatial nearest neighbor graph, which is input into the graph convolutional network. Then, a prototype adversarial strategy is proposed, which uses the labeled data of the source domain to reliably calculate the feature prototypes of different classes. Through the prototype adversarial strategy, the distances between different prototypes are appropriately extended, so that the clusters of different classes are far away from their respective decision boundaries, and the discriminability of features are also enhanced. The dual adversarial strategy is composed of the prototype adversarial strategy and the domain adversarial strategy. It is worth noting that the dual adversarial strategy does not require feature extractor and discriminator to work in turn, which can be implemented through a gradient reversal layer. Finally, on the basis of adapting the overall features of both the domains via the domain adversarial strategy, the source- and target-domain features are further adapted by minimizing the correlation alignment loss of each class of samples. Experimental results on two real HSI datasets of Botswana and Kennedy Space Center show the effectiveness of our proposed method.},
  archive      = {J_TAI},
  author       = {Yuhu Cheng and Yang Chen and Yi Kong and C. L. Philip Chen and Xuesong Wang},
  doi          = {10.1109/TAI.2022.3177168},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {922-932},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Graph dual adversarial network for hyperspectral image classification},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight convolutional-iConformer for sound event
detection. <em>TAI</em>, <em>4</em>(4), 910–921. (<a
href="https://doi.org/10.1109/TAI.2022.3173582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of a sound event detection (SED) system is no trivial task where one has to consider both audio tagging and temporal localization concurrently. Often model ensembling is adopted to increase the overall detection accuracy. However, this can result in a large system that may face deployment issues in a resource-constrained environment. Subsequently, strongly labeled data was found to improve the audio classification performance in sound-related domains; this may indicate that such data may be required for SED model development. However, such data will inevitably contain a certain level of noise. In order to reduce the number of parameters, we proposed a lightweight system that utilized an improved depthwise separable convolution and an improved conformer layer. This lightweight system is then trained using an extension of the binary cross-entropy loss which considers the reverse binary cross-entropy to combat the noise that may be present in the training data. Based on the proposed framework, our lightweight system can obtain an event-based F1-score of 52%, and the ensemble of four systems through posterior averaging can further improve the event-based F1-score to 53.5%. Such results indicate a minimum margin of 16% against the Detection and Classification of Acoustic Scenes and events (DCASE) 2020 challenge task 4 baseline system. By comparing against the nonensembled system of the first-place submission in DCASE 2020 challenge task 4, our nonensembled system can achieve a higher event-based F1-score of 6% with 75% fewer parameters. In terms of the performance of the ensembled system, our approach remains competitive against the ensembled system of the first-place submission in DCASE 2020 challenge task 4 and has a winning margin of 2.9% with 88% fewer parameters. Comparison with other state-of-the-art also indicates that our system performance is better despite using a lightweight system.},
  archive      = {J_TAI},
  author       = {Teck Kai Chan and Cheng Siong Chin},
  doi          = {10.1109/TAI.2022.3173582},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {910-921},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Lightweight convolutional-iConformer for sound event detection},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiview video-based 3-d hand pose estimation.
<em>TAI</em>, <em>4</em>(4), 896–909. (<a
href="https://doi.org/10.1109/TAI.2022.3195968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand pose estimation (HPE) can be used for a variety of human–computer interaction applications, such as gesture-based control for physical or virtual/augmented reality devices. Recent works have shown that videos or multiview images carry rich information regarding the hand, allowing for the development of more robust HPE systems. In this article, we present the multiview video-based three-dimensional (3-D) hand (MuViHand) dataset, consisting of multiview videos of the hand along with ground-truth 3-D pose labels. Our dataset includes more than 402 000 synthetic hand images available in 4560 videos. The videos have been simultaneously captured from six different angles with complex backgrounds and random levels of dynamic lighting. The data has been captured from ten distinct animated subjects using 12 cameras in a semicircle topology where six tracking cameras only focus on the hand and the other six fixed cameras capture the entire body. Next, we implement MuViHandNet, a neural pipeline consisting of image encoders for obtaining visual embeddings of the hand, recurrent learners to learn both temporal and angular sequential information, and graph networks with U-Net architectures to estimate the final 3-D pose information. We perform extensive experiments and show the challenging nature of this new dataset as well as the effectiveness of our proposed method. Ablation studies show the added value of each component in MuViHandNet, as well as the benefit of having temporal and sequential information in the dataset.},
  archive      = {J_TAI},
  author       = {Leyla Khaleghi and Alireza Sepas-Moghaddam and Joshua Marshall and Ali Etemad},
  doi          = {10.1109/TAI.2022.3195968},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {896-909},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiview video-based 3-D hand pose estimation},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated surface classification system using vibration
patterns—a case study with wheelchairs. <em>TAI</em>, <em>4</em>(4),
884–895. (<a href="https://doi.org/10.1109/TAI.2022.3190828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a barrier-free accessible path through the built environment is necessary for wheelchair users. Researchers have identified the effect of surface vibration on the health of wheelchair users and proposed various solutions to identify and annotate mobility barriers. However, the effects of surface-induced vibration on accessibility are yet to be investigated. In order to address this problem, we present the WheelShare system, which uses machine learning to identify surfaces and curbs from vibration captured by accelerometer and gyroscope sensors (both smartphone and wearable device). WheelShare allows users to share their collected data via crowdsourcing. Based on this surface-specific vibration, we recognize 32 different surfaces found across the built environment (in Austria, China, France, Germany, India, and the USA for 32 human subjects) with an accuracy of 97.5%. The recognized surfaces are then grouped into different categories based on their characteristics. Our novel contributions in this article include the following. 1) We have developed a machine-learning-enabled surface recognition system that uses sensor data to capture surface-induced vibration patterns. 2) We have tested the robustness of the system for devices with only accelerometer data (such as low-end smartphones) and achieved an accuracy of 91.6%. 3) We have deployed our prototype WheelShare system for real-time surface classification in new sites that are unseen in training and testing and successfully recognized the surface characteristics.},
  archive      = {J_TAI},
  author       = {Haoxiang Yu and Vaskar Raychoudhury and Snehanshu Saha and Janick Edinger and Roger O. Smith and Md Osman Gani},
  doi          = {10.1109/TAI.2022.3190828},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {884-895},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Automated surface classification system using vibration Patterns—A case study with wheelchairs},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Targeted high-utility itemset querying. <em>TAI</em>,
<em>4</em>(4), 871–883. (<a
href="https://doi.org/10.1109/TAI.2022.3171530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional high-utility itemset mining (HUIM) aims to determine all high-utility itemsets (HUIs) that satisfy the minimum utility threshold in transaction databases. However, in most applications, not all HUIs are interesting because only specific parts are required. Thus, targeted mining based on user preferences is more important than traditional mining tasks. This article is the first to propose a target-based HUIM problem and to provide a clear formulation of the targeted utility mining task in a quantitative transaction database. A tree-based algorithm known as T arget-based high- U tility ite M set querying (TargetUM) is proposed. The algorithm uses a lexicographic querying tree and three effective pruning strategies to improve the mining efficiency. We implemented experimental validation on several real and synthetic databases, and the results demonstrate that the performance of TargetUM is satisfactory, complete, and correct. Finally, owing to the lexicographic querying tree, the database no longer needs to be scanned repeatedly for multiple queries.},
  archive      = {J_TAI},
  author       = {Jinbao Miao and Shicheng Wan and Wensheng Gan and Jiayi Sun and Jiahui Chen},
  doi          = {10.1109/TAI.2022.3171530},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {871-883},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Targeted high-utility itemset querying},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantifying explainability of saliency methods in deep
neural networks with a synthetic dataset. <em>TAI</em>, <em>4</em>(4),
858–870. (<a href="https://doi.org/10.1109/TAI.2022.3228834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-hoc analysis is a popular category in eXplainable artificial intelligence (XAI) study. In particular, methods that generate heatmaps have been used to explain the deep neural network (DNN), a black-box model. Heatmaps can be appealing due to the intuitive and visual ways to understand them but assessing their qualities might not be straightforward. Different ways to assess heatmaps&#39; quality have their own merits and shortcomings. This article introduces a synthetic dataset that can be generated adhoc along with the ground-truth heatmaps for more objective quantitative assessment. Each sample data is an image of a cell with easily recognized features that are distinguished from localization ground-truth mask, hence facilitating a more transparent assessment of different XAI methods. Comparison and recommendations are made, shortcomings are clarified along with suggestions for future research directions to handle the finer details of select posthoc analysis methods. Furthermore, mabCAM is introduced as the heatmap generation method compatible with our ground-truth heatmaps. The framework is easily generalizable and uses only standard deep learning components.},
  archive      = {J_TAI},
  author       = {Erico Tjoa and Cuntai Guan},
  doi          = {10.1109/TAI.2022.3228834},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {858-870},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Quantifying explainability of saliency methods in deep neural networks with a synthetic dataset},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). User-centric explainability in healthcare: A knowledge-level
perspective of informed machine learning. <em>TAI</em>, <em>4</em>(4),
840–857. (<a href="https://doi.org/10.1109/TAI.2022.3227225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explaining increasingly complex machine learning will remain crucial to cope with risks, regulations, responsibilities, and human support in healthcare. However, extant explainable systems mostly provide explanations that mismatch clinical users’ conceptions and fail their expectations to leverage validated and clinically relevant information. A key to more user-centric and satisfying explanations can be seen in combining data-driven and knowledge-based systems, i.e., to utilize prior knowledge jointly with the patterns learned from data. We conduct a structured review of knowledge-informed machine learning in healthcare. In this article, we build on a framework to characterize user knowledge and prior knowledge embodied in explanations. Specifically, we explicate the types and contexts of knowledge to examine the fit between knowledge-informed approaches and users. Our results highlight that knowledge-informed machine learning is a promising paradigm to enrich former data-driven systems, yielding explanations that can increase formal understanding, convey useful medical knowledge, and are more intuitive. Although complying with medical conception, it still needs to be investigated whether knowledge-informed explanations increase medical user acceptance and trust in clinical machine learning-based information systems.},
  archive      = {J_TAI},
  author       = {Luis Oberste and Armin Heinzl},
  doi          = {10.1109/TAI.2022.3227225},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {840-857},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {User-centric explainability in healthcare: A knowledge-level perspective of informed machine learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The different faces of AI ethics across the world: A
principle-to-practice gap analysis. <em>TAI</em>, <em>4</em>(4),
820–839. (<a href="https://doi.org/10.1109/TAI.2022.3225132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) is transforming our daily life with many applications in healthcare, space exploration, banking, and finance. This rapid progress in AI has brought increasing attention to the potential impacts of AI technologies on society, with ethically questionable consequences. In recent years, several ethical principles have been released by governments, national organizations, and international organizations. These principles outline high-level precepts to guide the ethical development, deployment, and governance of AI. However, the abstract nature, diversity, and context-dependence of these principles make them difficult to implement and operationalize, resulting in gaps between principles and their execution. Most recent work analyzed and summarized existing AI principles and guidelines but did not provide findings on principle-to-practice gaps nor how to mitigate them. These findings are particularly important to ensure that AI practical guidances are aligned with ethical principles and values. In this article, we provide a contextual and global evaluation of current ethical AI principles for all continents, with the aim to identify potential principle characteristics tailored to specific countries or applicable across countries. Next, we analyze the current level of AI readiness and current practical guidances of ethical AI principles in different countries, to identify gaps in the practical guidance of AI principles and their causes. Finally, we propose recommendations to mitigate the principle-to-practice gaps.},
  archive      = {J_TAI},
  author       = {Lionel Nganyewou Tidjon and Foutse Khomh},
  doi          = {10.1109/TAI.2022.3225132},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {820-839},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {The different faces of AI ethics across the world: A principle-to-practice gap analysis},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An overview of artificial intelligence ethics. <em>TAI</em>,
<em>4</em>(4), 799–819. (<a
href="https://doi.org/10.1109/TAI.2022.3194503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has profoundly changed and will continue to change our lives. AI is being applied in more and more fields and scenarios such as autonomous driving, medical care, media, finance, industrial robots, and internet services. The widespread application of AI and its deep integration with the economy and society have improved efficiency and produced benefits. At the same time, it will inevitably impact the existing social order and raise ethical concerns. Ethical issues, such as privacy leakage, discrimination, unemployment, and security risks, brought about by AI systems have caused great trouble to people. Therefore, AI ethics, which is a field related to the study of ethical issues in AI, has become not only an important research topic in academia, but also an important topic of common concern for individuals, organizations, countries, and society. This article will give a comprehensive overview of this field by summarizing and analyzing the ethical risks and issues raised by AI, ethical guidelines and principles issued by different organizations, approaches for addressing ethical issues in AI, and methods for evaluating the ethics of AI. Additionally, challenges in implementing ethics in AI and some future perspectives are pointed out. We hope our work will provide a systematic and comprehensive overview of AI ethics for researchers and practitioners in this field, especially the beginners of this research discipline.},
  archive      = {J_TAI},
  author       = {Changwu Huang and Zeqi Zhang and Bifei Mao and Xin Yao},
  doi          = {10.1109/TAI.2022.3194503},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {799-819},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An overview of artificial intelligence ethics},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncovering incentives for implementing AI governance
programs: Evidence from the field. <em>TAI</em>, <em>4</em>(4), 792–798.
(<a href="https://doi.org/10.1109/TAI.2022.3171748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods and applications of artificial intelligence (AI) are increasingly impacting society. To manage this technology&#39;s positive and negative effects, stakeholders (governments, firms, and nonprofits) are developing programs such as principles, guidelines, codes, certifications, and standards, among others, as an alternative to regulation or hard law. The literature classifies these programs as soft law, defined as any initiative that creates substantive expectations that are not directly enforced by government. Although these programs are incredibly flexible and can adapt to the constantly changing nature of emerging technologies, their main weakness is their voluntary nature. In other words, organizations are responsible for deciding if the implementation of soft law is in their best interests. The objective of this article is to inform stakeholders of the three incentives that motivate the enforcement or implementation of soft law and illustrate them with examples taken from an analysis of over 600 AI-centric programs.},
  archive      = {J_TAI},
  author       = {Carlos Ignacio Gutierrez},
  doi          = {10.1109/TAI.2022.3171748},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {792-798},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Uncovering incentives for implementing AI governance programs: Evidence from the field},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Building trustworthy AI solutions: A case for practical
solutions for small businesses. <em>TAI</em>, <em>4</em>(4), 778–791.
(<a href="https://doi.org/10.1109/TAI.2021.3137091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building trustworthy artificial intelligence (AI) solutions, whether in academia or industry, must take into consideration a number of dimensions including legal, social, ethical, public opinion, and environmental aspects. A plethora of guidelines, principles, and toolkits have been published globally, but have seen limited grassroots implementation, especially among small- and medium-sized enterprises (SMEs), mainly due to the lack of knowledge, skills, and resources. In this article, we report on qualitative SME consultations over two events to establish their understanding of both data and AI ethical principles and to identify the key barriers SMEs face in their adoption of ethical AI approaches. We then use independent experts to review and code 77 published toolkits designed to build and support ethical and responsible AI practices, based on 33 evaluation criteria. The toolkits were evaluated considering their scope to address the identified SME barriers to adoption, human-centric AI principles, AI life cycle stages, and key themes around responsible AI and practical usability. Toolkits were ranked on the basis of criteria coverage and expert intercoder agreement. Results show that there is not a one-size-fits-all toolkit that addresses all criteria suitable for SMEs. Our findings show few exemplars of practical application, little guidance on how to use/apply the toolkits, and very low uptake by SMEs. Our analysis provides a mechanism for SMEs to select their own toolkits based on their current capacity, resources, and ethical awareness levels – focusing initially at the conceptualization stage of the AI life cycle and then extending throughout.},
  archive      = {J_TAI},
  author       = {Keeley Crockett and Edwin Colyer and Luciano Gerber and Annabel Latham},
  doi          = {10.1109/TAI.2021.3137091},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {778-791},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Building trustworthy AI solutions: A case for practical solutions for small businesses},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable machine learning for COVID-19: An empirical
study on severity prediction task. <em>TAI</em>, <em>4</em>(4), 764–777.
(<a href="https://doi.org/10.1109/TAI.2021.3092698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The black-box nature of machine learning models hinders the deployment of some high-accuracy medical diagnosis algorithms. It is risky to put one’s life in the hands of models that medical researchers do not fully understand or trust. However, through model interpretation, black-box models can promptly reveal significant biomarkers that medical practitioners may have overlooked due to the surge of infected patients in the COVID-19 pandemic. This research leverages a database of 92 patients with confirmed SARS-CoV-2 laboratory tests between 18th January 2020 and 5th March 2020, in Zhuhai, China, to identify biomarkers indicative of infection severity prediction. Through the interpretation of four machine learning models, decision tree, random forests, gradient boosted trees, and neural networks using permutation feature importance, partial dependence plot, individual conditional expectation, accumulated local effects, local interpretable model-agnostic explanations, and Shapley additive explanation, we identify an increase in N-terminal pro-brain natriuretic peptide, C-reaction protein, and lactic dehydrogenase, a decrease in lymphocyte is associated with severe infection and an increased risk of death, which is consistent with recent medical research on COVID-19 and other research using dedicated models. We further validate our methods on a large open dataset with 5644 confirmed patients from the Hospital Israelita Albert Einstein, at São Paulo, Brazil from Kaggle, and unveil leukocytes, eosinophils, and platelets as three indicative biomarkers for COVID-19.},
  archive      = {J_TAI},
  author       = {Han Wu and Wenjie Ruan and Jiangtao Wang and Dingchang Zheng and Bei Liu and Yayuan Geng and Xiangfei Chai and Jian Chen and Kunwei Li and Shaolin Li and Sumi Helal},
  doi          = {10.1109/TAI.2021.3092698},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {764-777},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Interpretable machine learning for COVID-19: An empirical study on severity prediction task},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning based truck-and-drone coordinated
delivery. <em>TAI</em>, <em>4</em>(4), 754–763. (<a
href="https://doi.org/10.1109/TAI.2021.3087666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 has brought a great challenge to the supply of daily necessities and medical items for home-quarantined people. Considering the unmanned operation, agility, and use of clean energy of drones, we propose a novel truck-and-drone coordinated delivery system that does not require direct human contact during the delivery process. All final deliveries are completed by the drone while the truck acts as a movable charging station and a carrier, so that the contagion risks are reduced. Moreover, we spilt the whole delivery problem into the customer-clustering problem and the routing problems of both the truck and the drone. An encoder–decoder framework combined with reinforcement learning is created to solve the routing problems without handcrafted designed heuristics. We design the different problem contexts specific to the truck routing problem and the drone routing problem. The experimental results show that the proposed approach has good generality and can consequently be applied to problems of different scales with high time efficiency.},
  archive      = {J_TAI},
  author       = {Guohua Wu and Mingfeng Fan and Jianmai Shi and Yanghe Feng},
  doi          = {10.1109/TAI.2021.3087666},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {754-763},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Reinforcement learning based truck-and-drone coordinated delivery},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Customized impression prediction from radiology reports
using BERT and LSTMs. <em>TAI</em>, <em>4</em>(4), 744–753. (<a
href="https://doi.org/10.1109/TAI.2021.3086435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical language processing has become an attractive field with the improvements of deep learning applications and the abundance of large unstructured narratives in the healthcare records. The capability to extract unstructured information from raw text to provide actionable information for healthcare personnel plays a vital role in healthcare workflows. In this article, we introduce a deep learning approach to automate the generation of radiology impressions by analyzing radiology findings and patient background information of each examination. Since the impression section of a radiology report is an essential conclusion, any errors can prove to be detrimental. Thus, we developed a deep learning system to prevent important clinical findings from being overlooked by using almost 1 million de-identified radiology reports obtained from the University of Chicago Medicine over the last 12 years. We propose to automate the generation of radiology reports by incorporating sequence-to-sequence neural network models with the power of bidirectional encoder representations from transformers (BERT). We tested our model in a real-time experimental setup with radiologists in a top tier academic institution and statistically validated the performance by using ROUGE metrics. Clinical validations have shown that 76% of our predictions are at least as accurate as human-generated impressions by radiologists. Furthermore, statistical validation metrics demonstrated higher ROUGE scores compared to previously published studies over two different test sets.},
  archive      = {J_TAI},
  author       = {Batuhan Gundogdu and Utku Pamuksuz and Jonathan H. Chung and Jessica M. Telleria and Peng Liu and Farrukh Khan and Paul J. Chang},
  doi          = {10.1109/TAI.2021.3086435},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {744-753},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Customized impression prediction from radiology reports using BERT and LSTMs},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of observer-based control with residual generator
using actor–critic reinforcement learning. <em>TAI</em>, <em>4</em>(4),
734–743. (<a href="https://doi.org/10.1109/TAI.2022.3215671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observer-based control has been widely used in mechatronic systems. In this article, an observer-based control integrated with a residual generator is designed in the framework of actor–critic reinforcement learning, which has been applied to robot systems. In the learning process, a critic function is constructed by the state of the original system and its twin system. Thus, the system parameters and control gain can be obtained simultaneously through trial-and-error learning. To achieve system stability and reliability, the observer-based control with the residual generator is designed based on the learned results. The performance and effectiveness of the proposed scheme are demonstrated through a robot test rig. After a short period of learning, the robot is controlled only with the measured joint angle, and meanwhile, the residual generator can be used for fault detection to improve the system reliability.},
  archive      = {J_TAI},
  author       = {Lu Qian and Xingwei Zhao and Peifeng Liu and Zhenwei Zhang and Yaqiong Lv},
  doi          = {10.1109/TAI.2022.3215671},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {734-743},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Design of observer-based control with residual generator using Actor–Critic reinforcement learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semisupervised soft-sensor of just-in-time learning with
structure entropy clustering and applications for industrial processes
monitoring. <em>TAI</em>, <em>4</em>(4), 722–733. (<a
href="https://doi.org/10.1109/TAI.2022.3217028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To monitor industrial processes properly, soft-sensors are widely used to predict significant but difficult-to-measure quality variables. However, the prediction performances of traditional data-driven soft-sensors are usually unacceptable once suffering from high-nonlinear, high-dimension, and imblance data issues. Therefore, a semisupervised soft-sensor, which is learned by a just-in-time method with structure entropy clustering (SS-JITL-SEC), is proposed aiming to improve prediction performance with a simpler way. Inspired by a divide and conquer strategy, a novel SEC method is proposed to achieve several clusters and then to translate the highly complex and nonlinear modeling problems into simple and linear ones. Moreover, the training dataset is extended through a mixed SS labeling approach. Finally, dissimilarity-based JITL works together with the resulting clustering subdatasets to formulate a local adaptive prediction model. Two datasets from different types of wastewater treatment plants are used to verify the effectiveness of the proposed soft-sensor. The results show that the SS-JITL-SEC soft-sensor can achieve better prediction performance than other standard counterparts, and even for effective process monitoring with the resulted residuals.},
  archive      = {J_TAI},
  author       = {Dong Li and Yiqi Liu and Daoping Huang and Chong Xu},
  doi          = {10.1109/TAI.2022.3217028},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {722-733},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A semisupervised soft-sensor of just-in-time learning with structure entropy clustering and applications for industrial processes monitoring},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Condition-based maintenance for performance degradation
under nonperiodic unreliable inspections. <em>TAI</em>, <em>4</em>(4),
709–721. (<a href="https://doi.org/10.1109/TAI.2022.3197680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of monitoring technology, increasing attention has been paid to condition-based maintenance (CBM). Also, if condition information for maintenance can be obtained with fewer inspections, the cost of the entire maintenance process could be reduced. However, in application, the monitoring equipment cannot maintain always a reliable operating state, for example, due to various uncertain factors, such as sensor errors, component tolerances, and environment disturbances, the inspections from the sensors are often unreliable. Motivated by these simple observations, we propose a nonperiodic CBM policy under unreliable inspections. The time interval of the nonperiodic inspections is obtained via an inspection scheduling function. The unknown parameters of the component degradation process are updated by GD, while simultaneously the maintenance decision variables are adjusted. A catastrophe strategy-based particle swarm optimization is used to set the optimal decision variables by minimizing the long-run cost rate. Application to laser degradation data illustrates the effectiveness of the proposed method.},
  archive      = {J_TAI},
  author       = {Yang Li and Yan Shi and Zhiyao Zhang and Ningyun Lu and Xiuli Wang and Enrico Zio},
  doi          = {10.1109/TAI.2022.3197680},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {709-721},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Condition-based maintenance for performance degradation under nonperiodic unreliable inspections},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Teacher–student uncertainty autoencoder for the
process-relevant and quality-relevant fault detection in the industrial
process. <em>TAI</em>, <em>4</em>(4), 698–708. (<a
href="https://doi.org/10.1109/TAI.2022.3185024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault detection plays an important role in process monitoring, while current fault detection methods only concentrate on process-relevant or quality-relevant faults. Therefore, in this article, a fault detection method based on the improved teacher–student network is proposed, in which both the process-relevant and quality-relevant faults are monitored. Concretely, the student network extracts representation features and the teacher network detects faults. As the features difference between the teacher and student networks can cause performance degradation when the features of teacher network are replaced by the one from student network, representation evaluation block (REB) is proposed to evaluate and reduce the features difference. As a concrete method of REB, uncertainty modeling is proposed to quantify the features difference and alleviate the aleatoric uncertainty, modeling features difference as a central isotropic Gaussian distribution. Then, asynchronous iterative is designed to implement teacher network and student network joint training. Accordingly, REB based on uncertainty modeling is applied in the teacher–student network named as teacher–student uncertainty autoencoder (TSUAE). Finally, a fault detection framework based on TSUAE is proposed, the effectiveness of which is verified in a wastewater treatment process.},
  archive      = {J_TAI},
  author       = {Dan Yang and Xin Peng and Yusheng Lu and Haojie Huang and Weimin Zhong},
  doi          = {10.1109/TAI.2022.3185024},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {698-708},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Teacher–Student uncertainty autoencoder for the process-relevant and quality-relevant fault detection in the industrial process},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer-learning-aided fault detection for traction drive
systems of high-speed trains. <em>TAI</em>, <em>4</em>(4), 689–697. (<a
href="https://doi.org/10.1109/TAI.2022.3177387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term operation may lead to performance degradation of the traction drive systems. It will naturally increase the difficulty of fault detection (FD). To ensure the safe and stable operation of the traction drive system, data-driven FD has received considerable attention, especially deep learning methods. By exploiting the idea of transfer learning, this article proposes a new FD method for traction converter faults in the traction drive systems of high-speed trains. Its structure consists of a federal neural network based on a variational autoencoder. The significant advantages of the proposed FD method based on transfer learning are summarized as follows: 1) FD is still valid for the systems with performance degradation; 2) it can also realize the FD function even if the physical model and related parameters are not provided; and 3) the proposed framework can adaptively adjust the model parameters by storing and reusing the prior knowledge in the neural network. Finally, the effectiveness of the proposed method is demonstrated through the platform of the traction drive control system.},
  archive      = {J_TAI},
  author       = {Chao Cheng and Xuedong Li and Pu Xie and Xiaoyue Yang},
  doi          = {10.1109/TAI.2022.3177387},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {689-697},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Transfer-learning-aided fault detection for traction drive systems of high-speed trains},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fault detection and diagnosis using statistic feature and
improved broad learning for traction systems in high-speed trains.
<em>TAI</em>, <em>4</em>(4), 679–688. (<a
href="https://doi.org/10.1109/TAI.2022.3172896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensors equipped in the high-speed trains can collect a lot of data with the normal working condition and different faults that occurred. In recent years, many data-driven methods were developed for fault detection and diagnosis (FDD). However, inaccurate diagnosis and costly computation are still the great challenges that exist. In order to address those issues, this article developed an FDD architecture using statistic feature and the improved broad learning system (BLS) to promote performance. It uses statistic feature to capture the inherit discrimination of normal data and fault data, and then adopts the improved BLS model to achieve the accurate and fast fault diagnosis without time-consuming training and mathematical models of high-speed trains. In validation, the proposed FDD scheme is first conducted on a software-based fault-injection simulation platform; it can give guiding significance for the subsequent hardware-in-the-loop simulation platform. All results show that the presented FDD framework achieves state-of-the-art performance than the other mainstream FDD methods.},
  archive      = {J_TAI},
  author       = {Li Guo and Runze Li and Bin Jiang},
  doi          = {10.1109/TAI.2022.3172896},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {679-688},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fault detection and diagnosis using statistic feature and improved broad learning for traction systems in high-speed trains},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disturbance robust abnormality diagnosis of fused magnesium
furnaces using deep neural networks. <em>TAI</em>, <em>4</em>(4),
669–678. (<a href="https://doi.org/10.1109/TAI.2022.3168251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operation abnormalities of fused magnesium furnaces (FMFs), e.g., semi-molten, can degrade the product quality and operation performance. The abnormalities can even lead to accidents caused by leakage of the fusing fluids with ultrahigh temperatures. Therefore, it is essential to identify the semi-molten abnormality timely and accurately. In view of the spatiotemporal characteristics of the image sequences of the furnace shell under abnormal conditions of the FMFs, and the existence of strong disturbances caused by water mist, white spot, and flame fluctuation on top of the furnace, this article establishes a novel deep learning architecture for operation abnormality diagnosis with robustness to disturbances of the FMFs. The new scheme is composed of two parts, i.e., a predictive neural networks (PredNet) for disturbance processing, and a deep three-dimensional convolutional recurrent neural networks (3DCRNN) for abnormality diagnosis. First, PredNet-based unsupervised learning is incorporated with image residual extraction for disturbance processing. Second, using the clean image sequences after disturbance processing, a new deep 3DCRNN that integrates three-dimensional CNN (3DCNN) and long-short term memory is proposed for enhanced spatiotemporal feature extraction and semi-molten abnormality diagnosis. The 3DCRNN successfully overcomes the limitation of conventional 3DCNN that focuses on local spatiotemporal extraction and loses the opportunities to capture long-term changes. The experimental results using the image sequences collected from a real FMF demonstrate the effectiveness of the proposed method.},
  archive      = {J_TAI},
  author       = {Qiang Liu and Yang Zhang and Gaochang Wu and Zizhu Fan},
  doi          = {10.1109/TAI.2022.3168251},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {669-678},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Disturbance robust abnormality diagnosis of fused magnesium furnaces using deep neural networks},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An extreme gradient boosting aided fault diagnosis approach:
A case study of fuse test bench. <em>TAI</em>, <em>4</em>(4), 661–668.
(<a href="https://doi.org/10.1109/TAI.2022.3165137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The health status of a fuse test bench is essential to monitor to ensure quality control of the fuse. A system failure during operation will lead to significant impacts on the final quality of fuses. Thus, it is important to have a fault diagnosis system to detect, classify, and identify the root causes of faults to prevent operation failure. An effective fault diagnosis system should have high accuracy, fast diagnosis time, and interpretable root cause analysis. This article proposes an integrated fault diagnosis system based on extreme gradient boosting for an automated fuse test bench to solve those challenges. The proposed diagnosis system is then validated using the dataset from PHM 2021 Data Challenge. Performance comparison of the fault diagnosis system with other standard approaches in practice is also carried out. Experimental results show that the diagnostic accuracy of the proposed system outperforms several standard fault diagnostic approaches.},
  archive      = {J_TAI},
  author       = {Muhammad Gibran Alfarizi and Jørn Vatn and Shen Yin},
  doi          = {10.1109/TAI.2022.3165137},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {661-668},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An extreme gradient boosting aided fault diagnosis approach: A case study of fuse test bench},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy observer based constraint control for switched systems
with actuator failures and partially unstable subsystems. <em>TAI</em>,
<em>4</em>(4), 650–660. (<a
href="https://doi.org/10.1109/TAI.2022.3165141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive fuzzy observer based tracking control problem for switched nonlinear systems with full-state constraints and actuator failures under the switching law with average dwell time. Since the states in the system are not measurable, a nonlinear fuzzy observer is designed to estimate the immeasurable states. At the same time, the tangent barrier Lyapunov functions are selected to solve the full-state constraint problem. In addition, because of the existence of unknown parameter in actuator failures, the adaptive approach is used. Further, owing to actuator failures in controller, which may lead to some subsystems to be unstable, the switching law under average dwell time is adopted. Thus, even if there exist the actuator failures and partially unstable subsystems, the proposed method can guarantee that all signals in the closed-loop system are bounded, the system output can track the desired signal, and all system states satisfy their corresponding constrained conditions. Moreover, the observer states can well estimate the immeasurable states. Finally, the effectiveness of the presented control scheme is verified by a simulation example.},
  archive      = {J_TAI},
  author       = {Li Tang and Meiying Yang and Yan-Jun Liu},
  doi          = {10.1109/TAI.2022.3165141},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {650-660},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fuzzy observer based constraint control for switched systems with actuator failures and partially unstable subsystems},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Association hierarchical representation learning for
plant-wide process monitoring by using multilevel knowledge graph.
<em>TAI</em>, <em>4</em>(4), 636–649. (<a
href="https://doi.org/10.1109/TAI.2022.3161860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to satisfy the safety requirements of plant-wide processes, distributed process monitoring methods are often used. However, few of them consider the problem on building multilevel knowledge blocks and the associations between different blocks, all of which are beneficial for plant-wide process monitoring in avoiding information conflict and even to improve monitoring accuracy. To handle these issues, a plant-wide process monitoring method is proposed, which is based on hierarchical graph representation learning with differentiable pooling by using a multilevel knowledge graph (MLKG). Specifically, the MLKG consists of device level, subprocess level, process level, etc. Each level has numerous blocks (nodes), which is first constructed by prior knowledge on monitoring variables to calculate the status of key components, such as the Hotelling’s statistics. Then, normalized mutual information is used to obtain the associations between monitoring variables, and the status of each block on each level can be updated. Based on this method, the MLKG can be completely constructed. In order to consider the association information of hierarchical representations of the MLKG, hierarchical graph representation learning is used to achieve plant-wide process monitoring. Results of case study on practical cobalt and nickel removal from zinc solution demonstrate the effectiveness and applicability of this method.},
  archive      = {J_TAI},
  author       = {Hao Ren and Zhiwen Chen and Xiaojun Liang and Chunhua Yang and Weihua Gui},
  doi          = {10.1109/TAI.2022.3161860},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {636-649},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Association hierarchical representation learning for plant-wide process monitoring by using multilevel knowledge graph},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial-intelligence-based quantitative fault
diagnosability analysis of spacecraft: An information geometry
perspective. <em>TAI</em>, <em>4</em>(4), 624–635. (<a
href="https://doi.org/10.1109/TAI.2022.3162189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To provide a theoretical foundation for the design of spacecraft and a reference for their on-orbit adjustment, this study develops methods for the quantitative diagnosability analysis of spacecraft through the introduction of a Riemannian manifold and artificial intelligence into a diagnosability analysis framework. These developed methods have three main advantages. First, they do not rely on any assumption regarding the statistical distribution of the observations. Second, all types of faults can be analyzed without information loss. Third, the computational burden is reduced, making it possible to perform on-orbit fault diagnosability analysis. The effectiveness and feasibility of the proposed methods are then verified via a numerical simulation and a spacecraft attitude control system.},
  archive      = {J_TAI},
  author       = {Dayi Wang and Fangzhou Fu and Han Yu and Weimeng Chu and Zhigang Wu and Wenbo Li},
  doi          = {10.1109/TAI.2022.3162189},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {624-635},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Artificial-intelligence-based quantitative fault diagnosability analysis of spacecraft: An information geometry perspective},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining sequential alarm pattern based on the incremental
causality PrefixSpan algorithm. <em>TAI</em>, <em>4</em>(4), 612–623.
(<a href="https://doi.org/10.1109/TAI.2022.3156052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alarm flood is the main reason of alarm system failure in the modern industry; therefore, it is an important issue to suppress the sequential alarm caused by anomalous propagation in an alarm flood. First, the alarm event is defined with time stamp, and the causal alarm pattern is defined as a closed frequent alarm pattern with a time constraint. An incremental causality PrefixSpan algorithm is proposed to address two objectives simultaneously: 1) effectively mine the causal alarm patterns from the known alarm data and 2) dynamically update them without rescanning the entire database when a new alarm flood occurs. The proposed algorithm is, therefore, particularly suitable for real-time operation. The experiments for the benchmark synthetic dataset and the Tennessee Eastman process are finished to verify the proposed method.},
  archive      = {J_TAI},
  author       = {Jing Wang and Ruixue Jia and Jinglin Zhou and Meng Zhou},
  doi          = {10.1109/TAI.2022.3156052},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {612-623},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Mining sequential alarm pattern based on the incremental causality PrefixSpan algorithm},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven soft sensing for batch processes using neural
network-based deep quality-relevant representation learning.
<em>TAI</em>, <em>4</em>(4), 602–611. (<a
href="https://doi.org/10.1109/TAI.2022.3145758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft sensors provide a means to reliably estimate unmeasurable variables, thereby playing a prevalent role in formulating closed-loop control in batch processes. In soft sensor development, enhancing quality-relevant information and eliminating quality-irrelevant information are important. This study proposes a neural network-based deep quality-relevant representation learning approach to improve the soft sensing performance in dynamic batch processes. The structure of a deep neural network is optimized in a layer-by-layer manner. First, given the generally abundant predictor variables, the maximal relevance–minimal redundancy criterion is used to optimize the input layer, select the most beneficial variables, and eliminate modeling redundancy. Second, mutual information-based quality-relevant representation selection is performed in the middle layers to enhance the quality-relevant information and eliminate the influence of irrelevant representations. Third, deep quality-relevant representations are extracted, and a soft sensor model is developed. The proposed method is tested on a fed-batch penicillin fermentation process and an industrial injection molding process. Lastly, it shows that the proposed method outperforms several state-of-the-art approaches, thereby confirming its effectiveness.},
  archive      = {J_TAI},
  author       = {Qingchao Jiang and Ziwen Wang and Shifu Yan and Zhixing Cao},
  doi          = {10.1109/TAI.2022.3145758},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {602-611},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Data-driven soft sensing for batch processes using neural network-based deep quality-relevant representation learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature-aligned stacked autoencoder: A novel semisupervised
deep learning model for pattern classification of industrial faults.
<em>TAI</em>, <em>4</em>(4), 592–601. (<a
href="https://doi.org/10.1109/TAI.2021.3134186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoder is a widely used deep learning method, which first extracts features from all data through unsupervised reconstruction, and then fine-tunes the network with labeled data. However, due to the limited number of labeled data samples, the network may lack sufficient generalization ability and is prone to overfitting. This article proposes a new semisupervised deep learning method called feature-aligned stacked autoencoder (FA-SAE). FA-SAE takes advantage of the unlabeled data during the fine-tuning process by aligning the feature of both labeled and unlabeled data. In FA-SAE, a new training loss function is designed by integrating the Sinkhorn distance measure of the difference between the features extracted from labeled and unlabeled data through the neural network into the cross-entropy classification loss. The effectiveness of the proposed FA-SAE is verified through its application to two industrial processes, and the application results demonstrated that the proposed FA-SAE has better generalization ability and higher fault classification accuracy as compared to the state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Xinmin Zhang and Hongyi Zhang and Zhihuan Song},
  doi          = {10.1109/TAI.2021.3134186},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {592-601},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Feature-aligned stacked autoencoder: A novel semisupervised deep learning model for pattern classification of industrial faults},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Special issue on artificial intelligence
methods for maintenance and safety of automation systems. <em>TAI</em>,
<em>4</em>(4), 589–591. (<a
href="https://doi.org/10.1109/TAI.2023.3267683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Order to ensure the safe operation of automation systems, maintenance and safety have been active issues for automation systems. By discovering quantitative or qualitative knowledge hidden in measurements of automation systems, artificial intelligence (AI) methods can carry on these tasks in a human-like way. Therefore, issues, such as AI-based fault diagnosis (FD), fault-tolerant control, together with fault prognostics receive enhanced attention in both engineering and research domains over the past decades. The primary objective of this Special Issue, titled “AI Methods for Maintenance and Safety of Automation Systems,” of the IEEE Transactions on Artificial Intelligence is to provide the related latest achievements made by researchers and practitioners on the one hand, and to identify critical issues and challenges for future investigation on the other hand.},
  archive      = {J_TAI},
  author       = {Hongtian Chen and Hao Luo and Nishchal Verma and Bin Jiang},
  doi          = {10.1109/TAI.2023.3267683},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {589-591},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Guest editorial: Special issue on artificial intelligence methods for maintenance and safety of automation systems},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A temporal type-2 fuzzy system for time-dependent
explainable artificial intelligence. <em>TAI</em>, <em>4</em>(3),
573–586. (<a href="https://doi.org/10.1109/TAI.2022.3210895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable artificial intelligence (XAI) focuses on transparent AI models and decisions, which are easy to understand, analyze, and augment by a nontechnical audience. Fuzzy logic systems (FLS)-based XAI provides an explainable framework while also modeling uncertainties in real-world environments. However, most real-life processes are not characterized by high uncertainty alone; they are also inherently time dependent, i.e., the processes are time variant. In this work, we present a novel temporal type-2 FLS-based approach for time-dependent XAI (TXAI) systems, which can account for the likelihood of a sample occurrence in the time domain by its the frequency. In the proposed temporal type-2 fuzzy sets (TT2FSs), a 4-D time-dependent membership function integrates the universe of discourse, its membership, and its frequency of occurrence across time. The TXAI system manifested better classification prowess in cross-validation tests, with a mean recall of 95.40% than a standard XAI system (based on nontemporal general type-2 fuzzy sets) that had a mean recall of 87.04%. TXAI also performed significantly better than most nonexplainable AI systems, with between 3.95% and 19.04% improvement gain in mean recall. In addition, TXAI can also outline the most likely time-dependent trajectories using the frequency and time dimensions embedded in the TXAI model; viz. given a rule at a determined time interval, what will be the next most likely rule at a subsequent time interval. In this regard, the proposed TXAI system can have profound implications for delineating the evolution of real-life time-dependent processes, such as behavioral or biological processes.},
  archive      = {J_TAI},
  author       = {Mehrin Kiani and Javier Andreu-Perez and Hani Hagras},
  doi          = {10.1109/TAI.2022.3210895},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {573-586},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A temporal type-2 fuzzy system for time-dependent explainable artificial intelligence},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary multilabel adversarial examples: An effective
black-box attack. <em>TAI</em>, <em>4</em>(3), 562–572. (<a
href="https://doi.org/10.1109/TAI.2022.3198629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attack. Minor malicious modifications of examples will lead to the DNN misclassification. Such maliciously modified examples are called adversarial examples. So far, the work on adversarial examples is mainly focused on multiclass classification tasks; there is less work in the field of multilabel classification. In this article, for the first time, a differential evolution (DE) algorithm that can effectively generate multilabel adversarial examples is proposed, which is called MLAE-DE. Different from traditional DE, we designed a complementary mutation operator for MLAE-DE, which can improve attack performance and reduce the number of fitness evaluations. As a black-box attack, MLAE-DE does not need to access model parameters and only uses model outputs to generate adversarial examples. Experiments on two typical multilabel classification models and three typical datasets under the black-box settings are conducted in this article. Experimental results demonstrate that, comparing with the existing black-box attack algorithms for multilabel classification models, the attack success rate of our proposed algorithm is much better.},
  archive      = {J_TAI},
  author       = {Linghao Kong and Wenjian Luo and Hongwei Zhang and Yang Liu and Yuhui Shi},
  doi          = {10.1109/TAI.2022.3198629},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {562-572},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Evolutionary multilabel adversarial examples: An effective black-box attack},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward personalization of user preferences in partially
observable smart home environments. <em>TAI</em>, <em>4</em>(3),
549–561. (<a href="https://doi.org/10.1109/TAI.2022.3178065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technologies used in smart homes have recently improved to learn the user preferences from feedback in order to enhance the user convenience and quality of experience. Most smart homes learn a uniform model to represent the thermal preferences of users, which generally fails when the pool of occupants includes people with different sensitivities to temperature, for instance, due to age and physiological factors. Thus, a smart home with a single optimal policy may fail to provide comfort when a new user with a different preference is integrated into the home. In this article, we propose a Bayesian reinforcement learning framework that can approximate the current occupant state in a partially observable smart home environment using its thermal preference and, then, identify the occupant as a new user or someone who is already known to the system. Our proposed framework can be used to identify users based on the temperature and humidity preferences of the occupant when performing different activities to enable personalization and improve comfort. We then compare the proposed framework with a baseline long short-term memory learner that learns the thermal preference of the user from the sequence of actions that it takes. We perform these experiments with up to five simulated human models each based on hierarchical reinforcement learning. The results show that our framework can approximate the belief state of the current user just by its temperature and humidity preferences across different activities with a high degree of accuracy.},
  archive      = {J_TAI},
  author       = {Shashi Suman and Francois Rivest and Ali Etemad},
  doi          = {10.1109/TAI.2022.3178065},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {549-561},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Toward personalization of user preferences in partially observable smart home environments},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TREND: Transferability-based robust ENsemble design.
<em>TAI</em>, <em>4</em>(3), 534–548. (<a
href="https://doi.org/10.1109/TAI.2022.3175172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models hold state-of-the-art performance in many fields, but their vulnerability to adversarial examples poses a threat to their ubiquitous deployment in practical settings. Additionally, adversarial inputs generated on one classifier have been shown to transfer to other classifiers trained on similar data, which makes the attacks possible even if model parameters are not revealed to the adversary. This property of transferability has not yet been systematically studied, leading to a gap in our understanding of robustness of neural networks to adversarial inputs. In this work, we study the effect of network architecture, optimizer, input, weight, and activation quantization on transferability of adversarial samples. We also study the transferability of different attacks. Our experiments reveal that transferability is significantly hampered by input quantization and architectural mismatch between source and target, and the choice of optimizer turns out to be critical. We observe that transferability is architecture-dependent for both weight and activation quantized models. To quantify transferability, we use simple metric and demonstrate the utility of the metric in designing a methodology to build ensembles with improved adversarial robustness. When attacking ensembles we observe that “gradient domination” by a single ensemble member model hampers existing attacks. To combat this we propose a new state-of-the-art ensemble attack. We compare the proposed attack with existing attack techniques to show its effectiveness. Finally, we show that an ensemble consisting of carefully chosen diverse networks achieves better adversarial robustness than would otherwise be possible with a single network. The source code for this work has been made available at https://github.com/purdue-nrl/TREND .},
  archive      = {J_TAI},
  author       = {Deepak Ravikumar and Sangamesh Kodge and Isha Garg and Kaushik Roy},
  doi          = {10.1109/TAI.2022.3175172},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {534-548},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {TREND: Transferability-based robust ENsemble design},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient few-shot classification via contrastive
pretraining on web data. <em>TAI</em>, <em>4</em>(3), 522–533. (<a
href="https://doi.org/10.1109/TAI.2022.3169463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot classification is challenging due to the limited data and labels. The existing algorithms usually resolve this problem by pretraining models with a considerable amount of annotated data, which share knowledge with the target domain. Nevertheless, large quantities of homogenous data samples are not always available. To tackle this obstacle, we develop a few-shot learning framework that prepares data automatically and still produces well-behaved models. This framework is implemented through conducting contrastive learning on unlabeled web images. Instead of requiring manually annotated data, this framework trains models via constructing pseudolabels. Additionally, since online data are virtually limitless and continue to be generated, the model can, thus, be empowered to constantly obtain up-to-date knowledge from the Internet. Furthermore, we observe that the generalization ability of learned representation is crucial for self-supervised learning. To present its importance, a naive yet efficient normalization strategy is proposed. Consequentially, this strategy boosts the accuracy of trained models significantly. We demonstrate the superiority of the proposed framework with experiments on miniImageNet, tieredImageNet, and Omniglot. The results indicate that our method has surpassed previous unsupervised counterparts by large margins and obtained performance comparable with some supervised ones.},
  archive      = {J_TAI},
  author       = {Zhuoling Li and Haohan Wang and Tymoteusz Świstek and En Yu and Haoqian Wang},
  doi          = {10.1109/TAI.2022.3169463},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {522-533},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Efficient few-shot classification via contrastive pretraining on web data},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling tradeoffs using preference-based feedback in
session-based recommender systems. <em>TAI</em>, <em>4</em>(3), 511–521.
(<a href="https://doi.org/10.1109/TAI.2022.3214801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cold-start is a challenge in most data-driven approaches across various paradigms of recommender systems research. Knowledge integration is becoming essential to alleviate the problem. Despite the availability of knowledge representations like the knowledge graph, there is a need for a richer semantic model to capture relationships among products in a domain. Recommender systems inspired by case-based reasoning uses rich domain knowledge in their recommendation process. Conversational case-based reasoning recommender systems (CCBR-RS) are session-based recommender systems that operate under cold-start assumptions. User preferences in such scenarios can only be learned based on the context of the current session. Tradeoffs have been used in literature to model the relationship among products in recommending appropriate products. The models in earlier works have been based on crisp sets, which lack flexibility in capturing varying degrees of tradeoffs, which is a more natural way of dealing with tradeoffs. This work proposes a flexible tradeoff representation scheme based on fuzzy sets to model tradeoffs in terms of linguistic terms and a fuzzy inference system that measures the similarity among tradeoffs represented in linguistic terms. We demonstrate the performance improvement in utilizing a more flexible model of tradeoffs by evaluating the methods on three datasets in a CCBR-RS framework.},
  archive      = {J_TAI},
  author       = {Anbarasu Sekar and Sutanu Chakraborti},
  doi          = {10.1109/TAI.2022.3214801},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {511-521},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Modeling tradeoffs using preference-based feedback in session-based recommender systems},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reconstruction of 3-d human poses from video.
<em>TAI</em>, <em>4</em>(3), 497–510. (<a
href="https://doi.org/10.1109/TAI.2022.3164065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning advances have made it possible to recoverfull 3-D meshes of human models from individual images. However, the extension of this notion to videos for recovering temporally coherent poses is still underexplored. A major challenge in this direction is the lack of appropriately annotated video data for learning the desired computational models. The existing human pose datasets only provide 2-D or 3-D skeleton joint annotations, whereas the datasets are also insufficiently recorded in constrained environments. We first contribute a technique to synthesize monocular action videos with rich 3-D annotations that are suitable for learning computational models for full mesh 3-D human pose recovery. Compared to the existing methods that simply “texture map” clothes onto the 3-D human pose models, our approach incorporates Physics-based realistic cloth deformations with human body movements. The generated videos cover a large variety of human actions, poses, and visual appearances, while the annotations record accurate human pose dynamics and human body surface information. Our second major contribution is an end-to-end trainable recurrent neural network for full pose mesh recovery from monocular videos. Using the proposed video data and a long short-term memory recurrent structure, our network explicitly learns to model the temporal coherence in videos and imposes geometric consistency over the recovered meshes. We establish the effectiveness of the proposed model with quantitative and qualitative analysis using the proposed and benchmark datasets.},
  archive      = {J_TAI},
  author       = {Jian Liu and Naveed Akhtar and Ajmal Mian},
  doi          = {10.1109/TAI.2022.3164065},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {497-510},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep reconstruction of 3-D human poses from video},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On computing evidential centroid through conjunctive
combination: An impossibility theorem. <em>TAI</em>, <em>4</em>(3),
487–496. (<a href="https://doi.org/10.1109/TAI.2022.3180973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The theory of belief functions (TBFs) is now a widespread framework to deal and reason with uncertain and imprecise information, in particular to solve information fusion and clustering problems. Combination functions (rules) and distances are essential tools common to both the clustering and information fusion problems in the context of TBF, which have generated considerable literature. Distances and combination between evidence corpus of TBF are indeed often used within various clustering and classification algorithms, however, their interplay and connections have seldom been investigated, which is the topic of this article. More precisely, we focus on the problem of aggregating evidence corpus to obtain a representative one, and we show through an impossibility theorem that in this case, there is a fundamental contradiction between the use of conjunctive combination rules on the one hand, and the use of distances on the other hand. Rather than adding new methodologies, such results are instrumental in guiding the user among the many methodologies that already exist. To illustrate the interest of our results, we discuss different cases where they are at play.},
  archive      = {J_TAI},
  author       = {Yiru Zhang and Sébastien Destercke and Zuowei Zhang and Tassadit Bouadi and Arnaud Martin},
  doi          = {10.1109/TAI.2022.3180973},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {487-496},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {On computing evidential centroid through conjunctive combination: An impossibility theorem},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovering high utility episodes in sequences.
<em>TAI</em>, <em>4</em>(3), 473–486. (<a
href="https://doi.org/10.1109/TAI.2022.3223965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence data are more commonly seen than other types of data (e.g., transaction data) in real-world applications. For the mining task from sequence data, several problems have been formulated, such as sequential pattern mining, episode mining, and sequential rule mining. As one of the fundamental problems, episode mining has often been studied. The common wisdom is that discovering frequent episodes is not useful enough. In this article, we propose an efficient utility mining approach, namely, UMEpi: utility mining of high-utility episodes from complex event sequences. We propose the concept of remaining utility of episodes and achieve a tighter upper bound, namely, episode-weighted utilization (EWU), which will provide better pruning. Thus, the optimized EWU-based pruning strategies can achieve better improvements in mining efficiency. The search space of UMEpi w.r.t. a prefix-based lexicographic sequence tree is spanned and determined recursively for mining high-utility episodes, by prefix-spanning in a depth-first way. Finally, extensive experiments on four real-life datasets demonstrate that UMEpi can discover the complete high-utility episodes from complex event sequences. Furthermore, the improved variants of UMEpi significantly outperform the baseline in terms of execution time, memory consumption, and scalability.},
  archive      = {J_TAI},
  author       = {Wensheng Gan and Jerry Chun-Wei Lin and Han-Chieh Chao and Philip S. Yu},
  doi          = {10.1109/TAI.2022.3223965},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {473-486},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Discovering high utility episodes in sequences},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparsing and smoothing for the seq2seq models. <em>TAI</em>,
<em>4</em>(3), 464–472. (<a
href="https://doi.org/10.1109/TAI.2022.3207982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current neural language models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over the target. While this setup provides solid results in several natural language processing (NLP) tasks, one unsatisfying aspect is its dense output. This density is wasteful, making models hard to interpret and assigning probability mass to many implausible outputs. To overcome this problem, we propose T-softmax, a simple but effective method to draw considerably sparse probability out of neural language models than softmax. Our method avoids dense output by truncating the unreliable tail of the probability distribution to improve the model&#39;s performance. In addition, we generalize logits with temperature, a critical regularization technique, from the softmax to T-softmax. To show our approach as a drop-in replacement for softmax, we evaluate them on three NLP tasks: summary generation, question answer, and math word problem. Experimental results show that our proposed model significantly improves performance without sacrificing speed; notably, in all experiments, our method outperforms the softmax.},
  archive      = {J_TAI},
  author       = {Shuai Zhao and Zhuoqian Liang and Jinming Wen and Jie Chen},
  doi          = {10.1109/TAI.2022.3207982},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {464-472},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Sparsing and smoothing for the seq2seq models},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regressing word and sentence embeddings for low-resource
neural machine translation. <em>TAI</em>, <em>4</em>(3), 450–463. (<a
href="https://doi.org/10.1109/TAI.2022.3187680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, neural machine translation (NMT) has achieved unprecedented performance in the automated translation of resource-rich languages. However, it has not yet managed to achieve a comparable performance over the many low-resource languages and specialized translation domains, mainly due to its tendency to overfit small training sets and consequently strive for new data. For this reason, in this article, we propose a novel approach to regularize the training of NMT models to improve their performance over low-resource language pairs. In the proposed approach, the model is trained to copredict the target training sentences both as the usual categorical outputs (i.e., sequences of words) and as word and sentence embeddings. The fact that word and sentence embeddings are pretrained over large corpora of monolingual data helps the model generalize beyond the available translation training set. Extensive experiments over three low-resource language pairs have shown that the proposed approach has been able to outperform strong state-of-the-art baseline models, with more marked improvements over the smaller training sets (e.g., up to &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$+6.57$&lt;/tex-math&gt;&lt;/inline-formula&gt; BLEU points in Basque–English translation). A further experiment on unsupervised NMT has also shown that the proposed approach has been able to improve the quality of machine translation even with no parallel data at all.},
  archive      = {J_TAI},
  author       = {Inigo Jauregi Unanue and Ehsan Zare Borzeshi and Massimo Piccardi},
  doi          = {10.1109/TAI.2022.3187680},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {450-463},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Regressing word and sentence embeddings for low-resource neural machine translation},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable natural language inference via identifying
important rationales. <em>TAI</em>, <em>4</em>(3), 438–449. (<a
href="https://doi.org/10.1109/TAI.2022.3178391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language inference (NLI) is an important task in the field of natural language processing (NLP), which requires certain common sense and logical reasoning abilities. Existing pretrained models applied to NLI have difficulty achieving good performance, and inference models based on deep learning also have the problem of poor interpretability. In this article, we propose an explainable NLI model by identifying important rationales, which can be obtained by the specially designed rationale extractor and selector. The pretrained models can be fine-tuned on the rationale-enhanced dataset. In addition, the extracted rationales can be used to generate natural language explanation (NLE) with a higher quality. Not only can NLE serve as an explanation for the prediction results, but it can also be combined with the original dataset to improve the performance of pretrained models. Experimental results on the Stanford natural language inference corpus (SNLI) show that the proposed model could obtain state-of-the-art results of 94.19% and 94.08%, respectively, on the SNLI development set, as well as on the test set and obtain good performance when transferred to the out-of-domain dataset (multiNLI). Furthermore, the proposed model has a certain improvement in computational efficiency due to the focus on important rationales.},
  archive      = {J_TAI},
  author       = {Zongbao Yang and Shoubin Dong and Jinlong Hu},
  doi          = {10.1109/TAI.2022.3178391},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {438-449},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Explainable natural language inference via identifying important rationales},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An automated deep reinforcement learning pipeline for
dynamic pricing. <em>TAI</em>, <em>4</em>(3), 428–437. (<a
href="https://doi.org/10.1109/TAI.2022.3186292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dynamic pricing problem is difficult due to the highly dynamic environment and unknown demand distributions. In this article, we propose a deep reinforcement learning (DRL) framework, which is a pipeline that automatically defines the DRL components for solving a dynamic pricing problem. The automated DRL pipeline is necessary because the DRL framework can be designed in numerous ways, and manually finding optimal configurations is tedious. The levels of automation make nonexperts capable of using DRL for dynamic pricing. Our DRL pipeline contains three steps of DRL design, including Markov decision process modeling, algorithm selection, and hyperparameter optimization. It starts with transforming available information to state representation and defining reward function using a reward shaping approach. Then, the hyperparameters are tuned using a novel hyperparameter optimization method that integrates Bayesian optimization and the selection operator of the genetic algorithm. We employ our DRL pipeline on reserve price optimization problems in online advertising as a case study. We show that using the DRL configuration obtained by our DRL pipeline, a pricing policy is obtained whose revenue is significantly higher than the benchmark methods. The evaluation is performed by developing a simulation for the real-time bidding environment that makes exploration possible for the reinforcement learning agent.},
  archive      = {J_TAI},
  author       = {Reza Refaei Afshar and Jason Rhuggenaath and Yingqian Zhang and Uzay Kaymak},
  doi          = {10.1109/TAI.2022.3186292},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {428-437},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An automated deep reinforcement learning pipeline for dynamic pricing},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing multidocument summarization by blending
reinforcement learning policies. <em>TAI</em>, <em>4</em>(3), 416–427.
(<a href="https://doi.org/10.1109/TAI.2022.3201807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider extractive summarization within a cluster of related texts (multidocument summarization). Unlike single-document summarization, redundancy is particularly important because sentences across related documents might convey overlapping information. Thus, sentence extraction in such a setting is difficult because one will need to determine which pieces of information are relevant while avoiding unnecessary repetitiveness. To solve this difficult problem, we propose a novel reinforcement learning-based method Policy Blending with maximal marginal relevance and Reinforcement Learning ( PoBRL ) for solving multidocument summarization. PoBRL jointly optimizes over the following objectives necessary for a high-quality summary: importance, relevance, and length. Our strategy decouples this multiobjective optimization into different subproblems that can be solved individually by reinforcement learning. Utilizing PoBRL, we then blend each learned policies to produce a summary that is a concise and a complete representation of the original input. Our empirical analysis shows high performance on several multidocument datasets. Human evaluation also shows that our method produces high-quality output.},
  archive      = {J_TAI},
  author       = {DiJia Su and Difei Su and John M. Mulvey and H.Vincent Poor},
  doi          = {10.1109/TAI.2022.3201807},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {416-427},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optimizing multidocument summarization by blending reinforcement learning policies},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reinforcement learning system for generating instantaneous
quality random sequences. <em>TAI</em>, <em>4</em>(3), 402–415. (<a
href="https://doi.org/10.1109/TAI.2022.3161893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random numbers are essential to most computer applications. Still, producing high-quality random sequences is a big challenge. Inspired by the success of artificial neural networks and reinforcement learning, we propose a novel and effective end-to-end learning system to generate pseudorandom sequences that operates under the upside-down reinforcement learning framework. It is based on manipulating the generalized information entropy metric to derive commands that instantaneously guide the agent toward the optimal random behavior. Using a wide range of evaluation tests, the proposed approach is compared against three state-of-the-art accredited pseudorandom number generators (PRNGs). The experimental results agree with our theoretical study and show that the proposed framework is a promising candidate for a wide range of applications.},
  archive      = {J_TAI},
  author       = {Yahya Almardeny and Alessio Benavoli and Noureddine Boujnah and Enrique Naredo},
  doi          = {10.1109/TAI.2022.3161893},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {402-415},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A reinforcement learning system for generating instantaneous quality random sequences},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial interdisciplinary artificial intelligence research
with machine education as an example. <em>TAI</em>, <em>4</em>(3),
399–401. (<a href="https://doi.org/10.1109/TAI.2023.3267663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This editorial poses the question of how we can increase the number of papers we publish on interdisciplinary artificial intelligence (AI) in the IEEE Transactions on AI (IEEE TAI).},
  archive      = {J_TAI},
  author       = {Hussein Abbass},
  doi          = {10.1109/TAI.2023.3267663},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {399-401},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Editorial interdisciplinary artificial intelligence research with machine education as an example},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving calibration and out-of-distribution detection in
deep models for medical image segmentation. <em>TAI</em>, <em>4</em>(2),
383–397. (<a href="https://doi.org/10.1109/TAI.2022.3159510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have proved to be powerful medical image segmentation models. In this study, we address some of the main unresolved issues regarding these models. Specifically, training of these models on small medical image datasets is still challenging, with many studies promoting techniques such as transfer learning. Moreover, these models are infamous for producing overconfident predictions and for failing silently when presented with out-of-distribution (OOD) test data. In this article, for improving prediction calibration we advocate for multitask learning, i.e., training a single model on several different datasets, spanning different organs of interest and different imaging modalities. We show that multitask learning can significantly improve model confidence calibration. For OOD detection, we propose a novel method based on spectral analysis of CNN feature maps. We show that different datasets, representing different imaging modalities and/or different organs of interest, have distinct spectral signatures, which can be used to identify whether or not a test image is similar to the images used for training. We show that our proposed method is more accurate than several competing methods, including methods based on prediction uncertainty and image classification.},
  archive      = {J_TAI},
  author       = {Davood Karimi and Ali Gholipour},
  doi          = {10.1109/TAI.2022.3159510},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {383-397},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Improving calibration and out-of-distribution detection in deep models for medical image segmentation},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of cardiovascular diseases in ECG images using
machine learning and deep learning methods. <em>TAI</em>, <em>4</em>(2),
373–382. (<a href="https://doi.org/10.1109/TAI.2022.3159505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases (heart diseases) are the leading cause of death worldwide. The earlier they can be predicted and classified; the more lives can be saved. Electrocardiogram (ECG) is a common, inexpensive, and noninvasive tool for measuring the electrical activity of the heart and is used to detect cardiovascular disease. In this article, the power of deep learning techniques was used to predict the four major cardiac abnormalities: abnormal heartbeat, myocardial infarction, history of myocardial infarction, and normal person classes using the public ECG images dataset of cardiac patients. First, the transfer learning approach was investigated using the low-scale pretrained deep neural networks SqueezeNet and AlexNet. Second, a new convolutional neural network (CNN) architecture was proposed for cardiac abnormality prediction. Third, the aforementioned pretrained models and our proposed CNN model were used as feature extraction tools for traditional machine learning algorithms, namely support vector machine, K-nearest neighbors, decision tree, random forest, and Naïve Bayes. According to the experimental results, the performance metrics of the proposed CNN model outperform the exiting works; it achieves 98.23% accuracy, 98.22% recall, 98.31% precision, and 98.21% F1 score. Moreover, when the proposed CNN model is used for feature extraction, it achieves the best score of 99.79% using the NB algorithm.},
  archive      = {J_TAI},
  author       = {Mohammed B. Abubaker and Bilal Babayiğit},
  doi          = {10.1109/TAI.2022.3159505},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {373-382},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Detection of cardiovascular diseases in ECG images using machine learning and deep learning methods},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust identification of figurative language in personal
health mentions on twitter. <em>TAI</em>, <em>4</em>(2), 362–372. (<a
href="https://doi.org/10.1109/TAI.2022.3175469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People often discuss their health on social media platforms. Discussion of personal experiences with diseases and symptoms can be useful in public health applications like adverse event surveillance. A major challenge comes from the need to distinguish personal health mentions from other uses of those terms, including figurative use, where words are used to mean something different. Public health applications require the separation of personal health mentions from other uses. Prior approaches incorporate some elements of context but could be improved to capture relationships between the linguistic characteristics of figurative expressions and the representations of the context. In this work, we investigate the role of context representation for identifying personal health mentions on social media and measure the impact of different representation choices on detecting figurative use of a range of disease and symptom words. We present an end-to-end approach that selects representations adaptively for different disease or symptom words. We conduct experiments using a publicly available health-mention dataset, annotated with ten disease or symptom labels. The results demonstrate that our approach outperforms the state of the art (SOTA) in the identification of figurative language use across a range of disease or symptom words, with an F1-score of 0.925 (an increase of 10.7% over the SOTA) and the proportion of correctly identified figurative mentions was 0.923 (an increase of 16.7% over the SOTA). An ablation analysis demonstrates that each of the new modules contributes to this increased performance.},
  archive      = {J_TAI},
  author       = {Usman Naseem and Jinman Kim and Matloob Khushi and Adam G. Dunn},
  doi          = {10.1109/TAI.2022.3175469},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {362-372},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Robust identification of figurative language in personal health mentions on twitter},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Landmark-aware and part-based ensemble transfer learning
network for static facial expression recognition from images.
<em>TAI</em>, <em>4</em>(2), 349–361. (<a
href="https://doi.org/10.1109/TAI.2022.3172272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition from images is a challenging problem in computer vision applications. Convolutional neural network (CNN), the state-of-the-art method for various computer vision tasks, has had limited success in predicting expressions from faces having extreme poses, illumination, and occlusion conditions. To mitigate this issue, CNNs are often accompanied by techniques like transfer, multitask, or ensemble learning that provide high accuracy at the cost of increased computational complexity. In this article, the authors propose a part-based ensemble transfer learning network that models how humans recognize facial expressions by correlating visual patterns emanating from facial muscles’ motor movements with a specific expression. The proposed network performs transfer learning from facial landmark localization to facial expression recognition. It consists of five subnetworks, and each subnetwork performs transfer learning from one of the five subsets of facial landmarks: eyebrows, eyes, nose, mouth, or jaw to expression classification. The network’s performance is evaluated using the Cohn-Kanade (CK+), Japanese female facial expression (JAFFE), and static facial expressions in the wild datasets, and it outperforms the benchmark for CK+ and JAFFE datasets by 0.51% and 5.34%, respectively. Additionally, the proposed ensemble network consists of only 1.65 M model parameters, ensuring computational efficiency during training and real-time deployment. Gradient-weighted class activation mapping visualizations of the network reveal the complementary nature of its subnetworks, a key design parameter of an effective ensemble network. Lastly, cross-dataset evaluation results show that the the proposed ensemble has a high generalization capacity, making it suitable for real-world usage.},
  archive      = {J_TAI},
  author       = {Rohan Wadhawan and Tapan K. Gandhi},
  doi          = {10.1109/TAI.2022.3172272},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {349-361},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Landmark-aware and part-based ensemble transfer learning network for static facial expression recognition from images},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multitarget stochastic configuration network and
applications. <em>TAI</em>, <em>4</em>(2), 338–348. (<a
href="https://doi.org/10.1109/TAI.2022.3162570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing stochastic configuration network (SCN)-based modeling methods are underperformed in handling multitarget regression problems. An important reason is that they ignore the intertarget correlations, which have an important effect on improving the modeling accuracy. To enhance the performance of these SCN-based models, in this article, a novel multitarget SCN (MTSCN) modeling approach is presented. The L 2,1 norm of a structure matrix can be utilized to explicitly reveal the correlations between multiple targets and an L 2 term are attached to the cost function of SCN. Considering the nonsmoothness of the constructed cost function, an alternating optimization algorithm is adopted to compute the structure matrix and the output weights of MTSCN. Then, a new supervisory mechanism is proposed to ensure the convergence of MTSCN. Finally, experimental results using the synthetic data and several real-world datasets show that the developed MTSCN is more superior to other modeling methods in resolving multitarget modeling problems.},
  archive      = {J_TAI},
  author       = {Qianjin Wang and Qiqiang Hong and Shang Wu and Wei Dai},
  doi          = {10.1109/TAI.2022.3162570},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {338-348},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multitarget stochastic configuration network and applications},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wave height prediction suitable for maritime transportation
based on green ocean of things. <em>TAI</em>, <em>4</em>(2), 328–337.
(<a href="https://doi.org/10.1109/TAI.2022.3168246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the application fields of the Internet of Things (IoT) involve all aspects. This article combines ocean research with the IoT, in order to investigate the wave height prediction to assist ships to improve the economy and safety of maritime transportation and proposes an ocean IoT Green Ocean of Things (GOoT) with a green and low-carbon concept. In the wave height prediction, we apply a hybrid model (EMD-TCN) combining the temporal convolutional network (TCN) and the empirical mode decomposition (EMD) to the buoy observation data. We then compare it with TCN, long short-term memory (LSTM), and hybrid model EMD-LSTM. By testing the data of eight selected NDBC buoys distributed in different sea areas, the effectiveness of the EMD-TCN hybrid model in wave height prediction is verified. The hysteresis problem in previous wave height prediction research is eliminated, while improving the accuracy of the wave height prediction. In the 24 h, 36 h, and 48 h wave height prediction, the minimum mean absolute errors are 0.1265, 0.1689, and 0.1963, respectively; the maximum coefficient of determination are 0.9388, 0.9019, and 0.8712, respectively. In addition, in the short-term prediction, the EMD-TCN hybrid model also performs well, and has strong versatility.},
  archive      = {J_TAI},
  author       = {Ranran Lou and Zhihan Lv and Mohsen Guizani},
  doi          = {10.1109/TAI.2022.3168246},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {328-337},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Wave height prediction suitable for maritime transportation based on green ocean of things},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-resolution urban flows forecasting with coarse-grained
spatiotemporal data. <em>TAI</em>, <em>4</em>(2), 315–327. (<a
href="https://doi.org/10.1109/TAI.2022.3153750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban flow monitoring and forecasting systems play important roles in smart city management. However, due to the long-lasting and enormous deployment cost of ubiquitous traffic monitoring devices (e.g., loop detectors, traffic video detection), it is very difficult to predict flow in high-resolution (HR) with limited monitoring devices. The existing spatiotemporal network based methods usually predict the urban flow in the same spatial scale without considering the spatial correlations between coarse-grained and fine-grained urban flows. To tackle these issues, we propose a HR spatiotemporal transformer network (HRSTT) to predict fine-grained urban flow. Specifically, residual convolution units are employed to constructs the high-level features of three-view temporal (e.g., closeness, period, and trend) flow data. Then, a transformer block is designed to jointly learn the spatiotemporal dynamic features of each temporal flow with self-attention mechanism. For the external factors (e.g., holidays, weather conditions) are extracted by embedding dense networks, which are fused with high level coarse-grained flow feature maps with gated-fusion scheme. Finally, the coarse-grained fusion feature maps are transferred to the distributional upsampling module, generating the fined-grained flow map of target predicted time. Furthermore, the proposed model is evaluated with several baselines on real-world TaxiBJ datasets, demonstrating the state-of-the-art performance of our approach on the fine-grained urban flow forecasting problem.},
  archive      = {J_TAI},
  author       = {Zhenyi Xu and Yu Kang and Yang Cao},
  doi          = {10.1109/TAI.2022.3153750},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {315-327},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {High-resolution urban flows forecasting with coarse-grained spatiotemporal data},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Projective fisher information for natural gradient descent.
<em>TAI</em>, <em>4</em>(2), 304–314. (<a
href="https://doi.org/10.1109/TAI.2022.3153593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improvements in neural network optimization algorithms have enabled shorter training times and the ability to reach state-of-the-art performance on various machine learning tasks. Fisher information based natural gradient descent is one such second-order method that improves the convergence speed and the final performance metric achieved for many machine learning algorithms. Fisher information matrices are also helpful to analyze the properties and expected behavior of neural networks. However, natural gradient descent is a high complexity method due to the need to maintain and invert covariance matrices. This is especially the case with modern deep neural networks, which have a very high number of parameters, and for which the problem often becomes computationally unfeasible. We suggest using the Fisher information for analysis of parameter space of fully connected and convolutional neural networks without calculating the matrix itself. We also propose a lower complexity natural gradient descent algorithm based on the projection of Kronecker factors of Fisher information combined with recursive calculation of inverses, which is computationally less complex and more stable. We finally share analysis and results showing that all these optimizations do not impact the accuracy while considerably lowering the optimization process’s complexity. These improvements should enable applying natural gradient descent methods for optimization to neural networks with a larger number of parameters, than possible previously.},
  archive      = {J_TAI},
  author       = {Piyush Kaul and Brejesh Lall},
  doi          = {10.1109/TAI.2022.3153593},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {304-314},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Projective fisher information for natural gradient descent},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering by the probability distributions from extreme
value theory. <em>TAI</em>, <em>4</em>(2), 292–303. (<a
href="https://doi.org/10.1109/TAI.2022.3153592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is an essential task to unsupervised learning. It tries to automatically separate instances into “coherent” subsets. As one of the most well-known clustering algorithms, &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -means assigns sample points at the boundary to a unique cluster, while it does not utilize the information of sample distribution or density. Comparably, it would potentially be more beneficial to consider the probability of each sample in a possible cluster. To this end, this article generalizes &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -means to model the distribution of clusters. Our novel clustering algorithm, thus, models the distributions of distances to centroids over a threshold by the generalized Pareto distribution (GPD) in extreme value theory. Notably, we propose the concept of centroid margin distance, use the GPD to establish a probability model for each cluster, and perform a clustering algorithm based on the covering probability function derived from the GPD. Such a GPD &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -means, thus, enables the clustering algorithm from the probabilistic perspective. Correspondingly, we also introduce a naive baseline, dubbed as generalized extreme value (GEV) &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -means. The GEV fits the distribution of the block maxima. In contrast, the GPD fits the distribution of distance to the centroid exceeding a sufficiently large threshold, leading to a more stable performance of GPD &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -means. Notably, GEV &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -means can also estimate the cluster structure and, thus, perform reasonably well over classical &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -means. Thus, extensive experiments on synthetic and real datasets demonstrate that GPD &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -means outperforms competitors.},
  archive      = {J_TAI},
  author       = {Sixiao Zheng and Ke Fan and Yanxi Hou and Jianfeng Feng and Yanwei Fu},
  doi          = {10.1109/TAI.2022.3153592},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {292-303},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Clustering by the probability distributions from extreme value theory},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dataset similarity to assess semisupervised learning under
distribution mismatch between the labeled and unlabeled datasets.
<em>TAI</em>, <em>4</em>(2), 282–291. (<a
href="https://doi.org/10.1109/TAI.2022.3168804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semisupervised deep learning (SSDL) is a popular strategy to leverage unlabeled data for machine learning when labeled data is not readily available. In real-world scenarios, different unlabeled data sources are usually available, with varying degrees of distribution mismatch regarding the labeled datasets. It begs the question, which unlabeled dataset to choose for good SSDL outcomes. Oftentimes, semantic heuristics are used to match unlabeled data with labeled data. However, a quantitative and systematic approach to this selection problem would be preferable. In this work, we first test the SSDL MixMatch algorithm under various distribution mismatch configurations to study the impact on SSDL accuracy. Then, we propose a quantitative unlabeled dataset selection heuristic based on dataset dissimilarity measures. These are designed to systematically assess how distribution mismatch between the labeled and unlabeled datasets affects MixMatch performance. We refer to our proposed method as deep dataset dissimilarity measures (DeDiMs), designed to compare labeled and unlabeled datasets. They use the feature space of a generic Wide-ResNet, which can be applied prior to learning, are quick to evaluate, and model agnostic. The strong correlation in our tests between MixMatch accuracy and the proposed DeDiMs suggests that this approach can be a good fit for quantitatively ranking different unlabeled datasets prior to SSDL training.},
  archive      = {J_TAI},
  author       = {Saul Calderon-Ramirez and Luis Oala and Jordina Torrents-Barrena and Shengxiang Yang and David Elizondo and Armaghan Moemeni and Simon Colreavy-Donnelly and Wojciech Samek and Miguel A. Molina-Cabello and Ezequiel López-Rubio},
  doi          = {10.1109/TAI.2022.3168804},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {282-291},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dataset similarity to assess semisupervised learning under distribution mismatch between the labeled and unlabeled datasets},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Imperceptible and sparse adversarial attacks via a
dual-population-based constrained evolutionary algorithm. <em>TAI</em>,
<em>4</em>(2), 268–281. (<a
href="https://doi.org/10.1109/TAI.2022.3168038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparse adversarial attack has attracted increasing attention due to the merit of a low attack cost via changing a small number of pixels. However, the generated adversarial examples are easily detected in vision since the perturbation to each pixel is relatively large. To achieve imperceptible and sparse adversarial attacks, this article formulates a bi-objective constrained optimization problem, simultaneously minimizing the &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell _{0}$&lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell _{2}$&lt;/tex-math&gt;&lt;/inline-formula&gt; distances to the original image, and proposes a dual-population-based constrained evolutionary algorithm to solve it. The proposed method solves the optimization problem by evolving two populations, where one population is responsible for finding feasible solutions (i.e., successful attacks) and the other one is to minimize both the &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell _{0}$&lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell _{2}$&lt;/tex-math&gt;&lt;/inline-formula&gt; distances. Moreover, a population initialization strategy and two genetic operators are customized to accelerate the convergence speed. Experimental results indicate that the proposed method can achieve high success rates with low attack costs, and strikes a better balance between the &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell _{0}$&lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell _{2}$&lt;/tex-math&gt;&lt;/inline-formula&gt; distances than state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Ye Tian and Jingwen Pan and Shangshang Yang and Xingyi Zhang and Shuping He and Yaochu Jin},
  doi          = {10.1109/TAI.2022.3168038},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {268-281},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Imperceptible and sparse adversarial attacks via a dual-population-based constrained evolutionary algorithm},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Annotation-efficient COVID-19 pneumonia lesion segmentation
using error-aware unified semisupervised and active learning.
<em>TAI</em>, <em>4</em>(2), 255–267. (<a
href="https://doi.org/10.1109/TAI.2022.3147440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the coronavirusdisease 2019 (COVID-19) pandemic continues, fast and automatic COVID-19-related pneumonia lesion segmentation method is in an urgent need. The current state-of-the-art methods for segmentation generally require sufficient amounts of annotated data for training. However, human expert annotation of such lesion on chest computed tomography (CT) scans is time-consuming and labor-intensive due to its heterogeneous appearance, ambiguous boundary, and large number of slices in 3-D CT images. Therefore, the purpose of this study is to present a novel annotation-efficient learning method for COVID-19 pneumonia lesion segmentation on CT. To make the best use of limited human expert annotation resources, we propose an error-aware unified semisupervised and active learning method. A novel error estimation network is proposed to estimate a voxelwise segmentation loss map, which is used to guide learning from unlabeled data for semisupervised learning and choose the most informative images to annotate next for active learning. Validation is carried out on segmenting pneumonia lesions in 110 chest CT scans. The experimental result demonstrates that the proposed method significantly boosts the segmentation accuracy given limited amount of human annotation, compared with a conventional fully supervised baseline (60.9% Dice to 72.0% at 30% labeled data). The performance is also competitive compared with other state-of-the-art annotation-efficient segmentation methods. The proposed method can significantly reduce the annotation effort needed to achieve accurate COVID-19 pneumonia lesion segmentation.},
  archive      = {J_TAI},
  author       = {Zhenghan Fang and Junjie Bai and Xinyu Guo and Xin Wang and Feng Gao and Hao-Yu Yang and Bin Kong and Ying Hou and Kunlin Cao and Qi Song and Jun Xia and Youbing Yin},
  doi          = {10.1109/TAI.2022.3147440},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {255-267},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Annotation-efficient COVID-19 pneumonia lesion segmentation using error-aware unified semisupervised and active learning},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble image explainable AI (XAI) algorithm for severe
community-acquired pneumonia and COVID-19 respiratory infections.
<em>TAI</em>, <em>4</em>(2), 242–254. (<a
href="https://doi.org/10.1109/TAI.2022.3153754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the onset of the COVID-19 pandemic in 2019, many clinical prognostic scoring tools have been proposed or developed to aid clinicians in the disposition and severity assessment of pneumonia. However, there is limited work that focuses on explaining techniques that are best suited for clinicians in their decision making. In this article, we present a new image explainability method named ensemble AI explainability (XAI), which is based on the SHAP and Grad-CAM++ methods. It provides a visual explanation for a deep learning prognostic model that predicts the mortality risk of community-acquired pneumonia and COVID-19 respiratory infected patients. In addition, we surveyed the existing literature and compiled prevailing quantitative and qualitative metrics to systematically review the efficacy of ensemble XAI, and to make comparisons with several state-of-the-art explainability methods (LIME, SHAP, saliency map, Grad-CAM, Grad-CAM++). Our quantitative experimental results have shown that ensemble XAI has a comparable absence impact (decision impact: 0.72, confident impact: 0.24). Our qualitative experiment, in which a panel of three radiologists were involved to evaluate the degree of concordance and trust in the algorithms, has showed that ensemble XAI has localization effectiveness (mean set accordance precision: 0.52, mean set accordance recall: 0.57, mean set &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;${{\bm{F}}_1}$&lt;/tex-math&gt;&lt;/inline-formula&gt; : 0.50, mean set IOU: 0.36) and is the most trusted method by the panel of radiologists (mean vote: 70.2%). Finally, the deep learning interpretation dashboard used for the radiologist panel voting will be made available to the community. Our code is available at https://github.com/IHIS-HealthInsights/Interpretation-Methods-Voting-dashboard .},
  archive      = {J_TAI},
  author       = {Lin Zou and Han Leong Goh and Charlene Jin Yee Liew and Jessica Lishan Quah and Gary Tianyu Gu and Jun Jie Chew and Mukundaram Prem Kumar and Christine Gia Lee Ang and Andy Wee An Ta},
  doi          = {10.1109/TAI.2022.3153754},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {242-254},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ensemble image explainable AI (XAI) algorithm for severe community-acquired pneumonia and COVID-19 respiratory infections},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Do preprocessing and class imbalance matter to the deep
image classifiers for COVID-19 detection? An explainable analysis.
<em>TAI</em>, <em>4</em>(2), 229–241. (<a
href="https://doi.org/10.1109/TAI.2022.3149971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a world withstanding the waves of a raging pandemic, respiratory disease detection from chest radiological images using machine-learning approaches has never been more important for a widely accessible and prompt initial diagnosis. A standard machine-learning disease detection workflow that takes an image as input and provides a diagnosis in return usually consists of four key components, namely input preprocessor, data irregularities (like class imbalance, missing and absent features, etc.) handler, classifier, and a decision explainer for better clarity. In this study, we investigate the impact of the three primary components of the disease-detection workflow leaving only the deep image classifier. We specifically aim to validate if the deep classifiers may significantly benefit from additional preprocessing and efficient handling of data irregularities in a disease-diagnosis workflow. To elaborate, we explore the applicability of seven traditional and deep preprocessing techniques along with four class imbalance handling approaches for a deep classifier, such as ResNet-50, in the task of respiratory disease detection from chest radiological images. While deep classifiers are more capable than their traditional counterparts, explaining their decision process is a significant challenge. Therefore, we also employ three gradient visualization algorithms to explain the decision of a deep classifier to understand how well each of them can highlight the key visual features of the different respiratory diseases.},
  archive      = {J_TAI},
  author       = {Arkaprabha Basu and Sourav Das and Sankha Subhra Mullick and Swagatam Das},
  doi          = {10.1109/TAI.2022.3149971},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {229-241},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Do preprocessing and class imbalance matter to the deep image classifiers for COVID-19 detection? an explainable analysis},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep-learning-based COVID-19 detection: Challenges and
future directions. <em>TAI</em>, <em>4</em>(2), 210–228. (<a
href="https://doi.org/10.1109/TAI.2022.3224097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19) is an ecumenical pandemic that has affected the whole world drastically by raising a global calamitous situation. Owing to this pernicious disease, millions of people have lost their lives. The scientists are still far from knowing how to tackle the coronavirus due to its multiple mutations found around the globe. The standard testing technique called polymerase chain reaction for the clinical diagnosis of COVID-19 is expensive and time consuming. However, to assist specialists and radiologists in COVID-19 detection and diagnosis, deep learning plays an important role. Many research efforts have been done that leverage deep learning techniques and technologies for the identification or categorization of COVID-19-positive patients, and these techniques are proved to be a powerful tool that can automatically detect or diagnose COVID-19 cases. In this article, we identify significant challenges regarding deep-learning-based systems and techniques that use different medical imaging modalities, including cough and breadth, chest X-ray, and computed tomography, to combat COVID-19 outbreak. We also pinpoint important research questions for each category of challenges.},
  archive      = {J_TAI},
  author       = {Qurat-ul-Ain Arshad and Wazir Zada Khan and Faisal Azam and Muhammad Khurram Khan},
  doi          = {10.1109/TAI.2022.3224097},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {210-228},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep-learning-based COVID-19 detection: Challenges and future directions},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online streaming features causal discovery algorithm based
on partial rank correlation. <em>TAI</em>, <em>4</em>(1), 197–208. (<a
href="https://doi.org/10.1109/TAI.2022.3151034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aimedat the problem of dynamic causal discovery in the era of artificial intelligence, this article combines partial rank correlation coefficients and streaming features in the field of Bayesian network structure learning and proposes a new online streaming feature causal discovery algorithm based on partial rank correlation named the partial rank casual discovery streaming feature based algorithm. This algorithm is not only suitable for Bayesian causal structure learning in dynamic feature spaces generated by sequential streams of features but can also effectively process multivariate linear Gaussian and nonlinear non-Gaussian data. We present three main contributions. First, for arbitrarily distributed datasets, which can be generated by the additive noise model, we proved that the partial rank correlation coefficient can be used as the criterion for the conditional independence test and explored the distribution of corresponding statistics. Second, the PCSDSF algorithm redefined the relevance based on partial rank statistic prospects and then redefined conditional dependence or independence. This method can significantly reduce the number of conditional independence tests and achieves a good time performance. Finally, theoretical analysis and several experiments proved the reliability of the algorithm. A simulation showed that on average, the PCSDSF algorithm outperforms existing algorithms in terms of both the accuracy and time performance.},
  archive      = {J_TAI},
  author       = {Jing Yang and Liufeng Jiang and Anbo Shen and Aiguo Wang},
  doi          = {10.1109/TAI.2022.3151034},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {197-208},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Online streaming features causal discovery algorithm based on partial rank correlation},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid method to measure distribution consistency of
mixed-attribute datasets. <em>TAI</em>, <em>4</em>(1), 182–196. (<a
href="https://doi.org/10.1109/TAI.2022.3151724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random sample partition (RSP) is a newly developed data management and processing model for Big Data processing and analysis. To apply the RSP model for Big Data computation tasks, it is very important to measure the distribution consistency of different datasets. Existing measurement methods for continuous-attribute and discrete-attribute datasets cannot directly deal with mixed-attribute datasets. In this article, we design a hybrid method to measure the distribution consistency among different mixed-attribute datasets by using a multilayer extreme learning machine (MLELM) and the generalized maximum mean discrepancy (GMMD) criterion, abbreviated as MLELM-GMMD. MLELM is first used to transform original mixed-attribute datasets into corresponding deep encoding datasets. Then, the GMMD criterion is applied to check the distribution consistency of the deep encoding datasets. Four experiments have been done to validate the feasibility and effectiveness of MLELM-GMMD, i.e., the impact of MLELM on the amount of information during mixed-attribute data transformation, the impact of MLELM on distributions of mixed-attribute data, the distribution consistencies of RSP and non-RSP data blocks, and the comparison with other measurement methods. Experimental results show that the proposed MLELM-GMMD method can measure the distribution consistency of mixed-attribute datasets more accurately than one-hot encoding-based methods.},
  archive      = {J_TAI},
  author       = {Yulin He and Xuan Ye and Defa Huang and Philippe Fournier-Viger and Joshua Zhexue Huang},
  doi          = {10.1109/TAI.2022.3151724},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {182-196},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A hybrid method to measure distribution consistency of mixed-attribute datasets},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Long short-term memory auto-encoder-based position
prediction model for fixed-wing UAV during communication failure.
<em>TAI</em>, <em>4</em>(1), 173–181. (<a
href="https://doi.org/10.1109/TAI.2022.3153763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAV&#39;s) safe flight is one of the most important tasks of ground control stations (GCS). However, sometimes due to communication failure between ground data terminal (GDT) and UAV, GCS is not able to ensure a safe flight, which could be hazardous. In such conditions, the accurate UAV position finding prediction becomes essential to serve as sustenance for UAVs. Hence, this article proposes a UAV&#39;s position finding prediction model using a deep neural network application for successfully pointing the GDT toward UAV in real time. The proposed work consists of three components, namely data preprocessing , long short-term memory autoencoder (LSTM-AE)- based deep learning model , and mathematical formulation to calculate the ground radar look angle. First, we use different preprocessing techniques on a historical raw dataset for better visualization and generate patterns. To achieve this, Tensor flow along with Keras packages have been used. Next, an LSTM autoencoder deep learning model is applied to the same dataset in order to predict an accurate 4-D position of the UAV. Finally, the output from the LSTM autoencoder model is used in a mathematical model to calculate the GDT look-angles called Azimuth and Elevation. Both the parameters have been used to point the GDT toward the predicted point on the Airspace. We evaluate our proposed model against famous evaluation matrices, namely mean absolute error, mean square error, and root-mean-square error to validate the experimental results.},
  archive      = {J_TAI},
  author       = {Ravi Kant and Poonam Saini and Julee Kumari},
  doi          = {10.1109/TAI.2022.3153763},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {173-181},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Long short-term memory auto-encoder-based position prediction model for fixed-wing UAV during communication failure},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Siamese cross-domain tracker design for seamless tracking of
targets in RGB and thermal videos. <em>TAI</em>, <em>4</em>(1), 161–172.
(<a href="https://doi.org/10.1109/TAI.2022.3151307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal (RGB and thermal) applications are swiftly gaining importance in the computer vision community with advancements in self-driving cars, robotics, Internet of Things, and surveillance applications. Both the modalities have complementary performance depending on illumination constraints. Hence, a judicious combination of both modalities will result in robust RGBT systems capable of all-day all-weather applications. Several studies have been proposed in the literature for integrating the multimodal sensor data for object tracking applications. Most of the proposed networks try to delineate the information into modality-specific and modality shared features and attempt to exploit the modality shared features in enhancing the modality specific information. In this work, we propose a novel perspective to this problem using a Siamese inspired network architecture. We design a custom Siamese cross-domain tracker architecture and fuse it with a mean shift tracker to drastically reduce the computational complexity. We also propose a constant false alarm rate inspired coasting architecture to cater for real-time track loss scenarios. The proposed method presents a complete and robust solution for object tracking across domains with seamless track handover for all-day all-weather operation. The algorithm is successfully implemented on a Jetson-Nano, the smallest graphics processing unit (GPU) board offered by NVIDIA Corporation.},
  archive      = {J_TAI},
  author       = {Chandrakanth V. and V. S. N. Murthy and Sumohana S. Channappayya},
  doi          = {10.1109/TAI.2022.3151307},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {161-172},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Siamese cross-domain tracker design for seamless tracking of targets in RGB and thermal videos},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). No-reference image quality assessment via multibranch
convolutional neural networks. <em>TAI</em>, <em>4</em>(1), 148–160. (<a
href="https://doi.org/10.1109/TAI.2022.3146804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {No-reference image quality assessment (NR-IQA) aims to evaluate image quality without using the original reference images. Since the early NR-IQA methods based on distortion types were only applicable to specific distortion scenarios, and lack of practicality, it is challenging to designing a universal NR-IQA method. In this article, a multibranch convolutional neural network (MB-CNN) based NR-IQA method is proposed, which includes a spatial-domain feature extractor, a gradient-domain feature extractor, and a weight mechanism. The spatial-domain feature extractor aims to extract the distortion features from the spatial domain. The gradient-domain feature extractor is used to guide the spatial-domain feature extractor to pay more attention to the distortions of the structure information. Particularly, the spatial-domain feature extractor uses the hierarchical feature merge module to realize multiscale feature representation, and the gradient-domain feature extractor uses pyramidal convolution to extract the multiscale structure information of the distorted image. In addition, a position vector is proposed to build the weight mechanism by considering the position relationships between patches and its entire image for improving the final prediction performance. We conduct the experiments on five representative databases: LIVE, TID2013, CSIQ, LIVE MD and Waterloo Exploration Database, and the experimental results show that the proposed NR-IQA method achieves the state-of-the-art performance, which demonstrate the effectiveness of our proposed NR-IQA method. The code ofthe proposed MB-CNN will be released at https://github.com/NUIST-Videocoding/MB-CNN .},
  archive      = {J_TAI},
  author       = {Zhaoqing Pan and Feng Yuan and Xu Wang and Long Xu and Xiao Shao and Sam Kwong},
  doi          = {10.1109/TAI.2022.3146804},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {148-160},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {No-reference image quality assessment via multibranch convolutional neural networks},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal feature selection for imbalanced text
classification. <em>TAI</em>, <em>4</em>(1), 135–147. (<a
href="https://doi.org/10.1109/TAI.2022.3144651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textual data suffers from two main problems, large number of features and class imbalance. Many conventional approaches and their variants exist in literature to solve both these problems. The classic synthetic minority oversampling technique (SMOTE) is the most explored technique for balancing the dataset. We introduced a new algorithm to balance the dataset, named distributed SMOTE (D_SMOTE), which overcomes the problem of lack of density and reducing the formation of small disjuncts. Further, another problem handled is the large number of features or high-dimensionality. To solve high-dimensionality, a novel feature selection technique is introduced known as modified biogeography-based optimization (M_BBO). The proposed model, M_BBO, performs modification in ranking of variables using feature weighting algorithm rather than randomly ranking. We have proposed two new expressions in D_SMOTE and one new expression in M_BBO. The extensive experimental results are computed out on four text classification datasets with four machine learning classifiers. The results are concluded using three performance measures, area under curve, G-mean, and F1-score. Our empirical and statistical observation for four class-imbalanced datasets shows that the proposed D_SMOTE outperforms the other similar oversampling technique. We have also compared our proposed algorithm, M_BBO+D_SMOTE, with other models on 17 imbalanced text classification datasets. Our model outperformed the other models in 14 datasets. We have also compared our model with bidirectional encoder representations from transformers. To validate the experimental analysis, statistical Friedman test is employed.},
  archive      = {J_TAI},
  author       = {Anshu Khurana and Om Prakash Verma},
  doi          = {10.1109/TAI.2022.3144651},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {135-147},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optimal feature selection for imbalanced text classification},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature subset selection with multi-scale fuzzy granulation.
<em>TAI</em>, <em>4</em>(1), 121–134. (<a
href="https://doi.org/10.1109/TAI.2022.3144242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a typical multigranularity data analysis model, multi-scale rough sets have attracted considerable attention in recent years. However, classical multi-scale rough sets and most of its extended models can only deal with discrete data, which limits its popularization and application. To overcome this problem, we investigate the fuzzy generalization of multi-scale rough sets as well as their application in feature selection for continuous data. To this end, a new type of decision systems, i.e., multi-scale fuzzy decision systems, is formalized to represent knowledge at different scales. Scaled fuzzy granules in terms of a family of scaled fuzzy relations are introduced, using which the granular structures of fuzzy lower and upper approximations are presented. A heuristic lattice-based optimal scale selection algorithm is then put forward from the viewpoint of maintaining the consistency of decision systems. Decision rules with strong generalization ability can be obtained by selecting appropriate scales. Finally, a forward feature selection algorithm was developed by means of the optimal scale to reduce redundant fuzzy relations. Extensive numerical experiments are further conducted to compare the proposed algorithm with some state-of-the-art algorithms. The experimental results show that our model can improve the generalization ability of fuzzy rough set, so as to be more feasible and effective.},
  archive      = {J_TAI},
  author       = {Zhehuang Huang and Jinjin Li},
  doi          = {10.1109/TAI.2022.3144242},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {121-134},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Feature subset selection with multi-scale fuzzy granulation},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multispace evolutionary search for large-scale optimization
with applications to recommender systems. <em>TAI</em>, <em>4</em>(1),
107–120. (<a href="https://doi.org/10.1109/TAI.2022.3156952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale optimization is vital in today’s artificial intelligence (AI) applications for extracting essential knowledge from huge volumes of data. In recent years, to improve the evolutionary algorithms used to solve optimization problems involving a large number of decision variables, many attempts have been made to simplify the problem solution space of a given problem for the evolutionary search. In the literature, the existing approaches can generally be categorized as decomposition-based methods and dimension-reduction-based methods. The former decomposes a large-scale problem into several smaller subproblems, while the latter transforms the original high-dimensional solution space into a low-dimensional space. However, it is worth noting that a given large-scale optimization problem may not always be decomposable, and it is also difficult to guarantee that the global optimum of the original problem is preserved in the reduced low-dimensional problem space. This article, thus, proposes a new search paradigm, namely the multispace evolutionary search, to enhance the existing evolutionary search methods for solving large-scale optimization problems. In contrast to existing approaches that perform an evolutionary search in a single search space, the proposed paradigm is designed to conduct a search in multiple solution spaces that are derived from the given problem, each possessing a unique landscape. The proposed paradigm makes no assumptions about the large-scale optimization problem of interest, such as that the problem is decomposable or that a certain relationship exists among the decision variables. To verify the efficacy of the proposed paradigm, comprehensive empirical studies in comparison to five state-of-the-art algorithms were conducted using the CEC2013 large-scale benchmark problems as well as an AI application in e-commerce, i.e., movie recommendation.},
  archive      = {J_TAI},
  author       = {Liang Feng and Qingxia Shang and Yaqing Hou and Kay Chen Tan and Yew-Soon Ong},
  doi          = {10.1109/TAI.2022.3156952},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {107-120},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multispace evolutionary search for large-scale optimization with applications to recommender systems},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive-learning-based generative adversarial network
for one-to-one voice conversion. <em>TAI</em>, <em>4</em>(1), 92–106.
(<a href="https://doi.org/10.1109/TAI.2022.3149858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voice conversion (VC) emerged as a significant domain of research in the field of speech synthesis in recent years due to its emerging application in voice-assistive technologies, such as automated movie dubbing speech-to-singing conversion, to name a few. VC deals with the conversion of the vocal style of one speaker to another speaker while keeping the linguistic contents unchanged. Nowadays, generative adversarial network (GAN) models are widely used for speech feature mapping from the source speaker to the target speaker. In this article, we propose an adaptive-learning-based GAN model, called ALGAN-VC, to improve the one-to-one VC of speakers. Our ALGAN-VC framework consists of some approaches to improve the speech quality and voice similarity between the source and target speakers. We incorporate a dense residual network architecture into the generator network for efficient speech feature learning between source and target speakers. Our framework also includes an adaptive learning mechanism to compute the loss function for the proposed model. Moreover, a boosted learning rate approach is incorporated to enhance the learning capability of the proposed model. The proposed model is tested on Voice Conversion Challenge 2016, 2018, and 2020 datasets along with our self-prepared Indian regional-language-based speech dataset. In addition, an emotional speech dataset is also considered for evaluating the model’s performance. The objective and subjective evaluations of the generated speech samples indicated that the proposed model elegantly performed the voice conversion task by achieving high speaker similarity and good speech quality.},
  archive      = {J_TAI},
  author       = {Sandipan Dhar and Nanda Dulal Jana and Swagatam Das},
  doi          = {10.1109/TAI.2022.3149858},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {92-106},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An adaptive-learning-based generative adversarial network for one-to-one voice conversion},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual attention and question categorization-based visual
question answering. <em>TAI</em>, <em>4</em>(1), 81–91. (<a
href="https://doi.org/10.1109/TAI.2022.3160418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering (VQA) aims at predicting an answer to a natural language question associated with an image. This work focuses on two important issues pertaining to VQA, which is a complex multimodal AI task: First, the task of answer prediction in a large output answer space, and second, to obtain enriched representation through cross-modality interactions. This work aims to address these two issues by proposing a dual attention (DA) and question categorization (QC)-based visual question answering model (DAQC-VQA). DAQC-VQA has three main network modules: First, a novel dual attention mechanism that helps toward the objective of obtaining an enriched cross-domain representation of the two modalities; second, a question classifier subsystem for identifying input (natural language) question category. The second module of question categorizer helps in reducing the answer search space; and third, a subsystem for predicting answer depending on the question category. All component networks of DAQC-VQA are trained in an end-to-end manner with a joint loss function. The performance of DAQC-VQA is evaluated on two widely used VQA datasets, viz., TDIUC and VQA2.0. Experimental results demonstrate competitive performance of DAQC-VQA against the recent state-of-the-art VQA models. An ablation analysis indicates that the enriched representation obtained using the proposed dual-attention mechanism helps improve performance.},
  archive      = {J_TAI},
  author       = {Aakansha Mishra and Ashish Anand and Prithwijit Guha},
  doi          = {10.1109/TAI.2022.3160418},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {81-91},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dual attention and question categorization-based visual question answering},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved explainable point cloud classifier (XPCC).
<em>TAI</em>, <em>4</em>(1), 71–80. (<a
href="https://doi.org/10.1109/TAI.2022.3150647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of objects from 3-D point clouds has become an increasingly relevant task across many computer-vision applications. However, few studies have investigated explainable methods. In this article, a new prototype-based and explainable classification method called eXplainable point cloud classifier (XPCC) is proposed. The XPCC method offers several advantages over previous explainable and nonexplainable methods. First, the XPCC method uses local densities and global multivariate generative distributions. Therefore, the XPCC provides comprehensive and interpretable object-based classification. Furthermore, the proposed method is built on recursive calculations, thus, is computationally very efficient. Second, the model learns continuously without the need for complete retraining and is domain transferable. Third, the proposed XPCC expands on the underlying learning method explainable deep neural networks (xDNN), and is specific to 3-D. As such, the following three new layers are added to the original xDNN architecture: 1) the 3-D point cloud feature extraction, 2) the global compound prototype weighting, and 3) the SoftMax function. Experiments were performed with the ModelNet40 benchmark, which demonstrated that XPCC is the only one to increase classification accuracy relative to the base algorithm when applied to the same problem. In addition, this article proposes a novel prototype-based visual representation that provides model- and object-based explanations. The prototype objects are superimposed to create a prototypical class representation of their data density within the feature space, called the compound prototype cloud. They allow a user to visualize the explainable aspects of the model and identify object regions that contribute to the classification in a human-understandable way.},
  archive      = {J_TAI},
  author       = {Nicholas I. Arnold and Plamen Angelov and Peter M. Atkinson},
  doi          = {10.1109/TAI.2022.3150647},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {71-80},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An improved explainable point cloud classifier (XPCC)},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group ensemble block: Subspace diversity improves
coarse-to-fine image retrieval. <em>TAI</em>, <em>4</em>(1), 60–70. (<a
href="https://doi.org/10.1109/TAI.2022.3206180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The random subspace method is an ensemble learning technique of great potential. However, its popularity does not match its potential because of its prohibitive cost and the lack of plug-and-play reusable modules. To fill this gap, we propose the random-subspace-based group ensemble block. The proposed ensemble block increases the diversity among subspaces by masking each sampled subspace. It also utilizes the diversity among a large number of subspaces, e.g., 1024, which is facilitated by combining the advantages of both averaging and concatenating strategies for merging different subspaces&#39; outcomes. In terms of cost, the proposed ensemble block contributes few parameters by parameter sharing, negligible computations by the linear transformation, and fast running speed by parallel processing for every subspace. In terms of reusability, it encapsulates every necessary step in one small block, and can be interchangeably used as a linear layer in neural networks. We show that our group ensemble block is complementary to existing methods for image retrieval such as class-level and instance-level discrimination, and can also be deployed for other tasks such as image classification. Experiments show that the proposed group ensemble block achieves the state-of-the-art accuracy on the CIFAR-100 and ImageNet datasets for the coarse-to-fine image retrieval problem, where the model is trained with coarse-level annotation (e.g., trees) and evaluated with fine-grained category (e.g., maple trees and oak trees).},
  archive      = {J_TAI},
  author       = {Zhihao Zhao and Shangqing Zhao and Samuel Cheng},
  doi          = {10.1109/TAI.2022.3206180},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {60-70},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Group ensemble block: Subspace diversity improves coarse-to-fine image retrieval},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of the machine learning algorithms for covid-19
case analysis. <em>TAI</em>, <em>4</em>(1), 44–59. (<a
href="https://doi.org/10.1109/TAI.2022.3142241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this article is to see how machine learning (ML) algorithms and applications are used in the COVID-19 inquiry and for other purposes. The available traditional methods for COVID-19 international epidemic prediction, researchers and authorities have given more attention to simple statistical and epidemiological methodologies. The inadequacy and absence of medical testing for diagnosing and identifying a solution is one of the key challenges in preventing the spread of COVID-19. A few statistical-based improvements are being strengthened to answer this challenge, resulting in a partial resolution up to a certain level. ML have advocated a wide range of intelligence-based approaches, frameworks, and equipment to cope with the issues of the medical industry. The application of inventive structure, such as ML and other in handling COVID-19 relevant outbreak difficulties, has been investigated in this article. The major goal of this article is to 1) Examining the impact of the data type and data nature, as well as obstacles in data processing for COVID-19. 2) Better grasp the importance of intelligent approaches like ML for the COVID-19 pandemic. 3) The development of improved ML algorithms and types of ML for COVID-19 prognosis. 4) Examining the effectiveness and influence of various strategies in COVID-19 pandemic. 5) To target on certain potential issues in COVID-19 diagnosis in order to motivate academics to innovate and expand their knowledge and research into additional COVID-19-affected industries.},
  archive      = {J_TAI},
  author       = {Shrikant Tiwari and Prasenjit Chanak and Sanjay Kumar Singh},
  doi          = {10.1109/TAI.2022.3142241},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {44-59},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A review of the machine learning algorithms for covid-19 case analysis},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale representation learning for image classification:
A survey. <em>TAI</em>, <em>4</em>(1), 23–43. (<a
href="https://doi.org/10.1109/TAI.2021.3135248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature representation has been widely used and developed recently. Multiscale features have led to remarkable breakthroughs for representation learning process in many computer vision tasks. This paper aims to provide a comprehensive survey of the recent multiscale representation learning achievements in classification tasks. Multiscale representation learning methods can be divided into two broad categories (multiscale geometric analysis and multiscale networks). Eleven kinds of multiscale geometric tools and seven kinds of multiscale networks are introduced. Some corresponding fundamental subproblems of these two broad categories are also described, including some concepts in representation process, specific representation methods with multiscale geometric analysis, and multiscale representation design strategies for networks. Then, the correlation between these two broad categories is illustrated, including respective characteristics, combination strategies, and characteristics of optimal representation. Some datasets and evaluation results are included to verify the effectiveness of the multiscale representation learning. Eventually, conclusion and future work are given, covering four directions [a) Choice and fusion; b) Self-adaption; c) Structure; and d) Generalization and proof].},
  archive      = {J_TAI},
  author       = {Licheng Jiao and Jie Gao and Xu Liu and Fang Liu and Shuyuan Yang and Biao Hou},
  doi          = {10.1109/TAI.2021.3135248},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {23-43},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiscale representation learning for image classification: A survey},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph representation learning meets computer vision: A
survey. <em>TAI</em>, <em>4</em>(1), 2–22. (<a
href="https://doi.org/10.1109/TAI.2022.3194869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph structure is a powerful mathematical abstraction, which can not only represent information about individuals but also capture the interactions between individuals for reasoning. Geometric modeling and relational inference based on graph data is a long-standing topic of interest in the computer vision community. In this article, we provide a systematic review of graph representation learning and its applications in computer vision. First, we sort out the evolution of representation learning on graphs, categorizing them into the nonneural network and neural network methods based on the way the nodes are encoded. Specifically, nonneural network methods, such as graph embedding and probabilistic graphical models, are introduced, and neural network methods, such as graph recurrent neural networks, graph convolutional networks, and variants of graph neural networks, are also presented. Then, we organize the applications of graph representation algorithms in various vision tasks (such as image classification, semantic segmentation, object detection, and tracking) for review and reference, and the typical graph construction approaches in computer vision are also summarized. Finally, on the background of biology and brain inspiration, we discuss the existing challenges and future directions of graph representation learning and computer vision.},
  archive      = {J_TAI},
  author       = {Licheng Jiao and Jie Chen and Fang Liu and Shuyuan Yang and Chao You and Xu Liu and Lingling Li and Biao Hou},
  doi          = {10.1109/TAI.2022.3194869},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {2-22},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Graph representation learning meets computer vision: A survey},
  volume       = {4},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
