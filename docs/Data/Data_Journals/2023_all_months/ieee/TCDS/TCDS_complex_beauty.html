<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcds---196">TCDS - 196</h2>
<ul>
<li><details>
<summary>
(2023). EMG-based cross-subject silent speech recognition using
conditional domain adversarial network. <em>TCDS</em>, <em>15</em>(4),
2282–2290. (<a href="https://doi.org/10.1109/TCDS.2023.3316701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning techniques have achieved great success in electromyography (EMG) decoding, but EMG-based cross-subject silent speech recognition (SSR) received less attention because of its high individual variability. Therefore, this article explores the field of cross-subject SSR to improve the recognition performance of EMG data collected from new subjects. First, this article reports on applying time-series features and 1-D convolutional neural networks (1D-CNNs) for cross-subject SSR. Second, this article proposes using a conditional domain adversarial network (CDAN) to solve the problem of reduced cross-subject SSR accuracy in the few samples’ data sets. It innovatively integrates the maximum mean difference (MMD) loss to get an improved CDAN (ICDAN). While 1D-CNN is a feature extraction network that can meet the needs of cross-subject SSR in large data sets, the recognition effect will be weakened in small data sets. Adding an ICDAN network after the feature extraction network can improve the problem of data distribution differences between the two domains, and further enhance recognition performance. The results show that the 1D-CNN model based on time-series features yields better results in the SSR of new subjects, and the ICDAN model can further improve the classification accuracy of cross-subjects in a few sample data sets by 14.88%.},
  archive      = {J_TCDS},
  author       = {Yakun Zhang and Huihui Cai and Jinghan Wu and Liang Xie and Minpeng Xu and Dong Ming and Ye Yan and Erwei Yin},
  doi          = {10.1109/TCDS.2023.3316701},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2282-2290},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EMG-based cross-subject silent speech recognition using conditional domain adversarial network},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). D2IFLN: Disentangled domain-invariant feature learning
networks for domain generalization. <em>TCDS</em>, <em>15</em>(4),
2269–2281. (<a href="https://doi.org/10.1109/TCDS.2023.3264615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) aims to learn a model that generalizes well to an unseen test distribution. Mainstream methods follow the domain-invariant representational learning philosophy to achieve this goal. However, due to the lack of priori knowledge to determine which features are domain specific and task-independent, and which features are domain invariant and task relevant, existing methods typically learn entangled representations, limiting their capacity to generalize to the distribution-shifted target domain. To address this issue, in this article, we propose novel disentangled domain-invariant feature learning networks (D2IFLN) to realize feature disentanglement and facilitate domain-invariant feature learning. Specifically, we introduce a semantic disentanglement network and a domain disentanglement network, disentangling the learned domain-invariant features from both domain-specific class-irrelevant features and domain-discriminative features. To avoid the semantic confusion in adversarial learning for domain-invariant feature learning, we further introduce a graph neural network to aggregate different domain semantic features during model training. Extensive experiments on three DG benchmarks show that the proposed D2IFLN performs better than the state of the art.},
  archive      = {J_TCDS},
  author       = {Zhengfa Liu and Guang CHen and Zhijun Li and Sanqing Qu and Alois Knoll and Changjun Jiang},
  doi          = {10.1109/TCDS.2023.3264615},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2269-2281},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {D2IFLN: Disentangled domain-invariant feature learning networks for domain generalization},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatiotemporal relationship cognitive learning for
multirobot air combat. <em>TCDS</em>, <em>15</em>(4), 2254–2268. (<a
href="https://doi.org/10.1109/TCDS.2023.3250819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {cognition is crucial to learning-based multirobot systems (MRSs). As an advanced application of MRSs for fierce confrontation, the relationships among autonomous air combat robots inherently present complex time-varying characteristics, which makes relationship cognition even more difficult. However, previous studies have only focused on spatial cooperative relationships, thus ignoring the potential impact of the temporal dynamics of relationships on long-term cooperative behaviors. To tackle this drawback, we propose a novel multiagent deep reinforcement learning (MADRL)-based autonomous air combat robots collaboration algorithm, called spatiotemporal aerial robots relationship co-optimization (STARCO). STARCO formulates the complex dynamic relationship cognition problem into a spatiotemporal deep graph neural network (GNN) learning problem. On this basis, we overcome the limitations of previous methods, by accurately capturing the key spatiotemporal patterns from aggressive air combat, and enable global collaborative decision making through joint strategy optimization. An empirical study shows that STARCO outperforms several state-of-the-art MARL baselines by 24.6% in learning performance. We also demonstrate that STARCO is capable of evolving various cooperative strategies comparable to human expert knowledge.},
  archive      = {J_TCDS},
  author       = {Haiyin Piao and Yue Han and Shaoming He and Chao Yu and Songyuan Fan and Yaqing Hou and Chengchao Bai and Li Mo},
  doi          = {10.1109/TCDS.2023.3250819},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2254-2268},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Spatiotemporal relationship cognitive learning for multirobot air combat},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contour and enclosed region refining for contour-based
instance segmentation. <em>TCDS</em>, <em>15</em>(4), 2241–2253. (<a
href="https://doi.org/10.1109/TCDS.2023.3247100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current contour-based instance segmentation methods predict sets of vertexes to form contours to enclose object instances in images for realizing instance segmentation. Due to the inaccuracy of contour vertexes for describing object instances, mask decay, and contour decay issues arise, limiting the performances of contour-based instance segmentation methods. In order to address these issues, in this article, we propose to design a contour and enclosed region refining network module, named CORE2, to integrate basic contour-based instance segmentation methods to obtain high-quality instance segmentation results. Specifically, we adopt a graph convolutional network to utilize correlation among initially predicted contour vertexes for refinement to address the contour decay issue. And, we predict and assemble a set of boundary-aware heatmaps to eliminate external regions enclosed within predicted object instance contours to relieve the mask decay problem. Furthermore, we propose several improvements that can be made to a basic contour-based instance segmentation method, i.e., polar generalized intersection over union loss, internal center, and hard sample polar centerness. Finally, extensive experiments are conducted on the COCO data set to evaluate the effectiveness of the proposed method. Experimental results show that our method can achieve 39.8 mean average precision on the COCO data set, which outperforms state-of-the-art contour-based instance segmentation methods.},
  archive      = {J_TCDS},
  author       = {Wenchao Gu and Shuang Bai},
  doi          = {10.1109/TCDS.2023.3247100},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2241-2253},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Contour and enclosed region refining for contour-based instance segmentation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network analysis on cortical morphometry in first-episode
schizophrenia. <em>TCDS</em>, <em>15</em>(4), 2228–2240. (<a
href="https://doi.org/10.1109/TCDS.2023.3245600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {First-episode schizophrenia (FES) results in abnormality of brain connectivity at different levels. Despite some successful findings on functional and structural connectivity of FES, relatively few studies have been focused on morphological connectivity, which may provide a potential biomarker for FES. In this study, we aim to investigate cortical morphological connectivity in FES. T1-weighted magnetic resonance image data from 92 FES patients and 106 healthy controls (HCs) are analyzed. We parcellate brain into 68 cortical regions, calculate the averaged thickness and surface area of each region, construct undirected networks by correlating cortical thickness or surface area measures across 68 regions for each group, and finally compute a variety of network-related topology characteristics. Our experimental results show that both the cortical thickness network and the surface area network in two groups are small-world networks; that is, those networks have high-clustering coefficients and low-characteristic path lengths. At certain network sparsity levels, both the cortical thickness network and the surface area network of FES have significantly lower clustering coefficients and local efficiencies than those of HC, indicating FES-related abnormalities in local connectivity and small-worldness. These abnormalities mainly involve the frontal, parietal, and temporal lobes. Further regional analyses confirm significant group differences in the node betweenness of the posterior cingulate gyrus for both the cortical thickness network and the surface area network. Our work supports that cortical morphological connectivity, which is constructed based on correlations across subjects’ cortical thickness, may serve as a tool to study topological abnormalities in neurological disorders.},
  archive      = {J_TCDS},
  author       = {Mowen Yin and Weikai Huang and Zhichao Liang and Quanying Liu and Xiaoying Tang},
  doi          = {10.1109/TCDS.2023.3245600},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2228-2240},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Network analysis on cortical morphometry in first-episode schizophrenia},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bilateral teleoperation system with learning-based
cognitive guiding force. <em>TCDS</em>, <em>15</em>(4), 2214–2227. (<a
href="https://doi.org/10.1109/TCDS.2023.3245216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem involving a fuzzy logic system (FLM)-based bilateral teleoperation system with a unified impedance/admittance structure and learning-based assisted cognitive guiding force, enhancing a smooth transition between augmentation and autonomous motion on the master side. The impedance structure regulates the impedance and admittance via the parameters determined by the FLM and, besides the variable impedance during interaction with the environment, the fuzzy $Q$ -learning algorithm generates an assisted cognitive guiding force to guide the trainee operator in free space for reaching the interaction surface smoothly and steadily. The degeneration of tracking performance in the bilateral teleoperation system caused by time delay is analyzed and reduced via a mixed-type error. Experimental results verify the stability analyses, and under the proposed control scheme, the human–robot–environment interaction is smooth and stable.},
  archive      = {J_TCDS},
  author       = {Zhiqiang Ma and Dailiang Shi and Zhengxiong Liu and Jianhui Yu and Panfeng Huang},
  doi          = {10.1109/TCDS.2023.3245216},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2214-2227},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A bilateral teleoperation system with learning-based cognitive guiding force},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-related potential-based collaborative brain-computer
interface for augmenting human performance using a low-cost, custom
electroencephalogram hyperscanning infrastructure. <em>TCDS</em>,
<em>15</em>(4), 2202–2213. (<a
href="https://doi.org/10.1109/TCDS.2023.3245048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electroencephalogram (EEG) hyperscanning technique has been demonstrated to facilitate the applicability of a collaborative brain–computer interface (cBCI) to augment human performance with respect to the collective intelligence of multiple brains. However, assembling a hyperscanning platform with commercial products inevitably introduces practical and cost burdens regarding labor and hardware setup, hindering group scalability. This work thus explores how effectively a low-cost, custom-made EEG hyperscanning platform can be achieved by demonstrating a cBCI framework. This work quantifies brain–computer interface (BCI) performance in collaborative and single-brain scenarios applied to the EEG data set collected from three subjects simultaneously participating in a target-distractor differentiation task over ten days. This work also compares various brain-fusion scenarios with feature-extraction methods for multiple brains. Given the 30 pseudo brains (i.e., 3 subjects $\times10$ day sessions), the decision-level committee voting outperformed the single-brain BCI performance and considerably improved by leveraging more pseudo brains. The 27-brain setting achieved the best information transfer rate (ITR) of 116.6±5.6 bits/min, which was a nearly 817% enhancement over the single-brain ITR (12.7±9.2 bits/min). In addition, the cBCI decision augmented the actual button-pressing time by 25 ms. Such a low-cost, custom-made hyperscanning infrastructure economically and practically favors multiple-brain applications in a larger group.},
  archive      = {J_TCDS},
  author       = {Wei-Jen Chen and Yuan-Pin Lin},
  doi          = {10.1109/TCDS.2023.3245048},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2202-2213},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Event-related potential-based collaborative brain-computer interface for augmenting human performance using a low-cost, custom electroencephalogram hyperscanning infrastructure},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Alternated greedy-step deterministic policy gradient.
<em>TCDS</em>, <em>15</em>(4), 2190–2201. (<a
href="https://doi.org/10.1109/TCDS.2023.3242274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The greedy-step $Q$ -learning (GQL) can effectively accelerate the $Q$ -value updating process. However, since it is an improved version of $Q$ -learning, the problem of $Q$ -value overestimation also exists. Since there are in total two max operators used to iteratively calculate $Q$ -value in GQL, many existing solutions to reduce the $Q$ -value estimation bias are invalid for GQL. To address the issue, an alternated greedy-step update (AGU) framework that consists of two independent $Q$ -value estimators is proposed in this study. In the proposed AGU framework, one estimator is to determine the time step that can maximize the estimated $n$ -step return and the other estimator is to update the prior estimator using the target value calculated on the basis of the determined time step. The convergence of the AGU framework is proved in theoretical. In addition, an alternated greedy-step deterministic policy gradient (AGDPG) that can be applied to continuous-action tasks is proposed by combining the AGU framework with deep deterministic policy gradient (DDPG). Experiments of AGDPG on continuous-action tasks of the MuJoCo platform highlight its superior performance.},
  archive      = {J_TCDS},
  author       = {Xuesong Wang and Jiazhi Zhang and Yang Gu and Longyang Huang and Kun Yu and Yuhu Cheng},
  doi          = {10.1109/TCDS.2023.3242274},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2190-2201},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Alternated greedy-step deterministic policy gradient},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient graph convolution network for skeleton-based
dynamic hand gesture recognition. <em>TCDS</em>, <em>15</em>(4),
2179–2189. (<a href="https://doi.org/10.1109/TCDS.2023.3242988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic hand gesture recognition has evolved as a prominent topic of computer vision research due to its vast applications in human–computer interaction, robotics, and other domains. Although there are numerous related recognition studies, the state-of-the-art (SOTA) methods are over-parametrized. Specifically, the number of model parameters is quite large, which results in high-computational costs. This work, referring to Song’s ResGCN, designs an efficient and lightweight graph convolutional network (GCN), named ResGCNeXt. ResGCNeXt learns rich features from skeleton information and achieves high accuracy with less number of model parameters. First, three data preprocessing strategies according to motion analysis are designed to provide sufficient features for the recognition model. Then, an efficient GCN structure combining bottleneck and group convolution is designed to reduce the number of model parameters without loss of accuracy. Furthermore, an attention block called SENet-part attention (SEPA) is added to improve channel and spatial feature learning. This study is validated on two benchmark data sets, and the experimental results show that ResGCNeXt provides competitive performance, especially, in significantly reducing the number of model parameters. Compared to HAN-2S, which is one of the best SOTA methods, our method has half model parameters and a 0.3% higher recognition rate.},
  archive      = {J_TCDS},
  author       = {Sheng-Hui Peng and Pei-Hsuan Tsai},
  doi          = {10.1109/TCDS.2023.3242988},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2179-2189},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An efficient graph convolution network for skeleton-based dynamic hand gesture recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Obstacle avoidance learning for robot motion planning in
human–robot integration environments. <em>TCDS</em>, <em>15</em>(4),
2169–2178. (<a href="https://doi.org/10.1109/TCDS.2023.3242373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the human–robot integration environment, it is essential for mobile robots to pass through the crowd and complete the navigation task smoothly. However, the current mainstream robot navigation algorithms only treat pedestrians as dynamic obstacles and passively avoid pedestrians in local planning. When encountering fast-moving pedestrians, local path planning often fails, causing the robot to stagnate, spin or shake in place, which in turn reduces the navigation efficiency and results in unnatural navigation trajectories. To address this problem, it is desirable for the robot to find a safe and convenient temporary target to avoid the collision with fast-moving pedestrians. In this article, we propose an obstacle avoidance learning method with the temporary target for the robot motion planning in the human–robot integration environment. The temporary target distribution is learned from imitations by using a conditional variational autoencoder (CVAE) framework, whereby the dynamic scenario information, including pedestrian information, the environmental information, and the robot information, is considered as the generation conditions. With the proposed method, the mobile robot first navigates to the temporary target area, and then plans the path toward the final target point. Experimental studies reveal that the proposed method can achieve satisfactory performance with respect to different scenario conditions.},
  archive      = {J_TCDS},
  author       = {Yang Hong and Zhiyu Ding and Yuan Yuan and Wenzheng Chi and Lining Sun},
  doi          = {10.1109/TCDS.2023.3242373},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2169-2178},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Obstacle avoidance learning for robot motion planning in Human–Robot integration environments},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low resource-reallocation defense strategies for repeated
security games with no prior knowledge and limited observability.
<em>TCDS</em>, <em>15</em>(4), 2156–2168. (<a
href="https://doi.org/10.1109/TCDS.2023.3241364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article takes into account general repeated security games with no prior knowledge, i.e., the game payoffs and the attacker’s behavior model are unknown and limited observability. Besides the traditional “regret” criterion, “reallocation times” is introduced as an additional criterion that provides a more comprehensive evaluation of the defense strategies. For such games, a novel random-walk perturbations with uniform exploration (RWP-UE) algorithm is proposed and we deduce the corresponding upper bound of the expected regret and expected reallocation times. Theoretical analysis shows that the RWP-UE algorithm achieves not only low regret with the same magnitude as existing achievements but also fewer reallocation times. Experiments are carried out against four types of attackers, and the results illustrate that the RWP-UE algorithm achieves superior performance.},
  archive      = {J_TCDS},
  author       = {Jin Zhu and Jinglong Zhang and Qiang Ling and Geir E. Dullerud},
  doi          = {10.1109/TCDS.2023.3241364},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2156-2168},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Low resource-reallocation defense strategies for repeated security games with no prior knowledge and limited observability},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sim-to-real learning-based framework for contact-rich
assembly by utilizing CycleGAN and force control. <em>TCDS</em>,
<em>15</em>(4), 2144–2155. (<a
href="https://doi.org/10.1109/TCDS.2023.3237734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (RL) has succeeded in robotic manipulation applications. However, training robots in the real world is challenging due to sample efficiency and safety concerns. Sim-to-real transfer has been proposed to address the aforementioned concerns but introduces the reality gap. In this work, we introduce a sim-to-real learning framework for vision-based assembly tasks and perform training in a simulation environment by employing raw image inputs from a single camera to address the aforementioned issues. We build a robotic Peg-in-Hole (PiH) training environment that requires low-level simulation knowledge. We also present a domain adaptation method based on a cycle-consistent generative adversarial network (CycleGAN) and a force control transfer approach to bridge the reality gap. The proposed framework, trained in a simulation environment with different environmental scenes, can be successfully transferred to a real PiH setup with a UR5e robot. We then reproduce these results with a Diana7 robot and different peg shapes to verify the generalization ability of the framework.},
  archive      = {J_TCDS},
  author       = {Yunlei Shi and Chengjie Yuan and Athanasios Tsitos and Lin Cong and Hamid Hadjar and Zhaopeng Chen and Jianwei Zhang},
  doi          = {10.1109/TCDS.2023.3237734},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2144-2155},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A sim-to-real learning-based framework for contact-rich assembly by utilizing CycleGAN and force control},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). YOLO-MS: Multispectral object detection via feature
interaction and self-attention guided fusion. <em>TCDS</em>,
<em>15</em>(4), 2132–2143. (<a
href="https://doi.org/10.1109/TCDS.2023.3238181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is essential for an autonomous driving sensing system. Since the light condition is changed in unconstrained scenarios, the detection accuracy based on visible images can be greatly degraded. Although the detection accuracy can be improved by fusing visible and infrared images, existing multispectral object detection (MOD) algorithms suffer from inadequate intermodal interaction and a lack of global dependence in the fusion approach. Thus, we propose an MOD framework called YOLO-MS by designing a feature interaction and self-attention fusion network (FISAFN) as the backbone network. Within the FISAFN, correlations between two modalities are extracted by the feature interaction module (FIM) for reconstructing the information components of each modality and enhancing capability of information exchange. To filter redundant features and enhance complementary features, long-range information dependence between two modalities are established by using a self-attention feature fusion module (SAFFM). Thus, a better information richness of the fused features can be achieved. Experimental results on the FLIR-aligned data set and the M3FD data set demonstrate that the proposed YOLO-MS performs favorably against state-of-the-art approaches, including feature-level fusion and pixel-level fusion. And further, the proposed YOLO-MS possesses good detection performance under diverse scene conditions.},
  archive      = {J_TCDS},
  author       = {Yumin Xie and Langwen Zhang and Xiaoyuan Yu and Wei Xie},
  doi          = {10.1109/TCDS.2023.3238181},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2132-2143},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {YOLO-MS: Multispectral object detection via feature interaction and self-attention guided fusion},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated reinforcement learning for collective navigation
of robotic swarms. <em>TCDS</em>, <em>15</em>(4), 2122–2131. (<a
href="https://doi.org/10.1109/TCDS.2023.3239815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancement of deep reinforcement learning (DRL) contributed to robotics by allowing automatic controller design. The automatic controller design is a crucial approach for designing swarm robotic systems, which require more complex controllers than a single robot system to lead a desired collective behavior. Although the DRL-based controller design method showed its effectiveness for swarm robotic systems, the reliance on the central training server is a critical problem in real-world environments where robot-server communication is unstable or limited. We propose a novel federated learning (FL)-based DRL training strategy federated learning DDPG (FLDDPG) for use in swarm robotic applications. Through the comparison with baseline strategies under a limited communication bandwidth scenario, it is shown that the FLDDPG method resulted in higher robustness and generalization ability into a different environment and real robots, while the baseline strategies suffer from the limitation of communication bandwidth. This result suggests that the proposed method can benefit swarm robotic systems operating in environments with limited communication bandwidth, e.g., in high radiation, underwater, or subterranean environments.},
  archive      = {J_TCDS},
  author       = {Seongin Na and Tomáš Rouček and Jiří Ulrich and Jan Pikman and Tomáš Krajník and Barry Lennox and Farshad Arvin},
  doi          = {10.1109/TCDS.2023.3239815},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2122-2131},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Federated reinforcement learning for collective navigation of robotic swarms},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepCPG policies for robot locomotion. <em>TCDS</em>,
<em>15</em>(4), 2108–2121. (<a
href="https://doi.org/10.1109/TCDS.2023.3250393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Central pattern generators (CPGs) form the neural basis of the observed rhythmic behaviors for locomotion in legged animals. The CPG dynamics organized into networks allow the emergence of complex locomotor behaviors. In this work, we take this inspiration for developing walking behaviors in multilegged robots. We present novel DeepCPG policies that embed CPGs as a layer in a larger neural network and facilitate end-to-end learning of locomotion behaviors in deep reinforcement learning (DRL) setup. We demonstrate the effectiveness of this approach on physics engine-based insectoid robots. We show that, compared to traditional approaches, DeepCPG policies allow sample-efficient end-to-end learning of effective locomotion strategies even in the case of high-dimensional sensor spaces (vision). We scale the DeepCPG policies using a modular robot configuration and multiagent DRL. Our results suggest that gradual complexification with embedded priors of these policies in a modular fashion could achieve nontrivial sensor and motor integration on a robot platform. These results also indicate the efficacy of bootstrapping more complex intelligent systems from simpler ones based on biological principles. Finally, we present the experimental results for a proof-of-concept insectoid robot system for which DeepCPG learned policies initially using the simulation engine and these were afterward transferred to real-world robots without any additional fine-tuning.},
  archive      = {J_TCDS},
  author       = {Aditya M. Deshpande and Eric Hurd and Ali A Minai and Manish Kumar},
  doi          = {10.1109/TCDS.2023.3250393},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2108-2121},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {DeepCPG policies for robot locomotion},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multidimensional time-series life cycle costs analysis of
intelligent substation. <em>TCDS</em>, <em>15</em>(4), 2099–2107. (<a
href="https://doi.org/10.1109/TCDS.2022.3147253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehensive and accurate life cycle costs (LCC) of intelligent substations are conducive to improving operation efficiency and reliability of power transmission. To reduce maintenance costs and enhance assets utilization, multiple dimensional time-series LCC analysis of intelligent substation based on principal component analysis long-term short-term memory (PCA-LSTM) is proposed in this article. This article first discusses the differences between smart substations and conventional substations in LCC analysis. Because it has an advantage over the recurrent neural network (RNN) in handling gradient vanishing and explosion, long-term–short-term memory (LSTM) is used instead of RNN. Considering that LCC is a comprehensive study which contains large number of variables, principal component analysis (PCA) is utilized to deal with the data dimensionality reduction preprocessing. Results show that PCA-LSTM performs better in root-mean-square error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE), as 0.182, 0.154 and 15.84%, respectively. In our case, the computation time is about half that of the pure LSTM. The calculation efficiency is $2.2\times $ compared to the original.},
  archive      = {J_TCDS},
  author       = {Yongtian Jia and Liming Ying},
  doi          = {10.1109/TCDS.2022.3147253},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2099-2107},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multidimensional time-series life cycle costs analysis of intelligent substation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enable fully customized assistance: A novel IMU-based motor
intent decoding scheme. <em>TCDS</em>, <em>15</em>(4), 2089–2098. (<a
href="https://doi.org/10.1109/TCDS.2021.3126001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trustworthy human-exoskeleton interaction essentially relates to determining the assistive force profile. Current methods of decoding human motor intent enable the customized determination of the assistive force profile by providing limited information of human kinetics. In this article, we propose and validate a novel motor intent decoding scheme that can enable a fully customized assistive force profile, where only inertial measurement units (IMUs) are used. First, we improve the robustness of the IMU-based kinematic estimation by sampling IMU measurements that well meet the hinge-joint assumption, and by online calibrating axes’ direction in order to avoid the post-hoc analysis of joint axes’ directions during the determination of the body-fixed coordinate frame. Second, using the calculated kinematics as input, we develop a computationally efficient dynamic model, through which kinetics of users can be calculated in real time. Finally, we leverage a cable-driven ankle exoskeleton method to validate the assistive performance of our motor intent decoding scheme. We perform experiments on ten healthy subjects to evaluate the accuracy of our algorithm, and the change of metabolic rate and muscle efforts under the exoskeleton’s assistance. The results show the improvement from determining the assistive force profile by nominal curves and the feasibility of our algorithm.},
  archive      = {J_TCDS},
  author       = {Chunzhi Yi and Shengping Zhang and Feng Jiang and Jie Liu and Zhen Ding and Chifu Yang and Huiyu Zhou},
  doi          = {10.1109/TCDS.2021.3126001},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2089-2098},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Enable fully customized assistance: A novel IMU-based motor intent decoding scheme},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SOZIL: Self-optimal zero-shot imitation learning.
<em>TCDS</em>, <em>15</em>(4), 2077–2088. (<a
href="https://doi.org/10.1109/TCDS.2021.3116604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot imitation learning has demonstrated its superiority to learn complex robotic tasks with less human participation. Recent studies show convincing performance under the condition that the robot follows the demonstration strictly by the learned inverse model. However, these methods are difficult to achieve satisfactory performance in imitation when the demonstration is suboptimal, and the learning of the learned inverse models is vulnerable to label ambiguity issues. In this article, we propose self-optimal zero-shot imitation learning (SOZIL) to tackle these problems. The contribution of SOZIL is twofold. First, goal consistency loss (GCL) is designed to learn the multistep goal-conditioned policy from exploration data. By directly using the goal state as supervision, GCL solves the label ambiguity problem caused by trajectory and action diversity. Second, estimation-based keyframe extraction (EKE) is developed to optimize demonstrations. We formulate the keyframe extraction process as a path optimization problem under suboptimal control. By predicting the performance of the learned policy in executing transitions of any two states, EKE creates a directed graph containing all candidate paths and extracts keyframes by solving the graph’s shortest path problem. Furthermore, the proposed method is evaluated with various simulated and real-world robotic manipulating experiments, such as cable harness assembly, rope manipulation, and block moving. Experimental results show that SOZIL achieves a superior success rate and manipulation efficiency than baselines.},
  archive      = {J_TCDS},
  author       = {Peng Hao and Tao Lu and Shaowei Cui and Junhang Wei and Yinghao Cai and Shuo Wang},
  doi          = {10.1109/TCDS.2021.3116604},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2077-2088},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SOZIL: Self-optimal zero-shot imitation learning},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging kernelized synergies on shared subspace for
precision grasping and dexterous manipulation. <em>TCDS</em>,
<em>15</em>(4), 2064–2076. (<a
href="https://doi.org/10.1109/TCDS.2021.3110406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manipulation in contrast to grasping is a trajectorial task that needs to use dexterous hands. Improving the dexterity of robot hands increases the controller complexity and thus requires to use the concept of postural synergies. Inspired from postural synergies, this research proposes a new framework called kernelized synergies that focuses on the reusability of the same subspace for precision grasping and dexterous manipulation. In this work, the computed subspace of postural synergies is parameterized by kernelized movement primitives (KMPs) to preserve its grasping and manipulation characteristics and allows its reuse for new objects. The grasp stability of the proposed framework is assessed with the force closure quality index, as a cost function. For performance evaluation, the proposed framework is initially tested on two different simulated robot hand models using the Syngrasp toolbox and experimentally, four complex grasping and manipulation tasks are performed and reported. Results confirm the hand agnostic approach of the proposed framework and its generalization to distinct objects irrespective of their dimensions.},
  archive      = {J_TCDS},
  author       = {Sunny Katyara and Fanny Ficuciello and Darwin G. Caldwell and Bruno Siciliano and Fei Chen},
  doi          = {10.1109/TCDS.2021.3110406},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2064-2076},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Leveraging kernelized synergies on shared subspace for precision grasping and dexterous manipulation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning in robot-assisted upper limb
rehabilitation: A focused review. <em>TCDS</em>, <em>15</em>(4),
2053–2063. (<a href="https://doi.org/10.1109/TCDS.2021.3098350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot-assisted rehabilitation, which can provide repetitive, intensive, and high-precision physics training, has a positive influence on the motor function recovery of stroke patients. Current robots need to be more intelligent and more reliable in clinical practice. Machine learning algorithms (MLAs) are able to learn from data and predict future unknown conditions, which is of benefit to improve the effectiveness of robot-assisted rehabilitation. In this article, we conduct a focused review on machine learning-based methods for robot-assisted upper limb rehabilitation. First, the current status of upper rehabilitation robots is presented. Then, we outline and analyze the designs and applications of MLAs for upper limb movement intention recognition, human–robot interaction control, and quantitative assessment of motor function. Meanwhile, we discuss the future directions of MLAs-based robotic rehabilitation. This review article provides a summary of MLAs for robotic upper limb rehabilitation and contributes to the design and development of future advanced intelligent medical devices.},
  archive      = {J_TCDS},
  author       = {Qingsong Ai and Zemin Liu and Wei Meng and Quan Liu and Sheng Q. Xie},
  doi          = {10.1109/TCDS.2021.3098350},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2053-2063},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Machine learning in robot-assisted upper limb rehabilitation: A focused review},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep reinforcement learning algorithm suitable for
autonomous vehicles: Double bootstrapped soft-actor–critic-discrete.
<em>TCDS</em>, <em>15</em>(4), 2041–2052. (<a
href="https://doi.org/10.1109/TCDS.2021.3092715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of modern society, autonomous systems have been broadly applied in people’s daily lives. Under the guidance of this trend, autonomous vehicles have gradually become popular. However, due to some adverse factors (such as insufficient computing force and limited communication bandwidth) in edge computing scenarios and the lack of autonomous decision-making ability, the safety of autonomous vehicles is not enough. It is a good solution to use a deep reinforcement learning (DRL) algorithm, which combines deep learning (DL) and reinforcement learning (RL), to provide a fast convergence speed and an appropriate decision-making ability. In this article, based on soft-actor–critic (SAC) and SAC-discrete (SAC-D), we propose a double bootstrapped SAC-D (DBSAC-D) algorithm. By introducing bootstrap, the ability to explore in action space is enhanced, the value of each action is accurately judged, the convergence process is accelerated and the consumption of the computing force is reduced. In addition, we also propose a novel sampling strategy, which balances the novelty and importance of the sampled data, and improves the training value of the sampled data to the network model. The experimental results show that our proposed algorithm achieves good performances in several traffic scenes and has a fast convergence speed.},
  archive      = {J_TCDS},
  author       = {Jiachen Yang and Jipeng Zhang and Meng Xi and Yutian Lei and Yiwen Sun},
  doi          = {10.1109/TCDS.2021.3092715},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2041-2052},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A deep reinforcement learning algorithm suitable for autonomous vehicles: Double bootstrapped soft-Actor–Critic-discrete},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance-based iterative learning control for
task-oriented rehabilitation: A pilot study in robot-assisted bilateral
training. <em>TCDS</em>, <em>15</em>(4), 2031–2040. (<a
href="https://doi.org/10.1109/TCDS.2021.3072096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active participation from human subjects can enhance the effectiveness of robot-assisted rehabilitation. Developing interactive control strategies for customized assistance is therefore essential for encouraging human–robot engagement. However, existing human–robot interactive control strategies lack precise evaluation indicators with effective convergence method to steadily and rapidly customize appropriate assistance during task-oriented training. This study proposes a performance-based iterative learning control algorithm for robot-assisted training, which aims at providing subject-specific robotic assistance to encourage active participation. Three performance indicators based on a Fugl-Meyer assessment (FMA) regression model are introduced to associate clinical scales with robot-based measures, and a fuzzy logic is employed for comprehensive performance evaluation. To increase efficient training time, a piecewise learning rate-based iterative law is applied to quickly converge to a subject-specific control parameter session by session. The proposed strategy is preliminarily estimated for a case of bilateral upper limb training with an end-effector-based robotic system. The experimental results with human subjects indicate that the proposed strategy can obtain appropriate parameters after only several iterations and adapt to random perturbations (like muscle fatigue).},
  archive      = {J_TCDS},
  author       = {Qing Miao and Zhijun Li and Kaiya Chu and Yudong Liu and Yuxin Peng and Shengquan Xie and Mingming Zhang},
  doi          = {10.1109/TCDS.2021.3072096},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2031-2040},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Performance-based iterative learning control for task-oriented rehabilitation: A pilot study in robot-assisted bilateral training},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). REAL-x—robot open-ended autonomous learning architecture:
Building truly end-to-end sensorimotor autonomous learning systems.
<em>TCDS</em>, <em>15</em>(4), 2014–2030. (<a
href="https://doi.org/10.1109/TCDS.2023.3270081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-ended learning is a core research field of developmental robotics and AI aiming to build learning machines and robots that can autonomously acquire knowledge and skills incrementally as infants. The first contribution of this work is to highlight the challenges posed by the previously proposed benchmark “REAL competition” fostering the development of truly open-ended learning robots. The benchmark involves a simulated camera-arm robot that: 1) in a first “intrinsic phase” acquires sensorimotor competence by autonomously interacting with objects and 2) in a second “extrinsic phase” is tested with tasks, unknown in the intrinsic phase, to measure the quality of previously acquired knowledge. The benchmark requires the solution of multiple challenges usually tackled in isolation, in particular exploration, sparse-rewards, object learning, generalization, task/goal self-generation, and autonomous skill learning. As a second contribution, the work presents a “REAL-X” architecture. Different systems implementing the architecture can solve different versions of the benchmark progressively releasing initial simplifications. The REAL-X systems are based on a planning approach that dynamically increases abstraction and on intrinsic motivations to foster exploration. Some systems achieves a good performance level in very demanding conditions. Overall, the REAL benchmark is shown to represent a valuable tool for studying open-ended learning in its hardest form.},
  archive      = {J_TCDS},
  author       = {Emilio Cartoni and Davide Montella and Jochen Triesch and Gianluca Baldassarre},
  doi          = {10.1109/TCDS.2023.3270081},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2014-2030},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {REAL-X—Robot open-ended autonomous learning architecture: Building truly end-to-end sensorimotor autonomous learning systems},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CBCL-PR: A cognitively inspired model for class-incremental
learning in robotics. <em>TCDS</em>, <em>15</em>(4), 2004–2013. (<a
href="https://doi.org/10.1109/TCDS.2023.3299755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For most real-world applications, robots need to adapt and learn continually with limited data in their environments. In this article, we consider the problem of few-shot incremental learning (FSIL), in which an AI agent is required to learn incrementally from a few data samples without forgetting the data it has previously learned. To solve this problem, we present a novel framework inspired by theories of concept learning in the hippocampus and the neocortex. Our framework represents object classes in the form of sets of clusters and stores them in memory. The framework replays data generated by the clusters of the old classes, to avoid forgetting when learning new classes. Our approach is evaluated on two object classification data sets resulting in state-of-the-art (SOTA) performance for class-incremental learning and FSIL. We also evaluate our framework for FSIL on a robot demonstrating that the robot can continually learn to classify a large set of household objects with limited human assistance.},
  archive      = {J_TCDS},
  author       = {Ali Ayub and Alan R. Wagner},
  doi          = {10.1109/TCDS.2023.3299755},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {2004-2013},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {CBCL-PR: A cognitively inspired model for class-incremental learning in robotics},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Speakers raise their hands and head during self-repairs in
dyadic conversations. <em>TCDS</em>, <em>15</em>(4), 1993–2003. (<a
href="https://doi.org/10.1109/TCDS.2023.3254808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People often encounter difficulties in building shared understanding during everyday conversation. The most common symptom of these difficulties are self-repairs, when a speaker restarts, edits, or amends their utterances mid-turn. Previous work has focused on the verbal signals of self-repair, i.e., speech disfluences (filled pauses, truncated words and phrases, and word substitutions or reformulations), and computational tools now exist that can automatically detect these verbal phenomena. However, face-to-face conversation also exploits rich nonverbal resources and previous research suggests that self-repairs are associated with distinct hand movement patterns. This article extends those results by exploring head and hand movements of both speakers and listeners using two motion parameters: 1) height (vertical position) and 2) 3-D velocity. The results show that speech sequences containing self-repairs are distinguishable from fluent ones: speakers raise their hands and head more (and move more rapidly) during self-repairs. We obtain these results by analyzing data from a corpus of 13 unscripted dialogs, and we discuss how these findings could support the creation of improved cognitive artificial systems for natural human–machine and human–robot interaction.},
  archive      = {J_TCDS},
  author       = {Elif Ecem Özkan and Patrick G. T. Healey and Tom Gurion and Julian Hough and Lorenzo Jamone},
  doi          = {10.1109/TCDS.2023.3254808},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1993-2003},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Speakers raise their hands and head during self-repairs in dyadic conversations},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kinematic primitives in action similarity judgments: A
human-centered computational model. <em>TCDS</em>, <em>15</em>(4),
1981–1992. (<a href="https://doi.org/10.1109/TCDS.2023.3240302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the role that kinematic features play in human action similarity judgments. The results of three experiments with human participants are compared with the computational model that solves the same task. The chosen model has its roots in developmental robotics and performs action classification based on learned kinematic primitives. The comparative experimental results show that both model and human participants can reliably identify whether two actions are the same or not. Specifically, most of the given actions could be similarity judged based on very limited information from a single feature domain (velocity or spatial). Both velocity and spatial features were however necessary to reach a level of human performance on evaluated actions. The experimental results also show that human performance on an action identification task indicated that they clearly relied on kinematic information rather than on action semantics. The results show that both the model and human performance are highly accurate in an action similarity task based on kinematic-level features, which can provide an essential basis for classifying human actions.},
  archive      = {J_TCDS},
  author       = {Vipul Nair and Paul Hemeren and Alessia Vignolo and Nicoletta Noceti and Elena Nicora and Alessandra Sciutti and Francesco Rea and Erik Billing and Mehul Bhatt and Francesca Odone and Giulio Sandini},
  doi          = {10.1109/TCDS.2023.3240302},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1981-1992},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Kinematic primitives in action similarity judgments: A human-centered computational model},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Do you want to make your robot warmer? Make it more
reactive! <em>TCDS</em>, <em>15</em>(4), 1971–1980. (<a
href="https://doi.org/10.1109/TCDS.2022.3222038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Endowing robots with the ability to respond appropriately to stimuli contributes to the perception of an illusion of “life” in robots, which is determinant for their acceptance as companions. This work aims to study how a series of bio-inspired reactive responses impact on the way in which participants perceive a social robot. In particular, the proposed system endows the robot with the ability to react to stimuli that are not only related to the current task but are also related to other external events. We conducted an experiment where the participants observed a video-recorded interaction with two robots: one was able to respond to both task-related and nontask-related events, while the other was only able to react to task-related events. To evaluate the experiment, we used the RoSAS questionnaire. The results yielded significant differences for two factors, showing that the addition of responses to nontask-related stimuli increased the robot’s warmth and competence.},
  archive      = {J_TCDS},
  author       = {Alexa Bertó Giménez and Enrique Fernández-Rodicio and Álvaro Castro-González and Miguel A. Salichs},
  doi          = {10.1109/TCDS.2022.3222038},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1971-1980},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Do you want to make your robot warmer? make it more reactive!},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual navigation subject to embodied mismatch.
<em>TCDS</em>, <em>15</em>(4), 1959–1970. (<a
href="https://doi.org/10.1109/TCDS.2023.3238840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the embodied visual navigation task, the agent navigates to a target location based on the visual observation it collects during the interaction with the environment. And, various approaches have been proposed to learn robust navigation strategies for this task. However, existing approaches assume that the action spaces in the training and testing phases are the same, which is usually not the case in reality. And, thus, it is difficult to directly apply these approaches on practical scenarios. In this article, we consider the situation where the action spaces in the training and testing phases are different, and a novel task of visual navigation subject to embodied mismatch is proposed. To solve the proposed task, we establish a two-stage robust adversary learning framework which can learn a robust policy to adapt the learned model to a new action space. In the first stage, an adversary training mechanism is used to learn a robust feature representation of the state. In the second stage, an adaptation training is used to transfer the learned strategy to a new action space with fewer training samples. Experiments of three types of embodied visual navigation tasks are conducted in 3-D indoor scenes demonstrating the effectiveness of the proposed approach.},
  archive      = {J_TCDS},
  author       = {Xinzhu Liu and Di Guo and Huaping Liu and Xinyu Zhang and Fuchun Sun},
  doi          = {10.1109/TCDS.2023.3238840},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1959-1970},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Visual navigation subject to embodied mismatch},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The role of object physical properties in human handover
actions: Applications in robotics. <em>TCDS</em>, <em>15</em>(4),
1948–1958. (<a href="https://doi.org/10.1109/TCDS.2022.3222088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observing how humans interact with each other and how they manipulate objects, offer insight on interaction mechanisms and how these are influenced by the physical properties of the used objects. These insights can support a more informed design of robot controllers meant to manipulate objects in collaboration with humans. In this work, we study human-to-human handovers of cups filled with various amount of liquid and textures and investigate to which extent the manipulation strategy depends on: 1) the individual preference; 2) whether the cup is filled with water or not; and 3) the cup physical properties. An analysis of the human giver’s hand acceleration, velocity, and position during the handover of different cups under two liquid level conditions, allows to distinguish between careful and not careful (normal) manipulation. We quantify to which extent the liquid level inside the cups influences the carefulness level of human manipulation. Finally, our study reveals that the cups’ physical properties, such as fragility, breakability, and deformability, play a role in shaping the carefulness of the manipulation. We apply these findings to human–robot scenarios by developing a robot controller capable of detecting, in realtime, if the human is being more careful than normal, and adapting the robot’s approach of interaction accordingly. Additionally, we show that the detection of a careful manipulation, depending on the experimental context, may provide the robot with information concerning the human partner’s intention or need for (manipulation) assistance.},
  archive      = {J_TCDS},
  author       = {Nuno Ferreira Duarte and Aude Billard and José Santos-Victor},
  doi          = {10.1109/TCDS.2022.3222088},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1948-1958},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The role of object physical properties in human handover actions: Applications in robotics},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive robot task learning: Human teaching proficiency
with different feedback approaches. <em>TCDS</em>, <em>15</em>(4),
1938–1947. (<a href="https://doi.org/10.1109/TCDS.2022.3186270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of versatile robot systems in diverse environments requires intuitive approaches for humans to flexibly teach them new skills. In our present work, we investigate different user feedback types to teach a real robot a new movement skill. We compare feedback as star ratings on an absolute scale for single roll-outs versus preference-based feedback for pairwise comparisons with respective optimization algorithms (i.e., a variation of co-variance matrix adaptation-evolution strategy (CMA-ES) and random optimization) to teach the robot the game of skill cup-and-ball. In an experimental investigation with users, we investigated the influence of the feedback type on the user experience of interacting with the different interfaces and the performance of the learning systems. While there is no significant difference for the subjective user experience between the conditions, there is a significant difference in learning performance. The preference-based system learned the task quicker, but this did not influence the users’ evaluation of it. In a follow-up study, we confirmed that the difference in learning performance indeed can be attributed to the human users’ performance.},
  archive      = {J_TCDS},
  author       = {Lukas Hindemith and Oleksandra Bruns and Arthur Maximilian Noller and Nikolas Hemion and Sebastian Schneider and Anna-Lisa Vollmer},
  doi          = {10.1109/TCDS.2022.3186270},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1938-1947},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Interactive robot task learning: Human teaching proficiency with different feedback approaches},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pick the right co-worker: Online assessment of cognitive
ergonomics in human–robot collaborative assembly. <em>TCDS</em>,
<em>15</em>(4), 1928–1937. (<a
href="https://doi.org/10.1109/TCDS.2022.3182811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–robot collaborative assembly systems enhance the efficiency and productivity of the workplace but may increase the workers’ cognitive demand. This article proposes an online and quantitative framework to assess the cognitive workload induced by the interaction with a co-worker, either a human operator or an industrial collaborative robot with different control strategies. The approach monitors the operator’s attention distribution and upper body kinematics benefiting from the input images of a low-cost stereo camera and cutting-edge artificial intelligence algorithms (i.e., head pose estimation and skeleton tracking). Three experimental scenarios with variations in workstation features and interaction modalities were designed to test the performance of our online method against state-of-the-art offline measurements. The results proved that our vision-based cognitive load assessment has the potential to be integrated into the new generation of collaborative robotic technologies. The latter would enable human cognitive state monitoring and robot control strategy adaptation for improving human comfort, ergonomics, and trust in automation.},
  archive      = {J_TCDS},
  author       = {Marta Lagomarsino and Marta Lorenzini and Pietro Balatti and Elena De Momi and Arash Ajoudani},
  doi          = {10.1109/TCDS.2022.3182811},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1928-1937},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Pick the right co-worker: Online assessment of cognitive ergonomics in Human–Robot collaborative assembly},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A platform for holistic embodied models of infant cognition,
and its use in a model of event processing. <em>TCDS</em>,
<em>15</em>(4), 1916–1927. (<a
href="https://doi.org/10.1109/TCDS.2022.3188152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most computational models of infant cognition focus on selected components of the cognitive system. In this article, we present a model that is more holistic in aspiration: a computational model of a “whole baby,” incorporating a graphical simulation of the baby’s body, and a multicomponent brain model. The model is realistic and fast enough to interact in real time with human users playing a caregiver role. This allows us to directly compare the behavior of the simulated baby interacting with users with that of real babies interacting with parents via video chat using touchscreen devices. To illustrate the model, we present components of the cognitive model involved in processing events, and in representing events in working memory and long-term memory. We focus on the processing of motion events, where an object moves from one location to another. We model how the baby perceives such events, and also how the baby produces them, through motor actions. We also introduce our framework for recording and annotating interactions between real babies and their parents, and we describe a preliminary evaluation of our motion event model against manually identified motion events from these interactions.},
  archive      = {J_TCDS},
  author       = {Mark Sagar and Alecia Moser and Annette M. E. Henderson and Sam Morrison and Nathan Pages and Alireza Nejati and Wan-Ting Yeh and Jonathan Conder and Alistair Knott and Khurram Jawed and Martin Takac},
  doi          = {10.1109/TCDS.2022.3188152},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1916-1927},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A platform for holistic embodied models of infant cognition, and its use in a model of event processing},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DisTop: Discovering a topological representation to learn
diverse and rewarding skills. <em>TCDS</em>, <em>15</em>(4), 1905–1915.
(<a href="https://doi.org/10.1109/TCDS.2023.3265200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An efficient way for a deep reinforcement learning (RL) agent to explore in sparse-rewards settings can be to learn a set of skills that achieves a uniform distribution of terminal states. We introduce DisTop, a new model that simultaneously learns diverse skills and focuses on improving rewarding skills. DisTop progressively builds a discrete topology of the environment using an unsupervised contrastive loss, a growing network, and a goal-conditioned policy. Using this topology, a state-independent hierarchical policy can select which skill to execute and learn. In turn, the new set of visited states allows an improved learned representation. Our experiments emphasize that DisTop is agnostic to the ground state representation and that the agent can discover the topology of its environment whether the states are high-dimensional binary data, images, or proprioceptive inputs. We demonstrate that this paradigm is competitive on MuJoCo benchmarks with state-of-the-art (SOTA) algorithms on both single-task dense rewards and diverse skill discovery without rewards. By combining these two aspects, we show that DisTop outperforms a SOTA hierarchical RL algorithm when rewards are sparse. We believe DisTop opens new perspectives by showing that bottom-up skill discovery combined with dynamic-aware representation learning can tackle different complex state spaces and reward settings.},
  archive      = {J_TCDS},
  author       = {Arthur Aubret and Laetitia Matignon and Salima Hassas},
  doi          = {10.1109/TCDS.2023.3265200},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1905-1915},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {DisTop: Discovering a topological representation to learn diverse and rewarding skills},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ontology to formalize a creative problem solving
activity. <em>TCDS</em>, <em>15</em>(4), 1891–1904. (<a
href="https://doi.org/10.1109/TCDS.2022.3210234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our study is set in an educational context: to better teach and assess 21st-century skills, such as computational thinking or creative problem solving, we propose to formalize a specific activity that involves these competencies. This activity, referred to as #CreaCube, is presented as an open-ended problem which consists of assembling a set of robotic cubes into an autonomous vehicle. We not only anchor our formalization in classical learning science frameworks but we also propose to draw on neuro-cognitive models to describe the observed behaviors of learners engaged in this activity. The chosen formalism is symbolic and is aligned on upper ontologies to ensure that the vocabulary is well specified. This allows for a better communication between the summoned research fields, namely, learning science, cognitive neuroscience, and computational modeling. Beyond this specification purpose, we suggest performing inferences using available reasoners to better guide the analysis of the observables collected during the experiments. This operationalization of a creative problem-solving activity is part of an exploratory research action. In addition, an effective proof of concept is described in this study.},
  archive      = {J_TCDS},
  author       = {Chloé Mercier},
  doi          = {10.1109/TCDS.2022.3210234},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1891-1904},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An ontology to formalize a creative problem solving activity},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concurrent skill composition using ensemble of primitive
skills. <em>TCDS</em>, <em>15</em>(4), 1879–1890. (<a
href="https://doi.org/10.1109/TCDS.2022.3177691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key characteristics of an open-ended cumulative learning agent is that it should use the knowledge gained from prior learning to solve future tasks. That characteristic is especially essential in robotics, as learning every perception-action skill from scratch is not only time consuming but may not always be feasible. In the case of reinforcement learning, this learned knowledge is called a policy. The lifelong learning agent should treat the policies of learned tasks as building blocks to solve those future tasks. One of the categorizations of tasks is based on its composition, ranging from primitive tasks to compound tasks that are either a sequential or concurrent combination of primitive tasks. Thus, the agent needs to be able to combine the policies of the primitive tasks to solve compound tasks, which are then added to its knowledge base. Inspired by modular neural networks, we propose an approach to compose policies for compound tasks that are concurrent combinations of disjoint tasks. Furthermore, we hypothesize that learning in a specialized environment leads to more efficient learning; hence, we create scaffolded environments for the robot to learn primitive skills for our mobile robot-based experiments. We then show how the agent can combine those primitive skills to learn solutions for compound tasks. That reduces the overall training time of multiple skills and creates a versatile agent that can mix and match the skills.},
  archive      = {J_TCDS},
  author       = {Paresh Dhakan and Kathryn Kasmarik and Philip Vance and Iñaki Rañó and Nazmul Siddique},
  doi          = {10.1109/TCDS.2022.3177691},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1879-1890},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Concurrent skill composition using ensemble of primitive skills},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From state transitions to sensory regularity: Structuring
uninterpreted sensory signals from naive sensorimotor experiences.
<em>TCDS</em>, <em>15</em>(4), 1864–1878. (<a
href="https://doi.org/10.1109/TCDS.2022.3226531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How could a naive agent build some internal, subjective, notions of continuity in its sensorimotor experiences? This is a key question for all sensorimotor approaches to perception when trying to make them face realistic interactions with an environment, including noise in the perceived sensations, errors in the generation of motor trajectories, or uncertainties in the agent’s internal representation of this interaction. This article proposes a detailed formalization, but also some experimental assessments, of the structure, a naive agent can leverage from its own uninterpreted sensorimotor flow to capture a subjective sensory continuity, making it able to discover some notions of closeness or regularities in its experience. The precise role of the agent’s actions is also questioned with respect to the spatial and temporal dynamics of its exploration of the environment. On this basis, the previous authors’ contribution on sensory prediction is extended to successfully handle noisy data in the agent’s sensorimotor flow.},
  archive      = {J_TCDS},
  author       = {Loïc Goasguen and Jean-Merwan Godon and Sylvain Argentieri},
  doi          = {10.1109/TCDS.2022.3226531},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1864-1878},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {From state transitions to sensory regularity: Structuring uninterpreted sensory signals from naive sensorimotor experiences},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and collision-free human–robot collaboration based
on intention and trajectory prediction. <em>TCDS</em>, <em>15</em>(4),
1853–1863. (<a href="https://doi.org/10.1109/TCDS.2022.3215093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–robot collaboration (HRC) is an important topic for manufacturing and household robotics. It is very challenging to ensure both efficiency and safety in HRC. This article presents an HRC pipeline that generates efficient and collision-free robot trajectories based on predictions of the human arm and hand (AH) motions. We train a recurrent neural network for AH trajectory prediction based on observed initial trajectory segments. To increase the accuracy of target estimation at an early stage, the observed and the predicted hand palm trajectories are combined to predict the current AH motion target using Gaussian mixture models (GMMs). An optimization-based trajectory generation algorithm is proposed to ensure the safety of the human while collaborating with the robot. The proposed system is validated in a shared-workspace scenario with human pick-and-place motions. The task can be safely and efficiently completed. The results demonstrate that our proposed pipeline can predict the human AH trajectory and estimate the motion target intended by the human accurately and early.},
  archive      = {J_TCDS},
  author       = {Jianzhi Lyu and Philipp Ruppel and Norman Hendrich and Shuang Li and Michael Görner and Jianwei Zhang},
  doi          = {10.1109/TCDS.2022.3215093},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1853-1863},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Efficient and collision-free Human–Robot collaboration based on intention and trajectory prediction},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trust in robot–robot scaffolding. <em>TCDS</em>,
<em>15</em>(4), 1841–1852. (<a
href="https://doi.org/10.1109/TCDS.2023.3235974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of robot trust in humans and other agents is not explored widely despite its importance for the near future human–robot symbiotic societies. Here, we propose that robots should trust partners that tend to reduce their computational load, which is analogous to human cognitive load. We test this idea by adopting an interactive visual recalling task. In the first set of experiments, the robot can get help from online instructors with different guiding strategies to decide which one it should trust based on the computational load it experiences during the experiments. The second set of experiments involves robot–robot interactions. Akin to the robot–online instructor case, the Pepper robot is asked to scaffold the learning of a less capable “infant” robot (Nao) with or without being equipped with the cognitive abilities of theory of mind and task experience memory to assess the contribution of these cognitive abilities to scaffolding performance. Overall, the results show that robot trust based on computational/cognitive load within a sequential decision-making framework leads to effective partner selection and robot–robot scaffolding. Thus, using the computational load incurred by the cognitive processing of a robot may serve as an internal signal for assessing the trustworthiness of interaction partners.},
  archive      = {J_TCDS},
  author       = {Murat Kirtay and Verena V. Hafner and Minoru Asada and Erhan Oztop},
  doi          = {10.1109/TCDS.2023.3235974},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1841-1852},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Trust in Robot–Robot scaffolding},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised multimodal word discovery based on double
articulation analysis with co-occurrence cues. <em>TCDS</em>,
<em>15</em>(4), 1825–1840. (<a
href="https://doi.org/10.1109/TCDS.2023.3307555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human infants acquire their verbal lexicon with minimal prior knowledge of language based on the statistical properties of phonological distributions and the co-occurrence of other sensory stimuli. This study proposes a novel fully unsupervised learning method for discovering speech units using phonological information as a distributional cue and object information as a co-occurrence cue. The proposed method can acquire words and phonemes from speech signals using unsupervised learning and utilize object information based on multiple modalities—vision, tactile, and auditory—simultaneously. The proposed method is based on the nonparametric Bayesian double articulation analyzer (NPB-DAA) discovering phonemes and words from phonological features, and multimodal latent Dirichlet allocation (MLDA) categorizing multimodal information obtained from objects. In an experiment, the proposed method showed higher word discovery performance than baseline methods. Words that expressed the characteristics of objects (i.e., words corresponding to nouns and adjectives) were segmented accurately. Furthermore, we examined how learning performance is affected by differences in the importance of linguistic information. Increasing the weight of the word modality further improved performance relative to that of the fixed condition.},
  archive      = {J_TCDS},
  author       = {Akira Taniguchi and Hiroaki Murakami and Ryo Ozaki and Tadahiro Taniguchi},
  doi          = {10.1109/TCDS.2023.3307555},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1825-1840},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Unsupervised multimodal word discovery based on double articulation analysis with co-occurrence cues},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Language-model-based paired variational autoencoders for
robotic language learning. <em>TCDS</em>, <em>15</em>(4), 1812–1824. (<a
href="https://doi.org/10.1109/TCDS.2022.3204452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human infants learn the language while interacting with their environment in which their caregivers may describe the objects and actions they perform. Similar to human infants, artificial agents can learn the language while interacting with their environment. In this work, first, we present a neural model that bidirectionally binds robot actions and their language descriptions in a simple object manipulation scenario. Building on our previous paired variational autoencoders (PVAEs) model, we demonstrate the superiority of the variational autoencoder over standard autoencoders by experimenting with cubes of different colors, and by enabling the production of alternative vocabularies. Additional experiments show that the model’s channel-separated visual feature extraction module can cope with objects of different shapes. Next, we introduce PVAE-BERT, which equips the model with a pretrained large-scale language model, i.e., bidirectional encoder representations from transformers (BERTs), enabling the model to go beyond comprehending only the predefined descriptions that the network has been trained on; the recognition of action descriptions generalizes to unconstrained natural language as the model becomes capable of understanding unlimited variations of the same descriptions. Our experiments suggest that using a pretrained language model as the language encoder allows our approach to scale up for real-world scenarios with instructions from human users.},
  archive      = {J_TCDS},
  author       = {Ozan Özdemir and Matthias Kerzel and Cornelius Weber and Jae Hee Lee and Stefan Wermter},
  doi          = {10.1109/TCDS.2022.3204452},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1812-1824},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Language-model-based paired variational autoencoders for robotic language learning},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable learned emergent communication for human–agent
teams. <em>TCDS</em>, <em>15</em>(4), 1801–1811. (<a
href="https://doi.org/10.1109/TCDS.2023.3236599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning interpretable communication is essential for multiagent and human–agent teams (HATs). In multiagent reinforcement learning for partially observable environments, agents may convey information to others via learned communication, allowing the team to complete its task. Inspired by human languages, recent works study discrete (using only a finite set of tokens) and sparse (communicating only at some time-steps) communication. However, the utility of such communication in HAT experiments has not yet been investigated. In this work, we analyze the efficacy of sparse–discrete methods for producing emergent communication that enables high agent-only and HAT performance. We develop agent-only teams that communicate sparsely via our scheme of Enforcers that sufficiently constrain communication to any budget. Our results show no loss or minimal loss of performance in benchmark environments and tasks. In HATs tested in benchmark environments, where agents have been modeled using the Enforcers, we find that a prototype-based method produces meaningful discrete tokens that enable human partners to learn agent communication faster and better than a one-hot baseline. Additional HAT experiments show that an appropriate sparsity level lowers the cognitive load of humans when communicating with teams of agents and leads to superior team performance.},
  archive      = {J_TCDS},
  author       = {Seth Karten and Mycal Tucker and Huao Li and Siva Kailas and Michael Lewis and Katia Sycara},
  doi          = {10.1109/TCDS.2023.3236599},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1801-1811},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Interpretable learned emergent communication for Human–Agent teams},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial special issue on emerging topics on
development and learning. <em>TCDS</em>, <em>15</em>(4), 1795–1800. (<a
href="https://doi.org/10.1109/TCDS.2023.3324805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue will encompass state-of-the-art research on emerging topics related to development and learning in natural and artificial systems. The primary focus of this special issue is to explore the facets of development and learning from a multidisciplinary perspective by convening researchers from the fields of computer science, robotics, psychology, and developmental studies. We invited researchers to share knowledge and research on how humans and animals develop sensing, reasoning, and actions, and how to exploit robots as research tools to test models of development and learning. We expected the submitted contributions to emphasize the interaction with social and physical environments and how cognitive and developmental capabilities can be transferred to computing systems and robotics. This approach is in harmony with the dual objectives of comprehending human and animal development while leveraging this understanding to enhance future intelligent technologies, particularly for robots that will engage in close interactions with humans.},
  archive      = {J_TCDS},
  author       = {Dingsheng Luo and Angelo Cangelosi and Alessandra Sciutti and Weiwei Wan and Ana Tanevska},
  doi          = {10.1109/TCDS.2023.3324805},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1795-1800},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial special issue on emerging topics on development and learning},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spatiotemporal channel attention residual network with
extended series mean amplitude spectrum for epilepsy detection.
<em>TCDS</em>, <em>15</em>(4), 1783–1794. (<a
href="https://doi.org/10.1109/TCDS.2022.3232121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately detecting electroencephalogram (EEG) signals in a specific period before the epileptic seizure can effectively predict epilepsy and reduce the harm caused by epilepsy to the patient. However, the current research rarely pays attention to the influence of spatiotemporal features on the detection of EEG signals before seizures. To accurately predict epilepsy before seizures, this article proposes a spatiotemporal channel attention residual network (STCARN) with extended series mean amplitude spectrum (MAS) of EEG signals. Specifically, the extended series MAS feature representation is first developed to rationally combine the temporal relevance of multiple MAS and the spatial relevance of EEG channels, fully representing the related activities of the brain. Furthermore, STCARN is proposed to extract the spatiotemporal information of extended series MASs by rationally fusing residual convolutional structure, channel attention mechanism, and recurrent network structure, which significantly improves the performance of epilepsy detection. STCARN is evaluated in the three-classification and five-classification tasks for epilepsy detection. The results indicate that STCARN achieves 99.98% and 99.51% accuracy in the two tasks, respectively.},
  archive      = {J_TCDS},
  author       = {Qi Wang and Chenxi Huang and Qingpeng Zeng and Chunquan Li and Ting Shu},
  doi          = {10.1109/TCDS.2022.3232121},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1783-1794},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A spatiotemporal channel attention residual network with extended series mean amplitude spectrum for epilepsy detection},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individual-level fMRI segmentation based on graphs.
<em>TCDS</em>, <em>15</em>(4), 1773–1782. (<a
href="https://doi.org/10.1109/TCDS.2023.3281271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the high complexity of fMRI data and the great spatial dependence of existing methods, a whole-brain functional segmentation algorithm with low computational overhead and low spatial structure dependence is proposed for individual-level fMRI segmentation in 3-D space. First, the spatial information and functional connectivity of each voxel in fMRI are utilized for presegmentation to create compact and functionally consistent super voxels, then extracts features, such as average spatial coordinates and average time series at the super voxel level to reduce the computational effort of the segmentation algorithm, and performs segmentation in a cut-free manner to obtain the optimal segmentation graph by minimizing the energy function. The results of contrast experiment demonstrated that the algorithm fully exploits the connectivity information of fMRI for segmentation, relies less on the spatial structure, and achieves better functional segmentation results, which is an effective whole-brain functional segmentation method.},
  archive      = {J_TCDS},
  author       = {Kevin W. Tong and Xiao-Yan Zhao and Yong-Xia Li and Ping Li},
  doi          = {10.1109/TCDS.2023.3281271},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1773-1782},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Individual-level fMRI segmentation based on graphs},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Short-interval priming effects: An EEG study of action
observation on motor imagery. <em>TCDS</em>, <em>15</em>(4), 1765–1772.
(<a href="https://doi.org/10.1109/TCDS.2022.3187777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to effectively set and adjust training tasks is the main challenge of task-oriented rehabilitation. Recent studies have shown that motor priming effects could be used as an important reference for task setting. However, the use of assessment scales to quantify priming effects lacks timeliness. This study focused on evaluating the motor priming effects on a smaller timescale using electroencephalogram (EEG). Six healthy adults were recruited to perform motor imagery (MI) of elbow flexion extension primed by the same action observation (AO). The control task replaced AO with arrows indicating different elbows. The motor priming effects were evaluated by the event-related desynchronization (ERD) of $\mu $ band (8–13 Hz) power and single-trial classification of left or right elbow MI. Both ERD intensity and classification accuracy of MI primed by AO were more significant than MI instructed by arrows. The results demonstrated that short-interval motor priming effects could be evaluated at the neural level and could be used as feasible neurofeedback to guide task-oriented rehabilitation.},
  archive      = {J_TCDS},
  author       = {Ziqi Sun and Yi-Chuan Jiang and Yishu Li and Jongbin Song and Mingming Zhang},
  doi          = {10.1109/TCDS.2022.3187777},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1765-1772},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Short-interval priming effects: An EEG study of action observation on motor imagery},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering based on eye tracking data for depression
recognition. <em>TCDS</em>, <em>15</em>(4), 1754–1764. (<a
href="https://doi.org/10.1109/TCDS.2022.3223128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attention-based approach would be a good way of detecting depression, assisting medical diagnosis, and treating the patients at risk earlier. In this article, a new approach of recognizing depression is proposed, which avoids eye movement event identification and directly performs clustering based on eye tracking data to obtain regions of interesting (ROIs), and then conducts depression recognition modeling. Based on these, a novel spatiotemporal clustering algorithm was proposed, i.e., ROI Clustering with Deflection Elimination, which takes the noisy data into consideration to better describe attention patterns. On the data set with 45 depression patients and 44 healthy controls, the proposed algorithm achieved the best classification accuracy of 76.25%, which has the potential to provide methodological reference on the assessment of mental disorders based on eye movements.},
  archive      = {J_TCDS},
  author       = {Minqiang Yang and Chenlei Cai and Bin Hu},
  doi          = {10.1109/TCDS.2022.3223128},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1754-1764},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Clustering based on eye tracking data for depression recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing visual coding through collaborative perception.
<em>TCDS</em>, <em>15</em>(4), 1744–1753. (<a
href="https://doi.org/10.1109/TCDS.2022.3203422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central challenge facing the nature human–computer interaction involves understanding how neural circuits process visual perceptual information to improve the user’s operation ability under complex tasks. Visual coding models aim to explore the biological characteristics of retinal ganglion cells to provide quantitative predictions of responses to a range of visual stimuli. The existing visual coding models lack adaptability in natural and complex scenes. Therefore this article proposes an enhanced visual coding model through collaborative perception. Our model first extracts the multimodal spatiotemporal features of the input video to simulate the retinal response characteristics adaptively. Second, it uses the basis function to compile the input stimulus into a multimodal stimulus matrix. Afterward, the upstream and downstream filters reform the stimulus matrix to generate the spike sequence. Experiments show that the proposed model reproduces the physiological characteristics of ganglion cells in the biological retina, leading to the high accuracy, good adaptability, and biological interpretability in comparison with its rivals.},
  archive      = {J_TCDS},
  author       = {Lingling An and Zhen Yan and Weizheng Wang and Jian K. Liu and Keping Yu},
  doi          = {10.1109/TCDS.2022.3203422},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1744-1753},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Enhancing visual coding through collaborative perception},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploratory cross-frequency coupling and scaling analysis of
neuronal oscillations stimulated by emotional images: An evidence from
EEG. <em>TCDS</em>, <em>15</em>(4), 1732–1743. (<a
href="https://doi.org/10.1109/TCDS.2022.3208238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion as a physiological state, plays an important role in neuroscience, cognitive science, rational thinking, decision making, and mental states. Therefore, how to quantitatively analyze the different emotion state is especially important. The traditional manner through the spectral power and amplitude of a certain frequency, which reflects the strength of local cortical activation. Cross-frequency coupling (CFC) and self-affinity are observed neural phenomena associated with cognitive processes. This measures, differ across brain areas and frequency bands in a task-relevant manner, changing quickly in response to cognitive events, motor actions, and correlates with emotion. In this study, the electroencephalogram (EEG) signals of all participants in response to stimulation were collected using a 64-channel amplifier recording system. “Exploratory CFC and scaling analysis” were applied to four different categories of emotions, including depression, relaxation, fear, and happiness. Three phase-amplitude coupling (PAC) indices [The Kullback–Leibler-based modulation index (KL-MI), the mean vector length modulation index (MVL-MI), and the phase-locking value (PLV)] and four scaling indices (Amplitude analysis, detrended fluctuation analysis (DFA), life-times, and waiting-times analysis) were introduced and calculated to find out the difference of each emotion. The results suggested that $\alpha $ and $\beta $ rhythm had a distinct coupling effect with high- $\gamma $ rhythm; furthermore, the prefrontal and temporal lobes were the main contributors to the value of PAC, which provides a theoretical basis for PAC to regulate the emotional process. These results indicate that the feasibility of a new method for emotional computing and have huge prospects in auxiliary diagnosis.},
  archive      = {J_TCDS},
  author       = {Jinlong Chao and Shuzhen Zheng and Chang Lei and Hong Peng and Bin Hu},
  doi          = {10.1109/TCDS.2022.3208238},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1732-1743},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Exploratory cross-frequency coupling and scaling analysis of neuronal oscillations stimulated by emotional images: An evidence from EEG},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph learning with co-teaching for EEG-based motor imagery
recognition. <em>TCDS</em>, <em>15</em>(4), 1722–1731. (<a
href="https://doi.org/10.1109/TCDS.2022.3174660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies have explored the use of deep neural networks for electroencephalography (EEG)-based motor imagery (MI) recognition, but most of the models focus on the recognition performance achieved for a single subject and are challenging to transfer due to individual differences and low signal-to-noise-ratio of EEG signals. To date, few studies have paid attention to the balance between generalizability and personalization across subjects. To this end, we propose a co-teaching graph learning method for cross-subject EEG-based MI recognition. First, A novel graph learning approach is designed to improve feature extraction from a typical graph structure containing raw EEG signals. Second, two graph learning models are constructed to filter noisy data by using a co-teaching training strategy, preventing overfitting on noisy samples obtained from different subjects. The proposed model shows a 5.4% and 3.2% increase in accuracy of single- and multisubject four-class MI recognition tasks compared to the previous best method, respectively. Experimental results also demonstrate that it is easy to derive a model that can represent generic knowledge of multiple MI subjects and can be fine-tuned efficiently for new subjects.},
  archive      = {J_TCDS},
  author       = {Yifan Zhang and Yang Yu and Bo Wang and Hui Shen and Gai Lu and Yingxin Liu and Ling-Li Zeng and Dewen Hu},
  doi          = {10.1109/TCDS.2022.3174660},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1722-1731},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Graph learning with co-teaching for EEG-based motor imagery recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel multiscale dilated convolution neural network with
gating mechanism for decoding driving intentions based on EEG.
<em>TCDS</em>, <em>15</em>(4), 1712–1721. (<a
href="https://doi.org/10.1109/TCDS.2023.3245042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods based on convolution neural networks (CNNs) have achieved good classification performance in decoding electroencephalography (EEG). In this article, a novel framework combining-gating mechanism and dilated CNN (GDCNN) is proposed for decoding EEG signals evoked by four different driving intentions. GDCNN provides different receptive fields and controls the information flow between convolution layers, which help to detect different sizes of information in EEG signals. The proposed method reaches accuracies of 93.17% and 73.33% for the subject-dependent and subject-independent experiments, outperforming several benchmark methods. The data augmentation that randomly concatenates multitrial EEG sequences is adopted to promote generalization of decoding model. This strategy effectively prevents overfitting and improves the decoding accuracies of EEGNet, DeepConvNet, and GDCNN by 4.3%, 4.75%, and 3.92%, respectively. These results indicate GDCNN is beneficial for decoding EEG and it has application potential in the brain–computer interface (BCI) systems based on video stimuli.},
  archive      = {J_TCDS},
  author       = {Jianxiang Sun and Yadong Liu and Zeqi Ye and Dewen Hu},
  doi          = {10.1109/TCDS.2023.3245042},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1712-1721},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A novel multiscale dilated convolution neural network with gating mechanism for decoding driving intentions based on EEG},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Brain–computer interface integrated with augmented reality
for human–robot interaction. <em>TCDS</em>, <em>15</em>(4), 1702–1711.
(<a href="https://doi.org/10.1109/TCDS.2022.3194603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–computer interface (BCI) has been gradually used in human–robot interaction systems. Steady-state visual evoked potential (SSVEP) as a paradigm of electroencephalography (EEG) has attracted more attention in the BCI system research due to its stability and efficiency. However, an independent monitor is needed in the traditional SSVEP-BCI system to display stimulus targets, and the stimulus targets map fixedly to some preset commands. These limit the development of the SSVEP-BCI application system in complex and changeable scenarios. In this study, the SSVEP-BCI system integrated with augmented reality (AR) is proposed. Furthermore, a stimulation interface is made by merging the visual information of the objects with stimulus targets, which can update the mapping relationship between stimulus targets and objects automatically to adapt to the change of the objects in the workspace. During the online experiment of the AR-based SSVEP-BCI cue-guided task with the robotic arm, the success rate of grasping is 87.50 ±3.10% with the SSVEP-EEG data recognition time of 0.5 s based on FB-tCNN. The proposed AR-based SSVEP-BCI system enables the users to select intention targets more ecologically and can grasp more kinds of different objects with a limited number of stimulus targets, resulting in the potential to be used in complex and changeable scenarios.},
  archive      = {J_TCDS},
  author       = {Bin Fang and Wenlong Ding and Fuchun Sun and Jianhua Shan and Xiaojia Wang and Chengyin Wang and Xinyu Zhang},
  doi          = {10.1109/TCDS.2022.3194603},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1702-1711},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Brain–Computer interface integrated with augmented reality for Human–Robot interaction},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Brain biometrics of steady-state visual evoked potential
functional networks. <em>TCDS</em>, <em>15</em>(4), 1694–1701. (<a
href="https://doi.org/10.1109/TCDS.2022.3160295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain biometrics, due to its unique confidentiality and concealment, has received the increasing attention of scientific researchers in recent years. It has shown that steady-state visual evoked potential (SSVEP) signals with a high signal-to-noise ratio and stable spectrum can be used as identification features. However, current studies rely on the extraction of features characterizing the activity of single brain regions, ignoring the functional coupling between different brain regions. In this study, we proposed a novel approach that considers the functional connectivity of SSVEP signals collected by different electrodes as practical biometric features. We investigated individual coherence (COH) connectivity and phase synchronization according to different visual stimulations in terms of frequency component analysis of different principal bands. Fifteen subjects were identified by the support vector machine, random forest, and $k$ -nearest neighbor algorithm. The results show that distinctive features in SSVEP functional networks among individuals can achieve high precision identification (up to 98%). The obtained identification performance shows that SSVEP spectral COH and phase synchronization have the potential for biometric applications.},
  archive      = {J_TCDS},
  author       = {Yibo Zhang and Hui Shen and Ming Li and Dewen Hu},
  doi          = {10.1109/TCDS.2022.3160295},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1694-1701},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Brain biometrics of steady-state visual evoked potential functional networks},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal graph convolutional neural network for emotion
recognition. <em>TCDS</em>, <em>15</em>(4), 1686–1693. (<a
href="https://doi.org/10.1109/TCDS.2022.3175538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional neural network (GCNN)-based methods have been widely used in electroencephalogram (EEG)-related works due to their advantages of considering the symmetrical connections of brain regions. However, the current GCNN-based methods do not fully explore other correlations between EEG channels. Many studies have proved that definite causal connections exist between brain regions. Therefore, this article proposes a causal GCNN (CGCNN) using the Granger causality (GC) test to calculate interchannel interactions. First, we consider causal relations between EEG channels and construct an asymmetric causal graph with direction. Then, we adopt depthwise separable convolution to extract emotional features from multichannel EEG signals. Experiments carried out on SEED and SEED-IV show that CGCNN has the ability to represent the causal information flow in different emotional states, and improve the classification accuracy to 93.36% on SEED and 75.48% on SEED-IV, respectively. The results outperform other existing methods, indicating that GC is more effective in revealing the correlations between EEG channels in emotion recognition.},
  archive      = {J_TCDS},
  author       = {Wanzeng Kong and Min Qiu and Menghang Li and Xuanyu Jin and Li Zhu},
  doi          = {10.1109/TCDS.2022.3175538},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1686-1693},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Causal graph convolutional neural network for emotion recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Residual GCB-net: Residual graph convolutional broad network
on emotion recognition. <em>TCDS</em>, <em>15</em>(4), 1673–1685. (<a
href="https://doi.org/10.1109/TCDS.2022.3147839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) data are commonly applied in the emotion recognition research area. It can accurately reflect the emotional changes of the human body by applying graphical-based algorithms or models. EEG signals are nonlinear signals. Biological tissues’ adjustment and adaptive ability will inevitably affect electrophysiological signals, making EEG have the typical nonlinear characteristics. The graph convolutional broad network (GCB-net) extracted features from nonlinear signals and abstract features via a stacked convolutional neural network. It adopted the broad concept and enhanced the feature by the broad learning system (BLS), obtaining sound results. However, it performed poorly with the increasing network depth, and the accuracy of some features decreased with BLS. This article proposed a residual graph convolutional broad network (Residual GCB-net), which promotes the performance on a deeper layer network and extracts higher level information. It substitutes the original convolutional layer with residual learning blocks, which solves the deep learning network degradation and extracts more features in deeper networks. In the SJTU emotion EEG data set (SEED), GCB-Res net could obtain the best accuracy (94.56%) on the all-frequency band of differential entropy (DE) and promote much on another feature. In Dreamer, it obtained the best accuracy (91.55%) on the dimension of Arousal. The result demonstrated the excellent classification performance of Residual GCB-net in EEG emotion recognition.},
  archive      = {J_TCDS},
  author       = {Qilin Li and Tong Zhang and C. L. Philip Chen and Ke Yi and Long Chen},
  doi          = {10.1109/TCDS.2022.3147839},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1673-1685},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Residual GCB-net: Residual graph convolutional broad network on emotion recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG-based emotion recognition using trainable adjacency
relation driven graph convolutional network. <em>TCDS</em>,
<em>15</em>(4), 1656–1672. (<a
href="https://doi.org/10.1109/TCDS.2023.3270170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing research interest in using deep learning to resolve the issue of electroencephalogram (EEG)-based emotion recognition. Current research emphasizes exploiting the useful information from each single EEG channel or each individual set of multichannel EEG, but overlooks the correlation information among different multichannel EEG sets. To explore such discriminative correlation information, we propose a novel and effective method, “trainable adjacency relation driven graph convolutional network (TARDGCN),” which contains two complementary modules: 1) trainable adjacency relation (TAR) and 2) graph convolutional network (GCN). TAR optimizes the local pairwise positions of multichannel EEG sets, which helps form an improved graphic representation for GCN to learn the global correlation among these sets for classification. The proposed method is capable of dealing with the problem of small sample size but large data variation in this issue. Our experimental results conducted on the databases DREAMER and DEAP in the subject-dependent and subject-independent modes show that TARDGCN outperforms the state-of-the-art approaches in classifying all of valence, arousal, and dominance.},
  archive      = {J_TCDS},
  author       = {Wei Li and Mingming Wang and Junyi Zhu and Aiguo Song},
  doi          = {10.1109/TCDS.2023.3270170},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1656-1672},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EEG-based emotion recognition using trainable adjacency relation driven graph convolutional network},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3-d tactile-based object recognition for robot hands using
force-sensitive and bend sensor arrays. <em>TCDS</em>, <em>15</em>(4),
1645–1655. (<a href="https://doi.org/10.1109/TCDS.2022.3215021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tactile sensing is a particularly important and challenging task for a modern robot to safely manipulate objects, interact with humans in a shared space, and provide various services. This article presents a 3-D tactile glove for robots with the combination of a piezoresistive-based force sensor array (412 sensors) covering the full hand and a resistive bend sensor array (five sensors) on the back of five fingers. Deep learning-based convolutional neural network (CNN) and multilayer perceptron network (MLP) based methods using the designed tactile glove are proposed for object recognition. In the experiment for recognizing 15 objects with a dexterous robot hand, an average classification accuracy of 93.67% has been achieved. Comparison experiments with three other typical classifiers (the Quadratic support vector machine, weighted KNN, and Bagged Trees) and our MLP and CNN methods show an average recognition accuracy of 91.67% with the 3-D tactile glove, revealing an accuracy improvement of 4.17% over only using the force sensor array. We further apply our 3-D tactile glove and the multimodal CNN to identify three other objects and demonstrate their generalization ability of tactile object recognition with an average success accuracy of 78.33%. The proposed 3-D tactile glove can be further used in human–robot interactions, the design of prosthetics and humanoid robots, and for improving the intelligence level in brain–computer collaborative systems.},
  archive      = {J_TCDS},
  author       = {Xiong Lu and Dong Sun and Hongbin Yin and Huafang Xu and Yuxing Yan and Changcheng Wu and Aaron Quigley},
  doi          = {10.1109/TCDS.2022.3215021},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1645-1655},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {3-D tactile-based object recognition for robot hands using force-sensitive and bend sensor arrays},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial special issue on hybrid brain–computer
collaborative intelligent system. <em>TCDS</em>, <em>15</em>(4),
1641–1644. (<a href="https://doi.org/10.1109/TCDS.2023.3325984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–machine fusion, also known as hybrid intelligence or brain–computer interface (BCI), is considered one of the most promising technologies of the 21st century. Its potential impact spans a wide range of disciplines, including cognitive science, information science, artificial intelligence, biology, neuroscience, and engineering. The research in this field aims to seamlessly integrate biological intelligence (i.e., the human brain) with machine intelligence (computers or robots) to create a new, powerful form of hybrid intelligence that far surpasses the limitations of current biological and machine intelligence systems. Brain–machine fusion not only signifies the convergence of cutting-edge science and technology but also heralds a new era in which the way humans interact with machines undergoes a profound transformation. The research in this field delves deep into the understanding of human thought processes and cognition, as well as the creation of novel sensory and motor channels to facilitate more natural and intuitive interactions. The scope of brain–machine fusion research extends beyond mere information exchange, encompassing the integration of emotions and motivations. Understanding and interpreting a user’s emotional state and motivations are crucial for optimizing the performance of fusion systems, aiding in better meeting user needs and providing more personalized experiences. A key objective is enhancing a user’s operational capacity in handling complex tasks. This can encompass highly intricate decision making, problem solving, and task execution, with broad applications in fields, such as healthcare, military, industry, and entertainment. Furthermore, brain–machine fusion necessitates the development of cognitive interaction models that can adapt actively to a user’s cognitive characteristics and integrate with machine learning algorithms to achieve personalized adaptability in intelligent systems, thereby enhancing the level of interaction between the user and the system.},
  archive      = {J_TCDS},
  author       = {Edmond Q. Wu and Pengwen Xiong and Aiguo Song and Peter X. Liu},
  doi          = {10.1109/TCDS.2023.3325984},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1641-1644},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial special issue on hybrid Brain–Computer collaborative intelligent system},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cerebellum-inspired spiking neural model with adapting
rate neurons. <em>TCDS</em>, <em>15</em>(3), 1628–1638. (<a
href="https://doi.org/10.1109/TCDS.2023.3237776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cerebellum plays an important role in smooth and coordinated motor control. Precise control requires the cerebellum to regulate movements in both space and time domains. Although many cerebellar models have been proposed, most of them focus on motor coordination, motor learning, or timing distinctly. Therefore, it is necessary to develop a cerebellar model which contributes to the proper execution of movements via motor learning that displays temporal specificity. In this article, we proposed a novel spiking neural network model to realize cerebellar processing with strong biomimicry, which is based on adapting rate neurons and has a cerebellum-inspired structure as well as biologically plausible cerebellar divergence/convergence ratios. The model was tested with the eyeblink classical conditioning (EBCC) experimental task. The simulation results verify that our implementation has improvements in both signal encoding and learning speed than previous studies. Furthermore, compared with neurophysiological data, our model shows a similar learning trend and can represent the fluctuation of the learning curves. It is demonstrated that our proposed cerebellar model experimentally reproduces the key features of the EBCC task and provides a new way to understand the timing and learning control neural mechanisms of the cerebellum.},
  archive      = {J_TCDS},
  author       = {Yin Liu and Rong Liu and Jiaxing Wang and Wenqian Chen and Yongxuan Wang and Changkai Sun},
  doi          = {10.1109/TCDS.2023.3237776},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1628-1638},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A cerebellum-inspired spiking neural model with adapting rate neurons},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pixel-level domain adaptation for real-to-sim object pose
estimation. <em>TCDS</em>, <em>15</em>(3), 1618–1627. (<a
href="https://doi.org/10.1109/TCDS.2023.3237502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferring robotic grasping skills learned from a simulator to the real world is beneficial in reducing the cost of labeling. However, the models trained on synthetic data are brittle when being applied to real-world data due to the domain gap. In this article, we propose cycle-consistency, content-consistency, and mapping-consistency (CCM) pixel-level domain adaptation (Pixel-DA), a novel real-to-sim approach to unsupervised domain adaptation for object pose estimations, which outperforms conventional domain adaptation methods in preserving structural information, semantic information, and object pose during the transfer. The pipeline decouples domain adaptation and pose estimation, which allows the CCM Pixel-DA method to be integrated into state-of-the-art object pose estimation networks. The proposed method is further integrated into a pipeline for robot grasping. Experimental results on a real-world robot grasping system validate that the system is capable of grasping real-world objects without object pose annotations in the real-world domain.},
  archive      = {J_TCDS},
  author       = {Kun Qian and Yanhui Duan and Chaomin Luo and Yongqiang Zhao and Xingshuo Jing},
  doi          = {10.1109/TCDS.2023.3237502},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1618-1627},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Pixel-level domain adaptation for real-to-sim object pose estimation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved algorithm for complete coverage path planning
based on biologically inspired neural network. <em>TCDS</em>,
<em>15</em>(3), 1605–1617. (<a
href="https://doi.org/10.1109/TCDS.2023.3237612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complete coverage path planning (CCPP) requires the mobile robots to traverse every part of the workspace, which is one of the major challenges in cleaning robots and many other robotic systems. The biologically inspired neural network (BINN) algorithm has been extensively applied in path planning, recently. In this article, a new CCPP strategy with BINN is proposed. The planned path of cleaning robot is not only determined by the dynamic neural activities but also by the distribution of obstacles in the environmental map. By distinguishing the connectivity between different areas of the environmental map, and using the proposed path backtracking algorithm, the improved CCPP algorithm can autonomously plan a collision-free path and reduce the path repetition ratio. Besides, an improved dynamic deadlock escape algorithm is presented to select the optimal escape target point. The simulation results show that the proposed CCPP algorithm without any templates or learning procedures is able to generate an orderly path in both known and unknown environment.},
  archive      = {J_TCDS},
  author       = {Linhui Han and Xiangquan Tan and Qingwen Wu and Xu Deng},
  doi          = {10.1109/TCDS.2023.3237612},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1605-1617},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An improved algorithm for complete coverage path planning based on biologically inspired neural network},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Electroencephalogram emotion recognition using combined
features in variational mode decomposition domain. <em>TCDS</em>,
<em>15</em>(3), 1595–1604. (<a
href="https://doi.org/10.1109/TCDS.2022.3233858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using electroencephalogram (EEG) to recognize human emotion has attracted increasing attention. However, feature extraction from EEG is a challenging work because it is a nonstationary continuous sequential signal. To obtain more pattern information, a combined feature extraction method in the variational mode decomposition (VMD) domain is proposed, which can extract local features of EEG signals to overcome the effects caused by nonstationarity. This method first decomposes EEG into several components using VMD and then combined features of differential entropy (DE) and short-time energy (STE) are extracted from each component. To optimize combined features, the important features are selected by tree modes, and the feature set is dimensionally reduced by further using linear discriminant analysis (LDA). Moreover, an XGBoost classifier with Bayesian optimization is presented to classify different emotional states. Binary-class and multiclass EEG emotion recognition are conducted on the DEAP data set, from which the experimental results show that accuracy of binary-class classification is 81.77% for high/low valence and 80.47% for high/low arousal, and accuracy of 91.41%, 94.27%, 94.27%, and 93.49% are obtained for HVHA, LVHA, LVLA, and HVLA, respectively, which demonstrate its effectiveness.},
  archive      = {J_TCDS},
  author       = {Zhen-Tao Liu and Si-Jun Hu and Jinhua She and Zhaohui Yang and Xin Xu},
  doi          = {10.1109/TCDS.2022.3233858},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1595-1604},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Electroencephalogram emotion recognition using combined features in variational mode decomposition domain},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individual susceptibility to vigilance decrement in
prolonged assisted driving revealed by alert-state wearable EEG
assessment. <em>TCDS</em>, <em>15</em>(3), 1586–1594. (<a
href="https://doi.org/10.1109/TCDS.2022.3231691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As vehicular automation handles more aspects of the driving task, human drivers are taxed with increasing demands to monitor and handle rare automation failures. Staying vigilant imposes a high-cognitive workload and so is hypothesized to pose varying difficulties across the driving population, leading to differences in individual susceptibility to vigilance decrement. To investigate this, the present study proposes the use of an objective neurometric of mental workload to characterize drivers’ severity of vigilance decrement during assisted driving with adaptive cruise control. Drivers performed a car-following task for approximately 1.5 h and performed emergency braking whenever the leading car brakes suddenly. The neurometric, frontal theta / parietal alpha ratio—measured from an alert-state driving period using a wearable electroencephalography headset—was found to be associated with the magnitude of drivers’ behavioral changes (cumulative slowing and erraticity of their braking reaction times) over the course of the assisted driving tour. This is the first study to explore the use of an alert-state objective measure in profiling individuals’ general susceptibility to vigilance decrements in assisted driving, which is highly relevant in the context of identifying higher risk drivers and designing driver-vehicle interaction systems to enhance automation use safety.},
  archive      = {J_TCDS},
  author       = {Manuel Seet and Anastasios Bezerianos and Maria Panou and Evangelos Bekiaris and Nitish Thakor and Andrei Dragomir},
  doi          = {10.1109/TCDS.2022.3231691},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1586-1594},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Individual susceptibility to vigilance decrement in prolonged assisted driving revealed by alert-state wearable EEG assessment},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Channel-correlation-based selective knowledge distillation.
<em>TCDS</em>, <em>15</em>(3), 1574–1585. (<a
href="https://doi.org/10.1109/TCDS.2022.3232569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a simple yet effective model compression method, knowledge distillation (or KD) is used to learn a small lightweight student network by transferring valuable knowledge from a pretrained cumbersome teacher network. However, existing KD methods usually consider the feature knowledge either in different layers or individual samples, failing to explore more detailed information in different channels from the perspective of sample relationships. Meanwhile, the negative influences contained in the teacher knowledge are also not well investigated, especially, when using the response-based knowledge. To address the above-mentioned issues, we devise a novel KD approach entitled channel correlation-based selective KD (or CCSKD). Specifically, to distill rich knowledge from feature representations, we not only consider the feature knowledge from different channels for individual samples but also take into account the relational knowledge based on per-channel features for different samples. Furthermore, to further distill positive response-based knowledge, a selective strategy is developed, i.e., selective KD, to progressively correct the negative influences from the teacher knowledge during the distillation process. We perform extensive experiments on three image classification data sets, CIFAR-100, Stanford Cars, and Tiny-ImageNet, to demonstrate the effectiveness of the proposed CCSKD, which outperforms recent state-of-the-art methods with a clear margin. Our codes are publicly available at https://github.com/gjplab/CCSKD .},
  archive      = {J_TCDS},
  author       = {Jianping Gou and Xiangshuo Xiong and Baosheng Yu and Yibing Zhan and Zhang Yi},
  doi          = {10.1109/TCDS.2022.3232569},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1574-1585},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Channel-correlation-based selective knowledge distillation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ABCP: Automatic blockwise and channelwise network pruning
via joint search. <em>TCDS</em>, <em>15</em>(3), 1560–1573. (<a
href="https://doi.org/10.1109/TCDS.2022.3230858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, an increasing number of model pruning methods are proposed to resolve the contradictions between the computer powers required by the deep learning models and the resource-constrained devices. However, for simple tasks like robotic detection, most of the traditional rule-based network pruning methods cannot reach a sufficient compression ratio with low accuracy loss and are time consuming as well as laborious. In this article, we propose automatic blockwise and channelwise network pruning (ABCP) to jointly search the blockwise and channelwise pruning action for robotic detection by deep reinforcement learning. A joint sample algorithm is proposed to simultaneously generate the pruning choice of each residual block and the channel pruning ratio of each convolutional layer from the discrete and continuous search space, respectively. The best pruning action taking both the accuracy and the complexity of the model into account is obtained finally. Compared with the traditional rule-based pruning method, this pipeline saves human labor and achieves a higher compression ratio with lower accuracy loss. Tested on the mobile robot detection data set, the pruned YOLOv3 model saves 99.5% floating-point operations, reduces 99.5% parameters, and achieves $\boldsymbol {37.3\times }$ speed up with only 2.8% mean of average precision (mAP) loss. On the sim2real detection data set for robotic detection task, the pruned YOLOv3 model achieves 9.6% better mAP than the baseline model, showing better robustness performance.},
  archive      = {J_TCDS},
  author       = {Jiaqi Li and Haoran Li and Yaran Chen and Zixiang Ding and Nannan Li and Mingjun Ma and Zicheng Duan and Dongbin Zhao},
  doi          = {10.1109/TCDS.2022.3230858},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1560-1573},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {ABCP: Automatic blockwise and channelwise network pruning via joint search},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ocular artifacts elimination from multivariate EEG signal
using frequency-spatial filtering. <em>TCDS</em>, <em>15</em>(3),
1547–1559. (<a href="https://doi.org/10.1109/TCDS.2022.3226775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electroencephalogram (EEG) signals record electrical activities generated by the brain cells and are used as a state-of-the-art diagnosis tool for various neural disorders. However, the unwanted artifacts often contaminate the recorded EEG signals and disturb the interpretation of the neuronal activity. This article aims to propose an efficient automatic method to eliminate the ocular artifacts (OAs) from the multichannel EEG signals with novel frequency-spatial filtering. The method combines dictionary-based spatial filtering and frequency-based signal decomposition method, namely, empirical wavelet transform (EWT). The artifact dictionary needed for spatial filtering is isolated from the raw data by: 1) selecting the contaminated channels and 2) frequency-domain filtering. More precisely, the $\delta $ -rhythms of identified highly contaminated channels are selected and placed into an artifact dictionary. Afterward, the $\delta $ -rhythms of multichannel EEG signals are spatially filtered using the built dictionary to seclude the OAs within a limited number of components. Furthermore, the artifact components are eliminated and clean $\delta $ -rhythms are recovered using the inverse spatial filtering technique. Finally, the clean $\delta $ -rhythms are combined with other EEG rhythms to reconstruct the OA-free signals. The proposed method is applied to OA-contaminated synthetic and real multichannel EEG signals with a convincing performance as compared to state-of-the-art approaches. The proposed method removes the OAs without affecting the background EEG information. The proposed method can ease sensor signal interpretation and further processing, e.g., for BCI applications.},
  archive      = {J_TCDS},
  author       = {Abhijit Bhattacharyya and Aarushi Verma and Radu Ranta and Ram Bilas Pachori},
  doi          = {10.1109/TCDS.2022.3226775},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1547-1559},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Ocular artifacts elimination from multivariate EEG signal using frequency-spatial filtering},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolving hebbian learning rules in voxel-based soft robots.
<em>TCDS</em>, <em>15</em>(3), 1536–1546. (<a
href="https://doi.org/10.1109/TCDS.2022.3226556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to Hebbian theory, synaptic plasticity is the ability of neurons to strengthen or weaken the synapses among them in response to stimuli. It plays a fundamental role in the processes of learning and memory of biological neural networks. With plasticity, biological agents can adapt on multiple timescales and outclass artificial agents, the majority of which still rely on static artificial neural network (ANN) controllers. In this work, we focus on voxel-based soft robots (VSRs), a class of simulated artificial agents, composed as aggregations of elastic cubic blocks. We propose a Hebbian ANN controller where every synapse is associated with a Hebbian rule that controls the way the weight is adapted during the VSR lifetime. For a given task and morphology, we optimize the controller for the task of locomotion by evolving, rather than the weights, the parameters of Hebbian rules. Our results show that the Hebbian controller is comparable, often better than a non-Hebbian baseline and that it is more adaptable to damages. We also provide novel insights into the inner workings of plasticity and demonstrate that “true” learning does take place, as the evolved controllers improve over the lifetime and generalize well.},
  archive      = {J_TCDS},
  author       = {Andrea Ferigo and Giovanni Iacca and Eric Medvet and Federico Pigozzi},
  doi          = {10.1109/TCDS.2022.3226556},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1536-1546},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Evolving hebbian learning rules in voxel-based soft robots},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic segmentation based on spatial pyramid pooling and
multilayer feature fusion. <em>TCDS</em>, <em>15</em>(3), 1524–1535. (<a
href="https://doi.org/10.1109/TCDS.2022.3225200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant progress has been made in semantic segmentation methods. Traditional semantic segmentation methods based on convolutional neural network (CNN) are prone to lose spatial information in the feature extraction stage, and pay less attention to global context information, especially, in some lightweight real-time semantic segmentation networks. This is a huge challenge for semantic segmentation tasks. In addition, although some methods have improved this problem to a certain extent, they are often embedded in specific networks and cannot be applied to other network models. Aiming at these problems, a semantic segmentation method based on multilayer feature fusion is proposed. The flexible and lightweight squeeze–excitation module is used to improve the spatial pyramid pooling (SPP) network, and the accuracy of the semantic segmentation method is further improved by extracting network feature information at different levels. To verify the efficiency and commonality of our methodology, we selected ERFNet and Deeplabv3 networks to experiment on Cityscapes and COCO data sets. Experiments show that our best method can improve 3.1% mIoU and 3.2% mAcc on the Cityscapes data set relative to ERFNet, and at the same time, our method can achieve 61.93 FPS on 1024 $\times $ 512 resolution images and the best improvement of 0.9% mIoU 1.4% mAcc was achieved on the Deeplabv3 network. The experimental results show that the improved multilayer feature fusion structure can improve the accuracy of the semantic segmentation network.},
  archive      = {J_TCDS},
  author       = {Jian Ji and Sitong Li and Xianfu Liao and Fangrong Zhang},
  doi          = {10.1109/TCDS.2022.3225200},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1524-1535},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Semantic segmentation based on spatial pyramid pooling and multilayer feature fusion},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HAIN: Multilabel classification with hierarchical
attention-based interaction network for multiturn dialogue texts.
<em>TCDS</em>, <em>15</em>(3), 1514–1523. (<a
href="https://doi.org/10.1109/TCDS.2022.3223924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiturn dialogue systems with cognitive capabilities have been popularly used in many applications and generate a large volume of dialogue texts. The dialogue texts of many dialogue systems contain multiple labels, and a typical case is that the customer requirements or complaints in an online multiturn dialogue service system often need to be classified into multiple categories or forwarded to multiple departments, that is, multilabel classification on multiturn dialogue texts. Traditional methods treat the dialogue text as the plain text, which makes it difficult for classifiers to learn important features. On the other hand, regarding each utterance as the basic semantic unit may destroy the semantic information integrity of the dialogue. Furthermore, the above methods all ignore the relationship between labels. To address above issues, we propose to use the utterance pair as the basic semantic unit and design a novel multilabel classification model for multiturn dialogue texts in this article. Specifically, we propose to use the multihead attention mechanism to learn the key features of multiple labels and design a new method to learn the correlation among labels. Besides, we propose to use the penalty term to allow multihead attention to focus on different semantics. Finally, we conduct sufficient experiments on four real-world multiturn dialogue data sets and the experimental results show that our model produces superior performance.},
  archive      = {J_TCDS},
  author       = {Bin Cao and Kai Wang and Kui Ma and Jing Fan and Rui Yan and Yueshen Xu},
  doi          = {10.1109/TCDS.2022.3223924},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1514-1523},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {HAIN: Multilabel classification with hierarchical attention-based interaction network for multiturn dialogue texts},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A squeeze-and-excitation and transformer-based cross-task
model for environmental sound recognition. <em>TCDS</em>,
<em>15</em>(3), 1501–1513. (<a
href="https://doi.org/10.1109/TCDS.2022.3222350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental sound recognition (ESR) is an emerging research topic in audio pattern recognition. Many tasks are presented to resort to computational models for ESR in real-life applications. However, current models are usually designed for individual tasks, and are not robust and applicable to other tasks. Cross-task models, which promote unified knowledge modeling across various tasks, have not been thoroughly investigated. In this article, we propose a cross-task model for three different tasks of ESR: 1) acoustic scene classification; 2) urban sound tagging; and 3) anomalous sound detection. An architecture named SE-Trans is presented that uses attention mechanism-based Squeeze-and-Excitation and Transformer encoder modules to learn the channelwise relationship and temporal dependencies of the acoustic features. FMix is employed as the data augmentation method that improves the performance of ESR. Evaluations for the three tasks are conducted on the recent databases of detection and classification of acoustic scenes and event challenges. The experimental results show that the proposed cross-task model achieves state-of-the-art performance on all tasks. Further analysis demonstrates that the proposed cross-task model can effectively utilize acoustic knowledge across different ESR tasks.},
  archive      = {J_TCDS},
  author       = {Jisheng Bai and Jianfeng Chen and Mou Wang and Muhammad Saad Ayub and Qingli Yan},
  doi          = {10.1109/TCDS.2022.3222350},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1501-1513},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A squeeze-and-excitation and transformer-based cross-task model for environmental sound recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Priors, progressions, and predictions in science learning:
Theory-based bayesian models of children’s revising beliefs of water
displacement. <em>TCDS</em>, <em>15</em>(3), 1487–1500. (<a
href="https://doi.org/10.1109/TCDS.2022.3220963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite sometimes noisy evidence (e.g., perceptual processing errors), young children are capable of predicting and evaluating events based on complex causal representations. Children rapidly revise their beliefs and learn scientific concepts—sometimes without prior knowledge of an underlying causal system. What might we need in our computational models of belief revision to similarly simulate children’s behaviors when learning such causal systems? Building from experimental data of elementary school children’s intuitive beliefs and predictions of water displacement, we propose three aspects of human inference and belief revision that warrant attention within the subfield of computational cognition. Each aspect is described by identifying the gaps between empirical findings and current computational implementations. Then, specific implementations of these aspects are built using models of theory-based Bayesian inference. First, we construct children’s prior beliefs at the individual level based on their prior behavior. Second, we approximate children’s learning using an “optimal” Bayesian model, revealing the dynamics of belief revision trial-by-trial. Third, we investigate that the role prediction may have in facilitating learning. By performing these key computational steps, we find support for contemporary claims that children may be approximately “Bayesian” learners and increase awareness of the importance of generating predictions in active learning.},
  archive      = {J_TCDS},
  author       = {Joseph A. Colantonio and Igor Bascandziev and Maria Theobald and Garvin Brod and Elizabeth Bonawitz},
  doi          = {10.1109/TCDS.2022.3220963},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1487-1500},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Priors, progressions, and predictions in science learning: Theory-based bayesian models of children’s revising beliefs of water displacement},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multitask neuroevolution for reinforcement learning with
long and short episodes. <em>TCDS</em>, <em>15</em>(3), 1474–1486. (<a
href="https://doi.org/10.1109/TCDS.2022.3221805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies have shown evolution strategies (ES) to be a promising approach for reinforcement learning (RL) with deep neural networks. However, the issue of high sample complexity persists in applications of ES to deep RL over long horizons. This article is the first to address the shortcoming of today’s methods via a novel neuroevolutionary multitasking (NuEMT) algorithm, designed to transfer information from a set of auxiliary tasks (of short episode length) to the target (full length) RL task at hand. The auxiliary tasks, extracted from the target, allow an agent to update and quickly evaluate policies on shorter time horizons. The evolved skills are then transferred to guide the longer and harder task toward an optimal policy. We demonstrate that the NuEMT algorithm achieves data-efficient evolutionary RL, reducing expensive agent-environment interaction data requirements. Our key algorithmic contribution in this setting is to introduce a first multitask skills transfer mechanism based on the statistical importance sampling technique. In addition, an adaptive resource allocation strategy is utilized to assign computational resources to auxiliary tasks based on their gleaned usefulness. Experiments on a range of continuous control tasks from the OpenAI Gym confirm that our proposed algorithm is efficient compared to recent ES baselines.},
  archive      = {J_TCDS},
  author       = {Nick Zhang and Abhishek Gupta and Zefeng Chen and Yew-Soon Ong},
  doi          = {10.1109/TCDS.2022.3221805},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1474-1486},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multitask neuroevolution for reinforcement learning with long and short episodes},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Soft contrastive learning with q-irrelevance abstraction for
reinforcement learning. <em>TCDS</em>, <em>15</em>(3), 1463–1473. (<a
href="https://doi.org/10.1109/TCDS.2022.3218940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difference between training and testing environments is a huge challenge to generalizing reinforcement learning (RL) algorithms. We propose a soft contrastive learning with a coarser approximate $Q$ -irrelevance abstraction for RL (SCQRL) to increase RL generalization. Specifically, we specify the coarser approximate $Q$ -irrelevance abstraction as the feature of the state with a theoretical analysis for better generalization ability. We construct a positive and negative sample selection mechanism based on the $Q$ value for contrastive learning to achieve efficient representation learning. Considering the selection error of positive and negative samples, we design soft contrastive learning and combine it with RL in the form of an auxiliary task to propose SCQRL. The generalization experiments on several Procgen environments demonstrate that SCQRL outperforms the excellent generalized RL algorithm.},
  archive      = {J_TCDS},
  author       = {Minsong Liu and Luntong Li and Shuai Hao and Yuanheng Zhu and Dongbin Zhao},
  doi          = {10.1109/TCDS.2022.3218940},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1463-1473},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Soft contrastive learning with Q-irrelevance abstraction for reinforcement learning},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multirobot distributed collaborative region coverage
search algorithm based on glasius bio-inspired neural network.
<em>TCDS</em>, <em>15</em>(3), 1449–1462. (<a
href="https://doi.org/10.1109/TCDS.2022.3218718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many constraints for a multirobot system to perform a region coverage search task in an unknown environment. To address this, we propose a novel multirobot distributed collaborative region coverage search algorithm based on Glasius bio-inspired neural network (GBNN). First, we develop an environmental information updating model to represent the dynamic search environment. This model converts the environmental information detected by the robot into dynamic neural activity landscape of GBNN. Second, we introduce the distributed model predictive control method in search path planning to improve search efficiency. In addition, we propose a distributed collaborative decision-making mechanism among the robots to produce several dynamic search subteams. Within each subteam, collaborative decisions are made among the robot members to optimize the solution and obtain the next movement path of each robot. Finally, we conduct experiments in three aspects to verify the effectiveness of the proposed method. Compared with three algorithms in this field, the experimental results demonstrate that the proposed algorithm exhibits good performance in a multirobot region coverage search task.},
  archive      = {J_TCDS},
  author       = {Bo Chen and Hui Zhang and Fangfang Zhang and Yanhong Liu and Cheng Tan and Hongnian Yu and Yaonan Wang},
  doi          = {10.1109/TCDS.2022.3218718},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1449-1462},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A multirobot distributed collaborative region coverage search algorithm based on glasius bio-inspired neural network},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel probabilistic network model for estimating
cognitive-gait connection using multimodal interface. <em>TCDS</em>,
<em>15</em>(3), 1430–1448. (<a
href="https://doi.org/10.1109/TCDS.2022.3222087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research in human gait analysis has captivated several computer vision researchers to solve human identification problems. The proposed work provides a novel approach for cognitive state estimation via multimodal analysis. The advantage of the multimodal system is to provide adequate motion signatures with the ensemble of multimodal gait data to ensure data reliability for classification. The relationship between human cognitive states and gait is predicted using both temporal and nontemporal probabilistic models. We estimate prior probability tables for the nontemporal probabilistic model known as a simple Bayesian model after analyzing the data acquired from the inertial measurement unit (IMU), electroencephalography (EEG), and multiple Kinect V2.0 sensors. A novel dynamic Bayesian network (DBN) is used as a probabilistic temporal model for estimating the most probable sequence of transitions among human cognitive states. We apply Gaussian mixture modeling with expectation maximization (GMM-EM) to tune transition and emission probabilities for maximizing the probability of the observed sequence. A promising estimation accuracy of 88.6% is obtained. Also, principal component analysis (PCA) and $k$ -nearest neighbor (kNN) algorithms are applied separately to calculate the input probabilities for DBN model. It is observed that both GMM-EM and kNN-based approaches outperform all the state-of-the-art techniques. Moreover, standard statistical tests are performed on the acquired data set to validate the experimental results.},
  archive      = {J_TCDS},
  author       = {Sumit Hazra and Acharya Aditya Pratap and Anup Nandy},
  doi          = {10.1109/TCDS.2022.3222087},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1430-1448},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A novel probabilistic network model for estimating cognitive-gait connection using multimodal interface},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep slice-crossed network with local weighted loss for
brain metastases segmentation. <em>TCDS</em>, <em>15</em>(3), 1419–1429.
(<a href="https://doi.org/10.1109/TCDS.2022.3213944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive deficits occur in up to 90% of patients with brain metastases and can be caused by the tumor itself and whole brain radiation therapy. Latest treatment options to reduce cognitive decline require high-precision metastases segmentation. This asks existing metastases segmentation models to be further improved as their limited ability to micro lesion perception. To reduce the segmentation error on brain tissues and improve the accuracy on micro metastases, a deep slice-crossed network (SCNet) and a local weighted loss are proposed, respectively. The SCNet contains a gated feature fusion module where the “gate” is learned individually from the slice-crossed label to roughly localize metastases. Then, in tumor regions, this gate allows intraslice information to pass for metastases enhancement, while in other regions, it allows interslice information to pass for vascular screening. The local weighted loss is proposed to address the inconsistency in size of intraclass objects. Unlike existing weighting approaches, it calculates the weight of each pixel from its neighborhood pixel distribution to enhance the learning of pixels within small tumors, which can be applied to common losses. To evaluate them, two data sets containing 1000 and 368 patients, respectively, were collated. Experimental results demonstrate that the proposed method perform satisfactorily compared with state-of-the-art brain metastasis segmentation models.},
  archive      = {J_TCDS},
  author       = {Xin Shu and Lei Zhang and Jiao Qu and Lituan Wang and Zizhou Wang and Wenjing Zhang and Ying Wang and Su Lui},
  doi          = {10.1109/TCDS.2022.3213944},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1419-1429},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep slice-crossed network with local weighted loss for brain metastases segmentation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Electroencephalogram-based motor imagery brain–computer
interface using multivariate iterative filtering and spatial filtering.
<em>TCDS</em>, <em>15</em>(3), 1408–1418. (<a
href="https://doi.org/10.1109/TCDS.2022.3214081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In motor imagery (MI)-based brain–computer interface (BCI), common spatial pattern (CSP) is most popularly used for discriminant feature extraction. However, the performance of CSP depends on the operational frequency bands, which are selected manually or set to a broad frequency range in most of the previously developed applications. Due to subject to subject or even trial to trial variability of frequency band affected by MI task, these methods suffer from the poor performance. We have proposed a novel approach, using combination of multivariate iterative filtering (MIF) and CSP (MIFCSP), to automatically select optimal frequency bands based on MIF which can be further used for discriminant feature extraction. MIF decomposes the signal into several multivariate intrinsic mode functions, from which features are extracted using CSP. We select the minimum number of most significant features for which highest classification accuracy is achieved. Subsequently, linear discriminant analysis (LDA) classifier is used to classify different MI tasks. Experimental results for BCI competition IV data set 2a and BCI competition III-IIIa are presented. For left-hand versus right-hand MI classification, proposed MIFCSP method provides 83.18% and 84.44% average accuracy, respectively. Superior classification performance confirms that MIFCSP is a promising candidate for MI BCI application.},
  archive      = {J_TCDS},
  author       = {Kritiprasanna Das and Ram Bilas Pachori},
  doi          = {10.1109/TCDS.2022.3214081},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1408-1418},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Electroencephalogram-based motor imagery Brain–Computer interface using multivariate iterative filtering and spatial filtering},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GRIMGEP: Learning progress for robust goal sampling in
visual deep reinforcement learning. <em>TCDS</em>, <em>15</em>(3),
1396–1407. (<a href="https://doi.org/10.1109/TCDS.2022.3216911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autotelic reinforcement learning (RL) agents sample their own goals, and try to reach them. They often prioritize goal sampling according to some intrinsic reward, ex. novelty or absolute learning progress (ALPs). Novelty-based approaches work robustly in unsupervised image-based environments when there are no distractors. However, they construct simple curricula that do not take the agent’s performance into account: in complex environments, they often get attracted by impossible tasks. ALP-based approaches, which are often combined with a clustering mechanism, construct complex curricula tuned to the agent’s current capabilities. Such curricula sample goals on which the agent is currently learning the most, and do not get attracted by impossible tasks. However, ALP approaches have not so far been applied to DRL agents perceiving complex environments directly in the image space. Goal regions guided intrinsically motivated goal exploration process (GRIMGEP), without using any expert knowledge, combines the ALP clustering approaches with novelty-based approaches and extends them to those complex scenarios. We experiment on a rich 3-D image-based environment with distractors using novelty-based exploration approaches: Skewfit and CountBased. We show that wrapping them with GRIMGEP—using them only in the cluster sampled by ALP—creates a better curriculum. The wrapped approaches are attracted less by the distractors, and achieve drastically better performances.},
  archive      = {J_TCDS},
  author       = {Grgur Kovač and Adrien Laversanne-Finot and Pierre-Yves Oudeyer},
  doi          = {10.1109/TCDS.2022.3216911},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1396-1407},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {GRIMGEP: Learning progress for robust goal sampling in visual deep reinforcement learning},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid brain–computer interface combining p300 potentials
and emotion patterns for detecting awareness in patients with disorders
of consciousness. <em>TCDS</em>, <em>15</em>(3), 1386–1395. (<a
href="https://doi.org/10.1109/TCDS.2022.3213194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a hybrid brain–computer interface (BCI) system combining P300 potential and emotion patterns was proposed to improve the performance of awareness detection. Two video clips were flashed randomly to evoke the P300 potential, while a laughing or crying video clip was used to induce the corresponding emotion pattern. The subjects were asked to concentrate on the laughing or crying video clip cued by the instruction and to count the flashes of the corresponding video clip. Two layers of classification were developed. In the first layer, P300 detection and emotion recognition were performed separately using two support vector machine (SVM) classifiers. Specifically, the activation, spatial, and connection patterns were fused in emotion recognition. In the second layer, the SVM scores of P300 detection and emotion recognition were fed into another SVM classifier to determine which video clip the subjects responded to. Six healthy subjects and eight patients with disorders of consciousness (DOC) were involved in the command-following experiment. The results showed that the accuracy of the hybrid BCI system was better than those of the single-modality systems. Furthermore, three patients were able to perform tasks (66%–72%) using our hybrid BCI, which indicated their residual awareness and emotion-related abilities.},
  archive      = {J_TCDS},
  author       = {Jiahui Pan and Lei Wang and Haiyun Huang and Jun Xiao and Fei Wang and Qimei Liang and Chengwei Xu and Yuanqing Li and Qiuyou Xie},
  doi          = {10.1109/TCDS.2022.3213194},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1386-1395},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A hybrid Brain–Computer interface combining p300 potentials and emotion patterns for detecting awareness in patients with disorders of consciousness},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Path planning of unmanned autonomous helicopter based on
hybrid satisficing decision-enhanced swarm intelligence algorithm.
<em>TCDS</em>, <em>15</em>(3), 1371–1385. (<a
href="https://doi.org/10.1109/TCDS.2022.3212062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of the air combat environment, the optimal flights path is no longer the only requirement for the path planning system of an unmanned autonomous helicopter (UAH). To enable the UAH path planning system to efficiently handle path planning problems in complex environments, a path planning method is proposed based on satisficing decision-enhanced hybrid swarm intelligence in this article. First, the UAH path planning is modeled as the multiobjective optimization problem. Besides the traditional flight cost of path planning, the performance of UAH and the safety constraints are both considered in this article to establish the fitness function of path planning. Then, a hybrid satisficing decision-enhanced swarm intelligence (HSD-SI) path planning algorithm is proposed based on the satisficing decision method. Through the collaboration and feedback between the hybrid algorithms, the satisfaction enhancement factor is dynamically adjusted so that the HSD-SI algorithm has multiple optimization properties. Thus, the UAH path planning system based on the HSD-SI algorithm can intelligently plan satisfactory flight paths according to the requirements of the mission. Simulation results verify the feasibility and effectiveness of the HSD-SI algorithm in dealing with the UAH path planning problems.},
  archive      = {J_TCDS},
  author       = {Zengliang Han and Mou Chen and Shuyi Shao and Tongle Zhou and Qingxian Wu},
  doi          = {10.1109/TCDS.2022.3212062},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1371-1385},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Path planning of unmanned autonomous helicopter based on hybrid satisficing decision-enhanced swarm intelligence algorithm},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Patient-specific seizure prediction from
electroencephalogram signal via multichannel feedback capsule network.
<em>TCDS</em>, <em>15</em>(3), 1360–1370. (<a
href="https://doi.org/10.1109/TCDS.2022.3212019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the use of convolutional neural networks (CNNs) has been common in electroencephalogram (EEG)-based seizure prediction. However, CNNs lose local and global connections and spatial information due to local connections and pooling operations. In addition, existing seizure prediction methods often require the design of special feature pre-extraction steps. Therefore, an end-to-end patient-specific seizure predictor based on the feedback capsule network (FB-CapsNet) is proposed in this study. It can characterize complex temporal information and precise spatial relationships, capture and integrate spatiotemporal properties directly from the raw EEG signal, and distinguish seizure states. The proposed FB-CapsNet first uses a feedback network to extract temporal information and 1-D convolution to reduce the data dimensionality. Then, it uses the capsule network to capture spatial information and other instantiation properties and store them in capsule vectors. Finally, it performs information flow between low-level and high-level capsules by a dynamic routing mechanism to obtain superior classification performance. The proposed FB-CapsNet achieves 95.7% sensitivity, 0.087/h false positive rate (FPR), and 0.948 area under curve (AUC) on the CHB-MIT scalp data set and 88.6% sensitivity, 0.127/h FPR, and 0.837 AUC on Kaggle data set.},
  archive      = {J_TCDS},
  author       = {Chang Li and Yuchang Zhao and Rencheng Song and Xiang Liu and Ruobing Qian and Xun Chen},
  doi          = {10.1109/TCDS.2022.3212019},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1360-1370},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Patient-specific seizure prediction from electroencephalogram signal via multichannel feedback capsule network},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic weighted filter bank domain adaptation for motor
imagery brain–computer interfaces. <em>TCDS</em>, <em>15</em>(3),
1348–1359. (<a href="https://doi.org/10.1109/TCDS.2022.3209801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A motor imagery (MI)-based brain–computer interface (BCI) is a promising system that can help neuromuscular injury patients recover or replace their motor abilities. Currently, before one uses MI-BCI, we need to collect a large amount of training data to train the decoding model, and this process is time consuming. When trained with a small amount of data, existing decoding methods generally do not perform well in MI decoding tasks. Therefore, it is important to improve the decoding performance with short calibration data. In this study, we propose a dynamic weighted filter bank domain adaptation framework that uses data from an existing subject to reduce the requirement of data from the new subject. A filter bank is used to explore information from different frequency subbands. A feature extractor with two 1-D convolutional layers is designed to extract electroencephalography features. The class-specific Wasserstein generative adversarial network (WGAN)-based domain adaptation network aligns the distribution of each class between the data from the new subject and the data from the existing subject. Additionally, we apply an attention network to dynamically allocate different weights for different frequency bands. We evaluate our method on a public MI data set and a self-collected data set. The experimental results show that the proposed method achieves the best decoding accuracy among the compared methods with different amounts of training data. On the public data set, our method achieves 8.88% and 7.16% higher decoding accuracy than the best comparing method with one block of training data on the two sessions, respectively. This indicates that our method can enhance MI decoding accuracy with a small amount of training data.},
  archive      = {J_TCDS},
  author       = {Yukun Zhang and Shuang Qiu and Wei Wei and Xuelin Ma and Huiguang He},
  doi          = {10.1109/TCDS.2022.3209801},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1348-1359},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Dynamic weighted filter bank domain adaptation for motor imagery Brain–Computer interfaces},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Double articulation analyzer with prosody for unsupervised
word and phone discovery. <em>TCDS</em>, <em>15</em>(3), 1335–1347. (<a
href="https://doi.org/10.1109/TCDS.2022.3210751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word and phone discovery are important tasks in the language development of human infants. Infants acquire words and phones from unsegmented speech signals using segmentation cues, such as distributional, prosodic, and co-occurrence information. Many pre-existing computational models designed to represent this process tend to focus on distributional or prosodic cues. In this study, we propose a nonparametric Bayesian probabilistic generative model called the prosodic hierarchical Dirichlet process-hidden language model (prosodic HDP-HLM) designed to perform simultaneous phone and word discovery from continuous speech signals encoded as time-series data that may exhibit a double articulation structure. Prosodic HDP-HLM, as an extension of HDP-HLM, considers both prosodic and distributional cues within a single integrative generative model. We further propose a prosodic double articulation analyzer (Prosodic DAA) based on an inference procedure derived for prosodic HDP-HLM. We conducted three experiments on different types of data sets, including, Japanese vowel sequence, utterances for teaching object names and features, and utterances following Zipf’s law, and the results demonstrated the validity of the proposed method. The results show that the Prosodic DAA successfully used prosodic cues and was able to discover words directly from continuous human speech using distributional and prosodic information in an unsupervised manner, outperforming a method that solely used distributional cues. In contrast, the phone discovery performance did not improve. We also show that prosodic cues contributed to word discovery performance more when the word frequency was distributed more naturally, i.e., following Zipf’s law.},
  archive      = {J_TCDS},
  author       = {Yasuaki Okuda and Ryo Ozaki and Soichiro Komura and Tadahiro Taniguchi},
  doi          = {10.1109/TCDS.2022.3210751},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1335-1347},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Double articulation analyzer with prosody for unsupervised word and phone discovery},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A complementary dual-branch network for appearance-based
gaze estimation from low-resolution facial image. <em>TCDS</em>,
<em>15</em>(3), 1323–1334. (<a
href="https://doi.org/10.1109/TCDS.2022.3210219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating gaze from a low-resolution (LR) facial image is a challenging task. Most current networks for gaze estimation focus on using face images of adequate resolution. Their performance degrades when the image resolution decreases due to information loss. This work aims to explore more helpful face and gaze information in a novel way to alleviate the problem of information loss in the LR gaze estimation task. Considering that all faces have a relatively fixed structure, it is feasible to reconstruct the residual information of face and gaze based on the solid constraint of the prior knowledge of face structure through learning an end-to-end mapping from pairs of low- and high-resolution (HR) images. This article proposes a complementary dual-branch network (CDBN) to achieve this task. A fundamental branch is designed to extract features of the major structural information from LR input. A residual branch is employed to reconstruct features containing the residual information as a supplement under the supervision of both the HR image and gaze direction. These two features are then fused and processed for gaze estimation. Experimental results on three widely used data sets, MPIIFaceGaze, EYEDIAP, and RT-GENE, show that the proposed CDBN achieves more accurate gaze estimation from the LR input image compared with the state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Zhesi Zhu and Dong Zhang and Cailong Chi and Ming Li and Dah-Jye Lee},
  doi          = {10.1109/TCDS.2022.3210219},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1323-1334},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A complementary dual-branch network for appearance-based gaze estimation from low-resolution facial image},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reinforcement learning method for rearranging scattered
irregular objects inside a crate. <em>TCDS</em>, <em>15</em>(3),
1314–1322. (<a href="https://doi.org/10.1109/TCDS.2022.3209680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arranging objects from a random and scattered distribution into an integral part has many applications, including bin packing, logistics, and other industrial fields. Measurement noises, manipulation uncertainties, models of irregular objects, and rich contacts bring considerable challenges to the improvements of the overall performance, such as seamlessness of the final pattern and task efficiency. In this article, we propose an end-to-end reinforcement learning strategy that generates a series of pushing movements for scattered irregular objects inside a crate. An abstracted and sparse reward function is proposed to evaluate the pushing performance, and the proximal policy optimization (PPO) learning method that simultaneously trains a convolutional neural network (CNN) and a fully connected actor network is developed for end-to-end decision making. The proposed method is evaluated in both simulation and real-world scenarios. The results show that the proposed method can arrange the scattered objects tightly to fit into each other in an efficient and flexible way, and can be transferred to the real world with unseen objects.},
  archive      = {J_TCDS},
  author       = {Liang Tang and Hao Liu and Huang Huang and Xinru Xie and Nailong Liu and Mou Li},
  doi          = {10.1109/TCDS.2022.3209680},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1314-1322},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A reinforcement learning method for rearranging scattered irregular objects inside a crate},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effect of a body part action on body perception of the other
inactive body part. <em>TCDS</em>, <em>15</em>(3), 1301–1313. (<a
href="https://doi.org/10.1109/TCDS.2022.3210659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rubber hand illusion (RHI) is an illusory experience in which a fake rubber hand is felt as if it were one’s own hand when the visible fake and invisible real hands are stimulated synchronously. Although several studies have suggested contributions of action to body ownership using the RHI paradigm, the relationship between body ownership and agency has not yet been fully revealed. To better understand this relationship, the present study investigated the transfer of body ownership between body parts induced by a body part action. Using an RHI paradigm involving virtual reality, we tested whether a simple finger action of the dominant (active) hand can induce the embodiment of a virtual hand corresponding to the nondominant (inactive) hand. We evaluated the illusory experience, perceptual changes, and physiological changes during the experiment with a subjective questionnaire, crossmodal congruency effect measurement, and skin temperature measurement, respectively. The results demonstrated that the finger action induced the embodiment of both virtual hands, causing a significant increase in agency of the inactive hand. This suggests that the illusory experience induced by an active body part contributes to an increase in agency as well as body ownership over the virtual body of the other inactive body part.},
  archive      = {J_TCDS},
  author       = {Masayuki Hara and Hisato Sugata and Naofumi Otsuru and Masaya Takasaki and Yuji Ishino and Takeshi Mizuno and Masahito Miki and Noriaki Kanayama},
  doi          = {10.1109/TCDS.2022.3210659},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1301-1313},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Effect of a body part action on body perception of the other inactive body part},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and application of biomimetic memory circuit based on
hippocampus mechanism. <em>TCDS</em>, <em>15</em>(3), 1289–1300. (<a
href="https://doi.org/10.1109/TCDS.2022.3205033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hippocampus, a special neuroanatomical structures, has been a realistic research model for the storage and retrieval of short-term and long-term memory. This article proposes a mathematic hippocampus model and bionic memory circuit which not only emulates the memory generation but also realizes the transformation from low to high-level memory. Based on the interior connections of the hippocampus, the proposed circuit reconstructs an episodic memory processing model and achieves the functions of multilevel memory generation. Memristor plays a vital role in imitating the plasticity of synapses in hippocampus recurrence, and its characteristics of switching dynamics are applied for controlling multilevel memory generation. Leveraging the proposed circuit, we propose a multilevel memorial generation system which has the capacities of perception quantification, memorial generation, and comprised the following: 1) receiver module; 2) quantitative module; 3) three-layer hippocampus memory circuit; and 4) memory generation module. The simulation results in PSpice indicate that the application of the model can quantize the episodic memory, afterward processing it by a three-layer hippocampus memory circuit to generate the multilevel memory. Moreover, this work paves the way for the memorial architecture in robotics by emulating the hippocampus memory principle.},
  archive      = {J_TCDS},
  author       = {Hegan Chen and Qinghui Hong and Wenqi Liu and Zhongrui Wang and Jiliang Zhang},
  doi          = {10.1109/TCDS.2022.3205033},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1289-1300},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Design and application of biomimetic memory circuit based on hippocampus mechanism},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction of treatment outcome in major depressive disorder
using ensemble of hybrid transfer learning and long short-term memory
based on EEG signal. <em>TCDS</em>, <em>15</em>(3), 1279–1288. (<a
href="https://doi.org/10.1109/TCDS.2022.3207350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major depressive disorder (MDD) is a widespread global mental disease. The effectiveness of selective serotonin reuptake inhibitors (SSRIs) antidepressants prescribed for MDD patients is limited and pretreatment assessment of treatment outcome is a vital task. In this study, a hybrid model based on transfer learning (TL) of powerful pretrained deep convolutional neural networks (CNNs) empowered with bidirectional long short-term memory (BLSTM) cells and attention mechanism were developed to classify responders (R) and nonresponders (NR) to SSRI antidepressants, using raw data images of pretreatment electro-encephalogram (EEG) signal obtained from 30 MDD patients. TL-LSTM-Attention models based on VGG16, Xception, and Densenet121 models were created for the classification of R/NR. Results show that the highest performance of hybrid TL-LSTM-Attention models was achieved by VGG16-LSTM-Attention with an accuracy of 98.21%, sensitivity of 96.22%, and specificity of 99.67%. An ensemble model based on weighted majority voting among hybrid models is constructed to surpass the performance of each single hybrid model. Our proposed ensemble model gained accuracy, sensitivity, and specificity of 98.84%, 97.80%, and 99.60%, respectively. Hence, the proposed model which is an ensemble of TL-LSTM-Attention models fed by raw data images of EEG signal leads to high confidence in the prediction of antidepressants treatment outcome.},
  archive      = {J_TCDS},
  author       = {Mohsen Sadat Shahabi and Ahmad Shalbaf},
  doi          = {10.1109/TCDS.2022.3207350},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1279-1288},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Prediction of treatment outcome in major depressive disorder using ensemble of hybrid transfer learning and long short-term memory based on EEG signal},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual place recognition via a multitask learning method
with attentive feature aggregation. <em>TCDS</em>, <em>15</em>(3),
1263–1278. (<a href="https://doi.org/10.1109/TCDS.2022.3206500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual place recognition has gained popularity in recent years. Mainstream convolutional neural network-based methods formulate it as a ranking task and optimize it in the paradigm of deep metric learning, however, the ranking-motivated losses concern only the ranking relationship for each query image and the compactness of intraplace feature distribution is seldom considered. It is still challenging due to varying viewpoints, illuminations, and even dynamic objects. In this article, a novel multitask learning framework is proposed, which combines the existing triplet ranking task and our designed binary classification task to jointly optimize the network for better generalization capability. Specifically, a binary classification network with the corresponding binary cross-entropy loss is designed in the classification task. In this way, the intraplace feature compactness and interplace feature separability are reinforced. At the testing stage, this classification network is discarded without increasing the computation cost. Furthermore, an attention module is presented to promote the network to concentrate on the salient regions by assigning different importance to each spatial position. Our method achieves the top-10 recalls of 97.27%, 94.6%, and 96.93% on Pitts250k-test, Tokyo 24/7, and TokyoTM-val data sets, respectively. Extensive experiments prove that the proposed network can learn discriminative global features with better robustness to viewpoints and environmental variations.},
  archive      = {J_TCDS},
  author       = {Peiyu Guan and Zhiqiang Cao and Junzhi Yu and Min Tan and Shuo Wang},
  doi          = {10.1109/TCDS.2022.3206500},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1263-1278},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Visual place recognition via a multitask learning method with attentive feature aggregation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PAU-net: Privileged action unit network for facial
expression recognition. <em>TCDS</em>, <em>15</em>(3), 1252–1262. (<a
href="https://doi.org/10.1109/TCDS.2022.3203822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) plays a vital role in affective cognition. However, there will be some limitations when facing the FER with single facial image data. Considering that extra data contains more information for molding, the facial action unit (AU) can be adopted as privileged information (PI) to assist the FER task. This article integrates AU information into an end-to-end deep network to support FER training. The proposed privileged action unit network (PAU-Net) gives ways of integrating AU information from the input aspect (type I) and output aspect (type II). Type I of PAU-Net takes AUs as input to guide the facial image network learning, which provides the AU-based emotion recognition result for the image-based FER model. While, type II of PAU-Net utilizes AUs as the output label for shallow layers of the network, which helps the model learn AU- related features and further assists advanced facial expression feature learning in subsequent layers. Note that PI enhances the network during the training and will not occur during the testing. Therefore, the network can still perform robustly with original input data in practice. Experiments are based on the CK+, MMI, and Oulu-CASIA data sets. The experimental results demonstrate the effectiveness of the proposed PAU-Net in FER tasks.},
  archive      = {J_TCDS},
  author       = {Xuehan Wang and Tong Zhang and C. L. Philip Chen},
  doi          = {10.1109/TCDS.2022.3203822},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1252-1262},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {PAU-net: Privileged action unit network for facial expression recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep decoupling classification and regression for visual
tracking. <em>TCDS</em>, <em>15</em>(3), 1239–1251. (<a
href="https://doi.org/10.1109/TCDS.2022.3202802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification and regression are two tasks that most Siamese-based trackers need to handle. However, most of the existing trackers only learn one feature embedding to handle these two types of task, making it difficult to optimize both simultaneously. To solve this problem, this article tries to deeply decouple classification and regression in the model structure. Specifically, two feature extraction backbone networks are used to divide the model into two branches to extract the heterogeneous features suitable for the two tasks, respectively. Inspired by the core idea of transformer, information interaction and fusion between multiple branches are achieved by the cross-attention mechanism, which can fully exploit the deep information dependence between multiple branches. In addition, the concept of channel-level information interaction is proposed by innovatively changing the generation mode of vector groups in the attention module. The experiments show that double Siamese tracker (DST) designed in this article greatly improves the accuracy of classification and regression. DST runs at 60 frames per second (FPS) on GPU, far above the real-time requirement.},
  archive      = {J_TCDS},
  author       = {Guang Han and Ruiyu Yang and Hua Gao and Sam Kwong},
  doi          = {10.1109/TCDS.2022.3202802},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1239-1251},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep decoupling classification and regression for visual tracking},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning human-to-robot dexterous handovers for
anthropomorphic hand. <em>TCDS</em>, <em>15</em>(3), 1224–1238. (<a
href="https://doi.org/10.1109/TCDS.2022.3203025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–robot interaction plays an important role in robots serving human production and life. Object handover between humans and robotics is one of the fundamental problems of human–robot interaction. The majority of current work uses parallel-jaw grippers as the end-effector device, which limits the ability of the robot to grab miscellaneous objects from human and manipulate them subsequently. In this article, we present a framework for human-to-robot dexterous handover using an anthropomorphic hand. The framework takes images captured by two cameras to complete handover scene understanding, grasp configurations prediction, and handover execution. To enable the robot to generalize to diverse delivered objects with miscellaneous shapes and sizes, we propose an anthropomorphic hand grasp network (AHG-Net), an end-to-end network that takes the single-view point clouds of the object as input and predicts the suitable anthropomorphic hand configurations with five different grasp taxonomies. To train our model, we build a large-scale data set with 1M hand grasp annotations from 5K single-view point clouds of 200 objects. We implement a handover system using a UR5 robot arm and HIT-DLR II anthropomorphic robot hand based on our presented framework, which can not only adapt to different human givers but generalize to diverse novel objects with various shapes and sizes. The generalizability, reliability, and robustness of our method are demonstrated on 15 different novel objects with arbitrary handover poses from frontal and lateral positions, a system ablation study, a grasp planner comparison, and a user study on 6 participants delivering 15 objects from two benchmark sets.},
  archive      = {J_TCDS},
  author       = {Haonan Duan and Peng Wang and Yiming Li and Daheng Li and Wei Wei},
  doi          = {10.1109/TCDS.2022.3203025},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1224-1238},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning human-to-robot dexterous handovers for anthropomorphic hand},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cerebellum-inspired prediction and correction model for
motion control of a musculoskeletal robot. <em>TCDS</em>,
<em>15</em>(3), 1209–1223. (<a
href="https://doi.org/10.1109/TCDS.2022.3200839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is an important issue that how to regulate the existing control models of musculoskeletal robots to improve the ability of motion learning and generalization. In this article, based on the motion modulation function of the cerebellum, a cerebellum-inspired prediction and correction model is proposed to carry out feedforward regulation of the original controller. First, drawing on the reservoir computing mechanism of the cerebellar granular layer, the cerebellum prediction model is established by using the echo state network. Incremental learning for the network is achieved using the replay method, which is able to process control signals with different distributions. The cerebellum prediction network can accurately predict the motion results of the robot under the action of the time-series control signals. Second, referring to the neural pathways of the cerebellum, the cerebellum correction model is constructed. The network learning rules are designed by drawing on the long-term potentiation and depression processes of cerebellar synaptic plasticity. The characteristics of the parameters in the network weight update equations are further analyzed. And the hyperparameter update rules of the correction network weights are proposed. To simulate the function of the cerebellum involved in the limb ballistic movement, the adaptive adjustment method for cerebellum correction duration is proposed. The cerebellum correction network can accurately modulate the original control signals by using the motion prediction results. In experiments, a musculoskeletal robot is used to verify the movement effects under the control of the cerebellum model. The results show that the cerebellum-inspired model can effectively improve the motion accuracy and learning efficiency of the musculoskeletal robot, and enhance the motion generalization ability and system robustness of the robot.},
  archive      = {J_TCDS},
  author       = {Jinhan Zhang and Jiahao Chen and Wei Wu and Hong Qiao},
  doi          = {10.1109/TCDS.2022.3200839},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1209-1223},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A cerebellum-inspired prediction and correction model for motion control of a musculoskeletal robot},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bionic dual-loop emotional learning circuit and its
application in radiation early warning monitoring. <em>TCDS</em>,
<em>15</em>(3), 1196–1208. (<a
href="https://doi.org/10.1109/TCDS.2022.3200470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional learning is a very important learning mechanism for the human body, and it is also an important part of artificial intelligence. This article proposes a bionic circuit, which simulates the dual loop of emotional learning in the human brain. The circuit uses the characteristics of the memristor to simulate the fast loop that quickly generates initial emotions to predict danger and the detour loop which generates a more refined emotion by further processing of the input information. The circuit includes: 1) thalamus module; 2) sensory cortex module; and 3) amygdala module. The first module sends the strongest input stimulus directly to the amygdala module for rapid processing. The second module processes the input stimuli and sends them to the amygdala module for emotion generation and judgment. The third module is responsible for generating output signals representing the corresponding emotions. The simulation results in PSPICE show that the output signal generated by the circuit can represent the characteristics of the dual loop of emotional learning, and more realistically simulate the biological process. By configuring the parameters, this circuit can be more flexibly applied to the radiation intensity early warning system to protect the safety of humans when working in a radiation environment.},
  archive      = {J_TCDS},
  author       = {Hanpu Zhou and Zhuoying Fei and Qinghui Hong and Jingru Sun and Sichun Du and Tao Li and Jiliang Zhang},
  doi          = {10.1109/TCDS.2022.3200470},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1196-1208},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Bionic dual-loop emotional learning circuit and its application in radiation early warning monitoring},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Query path generation via bidirectional reasoning for
multihop question answering from knowledge bases. <em>TCDS</em>,
<em>15</em>(3), 1183–1195. (<a
href="https://doi.org/10.1109/TCDS.2022.3198272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multihop question answering from knowledge bases (KBQA) is a hot research topic in natural language processing. Recently, the graph neural network-based (GNN-based) methods have achieved promising results as the KB can be organized as a knowledge graph (KG). However, they often suffered from the sparsity of the KG which was detrimental to the structure encoding and reasoning capabilities of GNN. Specifically, a KG is a sparse graph linked by directed relations and previous studies have paid scant attention to the directional characteristic of relations in the KG, limiting the patterns of relation path that GNN-based approaches could resolve. This study proposes a bidirectional recurrent GNN (BRGNN) to tackle these difficulties. To model the bidirectional information of relations, all adjacent relations of an entity are grouped by their directions, and they are separately aggregated into the entity representation in outward and inward directions. For the reasoning process, BRGNN simultaneously considers the neighbor relations in both directions to cover more patterns of relation paths and improve the recall of answers. Extensive experiments on three benchmarks: WebQuestionsSP, ComplexWebQuestions, and MetaQA, verify that BRGNN can answer more questions by taking into account the directional information, and it is competitive to all state-of-the-art approaches.},
  archive      = {J_TCDS},
  author       = {Geng Zhang and Jin Liu and Guangyou Zhou and Zhiwen Xie and Xiao Yu and Xiaohui Cui},
  doi          = {10.1109/TCDS.2022.3198272},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1183-1195},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Query path generation via bidirectional reasoning for multihop question answering from knowledge bases},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructing cognitive reasoning and decision making under
attribute granular computing using fuzzy petri nets. <em>TCDS</em>,
<em>15</em>(3), 1170–1182. (<a
href="https://doi.org/10.1109/TCDS.2022.3197616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the philosophical point of view, granular computing (GrC) attempts to abstract and formalize the way of human cognition to extract a structured thinking mode. Fuzzy Petri nets (FPNs) have a strong formal description and fuzzy reasoning abilities, which align with human cognitive thinking. However, the traditional FPNs are challenging to describe the following problems: formally expressing the transformation relations between information granules and information granule levels, and then carrying out formal reasoning and decision making based on cognitive thinking mode. This article introduces the qualitative mapping and the qualitative criterion transformation method to explore this problem. First, the problem of attribute information granulation is discussed by using the equivalence relation of attributes. Second, qualitative mapping is used to express the transformation relationship between information granules. The transformation relationship between attribute information granule levels is realized by qualitative criterion transformation, and the basic algebraic structure of attribute information granules is given. The basic network elements of FPNs can formally describe the relationships between these information granules. Finally, a cognitive reasoning and decision-making model for the formal description of attribute information granules based on the new FPN is constructed, and the proposed case study verifies the feasibility of the proposed method.},
  archive      = {J_TCDS},
  author       = {Ruqi Zhou and Huiyou Chang and Yuepeng Zhou and Jun Xu and Yonghe Lu and Jiali Feng},
  doi          = {10.1109/TCDS.2022.3197616},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1170-1182},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Constructing cognitive reasoning and decision making under attribute granular computing using fuzzy petri nets},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BGL-net: A brain-inspired global-local information fusion
network for alzheimer’s disease based on sMRI. <em>TCDS</em>,
<em>15</em>(3), 1161–1169. (<a
href="https://doi.org/10.1109/TCDS.2022.3204782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is an irreversible neurodegenerative disease, the most common form of dementia, affecting millions worldwide. Neuroimaging-based early AD diagnosis has become an effective approach, especially by using structural magnetic resonance imaging (sMRI). The convolutional neural network (CNN)-based method is challenging to learn dependencies between spatially distant positions in the various brain regions due to its local convolution operation. In contrast, the graph convolutional network (GCN)-based work can connect the brain regions to capture global information but is not sensitive to the local information in a single brain region. Unlike a separate CNN or GCN-based method, we proposed a brain-inspired global-local information fusion network (BGL-Net) to diagnose AD. It essentially inherits the advantages of both CNN and GCN. The experiments on three public data sets demonstrate the effectiveness and robustness of our BGL-Net. Our method achieved the best performance on three popular public data sets compared with the existing CNN and GCN-based methods. In addition, our visualization results of the learned brain connection on AD and normal people agree with many current AD clinical research.},
  archive      = {J_TCDS},
  author       = {Chen-Chen Fan and Hongjun Yang and Liang Peng and Xiao-Hu Zhou and Zhen-Liang Ni and Yan-Jie Zhou and Sheng Chen and Zeng-Guang Hou},
  doi          = {10.1109/TCDS.2022.3204782},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1161-1169},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {BGL-net: A brain-inspired global-local information fusion network for alzheimer’s disease based on sMRI},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible learning models utilizing different neural
plasticities. <em>TCDS</em>, <em>15</em>(3), 1150–1160. (<a
href="https://doi.org/10.1109/TCDS.2022.3197463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing models for vestibulo-ocular reflex (VOR) and optokinetic response (OKR) learning utilize neural circuit structure and capture a few characteristics of these two learning systems. However, it remains unclear how the error signals guide these learning processes. Here, we propose novel dynamic learning models using error feedback in a flexible fashion to account for both VOR and OKR learning. We first used a feedback modulation model (FMM) and found the error signals play an essential guiding role in the gain compensation of wild-type mice. However, this FMM cannot accurately reproduce gain changes during the recovery period. Therefore, we propose a nonuniform FMM using flexible plasticity learning rules of different memory sites to take into account the effect of classical linearity models in both training and recovery periods. To further study learning characteristics of gain reduction, we introduce a reversal-phase FMM and explore the contribution of synaptic plasticity to adaptive learning, in which characteristics and bidirectional synaptic plasticity in the VOR-decrease learning mode can be fully recovered. Taken together, our results suggest that, to explain VOR and OKR learning systems, one needs dynamical models with flexible and multiple components at different or same sites of neuronal circuits.},
  archive      = {J_TCDS},
  author       = {Lingling An and Ye Yuan and Yunhao Liu and Fan Zhao and Quan Wang and Jian K. Liu},
  doi          = {10.1109/TCDS.2022.3197463},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1150-1160},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Flexible learning models utilizing different neural plasticities},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recent advances of deep robotic affordance learning: A
reinforcement learning perspective. <em>TCDS</em>, <em>15</em>(3),
1139–1149. (<a href="https://doi.org/10.1109/TCDS.2023.3277288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular concept proposed in the field of psychology, affordance has been regarded as one of the important abilities that enable humans to understand and interact with the environment. Briefly, it captures the possibilities and effects of the actions of an agent applied to a specific object or, more generally, a part of the environment. This article provides a short review of the recent developments of deep robotic affordance learning (DRAL), which aims to develop data-driven methods that use the concept of affordance to aid in robotic tasks. We first classify these papers from a reinforcement learning (RL) perspective and draw connections between RL and affordances. The technical details of each category are discussed and their limitations are identified. We further summarize them and identify future challenges from the aspects of observations, actions, affordance representation, data-collection, and real-world deployment. A final remark is given at the end to propose a promising future direction of the RL-based affordance definition to include the predictions of arbitrary action consequences.},
  archive      = {J_TCDS},
  author       = {Xintong Yang and Ze Ji and Jing Wu and Yu-Kun Lai},
  doi          = {10.1109/TCDS.2023.3277288},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1139-1149},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Recent advances of deep robotic affordance learning: A reinforcement learning perspective},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep-learning-based diagnosis and prognosis of alzheimer’s
disease: A comprehensive review. <em>TCDS</em>, <em>15</em>(3),
1123–1138. (<a href="https://doi.org/10.1109/TCDS.2023.3254209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is the most prevalent neurodegenerative disorder and the most common cause of Dementia. Neuroimaging analyses, such as T1 weighted magnetic resonance imaging, positron emission tomography, and the deep learning (DL) approaches have attracted researchers for automated AD diagnosis in the early stages. Therefore, a review is required to understand DL algorithms to develop more efficient AD diagnosis methods. This article discusses a detailed review of automated early AD diagnosis using DL methods published from 2009 to 2022. The novelties of this article include: 1) introducing popular imaging modalities; 2) discussing early biomarkers for AD diagnosis using neuroimaging scans; 3) reviewing the popular online available data sets widely used; 4) systematically describing the various DL algorithms for accurate and early assessment of AD; 5) discussion on advantages and limitations of the DL-based model for AD diagnosis; and 6) provides an outlook toward future trends derived from our critical assessment.},
  archive      = {J_TCDS},
  author       = {Rahul Sharma and Tripti Goel and M. Tanveer and C. T. Lin and R. Murugan},
  doi          = {10.1109/TCDS.2023.3254209},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1123-1138},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep-learning-based diagnosis and prognosis of alzheimer’s disease: A comprehensive review},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Applied exoskeleton technology: A comprehensive review of
physical and cognitive human–robot interaction. <em>TCDS</em>,
<em>15</em>(3), 1102–1122. (<a
href="https://doi.org/10.1109/TCDS.2023.3241632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exoskeletons and orthoses are wearable mobile systems providing mechanical benefits to users. Despite significant improvements in the last decades, the technology is not fully mature to be adopted for strenuous and nonprogrammed tasks. To accommodate this insufficiency, different aspects of this technology need to be analyzed and improved. Numerous studies have tried to address some aspects of exoskeletons, e.g., mechanism design, intent prediction, and control scheme. However, most works have focused on a specific element of design or application without providing a comprehensive review framework. This study aims to analyze and survey the contributing aspects to this technology’s improvement and broad adoption. To address this, after introducing assistive devices and exoskeletons, the main design criteria will be investigated from both physical human–robot interaction (HRI) perspectives. In order to establish an intelligent HRI strategy and enable intuitive control for users, cognitive HRI will be investigated after a brief introduction to various approaches to their control strategies. The study will be further developed by outlining several examples of known assistive devices in different categories. Some guidelines for exoskeleton selection and possible mitigation of current limitations will be discussed.},
  archive      = {J_TCDS},
  author       = {Farhad Nazari and Navid Mohajer and Darius Nahavandi and Abbas Khosravi and Saeid Nahavandi},
  doi          = {10.1109/TCDS.2023.3241632},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1102-1122},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Applied exoskeleton technology: A comprehensive review of physical and cognitive Human–Robot interaction},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noninvasive brain imaging and stimulation in post-stroke
motor rehabilitation: A review. <em>TCDS</em>, <em>15</em>(3),
1085–1101. (<a href="https://doi.org/10.1109/TCDS.2022.3232581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides a comprehensive review of the current state of noninvasive brain imaging and brain stimulation in motor rehabilitation after stroke. The functional organization of the brain is determined by brain network connections. Brain imaging enables us to map the network organization of the brain, so as to assess the motor function in a stroke state. The noninvasive functional imaging techniques include electroencephalography (EEG) and magnetoencephalography (MEG) with the better temporal resolution, and functional magnetic resonance imaging (fMRI) and functional near-infrared spectroscopy (fNIRS) with better spatial resolution. Based on single-mode or multimode imaging methods, the changes of brain networks caused by stroke and during motor recovery progress can be observed. Brain stimulation is an effective way to reorganize brain networks. Transcranial magnetic stimulation (TMS) and transcranial direct current stimulation (tDCS) are the most widely studied noninvasive neuromodulation techniques. Combined with imaging, brain stimulation can alter brain networks in controlled ways and making neuromodulation a closed loop. This review also outlines the methods of facilitate motor rehabilitation: 1) multimode imaging for motor assessment; 2) brain stimulation combined with rehabilitation training; 3) close-loop motor rehabilitation; and 4) influencing factors motor rehabilitation.},
  archive      = {J_TCDS},
  author       = {Hui Chang and Yixuan Sheng and Jinbiao Liu and Hongyu Yang and Xiangyu Pan and Honghai Liu},
  doi          = {10.1109/TCDS.2022.3232581},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1085-1101},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Noninvasive brain imaging and stimulation in post-stroke motor rehabilitation: A review},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward teachable autotelic agents. <em>TCDS</em>,
<em>15</em>(3), 1070–1084. (<a
href="https://doi.org/10.1109/TCDS.2022.3231731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous discovery and direct instruction are two distinct sources of learning in children but education sciences demonstrate that mixed approaches such as assisted discovery or guided play result in improved skill acquisition. In the field of artificial intelligence (AI), these extremes, respectively, map to autonomous agents learning from their own signals and interactive learning agents fully taught by their teachers. In between should stand teachable autonomous agents (TAA): agents that learn from both internal and teaching signals to benefit from the higher efficiency of assisted discovery. Designing such agents will enable real-world nonexpert users to orient the learning trajectories of agents toward their expectations. More fundamentally, this may also be a key step to build agents with human-level intelligence. This article presents a roadmap toward the design of teachable autonomous agents from a developmental AI perspective. Building on developmental psychology and education sciences, we start by identifying key features enabling assisted discovery processes in child–tutor interactions. This leads to the production of a checklist of features that future TAAs will need to demonstrate. The checklist allows us to precisely pinpoint the various limitations of current reinforcement learning agents and to identify the promising first steps toward TAAs. It also shows the way forward by highlighting key research directions toward the design or autonomous agents that can be taught by ordinary people via natural pedagogy.},
  archive      = {J_TCDS},
  author       = {Olivier Sigaud and Ahmed Akakzia and Hugo Caselles-Dupré and Cédric Colas and Pierre-Yves Oudeyer and Mohamed Chetouani},
  doi          = {10.1109/TCDS.2022.3231731},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1070-1084},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Toward teachable autotelic agents},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep insights of learning-based micro expression
recognition: A perspective on promises, challenges, and research needs.
<em>TCDS</em>, <em>15</em>(3), 1051–1069. (<a
href="https://doi.org/10.1109/TCDS.2022.3226348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro expression recognition (MER) is a very challenging area of research due to its intrinsic nature and fine-grained changes. In the literature, the problem of MER has been solved through handcrafted/descriptor-based techniques. However, in recent times, deep learning (DL)-based techniques have been adopted to gain higher performance for MER. Also, rich survey articles on MER are available by summarizing the data sets, experimental settings, conventional, and DL methods. In contrast, these studies lack the ability to convey the impact of network design paradigms and experimental setting strategies for DL-based MER. Therefore, this article aims to provide a deep insight into the DL-based MER frameworks with a perspective on promises in network model designing, experimental strategies, challenges, and research needs. Also, the detailed categorization of available MER frameworks is presented in various aspects of model design and technical characteristics. Moreover, an empirical analysis of the experimental and validation protocols adopted by MER methods is presented. The challenges mentioned earlier and network design strategies may assist the affective computing research community in forge ahead in MER research. Finally, we point out the future directions, research needs, and draw our conclusions.},
  archive      = {J_TCDS},
  author       = {Monu Verma and Santosh Kumar Vipparthi and Girdhari Singh},
  doi          = {10.1109/TCDS.2022.3226348},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1051-1069},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep insights of learning-based micro expression recognition: A perspective on promises, challenges, and research needs},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of distraction effect of music stimuli during
cycling exercise with low intensity in terms of multiple time scale.
<em>TCDS</em>, <em>15</em>(3), 1043–1050. (<a
href="https://doi.org/10.1109/TCDS.2022.3157915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a lack of research on the distraction effects of music stimuli with the multiple time scales of biological functions. It should be preferable to propose an effective strategy for evaluating the time-varying behavior (TVB) of psychological responses to music during exercise on autonomic nervous activity (ANA). From the RR interval time series of electrocardiograms recorded during cycling with low intensity, we estimated ANA-related indices: time- and frequency-domain indices estimated from the fluctuations of the RR interval. Then, we searched the temporal distribution of the stimulus-response module (SRM) that appeared in the TVB. The inspection of distraction effects was done for properly selected ANA. Then, related indices and the ratings of perceived exertion for evaluating appropriate participants are based on both the impression of favorability to music and the SRM occurrences. The results inferred that the multiple time scales strategy could be of help to assess the suitable approach to identify the distraction effects of music stimuli.},
  archive      = {J_TCDS},
  author       = {Caijilahu Bao and Zhiqiang Ma and Zhuoyi Wu and Wenjun Gao and Gwanggil Jeon and Tohru Kiryu},
  doi          = {10.1109/TCDS.2022.3157915},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1043-1050},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Evaluation of distraction effect of music stimuli during cycling exercise with low intensity in terms of multiple time scale},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). So cloze yet so far: N400 amplitude is better predicted by
distributional information than human predictability judgements.
<em>TCDS</em>, <em>15</em>(3), 1033–1042. (<a
href="https://doi.org/10.1109/TCDS.2022.3176783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More predictable words are easier to process—they are read faster and elicit smaller neural signals associated with processing difficulty, most notably, the N400 component of the event-related brain potential. Thus, it has been argued that the prediction of upcoming words is a key component of language comprehension, and that studying the amplitude of the N400 is a valuable way to investigate the predictions we make. In this study, we investigate whether the linguistic predictions of computational language models (LMs) or humans better reflect the way in which natural language stimuli modulate the amplitude of the N400. One important difference in the linguistic predictions of humans versus computational LMs is that while LMs base their predictions exclusively on the preceding linguistic context, humans may rely on other factors. We find that the predictions of three top-of-the-line contemporary LMs—1) GPT-3; 2) RoBERTa; and 3) ALBERT—match the N400 more closely than human predictions. This suggests that the predictive processes underlying the N400 may be more sensitive to the statistics of language than previously thought.},
  archive      = {J_TCDS},
  author       = {James A. Michaelov and Seana Coulson and Benjamin K. Bergen},
  doi          = {10.1109/TCDS.2022.3176783},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1033-1042},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {So cloze yet so far: N400 amplitude is better predicted by distributional information than human predictability judgements},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shared perception is different from individual perception: A
new look on context dependency. <em>TCDS</em>, <em>15</em>(3),
1020–1032. (<a href="https://doi.org/10.1109/TCDS.2022.3185100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human perception is based on unconscious inference, where sensory input integrates with prior information. This phenomenon, known as context dependency, helps in facing the uncertainty of the external world with predictions built upon previous experience. On the other hand, human perceptual processes are inherently shaped by social interactions. However, how the mechanisms of context dependency are affected is to date unknown. If using previous experience—priors—is beneficial in individual settings, it could represent a problem in social scenarios, where other agents might not have the same priors, causing a perceptual misalignment on the shared environment. The present study addresses this question. We studied context dependency in an interactive setting with a humanoid robot iCub that acted as a stimuli demonstrator. Participants reproduced the lengths shown by the robot in two conditions: one with iCub behaving socially and another with iCub acting as a mechanical arm. The different behavior of the robot significantly affected the use of prior in perception. Moreover, the social robot positively impacted perceptual performances by enhancing accuracy and reducing participants’ overall perceptual errors. Finally, the observed phenomenon has been modeled following a Bayesian approach to deepen and explore a new concept of shared perception.},
  archive      = {J_TCDS},
  author       = {Carlo Mazzola and Francesco Rea and Alessandra Sciutti},
  doi          = {10.1109/TCDS.2022.3185100},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1020-1032},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Shared perception is different from individual perception: A new look on context dependency},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Behavioral optimization in a robotic serial reaching task
using predictive information. <em>TCDS</em>, <em>15</em>(3), 1012–1019.
(<a href="https://doi.org/10.1109/TCDS.2022.3176459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction is a powerful approach to minimize errors and control problems in familiar environments and tasks. In human motor execution of sequential action, context effects can be observed, such as anticipation of or predictive movement toward target objects, where later subactions are affected by the execution of earlier subactions. In this article, we present a simulation framework for a serial reaching task using a 4-DoF robotic arm to examine the learning of context effects in simulated robotic reinforcement learning agents. As we demonstrate, giving robotic agents access to predictive information about a future target object’s identity results in motion optimization, where the identity of the next target modulates earlier subactions. Specifically, agents learn to anticipate and predict the location of the next target object, and move toward it before it appears, thus achieving higher rewards than agents that were not given predictive information.},
  archive      = {J_TCDS},
  author       = {Deniz Sen and Roy De Kleijn and George Kachergis},
  doi          = {10.1109/TCDS.2022.3176459},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1012-1019},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Behavioral optimization in a robotic serial reaching task using predictive information},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning robot manipulation skills from human demonstration
videos using two-stream 2-d/3-d residual networks with self-attention.
<em>TCDS</em>, <em>15</em>(3), 1000–1011. (<a
href="https://doi.org/10.1109/TCDS.2022.3182877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning manipulation skills from observing human demonstration video is a promising aspect for intelligent robotic systems. Recent advances in Video-to-Command (V2C) provide an end-to-end approach to translate a video into robot plans. However, simultaneous V2C and action segmentation remain a major challenge for bimanual manipulations with fine-grained actions. Another concern is the generalization capability of end-to-end approaches in dealing with varied task parameters as well as environmental changes between the learned skills and the one-shot task demonstration for the robot to replay. In this article, we propose a two-stream network for robots to learn and segment manipulation subactions from human demonstration videos. Our framework with the self-attention mechanism can segment learned skills and generate action commands simultaneously. To arrive at refined plans in situations of underspecified or redundant human demonstrations, we utilize PDDL-based skill scripts to model the semantics of demonstrated activities and infer latent movements. Experimental results on the extended manipulation data set indicate that our approach generates more accurate commands than the state-of-the-art methods. Real-world experiment results on a Baxter robotic arm also demonstrated the feasibility of our method in reproducing fine-grained actions from video demonstrations.},
  archive      = {J_TCDS},
  author       = {Xin Xu and Kun Qian and Xingshuo Jing and Wei Song},
  doi          = {10.1109/TCDS.2022.3182877},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {1000-1011},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning robot manipulation skills from human demonstration videos using two-stream 2-d/3-D residual networks with self-attention},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning deep features for robotic inference from physical
interactions. <em>TCDS</em>, <em>15</em>(3), 985–999. (<a
href="https://doi.org/10.1109/TCDS.2022.3152383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to effectively handle multiple tasks that are not predefined, a robotic agent needs to automatically map its high-dimensional sensory inputs into useful features. As a solution, feature learning has empirically shown substantial improvements in obtaining representations that are generalizable to different tasks, compared to feature engineering approaches, but it requires a large amount of data and computational capacity. These challenges are specifically relevant in robotics due to the low signal-to-noise ratios inherent to robotic data, and to the cost typically associated with collecting this type of input. In this article, we propose a deep probabilistic method based on convolutional variational autoencoders (CVAEs) to learn visual features suitable for interaction and recognition tasks. We run our experiments on a self-supervised robotic sensorimotor data set. Our data were acquired with the iCub humanoid and are based on a standard object collection, thus being readily extensible. We evaluated the learned features in terms of usability for: 1) object recognition; 2) capturing the statistics of the effects; and 3) planning. In addition, where applicable, we compared the performance of the proposed architecture with other state-of-the-art models. These experiments demonstrate that our model is capable of capturing the functional statistics of action and perception (i.e., images) which performs better than existing baselines, without requiring millions of samples or any hand-engineered features.},
  archive      = {J_TCDS},
  author       = {Atabak Dehban and Shanghang Zhang and Nino Cauli and Lorenzo Jamone and José Santos-Victor},
  doi          = {10.1109/TCDS.2022.3152383},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {985-999},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning deep features for robotic inference from physical interactions},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial special issue on prediction and perception
in humans and robots. <em>TCDS</em>, <em>15</em>(3), 981–984. (<a
href="https://doi.org/10.1109/TCDS.2023.3293669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue addresses perceptual optimization processes related to attentional and predictive mechanisms. Optimal interaction with the environment requires that agents learn to anticipate and evaluate which sensory information is relevant for a task in a specific context so as to prioritize its processing. It has been suggested that during perception, the selection of sensory information depends on predictive and attentional mechanisms that have modulatory effects, enhancing/facilitating, or attenuating/canceling sensory signals. These mechanisms that modulate the way bottom-up incoming sensory information and top-down predictions based on previous experience are prioritized and integrated during perception, are commonly referred to as perceptual optimization processes.},
  archive      = {J_TCDS},
  author       = {Alejandra Ciria and Guido Schillaci and Bruno Lara and Emily S. Cross and Angelo Cangelosi},
  doi          = {10.1109/TCDS.2023.3293669},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {981-984},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial special issue on prediction and perception in humans and robots},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). View-adaptive graph neural network for action recognition.
<em>TCDS</em>, <em>15</em>(2), 969–978. (<a
href="https://doi.org/10.1109/TCDS.2022.3204905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based recognition of human actions has received attention in recent years because of the popularity of 3-D acquisition sensors. Existing studies use 3-D skeleton data from video clips collected from several views. The body view shifts from the camera perspective when humans perform certain actions, resulting in unstable and noisy skeletal data. In this article, we developed a view-adaptive (VA) mechanism that identifies the viewpoints across the sequence and transforms the skeleton view through a data-driven learning process to counteract the influence of variations. Most existing methods use fixed human-defined prior criterion to reposition skeletons. We utilized an unsupervised reposition approach and jointly designed a VA neural network based on the graph neural network (GNN). Our VA-GNN model can transform the skeletons of distinct views into a considerably more consistent virtual perspective over preprocessing approach. The VA module learns the best observed view because it determines the most suitable view and transforms the skeletons from the action sequence for end-to-end recognition along with suited graph topology with adaptive GNN. Thus, our strategy reduces the influence of view variance, allowing networks to focus on learning action-specific properties and resulting in improved performance. The accuracy achieved by the experiments on the four benchmark data sets.},
  archive      = {J_TCDS},
  author       = {Ali Raza Shahid and Mehmood Nawaz and Xinqi Fan and Hong Yan},
  doi          = {10.1109/TCDS.2022.3204905},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {969-978},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {View-adaptive graph neural network for action recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection and estimation of cognitive conflict during
physical human–robot collaboration. <em>TCDS</em>, <em>15</em>(2),
959–968. (<a href="https://doi.org/10.1109/TCDS.2022.3205168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots for physical human–robot collaboration (pHRC) often need to adapt their admittance and how they operate due to several factors. As the admittance of the system becomes variable throughout the workspace, it is not always straightforward for the operator to predict the robot’s behavior. Previous work demonstrated that cognitive conflicts can be detected during one-dimensional tasks. This work assesses whether cognitive conflicts can also be detected during two-dimensional tasks in pHRC and a classification problem is formulated. Different robot admittance profiles anticipating the stimulus translated into different levels of cognitive conflict. Several commonly used classification algorithms for EEG signals were evaluated to classify different levels of cognitive conflict. Results demonstrate that cognitive conflict level is lower when the admittance smoothly decreases before unexpected events when compared to conditions in which the admittance abruptly decreases before the stimulus. Among the classification algorithms, the convolutional neural network has shown the best results to classify different levels of cognitive conflict. Results suggest the feasibility of adaptive approaches for future pHRC control systems that close the loop on users through EEG signals. The detected human cognitive state can also be used to assess and improve the predictability of human–robot teams in various pHRC applications.},
  archive      = {J_TCDS},
  author       = {Stefano Aldini and Avinash K. Singh and Daniel Leong and Yu-Kai Wang and Marc G. Carmichael and Dikai Liu and Chin-Teng Lin},
  doi          = {10.1109/TCDS.2022.3205168},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {959-968},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Detection and estimation of cognitive conflict during physical Human–Robot collaboration},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In-ear SpO₂ for classification of cognitive workload.
<em>TCDS</em>, <em>15</em>(2), 950–958. (<a
href="https://doi.org/10.1109/TCDS.2022.3196841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain is the most metabolically active organ in the body, which increases its metabolic activity and, thus, oxygen consumption, with increasing cognitive demand. This motivates us to question whether the increased cognitive workload may be measurable through changes in blood oxygen saturation. To this end, we explore the feasibility of cognitive workload tracking based on in-ear SpO2 measurements, which are known to be both robust and exhibit minimal delay. We consider a cognitive workload assessment based on an $N$ -back task with a randomized order. It is shown that the 2-back and 3-back tasks (high cognitive workload) yield either the lowest median absolute SpO2 or largest median decrease in SpO2 in all of the subjects, indicating a measurable and statistically significant decrease in blood oxygen in response to the increased cognitive workload. This makes it possible to classify the four $N$ -back task categories, over 5-s epochs, with a mean accuracy of 90.6%, using features derived from in-ear pulse oximetry, including SpO2, pulse rate, and respiration rate. These findings suggest that in-ear SpO2 measurements provide sufficient information for the reliable classification of cognitive workload over short time windows, which promises a new avenue for real-time cognitive workload tracking.},
  archive      = {J_TCDS},
  author       = {Harry J. Davies and Ian Williams and Ghena Hammour and Metin Yarici and Michael J. Stacey and Barry M. Seemungal and Danilo P. Mandic},
  doi          = {10.1109/TCDS.2022.3196841},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {950-958},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {In-ear SpO₂ for classification of cognitive workload},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight source-free transfer for privacy-preserving
motor imagery classification. <em>TCDS</em>, <em>15</em>(2), 938–949.
(<a href="https://doi.org/10.1109/TCDS.2022.3193731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning utilizes data or knowledge in one problem to help solve a related problem. It is particularly useful in electroencephalogram (EEG)-based motor imagery (MI) classification, to handle high intrasubject and/or cross-subject variations. This article considers offline unsupervised cross-subject MI classification, i.e., we have labeled EEG trials from several source subjects, but only unlabeled EEG trials from the target subject. Existing transfer learning approaches usually make use of the source-domain data directly in target model learning. To protect the privacy of the source subjects, we propose lightweight source-free transfer (LSFT), which first generates source models locally and encapsulates them as model application programming interfaces (APIs), then constructs a virtual intermediate domain to transfer the knowledge in the source domains to the target domain, and finally performs feature adaptation learning. Compared with the existing deep transfer learning approaches, LSFT does not need to transfer from massive source data or models, is computationally efficient, and has a small number of parameters. Experiments on four benchmark MI data sets demonstrated that LSFT outperformed 13 different approaches, including several state-of-the-art transfer learning approaches that make use of the source-domain samples or model parameters directly.},
  archive      = {J_TCDS},
  author       = {Wen Zhang and Dongrui Wu},
  doi          = {10.1109/TCDS.2022.3193731},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {938-949},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Lightweight source-free transfer for privacy-preserving motor imagery classification},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An information dissemination model based on the rumor and
anti-rumor and stimulate-rumor and tripartite cognitive game.
<em>TCDS</em>, <em>15</em>(2), 925–937. (<a
href="https://doi.org/10.1109/TCDS.2022.3193576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a tripartite cognitive model of information dissemination based on the symbiosis and antagonism of multiple types of messages, as well as the polymorphism of the user cognitive process under the influence of multimessages. To begin, the euphoric state induced by users’ exposure to multimessages is proposed, which can be used to more realistically describe the complex psychological state of users during the cognitive game of multimessages. Second, the tripartite cognitive game theory is used to construct the user’s state transformation drive force. Meanwhile, the information entropy is introduced to further quantify message forwarding drive force, taking into account differences in the amount of information that users’ cognitive processes transform. Finally, a comprehensive macromodel based on the tripartite cognitive game and the rumor and anti-rumor and stimulate-rumor information dissemination model is proposed. Meanwhile, the Lotka–Voltaila equation is used to analyze and evaluate the coexistence/confrontation relationship of multimessages and the game state of the user cognitive process from a microperspective. Experiments show that the model can not only depict the coexistence/confrontation relationship of multiple messages, but it can also perceive the game situation of multiple messages.},
  archive      = {J_TCDS},
  author       = {Qian Li and Tiancheng Xiang and Tianji Dai and Yunpeng Xiao},
  doi          = {10.1109/TCDS.2022.3193576},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {925-937},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An information dissemination model based on the rumor and anti-rumor and stimulate-rumor and tripartite cognitive game},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning for pass detection and generation of
possession statistics in soccer. <em>TCDS</em>, <em>15</em>(2), 914–924.
(<a href="https://doi.org/10.1109/TCDS.2022.3194103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a reinforcement learning (RL) based technique to detect passes from the video of a soccer match. The detection of passes determines ball possession statistics of a soccer match. A sequence of video frames is mapped to a sequence of states, such as ball with team A or team B or ball not possessed either by team A or B. The agent of RL learns the frame-to-state mapping and the optimal policy to decide the mapping task. We propose a novel reward function by utilizing contextual information of the soccer game in order to help the agent decide the optimal policy. In this context, the advantage of RL is in the integration of a reward system in choosing an action that maps a video frame of a soccer match to one of three possible states. Unlike competing methods, we design the RL model in a way so that explicit identification of team labels of players is not required. We introduce a deep recurrent $Q$ -network (DRQN) to learn the optimal policy. For efficient training of the DRQN, we have proposed decorrelated experience replay (DER), a strategy that selects important experiences based on the correlations of the experiences stored in the replay memory. Experimental results show that at least 5.75% and 2.1% better accuracy are achieved in calculating pass and possession statistics, respectively, compared to similar approaches.},
  archive      = {J_TCDS},
  author       = {Saikat Sarkar and Dipti Prasad Mukherjee and Amlan Chakrabarti},
  doi          = {10.1109/TCDS.2022.3194103},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {914-924},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Reinforcement learning for pass detection and generation of possession statistics in soccer},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Residual dense attention networks for COVID-19 computed
tomography images super resolution. <em>TCDS</em>, <em>15</em>(2),
904–913. (<a href="https://doi.org/10.1109/TCDS.2022.3193121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus disease 2019 (COVID-19) is highly contagious and pathogenic, posing a serious threat to the public safety of the people. Owing to the low resolution of computed tomography (CT) images, it is essential to use super resolution (SR) reconstruction technology to improve the resolution of COVID-19 medical images. Aiming at the problems of the limited receptive field, low resolution, high complexity, and loss of edge information in the SR reconstruction method of residual learning, we present a residual dense attention network (RDAN) for COVID-19 CT image SR. First, to better extract features and reduce the number of parameters, we design the residual dense network module to extract detailed information from images. Second, we add the channel attention mechanism to enable the network to have adequate high-frequency information with larger weights to reduce the computational cost of the model. Finally, we filter and reorganize the multilayer image information by skip connections so that the network model can allow extensive use of image information of different depths. Comprehensive benchmark evaluation shows that our RDAN method dramatically improves the convergence speed of the network, solves the problem of missing information, and makes the reconstructed COVID-19 CT images have more apparent textures, richer details, and better visual effects that can effectively assist experts in diagnosis.},
  archive      = {J_TCDS},
  author       = {Defu Qiu and Yuhu Cheng and Xuesong Wang},
  doi          = {10.1109/TCDS.2022.3193121},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {904-913},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Residual dense attention networks for COVID-19 computed tomography images super resolution},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extending the omniglot challenge: Imitating handwriting
styles on a new sequential data set. <em>TCDS</em>, <em>15</em>(2),
896–903. (<a href="https://doi.org/10.1109/TCDS.2022.3196179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing effectiveness of one-/few-shot learning techniques in the context of handwritten character generation and recognition, the call to extend the commonly associated Omniglot challenge is becoming more pressing. However, the sequential Omniglot data set represents unrealistically written characters. Therefore, we present new data, a new challenge, and a new model as follows: on the data side, we introduce DigiLeTs, a data set containing 23 870 new trajectories of Latin letters and Arabic numbers from 77 participants with high natural variance within character types. On the challenge side, we introduce the task to imitate handwriting styles in a one-shot manner. On the model side, we extend a generative, recurrent neural network model equipped with a one-shot inference mechanism that allows to reuse previously extracted compositional encodings and that has already been proven promising to solve the original Omniglot challenge. The new model is able to reassemble previously learned components into new characters of new styles in a one-shot manner. Most surprisingly, in a strictly forward manner, it often generates plausible, unknown characters in an already known style. With this work, we hope to inspire future research to investigate how compositional structures develop and are employed for rapid, concept-oriented learning, imitation, and understanding.},
  archive      = {J_TCDS},
  author       = {Sarah Fabi and Sebastian Otte and Fedor Scholz and Julius Wührer and Matthias Karlbauer and Martin V. Butz},
  doi          = {10.1109/TCDS.2022.3196179},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {896-903},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Extending the omniglot challenge: Imitating handwriting styles on a new sequential data set},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical graph neural network based on semi-implicit
variational inference. <em>TCDS</em>, <em>15</em>(2), 887–895. (<a
href="https://doi.org/10.1109/TCDS.2022.3193398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural network (GNN) has obtained outstanding achievements in relational data. However, these data have uncertain properties, for example, spurious edges may be included. Recently, variational graph autoencoder (VGAE) has been proposed to solve this problem. However, the distributional assumptions in the variational family restrict the variational inference (VI) flexibility and they define variational families using mean field, which can not capture complex posterior distributional. To solve the above question, in this article, we proposed a novel GNN model based on semi-implicit VI (SIVI), which can embed the node to the latent space to improve VI flexibility and enhance VI expressiveness with mixing distribution. Specifically, to approximate the true posterior, a variational posterior was given utilizing a semi-implicit hierarchical variational framework, which can model complex posterior. Moreover, an iterative decoder is used to better capture graph properties. Besides, due to the hierarchical structure in our model, it can incorporation neighbor information between nodes. Experiments on multiple data sets, Our method has achieved state-of-the-art results compared to other similar methods. Particularly, on the citation data set Citeseer without features, our method outperforms VGAE by 9%.},
  archive      = {J_TCDS},
  author       = {Hai-Long Su and Zhi-Peng Li and Xiao-Bo Zhu and Li-Na Yang and Valeriya Gribova and Vladimir Fedorovich Filaretov and Anthony G. Cohn and De-Shuang Huang},
  doi          = {10.1109/TCDS.2022.3193398},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {887-895},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Hierarchical graph neural network based on semi-implicit variational inference},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental multilayer broad learning system with stochastic
configuration algorithm for regression. <em>TCDS</em>, <em>15</em>(2),
877–886. (<a href="https://doi.org/10.1109/TCDS.2022.3192536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broad learning system (BLS) is a novel randomized learning framework which has a faster modeling efficiency. Although BLS with incremental learning has a better extendibility for updating model rapidly, the incremental mode of BLS lacks a self-supervision mechanism which cannot adjust the structure adaptively. Learning from the idea of stochastic configuration network (SCN), a novel incremental multilayer BLS based on the stochastic configuration (SC) algorithm is proposed for regression, termed as IMLBLS-SC. First, to improve the feature learning ability, the SC algorithm is adopted to configure the parameters of enhancement nodes instead of random weights. Second, the multilayer model with enhancement nodes can be added gradually according to the supervision mechanism without human intervention. Third, all the enhancement nodes and feature nodes are fully connected with output nodes. Finally, two function approximation problems and eight classical data sets are selected to verify the regression performance of IMLBLS-SC, experimental results demonstrate that IMLBLS-SC outperforms the random vector functional-link neural network, SCN, BLS, and broad SCN.},
  archive      = {J_TCDS},
  author       = {Shifei Ding and Chenglong Zhang and Jian Zhang and Lili Guo and Ling Ding},
  doi          = {10.1109/TCDS.2022.3192536},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {877-886},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Incremental multilayer broad learning system with stochastic configuration algorithm for regression},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous skill learning of water polo ball heading for a
robotic fish: Curriculum and verification. <em>TCDS</em>,
<em>15</em>(2), 865–876. (<a
href="https://doi.org/10.1109/TCDS.2022.3189095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns the autonomous skill learning of water polo ball heading for a robotic fish in highly dynamic aquatic environments via multistage serial-parallel curriculum learning (MSPCL). First, a radio-controlled robotic fish with hybrid fin propulsion is presented. Moreover, the water polo ball heading task is decomposed into two sequential subtasks: 1) preparation and 2) shooting. In order to cope with the multistage complex tasks, based on the easy-to-hard curriculum learning strategy, a novel MSPCL framework is proposed for the robotic fish to learn complex skills, which mainly includes curriculum scheduler, difficulty criterion, serial-parallel curriculum generation, and performance measure modules. Furthermore, under the MSPCL framework, soft actor–critic (SAC) algorithm is utilized for training the policy network. Therefore, the robotic fish can learn how to head the water polo ball via the proposed MSPCL method. Comparative simulations are carried out to verify the effectiveness of the MSPCL framework. Finally, the proposed MSPCL method is applied to a physical robotic fish named RoboDact. Swimming pool experiments of water polo ball heading of the RoboDact are conducted to demonstrate the validity and robustness of the proposed method.},
  archive      = {J_TCDS},
  author       = {Tiandong Zhang and Rui Wang and Shuo Wang and Yu Wang and Long Cheng and Gang Zheng and Min Tan},
  doi          = {10.1109/TCDS.2022.3189095},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {865-876},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Autonomous skill learning of water polo ball heading for a robotic fish: Curriculum and verification},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bidirectional gated edge-labeling graph recurrent neural
network for few-shot learning. <em>TCDS</em>, <em>15</em>(2), 855–864.
(<a href="https://doi.org/10.1109/TCDS.2022.3187216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing graph-based methods for few-shot learning problem focused on either separately learning node features or edge features or simply utilizing graph convolution, failing to fully retain or exploit graph structure information. In this article, we proposed a bidirectional gated edge-labeling graph recurrent neural network (bi-GEGRN) which adopts both edge-labeling graph framework and graph convolution operation in the meta-learning scheme. We modified the gated graph neural network to adjacency matrix generator-based bidirectional formation which is able to process sequence graph data in two directions and then organically combined it with edge-labeled graph framework to cyclically upgrade features meanwhile aggregate graph structure information. In view of the excellent aggregating capability of graph convolution and good performance of the alternately cyclic update strategy, bi-GEGRN improves the information transferring between tasks in meta learning. To verify the validity and universality on both supervised and semi-supervised regimes, extensive experiments were conducted on three few-shot benchmark data sets and bi-GEGRN showed a good performance.},
  archive      = {J_TCDS},
  author       = {Qian Wang and Hefei Ling and Baiyan Zhang and Ping Li and Zongyi Li and Yuxuan Shi and Chengxin Zhao and Chuang Zhao},
  doi          = {10.1109/TCDS.2022.3187216},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {855-864},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Bidirectional gated edge-labeling graph recurrent neural network for few-shot learning},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel biologically inspired structural model for feature
correspondence. <em>TCDS</em>, <em>15</em>(2), 844–854. (<a
href="https://doi.org/10.1109/TCDS.2022.3188333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature correspondence is an essential issue in computer science, which could be well formulated by graph matching (GM). However, traditional GM is susceptible to outliers, thus limiting the applications. To address the issue, we present a biologically inspired feature descriptor (BIFD) corresponding to the simple and complex cell layers in primary visual cortex, which shows robust performance against deformations. Furthermore, we propose a novel biologically inspired structural model (BISM) for feature correspondence by fusing the graph structural information and appearance information described by BIFD in the images. The proposed BIFD imitates the cortical pooling operation across multiscale and multiangle cell layers, which makes BISM robust to outliers and distortions. To demonstrate the validity of the proposed method, we evaluate it in feature correspondence tasks on the public databases. The experimental results on synthetic data prove the validity of the proposed method.},
  archive      = {J_TCDS},
  author       = {Yan-Feng Lu and Xu Yang and Yi Li and Qian Yu and Zhi-Yong Liu and Hong Qiao},
  doi          = {10.1109/TCDS.2022.3188333},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {844-854},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A novel biologically inspired structural model for feature correspondence},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal self-paced locality-preserving learning for
diagnosis of alzheimer’s disease. <em>TCDS</em>, <em>15</em>(2),
832–843. (<a href="https://doi.org/10.1109/TCDS.2022.3189701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is an irreversible neurodegenerative disease that severely impairs human thinking and memory. The accurate diagnosis of AD and its prodromal stages, such as mild cognitive impairment (MCI), is very important for timely treatment or possible interventions of AD. Recent studies have shown that multiple neuroimaging and biological measures contain supplementary information for diagnosis and prognosis. Most existing methods are proposed to simply integrate the multimodal data and train the model using all samples once, which do not fully explore the structural information across the different modalities and ignore the significance of sample learning in the training process. In this article, we propose a multimodal self-paced locality-preserving learning (MSLPL) framework to preserve the inherent structural relationships of the original data and realize the sample selection process from “simple” to “complex.” Specifically, the model can project the neuroimaging and genetic data into the label space and learn dimensionality reduction manners with preserving locality structure. Meanwhile, the contributions of each sample are adaptively evaluated by weighting optimization so that the impact of noises can be reduced during model training by self-paced learning (SPL). Finally, a multikernel support vector machine (MK-SVM) is used to fuse the features selected from different modalities for the final prediction. We evaluate MSLPL on 913 subjects from the AD neuroimaging initiative (ADNI) database with imaging and genetic data. The experimental results demonstrate that the proposed method can achieve better classification performances compared with the start-of-the-art multimodality-based methods.},
  archive      = {J_TCDS},
  author       = {Xiaoke Hao and Ruxue Wang and Yingchun Guo and Yunjia Xiao and Ming Yu and Meiling Wang and Weibin Chen and Daoqiang Zhang},
  doi          = {10.1109/TCDS.2022.3189701},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {832-843},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multimodal self-paced locality-preserving learning for diagnosis of alzheimer’s disease},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SURRL: Structural unsupervised representations for robot
learning. <em>TCDS</em>, <em>15</em>(2), 819–831. (<a
href="https://doi.org/10.1109/TCDS.2022.3187186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Revolutionary advances have occurred in robot learning research, where a resurgence in reinforcement learning (RL) algorithms has fueled breakthroughs in acquiring complicated robotic skills without human intervention. Unfortunately, one limitation that has hampered RL methods is that RL may be quite inefficient, that is, it may cost unrealistic learning time and prohibitively large numbers of trajectories to provide implausible models for achieving multitask learning. In a broader perspective, the realization of robotics control by RL relies heavily on the availability of compact and expressive representations of the state spaces. In this article, we propose to learn vivid general structural representations by utilizing structural prior knowledge of robots. Particularly, a novel framework called structural unsupervised representations for robot learning (SURRL) is presented to enable multitask learning. The task-agnostic sample trajectories are leveraged to learn the structural representations that are constructed by graph autoencoder (GAE) in an unsupervised fashion. When learning a new task, the learned structural representations can be directly adopted for subsequent policy learning without training from scratch. Extensive experiments on continuous robotic environments, including dexterous manipulation tasks, characterize our method’s effectiveness to learn optimal policies for handling multiple tasks learning, demonstrating significant improvements over other competitive methods.},
  archive      = {J_TCDS},
  author       = {Fengyi Zhang and Yurou Chen and Hong Qiao and Zhiyong Liu},
  doi          = {10.1109/TCDS.2022.3187186},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {819-831},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SURRL: Structural unsupervised representations for robot learning},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Action potential parameters and spiking behavior of cortical
neurons: A statistical analysis for designing spiking neural networks.
<em>TCDS</em>, <em>15</em>(2), 808–818. (<a
href="https://doi.org/10.1109/TCDS.2022.3185028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cortical neurons exhibit several spiking dynamics both in in-vivo and in-vitro experiments. Neural spikes or action potentials (APs) are also observed in various shapes and forms. Statistical correlation between AP parameters and associated spiking behavior of a neuron is discussed in this article. Three fundamental parameters: 1) width; 2) height; and 3) energy of an AP along with spiking frequency and interspike interval (ISI) are extracted for 91 human cortical neurons selected from Allen Institute for Brain Science (AIBS) database. It has been shown that neurons firing narrow, short, and low-energy APs have higher spiking frequency compared to the neurons with wide and taller APs. For a rise in excitation, it has been presented that information gain for neurons firing wider spikes is less compared to information gain for neurons firing narrow spikes. It has been shown that neurons with low spiking frequency and high spiking frequency dissipate energy of similar order for total spiking activity for similar excitation. Implications of the statistical inferences drawn are explained for a computational model of a spiking neuron. The effect of changing AP width on the overall dynamics of a spiking neural network is also highlighted. The key findings of this study will be useful for designing spiking neural networks for various cognitive applications.},
  archive      = {J_TCDS},
  author       = {Ayan Chakraborty and Sashmita Panda and Saswat Chakrabarti},
  doi          = {10.1109/TCDS.2022.3185028},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {808-818},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Action potential parameters and spiking behavior of cortical neurons: A statistical analysis for designing spiking neural networks},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Four-criterion-optimization-based coordination motion
control of dual-arm robots. <em>TCDS</em>, <em>15</em>(2), 794–807. (<a
href="https://doi.org/10.1109/TCDS.2022.3182534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to address the coordination constraints and physical constraints subjected to the dual-arm robot simultaneously, a novel four-criterion-optimization coordination motion (FCOCM) scheme is proposed, which combines four optimization criteria, namely, the repetitive motion planning (RMP), minimum velocity norm (MVN), maximize manipulability (MM), and infinity-norm velocity minimization (INVM). The scheme can remedy discontinuity in the INVM scheme, eliminate joint angular drift phenomenon, prevent the high joint angular velocity and MM, thereby improving the motion efficiency and ensuring safety and accuracy in the process of tasks. Besides, this scheme also considers real-time trajectory feedback, satisfies physical constraints, and ensures the joint angular velocity is zero at the end of tasks. Furthermore, the improved FCOCM scheme is solved by a novel power-exponent-type variable-parameter recurrent neural network (PET-VPNN) model proposed in this article. A novel Sinh-tunable type activation function which achieves better convergence performance is also proposed. Simulations and experiment are presented to verify the superiority of the proposed coordination motion control method. This research is of great significance for the coordination motion control of dual-arm robots in complex path planning tasks.},
  archive      = {J_TCDS},
  author       = {Yuchuang Tong and Jinguo Liu and Xin Zhang and Zhaojie Ju},
  doi          = {10.1109/TCDS.2022.3182534},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {794-807},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Four-criterion-optimization-based coordination motion control of dual-arm robots},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Motion projection consistency-based 3-d human pose
estimation with virtual bones from monocular videos. <em>TCDS</em>,
<em>15</em>(2), 784–793. (<a
href="https://doi.org/10.1109/TCDS.2022.3185146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time 3-D human pose estimation is crucial for human–computer interaction. It is cheap and practical to estimate 3-D human pose only from monocular video. However, the recent bone-splicing-based 3-D human pose estimation method brings about the problem of cumulative error. In this article, the concept of virtual bones is proposed to solve such a challenge. The virtual bones are imaginary bones between nonadjacent joints. They do not exist in reality, but they bring new loop constraints for the estimation of 3-D human joints. The proposed network in this article predicts real bones and virtual bones, simultaneously. The final length of real bones is constrained and learned by the loop constructed by the predicted real bones and virtual bones. Besides, the motion constraints of joints in consecutive frames are considered. The consistency between the 2-D projected position displacement predicted by the network and the captured real 2-D displacement by the camera is proposed as a new projection consistency loss for the learning of the 3-D human pose. The experiments on the Human3.6M data set demonstrate the good performance of the proposed method. Ablation studies demonstrate the effectiveness of the proposed interframe projection consistency constraints and intraframe loop constraints.},
  archive      = {J_TCDS},
  author       = {Guangming Wang and Honghao Zeng and Ziliang Wang and Zhe Liu and Hesheng Wang},
  doi          = {10.1109/TCDS.2022.3185146},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {784-793},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Motion projection consistency-based 3-D human pose estimation with virtual bones from monocular videos},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face editing based on facial recognition features.
<em>TCDS</em>, <em>15</em>(2), 774–783. (<a
href="https://doi.org/10.1109/TCDS.2022.3182650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face editing generates a face image with the target attributes without changing the identity or other information. Current methods have achieved considerable performance; however, they cannot effectively retain the face’s identity and semantic information while controlling the attribute intensity. Inspired by two human cognitive characteristics, namely, the principle of global precedence and the principle of homology continuity, we propose a novel face editing approach called the information retention and intensity control generative adversarial network (IricGAN). It includes a learnable hierarchical feature combination (HFC) function, which can construct a sample’s source space through multiscale feature mixing; it can guarantee the integrity of the source space while significantly compressing the network. Additionally, the attribute regression module (ARM) can decouple different attribute paradigms in the source space to ensure the correct modification of the required attributes and preserve the other areas. The gradual process of modifying the face attributes can be simulated by applying different control strengths in the source space. In face editing experiments, both qualitative and quantitative results demonstrate that IricGAN achieves the best overall results among state-of-the-art alternatives. Target attributes can be continuously modified by refeeding the relationship of the source space and the image, and the independence of each attribute can be retained to the greatest extent. IricGAN: https://github.com/nanfangzhe/IricGAN .},
  archive      = {J_TCDS},
  author       = {Xin Ning and Shaohui Xu and Fangzhe Nan and Qingliang Zeng and Chen Wang and Weiwei Cai and Weijun Li and Yizhang Jiang},
  doi          = {10.1109/TCDS.2022.3182650},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {774-783},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Face editing based on facial recognition features},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual image decoding of brain activities using a dual
attention hierarchical latent generative network with multiscale feature
fusion. <em>TCDS</em>, <em>15</em>(2), 761–773. (<a
href="https://doi.org/10.1109/TCDS.2022.3181469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing visual stimulus from human brain activity measured with functional magnetic resonance imaging (fMRI) is a challenging decoding task for revealing the visual system. Recent deep learning approaches commonly neglect the relationship between hierarchical image features and different regions of the visual cortex, and fail to use global and local image features in reconstructing visual stimulus. To address these issues, in this article, a novel neural decoding framework is proposed by using a dual attention (DA) hierarchical latent generative network with multiscale feature fusion (DA-HLGN-MSFF) method. Specifically, the fMRI data are first encoded to hierarchical features of our image encoder network, which employs a multikernel convolution block to extract the multiscale spatial information of images. In order to reconstruct the perceived images and further improve the performance of our generator network, a DA block based on the channel-spatial attention mechanism is then proposed to exploit the interchannel relationships and spatial long-range dependencies of features. Moreover, a multiscale feature fusion block is finally adopted to aggregate the global and local information of features at different scales and synthesize the final reconstructed images in the generator network. Competitive experimental results on two public fMRI data sets demonstrate that our method is able to achieve promising reconstructing performance compared with the state-of-the-art methods. The codes of our proposed DA-HLGN-MSFF method will be open access on https://github.com/ljbuaa/HLDAGN .},
  archive      = {J_TCDS},
  author       = {Jie Luo and Weigang Cui and Jingyu Liu and Yang Li and Yuzhu Guo and Song Xu and Lina Wang},
  doi          = {10.1109/TCDS.2022.3181469},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {761-773},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Visual image decoding of brain activities using a dual attention hierarchical latent generative network with multiscale feature fusion},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object-oriented semantic SLAM based on geometric constraints
of points and lines. <em>TCDS</em>, <em>15</em>(2), 751–760. (<a
href="https://doi.org/10.1109/TCDS.2022.3188172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semantic visual simultaneous localization and mapping (SLAM), accurate object-level reconstruction of the environment based on the deep learning techniques is very crucial for high-level scene recognition and semantic object association. However, existing work handles this problem with the assumption of a simple world. There is still a challenge to improve the accuracy of object reconstruction based on images with a complicated environment background. In this work, we propose an improved object recovery method applying the DBSCAN algorithm based on geometric features. Outlier points and abnormal clusters can be identified by combining the clustering algorithm and the nonparametric test. In addition, we develop an adaptive sampling strategy based on line features with varying-step intervals, which can achieve a more accurate estimation of the object orientation. The proposed method is integrated with the ORB-SLAM2 framework to construct a real-time image-based reconstruction system. The qualitative and quantitative evaluation on public data sets and real-world scenarios demonstrates the robustness and effectiveness of our approach compared to the related work.},
  archive      = {J_TCDS},
  author       = {Teng Ran and Liang Yuan and Jianbo Zhang and Zhizhou Wu and Li He},
  doi          = {10.1109/TCDS.2022.3188172},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {751-760},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Object-oriented semantic SLAM based on geometric constraints of points and lines},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-trained spatial graph convolutional network for
unsupervised human-related anomalous event detection in complex scenes.
<em>TCDS</em>, <em>15</em>(2), 737–750. (<a
href="https://doi.org/10.1109/TCDS.2022.3183997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of previous works on abnormal event detection take this task as a novelty detection problem, which employ the supervised setting that needs videos containing only normal events for learning normal patterns. However, few works are developed under the unsupervised setting that detects anomaly without labeled normal videos. In this article, we develop a novel unsupervised algorithm using the skeleton feature for detecting human-related anomalous events. Our method applies the idea of self-training regression for iteratively updating the anomaly scores of skeletons for anomaly detection. In detail, each extracted skeleton is first decomposed into global and local feature components. Then, an unsupervised anomaly detector is operated on these two components to generate the initial anomalous and normal skeleton sets. These two sets are utilized to optimize parameters of an anomaly scoring module consisting of a spatial graph convolutional network (SGCN) and fully connected layers. The trained module is then employed to recalculate anomaly scores of all skeletons to update memberships of pseudo anomalous and normal skeletons set for the next training procedure, and this process is performed in an iterative way to get superior anomaly detection performance. Experimental results on two challenging data sets and their subsets that only contain human-related anomalies demonstrate our method outperforms several state-of-the-art supervised methods.},
  archive      = {J_TCDS},
  author       = {Nanjun Li and Faliang Chang and Chunsheng Liu},
  doi          = {10.1109/TCDS.2022.3183997},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {737-750},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A self-trained spatial graph convolutional network for unsupervised human-related anomalous event detection in complex scenes},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural encoding and decoding with a flow-based invertible
generative model. <em>TCDS</em>, <em>15</em>(2), 724–736. (<a
href="https://doi.org/10.1109/TCDS.2022.3176977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies on visual neural encoding and decoding have made significant progress, benefiting from the latest advances in deep neural networks having powerful representations. However, two challenges remain. First, the current decoding algorithms based on deep generative models always struggle with information losses, which may cause blurry reconstruction. Second, most studies model the neural encoding and decoding processes separately, neglecting the inherent dual relationship between the two tasks. In this article, we propose a novel neural encoding and decoding method with a two-stage flow-based invertible generative (FLIG) model to tackle the above issues. First, a convolutional autoencoder (CAE) is trained to bridge the stimuli space and the feature space. Second, an adversarial cross-modal normalizing flow is trained to build up a bijective transformation between image features and neural signals, with local and global constraints imposed on the latent space to render cross-modal alignment. The method eventually achieves bidirectional generation of visual stimuli and neural responses with a combination of the flow-based generator and the autoencoder. The FLIG model can minimize information losses and unify neural encoding and decoding into a single framework. Experimental results on different neural signals containing spike signals and functional magnetic resonance imaging demonstrate that our model achieves the best comprehensive performance among the comparison models.},
  archive      = {J_TCDS},
  author       = {Qiongyi Zhou and Changde Du and Dan Li and Haibao Wang and Jian K. Liu and Huiguang He},
  doi          = {10.1109/TCDS.2022.3176977},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {724-736},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Neural encoding and decoding with a flow-based invertible generative model},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cognitive fuzzy evidential reasoning approach for
multiexpert multicriterion decision making. <em>TCDS</em>,
<em>15</em>(2), 712–723. (<a
href="https://doi.org/10.1109/TCDS.2022.3177414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive fuzzy number (CFN), considering the joint part of membership and nonmembership degrees, is an interpretable information representation model to deal with the membership degree plus nonmembership degree is larger than one. However, the CFN has two points for improvement: 1) the objective determination of joint degree and 2) the missing of hesitancy degree in the distance measure of CFNs. To tackle these challenges, we first add the hesitancy degree to the distance measure of CFNs. Based on the improved distance, the score function is proposed. Then, we set up a score-based consensus maximization programming model to determine the joint degrees of CFNs. The evidential reasoning (ER) approach is a method to deal with uncertainty based on a collectively exhaustive and mutually exclusive discernment frame. We introduce the ER approach to aggregate the cognitive fuzzy decision matrices of experts into a collective one and then apply the ER approach to get the comprehensive evaluation value of each alternative. Alternatives are then ranked according to their comprehensive scores. We verify the applicability of the proposed CF-ER approach by an illustrative example. Sensitive analysis and comparative analysis are given to demonstrate the reliability of the proposed method.},
  archive      = {J_TCDS},
  author       = {Zhen Zeng and Lisheng Jiang and Huchang Liao},
  doi          = {10.1109/TCDS.2022.3177414},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {712-723},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A cognitive fuzzy evidential reasoning approach for multiexpert multicriterion decision making},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multistep-ahead chaotic time series prediction based on
hierarchical echo state network with augmented random features.
<em>TCDS</em>, <em>15</em>(2), 700–711. (<a
href="https://doi.org/10.1109/TCDS.2022.3176888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistep-ahead chaotic time series prediction is a kind of highly nonlinear problem, which puts forward higher requirements both for the dynamical memory and nonlinearity of the model. Echo state network (ESN) is frequently employed in the realm of chaotic time series modeling and prediction, but the basic ESN has been proved to have an antagonistic tradeoff between nonlinear transformation and memory capacity. To overcome this tradeoff, a new architecture named hierarchical ESN with augmented random features (HESN-ARF) is proposed. On the basis of the traditional linear random projection, the proposed HESN-ARF further leverages nonlinear kernel transformation to construct augmented random features, which can enable the linear and nonlinear properties to be fully represented. Moreover, the HESN-ARF utilizes low-rank kernel approximation to further reduce the computational cost, preserving the advantage of efficient modeling as much as possible while ensuring the capacities of nonlinear transformation and dynamical memory simultaneously. The proposed HESN-ARF can mine and learn the latent evolution patterns hidden in the dynamic system layer by layer through the hierarchical strategy, and achieves excellent performance in multistep-ahead chaotic time series prediction, as demonstrated by experimental findings on two synthetic chaotic systems and a real-world meteorological data set.},
  archive      = {J_TCDS},
  author       = {Xiaodong Na and Mengyuan Zhang and Weijie Ren and Min Han},
  doi          = {10.1109/TCDS.2022.3176888},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {700-711},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multistep-ahead chaotic time series prediction based on hierarchical echo state network with augmented random features},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HHFS: A hybrid hierarchical feature selection method for
ageing gene classification. <em>TCDS</em>, <em>15</em>(2), 690–699. (<a
href="https://doi.org/10.1109/TCDS.2022.3176548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most complicated processes in biological development, ageing remains poorly understood. These days more and more ageing-related gene data sets become available on the Web, where each instance is characterized by a set of hierarchically organized binary features. Traditional data mining methods show limitations in exploiting this hierarchical feature space. This article proposes a hybrid hierarchical feature selection (HHFS) method for classifying genes into prolongevity or anti-longevity ones. HHFS conducts lazy and eager feature selections sequentially, taking into account both uniqueness of a test instance and the whole characteristics of data sets. It adopts two complementary relevancy metrics (i.e., Gini purity and mutual information) to remove hierarchical redundancy. The experiments are conducted based on the ageing-related gene data of four model organisms. The results show that HHFS achieves significantly better prediction performance than several state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Dehui Li and Quanwang Wu and Mengchu Zhou and Fengji Luo},
  doi          = {10.1109/TCDS.2022.3176548},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {690-699},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {HHFS: A hybrid hierarchical feature selection method for ageing gene classification},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cognitive-driven ordinal preservation for multimodal
imbalanced brain disease diagnosis. <em>TCDS</em>, <em>15</em>(2),
675–689. (<a href="https://doi.org/10.1109/TCDS.2022.3175360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multimodal imbalanced data learning problems are becoming increasingly common in the real world, especially in brain disease diagnosis. Although multimodal data provide complementary information for decision making, it can also lead the model to be more sensitive to the adverse effects of imbalance. The existing imbalance learning methods are mostly based on single modality. In this article, we design a cognitive driven ordinal preservation model optimized in both feature and sample levels for multimodal imbalanced data. At the feature level, we project the original data into the label space by a second-order Laplacian manifold for better capturing the minor changes and preserving the discriminative information among samples. At the sample level, we derive the class-specific self-paced learning, simulating human cognitive mechanism, to drive data participating in learning from balance subset to the whole set, which can reduce the negative effects of imbalance on the learning model. Meanwhile, we impose the group sparse constraint on the projection matrix to embed the latent relationship pattern among different modalities, and theoretically prove its convergence. The proposed method is applied to multimodal brain disease diagnosis, including Alzheimer’s disease (AD) and epilepsy. The experimental results show that our method outperforms the existing imbalance and fusion algorithms.},
  archive      = {J_TCDS},
  author       = {Qi Zhu and Ting Zhu and Rui Zhang and Haizhou Ye and Kai Sun and Yong Xu and Daoqiang Zhang},
  doi          = {10.1109/TCDS.2022.3175360},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {675-689},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A cognitive-driven ordinal preservation for multimodal imbalanced brain disease diagnosis},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regional scalp EEGs analysis and classification on typical
childhood epilepsy syndromes. <em>TCDS</em>, <em>15</em>(2), 662–674.
(<a href="https://doi.org/10.1109/TCDS.2022.3175636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy syndromes are typical childhood nervous system diseases that may include different types of epilepsy seizures commonly seen, but far more complex than seizures. Accurate classification of epilepsy syndromes is crucial for diagnosis and treatment. Scalp electroencephalogram (EEG) provides a favorable basis for clinical diagnosis of epilepsy syndrome. In this article, we present a comprehensive analysis on the correlation between time-/frequency-domain regional scalp EEG features and typical epilepsy syndromes, and propose a transfer network-based classification model for epilepsy syndromes. Results on 63 children suffered from four typical epilepsy syndromes and 19 children from the normal control groups (NCGs) show that: 1) the features of the frontal polar region and the frontal region are always very similar, and the parietal region and the occipital region have similar features for each syndrome; 2) skewness is the most significant feature and Lemp-Ziv complexity (LZC) has the least contribution to distinguishing childhood epilepsy syndromes/NCGs; and 3) different individuals of the same syndrome have similarities, while the EEG characteristics of different syndromes are significantly different. A ResNet50 model based on deep transfer feature learning is applied to perform epilepsy syndromes/NCGs classification. The results show that feature selection based on the feature significance testing and correlation analysis can well enhance the classification performance.},
  archive      = {J_TCDS},
  author       = {Xiaonan Cui and Jiuwen Cao and Dinghan Hu and Tianlei Wang and Tiejia Jiang and Feng Gao},
  doi          = {10.1109/TCDS.2022.3175636},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {662-674},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Regional scalp EEGs analysis and classification on typical childhood epilepsy syndromes},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated video classification system driven by
characteristics of emotional human brainwaves caused by audiovisual
stimuli. <em>TCDS</em>, <em>15</em>(2), 651–661. (<a
href="https://doi.org/10.1109/TCDS.2022.3179427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a method of automatically classifying videos into human emotional categories by imitating human neural processes, in which emotion-specific electroencephalography (EEG) characteristics are generated by audiovisual stimulation. In the proposed method, sophisticated emotional features are first extracted from EEG signals that are generated while a subject watches a video, using a sample-attention-based deep neural network encoder. Next, the direct mapping relationship between the extracted emotional EEG features and audiovisual features extracted from the contents of the corresponding videos are learned by a deep belief network. For practical application, the EEG features corresponding to the input video are automatically generated based on the machine’s learned ability without measuring human EEG signals and subsequently applied to a segment-attention-based deep neural network decoder for emotion classification of the video. The experimental results demonstrated that the proposed method significantly outperforms the existing methods with an average accuracy of about 95% for classifying four emotional classes of the video. As for automated emotional video classification, our artificial emotional EEG features-based approach obtains competitive performance, comparable to models that directly measure EEG, and can be generalized to various audiovisual data sets.},
  archive      = {J_TCDS},
  author       = {Dong-Ki Jeong and Hyoung-Gook Kim and Jin Young Kim},
  doi          = {10.1109/TCDS.2022.3179427},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {651-661},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Automated video classification system driven by characteristics of emotional human brainwaves caused by audiovisual stimuli},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fisher regularized ε-dragging for image classification.
<em>TCDS</em>, <em>15</em>(2), 639–650. (<a
href="https://doi.org/10.1109/TCDS.2022.3175008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative least-squares regression (DLSR) has been shown to achieve promising performance in multiclass image classification tasks. Its key idea is to force the regression labels of different classes to move in opposite directions by means of the $\varepsilon $ -dragging technique, yielding a discriminative regression model exhibiting wider margins. However, the $\varepsilon $ -dragging technique ignores an important problem: its relaxation matrix is dynamically updated in optimization, which means the dragging values can also cause the labels from the same class to be uncorrelated. In order to learn a more powerful projection, as well as regression labels, we propose a Fisher regularized $\varepsilon $ -dragging framework (Fisher- $\varepsilon $ ) for image classification by constraining the relaxed labels using the Fisher criterion. On the one hand, the Fisher criterion improves the intraclass compactness of the relaxed labels during relaxation learning. On the other hand, it is expected further to enhance the interclass separability of $\varepsilon $ -dragging. Fisher- $\varepsilon $ for the first time ever attempts to integrate the Fisher criterion and $\varepsilon $ -dragging technique into a unified model because they are complementary in learning discriminative projection. Extensive experiments on various data sets demonstrate that the proposed Fisher- $\varepsilon $ method achieves performance that is superior to other state-of-the-art classification methods. The MATLAB codes are available at https://github.com/chenzhe207/Fisher-epsilon .},
  archive      = {J_TCDS},
  author       = {Zhe Chen and Xiao-Jun Wu and Josef Kittler},
  doi          = {10.1109/TCDS.2022.3175008},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {639-650},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Fisher regularized ε-dragging for image classification},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topological EEG nonlinear dynamics analysis for emotion
recognition. <em>TCDS</em>, <em>15</em>(2), 625–638. (<a
href="https://doi.org/10.1109/TCDS.2022.3174209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional recognition through exploring the electroencephalography (EEG) characteristics has been widely performed in recent studies. Nonlinear analysis and feature extraction methods for understanding the complex dynamical phenomena are associated with the EEG patterns of different emotions. The phase space reconstruction (PSR) is a typical nonlinear technique to reveal the dynamics of the brain neural system. Recently, the topological data analysis (TDA) scheme has been used to explore the properties of space, which provides a powerful tool to think over the phase space. In this work, we proposed a topological EEG nonlinear dynamics analysis approach using the PSR technique to convert EEG time series into phase space, and the persistent homology tool explores the topological properties of the phase space. We perform the topological analysis of EEG signals in different rhythm bands to build emotion feature vectors, which shows high distinguishing ability. We evaluate the approach with two well-known benchmark data sets: 1) the DEAP and 2) DREAMER data sets. The recognition results achieved accuracies of 99.37% and 99.35% in arousal and valence classification tasks with DEAP, and 99.96%, 99.93%, and 99.95% in arousal, valence, and dominance classifications tasks with DREAMER, respectively. The performances are supposed to be outperformed current state-of-art approaches in DREAMER (improved by 1% to 10% depends on temporal length), while comparable to other related works evaluated in DEAP. The proposed work is the first investigation in the emotion recognition-oriented EEG topological feature analysis, which brought a novel insight into the brain neural system nonlinear dynamics analysis and feature extraction.},
  archive      = {J_TCDS},
  author       = {Yan Yan and Xuankun Wu and Chengdong Li and Yini He and Zhicheng Zhang and Huihui Li and Ang Li and Lei Wang},
  doi          = {10.1109/TCDS.2022.3174209},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {625-638},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Topological EEG nonlinear dynamics analysis for emotion recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward accurate camouflaged object detection with in-layer
information enhancement and cross-layer information aggregation.
<em>TCDS</em>, <em>15</em>(2), 615–624. (<a
href="https://doi.org/10.1109/TCDS.2022.3172331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the process of the human visual observation mechanism we propose a novel architecture for camouflaged object detection (COD), which is composed of two main components: 1) in-layer information enhancement module (IIE) and 2) cross-layer information aggregation module (CIA). Specifically, the IIE module is to simulate the first step of human eyes detecting the target, namely, judging whether the camouflaged object exists and roughly be located. Besides, the CIA module is leveraged to imitate the second process of the human visual observation mechanism, refining the edge of the camouflaged object and eliminating interference. Since the shallow texture information and the deep semantic information are complementary, we combine the in-layer information with the cross-layer information to more accurately locate the target object and avoid noise and interference at the same time. Our model outperforms 13 state-of-the-art deep learning-based methods upon three public data sets in terms of four widely used metrics.},
  archive      = {J_TCDS},
  author       = {Hongbo Bi and Cong Zhang and Kang Wang and Ranwan Wu},
  doi          = {10.1109/TCDS.2022.3172331},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {615-624},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Toward accurate camouflaged object detection with in-layer information enhancement and cross-layer information aggregation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HMANet: Hyperbolic manifold aware network for skeleton-based
action recognition. <em>TCDS</em>, <em>15</em>(2), 602–614. (<a
href="https://doi.org/10.1109/TCDS.2022.3171550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition has attracted significant attentions in recent years. To model the skeleton data, most popular methods utilize graph convolutional networks to fuse nodes located in different parts of the graph to obtain rich geometric information. However, these methods cannot be generalized to different graph structures due to their dependencies on the input of the topological structure. In this article, we design a novel hyperbolic manifold aware network without introducing a dynamic graph. Instead, it leverages Riemannian geometry attributes of a hyperbolic manifold. Specifically, this method utilizes the Poincaré model to embed the tree-like structure of the skeleton into a hyperbolic space to automatically capture hierarchical features, which may explore the underlying manifold of the data. To extract spatio-temporal features in the network, the features in manifold space are projected to a tangent space, and a tangent space features translation method based on the Levi–Civita connection was proposed. In addition, we introduce the geometric knowledge of Riemannian manifolds to further explain how features are transformed in the tangent space. Finally, we conduct experiments on several 3-D skeleton data sets with different structures, successfully verifying the effectiveness and advancement of the proposed method.},
  archive      = {J_TCDS},
  author       = {Jinghong Chen and Chong Zhao and Qicong Wang and Hongying Meng},
  doi          = {10.1109/TCDS.2022.3171550},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {602-614},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {HMANet: Hyperbolic manifold aware network for skeleton-based action recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multiscale feature extraction network based on
channel-spatial attention for electromyographic signal classification.
<em>TCDS</em>, <em>15</em>(2), 591–601. (<a
href="https://doi.org/10.1109/TCDS.2022.3167042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The applications of myoelectrical interfaces are majorly limited by the efficacy of decoding motion intent in the electromyographic (EMG) signal. Currently, EMG classification methods often rely substantially on handcrafted features or ignore key channel and interfeature information for classification tasks. To address these issues, a multiscale feature extraction network (MSFEnet) based on channel-spatial attention is proposed to decode the EMG signal for the task of gesture recognition classification. Specifically, we fuse the spatiotemporal characteristics of the EMG signal with different scales. Then, we construct the feature channel attention module and the feature-spatial attention module to capture more key channels features and more key spatial features. To evaluate the efficacy of the proposed method, extensive experiments are conducted on two public data sets: 1) NinaPro DB2 and 2) CapgMyo DB-a. An average accuracy of 86.21%, 90.77%, 92.53%, and 98.85% has been achieved in Exercises B, C, and D of NinaPro DB2 and CapgMyo DB-a, respectively. The experimental results demonstrate that MSFEnet is more capable of extracting temporal and spatial fused features. It performs well in generalization and has higher classification accuracy compared with the state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Biao Sun and Beida Song and Jiajun Lv and Peiyin Chen and Xinlin Sun and Chao Ma and Zhongke Gao},
  doi          = {10.1109/TCDS.2022.3167042},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {591-601},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A multiscale feature extraction network based on channel-spatial attention for electromyographic signal classification},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cognitive workload estimation using variational autoencoder
and attention-based deep model. <em>TCDS</em>, <em>15</em>(2), 581–590.
(<a href="https://doi.org/10.1109/TCDS.2022.3163020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of cognitive workload using electroencephalogram (EEG) is an emerging research area. However, due to poor spatial resolution issues, features obtained from EEG signals often lead to poor classification results. As a good generative model, the variational autoencoder (VAE) extracts the noise-free robust features from the latent space that lead to better classification performance. The spatial attention-based method [convolutional block attention module (CBAM)] can improve the spatial resolution of EEG signals. In this article, we propose an effective VAE-CBAM-based deep model for estimating cognitive states from topographical videos. Topographical videos of four different conditions [baseline (BL), low workload (LW), medium workload (MW), and high workload (HW)] of the mental arithmetic task are taken for the experiment. Initially, the VAE extracts localized features from input images (extracted from topographical video), and CBAM infers the spatial–channel-level’s attention features from those localized features. Finally, the deep CNN-BLSTM model effectively learns those attention-based spatial features in a timely distributed manner to classify the cognitive state. For four-class and two-class classifications, the proposed model achieves 83.13% and 92.09% classification accuracy, respectively. The proposed model enhances the future research scope of attention-based studies in EEG applications.},
  archive      = {J_TCDS},
  author       = {Debashis Das Chakladar and Sumalyo Datta and Partha Pratim Roy and Vinod A. Prasad},
  doi          = {10.1109/TCDS.2022.3163020},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {581-590},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cognitive workload estimation using variational autoencoder and attention-based deep model},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bootstrapping concept formation in small neural networks.
<em>TCDS</em>, <em>15</em>(2), 566–580. (<a
href="https://doi.org/10.1109/TCDS.2022.3163022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The question how neural systems (of humans) can perform reasoning is still far from being solved. We posit that the process of forming Concepts is a fundamental step required for this. We argue that, first, Concepts are formed as closed representations, which are then consolidated by relating them to each other. Here, we present a model system (agent) with a small neural network that uses realistic learning rules and receives only feedback from the environment in which the agent performs virtual actions. First, the actions of the agent are reflexive. In the process of learning, statistical regularities in the input lead to the formation of neuronal pools representing relations between the entities observed by the agent from its artificial world. This information then influences the behavior of the agent via feedback connections replacing the initial reflex by an action driven by these relational representations. We hypothesize that the neuronal pools representing relational information can be considered as primordial Concepts, which may in a similar way be present in some prelinguistic animals, too. This system provides formal grounds for further discussions on what could be understood as a Concept and shows that associative learning is enough to develop concept-like structures.},
  archive      = {J_TCDS},
  author       = {Minija Tamosiunaite and Tomas Kulvicius and Florentin Wörgötter},
  doi          = {10.1109/TCDS.2022.3163022},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {566-580},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Bootstrapping concept formation in small neural networks},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal urban sound tagging with spatiotemporal context.
<em>TCDS</em>, <em>15</em>(2), 555–565. (<a
href="https://doi.org/10.1109/TCDS.2022.3160168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise pollution significantly affects our daily life and urban development. Urban sound tagging (UST) has attracted much attention recently, which aims to analyze and monitor urban noise pollution. One weakness of the previous UST studies is that the spatial and temporal context of sound signals, which contains complementary information about when and where the audio data was recorded, has not been investigated. To address this problem, in this article, we propose a multimodal UST system that deeply mines the audio and spatiotemporal context together. In order to incorporate the characteristics of different acoustic features, two sets of four spectrograms are first extracted as the inputs of residual neural networks. Then, the spatiotemporal context is encoded and combined with acoustic features to explore the efficiency of multimodal learning for discriminating sound signals. Moreover, a data filtering approach is adopted in text processing to further improve the performance of multimodality. We evaluate the proposed method on the UST challenge (task 5) of DCASE2020. Experimental results demonstrate the effectiveness of the proposed method.},
  archive      = {J_TCDS},
  author       = {Jisheng Bai and Jianfeng Chen and Mou Wang},
  doi          = {10.1109/TCDS.2022.3160168},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {555-565},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multimodal urban sound tagging with spatiotemporal context},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MultiselfGAN: A self-guiding neural architecture search
method for generative adversarial networks with multicontrollers.
<em>TCDS</em>, <em>15</em>(2), 544–554. (<a
href="https://doi.org/10.1109/TCDS.2022.3160475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, reinforcement learning (RL) and gradient optimization were applied with neural architecture search algorithms in the generative adversarial network (GAN) to achieve their state-of-the-art (SOTA) performance. However, the existing RL-based methods utilized the calculation of inception score or Fréchet inception distance as the reward value to guide the controller, which actually wasted much of searching time. In order to improve the search efficiency without degradation of performance, this article proposes recycling the discriminator to evaluate the performance of architectures, in other words, we propose to self-guide the search process. In the mean time, we introduce new concept of multiple controllers and the method of reward shaping to independently and effectively search the cell architectures. The experiments demonstrate the effectiveness and efficiency of our multiself GAN and the ablation study also exhibits its robustness.},
  archive      = {J_TCDS},
  author       = {Jiachen Shi and Guoqiang Zhou and Shudi Bao and Jun Shen},
  doi          = {10.1109/TCDS.2022.3160475},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {544-554},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {MultiselfGAN: A self-guiding neural architecture search method for generative adversarial networks with multicontrollers},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unbiased spatiotemporal representation with uncertainty
control for person reidentification. <em>TCDS</em>, <em>15</em>(2),
530–543. (<a href="https://doi.org/10.1109/TCDS.2022.3159557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For person reidentification (re-id), most current research aims to encode the spatial and temporal information by using convolutional neural networks (CNNs) to extract spatial features and recurrent neural networks (RNNs) or their variations to discover the time dependencies. However, it ignores the effect of the complex background, which leads to a biased spatial representation. Furthermore, it often uses the backpropagation through time (BPTT) to train RNNs. Unfortunately, it is hard to learn the long-term dependency via BPTT due to the gradient vanishing or exploding. The significance of a frame should not be biased by its position in a given sequence. In this article, a new method is proposed to learn an unbiased semantic representation for video-based person re-id. To handle the background clutter and occlusion, a two-branch CNN model is used to obtain the enriched representation from both the foreground person and original pedestrian images. Then, an unbiased bidirectional CNN architecture is developed to learn the unbiased spatial and temporal representation. The experimental results on three public data sets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TCDS},
  author       = {Xiu Zhang and Bir Bhanu},
  doi          = {10.1109/TCDS.2022.3159557},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {530-543},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Unbiased spatiotemporal representation with uncertainty control for person reidentification},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recurrent-neural-network-based polynomial noise resistance
model for computing dynamic nonlinear equations applied to robotics.
<em>TCDS</em>, <em>15</em>(2), 518–529. (<a
href="https://doi.org/10.1109/TCDS.2022.3159852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The solution of dynamic nonlinear equations plays an important role in the control of complex systems. However, as a common physical phenomenon, noise seriously affects the effectiveness of online solutions in the form of external disturbance, inaccurate modeling, or estimation errors. In reality, most noises have different values at different moments and can be described by a sufficiently high-order nonlinear function. Such a function can theoretically be fitted or approximated by a sufficiently high-order polynomial. Nevertheless, existing models may lose their solving ability in the face of such high-order polynomial noise, which greatly limits their applications. To this end, a generalized RNN-based polynomial noise resistance (RB-PNR) model is proposed to learn the characteristic of noises with their order and coefficients unknown and then eliminate them accurately in solving dynamic nonlinear equations. Theoretical analysis and numerical simulation results demonstrate that the RB-PNR design model achieves zero residual error under polynomial noise disturbance with unknown order and coefficients. In addition, applications on different robots and the design of a 2-D digital filter are conducted to verify further the excellent robustness and physical realization of the designed RB-PNR model.},
  archive      = {J_TCDS},
  author       = {Mei Liu and Jiachang Li and Shuai Li and Nianyin Zeng},
  doi          = {10.1109/TCDS.2022.3159852},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {518-529},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Recurrent-neural-network-based polynomial noise resistance model for computing dynamic nonlinear equations applied to robotics},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Morpho evolution with learning using a controller archive as
an inheritance mechanism. <em>TCDS</em>, <em>15</em>(2), 507–517. (<a
href="https://doi.org/10.1109/TCDS.2022.3148543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most work in evolutionary robotics centers on evolving a controller for a fixed body plan. However, previous studies suggest that simultaneously evolving both controller and body plan could open up many interesting possibilities. However, the joint optimization of body plan and control via evolutionary processes can be challenging in rich morphological spaces. This is because offspring can have body plans that are very different from either of their parents, leading to a potential mismatch between the structure of an inherited neural controller and the new body. To address this, we propose a framework that combines an evolutionary algorithm to generate body plans and a learning algorithm to optimize the parameters of a neural controller. The topology of this controller is created once the body plan of each offspring has been generated. The key novelty of the approach is to add an external archive for storing learned controllers that map to explicit “types” of robots (where this is defined with respect to the features of the body plan). By initiating learning from a controller with an appropriate structure inherited from the archive, rather than from a randomly initialized one, we show that both the speed and magnitude of learning increase over time when compared to an approach that starts from scratch, using two tasks and three environments. The framework also provides new insights into the complex interactions between evolution and learning.},
  archive      = {J_TCDS},
  author       = {Léni K. Le Goff and Edgar Buchanan and Emma Hart and Agoston E. Eiben and Wei Li and Matteo De Carlo and Alan F. Winfield and Matthew F. Hale and Robert Woolley and Mike Angus and Jon Timmis and Andy M. Tyrrell},
  doi          = {10.1109/TCDS.2022.3148543},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {507-517},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Morpho evolution with learning using a controller archive as an inheritance mechanism},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The influence of robot traits and evolutionary dynamics on
the reality gap. <em>TCDS</em>, <em>15</em>(2), 499–506. (<a
href="https://doi.org/10.1109/TCDS.2021.3112236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The elephant in the room for evolutionary robotics is the reality gap. In the history of the field, several studies investigated this phenomenon on fixed robot morphologies where only the controllers evolved. This article addresses the reality gap in a wider context, in a system where both morphologies and controllers evolve. In this context, the morphology of the robots becomes a variable with a currently unknown influence. To examine this influence, we construct a test suite of robots with various morphologies and evolve their controllers for an effective gait. Comparing the simulated and the real-world performance of evolved controllers sampled at different generations during the evolutionary process, we gain new insights into the factors that influence the reality gap.},
  archive      = {J_TCDS},
  author       = {Fuda van Diggelen and Eliseo Ferrante and Nihed Harrak and Jie Luo and Daan Zeeuwe and A. E. Eiben},
  doi          = {10.1109/TCDS.2021.3112236},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {499-506},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The influence of robot traits and evolutionary dynamics on the reality gap},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning: Rotation alignment with riemannian mean
for brain–computer interfaces and wheelchair control. <em>TCDS</em>,
<em>15</em>(2), 487–498. (<a
href="https://doi.org/10.1109/TCDS.2021.3082648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cross-session and cross-subject classification of motor imagery (MI) electroencephalogram (EEG) signals is challenging. This article presents a transfer learning (TL) method to address the cross-session and cross-subject classification of MI EEG signals, a tricky procedure in brain–computer interface (BCI). Method: We propose a rotation alignment domain adaptation method with Riemannian mean (RMRA). The method uses covariance matrix to represent data feature, and achieves data alignment by rotating the symmetric positive-definite (SPD) matrix in Riemannian space. In this process, our proposed matrix-TCA extends the traditional transfer component analysis (TCA) to a matrix form in order to function in the Riemannian framework. Data labels are not required, so the proposed method is unsupervised. In addition, we simplify the calculation process through Riemannian mean. Results: We have performed both offline and online experiments on multiple MI EEG data sets. Our results show that RMRA improves the cross-session and cross-subject classification accuracy. Conclusion and Significance: This article presents a new approach to cross-domain learning, which achieves desirable results and shows great promise in real-life application of the service robot (intelligent wheelchair).},
  archive      = {J_TCDS},
  author       = {Xianlun Tang and Xingchen Li and Wei Li and Bohui Hao and Ying Xie and Xiaoyuan Dang},
  doi          = {10.1109/TCDS.2021.3082648},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {487-498},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Transfer learning: Rotation alignment with riemannian mean for Brain–Computer interfaces and wheelchair control},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep binocular fixation prediction using a hierarchical
multimodal fusion network. <em>TCDS</em>, <em>15</em>(2), 476–486. (<a
href="https://doi.org/10.1109/TCDS.2021.3051010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D data are increasingly being used for myriad computer vision tasks. For such tasks, most methods simply concatenate or add feature vectors from RGB images and depth maps and allow the two modalities to complement each other mutually. However, such a fusion strategy results in inefficient and inadequate performance. In this study, we propose deep binocular fixation prediction based on a hierarchical multimodal fusion network that suitably combines RGB and depth maps. In the proposed method, a novel convolutional block attention module completely extracts image texture features and retains spatial information. In addition, a pyramid dilated-convolution module refines feature information, further improving the fusion of RGB and depth maps. Experimental results indicate that the proposed network achieves state-of-the-art performance on the NUS and NCTU data sets.},
  archive      = {J_TCDS},
  author       = {Wujie Zhou and Wenyu Liu and Jingsheng Lei and Ting Luo and Lu Yu},
  doi          = {10.1109/TCDS.2021.3051010},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {476-486},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep binocular fixation prediction using a hierarchical multimodal fusion network},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent prediction method for updraft of UAV that is
based on LSTM network. <em>TCDS</em>, <em>15</em>(2), 464–475. (<a
href="https://doi.org/10.1109/TCDS.2020.3048347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Updrafts widely exist in the nature and with the help of updrafts, an unmanned aerial vehicle (UAV) may improve its flight time performance. Some methods have been proposed for updraft prediction, such as the extended Kalman filter (EKF) or unscented Kalman filter (UKF). In this article, a prediction method based on the long short-term memory (LSTM) network is proposed. First, a flight simulation system is developed, which is used to generate the training data for the LSTM network. Then, an LSTM network is developed and trained for updraft prediction. Finally, some experiments are made to compare the LSTM network with traditional methods that are based on EKF and UKF. The results show that the LSTM network has a substantial advantage in terms of the prediction accuracy and convergence rate.},
  archive      = {J_TCDS},
  author       = {Yuxiang Zhang and Ke Li and Ke Li and Jingyi Liu},
  doi          = {10.1109/TCDS.2020.3048347},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {464-475},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Intelligent prediction method for updraft of UAV that is based on LSTM network},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid BCI using singular value decomposition values of
the fast walsh–hadamard transform coefficients. <em>TCDS</em>,
<em>15</em>(2), 454–463. (<a
href="https://doi.org/10.1109/TCDS.2020.3028785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main goals of a brain computer interface (BCI) is to enable a communication channel between the brain and electronic devices by converting neural activity into control commands either for devices or applications. Because of the excellent temporal resolution, low set-up cost, and noninvasive nature, BCI systems generally use electroencephalography (EEG) for an input signal. However, EEG suffers from poor spatial resolution, and it is contaminated by various external and internal artifacts, such as environmental magnetic noises and body movements. These limitations directly affect the performance of the EEG-based BCI system, and it might not work at the desired level. On the other hand, near-infrared spectroscopy (NIRS) has an advantage of relative robustness against body movements and electrical artifacts. Additionally, it is also a promising neural signal recording method which provides good spatial resolution. In this study, we particularly focused on compensating the limitations of EEG-based BCI system by adding simultaneous NIRS modality features. In order to show the effectiveness of our method, we used an open-access data set, which was recorded from 29 subjects with simultaneous EEG-NIRS system during the imagination of opening and closing either a left- or right-hand. The features were extracted by calculating the singular value decomposition values of the Fast Walsh–Hadamard transform coefficients. Afterward, the ${k}$ -nearest neighbor algorithm was performed to classify the features. The performance of the proposed method was evaluated in terms of classification accuracy and kappa value metrics. The achieved results showed that combining a hybrid BCI system with EEG-NIRS modalities can enhance the performance of a BCI by 6.75% compared to the single-modality solution of EEG.},
  archive      = {J_TCDS},
  author       = {Ebru Ergün and Onder Aydemir},
  doi          = {10.1109/TCDS.2020.3028785},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {454-463},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A hybrid BCI using singular value decomposition values of the fast Walsh–Hadamard transform coefficients},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional integration and separation of brain network based
on phase locking value during emotion processing. <em>TCDS</em>,
<em>15</em>(2), 444–453. (<a
href="https://doi.org/10.1109/TCDS.2020.3001642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The functional connection patterns of the brain during the processing of human emotions are complex and changeable. Therefore, the use of brain network information at the global scale is not sufficient to reflect the coupling relationship between brain regions, and it is impossible to accurately analyze the differences in information interaction patterns of the brain under different emotions. The purpose of this article is to study the functional connectivity of the brain by using the phase synchronization of electroencephalogram (EEG) channel information in different emotional states. Considering that the phase locking value (PLV) can effectively reflect the phase synchronization relationship between the EEG channels, it is adopted to describe the functional connection relationship of the brain. After the brain network based on PLV is constructed, considering that the topological structure of the brain network is complex and volatile, we merge the two types of attributes: 1) functional integration and 2) functional separation, to analyze the differences in brain connectivity for different emotions. Furthermore, the modular structure of the brain network is constructed through community detection to extract its more comprehensive local characteristics for connectivity analysis. The results show that compared with positive emotions, brain regions under negative emotions have higher phase synchronization, more complex brain connectivity patterns, and more obvious modular structures. In addition, key cortical brain regions associated with emotional stimulation have been identified as brain network hubs. The verification on the DEAP data set demonstrates that the analysis framework of this study effectively improved the accuracy of emotion recognition.},
  archive      = {J_TCDS},
  author       = {Zhong-Min Wang and Rui Zhou and Yan He and Xiao-Min Guo},
  doi          = {10.1109/TCDS.2020.3001642},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {444-453},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Functional integration and separation of brain network based on phase locking value during emotion processing},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the effects of leader–follower roles in dyadic
human–robot synchronization. <em>TCDS</em>, <em>15</em>(2), 434–443. (<a
href="https://doi.org/10.1109/TCDS.2020.2991864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of cobots working side by side to humans has recently drawn attention to the psychological aspects of human–robot interaction. We propose an exploratory study that investigates whether and how the role is taken on by the robot during a collaborative task influences the human’s psychophysiological response and production rate. We assume the cobot can either take the lead with respect to the human operator or comply with the partner’s decision; namely, being the leader or the follower within the dyad. Against this background, we examined the effects of the leader–follower paradigm on a collaborative tower-building task. We evaluated the stress induced on the subject by the cooperation with the robot, based on both the ECG measurements and on positive and negative affect scale (PANAS) and state-trait anxiety inventory questionnaires. Moreover, based on the measured cycle time, we estimated the user’s production rate. The results highlighted that when the human takes the lead, he/she is subject to a lower physiological stress and is less productive compared to the case where he/she follows the robot strategy.},
  archive      = {J_TCDS},
  author       = {Costanza Messeri and Andrea Maria Zanchettin and Paolo Rocco and Elena Gianotti and Alice Chirico and Stefano Magoni and Andrea Gaggioli},
  doi          = {10.1109/TCDS.2020.2991864},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {434-443},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {On the effects of Leader–Follower roles in dyadic Human–Robot synchronization},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goal-directed tactile exploration for body model learning
through self-touch on a humanoid robot. <em>TCDS</em>, <em>15</em>(2),
419–433. (<a href="https://doi.org/10.1109/TCDS.2021.3104881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An early integration of tactile sensing into motor coordination is the norm in animals, but still a challenge for robots. Tactile exploration through touches on the body gives rise to first body models and bootstraps further development such as reaching competence. Reaching to one’s own body requires connections of the tactile and motor space only. Still, the problems of high dimensionality and motor redundancy persist. Through an embodied computational model for the learning of self-touch on a simulated humanoid robot with artificial sensitive skin, we demonstrate that this task can be achieved 1) effectively and 2) efficiently at scale by employing the computational frameworks for the learning of internal models for reaching: intrinsic motivation and goal babbling. We relate our results to infant studies on spontaneous body exploration as well as reaching to vibrotactile targets on the body. We analyze the reaching configurations of one infant followed weekly between 4 and 18 months of age and derive further requirements for the computational model: accounting for 3) continuous rather than sporadic touch and 4) consistent redundancy resolution. Results show the general success of the learning models in the touch domain, but also point out limitations in achieving fully continuous touch.},
  archive      = {J_TCDS},
  author       = {Filipe Gama and Maksym Shcherban and Matthias Rolf and Matej Hoffmann},
  doi          = {10.1109/TCDS.2021.3104881},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {419-433},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Goal-directed tactile exploration for body model learning through self-touch on a humanoid robot},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D_DEN: Open-ended 3-d object recognition using dynamically
expandable networks. <em>TCDS</em>, <em>15</em>(2), 409–418. (<a
href="https://doi.org/10.1109/TCDS.2021.3075143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service robots, in general, have to work independently and adapt to the dynamic changes happening in the environment in real time. One important aspect in such scenarios is to continually learn to recognize newer object categories when they become available. This combines two main research problems, namely, continual learning and 3-D object recognition. Most of the existing research approaches include the use of deep convolutional neural networks (CNNs) focusing on image data sets. A modified approach might be needed for continually learning 3-D object categories. A major concern in using CNNs is the problem of catastrophic forgetting when a model tries to learn a new task. Despite various proposed solutions to mitigate this problem, there still exist some downsides of such solutions, e.g., computational complexity, especially when learning a substantial number of tasks. These downsides can pose major problems in robotic scenarios where real-time response plays an essential role. Toward addressing this challenge, we propose a new deep transfer learning approach based on a dynamic architectural method to make robots capable of open-ended learning about new 3-D object categories. Furthermore, we make sure that the mentioned downsides are minimized to a great extent. Experimental results showed that the proposed model outperformed state-of-the-art approaches with regards to accuracy and also substantially minimizes computational overhead. The code is available online at: https://github.com/sudhakaranjain/3D_DEN .},
  archive      = {J_TCDS},
  author       = {Sudhakaran Jain and Hamidreza Kasaei},
  doi          = {10.1109/TCDS.2021.3075143},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {409-418},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {3D_DEN: Open-ended 3-D object recognition using dynamically expandable networks},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient dialog policy learning with hindsight, user
modeling, and adaptation. <em>TCDS</em>, <em>15</em>(2), 395–408. (<a
href="https://doi.org/10.1109/TCDS.2021.3061121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goal-oriented dialog systems aim to efficiently and accurately exchange information with people using natural language. A goal-oriented dialog policy is used for suggesting language actions for such dialog systems. Reinforcement learning (RL) has been used for computing dialog policies from the experience of language-based interaction. Learning efficiency is particularly important in dialog policy learning, due to the considerable cost of interacting with human users, and the potentially very poor user experience from low-quality conversations. In this article, we develop deep RL algorithms to improve the efficiency of dialog policy learning. Our contribution is threefold, aiming at the central goal of improving the efficiency of dialog policy learning. First, we present a novel “hindsight” approach to make use of unsuccessful dialog instances to provide the dialog learning agent with extra positive feedback. Second, we introduce user modeling, and enable the dialog agent to learn from simulated interaction experience. Third, we have developed a metalearning algorithm that enables the dialog agent to adaptively learn from simulated users and hindsight experience at the same time. The threefold contribution altogether, for the first time, enables our dialog agent outperforming a number of state-of-the-art dialog policy learning methods, as demonstrated via our experimental results.},
  archive      = {J_TCDS},
  author       = {Keting Lu and Yan Cao and Xiaoping Chen and Shiqi Zhang},
  doi          = {10.1109/TCDS.2021.3061121},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {395-408},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Efficient dialog policy learning with hindsight, user modeling, and adaptation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining social and intrinsically motivated learning for
multitask robot skill acquisition. <em>TCDS</em>, <em>15</em>(2),
385–394. (<a href="https://doi.org/10.1109/TCDS.2021.3069341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an approach for coupling internally guided learning and social interaction in the context of a multitask robot skill acquisition framework. More specifically, we focus on learning a parametrized distribution of robot movement primitives by combining active intrinsically motivated learning and active imitation learning. We focus on the case where the learning modalities to use are not specified in advance by the experimenter, but are chosen actively by the robot through experiences. Such approach aims at combining experiential and observational learning as efficiently as possible, by relying on a skill acquisition mechanism in which the agent/robot can orchestrate different learning strategies in an iterative manner, and modulate the use of these modalities based on previous experiences. We demonstrate the effectiveness of our approach on a waste throwing task with a simulated 7-DoF Franka Emika robot, where at each iteration of the learning process the robot can actively choose between observational/imitation learning and experiential/intrinsically motivated learning.},
  archive      = {J_TCDS},
  author       = {Thibaut Kulak and Sylvain Calinon},
  doi          = {10.1109/TCDS.2021.3069341},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {385-394},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Combining social and intrinsically motivated learning for multitask robot skill acquisition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interest-driven exploration with observational learning for
developmental robots. <em>TCDS</em>, <em>15</em>(2), 373–384. (<a
href="https://doi.org/10.1109/TCDS.2021.3057758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been emphasized for a long time that real-world applications of developmental robots require lifelong and online learning. A major challenge in this field is the high sample-complexity of algorithms, which has led to the development of intrinsic motivation approaches to render learning more efficient. However, only few works have been demonstrated on real robots and although these robots are supposed to share the environment with humans, there is hardly any research to integrate intrinsic motivation with learning from an interacting teacher. In this article, we tackle the efficiency challenge by proposing a novel extrinsic–intrinsic motivation learning scheme. We specifically investigate how to combine intrinsic motivation with learning from observation to accelerate learning. Our novel scheme comprises four elements: 1) a probabilistic intrinsic motivation signal yielding the robot’s interest; 2) a probabilistic extrinsic motivation signal to expand the robot’s knowledge by learning from observation; 3) novelty detection; and 4) novelty degree methods to enable the robot to decide autonomously how and when to explore. The efficiency as well as the applicability of our methods are benchmarked in simulation experiments and demonstrated on a physical 7-degree of freedom left arm of Baxter robot.},
  archive      = {J_TCDS},
  author       = {Rania Rayyes and Heiko Donat and Jochen Steil and Michael Spranger},
  doi          = {10.1109/TCDS.2021.3057758},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {373-384},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Interest-driven exploration with observational learning for developmental robots},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goal-directed empowerment: Combining intrinsic motivation
and task-oriented behavior. <em>TCDS</em>, <em>15</em>(2), 361–372. (<a
href="https://doi.org/10.1109/TCDS.2020.3042938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empowerment is an information-theoretic measure representing the capacity of an agent to affect its environment. It quantifies its ability to inject information in the environment via its actions and to recapture this information through its sensors. In a nutshell, it measures the number of future options available and perceivable by the agent. Originally, the definition of empowerment does not depend on any particular extrinsic goal and it is determined only by the interaction of the agent with the world and the structure of its action-perception cycle. In this article, we introduce a new formalism that combines empowerment maximization with externally specifiable goal-directed behavior. This has two main implications: on the one hand, the study of the relationship between empowerment optimization and goal-directedness, to investigate to which extent these two desirable behaviors can co-exist; on the other hand, from a more operational point of view, the derivation of a method to generate a behavior (i.e., a policy of a Markov decision process) that is both empowered and goal-directed, in order to design agents capable of being as “empowered” as possible when facing any extrinsic task. Finally, we study how this hybrid policy is able to handle problems of uncertain or changing goals and delayed goal commitment.},
  archive      = {J_TCDS},
  author       = {Nicola Catenacci Volpi and Daniel Polani},
  doi          = {10.1109/TCDS.2020.3042938},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {361-372},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Goal-directed empowerment: Combining intrinsic motivation and task-oriented behavior},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning abstract representations through lossy compression
of multimodal signals. <em>TCDS</em>, <em>15</em>(2), 348–360. (<a
href="https://doi.org/10.1109/TCDS.2021.3108478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key competence for open-ended learning is the formation of increasingly abstract representations useful for driving complex behavior. Abstract representations ignore specific details and facilitate generalization. Here, we consider the learning of abstract representations in a multimodal setting with two or more input modalities. We treat the problem as a lossy compression problem and show that generic lossy compression of multimodal sensory input naturally extracts abstract representations that tend to strip away modalitiy specific details and preferentially retain information that is shared across the different modalities. Specifically, we propose an architecture that is able to extract information common to different modalities based on the compression abilities of generic autoencoder neural networks. We test the architecture with two tasks that allow: 1) the precise manipulation of the amount of information contained in and shared across different modalities and 2) testing the method on a simulated robot with visual and proprioceptive inputs. Our results show the validity of the proposed approach and demonstrate the applicability to embodied agents.},
  archive      = {J_TCDS},
  author       = {Charles Wilmot and Gianluca Baldassarre and Jochen Triesch},
  doi          = {10.1109/TCDS.2021.3108478},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {348-360},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning abstract representations through lossy compression of multimodal signals},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intrinsic plasticity for online unsupervised learning based
on soft-reset spiking neuron model. <em>TCDS</em>, <em>15</em>(2),
337–347. (<a href="https://doi.org/10.1109/TCDS.2020.3041610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic plasticity (IP) is an unsupervised, self-adaptive, local learning rule that was first found in biological nerve cells, and has been shown to be able to maximize neuronal information transmission entropy. In this article, we propose a soft-reset leaky integrate-and-fire (LIF) model, a spiking neuron model based on widely used LIF neurons, with a new IP learning rule that optimizes the neuronal membrane potential state to be exponentially distributed. Previous studies have generally used such as spiking neuron expected firing rate as the target variable to maximize output spike distribution. In contrast, the proposed soft-reset model can avoid the problem that conventional LIF neuronal membrane potential is not fully differentiable, hence the proposed IP rule can directly regulate the membrane potential as an auxiliary “output signal” to desired distribution to maximize its information entropy. We experimentally evaluated the proposed IP rule for pattern recognition on the spiking feed-forward and spiking convolutional neural network models. Experimental results verified that the proposed IP rule can effectively improve spiking neural network computational performance in terms of classification accuracy, spiking inference speed, and noise robustness.},
  archive      = {J_TCDS},
  author       = {Anguo Zhang and Yueming Gao and Yuzhen Niu and Xiumin Li and Qing Chen},
  doi          = {10.1109/TCDS.2020.3041610},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {337-347},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Intrinsic plasticity for online unsupervised learning based on soft-reset spiking neuron model},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploration with intrinsic motivation using
object–action–outcome latent space. <em>TCDS</em>, <em>15</em>(2),
325–336. (<a href="https://doi.org/10.1109/TCDS.2021.3062728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One effective approach for equipping artificial agents with sensorimotor skills is to use self-exploration. To do this efficiently is critical, as time and data collection are costly. In this study, we propose an exploration mechanism that blends action, object, and action outcome representations into a latent space, where local regions are formed to host forward model (FM) learning. The agent uses intrinsic motivation to select the FM with the highest learning progress (LP) to adopt at a given exploration step. This parallels how infants learn, as high LP indicates that the learning problem is neither too easy nor too difficult in the selected region. The proposed approach is validated with a simulated robot in a tabletop environment. The simulation scene comprises a robot and various objects, where the robot interacts with one of them each time using a set of parameterized actions and learns the outcomes of these interactions. With the proposed approach, the robot organizes its curriculum of learning as in existing intrinsic motivation approaches and outperforms them in learning speed. Moreover, the learning regime demonstrates features that partially match infant development; in particular, the proposed system learns to predict the outcomes of different skills in a staged manner.},
  archive      = {J_TCDS},
  author       = {Melisa Idil Sener and Yukie Nagai and Erhan Oztop and Emre Ugur},
  doi          = {10.1109/TCDS.2021.3062728},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {325-336},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Exploration with intrinsic motivation using Object–Action–Outcome latent space},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial special issue on intrinsically motivated
open-ended learning (IMOL). <em>TCDS</em>, <em>15</em>(2), 321–324. (<a
href="https://doi.org/10.1109/TCDS.2023.3276597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of lifelong [1] , continual [2] , progressive [3] , or open-ended [4] , [5] , [6] learning by artificial agents or robots is of interest to researchers because it permits robots to adapt to multiple tasks over the course of their life and progressively accumulate knowledge [1] , [3] , [7] . This means that the agent or robot is less likely to become obsolete due to environmental changes, and it may be better equipped to respond to new tasks through the accumulated knowledge. This can include motor skills [8] , high-level behaviors [9] , vision [10] , and social skills [11] , among others. On the other hand, the ability to model and thus “understand” lifelong learning is of potential relevance for robots that interact with children or the elderly, or software agents that interact with people in education or training settings.},
  archive      = {J_TCDS},
  author       = {Kathryn Kasmarik and Gianluca Baldassarre and Vieri Giuliano Santucci and Jochen Triesch},
  doi          = {10.1109/TCDS.2023.3276597},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {321-324},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial special issue on intrinsically motivated open-ended learning (IMOL)},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Obscenity detection in videos through a sequential ConvNet
pipeline classifier. <em>TCDS</em>, <em>15</em>(1), 310–318. (<a
href="https://doi.org/10.1109/TCDS.2022.3158613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of pornographic material available on the Web is staggering. This content is available freely on the Internet and without any restrictions which pose a threat to minors as they might be introduced to such content, which is harmful to their mental health for the long term and also there is huge threats of pornographic content generation to the celebrities, known figures due to invention of deep fakes. There are many software that blocks access to the visually disturbing sites that contains obscene, child pornography, or material “harmful” to minors but only a few software analysis motion content of the videos and mostly only image features are analyzed. To tackle this problem, we propose a frame sequence ConvNet pipeline using ResNet-18 for features extraction and analyzing ${N}$ frame feature map using the proposed ConvNet for the frame sequence classification, therefore implicitly encapsulating the motion information by encoding changes in the ResNet output feature vector, which achieves a state-of-the-art accuracy of 98.25% in classifying pornographic videos of the Pornography-800 data set and state-of-the-art accuracy of 97.15% in classifying videos of the Pornography-2k data set.},
  archive      = {J_TCDS},
  author       = {Neil Gautam and Dinesh Kumar Vishwakarma},
  doi          = {10.1109/TCDS.2022.3158613},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {310-318},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Obscenity detection in videos through a sequential ConvNet pipeline classifier},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3-d facial feature reconstruction and learning network for
facial expression recognition in the wild. <em>TCDS</em>,
<em>15</em>(1), 298–309. (<a
href="https://doi.org/10.1109/TCDS.2022.3157772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) in the wild unavoidably suffers from the effects of face posture, illumination, and partial occlusion. In this article, we attempt to alleviate the above negative effects and improve the performance of FER in the wild based on 3-D face feature reconstruction and learning. Three-dimensional face reconstruction not only can effectively make up for the facial apparent information missing inform a 2-D face images but can also extract accurate 3-D facial geometric information in self-occlusion and extreme illumination scenarios. Therefore, we propose a novel end-to-end trainable 3-D face feature reconstruction and learning network (3-DF-RLN) is proposed for FER in the wild. In 3-DF-RLN, the 2-D implicitly frontalized face apparent data and 3-D facial landmarks are reconstructed by a 3-D face reconstruction module and input to two feature extraction pathways. The appearance pathway learns apparent features from the reconstructed 2-D face apparent data using a convolutional neural network. The geometry pathway learns the geometric features from the reconstructed 3-D facial landmarks using a graph convolutional network. Finally, FER is achieved via the fusion of the two pathways. Extensive experiments were conducted to evaluate the proposed method with three benchmark databases, including Multi-PIE, RAF-DB, and AffectNet. The results show that the proposed 3-DF-RLN model has better FER performance, both in the lab and in the wild. In addition, the face graph from the geometry pathway reveals the correlations between facial landmarks in FER.},
  archive      = {J_TCDS},
  author       = {Ning Sun and Jianglong Tao and Jixin Liu and Haian Sun and Guang Han},
  doi          = {10.1109/TCDS.2022.3157772},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {298-309},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {3-D facial feature reconstruction and learning network for facial expression recognition in the wild},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blurry facial-image deconvolution via model-guided deep
neural network inspired from edge regularization. <em>TCDS</em>,
<em>15</em>(1), 285–297. (<a
href="https://doi.org/10.1109/TCDS.2022.3157766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The blind facial-image deconvolution problem is ill-posed so that the estimation of latent image and blur kernel is hard to obtain without additional priors or heuristics. Although data-driven methods with deep neural networks have achieved excellent performance in this issue, most existing methods are designed like black boxes without transparency and interpretability. Thus, a model-guided explainable approach is proposed to address the above issues on blurry-facial images’ recovery, named the model-guided deep neural network (MG-DNN). To break the barrier between the network design and interpretability, we optimize blind image deconvolution based on salient edge regularization (BID-SER) and use it to guide the proposed MG-DNN. First, basic units for processing image and kernel features are designed with convolutional operators. Then, unfolding the optimization of BID-SER, the multiblocks concatenation is designed by several kernel networks and image networks with basic units. Finally, comprehensive experiments evaluated by several metrics demonstrate that the proposed MG-DNN performs favorably with existing state-of-the-art methods while processing on three different face data sets.},
  archive      = {J_TCDS},
  author       = {Xiaoyuan Yu and Wei Xie},
  doi          = {10.1109/TCDS.2022.3157766},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {285-297},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Blurry facial-image deconvolution via model-guided deep neural network inspired from edge regularization},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convolutional multiple instance learning for sleep spindle
detection with label refinement. <em>TCDS</em>, <em>15</em>(1), 272–284.
(<a href="https://doi.org/10.1109/TCDS.2022.3159285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sleep spindles are closely associated with cognitive functions and neurological disorders; thus, spindle detection has been an important topic in sleep medicine. Recently, machine learning approaches have shown the potential in automatic sleep spindle detection by learning optimized features in a data-driven way, while they highly rely on labeled data, and the performance can be degraded when labels are inaccurate. However, accurate annotation of the spindle is usually difficult to obtain and high intraexpert and interexpert variance exist. In this work, we propose a convolutional neural network (CNN) with a label refinement component to learn an effective spindle detector with imperfect labels. Our approach consists of two stages: 1) a feature learning stage and 2) a label refinement stage. In the feature learning stage, a CNN-based multiple instance learning framework (CNN-MIL) is built for spindle feature learning. By assuming only parts of each labeled spindle segment contain true spindle patterns, the CNN-MIL model can learn most-likely spindle-related features from ambiguous labels. In the label refinement stage, we adjust the spindle labels by merging the original labels and labels predicted by CNN-MIL, and the modified labels are then used in the next round CNN-MIL feature learning. The two stages perform alternately for detector optimization. Extensive experiments demonstrated that our approach achieved the state-of-the-art performance.},
  archive      = {J_TCDS},
  author       = {Xuyun Sun and Yu Qi and Yueming Wang and Gang Pan},
  doi          = {10.1109/TCDS.2022.3159285},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {272-284},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Convolutional multiple instance learning for sleep spindle detection with label refinement},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multistream 3-d convolution neural network with parameter
sharing for human state estimation. <em>TCDS</em>, <em>15</em>(1),
261–271. (<a href="https://doi.org/10.1109/TCDS.2022.3153676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large amount of data is one challenge in electroencephalogram (EEG) analysis, in which the channel Two Dimensional (2-D), time One Dimensional (1-D), and spectral (1-D) are generally considered. Convolutional neural networks (CNNs) have drawn much attention for automatic feature learning in various fields. Meanwhile, many studies have demonstrated integration from multiple sources and decisions could boost performance. However, CNN for EEG analysis usually involves millions of parameters, which easily leads to overfitting. A new model of a multistream Three Dimensional (3-D) CNN with parameter sharing is proposed for EEG. Two EEG data sets: 1) the lane-keeping task (LKT) data set and 2) sleep data set are applied. For the LKT data set, the proposed multistream 3-D CNN with parameter sharing model achieves 0.5486 root-mean-square error (RMSE), showing improvement by at least 2.77% compared to the other approaches. In the sleep data set, the error rate of the proposed model was 24.65%, showing at least 10.28% improvement in performance compared to the other methods. The lower RMSE and error rate show that the multistream 3-D CNN with parameter sharing model efficiently extracts significant features from EEG data. Moreover, the sharing mechanism even reduces the risk of overfitting and the number of parameters by comprehending common representations among multiple streams.},
  archive      = {J_TCDS},
  author       = {Chin-Teng Lin and Jia Liu and Chieh-Ning Fang and Shih-Ying Hsiao and Yu-Cheng Chang and Yu-Kai Wang},
  doi          = {10.1109/TCDS.2022.3153676},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {261-271},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multistream 3-D convolution neural network with parameter sharing for human state estimation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Are nonimage data really necessary for disease prediction
with graph convolutional networks? <em>TCDS</em>, <em>15</em>(1),
252–260. (<a href="https://doi.org/10.1109/TCDS.2022.3152791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have been widely applied to automatic disease diagnosis based on neuroimaging data, and nonimage data are used to determine local connections in the GCN mode. However, previous studies reveal the GCN model may perform even worse than the linear model if nonimage data are inappropriate or unavailable. Considering that manually identifying disease-related nonimage data among numerous alternatives is time-consuming and nonimage data are usually not available in medical data sets for privacy reasons, this shortage limits the application of the GCN model. Besides, whether nonimage data are really necessary is also worth discussing, since much literature have revealed that neuroimaging data can well characterize nonimage data. To overcome the limitation, we apply a nonlocal operation (a special form of attention mechanism) to the GCN model (nonlocal GCN), which automatically determines local connection based on image data and no more relies on the nonimage data. The experiments on two public data sets show that the proposed model can perform better or achieve almost the same performance without using any nonimage data. The results on simulation data sets reveal that our model can perform better in feature-driven data sets if image data contain nonimage data, in which the nonimage data are not necessary.},
  archive      = {J_TCDS},
  author       = {Gen Shi and Yifan Zhu and Zhensen Chen and Jinyan Liu and Xuesong Li},
  doi          = {10.1109/TCDS.2022.3152791},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {252-260},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Are nonimage data really necessary for disease prediction with graph convolutional networks?},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fall-perceived action recognition of persons with
neurological disorders using semantic supervision. <em>TCDS</em>,
<em>15</em>(1), 242–251. (<a
href="https://doi.org/10.1109/TCDS.2022.3157813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent uncertain falls is one of the common cause of injury among elderly adults and persons suffering from the neurological disorder. It will be costlier to go through $24\times 7$ medical monitoring if we monitor a person suffering from the early stage of the neurological disorder. An “uncertain” action classification model can be a less costly and easily scalable. It can help to regularly monitor a person suffering from neurological declines and how frequent it relapse. In this article, we propose a video-based action recognition with fall detection architecture, FallNet, which learns the features of uncertain actions related to day-to-day activities. FallNet first incorporates semantic supervision using the per-class weight of uncertain action through class-wise weighted focal loss. It addresses both the class imbalance problem and the weak interclass separability issue. We design a joint training model to train the overall architecture efficiently in an end-to-end manner. We utilize benchmark data sets, OOPS, HMDB51, and Kinetics-600, for experimentation that has less falling action videos. Therefore, we have collected videos to create a data set, denoted by FallAction, that consists of different 15 falling action classes with an average of 100 videos per class. The proposed network gain an accuracy of 13.2% in OOPs, 2% in HMDB51, and 0.2% in Kinetics-600 data set.},
  archive      = {J_TCDS},
  author       = {Nitika Nigam and Tanima Dutta and Deepali Verma},
  doi          = {10.1109/TCDS.2022.3157813},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {242-251},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Fall-perceived action recognition of persons with neurological disorders using semantic supervision},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fusing attention network based on dilated convolution for
superresolution. <em>TCDS</em>, <em>15</em>(1), 234–241. (<a
href="https://doi.org/10.1109/TCDS.2022.3153090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks with different filters or multiple branches have achieved good performance for single superresolution (SR) in recent years. However, they ignore the high-frequency components of the multiscale context information of the low-resolution image. To solve this problem, we propose a fusing attention network based on dilated convolution (DFAN) for SR. Specifically, we first propose a dilated convolutional attention module (DCAM), which captures multiscale contextual information from different regions of LR images by locking multiple regions with different sizes of receptive fields. Then, we propose a multifeature attention block (MFAB), further focus on high-frequency components of multiscale contextual information, and extract more high-frequency features. Experimental results demonstrate that the proposed DFAN achieves performance improvements in terms of visual quality evaluation and quantitative evaluation.},
  archive      = {J_TCDS},
  author       = {Zhaoyang Song and Xiaoqiang Zhao and Yongyong Hui and Hongmei Jiang},
  doi          = {10.1109/TCDS.2022.3153090},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {234-241},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Fusing attention network based on dilated convolution for superresolution},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning of depth and ego-motion from videos
by alternative training and geometric constraints from 3-d to
2-d. <em>TCDS</em>, <em>15</em>(1), 223–233. (<a
href="https://doi.org/10.1109/TCDS.2022.3152241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning of depth and ego-motion from unlabeled monocular videos has acquired promising results and drawn extensive attention. Most of the existing methods jointly train the depth and pose networks by photometric consistency of adjacent views based on the principle of structure-from-motion (SFM). However, the coupled relationship of the depth and pose networks based on the scene reprojection seriously influences the learning performance due to the scale ambiguity of image reconstruction-based geometry learning or the error accumulation between the learning-based method and multiview geometry-based method. In this article, we aim to improve the performance of depth and pose estimation without the auxiliary tasks and reduce the influence of the above problems on algorithm performance by alternatively training each task and geometric constraints from 3-D to 2-D. Distinct from jointly training the depth and pose networks, our key idea is to better utilize the mutual dependency between two tasks by alternatively training each network with respective geometric constraints while fixing the other. To make the optimization process easier, the iterative closest point (ICP)-based 3-D structural consistency-embedded epipolar geometric constraints are further introduced into depth and pose networks learning, which can take full advantage of both geometric methods. Then, a log-scale 3-D structural consistency loss is designed to put more emphasis on the smaller depth values during training. Extensive experiments on various benchmark data sets indicate the superiority of our algorithm over the state-of-the-art self-supervised methods.},
  archive      = {J_TCDS},
  author       = {Jiaojiao Fang and Guizhong Liu},
  doi          = {10.1109/TCDS.2022.3152241},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {223-233},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Self-supervised learning of depth and ego-motion from videos by alternative training and geometric constraints from 3-D to 2-D},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). C-GRAIL: Autonomous reinforcement learning of multiple and
context-dependent goals. <em>TCDS</em>, <em>15</em>(1), 210–222. (<a
href="https://doi.org/10.1109/TCDS.2022.3152081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When facing the problem of autonomously learning to achieve multiple goals, researchers typically focus on problems where each goal can be solved using just one policy. However, in environments presenting different contexts, the same goal might need different skills to be solved. These situations pose two challenges: 1) recognize which are the contexts that need different policies to perform the goals and 2) learn the policies to accomplish the same goal in the identified relevant contexts. These two challenges are even harder if faced within an open-ended learning framework where potentially an agent has no information on the environment, possibly not even about the goals it can pursue. We propose a novel robotic architecture, contextual GRAIL (C-GRAIL), that solves these challenges in an integrated fashion. The architecture is able to autonomously detect new relevant contexts and ignore irrelevant ones, on the basis of the decrease of the expected performance for a given goal. Moreover, C-GRAIL can quickly learn the policies for new contexts leveraging on transfer learning techniques. The architecture is tested in a simulated robotic environment involving a robot that autonomously discovers and learns to reach relevant target objects in the presence of multiple obstacles generating several different contexts.},
  archive      = {J_TCDS},
  author       = {Vieri Giuliano Santucci and Davide Montella and Gianluca Baldassarre},
  doi          = {10.1109/TCDS.2022.3152081},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {210-222},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {C-GRAIL: Autonomous reinforcement learning of multiple and context-dependent goals},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implicit robot control using error-related potential-based
brain–computer interface. <em>TCDS</em>, <em>15</em>(1), 198–209. (<a
href="https://doi.org/10.1109/TCDS.2022.3151860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the application of using error-related potential (ErrP)-based brain–computer interface (BCI) paradigm to control robot movements with implicit commands. ErrP is a neural signal that is automatically evoked when the machine’s behavior deviates from the observer’s expectations. By continuously monitoring the presence of ErrP, the system infers the observer’s reaction toward robot movements and automatically translates them into control commands, allowing the implicit control of robot movements without interfering the observer’s other tasks. However, ErrP-based BCI has a major limitation: the ErrP is evoked after the robot has committed an error, which might be costly or dangerous in contexts, such as assembly line or autonomous driving. To address these limitations, we propose a novel robotic design for ErrP-based BCI that allows humans to continuously evaluate the robot’s intentions and intervene earlier, if necessary before the robot commits an error. We evaluate the proposed robotic design and BCI system via an experiment where a ground robot performs a binary target-reaching task. The high classification accuracy (77.57%) demonstrated that the proposed ErrP-based BCI is feasible for human–robot intention communication before the robot commits an error and has the potential to broaden the range of applications for ErrP-based BCIs.},
  archive      = {J_TCDS},
  author       = {Xiaofei Wang and Hsiang-Ting Chen and Yu-Kai Wang and Chin-Teng Lin},
  doi          = {10.1109/TCDS.2022.3151860},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {198-209},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Implicit robot control using error-related potential-based Brain–Computer interface},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Six-dimensional target pose estimation for robot autonomous
manipulation: Methodology and verification. <em>TCDS</em>,
<em>15</em>(1), 186–197. (<a
href="https://doi.org/10.1109/TCDS.2022.3151331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The autonomous and precise grasping operation of robots is considered challenging in situations where there are different objects with different shapes and postures. In this study, we proposed a method of 6-D target pose estimation for robot autonomous manipulation. The proposed method is based on: 1) a fully convolutional neural network for scene semantic segmentation and 2) fast global registration to achieve target pose estimation. To verify the validity of the proposed algorithm, we built a robot grasping operation system and used the point cloud model of the target object and its pose estimation results to generate the robot grasping posture control strategy. Experimental results showed that the proposed method can achieve a six-degree-of-freedom pose estimation for arbitrarily placed target objects and complete the autonomous grasping of the target. Comparative experiments demonstrated that the proposed target pose estimation method achieved a significant improvement in average accuracy and real-time performance compared with traditional methods.},
  archive      = {J_TCDS},
  author       = {Rui Wang and Congjia Su and Hao Yu and Shuo Wang},
  doi          = {10.1109/TCDS.2022.3151331},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {186-197},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Six-dimensional target pose estimation for robot autonomous manipulation: Methodology and verification},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial expression recognition through cross-modality
attention fusion. <em>TCDS</em>, <em>15</em>(1), 175–185. (<a
href="https://doi.org/10.1109/TCDS.2022.3150019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions are generally recognized based on handcrafted and deep-learning-based features extracted from RGB facial images. However, such recognition methods suffer from illumination/pose variations. In particular, they fail to recognize these expressions with weak emotion intensities. In this work, we propose a cross-modality attention-based convolutional neural network (CM-CNN) for facial expression recognition. We extract expression-related features from complementary facial images (gray-scale, local binary pattern, and depth images) to handle the illumination/pose variations and to capture appearance details that describe expressions with weak emotion intensities. Rather than directly concatenating the complementary features, we propose a novel cross-modality attention fusion network to enhance spatial correlations between any two types of facial images. Finally, the CM-CNN is optimized with an improved focal loss, which pays more attention to facial expressions with weak emotion intensities. The average classification accuracies on VT-KFER, BU-3DFE(P1), BU-3DFE(P2), and Bosphorus are 93.86%, 88.91%, 87.28%, and 85.16%, respectively. Evaluations on these databases demonstrate that our approach is competitive to state-of-the-art algorithms.},
  archive      = {J_TCDS},
  author       = {Rongrong Ni and Biao Yang and Xu Zhou and Angelo Cangelosi and Xiaofeng Liu},
  doi          = {10.1109/TCDS.2022.3150019},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {175-185},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Facial expression recognition through cross-modality attention fusion},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion recognition based on EEG brain rhythm sequencing
technique. <em>TCDS</em>, <em>15</em>(1), 163–174. (<a
href="https://doi.org/10.1109/TCDS.2022.3149953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a technique that analyzes electroencephalography (EEG) using brain rhythms ( $\delta $ , $\theta $ , $\alpha $ , $\beta $ , and $\gamma $ ) presented in a sequential format and applies it for emotion recognition. Although brain rhythms are regarded as reliable parameters in EEG-based emotion recognition, to achieve high accuracy by considering fewer optimal multichannel rhythmic features (MCRFs) has not been addressed in detail. Thus, the rhythm sequence for each channel is generated by choosing the strongest brain rhythm having the maximum instantaneous power for every 200-ms time bin. A ${k}$ -nearest neighbor ( ${k}$ -NN) classifier is employed for evaluating the rhythmic features extracted from different sequences, and the experimental validation was performed on three well-known emotional databases (DEAP, MAHNOB, and SEED). The results showed that approximately 30% of MCRFs for as high as 87%–92%, achieving high classification accuracies with a small number of data. Further investigation revealed that the frontal and parietal regions are active during the emotional process, as consistent as earlier studies. Therefore, the proposed technique demonstrates its availability and reliability for emotion recognition. It also provides a novel solution to find optimal channel-specific rhythmic features in EEG signal analysis.},
  archive      = {J_TCDS},
  author       = {Jia Wen Li and Shovan Barma and Sio Hang Pun and Mang I. Vai and Peng Un Mak},
  doi          = {10.1109/TCDS.2022.3149953},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {163-174},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Emotion recognition based on EEG brain rhythm sequencing technique},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cerebellum-inspired network model and learning approaches
for solving kinematic tracking control of redundant manipulators.
<em>TCDS</em>, <em>15</em>(1), 150–162. (<a
href="https://doi.org/10.1109/TCDS.2022.3149622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking control of redundant manipulators is always a basic and important issue in robotics. Existing studies have indicated that the pivotal region of the brain associated with human motion control is the cerebellum. This motivates us to devise a model-based cerebellum-inspired (MBCI) scheme and a model-free cerebellum-inspired (MFCI) scheme for the tracking control of redundant manipulators in this article. The MBCI scheme solves the inverse kinematics problem with a cerebellum model. By using the parameters and Jacobian matrix of the manipulator, the task space error is transformed into joint space error, which is taken as the teaching signal to train the cerebellum model designed based on the echo state network. The MFCI scheme is formed by coupling a cerebellum model and a multilayer perceptron (MLP). The MLP is able to generate approximate joint angle commands to the manipulator and the cerebellum model is utilized to fine-tune the MLP controller, thereby improving the tracking accuracy. In addition, leaky integrator neurons (LINs) are integrated into the cerebellum model to further improve the performance of the proposed schemes. Finally, comparative simulations and physical experiments on different types of redundant manipulators are conducted to verify the efficacy and merits of the proposed cerebellum-inspired schemes.},
  archive      = {J_TCDS},
  author       = {Ning Tan and Peng Yu and Fenglei Ni},
  doi          = {10.1109/TCDS.2022.3149622},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {150-162},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A cerebellum-inspired network model and learning approaches for solving kinematic tracking control of redundant manipulators},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible behavioral decision making of mobile robot in
dynamic environment. <em>TCDS</em>, <em>15</em>(1), 134–149. (<a
href="https://doi.org/10.1109/TCDS.2022.3149602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to achieve flexible behavioral decision making in a dynamic environment is an important prerequisite for a mobile robot to execute various tasks. Traditional methods suffer from discontinuous and unstable learning ability. This article brings the mobile robot continuous and stable learning ability in the dynamic environment, through improving the binding manner between the cerebellum and basal ganglia in the neuromodulatory system proposed in our previous work, thus to enhance its flexible behavioral decision-making performance. Moreover, a more biological significance index, i.e., the curiosity index, $Cur$ , is designed to mimic the activity switch of the locus coeruleus between the tonic and phasic mode, to modulate the exploration-exploitation tradeoff in the reinforcement learning (RL) of the basal ganglia. Influence of varying learning rate and varying discount rate on the performance of the RL is also investigated. The experiment results in static and dynamic environments, as well as the comparative experiment in the complex maze environment and the complex dynamic environment, demonstrate the potential of the proposed neuromodulatory system.},
  archive      = {J_TCDS},
  author       = {Jinbiao Zhu and Dongshu Wang and Jikai Si},
  doi          = {10.1109/TCDS.2022.3149602},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {134-149},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Flexible behavioral decision making of mobile robot in dynamic environment},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensing and navigation of wearable assistance cognitive
systems for the visually impaired. <em>TCDS</em>, <em>15</em>(1),
122–133. (<a href="https://doi.org/10.1109/TCDS.2022.3146828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a wearable vision-based assistance system to provide situational awareness for blind and visually impaired (BVI) people in indoor scenarios. The system is built upon nonintrusive wearable devices, including an RGB-D camera, an embedded computer, and haptic modules. First, the depth map and color images of the scene are obtained from an RGB-D camera, which provides 3-D environmental information. The modular work modes are then designed for different tasks, such as navigation and multitarget recognition. Then, the cognition results are summarized and presented to the user through verbal or haptic feedback. Our system is evaluated by a pilot test to validate its effectiveness of improving the navigation capabilities and multitarget recognition capabilities for the BVI in indoor environments. We present study results with different tasks, including navigation, object localization, face recognition, and text reading. The experiments prove that the system can meet the needs of the BVI in daily use.},
  archive      = {J_TCDS},
  author       = {Guoxin Li and Jiaqi Xu and Zhijun Li and Chao Chen and Zhen Kan},
  doi          = {10.1109/TCDS.2022.3146828},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {122-133},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Sensing and navigation of wearable assistance cognitive systems for the visually impaired},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing deep neural networks through neuroevolution with
stochastic gradient descent. <em>TCDS</em>, <em>15</em>(1), 111–121. (<a
href="https://doi.org/10.1109/TCDS.2022.3146327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved remarkable success in computer vision; however, training DNNs for satisfactory performance remains challenging and suffers from sensitivity to empirical selections of an optimization algorithm for training. Stochastic gradient descent (SGD) is dominant in training a DNN by adjusting neural network weights to minimize the DNN’s loss function. As an alternative approach, neuroevolution is more in line with an evolutionary process and provides some key capabilities that are often unavailable in SGD, such as the heuristic black-box search strategy based on individual collaboration in neuroevolution. This article proposes a novel approach that combines the merits of both neuroevolution and SGD, enabling evolutionary search, parallel exploration, and an effective probe for optimal DNNs. A hierarchical cluster-based suppression algorithm is also developed to overcome similar weight updates among individuals for improving population diversity. We implement the proposed approach in four representative DNNs based on four publicly available data sets. The experimental results demonstrate that the four DNNs optimized by the proposed approach all outperform corresponding ones optimized by only SGD on all data sets. The performance of DNNs optimized by the proposed approach also outperforms state-of-the-art deep networks. This work also presents a meaningful attempt for pursuing artificial general intelligence.},
  archive      = {J_TCDS},
  author       = {Haichao Zhang and Kuangrong Hao and Lei Gao and Bing Wei and Xuesong Tang},
  doi          = {10.1109/TCDS.2022.3146327},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {111-121},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Optimizing deep neural networks through neuroevolution with stochastic gradient descent},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Isokinetic muscle strength training strategy of an ankle
rehabilitation robot based on adaptive gain and cascade PID control.
<em>TCDS</em>, <em>15</em>(1), 100–110. (<a
href="https://doi.org/10.1109/TCDS.2022.3145998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Isokinetic muscle strength training refers to the mode of movement based on constant speed and variable resistance, which can guarantee the maximum resistance and force–distance output of each muscle in different angles of exercise. In this article, we develop an isokinetic muscle strength training strategy based on the adaptive gain and cascade proportion integration differentiation (PID) controller for ankle rehabilitation on our newly developed ankle robotic system. First, a heuristic threshold intention recognition method based on time-series integration was proposed to precisely recognize the motion intention, thus inducing the motion in this direction. Then, an adaptive gain algorithm was developed to provide speed gain for the isokinetic training, to avoid jitter during speed switching. Finally, the isokinetic characteristic was realized by the cascade PID controller, which can control the motion velocity in a given range with fast response speed. Experiments with healthy subjects showed good performance in the smoothness of the control system, the accuracy, and the real-time performance of velocity tracking. By introducing the isokinetic characteristic in ankle rehabilitation, the ankle robot can provide resistance training at a constant speed no matter how much force the patient uses, which is a very functional supplement and improvement for ankle rehabilitation.},
  archive      = {J_TCDS},
  author       = {Jianfeng Li and Yu Zhou and Mingjie Dong and Xi Rong},
  doi          = {10.1109/TCDS.2022.3145998},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {100-110},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Isokinetic muscle strength training strategy of an ankle rehabilitation robot based on adaptive gain and cascade PID control},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantized reservoir computing for spectrum sensing with
knowledge distillation. <em>TCDS</em>, <em>15</em>(1), 88–99. (<a
href="https://doi.org/10.1109/TCDS.2022.3147789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantization has been widely used to compress machine learning models for deployments on the field-programmable gate array (FPGA). However, quantization often degrades the accuracy of a model. In this work, we introduce a quantization approach to reduce the computation/storage resource consumption of a model without losing much accuracy. Spectrum sensing is a technique to identify the idle/busy bandwidths in cognitive radio. The spectrum occupancy of each bandwidth maintains a temporal correlation with previous and future time slots. A recurrent neural network (RNN) is very suitable for spectrum sensing. Reservoir computing (RC) is a computation framework derived from the theory of RNNs. It is a better choice than RNN for spectrum sensing on FPGA because it is easier to train and requires fewer computation resources. We apply our quantization approach to the RC to reduce the resource consumption on FPGA. A knowledge distillation (KD) called teacher-student mutual learning (TSML) is proposed for the quantized RC to minimize quantization errors. The TSML resolves the mismatched capacity issue of conventional KD and enables KD on small data sets. On the spectrum-sensing data set, the quantized RC trained with the TSML achieves comparable accuracy and reduces the resource utilization of digital signal processing (DSP) blocks, flip-flop (FF), and lookup table (LUT) by 53%, 40%, and 35%, respectively, compared to the RNN. The inference speed of the quantized RC is 2.4 times faster. The TSML improves the accuracy of the quantized RC by 2.39%, which is better than the conventional KD.},
  archive      = {J_TCDS},
  author       = {Shiya Liu and Lingjia Liu and Yang Yi},
  doi          = {10.1109/TCDS.2022.3147789},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {88-99},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Quantized reservoir computing for spectrum sensing with knowledge distillation},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collision-free navigation in human-following task using a
cognitive robotic system on differential drive vehicles. <em>TCDS</em>,
<em>15</em>(1), 78–87. (<a
href="https://doi.org/10.1109/TCDS.2022.3145915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As human–robot collaboration increases tremendously in real-world applications, a fully autonomous and reliable mobile robot for the collaboration has been a central research topic and investigated extensively in a large number of studies. One of the most pressing issues in such topic is the collision-free navigation that has a moving goal and unknown obstacles under the unstructured environment. In this article, a cognitive robotic system (CRS) is proposed for the robot to navigate itself to the moving target person without obstacle collision. This CRS consists of a cognitive agent, which is created based on the Soar cognitive architecture to reason its current situation and make action decision for the robot to avoid obstacles and reach the target position, and a speed planning module, which is based on dynamic window approach (DWA) to generate appropriate linear and angular velocities for driving the robot’s motors. For the implementation of the proposed system, we use a differential drive wheel robot equipped with two ultrawideband (UWB) sensors and a color depth camera as the experimental platform. Finally, to evaluate the performance of our system in actual operating conditions, we conduct experiments with a scenario that includes main tasks: avoiding consecutive unknown obstacles and turning at corner while the robot follows continuously human user along the corridor.},
  archive      = {J_TCDS},
  author       = {Chien Van Dang and Heungju Ahn and Jong-Wook Kim and Sang C. Lee},
  doi          = {10.1109/TCDS.2022.3145915},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {78-87},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Collision-free navigation in human-following task using a cognitive robotic system on differential drive vehicles},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-attention pooling-based long-term temporal network for
action recognition. <em>TCDS</em>, <em>15</em>(1), 65–77. (<a
href="https://doi.org/10.1109/TCDS.2022.3145839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Internet of Things (IoT), self-driving technology has been successful. Yet safe driving faces challenges due to such cases as pedestrians crossing roads. How to sense their movements and identify their behaviors from video data is important. Most of the existing methods fail to: 1) capture long-term temporal relationship well due to their limited temporal coverage and 2) aggregate discriminative representation effectively, such as caused by little or even no attention paid to differences among representations. To address such issues, this work presents a new architecture called a self-attention pooling-based long-term temporal network (SP-LTN), which can learn long-term temporal representations and aggregate those discriminative representations in an end-to-end manner, and on the other hand, effectively conduct long-term representation learning on a given video by capturing spatial information and mining temporal patterns. Next, it develops a self-attention pooling method to predict the importance scores of obtained representations for distinguishing them from each other and then weights them together to highlight the contributions of those discriminative representations in action recognition. Finally, it designs a new loss function that combines a standard cross-entropy loss function with a regularization term to further focus on the discriminative representations while restraining the impact of distractive ones on activity classification. Experimental results on two data sets show that our SP-LTN, fed by only red–green–blue (RGB) frames, outperforms the state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Huifang Li and Jingwei Huang and Mengchu Zhou and Qisong Shi and Qing Fei},
  doi          = {10.1109/TCDS.2022.3145839},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {65-77},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Self-attention pooling-based long-term temporal network for action recognition},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement-learning-based dynamic opinion maximization
framework in signed social networks. <em>TCDS</em>, <em>15</em>(1),
54–64. (<a href="https://doi.org/10.1109/TCDS.2022.3141952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic opinion maximization (DOM) is a significant optimization issue, whose target is to select some nodes in the network and prorogate the opinions of network nodes, and produce the optimum node opinions. Until now, the node opinions of related researches are unchanged and seldom focus on social relationships. In the real scenario, the dynamic process of network nodes over time and user preference have existed. Therefore, this article proposes the ${Q}$ -learning-based DOM (QDOM) framework in signed social networks to solve the OM problem, which is made up of two phases: 1) the activated dynamic opinion model and 2) the ${Q}$ -learning-based seeding process. We propose the activated dynamic opinion model based on stateless ${Q}$ -learning theory to derive the opinion propagation process. Moreover, we design the ${Q}$ -learning-based seeding algorithm to obtain the seed nodes. The experimental results on the four signed social network data sets demonstrate that the proposed framework outperforms the state-of-the-art approaches on positive opinions, the ratio of positive opinions, and activated nodes.},
  archive      = {J_TCDS},
  author       = {Qiang He and Yingjie Lv and Xingwei Wang and Jianhua Li and Min Huang and Lianbo Ma and Yuliang Cai},
  doi          = {10.1109/TCDS.2022.3141952},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {54-64},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Reinforcement-learning-based dynamic opinion maximization framework in signed social networks},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A weakly supervised-guided soft attention network for
classification of intracranial hemorrhage. <em>TCDS</em>,
<em>15</em>(1), 42–53. (<a
href="https://doi.org/10.1109/TCDS.2022.3141591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intracranial hemorrhage (ICH) classification from computed tomography (CT) scans is important for accurate diagnosis of stroke in emergency centers. However, it is challenged by low image contrast and the complex appearance of the lesion. Therefore, the diagnosis results may vary from doctor to doctor under different situations. Existing auxiliary diagnosis algorithms treat the features of each location as equally important, and they do not impose any constraints on the learned features. The learned features used for classification in the model contain noise or features that have no diagnostic significance, which greatly limits the performance and reliability of systems. To facilitate the ICH treatment, we propose to use the auxiliary weak-segmentation-label supervision in multiscale CNN to classify ICH lesions on brain CT images, where the ICH lesions are segmented and used to guide the attention of the classification network. The weak segmentation labels we use are generated by an unsupervised algorithm and do not require additional annotations. We conduct experiments on the RSNA ICH detection data set, and the results demonstrate that our method can achieve an average ACC of 98.1% and an average $F1$ score of 74.6% on the test set of nearly 80 000 CT images. The proposed weakly supervised-guided attention mechanism can accurately activate those neurons related to diagnosis, while inhibiting the activation of irrelevant areas and noise, thus achieving good performance.},
  archive      = {J_TCDS},
  author       = {Long Zhang and Wenlong Miao and Chuang Zhu and Yuanyuan Wang and Yihao Luo and Ruoning Song and Lian Liu and Jie Yang},
  doi          = {10.1109/TCDS.2022.3141591},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {42-53},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A weakly supervised-guided soft attention network for classification of intracranial hemorrhage},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptation through prediction: Multisensory active inference
torque control. <em>TCDS</em>, <em>15</em>(1), 32–41. (<a
href="https://doi.org/10.1109/TCDS.2022.3156664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptation to external and internal changes is of major importance for robotic systems in uncertain environments. Here, we present a novel multisensory active inference (AIF) torque controller for industrial arms that shows how prediction can be used to resolve adaptation. Our controller, inspired by the predictive brain hypothesis, improves the capabilities of current AIF approaches by incorporating learning and multimodal integration of low- and high-dimensional sensor inputs (e.g., raw images) while simplifying the architecture. We performed a systematic evaluation of our model on a 7DoF Franka Emika Panda robot arm by comparing its behavior with previous AIF baselines and classic controllers, analyzing both qualitatively and quantitatively adaptation capabilities and control accuracy. The results showed improved control accuracy in goal-directed reaching with high noise rejection due to multimodal filtering, and adaptability to dynamical inertial changes, elasticity constraints, and human disturbances without the need to relearn the model or parameter retuning.},
  archive      = {J_TCDS},
  author       = {Cristian Meo and Giovanni Franzese and Corrado Pezzato and Max Spahn and Pablo Lanillos},
  doi          = {10.1109/TCDS.2022.3156664},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {32-41},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Adaptation through prediction: Multisensory active inference torque control},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-pass online learning based on gradient descent for
multilayer spiking neural networks. <em>TCDS</em>, <em>15</em>(1),
16–31. (<a href="https://doi.org/10.1109/TCDS.2021.3140115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online supervised learning algorithms update synaptic weights in real-time during the running process of spiking neural networks (SNNs), which are important for modeling the behavior and cognitive process of the brain. This article proposes an online supervised learning algorithm based on gradient descent for multilayer feedforward SNNs, where precisely timed spike trains are used to represent neural information. The online learning rule is derived from the real-time error function and backpropagation mechanism. The synaptic weights are adjusted when an output neuron fires a spike. Results of spike train learning demonstrate that the proposed online learning algorithm can achieve higher learning accuracy and requires fewer learning epochs than the corresponding offline learning method and other typical supervised learning algorithms. Furthermore, the proposed algorithm is used for solving pattern classification problems, where the one-pass learning approach is employed for training SNNs. Results show that the proposed algorithm can obtain comparable classification accuracy compared with other state-of-the-art algorithms even in the case of only one iteration. It indicates that the proposed algorithm is effective for solving spatio-temporal pattern recognition problems.},
  archive      = {J_TCDS},
  author       = {Xianghong Lin and Tiandou Hu and Xiangwen Wang},
  doi          = {10.1109/TCDS.2021.3140115},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {16-31},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {One-pass online learning based on gradient descent for multilayer spiking neural networks},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vision-and-language navigation based on cross-modal feature
fusion in indoor environment. <em>TCDS</em>, <em>15</em>(1), 3–15. (<a
href="https://doi.org/10.1109/TCDS.2021.3139543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging for an agent to simultaneously decipher visual and language information and make decisions to perform corresponding actions. Recently, the vision-and-language navigation task has been proposed to allow the agent to navigate based on a language instruction and the currently visible visual point information in a 3-D indoor real environment. The key to this task is that the agent needs to understand the information of the two models of vision and language in an unknown environment to navigate effectively. In this study, we capture the alignment relationship between visual features and language features using a cross-modal feature fusion method. Attention is used to set up the cross-modal fusion module so that visual features contain language information and language features contain visual information, thereby allowing the model to learn more feature relationships and improving the success rate (SR) of agent navigation. Considering the practical significance of the navigation of the agent, we aim to shorten the trajectory length of the agent as much as possible while ensuring that the agent reaches the target position successfully. We employ a reinforcement learning algorithm based on the advantage actor critic to constrain the action selection of the agent to shorten the trajectory length. In order to further improve the performance of the model and reduce the difference between the performance of the agent in known environments and unknown environments, we propose the data augmentation method Cro-Speaker, and the three training methods Speaker data augmentation (SD), Cro-Speaker data augmentation (CSD), and Speaker and Cro-Speaker data augmentation (SCSD) based on this method. We evaluate the proposed method based on the Room-to-Room data set. The results show that the proposed method improves the SR of the agent navigation, shortens the length of the navigation trajectory, and exhibits a good generalization performance in known and unknown environments.},
  archive      = {J_TCDS},
  author       = {Shuhuan Wen and Xiaohan Lv and F. Richard Yu and Simeng Gong},
  doi          = {10.1109/TCDS.2021.3139543},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {3-15},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Vision-and-language navigation based on cross-modal feature fusion in indoor environment},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial IEEE transactions on cognitive and developmental
systems. <em>TCDS</em>, <em>15</em>(1), 2. (<a
href="https://doi.org/10.1109/TCDS.2023.3244049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the early spring decorated with pink, white, and yellow plum blossoms, as the Editor-in-Chief of the IEEE Transactions on Cognitive and Developmental Systems (TCDS), I would like to take this opportunity to wish everyone a healthy and productive new year of 2023!},
  archive      = {J_TCDS},
  author       = {Huajin Tang},
  doi          = {10.1109/TCDS.2023.3244049},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {2},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Editorial IEEE transactions on cognitive and developmental systems},
  volume       = {15},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
