<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc---272">TC - 272</h2>
<ul>
<li><details>
<summary>
(2023). A user mobility-based data placement strategy in a hybrid
cloud/edge environment using a causal-aware deep learning network.
<em>TC</em>, <em>72</em>(12), 3603–3616. (<a
href="https://doi.org/10.1109/TC.2023.3311921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing has become a prominent solution when it comes to mobile applications and data management, due to its ability to considerably reduce data transmission costs, and to analyze data requiring fewer computing resources, since the analysis occurs at lower data volumes, without having to relocate data to centralized infrastructures. One major challenge, indicates the optimal data placement regarding data-intensive applications and, in general, applications requiring vast transmission of large amount of data. In this paper, we propose a novel user mobility-based data placement strategy, considering a trade-off between latency and data migration, which has not been investigated before. We classify the users into three mobility classes; namely static, local or mobile, via the use of a causal-aware Deep Learning network. This information is then exploited in order to optimize the data placement through specific data placement and retrieval algorithms for each mobility class. We evaluate the performance of the proposed solution using simulations, and prove that our solution manages to reduce the average data accessing cost by 60\% for static or local users and 10\% for mobile users, while the average path length is reduced by 50\% for static and local users, and by 12\% for mobile users.},
  archive      = {J_TC},
  author       = {Chrysostomos Symvoulidis and Athanasios Kiourtis and George Marinos and Jean-Didier Totow Tom-Ata and George Manias and Argyro Mavrogiorgou and Dimosthenis Kyriazis},
  doi          = {10.1109/TC.2023.3311921},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3603-3616},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A user mobility-based data placement strategy in a hybrid Cloud/Edge environment using a causal-aware deep learning network},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SciNet: Codesign of resource management in cloud computing
environments. <em>TC</em>, <em>72</em>(12), 3590–3602. (<a
href="https://doi.org/10.1109/TC.2023.3310678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of distributed cloud computing technologies has been pivotal for the large-scale adoption of Artificial Intelligence (AI) based applications for high fidelity and scalable service delivery. Systematic resource management is central in maintaining optimal Quality of Service (QoS) in cloud platforms and is divided into three fundamental types: resource provisioning, AI model deployment and workload placement. To exploit the synergy among these decision types, it becomes imperative to concurrently design (co-design) the provisioning, deployment and placement decisions for optimal QoS. As users and cloud service providers shift to non-stationary AI-based workloads, frequent decision making imposes severe time constraints on the resource management models. Existing AI-based solutions often optimize decision types independently and tend to ignore the dependencies across various system performance aspects such as energy consumption and CPU utilization, making them perform poorly in large-scale cloud systems. To address this, we propose a novel method, called SciNet, that leverages a co-simulated digital-twin of the infrastructure to capture inter-metric dependencies and accurately estimate QoS scores. To avoid expensive simulation overheads at test time, SciNet trains a neural network based imitation learner that aims to mimic an oracle, which takes optimal decisions based on co-simulated QoS estimates. Offline model training and online decision making based on the imitation learner, enables SciNet to take optimal decisions while being time-efficient. Experiments with real-life AI-based benchmark applications on a public cloud testbed show that SciNet gives up to 48\% lower execution cost, 79\% higher inference accuracy, 71\% lower energy consumption and 56\% lower response times compared to the current state-of-the-art methods.},
  archive      = {J_TC},
  author       = {Shreshth Tuli and Giuliano Casale and Nicholas R. Jennings},
  doi          = {10.1109/TC.2023.3310678},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3590-3602},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SciNet: Codesign of resource management in cloud computing environments},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Avalon: A scalable and secure distributed transaction ledger
based on proof-of-market. <em>TC</em>, <em>72</em>(12), 3576–3589. (<a
href="https://doi.org/10.1109/TC.2023.3309472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology has gained widespread use. However, it faces several challenges including throughput, transaction delay, security, and decentralization. This paper presents the Avalon protocol based on a novel Proof-of-Market (PoM) consensus mechanism to address these issues. PoM is a type of Proof-of-Work (PoW) consensus that incorporates market-driven leader election and shifts PoW from mining pools to consumers based on transactions. The matching incentive mechanism makes PoM incentive compatible. PoM decouples the scalability and security of Bitcoin, which means that Avalon can optimize the capacity and interval of blocks without compromising other performance goals. Our analysis shows that Avalon can tolerate malicious nodes possessing up to $\bf{1/3}$ of the network&#39;s total computational power. Furthermore, the implementation of Avalon is similar to Bitcoin and is highly concise. We evaluate the performance of Avalon through a simulated network of over $\bf{1,000}$ nodes. Experimental results demonstrate that Avalon can achieve a throughput of $\bf{4,000}$ TPS (transactions per second), which is significantly better than state-of-the-art schemes ( $\bf{10\boldsymbol{\times}}$ Bitcoin-NG, $\bf{5\boldsymbol{\times}}$ ByzCoin, and $\bf{4\boldsymbol{\times}}$ Algorand). Additionally, it has a transaction confirmation delay of up to $\bf{40}$ s, which is twice better than Bitcoin-NG and ByzCoin while experiencing only minimal blockchain splits and maintaining excellent decentralization.},
  archive      = {J_TC},
  author       = {Weilin Chen and Wei Yang and Lide Xue and Bingren Chen and Youwen Zhu and Liusheng Huang},
  doi          = {10.1109/TC.2023.3309472},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3576-3589},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Avalon: A scalable and secure distributed transaction ledger based on proof-of-market},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MemPool: A scalable manycore architecture with a low-latency
shared l1 memory. <em>TC</em>, <em>72</em>(12), 3561–3575. (<a
href="https://doi.org/10.1109/TC.2023.3307796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared L1 memory clusters are a common architectural pattern (e.g., in GPGPUs) for building efficient and flexible multi-processing-element (PE) engines. However, it is a common belief that these tightly-coupled clusters would not scale beyond a few tens of PEs. In this work, we tackle scaling shared L1 clusters to hundreds of PEs while supporting a flexible and productive programming model and maintaining high efficiency. We present MemPool, a manycore system with 256 RV32IMAXpulpimg “Snitch” cores featuring application-tunable functional units. We designed and implemented an efficient low-latency PE to L1-memory interconnect, an optimized instruction path to ensure each PE&#39;s independent execution, and a powerful DMA engine and system interconnect to stream data in and out. MemPool is easy to program, with all the cores sharing a global view of a large, multi-banked, L1 scratchpad memory, accessible within at most five cycles in the absence of conflicts. We provide multiple runtimes to program MemPool at different abstraction levels and illustrate its versatility with a wide set of applications. MemPool runs at 600 MHz (60 gate delays) in typical conditions (TT/0.80 V/25 ${}^{\boldsymbol{\circ}}$ C) in 22 nm FDX technology and achieves a performance of up to 229 GOPS or 180 GOPS/W with less than 2\% of execution stalls.},
  archive      = {J_TC},
  author       = {Samuel Riedel and Matheus Cavalcante and Renzo Andri and Luca Benini},
  doi          = {10.1109/TC.2023.3307796},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3561-3575},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MemPool: A scalable manycore architecture with a low-latency shared l1 memory},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing DNNs with partially equivalent transformations
and automated corrections. <em>TC</em>, <em>72</em>(12), 3546–3560. (<a
href="https://doi.org/10.1109/TC.2023.3307795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) applications are typically represented by tensor programs. To boost the performance of DNN computations, existing works adopt fully equivalent transformations for tensor program optimization by guaranteeing the equivalence on each element of tensors. However, as there are thousands of elements in a tensor, such optimization misses the opportunities that allow the in-equivalence of minority elements. In this work, we propose Pet , the first work that introduces partially equivalent transformations to optimize tensor programs. To maintain the functional equivalence of tensor programs, Pet automatically finds and corrects the in-equivalent positions by leveraging the multi-linearity of DNN computations. Pet further uses a mutation manager to improve search efficiency. Evaluation results show that Pet can achieve up to 1.98 $\times$ and 2.20 $\times$ speedups on NVIDIA Tesla A100 and V100 respectively compared with existing DNN frameworks by introducing new optimization opportunities of partially equivalent transformations.},
  archive      = {J_TC},
  author       = {Haojie Wang and Jidong Zhai and Mingyu Gao and Feng Zhang and Tuowei Wang and Zixuan Ma and Shizhi Tang and Liyan Zheng and Wen Wang and Kaiyuan Rong and Yuanyong Chen and Zhihao Jia},
  doi          = {10.1109/TC.2023.3307795},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3546-3560},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing DNNs with partially equivalent transformations and automated corrections},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instruction profiling based predictive throttling for power
and performance. <em>TC</em>, <em>72</em>(12), 3532–3545. (<a
href="https://doi.org/10.1109/TC.2023.3306079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technology scaling has long been the driving force for reducing power consumption in microprocessor design. As scaling has reached its limits, new techniques are being adopted to address the power problem. Throttling is an architectural mechanism that slows down the pipeline stages to reduce instant dynamic power. However, power savings due to throttling is achieved at the expense of performance degradation. Throttling is commonly applied at fetch, issue, or commit stages where slowing down a particular stage may reduce dynamic power. A balanced bandwidth of fetch, issue, and commit are maintained in designing a pipeline such that execution can flow seamlessly. However, case studies have shown that bottleneck exists at different pipeline stages that result in performance loss. The loss of performance also indicates wasted dynamic power due to pipeline flush. In this paper, we use instruction profiling based on benchmark traces to identify instructions that are causing a significant bottleneck in the targeted processor architecture. Knowledge of the probable residence of congested instructions at different pipeline stages can enable effective CPU throttling. This paper shows that instruction profiling based predictive throttling at fetch and commit stages can save dynamic power at minimal performance loss.},
  archive      = {J_TC},
  author       = {Abdullah A. Owahid and Eugene B. John},
  doi          = {10.1109/TC.2023.3306079},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3532-3545},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Instruction profiling based predictive throttling for power and performance},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Private inference for deep neural networks: A secure,
adaptive, and efficient realization. <em>TC</em>, <em>72</em>(12),
3519–3531. (<a href="https://doi.org/10.1109/TC.2023.3305754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advances in deep neural networks (DNNs) have driven many companies to offer their carefully-trained DNNs as inference services for clients’ private data. The privacy concerns have increasingly motivated the need for private inference (PI), where DNN inferences are performed directly on encrypted data without revealing the client&#39;s private inputs to the server or revealing the server&#39;s proprietary DNN weights to the client. However, existing cryptographic protocols for PI suffer from impractically high latency, stemming mostly from non-linear operators like ReLU activations. In this paper, we propose PAPI, a Practical and Adaptive Private Inference framework. First, we develop an accuracy-adaptive neural architecture search (NAS) approach to generate DNN models tailored for high-efficiency ciphertext computation. Specifically, our NAS automatically generates the DNNs with fewer ReLUs while keeping the accuracy above a user-defined target. Second, we propose secure online/offline protocols for ReLU activation and its approximation variants (i.e., polynomial activations), which purely rely on the lightweight secret sharing techniques in the online execution and can well cope with our optimized DNNs in the ciphertext domain. Experimental results show that PAPI reduces online inference latency on the CIFAR-10/100 and ImageNet datasets by 2.7 ${\times}$ $\sim$ 7.8 ${\times}$ over the state-of-the-art.},
  archive      = {J_TC},
  author       = {Ke Cheng and Ning Xi and Ximeng Liu and Xinghui Zhu and Haichang Gao and Zhiwei Zhang and Yulong Shen},
  doi          = {10.1109/TC.2023.3305754},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3519-3531},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Private inference for deep neural networks: A secure, adaptive, and efficient realization},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). JointPS: Joint parameter server placement and flow
scheduling for machine learning clusters. <em>TC</em>, <em>72</em>(12),
3503–3518. (<a href="https://doi.org/10.1109/TC.2023.3305753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To distill more information from training data, more parameters are introduced into machine learning models. As a result, communication becomes the bottleneck of Distributed Machine Learning (DML) systems. To alleviate the communication resource contention among DML jobs, which prolongs the time to train machine learning models, in machine learning clusters, JointPS is proposed in this paper. JointPS first minimizes the completion time of a single training epoch for each DML job via jointly optimizing the parameter server placement and flow scheduling, and predicts the number of remaining training epochs for each DML job by leveraging a dynamic model fitting method. Then, JointPS can estimate the remaining time to complete each DML job. According to such estimation, JointPS schedules DML jobs following the Minimum Remaining Time First (MRTF) principle to minimize the average job completion time. To the best of our knowledge, JointPS should be the first work that minimizes the average completion time of network-intensive DML training jobs by jointly optimizing the parameter server placement and flow scheduling without modifying the DML models and training procedures. Through both testbed experiments and extensive simulations, we demonstrate that JointPS can reduce the average completion time of DML jobs by up to 88\% compared with state-of-the-art technology.},
  archive      = {J_TC},
  author       = {Yangming Zhao and Cheng Yang and Gongming Zhao and Yunfei Hou and Ting Wang and Chunming Qiao},
  doi          = {10.1109/TC.2023.3305753},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3503-3518},
  shortjournal = {IEEE Trans. Comput.},
  title        = {JointPS: Joint parameter server placement and flow scheduling for machine learning clusters},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LedgerMaze: An efficient privacy-preserving noninteractive
zero-knowledge scheme over account-model blockchain. <em>TC</em>,
<em>72</em>(12), 3489–3502. (<a
href="https://doi.org/10.1109/TC.2023.3305748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prosperity of blockchain has pushed various decentralized applications, e.g., cross-regional finance, due to its advantages of openness, immutability, and decentralization. The feature of openness inevitably leads to a serious privacy breach. Recently, various privacy-enhanced works (e.g., Zcash, Monero) were proposed focusing on this problem. However, most existing solutions either aim for the unspent transaction output (UTXO) model, or fail to provide full privacy protection for the account-based model with efficient performance. In this paper, we put forward LedgerMaze , an efficient privacy-preserving non-interactive zero-knowledge (NIZK) scheme over account-model blockchain. We design a novel scheme called cheque mechanism to cut the link between the sender/receiver relationship. Namely, a sender transfers money to a receiver&#39;s cheque, then the receiver can retrieve the cheque among a set of cheques for obfuscation without revealing the original one. We construct several efficient NIZK proofs for initializing the mechanism. Moreover, we further analyze the security properties of LedgerMaze . Experimental results show that LedgerMaze achieves comparable performance in communication and computation costs while retaining a full privacy guarantee, compared to previous similar constructions.},
  archive      = {J_TC},
  author       = {Zijian Bao and Debiao He and Wei Wei and Cong Peng and Xinyi Huang},
  doi          = {10.1109/TC.2023.3305748},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3489-3502},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LedgerMaze: An efficient privacy-preserving noninteractive zero-knowledge scheme over account-model blockchain},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating GNN training by adapting large graphs to
distributed heterogeneous architectures. <em>TC</em>, <em>72</em>(12),
3473–3488. (<a href="https://doi.org/10.1109/TC.2023.3305077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been successfully applied to many important application domains on graph data. As graphs become increasingly large, existing GNN training frameworks typically use mini-batch sampling during feature aggregation to lower resource burdens, which unfortunately suffer from long memory accessing latency and inefficient data transfer of vertex features from CPU to GPU. This paper proposes 2PGraph, a system that addresses these limitations of mini-batch sampling and feature aggregation and supports fast and efficient single-GPU and distributed GNN training. First, 2PGraph presents a locality awareness GNN-training scheduling method that schedules the vertices based on the locality of the graph topology, significantly accelerating the sampling and aggregation, improving the data locality of vertex access, and limiting the range of neighborhood expansion. Second, 2PGraph proposes a GNN-layer-aware feature caching method on available GPU resources with a hit rate up to 100 ${\bf\%}$ , which avoids redundant data transfer between CPU and GPU. Third, 2PGraph presents a self-dependence cluster-based graph partition method, achieving high sampling and cache efficiency for distributed environments. Experimental results on real-world graph datasets show that 2PGraph reduces memory access latency by up to 90 ${\boldsymbol{\%}}$ mini-batch sampling, and data transfer time by up to 99 ${\boldsymbol{\%}}$ . For distributed GNN training over an 8-GPU cluster, 2PGraph achieves up to 8.7 $\times$ performance speedup over state-of-the-art approaches.},
  archive      = {J_TC},
  author       = {Lizhi Zhang and Kai Lu and Zhiquan Lai and Yongquan Fu and Yu Tang and Dongsheng Li},
  doi          = {10.1109/TC.2023.3305077},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3473-3488},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating GNN training by adapting large graphs to distributed heterogeneous architectures},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving cluster utilization through adaptive resource
management for deep neural network and CPU jobs colocation. <em>TC</em>,
<em>72</em>(12), 3458–3472. (<a
href="https://doi.org/10.1109/TC.2023.3303988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep neural network (DNN) models are mainly trained using GPUs, many companies and research institutions build shared GPU clusters. These clusters host DNN training jobs, DNN inference jobs, and CPU jobs (jobs in traditional areas). DNN training jobs require GPU for main computation and CPU for auxiliary computation. Some DNN inference jobs could rely solely on CPU, while others must utilize both CPU and GPU. Our investigation demonstrates that the number of cores allocated to a training job significantly impacts its performance, and that DNN inference jobs can make use of the limited CPU cores on the GPU nodes. To accomplish this, we characterize representative deep learning models in terms of their CPU core requirements for their training jobs and inference jobs, and investigate their sensitivity to other CPU-side resource contention. Based on the characterization, we propose SODA, a scheduling system comprised of an adaptive CPU allocator, a multi-array job scheduler, a hardware-aware inference job placer, and a real-time contention eliminator. The experimental results indicate that SODA increases GPU utilization by an average of 19.9\%, while maintaining the quality of service target for all DNN inference jobs and the queuing performance of CPU jobs.},
  archive      = {J_TC},
  author       = {Han Zhao and Weihao Cui and Quan Chen and Jingwen Leng and Deze Zeng and Minyi Guo},
  doi          = {10.1109/TC.2023.3303988},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3458-3472},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improving cluster utilization through adaptive resource management for deep neural network and CPU jobs colocation},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Orchid: An online learning based resource partitioning
framework for job colocation with multiple objectives. <em>TC</em>,
<em>72</em>(12), 3443–3457. (<a
href="https://doi.org/10.1109/TC.2023.3303959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colocating multiple throughput-oriented jobs on the same server is a commonly used approach for improving system throughput in modern datacenters. The shared resources of the server are usually partitioned among the colocated jobs in order to prevent performance interference caused by resource contention. However, how to properly partition the shared resources among the colocated jobs is nontrival, because it usually has to trade off between two conflict objectives, as datacenter manager wants to maximize server throughput while job owners hope to experience a fair slowdown. Moreover, a desirable resource partitioning strategy should also be efficient, autonomous and adaptive. So far, this problem is not well-addressed in the literature due to several critical challenges. In this paper, we propose an online learning based framework, named Orchid , to address the multi-objective resource partitioning problem. Orchid leverages contextual multi-armed bandit (CMAB) to model the resource partitioning problem and uses a light-weight online learning algorithm to learn the optimal partitioning configuration according to some easy-to-collect runtime system status. Orchid has two distinguished properties compared to the existing solutions: first, it has the ability to trade off between the two objectives flexibly according to the aspiration of decision maker; second, it has the awareness about runtime system status, which can help to improve the efficiency of finding the optimal partitioning configuration and the adaptivity to dynamic environment changes. Moreover, Orchid does not require any prior knowledge of jobs, and incurs a very small computational overhead. Our evaluations show that Orchid achieves all the desired properties of a good resource partitioning strategy, which outperforms the state-of-the-art baselines with significant margins. Orchid is publicly available at https://github.com/OpenSourceOrchid/Orchid .},
  archive      = {J_TC},
  author       = {Ruobing Chen and Wangqi Peng and Yusen Li and Xiaoguang Liu and Gang Wang},
  doi          = {10.1109/TC.2023.3303959},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3443-3457},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Orchid: An online learning based resource partitioning framework for job colocation with multiple objectives},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VCMalloc: A virtually contiguous memory allocator.
<em>TC</em>, <em>72</em>(12), 3431–3442. (<a
href="https://doi.org/10.1109/TC.2023.3302731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents VCMalloc, a custom memory allocator based on a new dynamic memory management approach that keeps data allocated in a contiguous form in memory. It can preserve the virtual contiguity of data even after performing different memory operations like “Reallocation” and “Free” without consuming much system resources. A novel control plane that we call a “Memory Management System” is introduced with a set of algorithms for the physical-virtual memory mappings, assuring fast, efficient, and safe tracking of data and user pointers. The proposed allocator prioritizes virtual contiguity while it keeps the physical memory completely up to the operating system to manage. It was tested and compared with the current implementation of Malloc as well as a state-of-the-art memory allocator MIMalloc in the recent versions of the Microsoft Windows operating system. For the allocator itself, a considerable performance increase has been achieved in basic operations of up to 28\% increase in allocation speed and 26\% in reallocation speed. The data structures allocated by VCMalloc have shown remarkable performance increases of up to 31\% faster for matrix multiplication. Furthermore, VCMalloc has also demonstrated its superiority in real-world SPEC CPU 2017 benchmark applications.},
  archive      = {J_TC},
  author       = {Yacine Hadjadj and Chakib Mustapha Anouar Zouaoui and Nasreddine Taleb and Sarah Mazari and Mohamed El Bahri and Miloud Chikr El Mezouar},
  doi          = {10.1109/TC.2023.3302731},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3431-3442},
  shortjournal = {IEEE Trans. Comput.},
  title        = {VCMalloc: A virtually contiguous memory allocator},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An area-efficient in-memory implementation method of
arbitrary boolean function based on SRAM array. <em>TC</em>,
<em>72</em>(12), 3416–3430. (<a
href="https://doi.org/10.1109/TC.2023.3301156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing is an emerging computing paradigm to breakthrough the von-Neumann bottleneck. The SRAM based in-memory computing (SRAM-IMC) attracts great concerns from industries and academia, because the SRAM is technology compatible with the widely-used MOS devices. The digital SRAM-IMC scheme has advantages on stability and accuracy of computing results, compared with the analog SRAM-IMC schemes. However, few logic operations can be implemented by the current digital SRAM-IMC architectures. Designers have to insert some special logic modules to facilitate the complex computation. To address this issue, this work proposes an area-efficient implementation method of arbitrary Boolean function in SRAM array. Firstly, a two-input SRAM LUT is designed to realize the arbitrary two-input Boolean functions. Then, the logic merging and the spatial merging techniques are proposed to reduce the area consumption of the SRAM-IMC scheme. Finally, the SOP-based SRAM-IMC architecture is proposed, and the merged SOPs are mapped into and computed in it. The evaluation results on LGsynth’91, IWLS’93 and EPFL benchmarks show that, the area of the synthesis results based on the ABC tool is 3.69, 5.72 and 1.86 times of the circuit area from the proposed SRAM-IMC scheme in average respectively. Furthermore, the circuit area from the original SOP-based SRAM-IMC scheme is 2.07, 1.99 and 1.86 times in average of the circuit area from the proposed SRAM-IMC scheme respectively. The performance evaluation results show that the cycle consumption of the proposed SRAM-IMC scheme is independent to the scale of the input Boolean functions.},
  archive      = {J_TC},
  author       = {Sunrui Zhang and Xiaole Cui and Feng Wei and Xiaoxin Cui},
  doi          = {10.1109/TC.2023.3301156},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3416-3430},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An area-efficient in-memory implementation method of arbitrary boolean function based on SRAM array},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An edge-side real-time video analytics system with dual
computing resource control. <em>TC</em>, <em>72</em>(12), 3399–3415. (<a
href="https://doi.org/10.1109/TC.2023.3301136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video analytics systems conduct video preprocessing to filter out unnecessary frames and model inference using appropriately selected neural networks for high analytics speed. Video preprocessing is instruction-intensive computing (IIC) executed by CPU, and model inference is data-intensive computing (DIC) executed by GPU. In this paper, we show the analytics accuracy of existing systems can largely vary in fields, caused by the dynamic IIC and DIC workloads of different contents in applications. Unfortunately, cameras have fixed CPU/GPU resources and cannot effectively adapt to workload dynamics. We develop Gemini, a new edge-side real-time video analytics system enhanced by a dual-image FPGA. We take the advantage of negligible image switching time of dual-image FPGAs, pre-configure one CPU image and one GPU image and elastically multiplex the dual CPU-GPU resources in time dimension. Gemini requires both hardware and software revisions. In hardware, we overcome challenges of hardware-dependent application development, low communication efficiency between the microprocessor and FPGA, and high programming complexity by hardware abstraction, asynchronous data transfer mechanism and stub-skeleton middleware. In software, we overcome the challenge of adapting to the dynamic workloads by a bandit learning approach. We implement Gemini and show that Gemini can improve the analytics accuracy to 90.35\%.},
  archive      = {J_TC},
  author       = {Chuang Hu and Rui Lu and Qianlong Sang and Huanghuang Liang and Dan Wang and Dazhao Cheng and Jin Zhang and Qing Li and JunKun Peng},
  doi          = {10.1109/TC.2023.3301136},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3399-3415},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An edge-side real-time video analytics system with dual computing resource control},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling fine-grained spatial multitasking on systolic-array
NPUs using dataflow mirroring. <em>TC</em>, <em>72</em>(12), 3383–3398.
(<a href="https://doi.org/10.1109/TC.2023.3299030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Processing Units (NPUs) frequently suffer from low hardware utilization as the efficiency of their systolic arrays heavily depends on the characteristics of a deep neural network (DNN). Spatial multitasking is a promising solution to overcome the low NPU hardware utilization; however, the state-of-the-art spatial-multitasking NPU architecture achieves sub-optimal performance due to its coarse-grained systolic-array distribution and incurs significant implementation costs. In this paper, we propose dataflow-mirroring NPU (DM-NPU) , a novel spatial-multitasking NPU architecture supporting fine-grained systolic-array distribution. The key idea of DM-NPU is to reverse the dataflows of co-located DNNs in horizontal and/or vertical directions. DM-NPU can place allocation boundaries between any adjacent processing elements of a systolic array, both horizontally and vertically. We then propose DM-Perf , an accurate systolic-array NPU performance model, to maximize the spatial-multitasking performance of DM-NPU. Utilizing the existing performance models achieves sub-optimal performance as they cannot accurately capture the resource contention caused by spatial multitasking. DM-Perf, on the other hand, exploits the per-layer performance profiles of a DNN to accurately capture the resource contention. Our evaluation using MLPerf DNNs shows that DM-NPU and DM-Perf can greatly improve the performance by up to 35.1\% over the state-of-the-art NPU architecture and performance model.},
  archive      = {J_TC},
  author       = {Jinwoo Choi and Yeonan Ha and Jounghoo Lee and Sangsu Lee and Jinho Lee and Hanhwi Jang and Youngsok Kim},
  doi          = {10.1109/TC.2023.3299030},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3383-3398},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling fine-grained spatial multitasking on systolic-array NPUs using dataflow mirroring},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LAMP: Improving compression ratio for AMR applications via
level associated mapping-based preconditioning. <em>TC</em>,
<em>72</em>(12), 3370–3382. (<a
href="https://doi.org/10.1109/TC.2023.3297442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data compression can efficiently reduce the memory and persistence storage cost, which is highly desirable in modern computing systems, such as enterprise, cloud, and High-Performance Computing (HPC) environments. However, the main challenges of existing data compressors are the insufficient compression ratio and low throughput. This paper focuses on improving the compression ratio of state-of-the-art lossy compression algorithms from the view of applications. Besides, we also use the characteristics of the applications to reduce the runtime overhead. To this end, we explore the idea with Adaptive Mesh Refinement (AMR), which is widely adopted as a computational technique to reduce the amount of computation and memory required in scientific simulations. We propose Level Associated Mapping-based Preconditioning (LAMP) to improve the storage efficiency of AMR applications. The main idea is twofold. First, we utilize the high similarities among the adjacent AMR levels to precondition the data prior to compression. Second, AMR has a unique characteristic of grid structures. We utilize grid structures to rebuild a level associated mapping table, which significantly reduces the runtime overhead of LAMP. Thanks to the optimization techniques of General Matrix Multiplication (GEMM), we further accelerate the process of rebuilding AMR hierarchy for LAMP. Besides, we also block multiple adjacent coordinates within a box and further improve cache locality. The experimental results show that the compression ratios of LAMP are improved up to 63.8\% compared to directly compressing the data.},
  archive      = {J_TC},
  author       = {Yida Li and Huizhang Luo and Fenfang Li and Junqi Wang and Kenli Li},
  doi          = {10.1109/TC.2023.3297442},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3370-3382},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LAMP: Improving compression ratio for AMR applications via level associated mapping-based preconditioning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ParBFT: An optimized byzantine consensus parallelism scheme.
<em>TC</em>, <em>72</em>(12), 3354–3369. (<a
href="https://doi.org/10.1109/TC.2023.3296916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Byzantine fault-tolerance (BFT) consensus is a fundamental building block of distributed systems such as blockchains. However, implementations based on classic PBFT and most linear PBFT-variants still suffer from message communication complexity, restricting the scalability and performance of BFT algorithms when serving large-scale systems with growing numbers of peers. To tackle the scalability and performance challenges, we propose ParBFT , a new Byzantine consensus parallelism scheme combining classic BFT protocols and a novel Bilevel Mixed-Integer Linear Programming (BL-MILP)-based optimisation model. The core aim of ParBFT is to improve scalability via parallel consensus while providing enhanced safety (i.e. ensuring consistent total order across all correct replicas). Another core novelty is the integration of the BL-MILP model into ParBFT. The BL-MILP allows us to compute optimal numerical decisions for parallel committees (i.e. the optimal number of committees and peer allocation for each committee) and improve consensus performance while ensuring security. Finally, we test the performance of the proposed ParBFT on Microsoft Azure Cloud systems with 20 to 300 peers and find that ParBFT can achieve significant improvement compared to the state-of-the-art protocols.},
  archive      = {J_TC},
  author       = {Xiao Chen and Btissam Er-Rahmadi and Tiejun Ma and Jane Hillston},
  doi          = {10.1109/TC.2023.3296916},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3354-3369},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ParBFT: An optimized byzantine consensus parallelism scheme},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HPKA: A high-performance CRYSTALS-kyber accelerator
exploring efficient pipelining. <em>TC</em>, <em>72</em>(12), 3340–3353.
(<a href="https://doi.org/10.1109/TC.2023.3296899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CRYSTALS-Kyber (Kyber) was recently chosen as the first quantum resistant Key Encapsulation Mechanism (KEM) scheme for standardisation, after three rounds of the National Institute of Standards and Technology (NIST) initiated PQC competition which begin in 2016 and search of the best quantum resistant KEMs and digital signatures. Kyber is based on the Module-Learning with Errors (M-LWE) class of Lattice-based Cryptography, that is known to manifest efficiently on FPGAs. This work explores several architectural optimizations and proposes a high-performance and area-time (AT) product efficient hardware accelerator for Kyber. The proposed architectural optimizations include inter-module and intra-module pipelining, that are designed and balanced via FIFO based buffering to ensure maximum parallelisation. The implementation results show that compared to state-of-the-art designs, the proposed architecture delivers 25–51\% speedups for Kyber&#39;s three different security levels on Artix-7 and Zynq UltraScale+ devices, and a 50–75\% reduction in DSPs at comparable security level. Consequently, the proposed design achieve higher AT product efficiencies of 19–33\%.},
  archive      = {J_TC},
  author       = {Ziying Ni and Ayesha Khalid and Dur-e-Shahwar Kundi and Máire O’Neill and Weiqiang Liu},
  doi          = {10.1109/TC.2023.3296899},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {3340-3353},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HPKA: A high-performance CRYSTALS-kyber accelerator exploring efficient pipelining},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coupled incomplete cholesky and jacobi preconditioned
conjugate gradient on the new generation of sunway many-core
architecture. <em>TC</em>, <em>72</em>(11), 3326–3339. (<a
href="https://doi.org/10.1109/TC.2023.3296884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Preconditioned Conjugate Gradient method is one of the most important solvers in linear algebra system, and is widely used in scientific and engineering computing applications. Based on the Sunway heterogeneous many-core architecture, we propose a Coupled Incomplete Cholesky and Jacobi preconditioner (CICJ). The preconditioner applies a block Jacobi method to the matrix inversion in preconditioning process, localizes matrix inversions and completely eliminates the data correlation on slave cores. It strikes a better trade-off between convergence and parallelism than other preconditioners on the Sunway heterogeneous many-core architecture. Besides, a two-level software-controlled cache is designed for sparse matrix-vector multiplication operations, which makes full use of the Sunway heterogeneous many-core architecture. We apply our CICJ method on Intel, GPU, and Sunway, and the results show great generality on all three architectures. We also conduct experiments on the underwater submarine models using the open-source framework OpenFOAM. The results show that when the matrix column size is 0.82 billion and the number of non-zero values is 59 billion, our method accelerates the whole algorithm by 8.42 times compared with the diagonal incomplete Cholesky preconditioner (DIC) and 6.5 times compared with the geometric algebraic multi-grid preconditioner (GAMG) using 133,120 processors on the Sunway system.},
  archive      = {J_TC},
  author       = {Yuejin Ye and Heng Guo and Bingzhuo Wang and Pengxiao Wang and Dexun Chen and Fang Li},
  doi          = {10.1109/TC.2023.3296884},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3326-3339},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Coupled incomplete cholesky and jacobi preconditioned conjugate gradient on the new generation of sunway many-core architecture},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight blockchain-empowered secure and efficient
federated edge learning. <em>TC</em>, <em>72</em>(11), 3314–3325. (<a
href="https://doi.org/10.1109/TC.2023.3293731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as a privacy-preserving distributed Machine Learning paradigm, which collaboratively trains a shared global model across a number of end devices (clients) without exposing their raw data. However, FL typically assumes that all clients are benign and trust the coordinating central server, which is unrealistic for many real-world scenarios. In practice, clients can harm the FL process by sharing poisonous model updates while the server could malfunction or misbehave. Moreover, the deployment of FL for real-world applications is hindered by the high communication overhead between the server and clients that are often at the network edge with limited bandwidth. To address these key challenges, we propose a lightweight Blockchain-Empowered secure and efficient Federated Learning (BEFL) system. BEFL is built by integrating a communication-efficient and mutual-information guarded training scheme, a cost-effective Verifiable Random Function (VRF)-based consensus mechanism, and Inter-Planetary File System (IPFS)-enabled scalable blockchain architecture. Extensive simulation experiments using two benchmark FL datasets demonstrate that BEFL is resistant against byzantine clients launching data poisoning and model poisoning attacks, fault-tolerant against colluded malicious blockchain nodes, scalable to a large number of blockchain nodes, and communication-efficient at the network edge.},
  archive      = {J_TC},
  author       = {Rui Jin and Jia Hu and Geyong Min and Jed Mills},
  doi          = {10.1109/TC.2023.3293731},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3314-3325},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lightweight blockchain-empowered secure and efficient federated edge learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Balancing static islands in dynamically scheduled circuits
using continuous petri nets. <em>TC</em>, <em>72</em>(11), 3300–3313.
(<a href="https://doi.org/10.1109/TC.2023.3292590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-level synthesis (HLS) tools automatically transform a high-level program, for example in C/C++, into a low-level hardware description. A key challenge in HLS is scheduling, i.e. determining the start time of all the operations in the untimed program. A major shortcoming of existing approaches to scheduling—whether they are static (start times determined at compile-time), dynamic (start times determined at run-time), or a hybrid of both—is that the static analysis cannot efficiently explore the run-time hardware behaviours. Existing approaches either assume the timing behaviour in extreme cases, which can cause sub-optimal performance or larger area, or use simulation-based approaches, which take a long time to explore enough program traces. In this article, we propose an efficient approach using probabilistic analysis for HLS tools to efficiently explore the timing behaviour of scheduled hardware. We capture the performance of the hardware using Timed Continous Petri nets with immediate transitions, allowing us to leverage efficient Petri net analysis tools for making HLS decisions. We demonstrate the utility of our approach by using it to automatically estimate the hardware throughput for balancing the throughput for statically scheduled components (also known as static islands) computing in a dynamically scheduled circuit. Over a set of benchmarks, we show that our approach on average incurs a 2\% overhead in area-delay product compared to optimal designs by exhaustive search.},
  archive      = {J_TC},
  author       = {Jianyi Cheng and Estibaliz Fraca and John Wickerson and George A. Constantinides},
  doi          = {10.1109/TC.2023.3292590},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3300-3313},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Balancing static islands in dynamically scheduled circuits using continuous petri nets},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adversarial robust behavior sequence anomaly detection
approach based on critical behavior unit learning. <em>TC</em>,
<em>72</em>(11), 3286–3299. (<a
href="https://doi.org/10.1109/TC.2023.3292001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential deep learning models (e.g., RNN and LSTM) can learn the sequence features of software behaviors, such as API or syscall sequences. However, recent studies have shown that these deep learning-based approaches are vulnerable to adversarial samples. Attackers can use adversarial samples to change the sequential characteristics of behavior sequences and mislead malware classifiers. In this paper, an adversarial robustness anomaly detection method based on the analysis of behavior units is proposed to overcome this problem. We extract related behaviors that usually perform a behavior intention as a behavior unit, which contains the representative semantic information of local behaviors and can be used to improve the robustness of behavior analysis. By learning the overall semantics of each behavior unit and the contextual relationships among behavior units based on a multilevel deep learning model, our approach can mitigate perturbation attacks that target local and large-scale behaviors. In addition, our approach can be applied to both low-level and high-level behavior logs (e.g., API and syscall logs). The experimental results show that our approach outperforms all the compared methods, which indicates that our approach has better performance against obfuscation attacks.},
  archive      = {J_TC},
  author       = {Dongyang Zhan and Kai Tan and Lin Ye and Xiangzhan Yu and Hongli Zhang and Zheng He},
  doi          = {10.1109/TC.2023.3292001},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3286-3299},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An adversarial robust behavior sequence anomaly detection approach based on critical behavior unit learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Security relevant methods of android’s API classification: A
machine learning empirical evaluation. <em>TC</em>, <em>72</em>(11),
3273–3285. (<a href="https://doi.org/10.1109/TC.2023.3291998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Android operating system provides functions and methods to handle sensitive data to secure users’ data. The Android security literature extracts binary features from a method and classifies the method into one of the Security Relevant Method&#39;s classes, adding information about how the method handles sensitive data. However, the usage of binary features hinders the performance of some classifiers due to the high collision rate between instances. Although previous works have explored Security Relevant Method classification, an extensive study of machine learning algorithms over this problem has not been conceived. This work fills this gap, analyzing Monolithic classifiers, Multiple Classifier Systems, and Embedding algorithms to transform binary features into real-valued features, aiming to facilitate the classifier&#39;s work by minimizing the ambiguity promoted by the collision. Our analyzes show that META-DES, using a pool of Decision Trees trained with the Random Forest algorithm, statistically has the best results. We also find that, in general, distance-based classifiers have a disadvantage in binary features. Moreover, embedding techniques such as deep metric learning with triplet loss can reduce geometrical instance ambiguity, improving the performance of the weakest learning algorithms. However, its usage was detrimental to the performance of more robust techniques, such as dynamic ensemble models better suited for handling difficult cases. The dataset and code used for the experiments are available in the following repository: https://github.com/walbermr/android-srm-ml-evaluation .},
  archive      = {J_TC},
  author       = {Walber M. Rodrigues and Felipe N. Walmsley and George D. C. Cavalcanti and Rafael M. O. Cruz},
  doi          = {10.1109/TC.2023.3291998},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3273-3285},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Security relevant methods of android&#39;s API classification: A machine learning empirical evaluation},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards optimal application offloading in heterogeneous
edge-cloud computing. <em>TC</em>, <em>72</em>(11), 3259–3272. (<a
href="https://doi.org/10.1109/TC.2023.3290494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application offloading plays a crucial role in application deployment in edge-cloud computing. However, finding the optimal solution for application offloading is challenging due to the heterogeneous resources, computing dependency of tasks, and complex network. Existing research on application offloading problems often assumes that the communication delay between the edge and the cloud (inter-side) is symmetrical or that within the cloud or edge (intra-side) can be omitted. However, this assumption is not practical considering the distinct features of the clouds and the edge clusters. Therefore, we study application offloading in the heterogeneous edge-cloud environment by considering both intra-side communication delay between tasks assigned to the same side and asymmetry inter-side communication delay between edge and cloud sides. We first focus on the specific circumstances with a boundary condition that lead to an optimal offloading solution in a heterogeneous edge-cloud environment. Then we study the general case by designing an iterative algorithm with maximum gain technique to solve it. Furthermore, considering various bandwidths within each side and resource capacities of physical nodes, we develop two efficient algorithms by combining minimum cut and maximum gain approaches. Both simulations and real trace-based evaluations are conducted to validate that the proposed algorithms outperform existing solutions.},
  archive      = {J_TC},
  author       = {Tingxiang Ji and Xili Wan and Xinjie Guan and Aichun Zhu and Feng Ye},
  doi          = {10.1109/TC.2023.3290494},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3259-3272},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards optimal application offloading in heterogeneous edge-cloud computing},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Embedding hamiltonian paths in <span
class="math inline"><em>k</em></span>-ary <span
class="math inline"><em>n</em></span>-cubes with exponentially-many
faulty edges. <em>TC</em>, <em>72</em>(11), 3245–3258. (<a
href="https://doi.org/10.1109/TC.2023.3288766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The $k$ -ary $n$ -cube $Q_{n}^{k}$ is one of the most popular interconnection networks engaged as the underlying topology of data center networks, on-chip networks, and parallel and distributed systems. Due to the increasing probability of faulty edges in large-scale networks and extensive applications of the Hamiltonian path, it becomes more and more critical to investigate the fault tolerability of interconnection networks when embedding the Hamiltonian path. However, since the existing edge fault models in the current literature only focus on the entire status of faulty edges while ignoring the important information in the edge dimensions, their fault tolerability is narrowed to a minimal scope. This article first proposes the concept of the partitioned fault model to achieve an exponential scale of fault tolerance. Based on this model, we put forward two novel indicators for the bipartite networks (including $Q^{k}_{n}$ with even $k$ ), named partition-edge fault-tolerant Hamiltonian laceability and partition-edge fault-tolerant hyper-Hamiltonian laceability. Then, we exploit these metrics to explore the existence of Hamiltonian paths and unpaired 2-disjoint path cover in $k$ -ary $n$ -cubes with large-scale faulty edges. Moreover, we prove that all these results are optimal in the sense that the number of edge faults tolerated has attended to the best upper bound. Our approach is the first time that can still embed a Hamiltonian path and an unpaired 2-disjoint path cover into the $k$ -ary $n$ -cube even if the faulty edges grow exponentially.},
  archive      = {J_TC},
  author       = {Hongbin Zhuang and Xiao-Yan Li and Jou-Ming Chang and Cheng-Kuan Lin and Ximeng Liu},
  doi          = {10.1109/TC.2023.3288766},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3245-3258},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Embedding hamiltonian paths in $k$-ary $n$-cubes with exponentially-many faulty edges},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-channel: Scalable off-chain channels supporting fair
and atomic cross-chain operations. <em>TC</em>, <em>72</em>(11),
3231–3244. (<a href="https://doi.org/10.1109/TC.2023.3288765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-chain technology facilitates the interoperability among isolated blockchains on which users can freely communicate and transfer values. Existing cross-chain protocols suffer from the scalability problem when processing on-chain transactions. Off-chain channel, as a promising blockchain scaling technique, can enable micro-payment transactions without involving on-chain transaction settlement. However, existing channel schemes can only be applied to operations within a single blockchain, failing to support cross-chain services. Therefore in this paper, we propose $\mathsf {Cross}$ - $\mathsf {Channel}$ , the first off-chain channel to support cross-chain services. We introduce a novel hierarchical channel structure with a hierarchical interaction protocol, a new hierarchical settlement protocol, and a smart general fair exchange protocol, to ensure scalability, fairness, and atomicity of cross-chain interactions. Besides, $\mathsf {Cross}$ - $\mathsf {Channel}$ provides strong security and practicality by avoiding high latency in asynchronous networks.Through a 50-instance deployment of $\mathsf {Cross}$ - $\mathsf {Channel}$ on AliCloud, we demonstrate that $\mathsf {Cross}$ - $\mathsf {Channel}$ is well-suited for processing cross-chain transactions in high-frequency and large-scale, and brings a significantly enhanced throughput with a small amount of gas and delay overhead.},
  archive      = {J_TC},
  author       = {Yihao Guo and Minghui Xu and Dongxiao Yu and Yong Yu and Rajiv Ranjan and Xiuzhen Cheng},
  doi          = {10.1109/TC.2023.3288765},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3231-3244},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Cross-channel: Scalable off-chain channels supporting fair and atomic cross-chain operations},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling high-efficient ReRAM-based CNN training via
exploiting crossbar-level insignificant writing elimination.
<em>TC</em>, <em>72</em>(11), 3218–3230. (<a
href="https://doi.org/10.1109/TC.2023.3288763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been widely adopted in many deep learning applications. However, training a deep CNN requests intensive data transfer, which is both time and energy consuming. Using resistive random-access memory (ReRAM) to process data locally in memory is an emerging solution to eliminate the massive data movement. However, training cannot be efficiently supported with current ReRAM-based PIM accelerators because of the frequent and high-cost ReRAM writing operations from the delay, energy, and ReRAM lifetime perspectives. In this paper, we observe that activation induced and weight updating induced writing operations dominate the training energy on ReRAM-based accelerators. We then exploit and leverage a new angle in intermediate data (e.g., activations and errors) sparsity that fits the unique computation pattern in ReRAM crossbars to effectively eliminate the insignificant ReRAM writings, thus, enabling highly efficient CNN training without hurting the training accuracy. The experiment results show our proposed scheme achieves averagely $4.97\times$ ( $19.23\times$ ) energy saving and $1.38\times$ ( $30.08\times$ ) speedup compared to the state-of-the-art ReRAM-based accelerator (GPU). Our scheme also achieves $4.55\times$ lifetime enhancement compared to the state-of-the-art ReRAM accelerator.},
  archive      = {J_TC},
  author       = {Lening Wang and Qiyu Wan and Peixun Ma and Jing Wang and Minsong Chen and Shuaiwen Leon Song and Xin Fu},
  doi          = {10.1109/TC.2023.3288763},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3218-3230},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling high-efficient ReRAM-based CNN training via exploiting crossbar-level insignificant writing elimination},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A high-coverage and efficient instruction-level testing
approach for x86 processors. <em>TC</em>, <em>72</em>(11), 3203–3217.
(<a href="https://doi.org/10.1109/TC.2023.3288762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The processors have long been treated as trusted black boxes for running software. However, processors may have undocumented instructions and instruction flaws, which increase the attack surface of the computing system. Hardware-related attack surfaces can bypass malware detection tools, resulting in undefined system behavior, instability, and insecurity. Unfortunately, the existing testing methods for undocumented instructions and instruction flaws have issues of insufficient test coverage and low test efficiency. We proposed an approach Skipscan to address these issues, which tests both the legal instructions and the reserved instructions. For the first time, to improve the test coverage, Skipscan leverages an optimized combination algorithm to generate instruction prefix combinations, which covers the entire types of legal prefix combinations. To improve the test efficiency, Skipscan skips a considerable number of redundant legal instructions by leveraging the minimal test set of immediate and displacement operands. We evaluated Skipscan on eight x86 processors from Intel and AMD. The number of legal instructions and reserved instructions tested by Skipscan are 121.4 and 259.55 times that of Sandsifter on average, respectively. The test efficiency of Skipscan is on average 4 times that of Sandsifter. The ratio of legal instructions is reduced from 78.2\% to 20.1\% on average. Furthermore, we found more undocumented instructions on x86 processors and instruction flaws in x86 disassemblers.},
  archive      = {J_TC},
  author       = {Guang Wang and Ziyuan Zhu and Xu Cheng and Dan Meng},
  doi          = {10.1109/TC.2023.3288762},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3203-3217},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A high-coverage and efficient instruction-level testing approach for x86 processors},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FileDAG: A multi-version decentralized storage network built
on DAG-based blockchain. <em>TC</em>, <em>72</em>(11), 3191–3202. (<a
href="https://doi.org/10.1109/TC.2023.3288760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized Storage Networks (DSNs) can gather storage resources from mutually untrusted providers and form worldwide decentralized file systems. Compared to traditional storage networks, DSNs are built on top of blockchains, which can incentivize service providers and ensure strong security. However, existing DSNs face two major challenges. First, deduplication can only be achieved at the directory-level. Missing file-level deduplication leads to unavoidable extra storage and bandwidth cost. Second, current DSNs realize file indexing by storing extra metadata while blockchain ledgers are not fully exploited. To overcome these problems, we propose FileDAG, a DSN built on DAG-based blockchain to support file-level deduplication in storing multi-versioned files. When updating files, we adopt an increment generation method to calculate and store only the increments instead of the entire updated files. Besides, we introduce a two-layer DAG-based blockchain ledger, by which FileDAG can provide flexible and storage-saving file indexing by directly using the blockchain database without incurring extra storage overhead. We implement FileDAG and evaluate its performance with extensive experiments. The results demonstrate that FileDAG outperforms the state-of-the-art industrial DSNs considering storage cost and latency.},
  archive      = {J_TC},
  author       = {Hechuan Guo and Minghui Xu and Jiahao Zhang and Chunchi Liu and Dongxiao Yu and Schahram Dustdar and Xiuzhen Cheng},
  doi          = {10.1109/TC.2023.3288760},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3191-3202},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FileDAG: A multi-version decentralized storage network built on DAG-based blockchain},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HAOTuner: A hardware adaptive operator auto-tuner for
dynamic shape tensor compilers. <em>TC</em>, <em>72</em>(11), 3178–3190.
(<a href="https://doi.org/10.1109/TC.2023.3288758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning compilers with auto-tuners have the ability to generate high-performance programs, particularly tensor programs on accelerators. However, the performance of these tensor programs is shape-sensitive and hardware resource-sensitive. When the tensor shape is only known at runtime instead of compile time, auto-tuners must tune the tensor programs for every possible shape, leading to significant time and cost overhead. Additionally, if a tensor program tuned for one device is deployed on a different device, the performance may not be as optimal as before. To address these challenges, we propose HAOTuner, a hardware-adaptive deep learning operator auto-tuner specifically designed for dynamic shape tensors. We leverage the concept of micro-kernels as the unit of task allocation and have observed that the size of the micro-kernel greatly impacts performance. In HAOTuner, we determine the size of micro-kernels based not only on the tensor shapes but also on the available hardware resources. Specifically, we present an algorithm to select hardware-friendly micro-kernels as candidates, reducing the tuning time. We also design a cost model that is sensitive to hardware resources to support various hardware architectures. Furthermore, we provide a model transfer solution to enable fast deployment of the cost model on different hardware platforms. We evaluate HAOTuner on six different types of GPUs. The experiments demonstrate that HAOTuner surpasses the state-of-the-art dynamic shape tensor auto-tuner in terms of running time by an average of 26\% and tuning time by 25\%. Moreover, HAOTuner outperforms the state-of-the-art compiler with padding in terms of running time by an average of 39\% and tuning time by 6×.},
  archive      = {J_TC},
  author       = {Pengyu Mu and Yi Liu and Rui Wang and Guoxiang Liu and Zhonghao Sun and Hailong Yang and Zhongzhi Luan and Depei Qian},
  doi          = {10.1109/TC.2023.3288758},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3178-3190},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HAOTuner: A hardware adaptive operator auto-tuner for dynamic shape tensor compilers},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SUGAR: Efficient subgraph-level training via resource-aware
graph partitioning. <em>TC</em>, <em>72</em>(11), 3167–3177. (<a
href="https://doi.org/10.1109/TC.2023.3288755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have demonstrated a great potential in a variety of graph-based applications, such as recommender systems, drug discovery, and object recognition. Nevertheless, resource-efficient GNN learning is a rarely explored topic despite its many benefits for edge computing and Internet of Things (IoT) applications. To improve this state of affairs, this work proposes efficient su b g raph-level tr a ining via r esource-aware graph partitioning (SUGAR). SUGAR first partitions the initial graph into a set of disjoint subgraphs and then performs local training at the subgraph-level We provide a theoretical analysis and conduct extensive experiments on five graph benchmarks to verify its efficacy in practice. Our results across five different hardware platforms demonstrate great runtime speedup and memory reduction of SUGAR on large-scale graphs. We believe SUGAR opens a new research direction towards developing GNN methods that are resource-efficient, hence suitable for IoT deployment.},
  archive      = {J_TC},
  author       = {Zihui Xue and Yuedong Yang and Radu Marculescu},
  doi          = {10.1109/TC.2023.3288755},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3167-3177},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SUGAR: Efficient subgraph-level training via resource-aware graph partitioning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flock: Towards multitasking virtual machines for
function-as-a-service. <em>TC</em>, <em>72</em>(11), 3153–3166. (<a
href="https://doi.org/10.1109/TC.2023.3288751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FaaS, or function as a service, promises unprecedented cost-efficiency and elasticity thanks to its on-demand and fine-grained execution nature. However, modern FaaS platforms mainly adopt virtual machines (VMs) or containers as a computing abstraction, which incurs costs like high startup latency, large memory footprint, and high communication overhead. Multi-tasking virtual machines (MVMs), which allow co-executing multiple functions in the same managed language runtime, are appealing for FaaS due to their lightweight nature. Unfortunately, existing MVMs are not designed for FaaS. The proposed abstraction of MVMs does not provide specialized support for fine-grained, latency-sensitive functions and their chain-like execution patterns. Meanwhile, the underlying runtime still contains many global modules and lacks essential support for function-level resource accounting and isolation. To this end, this work proposes Flock , a retrofitted MVM for FaaS execution, which provides FaaS-aware abstractions named funclets and enhanced runtime support for isolation. Flock is implemented atop the HotSpot JVM of OpenJDK 8. Performance evaluation shows that Flock results in up to three orders of magnitude performance improvement over state-of-the-art FaaS platforms like OpenWhisk while providing sufficient isolation support for FaaS functions.},
  archive      = {J_TC},
  author       = {Ziming Zhao and Mingyu Wu and Xujie Cao and Haibo Chen and Binyu Zang},
  doi          = {10.1109/TC.2023.3288751},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3153-3166},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Flock: Towards multitasking virtual machines for function-as-a-service},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MGen: A framework for energy-efficient in-ReRAM acceleration
of multi-task BERT. <em>TC</em>, <em>72</em>(11), 3140–3152. (<a
href="https://doi.org/10.1109/TC.2023.3288749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multiple transformer models, such as BERT, have been utilized together to support multiple natural language processing (NLP) tasks in a system, also known as multi-task BERT. Multi-task BERT with very high weight parameters increases the area requirement of a processing in resistive memory (ReRAM) architecture, and several works have attempted to address this model size issue. Despite the reduced parameters, the number of multi-task BERT computations remains the same, leading to massive energy consumption in ReRAM-based deep neural network (DNN) accelerators. Therefore, we suggest a framework for better energy efficiency during the ReRAM acceleration of multi-task BERT. First, we analyze the inherent redundancies of multi-task BERT and the computational properties of the ReRAM-based DNN accelerator, after which we propose what is termed the model generator, which produces optimal BERT models supporting multiple tasks. The model generator reduces multi-task BERT computations while maintaining the algorithmic performance. Furthermore, we present task scheduler, which adjusts the execution order of multiple tasks, to run the produced models efficiently. As a result, the proposed framework achieves maximally 4.4× higher energy efficiency over the baseline, and it can also be combined with the previous multi-task BERT works to achieve both a smaller area and higher energy efficiency.},
  archive      = {J_TC},
  author       = {Myeonggu Kang and Hyein Shin and Junkyum Kim and Lee-Sup Kim},
  doi          = {10.1109/TC.2023.3288749},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3140-3152},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MGen: A framework for energy-efficient in-ReRAM acceleration of multi-task BERT},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Holistic and opportunistic scheduling of background i/os in
flash-based SSDs. <em>TC</em>, <em>72</em>(11), 3127–3139. (<a
href="https://doi.org/10.1109/TC.2023.3288748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background (BG) tasks are maintained indispensably in multiple layers of storage systems, from applications to flash-based SSDs. They launch a large amount of I/Os, causing significant interference with foreground (FG) I/O performance. Our key insight is that, to mitigate such interference, holistic scheduling of system-wide, multi-source BG I/Os is required and can only be realized at the underlying SSD layer. Only the SSD has a global view of all FG and BG I/Os as well as direct information and control about flash storage resources. We are thus inspired to propose a novel I/O scheduling architecture, called HuFu . It provides a framework for host software to register BG tasks and offload their I/O scheduling into the SSD. Then, the SSD-internal I/O scheduler prioritizes FG I/O processing, while BG I/Os are scheduled opportunistically by utilizing flash parallelism and idleness. To verify HuFu , we perform case studies on RocksDB and compares it with several state-of-the-art host-side I/O scheduling schemes. Experimental results show that HuFu can significantly alleviate performance interference caused by BG I/Os and improve SSD bandwidth utilization, thus improving the FG throughput, average and tail latencies (e.g., by about 18\% in a write-heavy workload).},
  archive      = {J_TC},
  author       = {Yu Wang and You Zhou and Fei Wu and Yu Zhong and Jian Zhou and Zhonghai Lu and Shu Li and Zhengyong Wang and Changsheng Xie},
  doi          = {10.1109/TC.2023.3288748},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3127-3139},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Holistic and opportunistic scheduling of background I/Os in flash-based SSDs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trust-preserving mechanism for blockchain assisted mobile
crowdsensing. <em>TC</em>, <em>72</em>(11), 3113–3126. (<a
href="https://doi.org/10.1109/TC.2023.3287043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain is envisioned as one of the promising technologies to address trust concern brought by mobile crowdsensing (MCS), due to its auditability, immutability and decentralization. Nevertheless, blockchain cannot fundamentally guarantee that the valuable sensed data outside the chain can enter the chain, although data integrity and consistency can be ensured once it is confirmed inside the chain. In addition, simply applying blockchain in MCS while ignoring possible abnormal saboteurs hidden in numerous devices may mislead the normal operation of blockchain, resulting in untrustworthy interactions. Consequently, it is highly desirable to build a trust-preserving mechanism (TPM) to fully enjoy the benefits of using blockchain in MCS. To this end, we first resort to a probabilistic trust assessment inferred from the interaction outcomes in blockchain, to incentivize participants to maintain the trustworthiness of interactions. By inferring trust to aid decision-making, trust decision is further made, including leader election and transaction data generation, to filter untrusted nodes from participating in blockchain process. Finally, extensive simulations are conducted to validate the effectiveness and efficiency of TPM, and improve the performance in terms of contribution rate, consensus accuracy and system stability.},
  archive      = {J_TC},
  author       = {Long Zhang and Gang Feng and Shuang Qin and Xiaoqian Li and Yao Sun and Bin Cao},
  doi          = {10.1109/TC.2023.3287043},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3113-3126},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Trust-preserving mechanism for blockchain assisted mobile crowdsensing},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AChain: A SQL-empowered analytical blockchain as a database.
<em>TC</em>, <em>72</em>(11), 3099–3112. (<a
href="https://doi.org/10.1109/TC.2023.3287036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various multi-party cooperations data stored on blockchains (i.e., on-chain data) should be decentralized consistent, verifiable, traceable, and immutable. Online analytical processing (OLAP) services are critical requirements in these applications. However, OLAP performances of existing blockchain systems are much worse than those of relational databases due to the lack of SQL support. In this paper, we propose a novel SQL-empowered analytical blockchain framework, aChain. It fully provides SQL-based OLAP services, while keeping the secure characteristics. Specifically, aChain relationally reorganizes on-chain data to support full SQL executions. Then, a relational versioning scheme is designed to ensure the atomicity and consistency of transactions. Furthermore, SQL-based APIs are designed based on an execute-order-validate architecture. Finally, we demonstrate that the performance of aChain and MySQL (in both cluster and non-cluster models) is at the same level on a typical OLAP benchmark, TPC-H.},
  archive      = {J_TC},
  author       = {Yuchao Wang and Yanguo Peng and Ximeng Liu and Zuobin Ying and Jiangtao Cui and Dongyao Niu and Xiaofang Xia},
  doi          = {10.1109/TC.2023.3287036},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3099-3112},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AChain: A SQL-empowered analytical blockchain as a database},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving bloom filter-based keyword search over
large encrypted cloud data. <em>TC</em>, <em>72</em>(11), 3086–3098. (<a
href="https://doi.org/10.1109/TC.2023.3285103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve the search over encrypted data in cloud server, Searchable Encryption (SE) has attracted extensive attention from both academic and industrial fields. The existing Bloom filter-based SE schemes can achieve similarity search, but will generally incur high false positive rates, and even leak the privacy of values in Bloom filters (BF). To solve the above problems, we first propose a basic P rivacy-preserving B loom filter-based K eyword S earch scheme using the Circular Shift and Coalesce-Bloom Filter (CSC-BF) and Symmetric-key Hidden Vector Encryption (SHVE) technology (namely PBKS), which can achieve effective search while protecting the values in BFs. Then, we design a new index structure T-CSCBF utilizing the T win Bloom Filter (TBF) technology. Based on this, we propose an improved scheme PBKS+, which assigns a unique inclusion identifier to each position in each BF with privacy protection. Formal security analysis proves that our schemes are secure against Indistinguishability under Selective Chosen-Plaintext Attack (IND-SCPA), and extensive experiments using real-world datasets demonstrate that our schemes are feasible in practice.},
  archive      = {J_TC},
  author       = {Yanrong Liang and Jianfeng Ma and Yinbin Miao and Da Kuang and Xiangdong Meng and Robert H. Deng},
  doi          = {10.1109/TC.2023.3285103},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3086-3098},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Privacy-preserving bloom filter-based keyword search over large encrypted cloud data},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Silent data corruptions: Microarchitectural perspectives.
<em>TC</em>, <em>72</em>(11), 3072–3085. (<a
href="https://doi.org/10.1109/TC.2023.3285094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today more than ever before, academia, manufacturers, and hyperscalers acknowledge the major challenge of silent data corruptions (SDCs) and aim on solutions to minimize its impact by avoiding, detecting, and mitigating SDCs. Recent studies on large scale datacenters conducted by Meta and Google report an unexpected rate of silent data corruption incidents that are attributed to modern microprocessor generations. Despite the acknowledged severity of the phenomenon, particularly at the datacenter scale, there is no in-depth analysis of the microarchitectural locations in a complex microprocessor that are more likely to generate an SDC at the program outputs. In this paper, we present a detailed analysis of the faulty behavior of many critical microarchitectural structures of a modern out-of-order microprocessor generating silent data corruptions. Our analysis unveils several observations, including: (i) the magnitude of silent data corruptions attributed to different hardware structures, (ii) the instruction-related parameters that are more likely to result in a silent data corruption, (iii) the extent to which the operating system affects the silent data corruption occurrences, and (iv) the byte positions of a word which are more likely to result in silent data corruptions. Collectively, such findings can assist decisions for hardware and software schemes for the reduction of the likelihood of silent data corruptions generation.},
  archive      = {J_TC},
  author       = {George Papadimitriou and Dimitris Gizopoulos},
  doi          = {10.1109/TC.2023.3285094},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3072-3085},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Silent data corruptions: Microarchitectural perspectives},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-jitter chains of periodic LET tasks via algebraic
rings. <em>TC</em>, <em>72</em>(11), 3057–3071. (<a
href="https://doi.org/10.1109/TC.2023.3283707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In embedded computing domains, including the automotive industry, complex functionalities are split across multiple tasks that form task chains . These tasks are functionally dependent and communicate partial computations through shared memory slots based on the Logical Execution Time (LET) paradigm. This paper introduces a model that captures the behavior of a producer-consumer pair of tasks in a chain, characterizing the timing of reading and writing events. Using ring algebra, the combined behavior of the pair can be modeled as a single periodic task. The paper also presents a lightweight mechanism to eliminate jitter in an entire chain of any size, resulting in a single periodic LET task with zero jitter. All presented methods are available in a public repository.},
  archive      = {J_TC},
  author       = {Enrico Bini and Paolo Pazzaglia and Martina Maggio},
  doi          = {10.1109/TC.2023.3283707},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3057-3071},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Zero-jitter chains of periodic LET tasks via algebraic rings},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Saca-AVF: A quantitative approach to analyze the
architectural vulnerability factors of CNN accelerators. <em>TC</em>,
<em>72</em>(11), 3042–3056. (<a
href="https://doi.org/10.1109/TC.2023.3283685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNN) accelerators are widely used in artificial intelligence applications, such as image recognitions, due to their superior computing performance. However, as manufacturing technology scales down, the shrinking of chip size and the increasing of integration density make CNN accelerators vulnerable to soft errors, which are key factors affecting the reliability of integrated circuits. For emerging CNN applications where reliability is critical (such as self-driving cars), visible faults in applications’ outputs caused by soft errors may lead to catastrophic consequences. Therefore, it is important to consider reliability into CNN accelerators’ architecture design. Recent modeling and fault injection approaches analyze the reliability features of CNN models, but none evaluate the soft error reliability of CNN accelerators from architecture level. The Architectural Vulnerability Factor (AVF) indicates the probability that a soft error results in faulty computation outcomes. The Architecturally Correct Execution (ACE) method analyzes AVF of structures by identifying ACE bits and calculating their residency time. Based on the traditional ACE-based AVF analysis model, we propose a quantitative approach saca-AVF to analyze the AVF of systolic array based CNN accelerators that perform homogeneous matrix arithmetic operations instead of various kinds of instructions. We explore the reliability features of CNN accelerators under different design choices at different levels leveraging saca-AVF. We also observe several reliability characteristics of CNN accelerator architecture. We believe our proposed saca-AVF and the observations we made are able to guide designers to discover the reliability hotspots of CNN accelerators and build highly reliable CNN accelerators.},
  archive      = {J_TC},
  author       = {Jingweijia Tan and Liqi Ping and Qixiang Wang and Kaige Yan},
  doi          = {10.1109/TC.2023.3283685},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3042-3056},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Saca-AVF: A quantitative approach to analyze the architectural vulnerability factors of CNN accelerators},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reaping both latency and reliability benefits with elaborate
sanitization design for 3D TLC NAND flash. <em>TC</em>, <em>72</em>(11),
3029–3041. (<a href="https://doi.org/10.1109/TC.2023.3272280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rising security concern on modern storage systems, the concept of data sanitization has been widely investigated recently. Among the existing works targeting data sanitization, an overwriting-based approach, namely one-shot sanitization, is one of the most efficient sanitization approaches. Nonetheless, we find that the one-shot sanitization approach would fail to achieve precise data sanitization for 3D TLC NAND flash, because of incurring undesired data errors. That is, how to simultaneously realize precise sanitization and high security with decent latency and reliability on emerging storage devices remains unsolved. This work proposes an elaborate sanitization design that skillfully manipulates the threshold voltage ( $V_{t}$ ) distribution of sanitized pages. Not only does the proposed design achieve precise sanitization and high security, but it also enhances read performance and data reliability. Specifically, this work elaborately sanitizes data by merging specific $V_{t}$ distributions of the target physical page on 3D TLC NAND flash. Besides, the proposed approach further takes lateral charge migration into consideration to improve data reliability. We conduct a series of experiments to evaluate our proposed approach on real 3D TLC NAND flash. The experiment results demonstrate the proposed approach can achieve elaborate data sanitization under various scenarios and improve read performance by 29\%.},
  archive      = {J_TC},
  author       = {Wei-Chen Wang and Chien-Chung Ho and Yung-Chun Li and Liang-Chi Chen and Yu-Ming Chang},
  doi          = {10.1109/TC.2023.3272280},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {3029-3041},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reaping both latency and reliability benefits with elaborate sanitization design for 3D TLC NAND flash},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Limon: A scalable and stable key-value engine for fast NVMe
devices. <em>TC</em>, <em>72</em>(10), 3017–3028. (<a
href="https://doi.org/10.1109/TC.2023.3283674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern fast NVMe devices with high throughput and ultra-low latency have brought new opportunities for persistent key-value (KV) engines. In this paper, we propose Limon, a persistent KV engine to exploit the performance potentials of fast NVMe devices. Limon targets three practical design aspects that existing KV engines fail to consider simultaneously: functionality, scalability, and stability. Limon carefully redesigns the index structure, on-disk KV record layout, and I/O processing of a persistent KV engine for these aspects. Specifically, Limon (i) proposes a semi-shared global index to improve scalability and range queries. (ii) employs a fast slab-based record layout with light-weight defragmentation to enable stable performance, and (iii) uses efficient asynchronous per-core I/O processing with two optimizations: DMA-backed buffer pool and page deduplication, to further improve scalability. Our evaluations with the YCSB benchmark and one production workload show that Limon outperforms state-of-the-art persistent key-value engines (i.e., SpanDB, KVell, and uDepot) by up to 1.2x to 3.8x and has the best scalability. Moreover, Limon has stable and predictable performance due to its novel record layout strategy.},
  archive      = {J_TC},
  author       = {Baoyue Yan and Jinbin Zhu and Bo Jiang},
  doi          = {10.1109/TC.2023.3283674},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {3017-3028},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Limon: A scalable and stable key-value engine for fast NVMe devices},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel path progression DAG scheduling. <em>TC</em>,
<em>72</em>(10), 3002–3016. (<a
href="https://doi.org/10.1109/TC.2023.3280137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing performance needs of modern cyber-physical systems leads to multiprocessor architectures being increasingly utilized. To efficiently exploit their potential parallelism in hard real-time systems, appropriate task models and scheduling algorithms that allow to provide timing guarantees are required. Such scheduling algorithms and the corresponding worst-case response time analyses usually suffer from resource over-provisioning due to pessimistic analyses based on worst-case assumptions. Hence, scheduling algorithms and analyses with high resource efficiency are required. A prominent fine-grained parallel task model is the directed-acyclic-graph (DAG) task model that is composed of precedence constrained subjobs. This paper studies the hierarchical real-time scheduling problem of sporadic arbitrary-deadline DAG tasks. We propose a parallel path progression scheduling property that is implemented with only two distinct subtask priorities, which allows to quantify the parallel execution of a user chosen collection of complete paths in the response time analysis. This novel approach significantly improves the state-of-the-art response time analyses for parallel DAG tasks for highly parallel DAG structures and can provably exhaust large core numbers. Two hierarchical scheduling algorithms are designed based on this property, extending the parallel path progression properties and improve the response time analysis for sporadic arbitrary-deadline DAG task sets.},
  archive      = {J_TC},
  author       = {Niklas Ueter and Mario Günzel and Georg von der Brüggen and Jian-Jia Chen},
  doi          = {10.1109/TC.2023.3280137},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {3002-3016},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Parallel path progression DAG scheduling},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Radix-64 floating-point division and square root: Iterative
and pipelined units. <em>TC</em>, <em>72</em>(10), 2990–3001. (<a
href="https://doi.org/10.1109/TC.2023.3280136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digit-recurrence algorithms are widely used in actual microprocessors to compute floating-point division and square root. These iterative algorithms present a good trade-off in terms of performance, area and power. Traditionally, commercial processors have iterative division and square root units where the iteration logic is used over several cycles. The main drawbacks of these iterative units are long latency and low throughput due to the reuse of part of the logic over several cycles, and its hardware complexity with separated logic for division and square root. We present a radix-64 floating-point division and square root algorithm with a common iteration for division and square root and where, to have an affordable implementation, each radix-64 iteration is made of two simpler radix-8 iterations. The radix-64 algorithm allows to get low-latency operations, and the common division and square root radix-64 iteration results in some area reduction. The algorithm is mapped into two different microarchitectures: a low-latency and low area iterative unit, and a low-latency and high-throughput pipelined unit. In both units speculation between consecutive radix-8 iterations is used to reduce the timing.},
  archive      = {J_TC},
  author       = {Javier D. Bruguera},
  doi          = {10.1109/TC.2023.3280136},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2990-3001},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Radix-64 floating-point division and square root: Iterative and pipelined units},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model poisoning attack on neural network without reference
data. <em>TC</em>, <em>72</em>(10), 2978–2989. (<a
href="https://doi.org/10.1109/TC.2023.3280133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the substantial computational cost of neural network training, adopting third-party models has become increasingly popular. However, recent works demonstrate that third-party models can be poisoned. Nonetheless, most model poisoning attacks require reference data, e.g., training dataset or data belonging to the target label, making them difficult to launch in practice. In this paper, we propose a reference data independent model poisoning attack that can (1) directly search for sensitive features with respect to the target label, (2) quantify the positive and negative effects of the model parameters on sensitive features, and (3) accomplish the training of poisoned model by our parameter selective update strategy. The extensive evaluation on datasets with a few classes and numerous classes show that the attack is (I) effective: the trigger input can be labeled as a deliberate class by the poisoned model with high probability; (II) covert: the performance of the poisoned model is almost indistinguishable from the intact model on non-trigger inputs; and (III) straightforward: an adversary only needs a little background knowledge to launch the attack. Overall, the evaluation results show that our attack achieves 95\%, 100\%, 81\%, 96\%, and 96\% success rates on Cifar10, Cifar100, ISIC2018, FaceScrub, and ImageNet datasets, respectively.},
  archive      = {J_TC},
  author       = {Xianglong Zhang and Huanle Zhang and Guoming Zhang and Hong Li and Dongxiao Yu and Xiuzhen Cheng and Pengfei Hu},
  doi          = {10.1109/TC.2023.3280133},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2978-2989},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Model poisoning attack on neural network without reference data},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling efficient spatio-temporal GPU sharing for network
function virtualization. <em>TC</em>, <em>72</em>(10), 2963–2977. (<a
href="https://doi.org/10.1109/TC.2023.3278541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By leveraging standard IT virtualization technology and Commercial-Off-The-Shelf (COTS) servers, Network Function Virtualization (NFV) decouples network functions from proprietary hardware devices for flexible service provisioning. But the potential of NFV is significantly limited by its performance inefficiency. With the unparalleled advantages of multi-core parallelism and high memory bandwidth, Graphics Processing Units (GPUs) are regarded as a promising way to accelerate Virtualized Network Functions (VNF). However, the special architecture of GPU brings new challenges to task scheduling and resource allocation. To this end, we propose a G PU o riented s patio- t emporal sharing framework for NFV called Gost , aiming for GPU based VNF performance promotion. The execution order and GPU resource allocation (i.e., the number of threads) are considered in task scheduling to minimize the end-to-end latency for VNF flows. First, we formulate the task scheduling problem into a nonlinear programming form, and then transform it into an equivalent Integer Linear Programming (ILP) form. The problem is proved as NP-hard. We customize the classical list scheduling algorithm and propose a List Scheduling based Spatio-Temporal GPU sharing strategy (LSSTG), whose achievable worst-case performance is also formally analyzed. We practically implement Gost prototype, based on which extensive experiments verify the high performance efficiency of LSSTG compared to state-of-the-art in terms of latency and throughput.},
  archive      = {J_TC},
  author       = {Deze Zeng and Andong Zhu and Lin Gu and Peng Li and Quan Chen and Minyi Guo},
  doi          = {10.1109/TC.2023.3278541},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2963-2977},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling efficient spatio-temporal GPU sharing for network function virtualization},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PODTherm-GP: A physics-based data-driven approach for
effective architecture-level thermal simulation of multi-core CPUs.
<em>TC</em>, <em>72</em>(10), 2951–2962. (<a
href="https://doi.org/10.1109/TC.2023.3278535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A thermal simulation methodology derived from the proper orthogonal decomposition (POD) and the Galerkin projection (GP), hereafter referred to as PODTherm-GP, is evaluated in terms of its efficiency and accuracy in a multi-core CPU. The GP projects the heat transfer equation onto a mathematical space whose basis functions are generated from thermal data enabled by the POD learning algorithm. The thermal solution data are collected from FEniCS using the finite element method (FEM) accounting for appropriate parametric variations. The GP incorporates physical principles of heat transfer in the methodology to reach high accuracy and efficiency. The dynamic power map for the CPU in FEM thermal simulation is generated from gem5 and McPACT, together with the SPLASH-2 benchmarks as the simulation workload. It is shown that PODTherm-GP offers an accurate thermal prediction of the CPU with a resolution as fine as the FEM. It is also demonstrated that PODTherm-GP is capable of predicting the dynamic thermal profile of the chip with a good accuracy beyond the training conditions. Additionally, the approach offers a reduction in degrees of freedom by more than 5 orders of magnitude and a speedup of 4 orders, compared to the FEM.},
  archive      = {J_TC},
  author       = {Lin Jiang and Anthony Dowling and Ming-C. Cheng and Yu Liu},
  doi          = {10.1109/TC.2023.3278535},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2951-2962},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PODTherm-GP: A physics-based data-driven approach for effective architecture-level thermal simulation of multi-core CPUs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Waterwave: A GPU memory flow engine for concurrent DNN
training. <em>TC</em>, <em>72</em>(10), 2938–2950. (<a
href="https://doi.org/10.1109/TC.2023.3278530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training Deep Neural Networks (DNN) concurrently is becoming increasingly important for deep learning practitioners, e.g., hyperparameter optimization (HPO) and neural architecture search (NAS) . The GPU memory capacity is the impediment that prohibits multiple DNNs from being trained on the same GPU due to the large memory usage during training. In this paper, we propose Waterwave , a GPU memory flow engine for concurrent deep learning training. First, to address the memory explosion brought by the long time lag between memory allocation and deallocation time, we develop an allocator tailored for multi-streams. By making the allocator aware of the stream information, a prioritized allocation is conducted based on the chunk&#39;s synchronization attributes, allowing us to provide useable memory after scheduling rather than waiting it to be really released after GPU computation. Second, Waterwave partitions the compute graph to a set of continuous node groups and then performs finer-grained scheduling: NodeGroup pipeline execution , to guarantee a proper memory requests order. Waterwave can accomplish up to 96.8\% of the maximum batch size of solo training. Additionally, in scenarios with high memory demand, Waterwave can outperform existing spatial sharing and temporal sharing by up to 12x and 1.49x, respectively.},
  archive      = {J_TC},
  author       = {Xuanhua Shi and Xuan Peng and Ligang He and Yunfei Zhao and Hai Jin},
  doi          = {10.1109/TC.2023.3278530},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2938-2950},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Waterwave: A GPU memory flow engine for concurrent DNN training},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimizing staleness and communication overhead in
distributed SGD for collaborative filtering. <em>TC</em>,
<em>72</em>(10), 2925–2937. (<a
href="https://doi.org/10.1109/TC.2023.3275107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed asynchronous stochastic gradient descent (ASGD) algorithms that approximate low-rank matrix factorizations for collaborative filtering perform one or more synchronizations per epoch where staleness is reduced with more synchronizations. However, high number of synchronizations would prohibit the scalability of the algorithm. We propose a parallel ASGD algorithm, $\eta$ -PASGD, for efficiently handling $\eta$ synchronizations per epoch in a scalable fashion. The proposed algorithm puts an upper limit of $K$ on $\eta$ , for a $K$ -processor system, such that performing $\eta =K$ synchronizations per epoch would eliminate the staleness completely. The rating data used in collaborative filtering are usually represented as sparse matrices. The sparsity allows for reduction in the staleness and communication overhead combinatorially via intelligently distributing the data to processors. We analyze the staleness and the total volume incurred during an epoch of $\eta$ -PASGD. Following this analysis, we propose a hypergraph partitioning model to encapsulate reducing staleness and volume while minimizing the maximum number of synchronizations required for a stale-free SGD. This encapsulation is achieved with a novel cutsize metric that is realized via a new recursive-bipartitioning-based algorithm. Experiments on up to 512 processors show the importance of the proposed partitioning method in improving staleness, volume, RMSE and parallel runtime.},
  archive      = {J_TC},
  author       = {Nabil Abubaker and Orhun Caglayan and M. Ozan Karsavuran and Cevdet Aykanat},
  doi          = {10.1109/TC.2023.3275107},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2925-2937},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Minimizing staleness and communication overhead in distributed SGD for collaborative filtering},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel covert timing channel based on bitcoin messages.
<em>TC</em>, <em>72</em>(10), 2913–2924. (<a
href="https://doi.org/10.1109/TC.2023.3275096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covert channels serve the construction of cyberspace security. By realizing the secure transmission of data, it is widely used in political and financial fields. Blockchain covert channels have higher reliability and concealment compared to traditional network-based covert channels. However, existing blockchain covert storage channels need to create a large number of transactions to transmit covert information. Creating transactions requires a transation fee, which means that the implementation of blockchain covert storage channels requires a high cost. Besides, created transactions remain on-chain permanently, leading to the threat of covert information being detected. To overcome these limitations, we propose a blockchain covert timing channel framework. Specifically, we utilize inv and getdata messages in the Bitcoin transaction broadcast as carriers and propose three modulation modes to achieve covert channels without cost and leaving no trace. We evaluate the concealment of our modes by K-S, KLD tests, and machine learning approaches. Experimental results show the indistinguishability between traffic carrying covert information and normal traffic. Our channels promise a capacity of 2.4 bit/s.},
  archive      = {J_TC},
  author       = {Liehuang Zhu and Qi Liu and Zhuo Chen and Can Zhang and Feng Gao and Zhongliang Yang},
  doi          = {10.1109/TC.2023.3275096},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2913-2924},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A novel covert timing channel based on bitcoin messages},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A feedback-driven DNN inference acceleration system for
edge-assisted video analytics. <em>TC</em>, <em>72</em>(10), 2902–2912.
(<a href="https://doi.org/10.1109/TC.2023.3275094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proposal of edge computing, lots of intelligence applications have made significant progress. For enormous video analysis, how to further accelerate the process is still a major challenge. To overcome the challenge, researchers propose various video frame filtering systems to reduce the data transmission. In this work, we propose a Feedback-Driven DNN Inference Acceleration system (FDDIA). FDDIA is committed to further reducing the latency of DNN inference and the transmission according to the feedback information. Specifically, on the device side, FDDIA first uses the detection results of prior frames as feedback to determine the key candidate regions and uses the inter-frame difference to determine the new object regions. Those regions containing large objects are down-sampled to further reduce the frame. On the edge side, FDDIA first integrates all candidate regions into a smaller new image. Then it remaps the detection result for the new image back to the original frame and returns the result to the device. We evaluate FDDIA on different video benchmarks for three object detection tasks. The results show FDDIA improves the average end-to-end latency by 44\% and average bandwidth usage by 41\% than the existing advanced method while maintaining a high accuracy.},
  archive      = {J_TC},
  author       = {Xianwei Lv and Qianqian Wang and Chen Yu and Hai Jin},
  doi          = {10.1109/TC.2023.3275094},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2902-2912},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A feedback-driven DNN inference acceleration system for edge-assisted video analytics},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards data-independent knowledge transfer in
model-heterogeneous federated learning. <em>TC</em>, <em>72</em>(10),
2888–2901. (<a href="https://doi.org/10.1109/TC.2023.3272801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Distillation (FD) extends classic Federated Learning (FL) to a more general training framework that enables model-heterogeneous collaborative learning by Knowledge Distillation (KD) across multiple clients and the server. However, existing KD-based algorithms usually require a set of shared input samples for each client to produce soft-prediction for distillation. Worse still, such a manual selection is accompanied by careful deliberations or prior information on clients’ private data distribution, which is not in line with the privacy-preserving characteristic of classic FL. In this paper, we propose a novel training framework to achieve data-independent knowledge transfer by properly designing a distributed generative adversarial network (GAN) between the server and clients that can synthesize shared feature representations to facilitate the FD training. Specifically, we deploy a generator on the server and reuse each local model as a federated discriminator to form a lightweight efficient distributed GAN that can automatically synthesize simulated global feature representations for distillation. Moreover, since the synthesized feature representations are usually more faithful and homologous with global data distribution, faster and better training convergence can be obtained. Extensive experiments on different tasks and heterogeneous models demonstrate the effectiveness of the proposed framework on model accuracy and communication overhead.},
  archive      = {J_TC},
  author       = {Jie Zhang and Song Guo and Jingcai Guo and Deze Zeng and Jingren Zhou and Albert Y. Zomaya},
  doi          = {10.1109/TC.2023.3272801},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2888-2901},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards data-independent knowledge transfer in model-heterogeneous federated learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Run-time resource management in CMPs handling multiple aging
mechanisms. <em>TC</em>, <em>72</em>(10), 2872–2887. (<a
href="https://doi.org/10.1109/TC.2023.3272800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Run-time resource management is fundamental for efficient execution of workloads on Chip Multiprocessors. Application- and system-level requirements (e.g., on performance versus power versus lifetime reliability) are generally conflicting each other, and any decision on resource assignment, such as core allocation or frequency tuning, may positively affect some of them while penalizing some others. Resource assignment decisions can be perceived in few instants of time on performance and power consumption, but not on lifetime reliability. In fact, this latter changes very slowly based on the accumulation of effects of various decisions over a long time horizon. Moreover, aging mechanisms are various and have different causes; most of them, such as Electromigration (EM), are subject to temperature levels, while Thermal Cycling (TC) is caused mainly by temperature variations (both amplitude and frequency). Mitigating only EM may negatively affect TC and vice versa. We propose a resource orchestration strategy to balance the performance and power consumption constraints in the short-term and EM and TC aging in the long-term. Experimental results show that the proposed approach improves the average Mean Time To Failure at least by 17\% and 20\% w.r.t. EM and TC, respectively, while providing same performance level of the nominal counterpart and guaranteeing the power budget.},
  archive      = {J_TC},
  author       = {Hashem Haghbayan and Antonio Miele and Onur Mutlu and Juha Plosila},
  doi          = {10.1109/TC.2023.3272800},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2872-2887},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Run-time resource management in CMPs handling multiple aging mechanisms},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and robust KPI outlier detection for large-scale
datacenters. <em>TC</em>, <em>72</em>(10), 2858–2871. (<a
href="https://doi.org/10.1109/TC.2023.3272288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure the performance of large-scale datacenters, operators need to monitor up to tens of millions of various-type KPIs, e.g., CPU utilization, memory utilization. For each KPI, it is crucial but challenging to detect outliers that deviate from its historical patterns or the patterns of other KPIs in the same period. In this work, we propose OutSpot , an unsupervised outlier detection framework that integrates hierarchical agglomerative clustering (HAC) with conditional variational autoencoder (CVAE), which significantly improves computational efficiency and comprehensively learns the above two patterns. Additionally, two simple yet effective techniques, soft threshold and median filter, are applied to precisely determine outlier KPIs. Using two real-world datasets collected from the datacenters owned by a top-tier global short video service provider and a top-tier domestic operator,respectively. It demonstrates that OutSpot achieves the best F1 score of 0.95 and 0.91, AUC of 0.99 and 0.99 on the two datasets, significantly outperforming seven baseline outlier detection methods.},
  archive      = {J_TC},
  author       = {Yongqian Sun and Daguo Cheng and Tiankai Yang and Yuhe Ji and Shenglin Zhang and Man Zhu and Xiao Xiong and Qiliang Fan and Minghan Liang and Dan Pei and Tianchi Ma and Yu Chen},
  doi          = {10.1109/TC.2023.3272288},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2858-2871},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient and robust KPI outlier detection for large-scale datacenters},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepFire2: A convolutional spiking neural network
accelerator on FPGAs. <em>TC</em>, <em>72</em>(10), 2847–2857. (<a
href="https://doi.org/10.1109/TC.2023.3272284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-inspired spiking neural networks (SNNs) replace the multiply-accumulate operations of traditional neural networks by integrate-and-fire neurons, with the goal of achieving greater energy efficiency. Specialized hardware implementations of those neurons clearly have advantages over general-purpose devices in terms of power and performance, but exhibit poor scalability when it comes to accelerating large neural networks. DeepFire2 introduces a hardware architecture which can map large network layers efficiently across multiple super logic regions in a multi-die FPGA. That gives more control over resource allocation and parallelism, benefiting both throughput and energy consumption. Avoiding the use of lookup tables to implement the AND operations of an SNN, prevents the layer size to be limited by logic resources. A deep pipeline does not only lead to an increased clock speed of up to 600 MHz. We double the throughput and power efficiency compared to our previous version of DeepFire, which equates to an almost 10-fold improvement over other previous implementations. Importantly, we are able to deploy a large ImageNet model, while maintaining a throughput of over 1500 frames per second.},
  archive      = {J_TC},
  author       = {Myat Thu Linn Aung and Daniel Gerlinghoff and Chuping Qu and Liwei Yang and Tian Huang and Rick Siow Mong Goh and Tao Luo and Weng-Fai Wong},
  doi          = {10.1109/TC.2023.3272284},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2847-2857},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DeepFire2: A convolutional spiking neural network accelerator on FPGAs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Custom CMOS ising machine based on relaxed
burer-monteiro-zhang heuristic. <em>TC</em>, <em>72</em>(10), 2835–2846.
(<a href="https://doi.org/10.1109/TC.2023.3272278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the maximum cut of large graphs may require impractically long time, necessitating approximate algorithms and/or specialized computing platforms. A heuristic by Burer, Monteiro and Zhang for max-cut has not only been shown to be advantageous in many respects, but is also applicable to other NP-complete problems. From the perspective of accelerated computing, the heuristic&#39;s implementational challenge lies in its gradient-descent dynamics, which could be reduced to several sinusoidal kernel operations applied to each edge of the graph. We had previously established the theoretical underpinnings of a relaxed dynamical heuristic for max-cut similar to the one proposed by Burer et al. but suited for accelerated computing on custom analog CMOS. In this work, we present the first fully custom analog integrated circuit implementing the dynamics of our heuristic on 130-nm CMOS technology. In an era of increasing specificity of computing machines, our algorithm-circuit co-design, originally for max-cut, introduces a versatile approach applicable to a diverse set of practical large-scale NP-complete problems.},
  archive      = {J_TC},
  author       = {Aditya Shukla and Mikhail Erementchouk and Pinaki Mazumder},
  doi          = {10.1109/TC.2023.3272278},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2835-2846},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Custom CMOS ising machine based on relaxed burer-monteiro-zhang heuristic},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <span class="math inline">$\tt{PoisonedGNN}$</span>:
Backdoor attack on graph neural networks-based hardware security
systems. <em>TC</em>, <em>72</em>(10), 2822–2834. (<a
href="https://doi.org/10.1109/TC.2023.3271126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have shown great success in detecting intellectual property (IP) piracy and hardware Trojans (HTs). However, the machine learning community has demonstrated that GNNs are susceptible to data poisoning attacks, which result in GNNs performing abnormally on graphs with pre-defined backdoor triggers (realized using crafted subgraphs). Thus, it is imperative to ensure that the adoption of GNNs should not introduce security vulnerabilities in critical security frameworks. Existing backdoor attacks on GNNs generate random subgraphs with specific sizes/densities to act as backdoor triggers. However, for Boolean circuits, backdoor triggers cannot be randomized since the added structures should not affect the functionality of a design. We explore this threat and develop PoisonedGNN as the first backdoor attack on GNNs in the context of hardware design. We design and inject backdoor triggers into the register-transfer- or the gate-level representation of a given design without affecting the functionality to evade some GNN-based detection procedures. To demonstrate the effectiveness of PoisonedGNN, we consider two case studies: (i) Hiding HTs and (ii) IP piracy. Our experiments on TrustHub datasets demonstrate that PoisonedGNN can hide HTs and IP piracy from advanced GNN-based detection platforms with an attack success rate of up to 100\%.},
  archive      = {J_TC},
  author       = {Lilas Alrahis and Satwik Patnaik and Muhammad Abdullah Hanif and Muhammad Shafique and Ozgur Sinanoglu},
  doi          = {10.1109/TC.2023.3271126},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2822-2834},
  shortjournal = {IEEE Trans. Comput.},
  title        = {$\tt{PoisonedGNN}$: Backdoor attack on graph neural networks-based hardware security systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MDTUpdate: A multi-block double tree update technique in
heterogeneous erasure-coded clusters. <em>TC</em>, <em>72</em>(10),
2808–2821. (<a href="https://doi.org/10.1109/TC.2023.3271064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A significant drawback of erasure codes is suffering from the expensive update overhead, and all parity blocks are regenerated once any update of one data block for consistency. Existing update techniques either neglect the multi-block updates scenario under update-intensive workloads or do not consider how to minimize the update cost in heterogeneous clusters. This paper presents the first systematic study on multi-block updates in heterogeneous clusters. We formulate the problem as a cost-based routing optimization model and propose a novel Multi-block Double Tree Update ( MDTUpdate ) technique. The key idea is to construct a double tree structure for multiple updated data blocks and all parity blocks, which avoids the congested link bottleneck. To reduce the update costs effectively, we exploit a hybrid update scheme that combines the data-delta and parity-delta schemes under the double tree structure. To accelerate the tree construction, we design a time-efficient greedy algorithm that timely determines the transmission route via perceiving the cost discrepancy among nodes while avoiding exhaustive enumeration. We further prove that our algorithm is optimal in minimizing the update costs. The experiments show that MDTUpdate can improve the update performance by up to 83.23\% over the existing techniques while incurring extremely lightweight running overhead.},
  archive      = {J_TC},
  author       = {Hai Zhou and Dan Feng and Yuchong Hu},
  doi          = {10.1109/TC.2023.3271064},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2808-2821},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MDTUpdate: A multi-block double tree update technique in heterogeneous erasure-coded clusters},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DHTS: A dynamic hybrid tiling strategy for optimizing
stencil computation on GPUs. <em>TC</em>, <em>72</em>(10), 2795–2807.
(<a href="https://doi.org/10.1109/TC.2023.3271060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stencil computation is an important class of computational modes in scientific computing applications. Loop tiling techniques have been widely studied to accelerate stencil computations on different architectures by exploiting parallelism and data locality. Recent advanced tiling methods enable the tile-wise concurrent start-up to improve the execution performance. However, such methods statically partition all dimensions of iteration space into tiles with predetermined complex shapes and sizes, and thus lead to low thread utilization and memory access efficiency on GPUs. In this paper, we present DHTS, a novel dynamic hybrid tiling strategy for stencil computations. DHTS employs static tiling on the outer dimensions to achieve concurrent start-up parallelism, while proposes a dynamic rectangular tiling method on the inner dimensions to improve thread utilization and memory access efficiency. By deriving tile size constraints, DHTS adaptively achieves equal-size workload of tiles, and therefore reducing idle threads and increasing coalesced memory accesses within tiles. We implement the proposed strategy with different complex tile shapes. Experimental results on Titan V and Tesla V100 GPUs show that DHTS effectively improves the execution performance of 2D/3D stencils compared to state-of-the-art tiling methods, and achieves the best improvement of 28×.},
  archive      = {J_TC},
  author       = {Song Liu and Zengyuan Zhang and Weiguo Wu},
  doi          = {10.1109/TC.2023.3271060},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2795-2807},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DHTS: A dynamic hybrid tiling strategy for optimizing stencil computation on GPUs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HIPEDAP: Energy-efficient hardware accelerators for hidden
periodicity detection. <em>TC</em>, <em>72</em>(10), 2781–2794. (<a
href="https://doi.org/10.1109/TC.2023.3270568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden periodicity detection (HPD) forms the basis of various emerging and complex applications such as detecting tandem repeats in DNA, absence seizure detection in EEG signals, etc. . The solutions to the period estimation problem were not satisfactorily accurate until Ramanujan sums (RS) were used to explore the periodic decomposition of signals. Its use in hidden periodicity detection was streamlined to form Ramanujan Filter Bank (RFB), but its usage in the applications proved to be computationally expensive. This paper proposes HIPED AP , an efficient set of hardware accelerators for hidden periodicity detection applications using Ramanujan Filter Bank. HIPED AP is developed by proposing several incrementally efficient microarchitectures, from Arch-A to E targeting improvements in different aspects of the design such as area, power, and performance. Further, the inherent error resilience exhibited by HPD applications enables us to propose an approximate architecture Arch-F, that synergistically applies multiple approximation techniques such as approximate adder, multiplier, and precision scaling on top of Arch-E, resulting in significant performance and energy benefits. Experimental results obtained after synthesizing the microarchitectures on 45 nm technology demonstrate that the optimized Arch-E design is able to achieve 4.7X, 8.2X, and 1.7X improvements in terms of area, frequency of execution, and power, respectively. Further, Arch-F demonstrate additional power savings of 14.6\% on average (max 32.2\%) over Arch-E for almost no loss in application-level quality. Finally, across a suite of practical applications, HIPED AP exhibited a speed-up in the range of $5.1 \;{\times }\; 10^{2}$ X to $3.2 \;{\times }\; 10^{4}$ compared to its software implementations.},
  archive      = {J_TC},
  author       = {Arghadip Das and Chandrachur Majumder and Debaprasad De and Arnab Raha and Mrinal Kanti Naskar},
  doi          = {10.1109/TC.2023.3270568},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2781-2794},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HIPEDAP: Energy-efficient hardware accelerators for hidden periodicity detection},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temperature-prediction based rate-adjusted time and space
mapping algorithm for 3D CNN accelerator systems. <em>TC</em>,
<em>72</em>(10), 2767–2780. (<a
href="https://doi.org/10.1109/TC.2023.3269696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the state-of-the-art convolutional neural networks (CNNs), the accuracy of the inference is increased by raising the number of the layer. However, it can cause the higher computation complexities of the CNNs. The 3D CNN accelerator is applied to solve the computation complexities for the CNNs. However, the 3D CNN accelerator will encounter the serious thermal problem. In this article, we propose the predicted rate-controlled and task-allocated dynamic thermal management (prctaDTM) for the 3D CNN accelerator systems. The proposed prctaDTM contains the rate-controlled temperature prediction (RCTP), workload movement (WM), and workload adjustment (WA) to control the workloads of the 3D CNN accelerator and adjust the parallelism for the 3D CNN accelerator. The goal is to solve the thermal problem of the 3D CNN accelerator. In our experiments, the latency for the 3D CNN accelerator can be reduced by 18.1\%–40.1\% and the peak temperature for the 3D accelerator can be controlled close to the predefined thermal limitation. Moreover, the prediction errors with RCTP can be constrained below 0.66˚C for different CNN applications.},
  archive      = {J_TC},
  author       = {Jin-Yi Lin and Shu-Yen Lin},
  doi          = {10.1109/TC.2023.3269696},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2767-2780},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Temperature-prediction based rate-adjusted time and space mapping algorithm for 3D CNN accelerator systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified parallel CORDIC-based hardware architecture for
LSTM network acceleration. <em>TC</em>, <em>72</em>(10), 2752–2766. (<a
href="https://doi.org/10.1109/TC.2023.3268400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have recently become the standard tool for solving various practical problems in a wide range of applications with state-of-the-art performance. Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM) are a subset of DNNs with fully connected single or multi-layer networks. The complex neurons and internal states of LSTM networks enable them to build a memory of events, making them ideal for time series applications. Despite the great potential of LSTM networks, their heterogeneous operations and computational resource requirements create a vast gap when it comes to the fast processing time required in real-time applications using low-power, low-cost edge devices. This work proposes a novel hardware architecture that combines serial-parallel computation with matrix algebra concepts and efficient low-power computer arithmetics for LSTM network acceleration. The hardware is based on a systolic ring of outer-product-based processing elements (PEs) and a reusable single activation function block (AFB). PEs and AFB are implemented using the coordinate rotation digital computer algorithm (CORDIC) in the linear and hyperbolic modes. Unlike most approaches, the proposed hardware can be configured to perform recurrent and non-recurrent fully connected layers (FC) computations, making it suitable for various low-power edge applications. The architecture is validated on the Xilinx PYNQ-Z1 development board using an open-source time series dataset. The implemented design achieves $\text{114} \mu \text{s}$ average latency and $\text{1.8 GOPS}$ throughput. The proposed design&#39;s low latency and $\text{0.438 W}$ power consumption makes it suitable for resource-constrained edge platforms.},
  archive      = {J_TC},
  author       = {Nadya A. Mohamed and Joseph R. Cavallaro},
  doi          = {10.1109/TC.2023.3268400},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2752-2766},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A unified parallel CORDIC-based hardware architecture for LSTM network acceleration},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for automated exploration of trojan attack space
in FPGA netlists. <em>TC</em>, <em>72</em>(10), 2740–2751. (<a
href="https://doi.org/10.1109/TC.2023.3266592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field Programmable Gate Arrays (FPGAs) provide a flexible compute platform for quick prototyping or hardware acceleration in diverse application domains. However, similar to the global semiconductor life-cycle in the modern supply chain, FPGA-based product development includes processes and interactions with potentially untrusted parties outside the traditional scrutiny of a completely in-house development cycle. An untrusted party/software can maliciously alter hardware intellectual property (IP) blocks mapped to an FPGA device during various stages of the FPGA life-cycle. Such malicious alterations, also known as hardware Trojans, have garnered significant research into their detection and prevention in the context of application-specific integrated circuit (ASIC) design flow. However, Trojan attacks in FPGAs have not enjoyed this same attention. Designers often rely on mapping ASIC-specific solutions and benchmarks to the FPGA domain, leaving much of the FPGA-specific Trojan space uncovered. The distinctive business model and architectural configurations of FPGAs also present unique Trojan attack opportunities for adversaries. To this end, we introduce a framework to automatically explore the hardware Trojan attack space in FPGA netlists, which can insert different FPGA-specific Trojans in a netlist enabling rapid exploration of potential Trojan attacks in an FPGA design: soft-template, monolithic and distributed dark silicon. The dark silicon Trojans use the under-utilized input space in FPGA primitives and other optimizations to realize Trojans with effectively zero area, delay, and power footprint. We generate over 1300 Trojan-inserted benchmarks using the introduced FPGA Trojan classes, and compare their impact on utilization, delay, and power and evaluate their stealthiness against Trojan detection.},
  archive      = {J_TC},
  author       = {Jonathan Cruz and Christopher Posada and Naren Vikram Raj Masna and Prabuddha Chakraborty and Pravin Gaikwad and Swarup Bhunia},
  doi          = {10.1109/TC.2023.3266592},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2740-2751},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A framework for automated exploration of trojan attack space in FPGA netlists},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characterization of i/o behaviors in cloud storage
workloads. <em>TC</em>, <em>72</em>(10), 2726–2739. (<a
href="https://doi.org/10.1109/TC.2023.3263726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As cloud platforms become increasingly popular, accurately understanding I/O behaviors in modern cloud storage is of paramount importance for system design and optimization. This paper sheds new light on the correlation of inter-arrival times of both read and write requests at the block level in four representative cloud storage workloads – AliCloud, Systor’17, MSRC and FIU. Our study reveals that I/O arrivals at the block level are very complex in modern cloud storage. There is a certain degree of correlation in the long-term timescale for request arrival intervals in AliCloud and Systor’17_read. Request arrival intervals in MSRC, FIU and Systor’17_write, however, are almost uncorrelated. The Gaussianity test confirms that I/O burstiness appears to be Gaussian in AliCloud_write and Systor’17_read, but the burstiness is non-Gaussian in other workloads. Importantly, we unfold the existence of self-similarity in cloud storage workloads with a certain degree of correlations, via visual evidence, the autocorrelation structure of the aggregated process of I/O request sequences, and Hurst parameter estimates. We further design an alpha-stable workload model for synthetic I/O generation, and the experimental results demonstrate that our model has an edge over conventional models in terms of accurately emulating I/O burstiness.},
  archive      = {J_TC},
  author       = {Qiang Zou and Yifeng Zhu and Jianxi Chen and Yuhui Deng and Xiao Qin},
  doi          = {10.1109/TC.2023.3263726},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {2726-2739},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Characterization of I/O behaviors in cloud storage workloads},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-design of approximate multilayer perceptron for
ultra-resource constrained printed circuits. <em>TC</em>,
<em>72</em>(9), 2717–2725. (<a
href="https://doi.org/10.1109/TC.2023.3251863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Printed Electronics (PE) exhibits on-demand, extremely low-cost hardware due to its additive manufacturing process, enabling machine learning (ML) applications for domains that feature ultra-low cost, conformity, and non-toxicity requirements that silicon-based systems cannot deliver. Nevertheless, large feature sizes in PE prohibit the realization of complex printed ML circuits. In this work, we present, for the first time, an automated printed-aware software/hardware co-design framework that exploits approximate computing principles to enable ultra-resource constrained printed multilayer perceptrons (MLPs). Our evaluation demonstrates that, compared to the state-of-the-art baseline, our circuits feature on average 6x (5.7x) lower area (power) and less than 1\% accuracy loss.},
  archive      = {J_TC},
  author       = {Giorgos Armeniakos and Georgios Zervakis and Dimitrios Soudris and Mehdi B. Tahoori and Jörg Henkel},
  doi          = {10.1109/TC.2023.3251863},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2717-2725},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Co-design of approximate multilayer perceptron for ultra-resource constrained printed circuits},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stateful serverless application placement in MEC with
function and state dependencies. <em>TC</em>, <em>72</em>(9), 2701–2716.
(<a href="https://doi.org/10.1109/TC.2023.3262947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing is emerging as an enabling technology for elastic and low-cost AI applications in the edge of core networks. It allows AI developers to decompose a complex training and time-sensitive inference task into multiple functions with dependency, and upload the task to a Multi-access Edge Computing platform (MEC) for execution. Serverless computing adopts a popular design principle: the disaggregation of storage and computation, making the functions ‘stateless’. However, most AI applications are ‘stateful’ and rely on an external storage service to manage their states (ephemeral data). This will incur a prohibitively long delay for delay-sensitive AI applications if external services storing the states are far from the serverless functions. Motivated by this critical issue, in this paper we investigate a fundamental problem in serverless computing – the stateful serverless application placement problem, for which, we first propose an efficient heuristic algorithm, and then devise an approximation algorithm with a provable approximation ratio for one of its special cases. We also consider the online version of the problem, and develop an online learning-driven algorithm with a bounded regret. The crux of the online algorithm is the adoption of the multi-armed bandits technique for dynamic admissions of inference requests, under the uncertainty of both data volumes of requests and network delays. We finally evaluate the performance of the proposed algorithms through experimental simulations. Simulation results show that the proposed algorithms outperform their counterparts, reducing at least 32\% in the total cost and 27\% of the average delay.},
  archive      = {J_TC},
  author       = {Zichuan Xu and Lizhen Zhou and Weifa Liang and Qiufen Xia and Wenzheng Xu and Wenhao Ren and Haozhe Ren and Pan Zhou},
  doi          = {10.1109/TC.2023.3262947},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2701-2716},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Stateful serverless application placement in MEC with function and state dependencies},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transition factors of power consumption models for CPA
attacks on cryptographic RISC-v SoC. <em>TC</em>, <em>72</em>(9),
2689–2700. (<a href="https://doi.org/10.1109/TC.2023.3262926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical cryptographic devices are vulnerable to side-channel information leakages during operation. They are widely used in software as well as hardware implementations, ranging from microcontrollers and microprocessors to hardware accelerators in System on Chips (SoCs). Nowadays, cryptographic RISC-V SoCs are becoming the most prominent solution compared to the rest. Cryptographic accelerators provide users with a very high level of flexibility and customization of chips suited to specific applications in these systems. First, this research aims to confirm the effectiveness of the Correlation Power Analysis attack on cryptographic SoCs based on three different power consumption models. In each model, the effectiveness of an attack depends on the transition factor, which is a ratio related to different characteristics of the device&#39;s power consumption. Then, we focus on modifying the configuration on the SoC and attacking the AES hardware implementation on these designs. The experimental results show that applying the Switching Distance model brings the highest performance. With our suggested range of transition factors, the number of traces needed to find the secret key can be reduced by 13.35\% in the best case.},
  archive      = {J_TC},
  author       = {Thai-Ha Tran and Ba-Anh Dao and Trong-Thuc Hoang and Van-Phuc Hoang and Cong-Kha Pham},
  doi          = {10.1109/TC.2023.3262926},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2689-2700},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Transition factors of power consumption models for CPA attacks on cryptographic RISC-V SoC},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reed-solomon coding algorithms based on reed-muller
transform for any number of parities. <em>TC</em>, <em>72</em>(9),
2677–2688. (<a href="https://doi.org/10.1109/TC.2023.3262922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the Reed-Muller (RM) transform, this paper proposes a Reed-Solomon (RS) encoding/erasure decoding algorithm for any number of parities. Specifically, we first generalize the previous RM-based syndrome calculation, which allows only up to seven parities, to support any number of parities. Then we propose a general encoding/erasure decoding algorithm. The proposed encoding algorithm eliminates the operations in solving linear equations, and this improves the computational efficiency of existing RM-based RS algorithms. In terms of erasure decoding, this paper employs the generalized RM-based syndrome calculation and lower–upper (LU) decomposition to accelerate the computational efficiency. Analysis shows that the proposed encoding/erasure decoding algorithm approaches the complexity of $\lfloor \lg T \rfloor + 1$ XORs per data bit with $N$ increasing, where $T$ and $N$ denote the number of parities and codeword length respectively. To highlight the advantage of the proposed RM-based algorithms, the implementations with Single Instruction Multiple Data (SIMD) technology are provided. Simulation results show that the proposed algorithms are competitive, as compared with other cutting-edge implementations.},
  archive      = {J_TC},
  author       = {Leilei Yu and Sian-Jheng Lin and Hanxu Hou and Zhengrui Li},
  doi          = {10.1109/TC.2023.3262922},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2677-2688},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reed-solomon coding algorithms based on reed-muller transform for any number of parities},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Faster public-key compression of SIDH with less memory.
<em>TC</em>, <em>72</em>(9), 2668–2676. (<a
href="https://doi.org/10.1109/TC.2023.3259321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the isogeny-based protocol, namely supersingular isogeny Diffie-Hellman (SIDH) has become highly attractive for its small public key size. In addition, one can utilize several techniques to further compress the public key. However, compared to other post-quantum protocols, the computational cost of SIDH is relatively high, and so is that of its public-key compression. On the other hand, the storage for pairing computation and discrete logarithms to speed up the current implementation of the key compression is somewhat large. In this paper, we mainly improve the performance of public-key compression of SIDH, especially the efficiency and the storage of pairing computation involved. Our experimental results show that the memory requirement for pairing computation is reduced by a factor of about 1.5. Meanwhile, the instantiation of public-key compression of SIDH is $6.95\%--10.44\%$ faster than the current state-of-the-art. Although SIKE is broken now, the techniques in this paper may benefit other isogeny-based cryptosystems which are still secure.},
  archive      = {J_TC},
  author       = {Kaizhan Lin and Jianming Lin and Weize Wang and Chang-An Zhao},
  doi          = {10.1109/TC.2023.3259321},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2668-2676},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Faster public-key compression of SIDH with less memory},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving log-based anomaly detection by pre-training
hierarchical transformers. <em>TC</em>, <em>72</em>(9), 2656–2667. (<a
href="https://doi.org/10.1109/TC.2023.3257518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained models, such as BERT, have resulted in significant pre-trained models, such as BERT, have resulted in significant improvements in many natural language processing (NLP) applications. However, due to differences in word distribution and domain data distribution, applying NLP advancements to log analysis directly faces some performance challenges. This paper studies how to adapt the recently introduced pre-trained language model BERT for log analysis. In this work, we propose a pre-trained log representation model with hierarchical bidirectional encoder transformers (namely, HilBERT). Unlike previous work, which used raw text as pre-training data, we parse logs into templates before using the log templates to pre-train HilBERT. We also design a hierarchical transformers model to capture log template sequence-level information. We use log-based anomaly detection for downstream tasks and fine-tune our model with different log data. Our experiments demonstrate that HilBERT outperforms other baseline techniques on unstable log data. While BERT obtains performance comparable to that of previous state-of-the-art models, HilBERT can significantly address the problem of log instability and achieve accurate and robust results.},
  archive      = {J_TC},
  author       = {Shaohan Huang and Yi Liu and Carol Fung and He Wang and Hailong Yang and Zhongzhi Luan},
  doi          = {10.1109/TC.2023.3257518},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2656-2667},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improving log-based anomaly detection by pre-training hierarchical transformers},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ZPerf: A statistical gray-box approach to performance
modeling and extrapolation for scientific lossy compression.
<em>TC</em>, <em>72</em>(9), 2641–2655. (<a
href="https://doi.org/10.1109/TC.2023.3257517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the scaling up of simulation-based scientific discovery on high-performance computing systems, the disparity between compute and I/O has increased, forcing domain scientists to save only a small amount of simulation data to persistent storage. This can result in the loss of essential physics fields that are needed for data analysis. While error-bounded lossy compression has made tremendous progress in bridging the gap between compute and I/O, the lack of understanding of compression performance remains a key hurdle to its wide adoption. In this work, we present zPerf, a statistical gray-box performance modeling approach for scientific lossy compression. Our contributions are threefold: 1) We develop zPerf to estimate the performance of lossy compression techniques, based on in-depth understanding and statistical modeling for data features and core compression metrics; 2) We demonstrate the in-detailed implementation of zPerf using two case studies, where we derive the performance modeling for SZ and ZFP, two leading lossy compressors; 3) We evaluate the effectiveness of zPerf on real-world datasets across various domains. Based on the evaluation, we demonstrate the efficacy of the zPerf performance model; 4) We further discuss three case studies where zPerf is applied to extrapolate the compression ratio of SZ and ZFP with alternative encoding schemes as well as ZFP with an alternative transform scheme. Through the case studies, we demonstrate the potential of zPerf for exploring the design space of lossy compression, which has hardly been studied in the literature.},
  archive      = {J_TC},
  author       = {Jinzhen Wang and Qi Chen and Tong Liu and Qing Liu and Xubin He},
  doi          = {10.1109/TC.2023.3257517},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2641-2655},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ZPerf: A statistical gray-box approach to performance modeling and extrapolation for scientific lossy compression},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating graph convolutional networks through a
PIM-accelerated approach. <em>TC</em>, <em>72</em>(9), 2628–2640. (<a
href="https://doi.org/10.1109/TC.2023.3257514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) are promising to enable machine learning on graph data. GCNs show potential vertex-level and intra-vertex parallelism for GPU acceleration, but their irregular memory accesses arising in aggregation operations and the inherent sparsity for vertex features of graphs cause inefficiencies on the GPU. In this paper, we present gPIM, which aims to accelerate GCNs inference through a processing-in-memory (PIM) enabled architecture. gPIM is expected to perform compute-intensive combination on the GPU while aggregation and memory-bound combination are offloaded to the PIM-featured hybrid memory cubes (HMCs). To maximize the efficiency of such GPU-HMC architecture, gPIM is novel with two key designs: 1) A GCN-induced graph partitioning that minimizes communication overheads between cubes, 2) A programmer-transparent performance estimation mechanism that predicts the performance bound of operations accurately for workload offloading. Experimental results show that gPIM significantly outperforms Intel Xeon E5-2680v3 CPU (8,979.52×), NVIDIA Tesla V100 GPU (96.01×), and a state-of-the-art GCN accelerator AWB-GCN (4.18×).},
  archive      = {J_TC},
  author       = {Hai Jin and Dan Chen and Long Zheng and Yu Huang and Pengcheng Yao and Jin Zhao and Xiaofei Liao and Wenbin Jiang},
  doi          = {10.1109/TC.2023.3257514},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2628-2640},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating graph convolutional networks through a PIM-accelerated approach},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAMBA: Sparsity aware in-memory computing based machine
learning accelerator. <em>TC</em>, <em>72</em>(9), 2615–2627. (<a
href="https://doi.org/10.1109/TC.2023.3257513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) inference is typically dominated by highly data-intensive Matrix Vector Multiplication (MVM) computations that may be constrained by memory bottleneck due to massive data movement between processor and memory. Although analog in-memory computing (IMC) ML accelerators have been proposed to execute MVM with high efficiency, the latency and energy of such computing systems can be dominated by the large latency and energy costs from analog-to-digital converters (ADCs). Leveraging sparsity in ML workloads, reconfigurable ADCs can save MVM energy and latency by reducing the required ADC bit precision. However, such improvement in latency can be hindered by non-uniform sparsity of the weight matrices mapped into hardware. Moreover, data movement between MVM processing cores may become another factor that delays the overall system-level performance. To address these issues, we propose SAMBA, Sparsity Aware IMC Based Machine Learning Accelerator. First, we propose load balancing during mapping of weight matrices into physical crossbars to eliminate non-uniformity in the sparsity of mapped matrices. Second, we propose optimizations in arranging and scheduling the tiled MVM hardware to minimize the overhead of data movement across multiple processing cores. Our evaluations show that the proposed load balancing technique can achieve performance improvement. The proposed optimizations can further improve both performance and energy-efficiency regardless of sparsity condition. With the combination of load balancing and data movement optimization in conjunction with reconfigurable ADCs, our proposed approach provides up to 2.38x speed-up and 1.54x energy-efficiency over stateof- art analog IMC based ML accelerators for ImageNet datasets on Resnet-50 architecture.},
  archive      = {J_TC},
  author       = {Dong Eun Kim and Aayush Ankit and Cheng Wang and Kaushik Roy},
  doi          = {10.1109/TC.2023.3257513},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2615-2627},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SAMBA: Sparsity aware in-memory computing based machine learning accelerator},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Byzantine-resilient federated learning at edge. <em>TC</em>,
<em>72</em>(9), 2600–2614. (<a
href="https://doi.org/10.1109/TC.2023.3257510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both Byzantine resilience and communication efficiency have attracted tremendous attention recently for their significance in edge federated learning. However, most existing algorithms may fail when dealing with real-world irregular data that behaves in a heavy-tailed manner. To address this issue, we study the stochastic convex and non-convex optimization problem for federated learning at edge and show how to handle heavy-tailed data while retaining the Byzantine resilience, communication efficiency and the optimal statistical error rates simultaneously. Specifically, we first present a Byzantine-resilient distributed gradient descent algorithm that can handle the heavy-tailed data and meanwhile converge under the standard assumptions. To reduce the communication overhead, we further propose another algorithm that incorporates gradient compression techniques to save communication costs during the learning process. Theoretical analysis shows that our algorithms achieve order-optimal statistical error rate in presence of Byzantine devices. Finally, we conduct extensive experiments on both synthetic and real-world datasets to verify the efficacy of our algorithms.},
  archive      = {J_TC},
  author       = {Youming Tao and Sijia Cui and Wenlu Xu and Haofei Yin and Dongxiao Yu and Weifa Liang and Xiuzhen Cheng},
  doi          = {10.1109/TC.2023.3257510},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2600-2614},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Byzantine-resilient federated learning at edge},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DownShift: Tuning shift reduction with reliability for
racetrack memories. <em>TC</em>, <em>72</em>(9), 2585–2599. (<a
href="https://doi.org/10.1109/TC.2023.3257509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-dense non-volatile racetrack memories (RTMs) have been investigated at various levels in the memory hierarchy for improved performance and reduced energy consumption. However, the innate shift operations in RTMs, required for data access, incur performance penalties and can induce position errors. These factors can hinder their applicability in replacing low-latency, reliable on-chip memories. Intelligent placement of memory objects in RTMs can significantly reduce the number of shifts per memory access with little to no hardware overhead. However, existing placement strategies may lead to sub-optimal performance when applied to different architectures. Additionally, the impact of these shift optimization techniques on RTM reliability has been insufficiently investigated. We propose DownShift, a generalized data placement mechanism that improves upon prior approaches by taking into account (1) the timing and liveliness information of memory objects and (2) the underlying memory architecture, including required shifting fault tolerance. Thus, we also propose a collaboratively designed new shift alignment reliability technique called GROGU. GROGU leverages the reduced shift window made possible through DownShift allowing improved reliability, area, and energy compared to the state-of-the-art reliability approaches. DownShift reduces the number of shifts, runtime, and energy consumption by 3.24×, 47.6\%, and 70.8\% compared to the state-of-the-art. GROGU consumes 2.2× less area and 1.3× less energy while providing 16.8× improvement in shift fault tolerance compared to the leading reliability approach for a latency degradation of only 3.2\%.},
  archive      = {J_TC},
  author       = {Asif Ali Khan and Sebastien Ollivier and Fazal Hameed and Jeronimo Castrillon and Alex K. Jones},
  doi          = {10.1109/TC.2023.3257509},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2585-2599},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DownShift: Tuning shift reduction with reliability for racetrack memories},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TurboGNN: Improving the end-to-end performance for
sampling-based GNN training on GPUs. <em>TC</em>, <em>72</em>(9),
2571–2584. (<a href="https://doi.org/10.1109/TC.2023.3257507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNN) have evolved as powerful models for graph representation learning. Sampling-based training methods have been introduced to train large graphs without compromising accuracy. However, it is challenging for the existing GNN systems to effectively utilize multi-core accelerators, especially GPUs, due to a large number of atomic operations and unbalanced workload originating from the serial execution of multiple GNN processing stages. In this paper, we propose a combination of optimization techniques to accelerate the end-to-end performance of the sampling-based GNN training process. Specifically, we propose an adaptive shared memory-based sampling technique and a degree-guided thread block scheduling strategy to optimize the graph sampling. Further, based on the observations of resource demand in different training stages, we propose an asynchronous pipeline-based scheduling method, which accelerates the GNN training by decoupling different training stages into a pipeline and therefore improves the GPU resource utilization significantly. The experimental results show that compared with the existing work, the proposed methods can achieve up to 5.6X performance speedup in the end-to-end performance.},
  archive      = {J_TC},
  author       = {Wenchao Wu and Xuanhua Shi and Ligang He and Hai Jin},
  doi          = {10.1109/TC.2023.3257507},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2571-2584},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TurboGNN: Improving the end-to-end performance for sampling-based GNN training on GPUs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tissue p systems with states in cells. <em>TC</em>,
<em>72</em>(9), 2561–2570. (<a
href="https://doi.org/10.1109/TC.2023.3257506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tissue-like P systems with channel states are a type of classical membrane systems in which objects transferred among regions are controlled by states placed in the channels between regions. However, an important biological fact is the existence of a “barrier” to the diffusion of signal molecules, which tend to remain confined to some particular micro-habitat. This feature allows quorum sensing to convey information about the physiological state of spatially separated sub-populations. Therefore, in this article, we design a novel class-variant of P systems named tissue P systems with states in cells (TSIC P systems). Here, each cell contains one and only one state at any moment (the environment has no state), and objects transferred among regions are controlled by states (or a state) that are placed in the corresponding cells (or a cell). We discuss the computability theory of TSIC P systems by showing that Turing universality is acquired by TSIC P systems, which are worked both in a flat maximal parallelism and in a maximal parallelism. In addition, when cell division is considered in TSIC P systems, then tissue P systems with states in cells and cell division (TSICD P systems) are constructed. The (presumed) computational efficiency of TSICD P systems is reached by offering a uniform solution to the satisfiability problem.},
  archive      = {J_TC},
  author       = {Bosheng Song and Kenli Li and David Orellana-Martín and Xiangxiang Zeng and Mario J. Pérez-Jiménez},
  doi          = {10.1109/TC.2023.3257506},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2561-2570},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tissue p systems with states in cells},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Acceleration of control intensive applications on
coarse-grained reconfigurable arrays for embedded systems. <em>TC</em>,
<em>72</em>(9), 2548–2560. (<a
href="https://doi.org/10.1109/TC.2023.3257504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedded systems confront two opposite goals: low-power operation and high performance. The current trend to reach these goals is toward heterogeneous platforms, including multi-core architectures with heterogeneous cores and hardware accelerators. The latter can be divided into custom accelerators (e.g., ASICs) and programmable domain-specific cores (e.g., DSIPs). VWR2A Denkinger et al. 2022 is a programmable architecture that integrates high computational density and low power memory structures. The flexibility of VWR2A allows a large portion of applications to be covered, resulting in better performance and energy efficiency than ASICs and general-purpose processors. However, while this has been well studied for data-intensive kernels, this is not the case for control-intensive kernels —code with complex if-else and nested loop structures. Traditionally, control-intensive code is left to be executed by the host processor. This situation unnecessarily restricts the potential impact of energy-efficient acceleration, especially at the application level. In this paper, we evaluate the performance and energy consumption of VWR2A for control-intensive code and compare it with an ARM Cortex-M4 processor and a RISC-V Ibex processor. The performance and energy consumption are evaluated at the kernel and application levels. Our results confirm that VWR2A is faster and more energy-efficient than the two considered general-purpose processors also for control-intensive code.},
  archive      = {J_TC},
  author       = {Benoît Walter Denkinger and Miguel Peón-Quirós and Mario Konijnenburg and David Atienza and Francky Catthoor},
  doi          = {10.1109/TC.2023.3257504},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2548-2560},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Acceleration of control intensive applications on coarse-grained reconfigurable arrays for embedded systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information leakage attacks exploiting cache replacement in
commercial processors. <em>TC</em>, <em>72</em>(9), 2536–2547. (<a
href="https://doi.org/10.1109/TC.2023.3254918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caches have been used to construct various covert and side channels. Most existing cache channels exploit the timing difference between cache hits and cache misses. We highlight that cache misses in different states may have more significant time differences. This paper presents in detail how replacement latency differences can be used to construct timing-based channels (called WB channels) to leak information. Any modification to a cache line by a sender will set it to the dirty state, and the receiver can observe this through measuring the latency of replacing this cache set. This paper evaluates WB channels from implementation complexity, stability, scalability, bandwidth, and stealthiness. Experimental results show that WB channel can not only transmit information covertly with high bandwidth but also has the strong anti-interference ability. Moreover, many previous cache defense and detection mechanisms target attacks exploiting the timing difference between cache hits and cache misses. This paper discusses the effectiveness of the WB channels against some such strategies. Moreover, this paper shows how to use our WB channel to mount a side-channel attack against a real-world security-sensitive application, such as AES implemented in OpenSSL-1.0.1e.},
  archive      = {J_TC},
  author       = {Yujie Cui and Hongwei Cui and Xu Cheng},
  doi          = {10.1109/TC.2023.3254918},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2536-2547},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Information leakage attacks exploiting cache replacement in commercial processors},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FlexBlock: A flexible DNN training accelerator with
multi-mode block floating point support. <em>TC</em>, <em>72</em>(9),
2522–2535. (<a href="https://doi.org/10.1109/TC.2023.3253050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training deep neural networks (DNNs), expensive floating point arithmetic units are used in GPUs or custom neural processing units (NPUs). To reduce the burden of floating point arithmetic, community has started exploring the use of more efficient data representations, e.g., block floating point (BFP). The BFP format allows a group of values to share an exponent, which effectively reduces the memory footprint and enables cheaper fixed point arithmetic for multiply-accumulate (MAC) operations. However, existing BFP-based DNN accelerators are targeted for a specific precision, making them less versatile. In this paper, we present FlexBlock, a DNN training accelerator with three BFP modes, possibly different among activation, weight, and gradient tensors. By configuring FlexBlock to a lower BFP precision, the number of MACs handled by the core increases by up to 4× in 8-bit mode or 16× in 4-bit mode compared to 16-bit mode. To reach this theoretical upper bound, FlexBlock maximizes the core utilization at various precision levels or layer types, and allows dynamic precision control to keep throughput at its peak without sacrificing training accuracy. We evaluate the effectiveness of FlexBlock using representative DNNs on CIFAR, ImageNet and WMT14 datasets. As a result, training in FlexBlock significantly improves training speed by 1.5 $\sim 5.3\times$ and energy efficiency by 2.4 $\sim 7.0\times$ compared to other training accelerators.},
  archive      = {J_TC},
  author       = {Seock-Hwan Noh and Jahyun Koo and Seunghyun Lee and Jongse Park and Jaeha Kung},
  doi          = {10.1109/TC.2023.3253050},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2522-2535},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FlexBlock: A flexible DNN training accelerator with multi-mode block floating point support},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing GPU-based graph sampling and random walk for
efficiency and scalability. <em>TC</em>, <em>72</em>(9), 2508–2521. (<a
href="https://doi.org/10.1109/TC.2023.3251860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph sampling and random walk algorithms are playing increasingly important roles today because they can significantly reduce graph size while preserving structural information, thus enabling computationally intensive tasks on large-scale graphs. Current frameworks designed for graph sampling and random walk tasks are generally not efficient in terms of memory requirement and throughput. Not to mention that some of them result in biased results. To solve the above problems, we introduce Skywalker+, a high-performance graph sampling and random walk framework on multiple GPUs supporting multiple algorithms. Skywalker+ makes four key contributions: First, it realizes highly paralleled alias method on GPUs. Second, it applies finely adjusted workload-balancing techniques and locality-aware execution modes to present a highly efficient execution engine. Third, it optimizes the GPU memory usage with efficient buffering and data compression schemes. Last, it scales to multi-GPU to further enhance the system throughput. Abundant experiments show that Skywalker+ exhibits significant advantage over the baselines both in performance and utility.},
  archive      = {J_TC},
  author       = {Pengyu Wang and Cheng Xu and Chao Li and Jing Wang and Taolei Wang and Lu Zhang and Xiaofeng Hou and Minyi Guo},
  doi          = {10.1109/TC.2023.3251860},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2508-2521},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing GPU-based graph sampling and random walk for efficiency and scalability},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object fingerprint cache for heterogeneous memory system.
<em>TC</em>, <em>72</em>(9), 2496–2507. (<a
href="https://doi.org/10.1109/TC.2023.3251852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous memory systems promise to provide both large storage capacity and high performance. Prior DRAM cache systems have metadata scalability issue and suffer from low cache hit rate, low DRAM space utilization, and significant data migration overhead. We observe that instances of an object type exhibit stable and predictable memory access patterns in its constituent cachelines and these patterns are referred to as object fingerprints. We propose the hardware-assisted cache that manages DRAM at the object type level and fetches data block at the granularity of a cacheline, by exploiting object fingerprint. To address its design challenges, we first present a software-hardware co-design to convey the software information to hardware. Second, we design multiple granularity sector caches that can be dynamically adjusted to adapt to changing behaviors and improve DRAM cache utilization. To address the challenges of large metadata storage overhead, we propose to bound possible sizes for each sector cache. Experimental results show our designs improve DRAM cache hit rate by 21.6\%, boost IPC by 19.8\%, and reduce data migration traffic by 51.6\% on average, compared with state-of-art DRAM caches. More importantly, our online object fingerprint learning method is 2.3\% inferior to the offline one in terms of IPC.},
  archive      = {J_TC},
  author       = {Fang Zhou and Song Wu and Jianhui Yue and Hai Jin and Jiangqiu Shen},
  doi          = {10.1109/TC.2023.3251852},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2496-2507},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Object fingerprint cache for heterogeneous memory system},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EPRICE: An efficient and privacy-preserving real-time
incentive system for crowdsensing in industrial internet of things.
<em>TC</em>, <em>72</em>(9), 2482–2495. (<a
href="https://doi.org/10.1109/TC.2023.3251850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crowdsensing, we can leverage intelligent devices and real-time incentive mechanisms to facilitate the collection of reliable and timely data in Industrial Internet of Things (IIoT) settings. In such a setting, one can use cryptographic primitives to support data privacy preservation and quality-aware reward distribution simultaneously. However, existing approaches might incur expensive computation costs, suffer from overflow problems, or rely on implicit security conditions. In this paper, we propose an E fficient and P rivacy-preserving R eal-time I ncentive system for C rowds E nsing (EPRICE), designed to estimate the reliability of sensing data in a privacy-preserving setting. The theoretical analysis demonstrates that our proposed system achieves a high level of privacy-preserving for real-time reward distribution and supports practical privacy-preserving properties. The experimental findings show that our proposed EPRICE system significantly decreases the computation costs by three orders of magnitude compared with other competing schemes.},
  archive      = {J_TC},
  author       = {Qi Feng and Debiao He and Min Luo and Xinyi Huang and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TC.2023.3251850},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2482-2495},
  shortjournal = {IEEE Trans. Comput.},
  title        = {EPRICE: An efficient and privacy-preserving real-time incentive system for crowdsensing in industrial internet of things},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disjoint paths construction and fault-tolerant routing in
BCube of data center networks. <em>TC</em>, <em>72</em>(9), 2467–2481.
(<a href="https://doi.org/10.1109/TC.2023.3251849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BCube is a promising structure of data center network, as it can significantly improve the performance of typical applications. With the expansion of network scale and increasement of complexity, reliability and stability of networks have become more essential. In this paper, we study the fault-tolerant routings in BCube. First, we design a fault-tolerant routing algorithm based on node disjoint multi-paths. The proposed multi-path routing has stronger fault tolerance, since each path has no other common nodes except the source node and the destination node. Second, we investigate an effective fault-tolerant routing based on routing capabilities algorithm for BCube. The proposed algorithm has higher fault tolerance and success rate of finding feasible routes, since it does not limit the faults number. Third, we present an adaptive path finding algorithm for establishing virtual links between any two nodes in BCube, which can shorten the diameter of BCube. Extensive simulation results show that the proposed routing scheme outperforms the existing popular algorithms. Compared with the state-of-the-art fault-tolerant routing algorithms, the proposed algorithm has a 21.5\% to 25.3\% improvement on both throughput and packet arrival rate. Meanwhile, it reduces the average latency of 18.6\% and the maximum latency of 23.7\% in networks.},
  archive      = {J_TC},
  author       = {Weibei Fan and Fu Xiao and Hui Cai and Xiaobai Chen and Shui Yu},
  doi          = {10.1109/TC.2023.3251849},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2467-2481},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Disjoint paths construction and fault-tolerant routing in BCube of data center networks},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-speed VLSI architectures for modular polynomial
multiplication via fast filtering and applications to lattice-based
cryptography. <em>TC</em>, <em>72</em>(9), 2454–2466. (<a
href="https://doi.org/10.1109/TC.2023.3251847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a low-latency hardware accelerator for modular polynomial multiplication for lattice-based post-quantum cryptography and homomorphic encryption applications. The proposed novel modular polynomial multiplier exploits the fast finite impulse response (FIR) filter architecture to reduce the computational complexity of the schoolbook modular polynomial multiplication. We also extend this structure to fast $M$ -parallel architectures while achieving low-latency, high-speed, and full hardware utilization. We comprehensively evaluate the performance of the proposed architectures under various polynomial settings as well as in the Saber scheme for post-quantum cryptography as a case study. The experimental results show that our proposed modular polynomial multiplier reduces the computation time and area-time product, respectively, compared to the state-of-the-art designs.},
  archive      = {J_TC},
  author       = {Weihang Tan and Antian Wang and Xinmiao Zhang and Yingjie Lao and Keshab K. Parhi},
  doi          = {10.1109/TC.2023.3251847},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2454-2466},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-speed VLSI architectures for modular polynomial multiplication via fast filtering and applications to lattice-based cryptography},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain-based fair and fine-grained data trading with
privacy preservation. <em>TC</em>, <em>72</em>(9), 2440–2453. (<a
href="https://doi.org/10.1109/TC.2023.3251846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a blockchain-based fair and privacy-preserving data trading scheme that supports fine-grained data selling. First, to achieve fairness for trading participants, by incorporating attribute-based credentials, encryption, and zero-knowledge proof, we design a data trading scheme where a buyer first publishes the required data attributes on the blockchain, and a data seller can demonstrate data availability in ciphertext by only disclosing the required attributes to a data buyer and proving the authenticity of data. A data buyer transfers funds only if the correct key material is uploaded to the blockchain. Second, to guarantee fine-grained data trading and preserve identity privacy, we build a Merkle hash tree on the ciphertexts of data with a signature on its root node, which allows a data seller to split data into blocks and remove the sensitive information from the data without affecting data availability verification. The public key of the data seller is not leaked to the data buyer during the trading. Moreover, different trading transactions from the same data seller cannot be linked. We formally prove that our scheme achieves the desired security properties: fairness and privacy preservation. Simulation results demonstrate the feasibility and efficiency of the proposed scheme.},
  archive      = {J_TC},
  author       = {Liang Xue and Jianbing Ni and Dongxiao Liu and Xiaodong Lin and Xuemin Shen},
  doi          = {10.1109/TC.2023.3251846},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2440-2453},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Blockchain-based fair and fine-grained data trading with privacy preservation},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model based verification of spiking neural networks in cyber
physical systems. <em>TC</em>, <em>72</em>(9), 2426–2439. (<a
href="https://doi.org/10.1109/TC.2023.3251841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs) have found increasing utility in designing safety-critical Cyber-Physical Systems (CPSs) such as implantable medical devices, autonomous vehicles, and space robotics due to their capability to operate on information represented in temporal coding and exhibit various behavioural modalities. Thus, there has been recent interest in formally verifying their timing behaviours and providing soundness guarantees of their diverse characteristics. However, beyond the simplistic Leaky Integrate and Fire (LIF) model, which only mimics 3 spiking behaviours, there is a lack of unifying methodology in literature to verify complex dynamics of biological neurons exhibiting 20 spiking behaviours as demonstrated by the pioneering work of Izhikevich. There is also a complete lack of formulation for the verification of SNN-based systems. This paper bridges these gaps by proposing a model-based approach for designing SNN-based controllers in CPS. We propose sound structural transformations translating any spiking neuron into networks of Timed Automata (TA), model the complex Izhikevich neural model and formally verify all 20 timing behaviours it exhibits for the first time. We then present two case studies that were modelled as SNNs using our approach: the PID controller, and the Car-Following controller, and subsequently attempt static model checking and statistical verification of their generated TA models for safety guarantees.},
  archive      = {J_TC},
  author       = {Ankit Pradhan and Jonathan King and Srinivas Pinisetty and Partha S. Roop},
  doi          = {10.1109/TC.2023.3251841},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {2426-2439},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Model based verification of spiking neural networks in cyber physical systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differential fault attack on rasta and <span
class="math inline">FiLIP<sub>DSM</sub></span>. <em>TC</em>,
<em>72</em>(8), 2418–2425. (<a
href="https://doi.org/10.1109/TC.2023.3244629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose Differential Fault Attack (DFA) on two Fully Homomorphic Encryption (FHE) friendly stream ciphers Rasta and $\text{FiLIP}_{\text{DSM}}$ . Design criteria of Rasta rely on affine layers and nonlinear layers, whereas $\text{FiLIP}_{\text{DSM}}$ relies on permutations and a nonlinear filter function. Here we show that the secret key of these two ciphers can be recovered by injecting only 1 bit fault in the initial state. Our DFA on full round (# rounds $=6$ ) Rasta with 219 block size requires only one block (i.e., 219 bits) of normal and faulty keystream bits. In the case of our DFA on FiLIP-430 (one instance of $\text{FiLIP}_{\text{DSM}}$ ), we need 30000 normal and faulty keystream bits.},
  archive      = {J_TC},
  author       = {R Radheshwar and Meenakshi Kansal and Pierrick Méaux and Dibyendu Roy},
  doi          = {10.1109/TC.2023.3244629},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2418-2425},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Differential fault attack on rasta and $\text{FiLIP}_{\text{DSM}}$},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HW/SW co-design for reliable TCAM- based in-memory
brain-inspired hyperdimensional computing. <em>TC</em>, <em>72</em>(8),
2404–2417. (<a href="https://doi.org/10.1109/TC.2023.3248286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-inspired hyperdimensional computing (HDC) is continuously gaining remarkable attention. It is a promising alternative to traditional machine-learning approaches due to its ability to learn from little data, lightweight implementation, and resiliency against errors. However, HDC is overwhelmingly data-centric similar to traditional machine-learning algorithms. In-memory computing is rapidly emerging to overcome the von Neumann bottleneck by eliminating data movements between compute and storage units. In this work, we investigate and model the impact of imprecise in-memory computing hardware, namely TCAM cells, on the inference accuracy of HDC. Our modeling is based on 14nm FinFET technology fully calibrated with Intel measurement data. We accurately model, for the first time, the voltage-dependent error probability in SRAM-based and FeFET-based in-memory computing. Thanks to HDC&#39;s resiliency against errors, the complexity of the underlying hardware can be reduced, providing large energy savings of up to 6x. Experimental results for SRAM reveal that variability-induced errors have a probability of up to 39\%. Despite such a high error probability, the inference accuracy is only marginally impacted. This opens doors to explore new tradeoffs. We also demonstrate that the resiliency against errors is application-dependent. In addition, we investigate the robustness of HDC against errors with emerging non-volatile FeFET devices instead of mature CMOS-based SRAMs. We demonstrate that inference accuracy does remain high despite the larger error probability, while large area and power savings can be obtained. All in all, HW/SW co-design is the key for efficient yet reliable in-memory HDC for both conventional CMOS technology and upcoming emerging technologies.},
  archive      = {J_TC},
  author       = {Simon Thomann and Paul R. Genssler and Hussam Amrouch},
  doi          = {10.1109/TC.2023.3248286},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2404-2417},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HW/SW co-design for reliable TCAM- based in-memory brain-inspired hyperdimensional computing},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient error detection for matrix multiplication with
systolic arrays on FPGAs. <em>TC</em>, <em>72</em>(8), 2390–2403. (<a
href="https://doi.org/10.1109/TC.2023.3248282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix multiplication has always been a cornerstone in computer science. In fact, linear algebra tools permeate a wide variety of applications: from weather forecasting, to financial market prediction, radio signal processing, computer vision, and more. Since many of the aforementioned applications typically impose strict performance and/or fault tolerance constraints, the demand for fast and reliable matrix multiplication (MxM) is at an all-time high. Typically, increased reliability is achieved through redundancy. However, coarse-grain duplication incurs an often prohibitive overhead, higher than 100\%. Thanks to the peculiar characteristics of the MxM algorithm, more efficient algorithm-based hardening solutions have been designed to detect (and even correct) some types of errors with lower overhead. We show that, despite being more efficient, current solutions are still sub-optimal in certain scenarios, particularly when considering persistent faults in Field-Programmable Gate-Arrays (FPGAs). Based on a thorough analysis of the fault model, we propose an error detection technique for MxM that decreases both algorithmic and architectural costs by over a polynomial degree, when compared to existing algorithm-based strategies. Furthermore, we report arithmetic overheads at the application level to be under 1\% for three state-of-the-art Convolutional Neural Networks (CNNs).},
  archive      = {J_TC},
  author       = {Fabiano Libano and Paolo Rech and John Brunhaver},
  doi          = {10.1109/TC.2023.3248282},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2390-2403},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient error detection for matrix multiplication with systolic arrays on FPGAs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BlockExplorer: Exploring blockchain big data via parallel
processing. <em>TC</em>, <em>72</em>(8), 2377–2389. (<a
href="https://doi.org/10.1109/TC.2023.3248280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today&#39;s blockchain systems store detailed runtime information in the format of transactions and blocks, which are valuable not only to understand the finance of blockchain-based ecosystems but also to audit the security of on-chain applications. However, exploring this blockchain “Big Data” is challenging due to data heterogeneity and the huge amount. Existing blockchain exploration techniques are either incomplete or inefficient, making them inapt in time-sensitive applications. This paper presents ${\sf BlockExplorer}$ , an efficient and flexible blockchain exploration system for Ethereum. ${\sf BlockExplorer}$ builds on a master-slave architecture, where the master partitions all blocks into multiple non-overlapped sets and each slave simultaneously processes Ethereum Big Data based on a set of blocks. ${\sf BlockExplorer}$ implements a transaction-based partitioning approach to address load balance among slaves, and a code instrumentation approach to acquire complete Ethereum Big Data. The evaluation shows that ${\sf BlockExplorer}$ accelerates the data acquisition performance of the state-of-the-art by 4.1×, while the workload difference among slaves is up to 18\%. To demonstrate the application of ${\sf BlockExplorer}$ , we develop three apps upon ${\sf BlockExplorer}$ to detect real-life attacks against Ethereum and show that our apps can detect attacks in a large range of blocks (e.g., ten million) within a short time (e.g., multiple hours).},
  archive      = {J_TC},
  author       = {Shipeng Li and Jingwei Li and Yuxing Tang and Xiapu Luo and Zheyuan He and Zihao Li and Xi Cheng and Yang Bai and Ting Chen and Yuzhe Tang and Zhe Liu and Xiaosong Zhang},
  doi          = {10.1109/TC.2023.3248280},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2377-2389},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BlockExplorer: Exploring blockchain big data via parallel processing},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient integrity auditing mechanism with secure
deduplication for blockchain storage. <em>TC</em>, <em>72</em>(8),
2365–2376. (<a href="https://doi.org/10.1109/TC.2023.3248278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive nodes in a blockchain form an off-chain distributed storage network to provide storage resources for users to meet large data upload requirements. However, this storage approach introduces security and performance issues. Firstly, it is difficult to guarantee the integrity of the data uploaded, and these data may be easily corrupted or lost. Moreover, uploading excessive duplicate data leads to a waste of storage resources. In this study, to address these issues, with a double-copy storage model for blockchain off-chain storage, a novel public auditing scheme with client-side deduplication is proposed to reduce the storage overhead of nodes and check the integrity of the off-chain data. Based on smart contracts, our scheme could realize efficient user ownership and off-chain data integrity verification automatically. In addition, both data encryption and deduplication are achieved based on message-locked encryption and an improved authenticator generation algorithm. Security analysis and experimental comparisons show that the proposed scheme is effective and practical.},
  archive      = {J_TC},
  author       = {Qingyang Zhang and Dongfang Sui and Jie Cui and Chengjie Gu and Hong Zhong},
  doi          = {10.1109/TC.2023.3248278},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2365-2376},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient integrity auditing mechanism with secure deduplication for blockchain storage},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards complete and scalable emulation of quantum
algorithms on high-performance reconfigurable computers. <em>TC</em>,
<em>72</em>(8), 2350–2364. (<a
href="https://doi.org/10.1109/TC.2023.3248276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary quantum computers face many critical challenges that limit their usefulness for practical applications. A primary limiting factor is classical-to-quantum (C2Q) data encoding, which requires specific circuits for quantum state initialization. The required state initialization circuits are often complex and violate decoherence constraints, particularly for I/O intensive applications. Existing Noisy Intermediate-Scale Quantum (NISQ) devices are noise-sensitive and have low quantum bit (qubit) counts, thus limiting the applicability of C2Q circuits for encoding large and realistic datasets. This has made the study of complete and realistic circuits that include data encoding challenging and has also led to a heavy dependency on costly and resource-intensive simulations on classical platforms. In this work, we propose a cost-effective, classical-hardware-accelerated framework for realistic and complete emulation of quantum algorithms. The emulation framework incorporates components for the critical C2Q data encoding process, as well as architectures for quantum algorithms such as the quantum Haar transform (QHT). The framework is used to investigate optimizations for C2Q and QHT algorithms, and the corresponding optimized quantum circuits are presented. The framework is implemented on a High-Performance Reconfigurable Computer (HPRC) which emulates the proposed QHT circuits combined with proposed C2Q data encoding methods. For performance benchmarks, CPU-based emulations and simulations on a state-of-the-art quantum computing simulator are also carried out. Results show that the proposed hardware-accelerated emulation framework is more efficient in terms of speed and scalability compared to CPU-based emulation and simulation.},
  archive      = {J_TC},
  author       = {Esam El-Araby and Naveed Mahmud and Mingyoung Joshua Jeng and Andrew MacGillivray and Manu Chaudhary and Md. Alvir Islam Nobel and SM Ishraq Ul Islam and David Levy and Dylan Kneidel and Madeline R. Watson and Jack G. Bauer and Andrew E. Riachi},
  doi          = {10.1109/TC.2023.3248276},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2350-2364},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards complete and scalable emulation of quantum algorithms on high-performance reconfigurable computers},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AAPP: An accelerative and adaptive path planner for robots
on GPU. <em>TC</em>, <em>72</em>(8), 2336–2349. (<a
href="https://doi.org/10.1109/TC.2023.3248274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal path planning is one of the major bottlenecks for the effective navigation of robots working towards accomplishing complex missions. To overcome the bottleneck and support efficient applications, this paper presents AAPP , an accelerative and adaptive path planner based on RRT*, a popular path planning algorithm, on GPU, to alleviate four main performance limitations, i.e., bandwidth limitation, load imbalance, high computing complexity, and the choice of parameters. First, AAPP employs a data storage structure, named simplified compressed sparse rows (SCSR), to compress the large-scale map data and increase the utilization of bandwidth. Second, to exploit the computing performance of GPU, we propose a two-layer parallel framework for RRT* based on SCSR format, named TLRRT*, by using the dynamic parallelism technique. Third, aiming at the problems of parallel load imbalance and high computing complexity in TLRRT*, we further design a two-stage parallel framework, named TSRRT*, that fully exploits hardware heterogeneity (CPU/GPU) by scheduling tasks on CPU and GPU adaptively. Finally, we present optimizations for AAPP to adaptively select execution schemes and parameters. Experimental results on a heterogeneous CPU/GPU machine show that AAPP yields the speedup up to $22.72\times$ over the RRT* algorithm. Compared to the state-of-the-art, AAPP can handle large-scale datasets and obtain feasible solutions with shorter trajectory lengths.},
  archive      = {J_TC},
  author       = {Juan Liu and Guoqing Xiao and Fan Wu and Xiangke Liao and Kenli Li},
  doi          = {10.1109/TC.2023.3248274},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2336-2349},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AAPP: An accelerative and adaptive path planner for robots on GPU},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hitchhiker: Accelerating ORAM with dynamic scheduling.
<em>TC</em>, <em>72</em>(8), 2321–2335. (<a
href="https://doi.org/10.1109/TC.2023.3248272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oblivious RAM (ORAM) remains a bittersweet protection of memory access patterns because of its prohibitively high overhead. The root cause is that ORAM hides intended accesses among a sufficiently large number of dummy accesses. Most existing optimizations mitigate memory accesses using architectural enhancements (e.g., cache) yet few of them improve the efficiency of ORAM primitives per se. In this paper, we identify path-grained static scheduling as a fundamental ORAM performance bottleneck. We propose level-grained dynamic scheduling that directly optimizes ORAM primitives to boost efficiency. It enables ORAM to service more than one request per path and write paths batch wise. We can thus boost ORAM efficiency through handling queued requests as soon as possible and remove as many redundant accesses as possible. Since optimized memory accesses still target the same set of paths, dynamic scheduling preserves ORAM security. We implement dynamic scheduling through Hitchhiker ORAM. In comparison with the state-of-the-art primitive-optimized Fork Path ORAM, Hitchhiker ORAM yields 31.5\% fewer memory accesses, 60.2\% shorter latency, and 40.7\% less energy consumption, being 2.5× faster. In comparison with the state-of-the-art architecture-optimized $\rho$ , Hitchhiker ORAM is 1.5× faster and the integrated version— $\rho$ -Hitchhiker ORAM is 2.0× faster.},
  archive      = {J_TC},
  author       = {Jingsen Zhu and Mengming Li and Xingjian Zhang and Kai Bu and Miao Zhang and Tianqi Song},
  doi          = {10.1109/TC.2023.3248272},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2321-2335},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hitchhiker: Accelerating ORAM with dynamic scheduling},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TREEHOUSE: A secure asset management infrastructure for
protecting 3DIC designs. <em>TC</em>, <em>72</em>(8), 2306–2320. (<a
href="https://doi.org/10.1109/TC.2023.3248269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The push to meet growing user requirements and manufacturing challenges at lower technology nodes have motivated chip designers to adopt non-traditional design techniques. 2.5D/3DIC stacking has gained popularity in recent years since it enables chip manufacturers to integrate complex IPs to meet user demands without incurring design penalties. However, the non-traditional nature of the supply chain also means that additional challenges exist for verification and testing of the manufactured design, making the trust assurance of these designs an extremely challenging proposition. While there have been works focussing on securing 3DIC designs, very few address a completely untrusted supply chain. A robust security countermeasure must address the diverse trust requirements of the IPs in the design and the distributed supply chain requirements while ensuring that the functionality and performance overheads of the IC are not violated. We present TREEHOUSE , a trust assurance solution to counter piracy, reverse-engineering, and counterfeiting attacks. TREEHOUSE uses scan authentication to detect piracy and counterfeiting, scan-and functional-locking to prevent reverse-engineering. We evaluate the efficiency of our proposed scheme on an example 3DIC design. We show that TREEHOUSE incurs less than 1\% area and power overheads while incurring less than 1\% increase in overall gate count for each layer.},
  archive      = {J_TC},
  author       = {Patanjali SLPSK and Sandip Ray and Swarup Bhunia},
  doi          = {10.1109/TC.2023.3248269},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2306-2320},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TREEHOUSE: A secure asset management infrastructure for protecting 3DIC designs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive test pattern generation approach exploiting
the SAT attack for logic locking. <em>TC</em>, <em>72</em>(8),
2293–2305. (<a href="https://doi.org/10.1109/TC.2023.3248268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for reducing manufacturing defect escape in today&#39;s safety-critical applications requires increased fault coverage. However, generating a test set using commercial automatic test pattern generation (ATPG) tools that lead to zero-defect escape is still an open problem. It is challenging to detect all stuck-at faults to reach 100\% fault coverage. In parallel, the hardware security community has been actively involved in developing solutions for logic locking to prevent IP piracy. In logic locking, locks are inserted in different locations of the netlist to modify the original functionality. Unless the correct key is programmed into the IC, the circuit functions incorrectly. Unfortunately, the Boolean satisfiability (SAT) based attack, introduced in (Subramanyan et al. 2015), can determine the secret key efficiently, and break different logic locking schemes. In this article, we propose a novel test pattern generation approach using the powerful SAT attack on logic locking. A stuck-at fault is modeled as a locked gate with a secret key, where it can effectively deduce the satisfiable assignment with reduced backtracks under key initialization of the SAT attack. The input pattern that determines the key is a test for the stuck-at fault. We propose two different approaches for test pattern generation. First, a single stuck-at fault is targeted, and a corresponding locked circuit with one key bit is created. This approach generates one test pattern per fault. Second, we consider a group of faults and convert the circuit to its locked version with multiple key bits. The inputs obtained from the SAT attack tool are the test set for detecting this group of faults. Our approach can find test patterns for all hard-to-detect faults that were previously undetected in commercial ATPG tools. The proposed test pattern generation approach can efficiently detect redundant faults as well. We demonstrate the effectiveness of the approach on ITC’99 benchmarks. The results show that we can detect all the hard-to-detect faults and identify redundant faults and a 100\% stuck fault coverage is achieved. In addition, we show that test generation time saving becomes significant for Approach 2 as multiple faults help reduce or remove conflicts.},
  archive      = {J_TC},
  author       = {Yadi Zhong and Ujjwal Guin},
  doi          = {10.1109/TC.2023.3248268},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2293-2305},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A comprehensive test pattern generation approach exploiting the SAT attack for logic locking},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdEle+: An adaptive congestion-and-energy-aware elevator
selection for partially connected 3D networks-on-chip. <em>TC</em>,
<em>72</em>(8), 2278–2292. (<a
href="https://doi.org/10.1109/TC.2023.3248260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical die stacking of 3D Networks-on-Chip (3D NoCs) is enabled using inter-layer Through-Silicon-Via (TSV) links. However, TSV technology suffers from low reliability and high fabrication costs. To mitigate these costs, Partially Connected 3D NoCs (PC-3DNoCs), which use fewer TSV links, have been introduced. Nevertheless, with fewer vertical links (a.k.a. elevators), elevator-less routers will have to send their traffic to nearby elevators for inter-layer traffic, increasing the traffic load and congestion at these elevators and potentially reducing performance. Therefore, it is important that elevator-less routers choose elevators that balance the traffic load among the available elevators. To address this problem, we present an adaptive congestion- and energy-aware elevator-selection algorithm, called AdEle+. AdEle+ employs an offline multi-objective simulated-annealing-based optimization to find good elevator subsets for routers. During high traffic loads, AdEle+ uses an adaptive and online elevator selection algorithm to select an elevator from the elevator subset to dynamically manage traffic congestion on elevators. Moreover, in low congestion circumstances, AdEle+ switches to a distance-based selection to improve energy efficiency. Compared to state-of-the-art selection algorithms under various PC-3DNoC configurations and traffic patterns, AdEle+ reduces the average latency by 9.5\% on average and up to 11.2\% while reducing the hardware overhead by 10.1\%},
  archive      = {J_TC},
  author       = {Ebadollah Taheri and Ryan Gary Kim and Mahdi Nikdast},
  doi          = {10.1109/TC.2023.3248260},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2278-2292},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AdEle+: An adaptive congestion-and-energy-aware elevator selection for partially connected 3D networks-on-chip},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new federated scheduling algorithm for arbitrary-deadline
DAG tasks. <em>TC</em>, <em>72</em>(8), 2264–2277. (<a
href="https://doi.org/10.1109/TC.2023.3244632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A parallel task can always be modelled as a directed acyclic graph (DAG), where sequential instruction blocks are modelled as vertices and data dependencies or resource constraints are modelled as edges. We propose a new federated scheduling algorithm for arbitrary-deadline sporadic DAG tasks, assuming that the exact structures of DAG tasks are unknown before runtime. Federated scheduling algorithms are a class of algorithms that can efficiently schedule DAG tasks by assigning several processors exclusively to each task. Existing studies have shown the advantages of federated scheduling, which include increasing the analytical schedulability and minimising the scheduling overhead. We are particularly focused on the scheduling of any task with a deadline longer than its release period; in this case, multiple jobs generated by the task could run concurrently. For such tasks, our algorithm is different from most federated scheduling algorithms in that it assigns dedicated processors to each job instead of letting jobs released by the same task share processors. The main idea is to increase the analytical schedulability by avoiding interference between jobs. The simulation results show that our algorithm outperforms existing algorithms when the exact structures of tasks are unknown before runtime.},
  archive      = {J_TC},
  author       = {Fei Guan and Long Peng and Jiaqing Qiao},
  doi          = {10.1109/TC.2023.3244632},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2264-2277},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A new federated scheduling algorithm for arbitrary-deadline DAG tasks},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Operand-oriented virtual memory support for near-memory
processing. <em>TC</em>, <em>72</em>(8), 2250–2263. (<a
href="https://doi.org/10.1109/TC.2023.3243881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual memory support is one of the major challenges of near-memory processing (NMP). Many previous works focused on this issue, but there are practical limitations that conventional CPU hardware or memory allocation schemes should be modified. Another technique uses a specialized page table for NMP to avoid such limitations. However, the previous work proposed NMP-specific page table that has static page table walk latency regardless of data size. This causes unnecessarily long address translation time for relatively small data. In this paper, we propose an operand-oriented technique for virtual memory support. Our scheme does not pre-determine the size of shared space; rather, it allocates shared space depending on the size of operands data for NMP. Then, we significantly reduce page table walk latency by using our flexible page table, which adapts the page table hierarchy to the size of shared spaces. To prove our concept, we implement our scheme in a full-system simulator and an FPGA-based verification platform. We then compared it with CPU&#39;s page table and the previous NMP-specific page table. The experimental results show that our technique outperforms page table walk latency by 69.3 percent and 43.8 percent compared to the CPU&#39;s page table and the comparison, respectively.},
  archive      = {J_TC},
  author       = {Duheon Choi and Taeyang Jeong and Joonhyeok Yeom and Eui-Young Chung},
  doi          = {10.1109/TC.2023.3243881},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2250-2263},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Operand-oriented virtual memory support for near-memory processing},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A variation-aware quantum circuit mapping approach based on
multi-agent cooperation. <em>TC</em>, <em>72</em>(8), 2237–2249. (<a
href="https://doi.org/10.1109/TC.2023.3242208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum circuit mapping is an essential process required by executing quantum circuits using a noisy intermediate-scale quantum (NISQ) device. Since qubits and quantum gates of a NISQ device are error-prone and variable in quality, it is crucial to choose qubits or quantum gates in a variation-aware manner to maximize the success rate of executing circuits. To this end, this article proposes a variation-aware method for quantum circuit mapping through the cooperation of multiple agents. Each agent in the proposed method can gradually construct a physical circuit that respects the device&#39;s connectivity constraints by inserting a SWAP gate at each step. Moreover, at each step, the circuit information of each agent is shared within the agent population through a communication mechanism that combines global and local information exchange, so that agents with poor fitness can get an opportunity to improve their physical circuits. The experimental results on extensive benchmark circuits confirm that the proposed method can effectively and consistently improve the overall circuit fidelity compared with the state-of-the-art methods.},
  archive      = {J_TC},
  author       = {Pengcheng Zhu and Weiping Ding and Lihua Wei and Xueyun Cheng and Zhijin Guan and Shiguang Feng},
  doi          = {10.1109/TC.2023.3242208},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2237-2249},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A variation-aware quantum circuit mapping approach based on multi-agent cooperation},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hydra: Deadline-aware and efficiency-oriented scheduling for
deep learning jobs on heterogeneous GPUs. <em>TC</em>, <em>72</em>(8),
2224–2236. (<a href="https://doi.org/10.1109/TC.2023.3242200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid proliferation of deep learning (DL) jobs running on heterogeneous GPUs, scheduling DL jobs to meet various scheduling requirements, such as meeting deadlines and reducing job completion time (JCT), is critical. Unfortunately, existing efficiency-oriented and deadline-aware efforts are still rudimentary. They lack the capability of scheduling jobs to meet deadline requirements while reducing total JCT, especially when the jobs have various execution times on heterogeneous GPUs. Therefore, we present Hydra, a novel quantitative cost comparison approach, to address this scheduling issue. Here, the cost represents the total JCT plus a dynamic penalty calculated from the total tardiness (i.e., the delay time of exceeding the deadline) of all jobs. Hydra adopts a sampling approach that exploits the inherent iterative periodicity of DL jobs to estimate job execution times accurately on heterogeneous GPUs. Then, Hydra considers various combinations of job sequences and GPUs to obtain the minimized cost by leveraging an efficient branch-and-bound algorithm. Finally, the results of evaluation experiments on Alibaba traces show that Hydra can reduce total tardiness by 85.8\% while reducing total JCT as much as possible, compared with state-of-the-art efforts.},
  archive      = {J_TC},
  author       = {Zichao Yang and Heng Wu and Yuanjia Xu and Yuewen Wu and Hua Zhong and Wenbo Zhang},
  doi          = {10.1109/TC.2023.3242200},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2224-2236},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hydra: Deadline-aware and efficiency-oriented scheduling for deep learning jobs on heterogeneous GPUs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empowering authenticated and efficient queries for STK
transaction-based blockchains. <em>TC</em>, <em>72</em>(8), 2209–2223.
(<a href="https://doi.org/10.1109/TC.2023.3241263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the attractive properties of decentralization, unforgeability, transparency, and traceability, blockchain is increasingly being used in various scenarios such as supply chain and public services, where massive Spatial-Temporal-Keywords (STK) transactions need to be packaged. However, due to the multi-dimensionality and randomness of STK transactions, existing solutions fail to enable queries in a verifiable and efficient way for blockchains storing multidimensional transactions. To this end, this article takes the first step to propose an authenticated and efficient query approach in hybrid blockchain systems consisting of on-chain and off-chain parts. We first design a data structure named MRK-Tree in the block body, which organizes STK transactions for efficient nodes pruning of both kNN and range queries. Then we propose an improved block header, which improves the efficient pruning of blocks on the basis of ensuring the authentication of query results. Also, we design a cross-block searching algorithm named Efficient Block Pruning (EBP) and intra-block searching algorithms named Authenticated kNN/Range Query (AKQ/ARQ) to accelerate authenticated queries for multiple MRK-Trees in the hybrid blockchain systems. Authentication mechanisms are proposed to ensure the soundness and completeness of query results. Rigorous security analysis validates the practicability of the proposed approach. We build a blockchain prototype to comprehensively evaluate the performance of proposed query schemes. Extensive evaluation results with real datasets reveal that our approach can ensure authenticated queries, meanwhile improving the time efficiency by up to 36.45x and space efficiency by up to 4 orders of magnitude compared with the well-known benchmark query schemes.},
  archive      = {J_TC},
  author       = {Hao Xu and Bin Xiao and Xiulong Liu and Li Wang and Shan Jiang and Weilian Xue and Jianrong Wang and Keqiu Li},
  doi          = {10.1109/TC.2023.3241263},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2209-2223},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Empowering authenticated and efficient queries for STK transaction-based blockchains},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trouble-shooting at GAN point: Improving functional safety
in deep learning accelerators. <em>TC</em>, <em>72</em>(8), 2194–2208.
(<a href="https://doi.org/10.1109/TC.2023.3241218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Deep Neural Networks (DNNs) in real-time mission critical applications has promoted the implementation of custom-built DNN inference accelerators. These accelerators require a considerable amount of on-chip memory to store millions of trained DNN parameters for executing inference at the edge. Drastic technology scaling in recent years have made these memory circuits highly vulnerable to faults due to various reasons like aging, latent defects, single event upsets, etc. Such faults are highly detrimental to the classification accuracy of the DNN accelerator, leading to the crucial Functional Safety (FuSa) violation. This can eventuate to catastrophic circumstances, when used in mission-critical applications. In order to detect such violations in mission mode, we propose to generate a set of functional test patterns by leveraging the concept of Generative Adversarial Networks (GANs), that are independent of the DNN model and the accelerator characteristics. Our experimental results demonstrate that, the generated test patterns significantly improve FuSa violation detection coverage by up to 130.28\%, compared to existing techniques. To the best of our knowledge, this is the first work that generates GAN-based test patterns in order to perform FuSa violation detection in mission-critical DNN accelerators.},
  archive      = {J_TC},
  author       = {Shamik Kundu and Suvadeep Banerjee and Arnab Raha and Fei Su and Suriyaprakash Natarajan and Kanad Basu},
  doi          = {10.1109/TC.2023.3241218},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2194-2208},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Trouble-shooting at GAN point: Improving functional safety in deep learning accelerators},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding and mitigating twin function misuses in
operating system kernel. <em>TC</em>, <em>72</em>(8), 2181–2193. (<a
href="https://doi.org/10.1109/TC.2023.3240365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major operating system kernels expose twin functions, which are groups of internal primitives that have mostly common but slightly diverging semantics, to kernel modules and subsystems. They are created to make the basic primitives work well in various scenarios. Unfortunately, though being expected as solutions, twin functions may turn to problem-makers in practice. As we have observed from over 500 patches applied to upstream Linux and FreeBSD, developers choose an improper one from the twins, leaving the kernel with stability and security bugs as well as error-prone code. In this paper, we aim to understand and mitigate the twin function misuse problem. First, we provide an informative discussion on the misuse-fix patches. We find that violating the constraints from calling context, missing the primitives with better performance, lacking the necessary security enhancements, and breaking the kernel coding style are the four major factors that lead to misuse. We then identify the programming rules from the patches and apply them with a static program analysis tool extended from Coccinelle, including callgraph tainting and type-based function pointer resolving. We have 136 patches accepted by the Linux community and fix 320 new misuses in the upstream Linux kernel.},
  archive      = {J_TC},
  author       = {Jinyu Gu and Jiacheng Shi and Haroran Su and Wentai Li and Binyu Zang and Haibing Guan and Haibo Chen},
  doi          = {10.1109/TC.2023.3240365},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2181-2193},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Understanding and mitigating twin function misuses in operating system kernel},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-enhanced decentralized federated learning at dynamic
edge. <em>TC</em>, <em>72</em>(8), 2165–2180. (<a
href="https://doi.org/10.1109/TC.2023.3239542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized Federated Learning (DeFL) plays a critical role in improving effectiveness of training and has been proved to give great scope to the development of edge computing. However, on the one hand, inaccessibility of private data and excessively exploiting the data throughout the learning process have become a public concern, and on the other hand the connections between server-less edge devices are always varying due to the mobility of edge intelligent devices. To address the above issues, we propose a P rivacy- E nhanced - D ynamic - D ecentralized - F ederated - L earning algorithm called PED $ ^{2}$ FL in a dynamic edge environment. We design the PED $ ^{2}$ FL under the analog transmission scheme, where mobile edge devices transmit privacy preserving data simultaneously and accomplish efficient information aggregation with doubly-stochastic adjacent matrices. With thorough analysis, it can be demonstrated that PED $ ^{2}$ FL satisfies $(\epsilon,\delta)$ -differential privacy while the per-device privacy budget decays exponentially with the number of the neighbors, which greatly improved the data utility compared to the fixed budget in the orthogonal transmission strategy. PED $ ^{2}$ FL has the same convergence rate $\mathcal {O}(\sqrt{\frac{1}{KN}})$ as the non-private decentralized learning algorithm D-PSGD without enhanced privacy protection, where $K$ and $N$ are the total iterations and the number of nodes, respectively. Extensive experiments show that algorithm PED $ ^{2}$ FL also performs well with real-world settings.},
  archive      = {J_TC},
  author       = {Shuzhen Chen and Yangyang Wang and Dongxiao Yu and Ju Ren and Congan Xu and Yanwei Zheng},
  doi          = {10.1109/TC.2023.3239542},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2165-2180},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Privacy-enhanced decentralized federated learning at dynamic edge},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Spin-variable reduction method for handling linear equality
constraints in ising machines. <em>TC</em>, <em>72</em>(8), 2151–2164.
(<a href="https://doi.org/10.1109/TC.2023.3239539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a spin-variable reduction method for Ising machines to handle linear equality constraints in a combinatorial optimization problem. Ising machines including quantum-annealing machines can effectively solve combinatorial optimization problems. They are designed to find the lowest-energy solution of a quadratic unconstrained binary optimization (QUBO), which is mapped from the combinatorial optimization problem. The proposed method reduces the number of binary variables to formulate the QUBO compared to the conventional penalty method. We demonstrate a sufficient condition to obtain the optimum of the combinatorial optimization problem in the spin-variable reduction method and its general applicability. We apply it to typical combinatorial optimization problems, such as the graph $k$ -partitioning problem and the quadratic assignment problem. Experiments using simulated-annealing and quantum-annealing based Ising machines demonstrate that the spin-variable reduction method outperforms the penalty method. The proposed method extends the application of Ising machines to larger-size combinatorial optimization problems with linear equality constraints.},
  archive      = {J_TC},
  author       = {Tatsuhiko Shirai and Nozomu Togawa},
  doi          = {10.1109/TC.2023.3239539},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2151-2164},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Spin-variable reduction method for handling linear equality constraints in ising machines},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient fault-tolerant consensus for collaborative
services in edge computing. <em>TC</em>, <em>72</em>(8), 2139–2150. (<a
href="https://doi.org/10.1109/TC.2023.3238138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many edge computing applications, edge devices are required to reach fault-tolerant consensus in order to provide collaborative services in outdoor environments. In this paper, we study a comprehensive $(a,b)$ -majority consensus problem based on a novel failure model, which takes $a$ distinct opinions as inputs and outputs a $b$ -majority opinion as the final agreement. This problem formulation is drastically different from traditional ones, which usually require a majority consensus from the binary opinions of multiple supporters. It is more practical and flexible as it can accommodate more than 2 input opinions and output one that satisfies the application requirement defined by parameter $b$ . We also consider physical layer in our failure model while previous models mainly focus on faults occurred in protocol layer and data layer. Based on this more realistic failure model and a more practical consensus problem definition, we present a distributed protocol for $n$ edge devices to reach an $(a,b)$ -majority consensus within $\Theta (n)$ time steps with high probability. Empirical results from our simulation studies validate the fault tolerance property and efficiency of our work in achieving the $(a,b)$ -majority consensus.},
  archive      = {J_TC},
  author       = {Guanlin Jing and Yifei Zou and Dongxiao Yu and Chuanwen Luo and Xiuzhen Cheng},
  doi          = {10.1109/TC.2023.3238138},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2139-2150},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient fault-tolerant consensus for collaborative services in edge computing},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Massively parallel circuit setup in GPU-SPICE. <em>TC</em>,
<em>72</em>(8), 2127–2138. (<a
href="https://doi.org/10.1109/TC.2020.3032343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SPICE simulations are the industry standard to analyze circuits for decades. However, they are computationally complex as each circuit is simulated at the transistor-level where individual transistor is modeled with dozens of sophisticated equations. This limits the practicality of SPICE simulations to relatively small circuits. However, this is in a direct conflict with the ever-increasing demands of circuit designers in which SPICE simulations for large circuits (e.g., DSPs, AES, etc.) at full accuracy are inevitably required to fulfill new industrial standards like automotive safety ISO 26262 with tool confidence level 1. To accelerate SPICE simulation without sacrificing accuracy, state-of-the-art approaches have started to employ GPUs to parallelize the LU-factorization and device linearization phases. Instead of focusing on these phases, this article demonstrates for the first time that when large circuits come into play, a new and equally important performance bottleneck emerges at the circuit setup phase. Speeding up the circuit setup phase in SPICE is our key focus in this paper. Our two implementations demonstrate that our GPU-based circuit setup reduces the analysis time from 4.5 days to merely 89 seconds for a 256-bit multiplier, which consists of more than 1M transistors. Our achieved speedup is 4396x compared to the baseline (open-source NGSPICE) and more than 2x compared to commercial (HSPICE and Spectre) SPICE circuit setup.},
  archive      = {J_TC},
  author       = {Victor M. van Santen and Fu Lam Florian Diep and Jörg Henkel and Hussam Amrouch},
  doi          = {10.1109/TC.2020.3032343},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {2127-2138},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Massively parallel circuit setup in GPU-SPICE},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exact and approximate squarers for error-tolerant
applications. <em>TC</em>, <em>72</em>(7), 2120–2126. (<a
href="https://doi.org/10.1109/TC.2022.3228592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing is considered an innovative paradigm with wide applications to high performance and low power systems. These applications have relaxed requirements for accuracy, so they can tolerate errors in results and achieve high performance. In approximate computing, multipliers have been widely studied, but squarers (as similar schemes) have not received much attention. In this paper, an accurate squarer is designed based on a Radix-8 Booth-folding square algorithm to reduce the number of partial products and the depth of the partial product array. Several approximate squarers (R8AS1, R8AS2 and R8AS3) are proposed based on the exact squarer to reduce power and delay. Two approximate partial product generators are also designed to simplify the Radix-8 Booth square encoder in R8AS1 and R8AS2. In addition, approximate compressors with compensation are used in the partial product compression stage to reduce additional area and power consumption in R8AS3. Synthesis results for power, area, and delay at 28nm CMOS technology are presented. Compared with designs in the technical literature with the same accuracy, the proposed 16-bit designs reduce the PDP by 37\%; in general, the PDP is decreased by up to 51\%. Finally, the proposed approximate squarers are implemented in a square-law detector as a communication application and achieve an SNR close to 30dB. Also, the three proposed approximate squarers are applied to the k-means clustering algorithm for machine learning to accomplish high performance in classification.},
  archive      = {J_TC},
  author       = {Ke Chen and Chenyu Xu and Haroon Waris and Weiqiang Liu and Paolo Montuschi and Fabrizio Lombardi},
  doi          = {10.1109/TC.2022.3228592},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {2120-2126},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exact and approximate squarers for error-tolerant applications},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructing high radix quotient digit selection tables for
SRT division and square root. <em>TC</em>, <em>72</em>(7), 2111–2119.
(<a href="https://doi.org/10.1109/TC.2023.3235978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High radix SRT division plays an important role in contemporary microprocessors as the quotient digit selection tables effectively reduce the computation complexity of the quotient digits. The quotient digit selection table is constructed according to the rounded lower and upper bounds of the overlapping regions in the traditional method. The table construction process lacks in mathematical rigor and consequently is susceptible to error. This paper proposes an algebraic method for computing the quotient digit selection tables. We characterize the quotient digit selection functions to construct quotient digit selection tables required for SRT division and SRT square root with any valid redundancy. The functions include the maximum and minimum legal quotient digit selections. We compute the truncations of the remainder $p$ and divisor $d$ when $d \in [1,2)$ . We implement procedures to compute quotient digit selection tables by our functions. The computation of the quotient digit selection table for the case radix-4 with the quotient digit set $[-2,2]$ is presented by using the minimum quotient digit selection function. Our functions can compute quotient digit selection tables in the design phase of SRT division and square root by given radix and redundancy.},
  archive      = {J_TC},
  author       = {Zixuan Liu and Xiaoyu Song and Zhuowei Wang and Yan Wang and Jian-Tao Zhou},
  doi          = {10.1109/TC.2023.3235978},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {2111-2119},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Constructing high radix quotient digit selection tables for SRT division and square root},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Engineering practical rank-code-based cryptographic schemes
on embedded hardware. A case study on ROLLO. <em>TC</em>,
<em>72</em>(7), 2094–2110. (<a
href="https://doi.org/10.1109/TC.2022.3225080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the practical performance of rank-code based cryptography on FPGA platforms by presenting a case study on the quantum-safe KEM scheme based on LRPC codes called ROLLO, which was among NIST post-quantum cryptography standardization round-2 candidates. Specifically, we present an FPGA implementation of the encapsulation and decapsulation operations of the ROLLO KEM scheme with some variations to the original specification. The design is fully parameterized, using code-generation scripts to support a wide range of parameter choices for security levels specified in ROLLO. At the core of the ROLLO hardware, we presented a generic approach for hardware-based Gaussian elimination, which can process both non-singular and singular matrices. Previous works on hardware-based Gaussian elimination can only process non-singular ones. However, a plethora of cryptosystems, for instance, quantum-safe key encapsulation mechanisms based on rank-metric codes, ROLLO and RQC, which are among NIST post-quantum cryptography standardization round-2 candidates, require performing Gaussian elimination for random matrices regardless of the singularity. To the best of our knowledge, this work is the first hardware implementation for rank-code-based cryptographic schemes. The experimental results suggest rank-code-based schemes can be highly efficient.},
  archive      = {J_TC},
  author       = {Jingwei Hu and Wen Wang and Kris Gaj and Liping Wang and Huaxiong Wang},
  doi          = {10.1109/TC.2022.3225080},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {2094-2110},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Engineering practical rank-code-based cryptographic schemes on embedded hardware. a case study on ROLLO},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <span class="math inline"><em>O</em>(<em>N</em>)</span>O(n)
memory-free hardware architecture for burrows-wheeler transform.
<em>TC</em>, <em>72</em>(7), 2080–2093. (<a
href="https://doi.org/10.1109/TC.2022.3226295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel hardware architecture for Burrows-Wheeler Transform (BWT) scheme is presented. The core idea is to have a memory-free strategy that does not involve any software overhead during BWT operation. This is achieved by introducing a register-file concept and utilizing basic digital logic circuits to perform the entire BWT operation. Additionally, this is a kind of transformation scheme that does not utilize any kind of matrix during transformation, and thereby, it is free from run-time memory consumption. It efficiently handles the string terminating mechanism in the proposed design without involving any extra terminating symbol. This string terminator-free architecture eventually reduces additional operation and storage space to maintain the string, and thereby, the architecture does not necessitate any register read and write operations. This architecture exhibits efficient transformation without involving any indexing method or sorting mechanism during an inverse transformation operation. This architecture achieves $O(N)$ time complexity compared to $O(N^{2})$ and $O(N \log N)$ as experienced by the existing state-of-the-art approaches.},
  archive      = {J_TC},
  author       = {Surajeet Ghosh and Sanchita Saha Ray},
  doi          = {10.1109/TC.2022.3226295},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {2080-2093},
  shortjournal = {IEEE Trans. Comput.},
  title        = {${O(N)}$O(N) memory-free hardware architecture for burrows-wheeler transform},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Investigating black-box function recognition using hardware
performance counters. <em>TC</em>, <em>72</em>(7), 2065–2079. (<a
href="https://doi.org/10.1109/TC.2022.3226302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents new methods and results for recognising black-box program functions using hardware performance counters (HPC), where an investigator can invoke and measure function calls. Important use cases include analysing compiled libraries, e.g., static and dynamic link libraries, and trusted execution environment (TEE) applications. We develop a generic approach to classify a comprehensive set of hardware events, e.g., branch mis-predictions and instruction retirements, to recognise standard benchmarking and cryptographic library functions. This includes various signing, verification and hash functions, and ciphers in numerous modes of operation. Three architectures are evaluated using off-the-shelf Intel/X86-64, ARM, and RISC-V CPUs. Next, we show that several known CVE-numbered OpenSSL vulnerabilities can be detected using HPC differences between patched and unpatched library versions. Further, we demonstrate that standardised cryptographic functions within ARM TrustZone TEE applications can be recognised using non-secure world HPC measurements, applying to platforms that insecurely perturb the performance monitoring unit (PMU) during TEE execution. High accuracy was achieved in all cases (86.22–99.83\%) depending on the application, architectural, and compilation assumptions. Lastly, we discuss mitigations, outstanding challenges, and directions for future research.},
  archive      = {J_TC},
  author       = {Carlton Shepherd and Benjamin Semal and Konstantinos Markantonakis},
  doi          = {10.1109/TC.2022.3226302},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {2065-2079},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Investigating black-box function recognition using hardware performance counters},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-block: Exploiting cross-layer and direct storage access
for decentralized blockchain storage systems. <em>TC</em>,
<em>72</em>(7), 2052–2064. (<a
href="https://doi.org/10.1109/TC.2022.3226305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized storage systems such as blockchain storage applications adopt the distributed storage technology and use distributed storage nodes to store the persistent data. For each off-chain storage node, keyvalue (KV) stores are normally used to manage data. As the most common data structure for KV store, Log Structured Merge Tree (LSM-Tree) eliminates random write operations and keeps acceptable read performance. Although LSM-Tree-based decentralized storage system can provide a secure and reliable storage platform, the unique feature of blockchain applications is not fully exploited. In blockchain storage applications, the generation of keys for KV stores is based on the encrypted data. A physical block in a solid-state drive (SSD) could be filled with data from different system users. This mixture of workloads will lead to the inefficient usage of physical spaces in the SSD and cause extra compaction operations for LSM-Tree. This paper presents Meta-Block, a cross-layer and efficient storage management strategy for decentralized blockchain storage applications. The objective is to capture the features of blockchain storage applications and reduce unnecessary read and write operations across different storage layers. As a cross-layer design, Meta-Block redesigns the organization of LSM-Tree, which can effectively reduce the write amplification. We also design a data prefetching strategy to speed up the indexing and enable direct storage access. We demonstrate the viability of the proposed technique using a set of extensive experiments. Experimental results show that Meta-Block can effectively reduce the write amplification and extend the lifetime of SSDs in comparison with representative schemes.},
  archive      = {J_TC},
  author       = {Yi Wang and Jing Liao and Jing Yang and Zhengda Li and Chenlin Ma and Rui Mao},
  doi          = {10.1109/TC.2022.3226305},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {2052-2064},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Meta-block: Exploiting cross-layer and direct storage access for decentralized blockchain storage systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling cyber-physical production systems with SystemC-AMS.
<em>TC</em>, <em>72</em>(7), 2039–2051. (<a
href="https://doi.org/10.1109/TC.2022.3226567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneous nature of SystemC-AMS makes it a perfect candidate solution to support Cyber-Physical Production Systems (CPPSs), i.e., systems that are characterized by a tight interaction of the cyber part with the surrounding physical world and with manufacturing production processes. Nonetheless, the support for the modeling of physical and mechanical dynamics typical of production machinery goes far beyond the initial application scenario of SystemC-AMS, thus limiting its effectiveness and adoption in the production and manufacturing context. This paper starts with an analysis of the current adoption of SystemC-AMS to highlight the open points that still limit its effectiveness, with the goal of pinpointing current issues and to propose solutions that could improve its effectiveness, and make SystemC-AMS an essential resource also in the new Industry 4.0 scenario.},
  archive      = {J_TC},
  author       = {Enrico Fraccaroli and Sara Vinco},
  doi          = {10.1109/TC.2022.3226567},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {2039-2051},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Modeling cyber-physical production systems with SystemC-AMS},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-efficient 3-d data collection forMulti-UAV assisted
mobile crowdsensing. <em>TC</em>, <em>72</em>(7), 2025–2038. (<a
href="https://doi.org/10.1109/TC.2022.3227869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile CrowdSensing (MCS) is an emerging paradigm that employs massive mobile devices (MDs) to complete sensing tasks cooperatively. To provide ubiquitous MCS services, Unmanned Aerial Vehicle (UAV), featured by high agility and flexibility, becomes increasingly attractive as a powerful assistant for MCS to collect sensing data in hard-to-reach and infrastructure-restrained areas. Focusing on urban MCS scenarios where a tremendous amount of data needs to be uploaded by massive mobile devices, we propose a Three-Dimensional Multi-UAV assisted crowdsensing, termed 3DM, to collect sensing data efficiently in an infrastructure-free manner. Different from the existing methods, 3DM has two unique advantages: 1) removing the assumption of the ideal distributions of mobile devices and 2) fully exploiting the 3D flexibility to optimize the device matching and data transmission between UAVs and MDs. By employing a joint optimization metric that incorporates both energy efficiency and collection latency, 3DM dynamically maintains cost-effective UAV-MD links and 3D UAVs trajectories thus completes the collection tasks with less time and energy. Compared with the baseline algorithm and two state-of-the-art counterparts, extensive simulations demonstrate that 3DM saves at least 50\% energy and 25\% time of baseline while achieving 76\% improvement of the sub-optimal competitor on overall utility.},
  archive      = {J_TC},
  author       = {Luwei Fu and Zhiwei Zhao and Geyong Min and Wang Miao and Liang Zhao and Wenjie Huang},
  doi          = {10.1109/TC.2022.3227869},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {2025-2038},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy-efficient 3-D data collection forMulti-UAV assisted mobile crowdsensing},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fault-free: A framework for analysis and mitigation of
stuck-at-fault on realistic ReRAM-based DNN accelerators. <em>TC</em>,
<em>72</em>(7), 2011–2024. (<a
href="https://doi.org/10.1109/TC.2022.3227871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resistive RAM(ReRAM) is gaining attention as a suitable memory platform for accelerating deep neural networks(DNNs) in an energy-efficient way. However, energy-efficient ReRAM-based DNN accelerators suffer from serious Stuck-At-Fault(SAF) issues that significantly degrade the inference accuracy. SAF is a device-level non-ideality, and the problems of SAF worsen in the realistic ReRAM with low cell resolution. To address the problem in the realistic ReRAM, we present a framework for mitigating SAF on ReRAM-based accelerators(Fault-free). We first analyze the impact of SAF on low-resolution cells. Based on the analysis, we present offline compilation, which drastically reduces the impact of SAF on inference accuracy. At the first stage, we extract indices of distorted weights due to SAF. For extracted weights, fault-aware weight decomposition and closest value mapping are applied to minimize the error of weights. In the online phase, the target DNN model is executed on the ReRAM-based accelerator along with lightweight compensation units. The online compensation is selectively performed for a small portion of weights to reduce the hardware overhead. With the proposed framework, the ReRAM-based accelerator successfully ensures the inference accuracy of various DNN models with an average area of 5\% and an energy overhead of 0.8\% from an ideal ReRAM-based accelerator.},
  archive      = {J_TC},
  author       = {Hyein Shin and Myeonggu Kang and Lee-Sup Kim},
  doi          = {10.1109/TC.2022.3227871},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {2011-2024},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fault-free: A framework for analysis and mitigation of stuck-at-fault on realistic ReRAM-based DNN accelerators},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CARM: CUDA-accelerated RNS multiplication in word-wise
homomorphic encryption schemes for internet of things. <em>TC</em>,
<em>72</em>(7), 1999–2010. (<a
href="https://doi.org/10.1109/TC.2022.3227874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic encryption (HE), which allows computation over encrypted data, has often been used to preserve privacy. However, the computationally heavy nature and complexity of network topologies make the deployment of HE schemes in the Internet of Things (IoT) scenario difficult. In this work, we propose CARM, the first optimized GPU implementation that covers BGV, BFV and CKKS, targeting for accelerating homomorphic multiplication using GPU in heterogeneous IoT systems. Our solution is suitable for accelerating RNS homomorphic multiplication on both high-performance and embedded GPUs, as it is a parametric and generic design and offers various trade-offs between resource and efficiency. We offer constant-time low-level arithmetic with minimum instructions and memory usage, as well as performance- and memory-prior configurations. Through this, we can provide more real-time evaluation results and relieve the computational pressure on cloud devices. We deploy our implementations on two GPUs. Compared to the CPU implementation, we achieve up to $378.4\times$ , $234.5\times$ , and $287.2\times$ speedup for homomorphic multiplication of BGV, BFV, and CKKS on Tesla V100S, and $8.8\times$ , $9.2\times$ , and $10.3\times$ on Jetson AGX Xavier, respectively.},
  archive      = {J_TC},
  author       = {Shiyu Shen and Hao Yang and Yu Liu and Zhe Liu and Yunlei Zhao},
  doi          = {10.1109/TC.2022.3227874},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1999-2010},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CARM: CUDA-accelerated RNS multiplication in word-wise homomorphic encryption schemes for internet of things},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ALPINE: Analog in-memory acceleration with tight processor
integration for deep learning. <em>TC</em>, <em>72</em>(7), 1985–1998.
(<a href="https://doi.org/10.1109/TC.2022.3230285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analog in-memory computing (AIMC) cores offers significant performance and energy benefits for neural network inference with respect to digital logic (e.g., CPUs). AIMCs accelerate matrix-vector multiplications, which dominate these applications’ run-time. However, AIMC-centric platforms lack the flexibility of general-purpose systems, as they often have hard-coded data flows and can only support a limited set of processing functions. With the goal of bridging this gap in flexibility, we present a novel system architecture that tightly integrates analog in-memory computing accelerators into multi-core CPUs in general-purpose systems. We developed a powerful gem5-based full system-level simulation framework into the gem5-X simulator, ALPINE, which enables an in-depth characterization of the proposed architecture. ALPINE allows the simulation of the entire computer architecture stack from major hardware components to their interactions with the Linux OS. Within ALPINE, we have defined a custom ISA extension and a software library to facilitate the deployment of inference models. We showcase and analyze a variety of mappings of different neural network types, and demonstrate up to 20.5x/20.8x performance/energy gains with respect to a SIMD-enabled ARM CPU implementation for convolutional neural networks, multi-layer perceptrons, and recurrent neural networks.},
  archive      = {J_TC},
  author       = {Joshua Klein and Irem Boybat and Yasir Mahmood Qureshi and Martino Dazzi and Alexandre Levisse and Giovanni Ansaloni and Marina Zapater and Abu Sebastian and David Atienza},
  doi          = {10.1109/TC.2022.3230285},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1985-1998},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ALPINE: Analog in-memory acceleration with tight processor integration for deep learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructing multiple CISTs on BCube-based data center
networks in the occurrence of switch failures. <em>TC</em>,
<em>72</em>(7), 1971–1984. (<a
href="https://doi.org/10.1109/TC.2022.3230288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scale of data center networks (DCNs) has grown rapidly with the increasing popularity of cloud computing, data explosion, and the dramatic drop in setup costs. Thus, inevitable component failures (including switches and servers) will become more frequent. A DCN requires maintaining regular and reliable operation and providing efficient routing algorithms for transmitting data between servers. Particularly, fault-tolerant routing is necessary. Recently, constructing completely independent spanning trees (CISTs) on DCNs has received much attention as a dual-CIST (i.e., two CISTs) suffices to configure protection routing, which is a fault-tolerant routing. Moreover, the protection routing can additionally realize a secure mechanism if it is configured by more CISTs. BCube is a server-centric DCN with many advantages, and many variations were deformed from BCube with application requirements, such as RCube and RRect. In this paper, we provide a unified framework called BCube-based DCN (BDCN) that integrates the representation of the logic graphs of DCNs mentioned above, facilitating consistent algorithms’ design. Then, we develop efficient algorithms to construct multiple CISTs on BDCN under the consideration of switch failures. Note that this is the first study that constructs multiple CISTs in DCNs with switch failures. Finally, using standard metrics, such as average path length (APL) and transmission failure rate (TFR), we evaluate the performance of the fault-tolerant routing through experiments.},
  archive      = {J_TC},
  author       = {Wanling Lin and Xiao-Yan Li and Jou-Ming Chang and Xiaohua Jia},
  doi          = {10.1109/TC.2022.3230288},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1971-1984},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Constructing multiple CISTs on BCube-based data center networks in the occurrence of switch failures},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PCB hardware trojan run-time detection through machine
learning. <em>TC</em>, <em>72</em>(7), 1958–1970. (<a
href="https://doi.org/10.1109/TC.2022.3230877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modern semiconductor electronic devices are becoming increasingly vulnerable to malicious implants called Hardware Trojans (HT). This problem is also greatly related to Printed Circuit Boards (PCB), which are widely used in almost all electronic devices. In this paper, two machine learning (ML) methods have been applied to detect HTs running on power from I/Os of legitimate chips on a PCB. A PCB prototype has been fabricated to obtain real-life data, which was used to train two ML algorithms: One-Class Support Vector Machine and Local Outlier Factor. For validation of the ML classifiers, one hundred categories of HT devices have been modelled and inserted into the Validation and Testing datasets. Simulation results show that using the proposed methodology an HT device can be detected with high prediction accuracy (F1-score above $99.7\%$ for a $50mW$ HT). Further, the ML model has been uploaded to the prototype PCB for hard-silicon validation of the methodology. To the best of our knowledge, this is the first work on real-time detection of PCB HTs, which are powered from the I/O pins of legitimate ICs. Experimental results show that the performance of the ML model on a real-life prototype is consistent with that of the simulations.},
  archive      = {J_TC},
  author       = {Gor Piliposyan and Saqib Khursheed},
  doi          = {10.1109/TC.2022.3230877},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1958-1970},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PCB hardware trojan run-time detection through machine learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An energy efficient and runtime reconfigurable accelerator
for robotic localization. <em>TC</em>, <em>72</em>(7), 1943–1957. (<a
href="https://doi.org/10.1109/TC.2022.3230899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient localization of robots under limited on-board resources has fueled specialized localization accelerators. Despite many recent efforts, accelerating robotic localization is still fundamentally challenging. To tackle the challenges, the paper proposes a configurable hardware architecture and a design space optimization method to automatically generate an optimal accelerator design under the design constraints. Data locality, sparsity, and fixed-point arithmetic optimization techniques that are specific to the localization algorithm are exploited to customize the accelerator. In addition, a low-cost runtime configuration mechanism is proposed to enable the accelerator to continuously optimize itself at runtime according to the operating environment to save power while sustaining performance and accuracy. The evaluation on FPGA demonstrates that the proposed accelerator achieves orders of magnitude performance improvement and/or energy savings compared to the software implementation on Intel and Arm CPUs; and substantially outperforms existing FPGA accelerators in terms of performance and energy.},
  archive      = {J_TC},
  author       = {Qiang Liu and Yuhui Hao and Weizhuang Liu and Bo Yu and Yiming Gan and Jie Tang and Shao-Shan Liu and Yuhao Zhu},
  doi          = {10.1109/TC.2022.3230899},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1943-1957},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An energy efficient and runtime reconfigurable accelerator for robotic localization},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VRBC: A verifiable redactable blockchain with efficient
query and integrity auditing. <em>TC</em>, <em>72</em>(7), 1928–1942.
(<a href="https://doi.org/10.1109/TC.2022.3230900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by various legal obligations and service requirements, the redactable blockchain was introduced to balance the modifiability and immutability of blockchain technology. However, such a blockchain inevitably generates one or even more acceptable versions for the same block data, enabling malicious full nodes to deceive light/new nodes with old data and even disrupt the consistency of the blockchain ledger. In this paper, we introduce the concept of verifiable redactable blockchain (VRBC) to provide efficient validity verification for on-chain data. To this end, we design a novel authentication data structure, called blockchain authentication tree (BAT), which utilizes a chameleon hash function and aggregatable vector commitment to bind continuously-appended blocks. Based on this, we propose an efficient VRBC scheme supporting integrity auditing, which not only allows light nodes to query and validate on-chain data, but also enables new nodes to check the integrity of the blockchain ledger before synchronizing it, effectively avoiding resource waste and security risks caused by invalid queries and ledger synchronization. Furthermore, we introduce some optimized strategies to improve the performance of the proposed scheme and extend it to transaction-level and permissionless VRBC, respectively. Finally, we demonstrate the practicability of the proposed scheme through detailed security analysis and visual performance evaluation.},
  archive      = {J_TC},
  author       = {Guohua Tian and Jianghong Wei and Mirosław Kutyłowski and Willy Susilo and Xinyi Huang and Xiaofeng Chen},
  doi          = {10.1109/TC.2022.3230900},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1928-1942},
  shortjournal = {IEEE Trans. Comput.},
  title        = {VRBC: A verifiable redactable blockchain with efficient query and integrity auditing},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PrivAim: A dual-privacy preserving and quality-aware
incentive mechanism for federated learning. <em>TC</em>, <em>72</em>(7),
1913–1927. (<a href="https://doi.org/10.1109/TC.2022.3230904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy protection and incentive mechanism are two fundamental problems in federated learning (FL), which aim at protecting the privacy of data owners and stimulating them to share more resources, respectively. Recent works have proposed differential privacy (DP) based privacy-preserving incentive mechanisms to solve both problems simultaneously. However, almost all of them took the privacy level as the only incentive item, without considering other factors, such as data quantity and quality. Moreover, an untrusted server can further infer sensitive information from the bids that reflect the true costs of data owners. To solve these problems, in this paper, we propose a dual-privacy preserving and quality-aware incentive mechanism, PrivAim, for federated learning. Specifically, it utilizes differential privacy to protect the local models and true costs against the untrusted parameter server, and carefully designs a multi-dimensional reverse auction mechanism to incentivize data owners with high quality and low cost to participate in FL without knowing the true bids. We theoretically prove that PrivAim satisfies $\Delta b$ -truthfulness, individual rational, computational efficiency, and differential privacy. Extensive experiments show that PrivAim can effectively protect bid privacy, and achieve at least 21\% and 6\% improvement on social welfare and model accuracy, respectively, compared to the state-of-the-art.},
  archive      = {J_TC},
  author       = {Dan Wang and Ju Ren and Zhibo Wang and Yichuan Wang and Yaoxue Zhang},
  doi          = {10.1109/TC.2022.3230904},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1913-1927},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PrivAim: A dual-privacy preserving and quality-aware incentive mechanism for federated learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revocable blockchain-aided attribute-based encryption with
escrow-free in cloud storage. <em>TC</em>, <em>72</em>(7), 1901–1912.
(<a href="https://doi.org/10.1109/TC.2023.3234210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive amount of data generated by the Internet of Things (IoT) and the need to store that data presents a huge challenge for storage. However, meeting this challenge has also driven the development of storage technologies, especially those related to cloud storage. Although attribute-based encryption (ABE) schemes are commonly used to achieve data confidentiality and fine-grained access control in cloud storage, there is still an inherent problem with ABE schemes, namely the key escrow problem. In this paper, we propose a revocable blockchain-aided ABE with escrow-free (BC-ABE-EF) system that resolves the key escrow problem by replacing the traditional key authority with a consortium blockchain. The keys are generated between the blockchain and the data user through a secure key issuing protocol, and the blockchain cannot obtain the user&#39;s full key alone. Furthermore, utilize the decryption cloud server to schedule pre-decryption operations in cloud and introduce a group manager to update the group keys of unrecovered users and generate re-encryption keys. The security analysis shows that our scheme is secure under the Decisional Computation Diffie Hellman (DCDH) assumption. The effectiveness of the scheme is demonstrated by simulating the BC-ABE-EF scheme and comparing it based on performance analysis.},
  archive      = {J_TC},
  author       = {Yuyan Guo and Zhenhua Lu and Hui Ge and Jiguo Li},
  doi          = {10.1109/TC.2023.3234210},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1901-1912},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Revocable blockchain-aided attribute-based encryption with escrow-free in cloud storage},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and analysis of RSA and paillier homomorphic
cryptosystems using PSO-based evolutionary computation. <em>TC</em>,
<em>72</em>(7), 1886–1900. (<a
href="https://doi.org/10.1109/TC.2023.3234213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-lattice based homomorphic encryption schemes usually involve huge modular exponentiation operations. Thus, improving the efficiency of modular exponentiation for large exponents is a real-world issue. Modular exponentiation can be computed by a series of modular multiplications. However, performing a series of modular multiplications is computationally expensive. This paper proposes hardware /software co-design for efficient modular exponentiation. This article first explores the use of particle swarm optimization (PSO) for the modular exponentiation in software, and present an efficient hardware co-design utilizing FPGA. Our findings reveal that the suggested PSO approach surpasses all other deterministic and non-deterministic approaches already in use. Further, we also demonstrate a comprehensive analysis for the optimal performance and parameter selection of our proposed PSO approach. Finally, we implement homomorphic encryption schemes, such as RSA and Paillier, using our PSO-based hardware /software co-design. Our approach gains resource savings for 1024-bit as follows: RSA encryption/decryption - 60.7\% (area) and 65.3\% (DSP); Paillier encryption - 46.3\% (area) and 40\% (DSP) and Paillier decryption - 73.7\% (area) and 66.6\% (DSP). We have obtained area-time improvements of 1024-bit as follows: RSA encryption/decryption - 2.7x; Paillier encryption - 2x and Paillier decryption - 4.6x using Xilinx Virtex-7 FPGAs.},
  archive      = {J_TC},
  author       = {Sathi Sarveswara Reddy and Sharad Sinha and Wei Zhang},
  doi          = {10.1109/TC.2023.3234213},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1886-1900},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Design and analysis of RSA and paillier homomorphic cryptosystems using PSO-based evolutionary computation},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal rack-coordinated updates in erasure-coded data
centers: Design and analysis. <em>TC</em>, <em>72</em>(7), 1871–1885.
(<a href="https://doi.org/10.1109/TC.2023.3234215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure coding has been extensively deployed in today&#39;s data centers to tackle prevalent failures, yet it is prone to substantial cross-rack traffic for parity updates. In this article, we propose a new rack-coordinated update mechanism to suppress the cross-rack update traffic, which comprises two successive phases: a delta-collecting phase that collects data delta chunks, and another selective parity update phase that renews the parity chunks based on the update pattern and parity layout. We further design ${\sf RackCU}$ , an optimal rack-coordinated update solution that achieves the theoretical lower bound of the cross-rack update traffic. We also perform reliability analysis, demonstrating that ${\sf RackCU}$ can attain a lower data loss probability via shortening the update procedure. We conduct extensive evaluations, in terms of large-scale simulation and real-world data center experiments. We show that ${\sf RackCU}$ can reduce 16.5-77.1\% of the cross-rack update traffic and hence improve 24.9-772.0\% of the update throughput.},
  archive      = {J_TC},
  author       = {Guowen Gong and Zhirong Shen and Liang Chen and Suzhen Wu and Xiaolu Li and Patrick P. C. Lee and Zhiguo Wan and Jiwu Shu},
  doi          = {10.1109/TC.2023.3234215},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1871-1885},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimal rack-coordinated updates in erasure-coded data centers: Design and analysis},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Split: A hash-based memory optimization method for
zero-knowledge succinct non-interactive argument of knowledge
(zk-SNARK). <em>TC</em>, <em>72</em>(7), 1857–1870. (<a
href="https://doi.org/10.1109/TC.2023.3235975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zk-SNARK) is a practical zero-knowledge proof system for Rank-1 Constraint Satisfaction (R1CS), enabling privacy preservation and addressing the previous scalability concerns on zero-knowledge proofs. Existing constructions of zk-SNARKs require huge memory overhead to generate proofs in that the size of the zk-SNARK circuit can be large even for a very simple use case, which limits the applications for regular resource-constrained users. To reduce the memory utilization of zk-SNARKs, this paper presents a hash-based method “Split”. Concretely, Split intends to partition the zk-SNARK circuits so that components can be processed sequentially while ensuring strong security properties leveraging hash circuits. As a zk-SNARK circuit is partitioned, obsolete variables are no longer preserved in the memory. We further propose an enhanced Split as $n$ -Split, which leads to better optimization by properly choosing multiple splits. Our experimental results validate the effectiveness and efficiency of Split in conserving memory usage for resource-constrained provers as long as the circuit can be partitioned to a Good Split, indicating that via Split zk-SNARKs can be brought one step closer to practical applications.},
  archive      = {J_TC},
  author       = {Huayi Qi and Ye Cheng and Minghui Xu and Dongxiao Yu and Haipeng Wang and Weifeng Lyu},
  doi          = {10.1109/TC.2023.3235975},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1857-1870},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Split: A hash-based memory optimization method for zero-knowledge succinct non-interactive argument of knowledge (zk-SNARK)},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A mutation-enabled proactive defense against
service-oriented man-in-the-middle attack in kubernetes. <em>TC</em>,
<em>72</em>(7), 1843–1856. (<a
href="https://doi.org/10.1109/TC.2023.3238125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kubernetes (K8s) has become a core technology for cloud-native applications. However, a design flaw of the external IP in K8s leads to the service-oriented man-in-the-middle attack. Existing solutions (e.g., script monitor) attempt to address it passively, which allows attackers enough analysis time to bypass these static rule reviews. Differently, we propose a mutation-enabled proactive defense mechanism, aiming to change the asymmetry between attackers and defenders. It involves the address mutation (i.e., network identification) module and the connection ID (i.e., communication identification) mutation module. In the former module, we analyze mutation constraints and prove the corresponding mutation grouping problem to be NP-hard. Then, a maximally coloring-driven mutation grouping algorithm is developed. Since the address allocation time grows linearly with the service size, we design a prefetched address allocation algorithm. After designing the interaction flow between modules, we present a randomized algorithm in the latter module. Thus our mechanism does not affect methods oriented to other attacks. Eventually, it can continuously interrupt the attack and keep the service connection by incrementally updating K8s and the transport layer protocol. Experiments in the Alibaba cloud demonstrate that it can effectively defend against the attack with an acceptable performance loss.},
  archive      = {J_TC},
  author       = {Tengchao Ma and Changqiao Xu and Shujie Yang and Yiting Huang and Qingzhao An and Xiaohui Kuang and Luigi Alfredo Grieco},
  doi          = {10.1109/TC.2023.3238125},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1843-1856},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A mutation-enabled proactive defense against service-oriented man-in-the-middle attack in kubernetes},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KaratSaber: New speed records for saber polynomial
multiplication using efficient karatsuba FPGA architecture. <em>TC</em>,
<em>72</em>(7), 1830–1842. (<a
href="https://doi.org/10.1109/TC.2023.3238129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SABER is a round 3 candidate in the NIST Post-Quantum Cryptography Standardization process. Polynomial convolution is one of the most computationally intensive operation in Saber Key Encapsulation Mechanism, that can be performed through widely explored algorithms like the schoolbook polynomial multiplication algorithm (SPMA) and Number Theoretic Transform (NTT). While SPMA multiplier has a slow latency performance, the NTT-based multiplier usually requires large hardware. In this work, we propose KaratSaber, an optimized Karatsuba polynomial multiplier architecture with a balanced hardware efficiency (throughput-per-slice, TPS) compared to NTT and SPMA based designs. KaratSaber employs several techniques for an efficient design: a parallel grid input technique for efficient pre-processing stage in Karatsuba-based polynomial multiplier, a novel instruction code result-mapping technique catering the negacyclic operations improves the post-processing stage efficiency, a double multiplicand shifter-based multiplier doubles the throughput at the multiplication stage. Combining these three techniques, the proposed KaratSaber architecture is 7.47× faster compared to the state-of-the-art SPMA Saber architecture at the expense of 4.96× additional hardware resources; making KaratSaber 46.04\% more area-time efficient. When compared to LWRPro, a recent Karatsuba Saber architecture, KaratSaber architecture achieves a 2.11× higher throughput by only utilizing 1.92× additional hardware; thus gaining a 10.44\% improvement in area-time efficiency},
  archive      = {J_TC},
  author       = {Zheng-Yan Wong and Denis C.-K. Wong and Wai-Kong Lee and Kai-Ming Mok and Wun-She Yap and Ayesha Khalid},
  doi          = {10.1109/TC.2023.3238129},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1830-1842},
  shortjournal = {IEEE Trans. Comput.},
  title        = {KaratSaber: New speed records for saber polynomial multiplication using efficient karatsuba FPGA architecture},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design space exploration for efficient quantum
most-significant digit-first arithmetic. <em>TC</em>, <em>72</em>(6),
1822–1829. (<a href="https://doi.org/10.1109/TC.2022.3215891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing has been considered as an emerging approach in addressing problems which are not easily solvable using classical computers. In parallel to the physical implementation of quantum processors, quantum algorithms have been actively developed for real-life applications to show quantum advantages, many of which benefit from quantum arithmetic algorithms and their efficient implementations. As one of the most important operations, quantum addition has been adopted in Shor&#39;s algorithm and quantum linear algebra algorithms. Although various least-significant digit-first quantum adders have been introduced in previous work, interest in investigating the efficient implementation of most-significant digit-first addition is growing. In this work, we propose a novel design method for most-significant digit-first addition with several quantum circuit optimisations to reduce the number of quantum bits (i.e. qubits), quantum gates, and circuit depth. An open-source library of different arithmetic operators based on our proposed method is presented, where all circuits are implemented on IBM Qiskit SDK. Extensive experiments demonstrate that our proposed design, together with the optimisation techniques, reduces T-depth by up-to 4.0×, T-count by 3.5×, and qubit consumption by 1.2×.},
  archive      = {J_TC},
  author       = {He Li and Jiawei Liang and Hongxiang Fan and Yongming Tang},
  doi          = {10.1109/TC.2022.3215891},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1822-1829},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Design space exploration for efficient quantum most-significant digit-first arithmetic},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A privacy-preserving comparison protocol. <em>TC</em>,
<em>72</em>(6), 1815–1821. (<a
href="https://doi.org/10.1109/TC.2022.3215640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiparty comparison allows to compare two integers $x$ and $y$ blindly, where a set of players hold the shares of the elements $x and y, x, y \in \mathbb {F}_{p}$ , a prime field. The existing multiparty comparison protocols execute in constant rounds, but the number of multiplications depends on the size of the prime $p$ , i.e., the communication complexity will be high for large prime $p$ . In this paper, we present a multiparty comparison protocol with constant rounds in which the number of multiplications depends on the number of players rather than the prime $p$ itself. This multiparty comparison protocol is further extended to design a multiparty equality-test protocol. An equality-test protocol computes the equality of shares in constant rounds and its number of multiplications depends on the number of players. Our proposed protocols multiparty comparison and equality-test are unconditionally secure against the active and passive attacks and have $O(n)$ communication complexity, where $n$ is the number of players. We also present an efficient technique for fault detection that can verify the correctness of various protocols.},
  archive      = {J_TC},
  author       = {Kartick Sutradhar and Hari Om},
  doi          = {10.1109/TC.2022.3215640},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1815-1821},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A privacy-preserving comparison protocol},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating federated learning with a global biased
optimiser. <em>TC</em>, <em>72</em>(6), 1804–1814. (<a
href="https://doi.org/10.1109/TC.2022.3212631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a recent development in distributed machine learning that collaboratively trains models without training data leaving client devices, preserving data privacy. In real-world FL, the training set is distributed over clients in a highly non-Independent and Identically Distributed (non-IID) fashion, harming model convergence speed and final performance. To address this challenge, we propose a novel, generalised approach for incorporating adaptive optimisation into FL with the Federated Global Biased Optimiser (FedGBO) algorithm. FedGBO accelerates FL by employing a set of global biased optimiser values during training, reducing ‘client-drift’ from non-IID data whilst benefiting from adaptive optimisation. We show that in FedGBO, updates to the global model can be reformulated as centralised training using biased gradients and optimiser updates, and apply this framework to prove FedGBO&#39;s convergence on nonconvex objectives when using the momentum-SGD (SGDm) optimiser. We also conduct extensive experiments using 4 FL benchmark datasets (CIFAR100, Sent140, FEMNIST, Shakespeare) and 3 popular optimisers (SGDm, RMSProp, Adam) to compare FedGBO against six state-of-the-art FL algorithms. The results demonstrate that FedGBO displays superior or competitive performance across the datasets whilst having low data-upload and computational costs, and provide practical insights into the trade-offs associated with different adaptive-FL algorithms and optimisers.},
  archive      = {J_TC},
  author       = {Jed Mills and Jia Hu and Geyong Min and Rui Jin and Siwei Zheng and Jin Wang},
  doi          = {10.1109/TC.2022.3212631},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1804-1814},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating federated learning with a global biased optimiser},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PyTracer: Automatically profiling numerical instabilities in
python. <em>TC</em>, <em>72</em>(6), 1792–1803. (<a
href="https://doi.org/10.1109/TC.2022.3224377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical stability is a crucial requirement of reliable scientific computing. However, despite the pervasiveness of Python in data science, analyzing large Python programs remains challenging due to the lack of scalable numerical analysis tools available for this language. To fill this gap, we developed PyTracer, a profiler to quantify numerical instability in Python applications. PyTracertransparently instruments Python code to produce numerical traces and visualize them interactively in a Plotly dashboard. We designed PyTracerto be agnostic to numerical noise model, allowing for numerical profiling through Monte-Carlo Arithmetic, random rounding, random data perturbation, or structured noise for a particular application. We illustrate PyTracer&#39;s capabilities by testing the numerical stability of key functions in both SciPy and Scikit-learn, two dominant Python libraries for mathematical modeling. Through these evaluations, we demonstrate PyTraceras a scalable, automated, and generic framework for numerical profiling in Python.},
  archive      = {J_TC},
  author       = {Yohan Chatelain and Nigel Yong Sao Young and Gregory Kiar and Tristan Glatard},
  doi          = {10.1109/TC.2022.3224377},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1792-1803},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PyTracer: Automatically profiling numerical instabilities in python},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PMLiteDB: Streamlining access paths for high-performance
persistent memory document database systems. <em>TC</em>,
<em>72</em>(6), 1778–1791. (<a
href="https://doi.org/10.1109/TC.2022.3224372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of byte-addressable persistent memory opens an important opportunity for document databases to read and write durable data fetching them into DRAM. Reaping the benefit of persistent memory is not straightforward, as existing document databases are tailored for disk storage. They assume that the disk and DRAM data movement dominates the performance. However, this paper points out that data indexing becomes the performance bottleneck when porting document databases to persistent memory. The paper proposes PMLiteDB, the first persistent memory document database with streamlined access paths. PMLiteDB introduces two techniques, direct reading and selective caching . Direct reading streamlines the translation from document IDs to the address of documents whenever possible by swizzling the IDs into persistent memory references . It guarantees to use only up-to-date persistent memory references when document movements invalidate associated references. Selective caching reduces data movements between DRAM and persistent memory by selectively caching only frequently accessed persistent memory data pages with a DRAM buffer. For other pages, the database loads data on them directly without caching. Compared to the design that adopts persistent memory as a fast disk without exploiting the byte-addressability, PMLiteDB achieves 2.33× on average and up to 6.18× speedup.},
  archive      = {J_TC},
  author       = {Hai Jin and Shuo Wei and Yan Sha and Chencheng Ye and Haikun Liu and Xiaofei Liao},
  doi          = {10.1109/TC.2022.3224372},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1778-1791},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PMLiteDB: Streamlining access paths for high-performance persistent memory document database systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SIGNED: A challenge-response scheme for electronic hardware
watermarking. <em>TC</em>, <em>72</em>(6), 1763–1777. (<a
href="https://doi.org/10.1109/TC.2022.3223304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of distributed manufacturing ecosystems for electronic hardware involving untrusted parties has led to diverse trust issues. In particular, Intellectual Property (IP) piracy, reverse engineering, and overproduction pose significant threats to integrated circuits (IC) manufacturers. Watermarking has been one of the solutions employed by the semiconductor industry to overcome many of the trust issues. However, existing watermarking techniques often suffer from one or more of the following deficiencies: (1) low structural coverage, (2) applicability to specific design abstraction level (e.g., gate or layout), (3) high design overhead, and (4) vulnerabilities to removal or tampering attacks. We address these deficiencies by introducing a new watermarking scheme, called SIGNED : S ignature I nsertion through challen G e respo N se in E lectronic D esign. SIGNED relies on a challenge-response protocol-based interrogation scheme for generating the watermark. It identifies strategic locations of an input design and samples them in response to select input patterns to form a set of compact signatures representing the functional and structural characteristics of a design. We show that this signature set can be used as high-quality watermark of an IP to verify its provenance. We evaluate SIGNED on the ISCAS85, ITC, and MIT CEP benchmark circuits with respect to all major quality parameters of hardware watermark. We show that SIGNED achieves excellent structural coverage and robustness against identification and removal attacks, while introducing modest design overheads.},
  archive      = {J_TC},
  author       = {Patanjali SLPSK and Abhishek Anil Nair and Chester Rebeiro and Swarup Bhunia},
  doi          = {10.1109/TC.2022.3223304},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1763-1777},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SIGNED: A challenge-response scheme for electronic hardware watermarking},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data distribution for heterogeneous storage systems.
<em>TC</em>, <em>72</em>(6), 1747–1762. (<a
href="https://doi.org/10.1109/TC.2022.3223302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of data in many science and engineering domains poses significant challenges to storage systems. Data distribution is a critical component in large-scale distributed storage systems and plays a vital role in placing petabytes of data and beyond, among tens to hundreds of thousands of storage devices. Meantime, heterogeneous storage systems, such as those having devices with hard disk drives (HDDs) and storage class memories (SCMs), have become increasingly popular for massive data storage due to their distinct and complement characteristics. This paper presents a new data distribution algorithm called SUORA (Scalable and Uniform storage via Optimally-adaptive and Random number Addressing) specifically for heterogeneous devices to maximize the benefits of them. SUORA provides a fully symmetric, highly efficient methodology to distribute data across a hybrid and tiered storage cluster. It divides heterogeneous devices into different buckets and segments, and adopts pseudo-random functions to map data onto them with the balanced consideration of capacity, performance and life-time. By analyzing hotness and access patterns, SUORA gradually moves hot data from HDDs to SCMs to optimize the throughput, and moves cold data reversely for load balance. It combines data replication with migration to significantly reduce movement overhead while making data placement more adaptive to different workloads. Extensive evaluations on simulation and Sheepdog storage system show that, with considering distinct characteristics of various devices thoroughly, SUORA improves the overall performance efficiency of heterogeneous storage systems.},
  archive      = {J_TC},
  author       = {Jiang Zhou and Yong Chen and Mai Zheng and Weiping Wang},
  doi          = {10.1109/TC.2022.3223302},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1747-1762},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Data distribution for heterogeneous storage systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-performance tensor learning primitives using GPU tensor
cores. <em>TC</em>, <em>72</em>(6), 1733–1746. (<a
href="https://doi.org/10.1109/TC.2022.3222955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor learning is a powerful tool for big data analytics and machine learning, e.g., gene analysis and deep learning. However, tensor learning algorithms are compute-intensive since their time and space complexities grow exponentially with the order of tensors, which hinders their application. In this paper, we exploit the parallelism of tensor learning primitives using GPU tensor cores and develop high-performance tensor learning algorithms. First, we propose novel hardware-oriented optimization strategies for tensor learning primitives on GPU tensor cores. Second, for big data analytics, we employ the optimized tensor learning primitives to accelerate the CP tensor decomposition and then apply it for gene analysis. Third, we optimize the Tucker tensor decomposition and propose a novel Tucker tensor layer to compress deep neural networks. We employ natural gradients to train the neural networks, which only involve a forward pass without backpropagation and thus are suitable for GPU computations. Compared with TensorLab and TensorLy libraries on an A100 GPU, our third-order CP tensor decomposition achieves up to $16.32\times$ and $32.25\times$ speedups; and $6.09\times$ and $6.72\times$ speedups for our third-order Tucker tensor decomposition. The proposed fourth-order CP and Tucker tensor decompositions achieve up to $30.65\times$ and $5.41\times$ speedups over the TensorLab. Our CP tensor decomposition for gene analysis achieves up to $5.88\times$ speedup over TensorLy. Compared with a conventional fully connected neural network, our Tucker tensor layer neural network achieves an accuracy of $97.9\%$ , a speedup of $4.47\times$ , and a compression ratio of $2.92$ at the cost of $0.4\%$ drop in accuracy.},
  archive      = {J_TC},
  author       = {Xiao-Yang Liu and Zeliang Zhang and Zhiyuan Wang and Han Lu and Xiaodong Wang and Anwar Walid},
  doi          = {10.1109/TC.2022.3222955},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1733-1746},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-performance tensor learning primitives using GPU tensor cores},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the effects of transaction data access patterns on
performance in lock-based concurrency control. <em>TC</em>,
<em>72</em>(6), 1718–1732. (<a
href="https://doi.org/10.1109/TC.2022.3222084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transaction Processing (TP) plays a primary role in the design and implementation of IT applications and services. Many TP systems exploit lock-based concurrency control to guarantee atomicity and isolation of transactions that access shared data. In this article, we show that transaction data access patterns, in particular the order of data accesses along the transaction execution, have a noticeable impact on how lock-based concurrency control affects performance. We show that the performance can remarkably change depending on whether transactions, or a percentage of them, access data items following some common ordering rule or not. We investigate on this aspect and its root causes through an analytical modeling approach, and with the evidence of data gathered through both simulation and the execution of real transactional workloads. Finally, we show how the findings of our study can be easily exploited for improving the performance of common transactional workloads.},
  archive      = {J_TC},
  author       = {Pierangelo Di Sanzo and Francesco Quaglia},
  doi          = {10.1109/TC.2022.3222084},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1718-1732},
  shortjournal = {IEEE Trans. Comput.},
  title        = {On the effects of transaction data access patterns on performance in lock-based concurrency control},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-latency hardware architecture for VDF evaluation in
class groups. <em>TC</em>, <em>72</em>(6), 1706–1717. (<a
href="https://doi.org/10.1109/TC.2022.3219723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The verifiable delay function (VDF), as a kind of cryptographic primitives, has recently been adopted quite often in decentralized systems. Highly correlated to the security of VDFs, the fastest implementation for VDF evaluation is generally desired to be publicly known. In this paper, for the first time, we propose a low-latency hardware implementation for the complete VDF evaluation in the class group by jointly exploiting optimizations. On one side, we reduce the required computational cycles by decreasing the hardware-unfriendly divisions and increase the parallelism of computations by reducing the data dependency. On the other side, we provide low-latency large-number divisors, multipliers, and adders, respectively, while those operators are generally very hard to be accelerated. Besides, we carefully schedule the sub-modules and devise the low-latency architecture for the complete VDF evaluation. Finally, the proposed design is coded and synthesized under the TSMC 28-nm CMOS technology. The experimental results show that our design can achieve a speedup of 3.5x compared to the optimal C++ implementation for the VDF evaluation over an advanced CPU. Moreover, compared to the state-of-the-art hardware implementation for the squaring, a key step of VDF, we achieve about 2x speedup.},
  archive      = {J_TC},
  author       = {Danyang Zhu and Jing Tian and Minghao Li and Zhongfeng Wang},
  doi          = {10.1109/TC.2022.3219723},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1706-1717},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Low-latency hardware architecture for VDF evaluation in class groups},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benchmarking quantum(-inspired) annealing hardware on
practical use cases. <em>TC</em>, <em>72</em>(6), 1692–1705. (<a
href="https://doi.org/10.1109/TC.2022.3219257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum(-inspired) annealers show promise in solving combinatorial optimisation problems in practice. There has been extensive researches demonstrating the utility of D-Wave quantum annealer and quantum-inspired annealer, i.e., Fujitsu Digital Annealer on various applications, but few works are comparing these platforms. In this paper, we benchmark quantum(-inspired) annealers with three combinatorial optimisation problems ranging from generic scientific problems to complex problems in practical use. In the case where the problem size goes beyond the capacity of a quantum(-inspired) computer, we evaluate them in the context of decomposition. Experiments suggest that both annealers are effective on problems with small size and simple settings, but lose their utility when facing problems in practical size and settings. Decomposition methods extend the scalability of annealers, but they are still far away from practical use. Based on the experiments and comparison, we discuss the advantages and limitations of quantum(-inspired) annealers, as well as the research directions that may improve the utility and scalability of the these emerging computing technologies.},
  archive      = {J_TC},
  author       = {Tian Huang and Jun Xu and Tao Luo and Xiaozhe Gu and Rick Goh and Weng-Fai Wong},
  doi          = {10.1109/TC.2022.3219257},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1692-1705},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Benchmarking quantum(-inspired) annealing hardware on practical use cases},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A case for partitioned bloom filters. <em>TC</em>,
<em>72</em>(6), 1681–1691. (<a
href="https://doi.org/10.1109/TC.2022.3218995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a partitioned Bloom Filter (PBF) the bit vector is split into disjoint parts, one per hash function. Contrary to hardware designs, where they prevail, software implementations mostly ignore PBFs, considering them worse than standard Bloom filters (SBF), due to the slightly larger false positive rate (FPR). In this paper, by performing an in-depth analysis, first we show that the FPR advantage of SBFs is smaller than thought; more importantly, by deriving the per-element FPR, we show that SBFs have weak spots in the domain: elements that test as false positives much more frequently than expected. This is relevant in scenarios where an element is tested against many filters. Moreover, SBFs are prone to exhibit extremely weak spots if naive double hashing is used, something occurring in mainstream libraries. PBFs exhibit a uniform distribution of the FPR over the domain, with no weak spots, even using naive double hashing. Finally, we survey scenarios beyond set membership testing, identifying many advantages of having disjoint parts, in designs using SIMD techniques, for filter size reduction, test of set disjointness, and duplicate detection in streams. PBFs are better, and should replace SBFs, in general purpose libraries and as the base for novel designs.},
  archive      = {J_TC},
  author       = {Paulo Sérgio Almeida},
  doi          = {10.1109/TC.2022.3218995},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1681-1691},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A case for partitioned bloom filters},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-aware scenario-based mapping of deep learning
applications onto heterogeneous processors under real-time constraints.
<em>TC</em>, <em>72</em>(6), 1666–1680. (<a
href="https://doi.org/10.1109/TC.2022.3218991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To cope with the increasing demand for deep learning applications in embedded systems, emerging embedded devices tend to equip multiple heterogeneous processors, including GPU and deep learning hardware accelerator, called neural processing unit (NPU). It becomes popular to run multiple deep learning (DL) applications simultaneously to provide several functionalities. In this work, we assume that applications have real-time constraints that may vary at run time. While extensive studies have been conducted recently to find an efficient mapping of multiple DL applications on various hardware platforms, they do not consider the constraints imposed by the NPU and the associated software development kit (SDK) in a real embedded platform. In this paper, we propose a novel energy-aware mapping methodology of multiple DL applications onto a real embedded system that has multiple heterogeneous processors. The objective is to minimize energy consumption while satisfying the real-time constraints of all applications. In the proposed scheme, we first select Pareto-optimal mapping solutions for each application. Then mapping combination is explored, considering the scenario that indicates the dynamism of applications while satisfying the constraints. Also, we reduce energy consumption by tuning the frequency of processors. We could satisfy up to 40\% higher deadline constraints and reduce the energy consumption by 22\% ∼ 31\% compared to the static mapping methods with real-life applications and different scenarios on a real platform.},
  archive      = {J_TC},
  author       = {Jangryul Kim and Soonhoi Ha},
  doi          = {10.1109/TC.2022.3218991},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1666-1680},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy-aware scenario-based mapping of deep learning applications onto heterogeneous processors under real-time constraints},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DESCO: Decomposition-based co-design to improve fault
tolerance of security-critical tasks in cyber physical systems.
<em>TC</em>, <em>72</em>(6), 1652–1665. (<a
href="https://doi.org/10.1109/TC.2022.3218987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidentiality-Specific Faults (CSFs) will put cyber physical systems in threat, since they can result in corrupted information or even retrieve the cryptographic key of security-critical applications. In this paper, we will look into fault-tolerant co-design optimization for security-critical cyber physical systems with resource constraints, such that the encryption/decryption of confidential messages are protected against transient CSF faults. We consider imperfect fault detection mechanisms to identify transient CSF faults happened on confidentiality protection, and utilize duplication code to recovery from such faults. We utilize FPGA to accelerate the executions of security tasks, reducing the overheads of fault-tolerant implementations. The system-level design problem is formulated as a two-objective optimization problem, i.e., to minimize the average reliability degradation of the fault tolerant assignments and to minimize the balanced degree of the reliability degradation, subject to available FPGA budget, deadline, and application execution constraints. Since finding Pareto-optimal solutions is NP-hard, we propose an improved multi-objective optimization algorithm, called DEcomposition-based Security Co-design Optimization (DESCO), to search for Pareto-optimal solutions of fault-tolerant assignments. Experimental results demonstrate that DESCO is effective and can outperform other candidates, proving that our approach is promising in dealing with system-level optimization problem for security-critical applications on cyber physical systems.},
  archive      = {J_TC},
  author       = {Wei Jiang and Xinke Liao and Jinyu Zhan and Deepak Adhikari and Ke Jiang},
  doi          = {10.1109/TC.2022.3218987},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1652-1665},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DESCO: Decomposition-based co-design to improve fault tolerance of security-critical tasks in cyber physical systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Birds of the same feather flock together: A dual-mode
circuit candidate for strong PUF-TRNG functionalities. <em>TC</em>,
<em>72</em>(6), 1636–1651. (<a
href="https://doi.org/10.1109/TC.2022.3218986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically Unclonable Functions (PUFs) and True Random Number Generators (TRNGs) are two highly useful hardware primitives to build up the root-of-trust for embedded devices in Internet-of-Things and Cyber-Physical System applications. These applications demand the primitives be lightweight, yet flexible. However, PUFs are designed to offer repetitive and instance-specific randomness, whereas TRNGs are expected to be invariably random. A challenging but thought-provoking problem from a hardware designer&#39;s perspective would be to design a circuit that serves the purpose of both PUF and TRNG depending on the exact requirement of the application. Here, we present a dual-mode PUF-TRNG design that utilises two different hardware-intrinsic properties, i.e., oscillatory metastability of Transition Effect Ring Oscillator (TERO) cell and propagation delay of a buffer within the cell to achieve this goal. A 48.62\% reduction in area is accomplished due to the integration in comparison to separate instances of standalone PUFs/ TRNG designs, built from Programmable Delay Line (PDL) based Arbiter PUFs (APUFs) and TERO-TRNG. Our final design has a hardware footprint of 618 Look-Up Tables (LUTs) and 447 Flip-Flops (FFs). Furthermore, experimental analysis of the state-of-the-art modelling attacks, reliability attacks on the proposed PUF design shows a prediction accuracy of 55.37\% and 50.14\% respectively for 5.2M Challenge Response Pairs (CRPs). Additionally, the TRNG passes evaluation through National Institute of Standards and Technology (NIST) Special Publication (SP) 800-22 and German Federal Office for Information Security (BSI) Application Notes and Interpretation of the Scheme (AIS)-31 tests.},
  archive      = {J_TC},
  author       = {Kuheli Pratihar and Urbi Chatterjee and Manaar Alam and Rajat Subhra Chakraborty and Debdeep Mukhopadhyay},
  doi          = {10.1109/TC.2022.3218986},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1636-1651},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Birds of the same feather flock together: A dual-mode circuit candidate for strong PUF-TRNG functionalities},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the design of iterative approximate floating-point
multipliers. <em>TC</em>, <em>72</em>(6), 1623–1635. (<a
href="https://doi.org/10.1109/TC.2022.3216465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate multipliers provide power and area-saving for error-resilient applications. In this paper, we first propose two approximate floating-point multipliers based on two-dimensional pseudo-Booth encoding: floating-point pseudo-Booth (PB), and floating-point iterative pseudo-Booth (IPB). The accuracy of proposed multipliers can be tuned by three parameters: iteration, encoder&#39;s radix (R), and word length after truncation (W). Next, we developed the conventional iterative multipliers with a simplified steering circuit for their correction part to eliminate the power consumption of multipliers. The proposed iterative multipliers are compared with conventional iterative integer multipliers implemented by a simplified steering circuit for the floating-point area. The results reveal that the proposed PB-R4-W4 and IPB-R16-W19, compared to the exact floating-point multiplier, provide up to 98.9\% and 67.5\% reductions in power consumption, respectively, in TSMC 180nm CMOS technology. Also, their MRED values are, respectively, 2.9\% and $({7.4 \times {{10}}^{ - 4}})$\%. Finally, we evaluated the functionality of the proposed multipliers for real-life applications, including a hyper-plane classifier and two image processing applications of smoothing and sharpening.},
  archive      = {J_TC},
  author       = {Ahmad Towhidy and Reza Omidi and Karim Mohammadi},
  doi          = {10.1109/TC.2022.3216465},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1623-1635},
  shortjournal = {IEEE Trans. Comput.},
  title        = {On the design of iterative approximate floating-point multipliers},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Load balancing in compute clusters with delayed feedback.
<em>TC</em>, <em>72</em>(6), 1610–1622. (<a
href="https://doi.org/10.1109/TC.2022.3215907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load balancing arises as a fundamental problem, underlying the dimensioning and operation of many computing and communication systems, such as job routing in data center clusters, multipath communication, Big Data and queueing systems. In essence, the decision-making agent maps each arriving job to one of the possibly heterogeneous servers while aiming at an optimization goal such as load balancing, low average delay or low loss rate. One main difficulty in finding optimal load balancing policies here is that the agent only partially observes the impact of its decisions, e.g., through the delayed acknowledgements of the served jobs. In this paper, we provide a partially observable (PO) model that captures the load balancing decisions in parallel buffered systems under limited information of delayed acknowledgements. We present a simulation model for this PO system to find a load balancing policy in real-time using a scalable Monte Carlo tree search algorithm. We numerically show that the resulting policy outperforms other limited information load balancing strategies such as variants of Join-the-Most-Observations and has comparable performance to full information strategies like: Join-the-Shortest-Queue, Join-the-Shortest-Queue(d) and Shortest-Expected-Delay. Finally, we show that our approach can optimise the real-time parallel processing by using network data provided by Kaggle.},
  archive      = {J_TC},
  author       = {Anam Tahir and Bastian Alt and Amr Rizk and Heinz Koeppl},
  doi          = {10.1109/TC.2022.3215907},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1610-1622},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Load balancing in compute clusters with delayed feedback},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating random forest on memory-constrained devices
through data storage optimization. <em>TC</em>, <em>72</em>(6),
1595–1609. (<a href="https://doi.org/10.1109/TC.2022.3215898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random forests is a widely used classification algorithm. It consists of a set of decision trees each of which is a classifier built on the basis of a random subset of the training data-set. In an environment where the memory work-space is low in comparison to the data-set size, when training a decision tree, a large proportion of the execution time is related to I/O operations. These are caused by data blocks transfers between the storage device and the memory work-space (in both directions). Our analysis of random forests training algorithms showed that there are two major issues : (1) Block Under-utilization: data blocks are poorly used when loaded into memory and have to be reloaded multiple times, meaning that the algorithm exhibits a poor spatial locality; (2) Data Over-read: the data-set is supposed to be fully loaded in memory whereas a large proportion of data are not effectively useful when building a decision tree. Our proposed solution is structured to address these two issues. First, we propose to reorganize the data-set in such a way to enhance spatial locality and second, to remove the assumption that the data-set is entirely loaded into memory and access data only when effectively needed. Our experiments show that this method made it possible to reduce random forest building time by 51 to 95\% in comparison to a state-of-the-art method.},
  archive      = {J_TC},
  author       = {Camélia Slimani and Chun-Feng Wu and Stéphane Rubini and Yuan-Hao Chang and Jalil Boukhobza},
  doi          = {10.1109/TC.2022.3215898},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1595-1609},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating random forest on memory-constrained devices through data storage optimization},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor based multivariate polynomial modulo multiplier for
cryptographic applications. <em>TC</em>, <em>72</em>(6), 1581–1594. (<a
href="https://doi.org/10.1109/TC.2022.3215638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modulo polynomial multiplication is an essential mathematical operation in the area of finite field arithmetic. Polynomial functions can be represented as tensors, which can be utilized as basic building blocks for various lattice-based post-quantum cryptography schemes. This paper presents a tensor-based novel modulo multiplication method for multivariate polynomials over $GF(2^{m})$ and is realized on the hardware platform (FPGA). The proposed method consumes $6.5\times$ less power and achieves more than $6\times$ speedup compared to other contemporary single variable polynomial multiplication implementations. Our method is embarrassingly parallel and easily scalable for multivariate polynomials. Polynomial functions of nine variables, where each variable is of degree 128, are tested with the proposed multiplier, and its corresponding area, power, and power-delay-area product (PDAP) are presented. The computational complexity of single variable and multivariate polynomial multiplications are $O(n)$ and $O(np)$ , respectively, where $n$ is the maximum degree of a polynomial having $p$ variables. Due to its high speed, low latency, and scalability, the proposed modulo multiplier can be used in a wide range of applications.},
  archive      = {J_TC},
  author       = {Bikram Paul and Angana Nath and Srinivasan Krishnaswamy and Jan Pidanic and Zdenek Nemec and Gaurav Trivedi},
  doi          = {10.1109/TC.2022.3215638},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1581-1594},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tensor based multivariate polynomial modulo multiplier for cryptographic applications},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified cryptoprocessor for lattice-based signature and
key-exchange. <em>TC</em>, <em>72</em>(6), 1568–1580. (<a
href="https://doi.org/10.1109/TC.2022.3215064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose design methodologies for building a compact, unified and programmable cryptoprocessor architecture that computes post-quantum key agreement and digital signature. Synergies in the two types of cryptographic primitives are used to make the cryptoprocessor compact. As a case study, the cryptoprocessor architecture has been optimized targeting the signature scheme ’CRYSTALS-Dilithium’ and the key encapsulation mechanism (KEM) ’Saber,’ both finalists in the NIST&#39;s post-quantum cryptography standardization project. The programmable cryptoprocessor executes key generations, encapsulations, decapsulations, signature generations, and signature verifications for all the security levels of Dilithium and Saber. On a Xilinx Ultrascale+ FPGA, the proposed cryptoprocessor consumes 18,406 LUTs, 9,323 FFs, 4 DSPs, and 24 BRAMs. It achieves 200 MHz clock frequency and finishes CCA-secure key-generation/encapsulation/decapsulation operations for LightSaber in 29.6/40.4/ 58.3 $\mu$ s; for Saber in 54.9/69.7/94.9 $\mu$ s; and for FireSaber in 87.6/108.0/139.4 $\mu$ s, respectively. It finishes key-generation/sign/verify operations for Dilithium-2 in 70.9/151.6/75.2 $\mu$ s; for Dilithium-3 in 114.7/237/127.6 $\mu$ s; and for Dilithium-5 in 194.2/342.1/228.9 $\mu$ s, respectively, for the best-case scenario. On UMC 65 nm library for ASIC the latency is improved by a factor of two due to a 2× increase in clock frequency.},
  archive      = {J_TC},
  author       = {Aikata Aikata and Ahmet Can Mert and David Jacquemin and Amitabh Das and Donald Matthews and Santosh Ghosh and Sujoy Sinha Roy},
  doi          = {10.1109/TC.2022.3215064},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1568-1580},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A unified cryptoprocessor for lattice-based signature and key-exchange},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance-power tradeoff in heterogeneous SaaS clouds with
trustworthiness guarantee. <em>TC</em>, <em>72</em>(6), 1554–1567. (<a
href="https://doi.org/10.1109/TC.2022.3214626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-as-a-service (SaaS) clouds grow dramatically due to cost-effectiveness, availability, and flexibility. Quality of service (QoS) and power, which represent performance and cost, respectively, are conflicting yet critical issues in the service scheduling of SaaS clouds, and some researchers have investigated the tradeoff between them. However, existing works do not involve QoS attacks in which untrusted service providers provide fake QoS values to absorb service requests, resulting in lower user experience and system profits. In this paper, we jointly consider the QoS performance, queue congestion, and energy consumption to formulate the performance-power tradeoff while considering QoS attacks. To address this NP scheduling problem, we propose a Lyapunov-based decomposition strategy that converts the original problem into three equivalent subproblems. By aggregating the solving strategies for the three subproblems, we develop the online service selection and trustworthiness management algorithm that optimizes the performance–power tradeoff while resisting QoS attacks. In addition, a light-weighted trustworthiness management strategy is designed to update trustworthiness values without storing large amounts of past information. Mathematical analyses and simulations demonstrate that our proposed control framework realizes detection and resistance of QoS attacks and a $[O(1 / V), O(V)]$ tradeoff between performance and power with a performance-power tradeoff parameter V.},
  archive      = {J_TC},
  author       = {Xinghui Zhu and Zijie Di and Qingsong Yao and Xuewen Dong and Jiandong Wang and Yulong Shen},
  doi          = {10.1109/TC.2022.3214626},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1554-1567},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Performance-power tradeoff in heterogeneous SaaS clouds with trustworthiness guarantee},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Eidetic: An in-memory matrix multiplication accelerator for
neural networks. <em>TC</em>, <em>72</em>(6), 1539–1553. (<a
href="https://doi.org/10.1109/TC.2022.3214151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the Eidetic architecture, which is an SRAM-based ASIC neural network accelerator that eliminates the need to continuously load weights from off-chip, while also minimizing the need to go off chip for intermediate results. Using in-situ arithmetic in the SRAM arrays, this architecture can supports a variety of precision types allowing for effective inference. We also present different data mapping policies for matrix-vector based networks (RNN and MLP) on the Eidetic architecture and describe the tradeoffs involved. With this architecture, multiple layers of a network can be concurrently mapped, storing both the layer weights and intermediate results on-chip, removing the energy and latency penalty of off-chip memory accesses. We evaluate Eidetic on Google&#39;s Neural Machine Translation System (GNMT) encoder and demonstrate a 17.20× increase in throughput and 7.77× reduction in average latency over a single TPUv2 chip.},
  archive      = {J_TC},
  author       = {Charles Eckert and Arun Subramaniyan and Xiaowei Wang and Charles Augustine and Ravishankar Iyer and Reetuparna Das},
  doi          = {10.1109/TC.2022.3214151},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1539-1553},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Eidetic: An in-memory matrix multiplication accelerator for neural networks},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ADLPT: Improving 3D NAND flash memory reliability by
adaptive lifetime prediction techniques. <em>TC</em>, <em>72</em>(6),
1525–1538. (<a href="https://doi.org/10.1109/TC.2022.3214115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NAND flash memory has become increasingly popular in various computing systems. Although NAND flash memory offers attractive performance, it suffers limited operable programming and erasing cycles. To improve the reliability of flash-based systems, previous works introduce machine learning models to predict flash lifetime. These works generally focus on improving prediction accuracy but present little research about the resources required for flash lifetime prediction. In application scenarios, the overheads and the frequency of lifetime predictions are important for storage systems. Excessive prediction actions would lead to unnecessary resource consumption. For building an efficient storage system, resource requirements need to be taken into consideration when designing flash lifetime prediction schemes. In this paper, we propose adaptive lifetime prediction techniques (ADLPT) that minimize redundant prediction operations by exploiting reliability variation. To explore reliability variation, we investigate the error distribution of different 3D flash chips. Based on the investigation, a prediction judgment method is presented. The method identifies the necessary prediction by detecting the variation of erase duration and raw bit errors. Furthermore, we provide a method to improve the performance of the static model. The experimental result shows that our approach can reduce about 90\% of redundant predictions with over 0.8 F1-Score.},
  archive      = {J_TC},
  author       = {Yuqian Pan and Zhaojun Lu and Haichun Zhang and Haoming Zhang and Md Tanvir Arafin and Zhenglin Liu and Gang Qu},
  doi          = {10.1109/TC.2022.3214115},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {1525-1538},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ADLPT: Improving 3D NAND flash memory reliability by adaptive lifetime prediction techniques},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast hardware pseudorandom number generator based on
xoroshiro128. <em>TC</em>, <em>72</em>(5), 1518–1524. (<a
href="https://doi.org/10.1109/TC.2022.3204226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Graphcore Intelligent Processing Unit contains an original pseudorandom number generator (PRNG) called xoroshiro128aox, based on the ${ \boldsymbol{F}}_{2}$ -linear generator xoroshiro128. It is designed to be cheap to implement in hardware and provide high-quality statistical randomness. In this paper, we present a rigorous assessment of the generator&#39;s quality using standard statistical test suites and compare the results with the fast contemporary PRNGs xoroshiro128+, pcg64 and philox4x32-10. We show that xoroshiro128aox mitigates the known weakness in the lower order bits of xoroshiro128+ with a new ’AOX’ output function by passing the BigCrush and PractRand suites, but we note that the function has some minor non uniformities. We focus our testing with specific tests for linear artefacts to highlight the weaknesses of both xoroshiro128 PRNGs, but conclude that they are hard to detect, and xoroshiro128aox otherwise provides a good trade off between statistical quality and hardware implementation cost.},
  archive      = {J_TC},
  author       = {James Hanlon and Stephen Felix},
  doi          = {10.1109/TC.2022.3204226},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1518-1524},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A fast hardware pseudorandom number generator based on xoroshiro128},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking DRAM’s page mode with STT-MRAM. <em>TC</em>,
<em>72</em>(5), 1503–1517. (<a
href="https://doi.org/10.1109/TC.2022.3207131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spin torque magnetic random access memory (STT-MRAM) is a promising candidate for drop-in replacement for DRAM-based main memory because of its higher energy efficiency and similar latency to dynamic random access memory (DRAM). However, simply replacing DRAM with STT-MRAM without optimizations severely limits STT-MRAM from exploiting its full potential. STT-MRAM employs costly sense amplifiers that demand an order of magnitude more area and power than DRAM. To manage the high cost, STT-MRAM shares one sense amplifier across multiple bit-lines, exploiting the non-destructive nature of its read operation. This sense amplifier sharing reduces the size of row buffers; as a result, it incurs higher activation energy and lower performance. Other issues arise if STT-MRAM is required to be compatible with DRAM interfaces and policies. To address these challenges in a cost-effective manner, we propose STT-MRAM ARchiTecture supporting smart activation and sensing (SMART) that, unlike DRAM and conventional STT-MRAM, waits to do bit-line sensing until after receiving a column access command instead of a row activation command. This results in several benefits: larger pages, fewer sense amplifiers, lower activation power, higher bank-level parallelism, shorter latency, fewer address pins, and more efficient repairing of defective columns than conventional STT-MRAM. Our evaluation shows that SMART consumes lower energy while providing higher performance than conventional STT-MRAM and DRAM. Additionally, SMART consumes less area compared to conventional STT-MRAM.},
  archive      = {J_TC},
  author       = {Byoungchan Oh and Nilmini Abeyratne and Nam Sung Kim and Jeongseob Ahn and Ronald G. Dreslinski and Trevor Mudge},
  doi          = {10.1109/TC.2022.3207131},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1503-1517},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Rethinking DRAM&#39;s page mode with STT-MRAM},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ROLLED: Racetrack memory optimized linear layout and
efficient decomposition of decision trees. <em>TC</em>, <em>72</em>(5),
1488–1502. (<a href="https://doi.org/10.1109/TC.2022.3197094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern low power distributed systems tend to integrate machine learning algorithms. In resource-constrained setups, the execution of the models has to be optimized for performance and energy consumption. Racetrack memory (RTM) promises to achieve these goals by offering unprecedented integration density, smaller access latency, and reduced energy consumption. However, to access data in RTM, it needs to be shifted to the access port first. We investigate decision trees and develop placement strategies to reduce the total number of shifts in RTM. Decision trees allow profiling during training, resulting in tree paths’ access probabilities. We map tree nodes to RTM so that the total number of shifts is minimal. Concretely, we present two different placement approaches: 1) where tree nodes are closely packed and placed uniformly in a single RTM location and 2) where decision tree nodes are decomposed to separate RTM blocks. We discuss theoretical cost models for both approaches, we formally prove an upper bound of $4\times$ for the unified and an upper bound of $12\times$ for the decomposed organization towards the optimal placement. We conduct a thorough experimental evaluation to compare our algorithms to the state-of-the-art placement strategies Our experimental evaluations show that the unified and decomposed solutions reduce the number of shifts by $58.1\%$58.1\% and $80.1\%$80.1\% , respectively, leading to a $53.8\%$53.8\% and $46.3\%$46.3\% reduction in the overall runtime and $52.6\%$52.6\% and $61.7\%$61.7\% reduction in the energy consumption, compared to a naive baseline.},
  archive      = {J_TC},
  author       = {Christian Hakert and Asif Ali Khan and Kuan-Hsun Chen and Fazal Hameed and Jeronimo Castrillon and Jian-Jia Chen},
  doi          = {10.1109/TC.2022.3197094},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1488-1502},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ROLLED: Racetrack memory optimized linear layout and efficient decomposition of decision trees},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ISPA: Exploiting intra-SM parallelism in GPUs via
fine-grained resource management. <em>TC</em>, <em>72</em>(5),
1473–1487. (<a href="https://doi.org/10.1109/TC.2022.3214088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging GPUs have multiple Streaming Multiprocessors (SM), while each SM is comprised of CUDA Cores and Tensor Cores. While CUDA Cores do the general computation, Tensor Cores are designed to speed up matrix multiplication for deep learning applications. However, a GPU kernel often either uses CUDA Cores or Tensor Cores, leaving the other processing units idle. Although many prior research works have been proposed to co-locate kernels to improve GPU utilization, they cannot leverage the Intra-SM CUDA Core-Tensor Core Parallelism. Specifically, ISPA designs persistent and elastic block to solve the thread slot and shared memory contention between co-located kernels. ISPA also adopts the register allocation method to manage the register contention. These resource management methods are applicable for both white-box kernels and $cudnn$ kernels. Experimental results on an Nvidia 2080Ti GPU show that ISPA improves the system-wide throughput by 15.3\% for white-box workloads, and 7.1\% for $cudnn$ -based workloads compared with prior co-location work.},
  archive      = {J_TC},
  author       = {Han Zhao and Weihao Cui and Quan Chen and Minyi Guo},
  doi          = {10.1109/TC.2022.3214088},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1473-1487},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ISPA: Exploiting intra-SM parallelism in GPUs via fine-grained resource management},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memristor-based spectral decomposition of matrices and its
applications. <em>TC</em>, <em>72</em>(5), 1460–1472. (<a
href="https://doi.org/10.1109/TC.2022.3202746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently developed memristor technology allows for extremely fast implementation of a number of important matrix operations and algorithms. Moreover, the existence of fast matrix-vector operations offers the opportunity to design new matrix algorithms that exploit these operations. Here, we focus on the spectral decomposition of matrices, a task that plays an important role in a wide variety of applications from different engineering and scientific fields, including network science, control theory, advanced dynamics, and quantum mechanics. While there are a number of algorithms designed to find eigenvalues and eigenvectors of a matrix, these methods often suffer from poor running time performance. In this work, we present an algorithm for finding eigenvalues and eigenvectors that is designed to be used on memristor crossbar arrays. Although this algorithm can be implemented in a non-memristive system, its fast running time relies on the availability of extremely fast matrix-vector multiplication, as is offered by a memristor crossbar array. In this paper, we (1) show the running time improvements of existing eigendecomposition algorithms when matrix-vector multiplications are performed on a memristor crossbar array, and (2) present EigSweep , a novel, fully-parallel, fast and flexible eigendecomposition algorithm that gives an improvement in running time over traditional eigendecomposition algorithms when all are accelerated by a memristor crossbar. We discuss algorithmic aspects as well as hardware-related aspects of the implementation of EigSweep , and perform an extensive experimental analysis on real-world and synthetic matrices.},
  archive      = {J_TC},
  author       = {Zeinab S. Jalali and Chenghong Wang and Griffin Kearney and Geng Yuan and Caiwen Ding and Yinan Zhou and Yanzhi Wang and Sucheta Soundarajan},
  doi          = {10.1109/TC.2022.3202746},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1460-1472},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Memristor-based spectral decomposition of matrices and its applications},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A low-cost burn-in tester architecture to supply effective
electrical stress. <em>TC</em>, <em>72</em>(5), 1447–1459. (<a
href="https://doi.org/10.1109/TC.2022.3199994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Burn-In test equipment usually owns extensive memory capabilities to store pre-computed patterns to be applied to the circuit inputs as well as ad-hoc circuitries to drive and read the DUT pins during the BI phase. The solution proposed in this paper dramatically reduces the memory size requirement and just demands a generic microcontroller unit (MCU) equipped with a couple of embedded processors, some standard common peripheral units, and a few KB memories. Moreover, the proposed Burn-In tester could be integrated into a System Level Test equipment which is typically based on MCUs to communicate functionally with the DUT. This paper provides full details about the architecture of such a low-cost innovative tester, which can supply the DUT with unlimited pseudo-random patterns created autonomously by the MCU firmware from any selected seed. The tester prototype developed to collect experimental results includes a low-cost System-on-Chip based on a multi-core MCU and a set of peripheral cores, encompassing timers and Direct Memory Access modules. The tester prototype is used to stress an automotive chip accounting for about 20 million gates, 700 thousand scan flip-flops, and several scan modes. The combination of pseudo-random pattern generation with the ability to control different scan and Design for Testability (DfT) modes, including LBIST, permits to reach a higher coverage of stress metrics than by the application of a limited set of pre-computed ATPG patterns. The toggle coverage level reached is up to 95.89\%. The application speed achieved by the tester with non-optimized connections is up to about 10MHz.},
  archive      = {J_TC},
  author       = {Francesco Angione and Davide Appello and Paolo Bernardi and Claudia Bertani and Giovambattista Gallo and Stefano Littardi and Giorgio Pollaccia and Walter Ruggeri and Matteo Sonza Reorda and Vincenzo Tancorre and Roberto Ugioli},
  doi          = {10.1109/TC.2022.3199994},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1447-1459},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A low-cost burn-in tester architecture to supply effective electrical stress},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating reinforcement learning-based CCSL specification
synthesis using curiosity-driven exploration. <em>TC</em>,
<em>72</em>(5), 1431–1446. (<a
href="https://doi.org/10.1109/TC.2022.3197956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Clock Constraint Specification Language (CCSL) has been widely acknowledged as a promising system-level specification for the modeling and analysis of timing behaviors of real-time and embedded systems. However, along with the increasing complexity of modern systems coupled with strict time-to-market constraints, it becomes more and more difficult for requirement engineers to accurately figure out CCSL specifications from natural language-based requirement documents, since they lack both expertise in formal CCSL modeling and design automation tools to support quick and automatic generation of CCSL specifications. To solve the above problem, in this paper we introduce a novel and efficient Reinforcement Learning (RL)-based synthesis approach that can facilitate requirement engineers to quickly figure out their expected CCSL specifications. For a given incomplete CCSL specification, our approach adopts RL-based enumeration to explore all the feasible solutions to fill the holes within CCSL constraints, and leverages curiosity-driven exploration to accelerate the enumeration process. Based on the combination of our proposed curiosity-driven exploration heuristic and deductive reasoning techniques, our approach can not only prune unfruitful enumeration solutions effectively, but also optimize the enumeration process to search for the tightest solution quickly, thus the overall synthesis process can be accelerated dramatically. Comprehensive experimental results demonstrate that our approach significantly outperforms state-of-the-art methods in terms of both synthesis time and synthesis accuracy.},
  archive      = {J_TC},
  author       = {Ming Hu and Min Zhang and Frédéric Mallet and Xin Fu and Mingsong Chen},
  doi          = {10.1109/TC.2022.3197956},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1431-1446},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating reinforcement learning-based CCSL specification synthesis using curiosity-driven exploration},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Systematic prevention of on-core timing channels by full
temporal partitioning. <em>TC</em>, <em>72</em>(5), 1420–1430. (<a
href="https://doi.org/10.1109/TC.2022.3212636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microarchitectural timing channels enable unwanted information flow across security boundaries, violating fundamental security assumptions. They leverage timing variations of several state-holding microarchitectural components and have been demonstrated across instruction set architectures and hardware implementations. Analogously to memory protection, (Ge et al. 2019) have proposed time protection for preventing information leakage via timing channels. They also showed that time protection calls for hardware support. This work leverages the open and extensible RISC-V instruction set architecture (ISA) to introduce the temporal fence instruction fence.t , which provides the required mechanisms by clearing vulnerable microarchitectural state and guaranteeing a history-independent context-switch latency. We propose and discuss three different implementations of fence.t and implement them on an experimental version of the seL4 microkernel (Klein et al. 2014) and CVA6, an open-source, in-order, application class, 64-bit RISC-V core (Zaruba and Benini 2019). We find that a complete, systematic, ISA-supported erasure of all non-architectural core components is the most effective implementation while featuring a low implementation effort, a minimal performance overhead of less than 1\%, and negligible hardware costs.},
  archive      = {J_TC},
  author       = {Nils Wistoff and Moritz Schneider and Frank K. Gürkaynak and Gernot Heiser and Luca Benini},
  doi          = {10.1109/TC.2022.3212636},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1420-1430},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Systematic prevention of on-core timing channels by full temporal partitioning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attacking the privacy of approximate membership check
filters by positive concentration. <em>TC</em>, <em>72</em>(5),
1409–1419. (<a href="https://doi.org/10.1109/TC.2022.3202753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate membership check filters are increasingly used to speed up data processing in many applications. Also, privacy is becoming a key design objective for many systems and thus, the privacy of filters needs to be carefully considered. Previous works have shown that an attacker that knows the implementation details of the filter and has access to its content, may be able to extract some information about the elements stored in the filter. This attack is, however, specific to Bloom filters and requires that the universe of elements must be small. In this article, we show that in many practical settings, an attacker that has only a black-box access to the filter, can extract information about the elements stored in the filter regardless of the specific filter type and the universe size. This is possible based on the key observation that in many applications, the elements stored in the filter are not randomly chosen, but they are concentrated in one or more parts of the universe of elements. To identify these parts, the positive probability can be measured on different parts of the universe; the parts having significantly larger values than the average positive probability for the filter are the ones on which the filter elements are concentrated. This approach is formalized and applied to several case studies showing the process by which the attacker can get additional information about the elements stored for the filters in a wide range of scenarios.},
  archive      = {J_TC},
  author       = {Pedro Reviriego and Alfonso Sánchez-Macián and Elena Merino-Gómez and Ori Rottenstreich and Shanshan Liu and Fabrizio Lombardi},
  doi          = {10.1109/TC.2022.3202753},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1409-1419},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Attacking the privacy of approximate membership check filters by positive concentration},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Breaking fault attack countermeasures with side-channel
information. <em>TC</em>, <em>72</em>(5), 1396–1408. (<a
href="https://doi.org/10.1109/TC.2022.3211437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the persistent fault-based collision attack (PFCA) (Zheng et al. 2021), the adversary captures the information that the intermediate states have collided through identical correct/incorrect ciphertexts. However, fault countermeasures achieve suppression of incorrect ciphertexts and prevent the PFCA. In this paper, we measure the collision of internal states (or state bytes) using side-channel information. First, for round-level countermeasures, we identify state bytes hitting the same persistent fault during the first round of encryption by the shortest runtime. Additionally, we design sliding-window algorithms to automatically identify the runtime of one-round encryptions suitable for different execution environments. Second, for algorithm-level protections, we detect the collision of the internal states after the first round of encryption through the maximum similarity of power consumption traces. Meanwhile, to address the low success rate of key recovery caused by miss detection due to noise within runtime or power consumption, we further revise the original filtering algorithm in PFCA. Third, we implement round-level protected AES on PC to measure runtime, and both AES protected by round-level (or algorithm-level) countermeasures and SM4 (ISO/IEC 2021) protected by a round-level countermeasure on a smart card to collect power consumption. Finally, the experimental result proves that the revised PFCA successfully recovers the key.},
  archive      = {J_TC},
  author       = {Shihui Zheng and Ruihao Xing and Junlong Lai and Junkai Liu and Haofeng Wang and Changhai Ou},
  doi          = {10.1109/TC.2022.3211437},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1396-1408},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Breaking fault attack countermeasures with side-channel information},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight authentication scheme for data dissemination in
cloud-assisted healthcare IoT. <em>TC</em>, <em>72</em>(5), 1384–1395.
(<a href="https://doi.org/10.1109/TC.2022.3207138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in the Internet of Things (IoT) and cloud computing technologies have accelerated the development of various practical applications, including healthcare systems. Adequately revealing the collected healthcare data in a cloud-assisted healthcare IoT system brings a huge potential for improving the safety, quality, and efficiency of healthcare services. However, often the data collected in a healthcare IoT system is vital and sensitive. The dissemination of such data is also vulnerable to malicious attacks such as tampering, eavesdropping, and forgery. Thus, disseminated data&#39;s integrity, authenticity, and privacy are elementary security demands to end-users and owners. Also, the resource-constrained nature of healthcare IoT devices invalidates the existing solutions. To address the above challenges, we propose a lightweight and secure redactable signature scheme with coarse-grained additional redaction control (CRS) for secure dissemination of healthcare data in a cloud-assisted healthcare IoT system. The security analysis indicates our CRS is secure against signature forgery, additional redaction attacks, and redacted version linkability. Compared to other existing solutions, our scheme can achieve some level of security but less computational complexity and communication overhead.},
  archive      = {J_TC},
  author       = {Jianghua Liu and Jian Yang and Wei Wu and Xinyi Huang and Yang Xiang},
  doi          = {10.1109/TC.2022.3207138},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1384-1395},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lightweight authentication scheme for data dissemination in cloud-assisted healthcare IoT},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An economy-oriented GPU virtualization with dynamic and
adaptive oversubscription. <em>TC</em>, <em>72</em>(5), 1371–1383. (<a
href="https://doi.org/10.1109/TC.2022.3199998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPU is becoming attractive around multiple academic and industrial area because of its massively parallel computing ability. However, there are still some obstacles which the GPU virtualization technologies should overcome to reach their maturity. These obstacles mainly include the problem of resource allocation strategy to guarantee possible higher yield. This shortage has already become an obvious barrier to the practical GPU usage in the cloud for satisfying business and academical requirements. There are many mature pieces of research in the area of oversubscribed cloud computing to enhance economic efficiency. However, the study on GPU oversubscription is almost blank for the just started use of GPU in cloud computing. This paper introduces gOver, an economy-oriented GPU resource oversubscription system based on the GPU virtualization platform. gOver is able to share and modulate GPU resource among workloads in an adaptive and dynamic manner, guaranteeing the QoS level at the same time. We evaluate the proposed gOver strategy with designed experiments with specific workload characteristics. The experimental results show that our dynamic GPU oversubscription solution improves the economic efficiency by 20\% over traditional GPU sharing strategy, and outperforms the static oversubscription method by much better stability in QoS control.},
  archive      = {J_TC},
  author       = {Jianguo Yao and Qiumin Lu and Run Tian and Keqin Li and Haibing Guan},
  doi          = {10.1109/TC.2022.3199998},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1371-1383},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An economy-oriented GPU virtualization with dynamic and adaptive oversubscription},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COP: A combinational optimization power budgeting method for
manycore systems in dark silicon. <em>TC</em>, <em>72</em>(5),
1356–1370. (<a href="https://doi.org/10.1109/TC.2022.3211417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dark silicon is a phenomenon of under-utilization in today&#39;s manycore systems due to power and thermal limitations. In order to improve the performance of dark silicon systems, it is necessary to adopt dynamic power constraints for different core mapping decisions. However, existing power budgeting methods are generally over pessimistic, e.g., Thermal Safe Power (TSP), or over optimistic, e.g., Greedy based Dynamic Power (GDP). This paper proposes a practical power budgeting method, called Combinational Optimization Power (COP). Different from existing methods, which ignore some actual factors, such as communication overhead and lifetime reliability, COP formulates the power budgeting problem as a thermal-constrained combinational optimization power problem. For the steady-state case, COP achieves the target fusion of optimized temperature and communication energy consumption by applying task priority ranking and task-to-core mapping. For the transient case, COP uses the rainflow counting algorithm to construct the reliability framework based on the thermal cycling failure mechanism, and then establishes a linear time-invariant transient temperature model to obtain the core mapping selection and the corresponding dynamic power budget. Experimental results demonstrate that COP is capable of providing an optimized core mapping decision, which can maximize power budget while ensuring the system performance.},
  archive      = {J_TC},
  author       = {Xin Li and Zhi Li and Yaqi Ju and Xiaofei Zhang and Rongyao Wang and Wei Zhou},
  doi          = {10.1109/TC.2022.3211417},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1356-1370},
  shortjournal = {IEEE Trans. Comput.},
  title        = {COP: A combinational optimization power budgeting method for manycore systems in dark silicon},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting the common case when accelerating input-dependent
stream processing by FPGA. <em>TC</em>, <em>72</em>(5), 1343–1355. (<a
href="https://doi.org/10.1109/TC.2022.3200576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FPGAs have traditionally been successful in accelerating stream processing applications where the amount and type of work performed on each record—e.g., image, packet—do not depend on the record&#39;s contents. On the other hand, accelerating ‘input-dependent’ stream processing on FPGAs presents a much more challenging problem where different records in the stream can require widely different operations. It is inefficient and unnecessary to support all operations at the same throughput when the distributions of the operations are skewed. In this paper, we examine the application of the ”make the common case fast” strategy to efficiently accelerate ‘input-dependent’ stream processing on FPGAs. In particular, we study the use of fast-slow path and early-exit techniques in the design of an FPGA-accelerated network intrusion prevention system (IPS). To avoid overfitting when common-case behavior is varied, we further examine compile-time re-tuning and runtime adaptation techniques. A quantitative analysis shows that fast-slow path and early-exit techniques can save an order of magnitude of resources compared to a common-case unaware IPS design. Compile-time re-tuning to specific conditions further achieves 30\% – 94\% BRAM savings relative to a generalized design. Adding runtime adaptation improves the zero-loss throughput by 1.43 – 2.75 × compared to a fixed design.},
  archive      = {J_TC},
  author       = {Zhipeng Zhao and Joseph Melber and Siddharth Sahay and Shashank Obla and Eriko Nurvitadhi and James C. Hoe},
  doi          = {10.1109/TC.2022.3200576},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1343-1355},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploiting the common case when accelerating input-dependent stream processing by FPGA},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated customization of on-device inference for
quality-of-experience enhancement. <em>TC</em>, <em>72</em>(5),
1329–1342. (<a href="https://doi.org/10.1109/TC.2022.3208207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid uptake of intelligent applications is pushing deep learning (DL) capabilities to mobile devices. However, the heterogeneities in device capacity, DNN performances, and user preferences make it challenging to provide satisfactory Quality of Experience (QoE) to mobile users. This paper studies automated customization for DL inference on mobile devices (termed as on-device inference), and our goal is to enhance user QoE by configuring the on-device inference with an appropriate DNN for users under different usage scenarios. The core of our method is a DNN selection module that learns user QoE patterns on-the-fly and identifies the best-fit DNN for on-device inference with the learned knowledge. It leverages an online learning algorithm, NeuralUCB , that has excellent generalization ability for handling various user QoE patterns. We also embed the knowledge transfer technique in NeuralUCB to expedite the learning process. However, NeuralUCB frequently solicits QoE ratings from users, which incurs non-negligible inconvenience. To address this problem, we design feedback solicitation schemes to reduce the number of QoE solicitations while maintaining the learning efficiency of NeuralUCB. A pragmatic problem, aggregated QoE , is further investigated to improve the practicality of our framework. We conduct experiments on both synthetic and real-world data. The results indicate that our method efficiently learns the user QoE pattern with few solicitations and provides drastic QoE enhancement for mobile devices.},
  archive      = {J_TC},
  author       = {Yang Bai and Lixing Chen and Shaolei Ren and Jie Xu},
  doi          = {10.1109/TC.2022.3208207},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1329-1342},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Automated customization of on-device inference for quality-of-experience enhancement},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Serving multi-DNN workloads on FPGAs: A coordinated
architecture, scheduling, and mapping perspective. <em>TC</em>,
<em>72</em>(5), 1314–1328. (<a
href="https://doi.org/10.1109/TC.2022.3214113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Network (DNN) INFerence-as-a-Service (INFaaS) is the dominating workload in current data centers, for which FPGAs become promising hardware platforms because of their high flexibility and energy efficiency. The dynamic and multi-tenancy nature of INFaaS requires careful design in three aspects: multi-tenant architecture, multi-DNN scheduling, and multi-core mapping. These three factors are critical to the system latency and energy efficiency but are also challenging to optimize since they are tightly coupled and correlated. This paper proposes H3M , an automatic Design Space Exploration (DSE) framework to jointly optimize the architecture , scheduling , and mapping for serving INFaaS on cloud FPGAs. H3M explores: (1) the architecture design space with Heterogeneous spatial Multi-tenant sub-accelerators, (2) layer-wise scheduling for Heterogeneous Multi-DNN workloads, and (3) single-layer mapping to the Homogeneous Multi-core architecture. H3M beats state-of-the-art multi-tenant DNN accelerators, Planaria and Herald, by up to 7.5× and 3.6× in Energy-Delay-Product (EDP) reduction on the ASIC platform. On the Xilinx U200 and U280 FPGA platforms, H3M offers 2.1-5.7× and 1.8-9.0× EDP reduction over Herald.},
  archive      = {J_TC},
  author       = {Shulin Zeng and Guohao Dai and Niansong Zhang and Xinhao Yang and Haoyu Zhang and Zhenhua Zhu and Huazhong Yang and Yu Wang},
  doi          = {10.1109/TC.2022.3214113},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1314-1328},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Serving multi-DNN workloads on FPGAs: A coordinated architecture, scheduling, and mapping perspective},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DMACN: A dynamic multi-attribute caching mechanism for
NDN-based remote health monitoring system. <em>TC</em>, <em>72</em>(5),
1301–1313. (<a href="https://doi.org/10.1109/TC.2022.3197955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Data Network (NDN) advocates the philosophy of accessing IoT data owing to its location independence feature. This enables routers to pre-cache content and serves the future requests for the same content on a local basis. Such architecture demonstrates huge application potential in the E-health field. In order to achieve efficient healthcare treatment and administration for both patients and medical professions, the optimization of storing patients’ real-time, large-scale physical data is of necessity. We propose a D ynamic M ulti- A ttribute C aching mechanism for N DN-Based remote health monitoring system (DMACN). In our model, we adopted a predictable consumer-driven freshness mechanism with low computation cost to satisfy the freshness-sensitive nature of health data. A novel content popularity model based on Analytic Hierarchy Process (AHP) and medical-grade parameters are proposed to handle the doctor-decision-making-coupled Interest sending mechanism of a remote health monitoring system. The final simulation results show DMACN has strong robustness against intensive requesting and complex contents. It is also shown that its performance surpasses existing mechanisms. The Cache Hit Ratio exceeds 37.5\% to FIFO and LRU, 220\% compared with CPFC, and 55\% for CTDICR for both consumer and producer tests. On the other hand, the Latency is about 23.8\% lower than FIFO and LRU, 46.6\% lower compared with CPFC, and 35.4\% for CTDICR.},
  archive      = {J_TC},
  author       = {Pushpendu Kar and Kewei Chen and Jiayi Shi},
  doi          = {10.1109/TC.2022.3197955},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1301-1313},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DMACN: A dynamic multi-attribute caching mechanism for NDN-based remote health monitoring system},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Type-aware federated scheduling for typed DAG tasks on
heterogeneous multicore platforms. <em>TC</em>, <em>72</em>(5),
1286–1300. (<a href="https://doi.org/10.1109/TC.2022.3202748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To utilize the performance benefits of heterogeneous multicore platforms in real-time systems, we need task models that expose the parallelism and heterogeneity of the workload, such as typed DAG tasks, as well as scheduling algorithms that effectively exploit this information. In this article, we introduce type-aware federated scheduling algorithms for sporadic typed DAG tasks with implicit deadlines running on a heterogeneous multicore platform with two different types of cores. In type-aware federated scheduling, a task can be executed in one of the three strategies: Exclusive Allocation , Semi-Exclusive Allocation , and Sequential and Share . In Exclusive Allocation , clusters of cores of both core types are exclusively allocated to tasks, while cores of only one type are exclusively allocated to tasks in Semi-Exclusive Allocation . The workload of the other type from tasks in Semi-Exclusive Allocation and the workload from tasks in Sequential and Share share the cores that are not exclusively allocated to any task. We prove that our type-aware federated scheduling algorithm has a capacity augmentation bound of 7.25. We also show that no constant capacity augmentation bound can be obtained without Semi-Exclusive Allocation . Compared to the state of the art, the type-aware federated scheduling algorithm achieves better schedulability, especially for task sets with skewed workload.},
  archive      = {J_TC},
  author       = {Ching-Chi Lin and Junjie Shi and Niklas Ueter and Mario Günzel and Jan Reineke and Jian-Jia Chen},
  doi          = {10.1109/TC.2022.3202748},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1286-1300},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Type-aware federated scheduling for typed DAG tasks on heterogeneous multicore platforms},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PMDB: A range-based key-value store on hybrid NVM-storage
systems. <em>TC</em>, <em>72</em>(5), 1274–1285. (<a
href="https://doi.org/10.1109/TC.2022.3202755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging Nov-Volatile Memory (NVM) may replace DRAM as main memory in future computers. However, data will likely still be stored on storage due to the enormous large size of available data. We investigate how key-value stores can be efficiently designed and implemented in a hybrid system, called NVM-Storage system, consisting of NVM as memory and traditional storage. We first discuss the performance trade-offs among Put, Get, and Range Query of the existing designs. Then, we propose PMDB, a range-based key-value store on NVM-Storage systems. PMDB achieves good performance for Put, Get and Range Query at the same time by utilizing a range-based data management and deploying a light-weight index on NVM. We compare PMDB with the state-of-the-art schemes including SLM-DB [21] and MatrixKV [40] for hybrid NVM-storage systems. Evaluation results indicate that in workloads with mixed Put, Get and Range Queries, PMDB outperforms existing key-value stores by $1.16\times$ – $2.49\times$ .},
  archive      = {J_TC},
  author       = {Baoquan Zhang and Haoyu Gong and David H.C. Du},
  doi          = {10.1109/TC.2022.3202755},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1274-1285},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PMDB: A range-based key-value store on hybrid NVM-storage systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards globally optimal design of multipliers for FPGAs.
<em>TC</em>, <em>72</em>(5), 1261–1273. (<a
href="https://doi.org/10.1109/TC.2023.3238128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of a multiplier typically consists of three steps: (1) partial product generation, (2) compressor tree design and (3) the selection of the final adder. Conventionally, these three steps are performed consecutively. However, when targeting FPGAs, there are many possibilities in all three design steps that heavily influence each other. This proposal presents for the first time a holistic optimization, combining all three optimization steps yielding a minimum amount of look-up-tables (LUTs) while it can also guarantee the minimal number of (pipeline) stages. An ILP-formulation for the determination of a combined, globally optimal solution for the multiplier tiling, compressor tree generation and final adder selection is proposed. With globally optimal we mean that the best solution is found for a given set of sub-multipliers for partial product generation, compressors and final adder.This allows to improve the quality and evaluate the limitations of existing heuristic 3-step approaches. It is shown experimentally for the example of Xilinx FPGAs, that globally optimal solutions can be obtained for multiplier sizes of practical relevance, leading to significant LUT reductions. Additional packing density experiments show that a significantly larger number of multiplier instances can be mapped to the same device.},
  archive      = {J_TC},
  author       = {Andreas Böttcher and Martin Kumm},
  doi          = {10.1109/TC.2023.3238128},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1261-1273},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards globally optimal design of multipliers for FPGAs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A resource efficient software-hardware co-design of
lattice-based homomorphic encryption scheme on the FPGA. <em>TC</em>,
<em>72</em>(5), 1247–1260. (<a
href="https://doi.org/10.1109/TC.2022.3198628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lattice-based homomorphic encryption schemes provide strong resistance against quantum and classical computer-based adversary security attacks. In this article, we present a software-hardware co-design of two partially homomorphic encryption (PHE) schemes employing an ARM-System on Chip (ARM-SoC) and an field programmable gate array (FPGA). This provides necessary acceleration to PHE methods in the ecosystem mentioned above. The first PHE scheme is designed for generic homomorphic encryption, while the second scheme is aimed at resource optimized lightweight IoT-driven applications. For seamless assimilation, a robust and reliable low latency data transfer protocol is developed between the FPGA-based accelerator IP and ARM-SoC host system. The proposed PHE schemes are realized using Verilog hardware description language on multiple FPGA platforms. The proposed lightweight scheme is $52.71\times$ more resource-efficient than the pipelined BGV RLWE-based method. It exhibits $1.43\times$ and $1.29\times$ better throughput than non-pipelined and pipelined realizations of the BGV RLWE-based scheme. The proposed hardware accelerators realized on FPGA platforms having lesser clock speed and consuming lower resources showcase significant speedup compared to their software implementations making our proposed method an efficient alternative to enhance security in edge-enabled IoT devices.},
  archive      = {J_TC},
  author       = {Bikram Paul and Tarun Kumar Yadav and Balbir Singh and Srinivasan Krishnaswamy and Gaurav Trivedi},
  doi          = {10.1109/TC.2022.3198628},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1247-1260},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A resource efficient software-hardware co-design of lattice-based homomorphic encryption scheme on the FPGA},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resilient embedded systems designs via on the fly generation
of adaptive degenerate components using machine learning. <em>TC</em>,
<em>72</em>(5), 1236–1246. (<a
href="https://doi.org/10.1109/TC.2022.3204230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defined radio (SDR) provides significant advantages over traditional analog radio systems and are becoming increasingly relied on for ”mission critical” applications. This along with risk of trojans, single-event upsets and human error creates the necessity for fault tolerant systems. Redundancy has been traditionally used to implement fault tolerance but incurs a substantial area overhead which is undesirable in most applications. Advancements in field-programmable gate array and system on a chip technologies have made implementing machine learning (ML) algorithms within embedded systems feasible. In this paper we explore the use of ML to implement fault tolerance in an SDR. Our approach, which we call adaptive component-level degeneracy (ACD), uses a ML model to learn the functionality of an SDR component. Once trained, the model can detect when the component is compromised and mitigate the issue with its own output. We demonstrate the ability of our model to learn multiple simulated SDR components. We compare the one-dimensional convolutional neural network and bidirectional recurrent neural network architectures at modeling time series components. We also implement ACD within a real-time SDR system using GNU Radio Companion. The results show great potential for the utilization of ML techniques for improving embedded system reliability.},
  archive      = {J_TC},
  author       = {Corey Butts and Rashmi Jha and Temesguen Messay-Kebede and David Kapp},
  doi          = {10.1109/TC.2022.3204230},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1236-1246},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Resilient embedded systems designs via on the fly generation of adaptive degenerate components using machine learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum secret permutating protocol. <em>TC</em>,
<em>72</em>(5), 1223–1235. (<a
href="https://doi.org/10.1109/TC.2022.3207121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern cryptography, distributing a private and unique index number to each participant is an important cryptographic task, which can be adopted to efficiently solve many complicated secure multiparty computations. In this paper, we define this cryptographic primitive, called Secret Permutating, in which every one of $n$ participants can get a random but unique secret ${k}_i \in { {1,2, \ldots,n} }$ . Furthermore, we focus on the unconditional security of Secret Permutating based on laws of quantum mechanics. Accordingly, by local Pauli operators and entanglement swapping of Bell states, we design novel quantum Secret Permutating protocols. What&#39;s more, to reduce the communicational complexity, we exploit the uniform, random and independent properties of quantum measurements to evenly divide all participants into many secret groups with the small approximate sizes. Finally, the analysis results and simulated experiments show that the proposed protocols have the unconditional security and the good feasibility.},
  archive      = {J_TC},
  author       = {Run-Hua Shi and Yi-Fei Li},
  doi          = {10.1109/TC.2022.3207121},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {1223-1235},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Quantum secret permutating protocol},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traffic characterization based stochastic modelling of
network-on-chip. <em>TC</em>, <em>72</em>(4), 1215–1222. (<a
href="https://doi.org/10.1109/TC.2022.3191965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trend towards multi-core and many-core processors has changed the landscape of computers and servers. Now the performance of a microprocessor heavily depends not only on the data path but also on the memory technology and communication technology. On-chip communication networks or Network-on-Chip is a component which facilitates communication between the cores of a microprocessor. In this manuscript we propose an analytical model for the analysis of Network-on-Chip performance based on Jackson queuing networks for applications that exhibit Poisson injection process. The injection process statistics and the traffic profile of PARSEC benchmarks have been characterized. We have shown that injection process of PARSEC benchmarks does not match with the well known probability distributions using Quantile-Quantile plot and Kolmogorov-Smirnov test. For realistic benchmarks that does not follow Poisson injection process we propose $G/D/1$ queuing model for the NoC router. We have carried out performance analysis using the traffic characterization in conjunction with our analytical model for prediction of performance metrics. The analytical model offers a speed up of around 13 times in comparison with the simulation based performance evaluation. The percentage of error between the analytical model and simulation is less than 6\% for most of the benchmarks except Canneal and Ferret. We have also compared our analytical model results with the results of SNIPER simulator.},
  archive      = {J_TC},
  author       = {Vijaya Bhaskar Adusumilli and Venkatesh TG},
  doi          = {10.1109/TC.2022.3191965},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1215-1222},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Traffic characterization based stochastic modelling of network-on-chip},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PackCache: An online cost-driven data caching algorithm in
the cloud. <em>TC</em>, <em>72</em>(4), 1208–1214. (<a
href="https://doi.org/10.1109/TC.2022.3191969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a data caching problem in the cloud environment, where multiple frequently co-utilised data items could be packed as a single item being transferred to serve a sequence of data requests dynamically with reduced cost. To this end, we propose an online algorithm with respect to a homogeneous cost model, called PackCache , that can leverage the FP-Tree technique to mine those frequently co-utilised data items for packing whereby the incoming requests could be cost-effectively served online by exploiting the concept of anticipatory caching. We show the algorithm is $2/\alpha$ competitive, reaching the lower bound of the competitive ratio for any deterministic online algorithm on the studied caching problem, and also time and space efficient to serve the requests. Finally, we evaluate the performance of the algorithm via experimental studies to show its actual cost-effectiveness and scalability.},
  archive      = {J_TC},
  author       = {Jiashu Wu and Hao Dai and Yang Wang and Yong Zhang and Dong Huang and Chengzhong Xu},
  doi          = {10.1109/TC.2022.3191969},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1208-1214},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PackCache: An online cost-driven data caching algorithm in the cloud},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STfusion: Fast and flexible multi-NN execution using
spatio-temporal block fusion and memory management. <em>TC</em>,
<em>72</em>(4), 1194–1207. (<a
href="https://doi.org/10.1109/TC.2022.3218428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To maximize the cost-effectiveness of neural network (NN) accelerators, architects are actively developing single-chip accelerators which can execute many NNs simultaneously. However, previous approaches fail to achieve full performance potential by exploiting only spatial or temporal resource sharing (SS or TS). They also do not consider memory management that can significantly affect performance. This limitation leads to the dire need for a new multi-NN accelerator taking both opportunities with careful memory management. But, it is extremely challenging to design an ideal spatio-temporal sharing accelerator because it requires (1) an algorithm that determines the degree of SS/TS in large exploration spaces, (2) a new STS-enabled accelerator devised with diverse design points, and (3) carefully-designed memory management that minimizes resource contention during numerous data transfers upon reconfiguration. To this end, we propose STfusion, a fast and flexible multi-NN execution architecture. First, STfusion partitions an accelerator into multiple smaller TS-enabled accelerators. Second, STfusion dynamically fuses small accelerators to adjust the accelerator sizes. Third, STfusion manages on-chip buffer in a page-granularity for stall-free data transfers. Lastly, STfusion provides an algorithm that determines the degree of SS/TS to achieve high throughput while satisfying QoS goals. Our evaluation shows that STfusion significantly outperforms state-of-the-art multi-NN accelerators.},
  archive      = {J_TC},
  author       = {Eunjin Baek and Eunbok Lee and Taehun Kang and Jangwoo Kim},
  doi          = {10.1109/TC.2022.3218428},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1194-1207},
  shortjournal = {IEEE Trans. Comput.},
  title        = {STfusion: Fast and flexible multi-NN execution using spatio-temporal block fusion and memory management},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OsmoticGate: Adaptive edge-based real-time video analytics
for the internet of things. <em>TC</em>, <em>72</em>(4), 1178–1193. (<a
href="https://doi.org/10.1109/TC.2022.3193630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing has gained momentum in recent years, and can provide more immediate analysis of streaming video data. However, the edge devices often lack the computing capabilities (processing power, memory) to guarantee reasonable performance (e.g., accuracy, latency, throughput) for complex video analytics tasks. To alleviate this critical problem, the prevalent trend is to offload some video analytics tasks from the edge devices to the cloud. However, existing offloading approaches fail to consider the dynamic nature of the video analytical tasks (e.g., varying encoding format for different video content) and are unable to adapt system dynamics (e.g., varying workload between the edge and the cloud). To overcome the limitation of existing approaches, we develop an edge-cloud offloading performance model based on the concept of hierarchical queues. The resource constraints (e.g., computing capacity and network bandwidth) of each edge nodes and dynamic edge-cloud network conditions are used to parameterize the performance model. Since finding optimal solutions for the performance model is NP-hard, we develop a two-stage gradient-based algorithm and compare it with some state-of-the-art (SOTA) solutions (e.g., FastVA, DeepDecision, Hill Climbing). Experiments have shown our performance model&#39;s advantages and the stability of the proposed offloading approach given different systems (edge-cloud) and video analytics application dynamics.},
  archive      = {J_TC},
  author       = {Bin Qian and Zhenyu Wen and Junqi Tang and Ye Yuan and Albert Y. Zomaya and Rajiv Ranjan},
  doi          = {10.1109/TC.2022.3193630},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1178-1193},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OsmoticGate: Adaptive edge-based real-time video analytics for the internet of things},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DISCO: Time-compositional cache coherence for multi-core
real-time embedded systems. <em>TC</em>, <em>72</em>(4), 1163–1177. (<a
href="https://doi.org/10.1109/TC.2022.3193624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tasks in modern embedded systems share data and communicate among each other. Nonetheless, the majority of research in real-time systems either assumes that tasks do not share data or prohibits data sharing by design. Only recently, some works investigated solutions to address this limitation and enable data sharing. However, we find these works to suffer from severe limitations. In particular, proposed predictable cache coherence protocols increase the worst-case memory latency (WCL) quadratically due to coherence interference and breaks compositionality by coupling the design and timing analysis of the coherence with the underlying bus arbitration policy. In this paper, we argue that a protocol that distinguishes between non-modifying (read) and modifying (write) memory accesses is key towards reducing the effects of coherence interference on WCL. Accordingly, we propose DISCO, a discriminative coherence solution that capitalizes on this observation to 1) balance average-case performance and WCL, and 2) more importantly, achieves compositionality in the existing of coherence by enabling the decomposition of the effects from coherence and arbitration components. DISCO achieves 7.2× lower latency bounds compared to the state-of-the-art predictable coherence protocol. DISCO also achieves up to 11.4× (5.3× on average) better performance than private cache bypassing for the SPLASH-3 benchmarks.},
  archive      = {J_TC},
  author       = {Mohamed Hassan},
  doi          = {10.1109/TC.2022.3193624},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1163-1177},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DISCO: Time-compositional cache coherence for multi-core real-time embedded systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An in-module disturbance barrier for mitigating write
disturbance in phase-change memory. <em>TC</em>, <em>72</em>(4),
1150–1162. (<a href="https://doi.org/10.1109/TC.2022.3197071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Write disturbance error (WDE) appears as a serious reliability problem preventing phase-change memory (PCM) from general commercialization, and therefore several studies have been proposed to mitigate WDEs. Verify-and-correction (VnC) eliminates WDEs by always verifying the data correctness on neighbors after programming, but incurs significant performance overhead. Encoding-based schemes mitigate WDEs by reducing the number of WDE-vulnerable data patterns; however, mitigation performance notably fluctuates with applications. Moreover, encoding-based schemes still rely on VnC-based schemes. Cache-based schemes lower WDEs by storing data in a write cache, but it requires several megabytes of SRAM to significantly mitigate WDEs. Despite the efforts of previous studies, these methods incur either significant performance or area overhead. Therefore, a new approach, which does not rely on VnC-based schemes or application data patterns, is highly necessary. Furthermore, the new approach should be transparent to processors (i.e., in-module), because the characteristic of WDEs is determined by manufacturers of PCM products. In this paper, we present an in-module disturbance barrier (IMDB) that mitigates WDEs on demand. IMDB includes a two-level hierarchy comprising two SRAM-based tables, whose entries are managed with a dedicated replacement policy that sufficiently utilizes the characteristics of WDEs. The naive implementation of the replacement policy requires hundreds of read ports on SRAM, which is infeasible in real hardware; hence, an approximate comparator is also designed. We also conduct a rigorous exploration of architecture parameters to obtain a cost-effective design. The proposed method significantly reduces WDEs without noticeable speed degradation or additional energy consumption compared to previous methods.},
  archive      = {J_TC},
  author       = {Hyokeun Lee and Seungyong Lee and Byeongki Song and Moonsoo Kim and Seokbo Shim and Hyuk-Jae Lee and Hyun Kim},
  doi          = {10.1109/TC.2022.3197071},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1150-1162},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An in-module disturbance barrier for mitigating write disturbance in phase-change memory},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tolerance of siamese networks (SNs) to memory errors:
Analysis and design. <em>TC</em>, <em>72</em>(4), 1136–1149. (<a
href="https://doi.org/10.1109/TC.2022.3186628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers memory errors in a Siamese Network (SN) through an extensive analysis and proposes two schemes (using a weight filter and a code) to provide efficient hardware solutions for error tolerance. Initially the impact of memory errors on the weights of the SN (stored as floating-point (FP) numbers) is analyzed; this shows that the degradation is mostly caused by outliers in weights. Two schemes are subsequently proposed. An analysis is pursued to establish the filter&#39;s bounds selection by the maximum/minimum values of the weight distributions, by which outliers can be removed from the operation of the SN. A code scheme for protecting the sign and exponent bits of each weight in an FP number, is also proposed; this code incurs in no memory overhead by utilizing the 4 least significant bits (LSB) to store parity bits. Simulation shows that the filter has a better performance for multi-bit errors correction (a reduction of 95.288\% in changed predictions), while the code achieves superior results in single-bit errors correction (a reduction of 99.775\% in changed predictions). The combined method that uses the two proposed schemes, retains their advantages, so adaptive to all scenarios; The ASIC-based FP designs of the SN using serial and hybrid implementations are also presented; these pipelined designs utilize a novel multi-layer perceptron (MLP) (as branch networks of the SN) that operates at a frequency of 681.2 MHz (at a 32nm technology node), so significantly higher than existing designs found in the technical literature. The proposed error-tolerant approaches also show advantages in overheads comparing with for example traditional error correction code (ECC). These error-tolerant MLP-based designs are well suited to hardware/power-constrained platforms.},
  archive      = {J_TC},
  author       = {Ziheng Wang and Farzad Niknia and Shanshan Liu and Pedro Reviriego and Paolo Montuschi and Fabrizio Lombardi},
  doi          = {10.1109/TC.2022.3186628},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1136-1149},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tolerance of siamese networks (SNs) to memory errors: Analysis and design},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MarCNNet: A markovian convolutional neural network for
malware detection and monitoring multi-core systems. <em>TC</em>,
<em>72</em>(4), 1122–1135. (<a
href="https://doi.org/10.1109/TC.2022.3184520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging side-channels enables zero-overhead detection of anomalies. These channels offer a non-instrumented program profiling capability by means of the distinct signatures generated by processing unintentional signals emitted during executions. In this paper, we propose a Markov based convolutional neural network (CNN) to monitor programs against anomalies on multi-core devices. We refer to the proposed framework as MarCNNet. In the model, the output of the CNN estimates the likelihood of the current state of the program, and the Markov Model tracks the process based on these estimates. If the estimates do not match the Markov model state diagram, it alerts anomaly, otherwise, it keeps monitoring. The framework also simplifies the training process because dependency among states is crucial for the Markov part of the model, but not for the CNN. Therefore, the neural network is trained by treating each state independent. However, for a test signal, both CNN and Markov parts of the framework are considered for malware detection to utilize the program flow. We tested the proposed model for various devices with different number of cores and threads of processes and demonstrated that the framework can detect malware with no false negatives, and a false positive rate less than 2\%.},
  archive      = {J_TC},
  author       = {Baki Berkay Yilmaz and Frank Werner and Sunjae Y. Park and Elvan Mert Ugurlu and Erik Jorgensen and Milos Prvulovic and Alenka Zajić},
  doi          = {10.1109/TC.2022.3184520},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1122-1135},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MarCNNet: A markovian convolutional neural network for malware detection and monitoring multi-core systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Block-wise computation of cyclic redundancy code using
factored toeplitz matricesin lieu of look-up table. <em>TC</em>,
<em>72</em>(4), 1110–1121. (<a
href="https://doi.org/10.1109/TC.2022.3189574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyclic Redundancy Check (CRC) or Cyclic Redundancy Code is a Cyclic Error Detection Code used to preserve the integrity of data in storage and transmission applications. CRC of a stream of message bits is usually calculated block-wise in parallel with the help of a Look-Up Table (LUT) in software or State Space transformation matrix in hardware. Presented here is a novel method and architecture of parallel computation of Cyclic Redundancy Codes without any Look-Up Table for an arbitrary generating polynomial which is programmable at runtime. The method reduces computational complexity and storage requirements by implicitly factorizing the transformation matrix needed to compute the remainder into two simpler Toeplitz matrices. The resulting hardware architecture is suitable for embedded applications.},
  archive      = {J_TC},
  author       = {Arindam Das},
  doi          = {10.1109/TC.2022.3189574},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1110-1121},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Block-wise computation of cyclic redundancy code using factored toeplitz matricesin lieu of look-up table},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward comprehensive shifting fault tolerance for
domain-wall memories with PIETT. <em>TC</em>, <em>72</em>(4), 1095–1109.
(<a href="https://doi.org/10.1109/TC.2022.3188206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spintronic domain-wall memories (DWMs) offer improved memory density and energy compared to conventional memories, but are susceptible to shifting faults. We propose PIETT ( P inning, I nsertion, E rasure, and T ranslation-fault T olerance) for improved misalignment correction versus the state of the art. PIETT proposes a derived error correction combined with multi-domain access approach to detect and correct a minimum of three misalignment faults after an arbitrary shift distance. Moreover, the rate of both misalignment and pinning faults are characterized in DWM nanowires, demonstrating that pinning faults are a significant concern to DWM. As such, PIETT is the first method to combine correction of misalignment and pinning faults in random access DWMs. It also introduces novel PIETT Transverse Access Points (TAPs), which utilize a novel write access mode that can set/reset multiple domains in a single intrinsic operation and can store shift distance detection codes. By allowing checks between shifts of the intrinsic shift distance (e.g., 3 domains), using a single TAP per nanowire expands misalignment protection and determines the needed corrective shifts to correct faults in all nanowires. Two TAPs expands misalignment protection to correct misalignment by more than one position and detects pinning by detecting different shift distances at each extremity of the nanowire. PIETT leverages knowledge of pinned nanowire locations to guide a modified SECDED ECC with one additional parity bit stored in additional parity nanowires. Thus, PIETT in TAP mode can correct unlimited, potentially multi-position, misalignment faults and either up to three pinning faults or up to two pinning faults with up to one bit flip fault using scrubbing. PIETT provides 8 to 21 orders of magnitude improvement in mean-time-to-failure with similar or better area overhead and only a 1\% system performance degradation compared to state of the art DWM misalignment correction.},
  archive      = {J_TC},
  author       = {Sebastien Ollivier and Stephen Longofono and Prayash Dutta and Jingtong Hu and Sanjukta Bhanja and Alex K. Jones},
  doi          = {10.1109/TC.2022.3188206},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1095-1109},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Toward comprehensive shifting fault tolerance for domain-wall memories with PIETT},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SurgeNAS: A comprehensive surgery on hardware-aware
differentiable neural architecture search. <em>TC</em>, <em>72</em>(4),
1081–1094. (<a href="https://doi.org/10.1109/TC.2022.3188175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable neural architecture search (NAS) is an emerging paradigm to automate the design of top-performing convolutional neural networks (CNNs). Nonetheless, existing differentiable NAS methods suffer from several crucial weaknesses, such as inaccurate gradient estimation, high memory consumption, search fairness, etc . In this work, we introduce a novel hardware-aware differentiable NAS framework, namely SurgeNAS, in which we leverage the one-level optimization to avoid inaccuracy in gradient estimation. To this end, we propose an effective identity mapping regularization to alleviate the over-selecting issue. Besides, to mitigate the memory bottleneck, we propose an ordered differentiable sampling approach, which significantly reduces the search memory consumption to the single-path level, thereby allowing to directly search on target tasks instead of small proxy tasks. Meanwhile, it guarantees the strict search fairness. Moreover, we introduce a graph neural networks (GNNs) based predictor to approximate the on-device latency, which is further integrated into SurgeNAS to enable the latency-aware architecture search. Finally, we analyze the resource underutilization issue, in which we propose to scale up the searched SurgeNets within Comfort Zone to balance the computation and memory access, which brings considerable accuracy improvement without deteriorating the execution efficiency. Extensive experiments are conducted on ImageNet with diverse hardware platforms, which clearly show the effectiveness of SurgeNAS in terms of accuracy, latency, and search efficiency.},
  archive      = {J_TC},
  author       = {Xiangzhong Luo and Di Liu and Hao Kong and Shuo Huai and Hui Chen and Weichen Liu},
  doi          = {10.1109/TC.2022.3188175},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1081-1094},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SurgeNAS: A comprehensive surgery on hardware-aware differentiable neural architecture search},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAFLA: Scheduling multiple real-time periodic task graphs on
heterogeneous systems. <em>TC</em>, <em>72</em>(4), 1067–1080. (<a
href="https://doi.org/10.1109/TC.2022.3191970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many modern Cyber Physical Systems (CPSs) are composed of multiple independent periodically executing real-time control tasks having inter-dependent component sub-tasks. Each such control task is therefore usually represented as Directed-acyclic Task Graphs (DTGs). These CPSs are often distributed in nature and are quickly shifting from homogeneous to heterogeneous processing platforms in order to meet ever increasing demands for performance and energy savings, within limited resource budgets. In spite of the practical relevance of the problem in today&#39;s CPS design scenario, very few research works in literature have tried to address this due to its inherent computational as well as design complexity. This work endeavors to solve the problem of co-scheduling a set of periodic real-time applications each modelled as an independent DTG, to be executed on a distributed platform consisting of heterogeneous processors communicating using shared buses. Assuming the processing platform to be DVFS (Dynamic Voltage Frequency Scaling) enabled, we attempt to minimize dynamic energy dissipation associated with the execution of all DTGs over an hyperperiod $\mathcal {H}$ while ensuring that no DTG instance within $\mathcal {H}$ misses its deadline. The problem has first been formally represented as a constraint optimization problem. However, an optimal solution using standard solvers become prohibitively compute as well as memory intensive and doesn&#39;t scale even for moderate problem sizes. Hence, in this work, we attempt to develop a three-phase list-based hierarchical scheduling algorithm called Slack Aware Frequency Level Allocator ( SAFLA ). The efficacy of SAFLA has been critically evaluated through simulation using benchmark DTGs.},
  archive      = {J_TC},
  author       = {Sanjit Kumar Roy and Rajesh Devaraj and Arnab Sarkar},
  doi          = {10.1109/TC.2022.3191970},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1067-1080},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SAFLA: Scheduling multiple real-time periodic task graphs on heterogeneous systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combined fault and DPA protection for lattice-based
cryptography. <em>TC</em>, <em>72</em>(4), 1055–1066. (<a
href="https://doi.org/10.1109/TC.2022.3197073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progress on constructing quantum computers and the ongoing standardization of post-quantum cryptography (PQC) have led to the development and refinement of promising new digital signature schemes and key encapsulation mechanisms (KEM). Especially lattice-based schemes have gained some popularity in the research community, presumably due to acceptable key, ciphertext, and signature sizes as well as good performance results and cryptographic strength. However, in some practical applications like smart cards, it is also crucial to secure cryptographic implementations against side-channel and fault attacks. In this work, we analyze the so-called redundant number representation (RNR) that can be used to counter side-channel attacks. We show how to avoid security issues with the RNR due to unexpected de-randomization and we apply it to the Kyber KEM and show that the RNR has a very low overhead. We then verify the RNR methodology by practical experiments, using the non-specific t-test methodology and the ChipWhisperer platform. Furthermore, we present a novel countermeasure against fault attacks based on the Chinese remainder theorem (CRT). On an ARM Cortex-M4, our implementation of the RNR and fault countermeasure offers better performance than masking and redundant calculation. Our methods thus have the potential to expand the toolbox of a defender implementing lattice-based cryptography with protection against two common physical attacks.},
  archive      = {J_TC},
  author       = {Daniel Heinz and Thomas Pöppelmann},
  doi          = {10.1109/TC.2022.3197073},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1055-1066},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Combined fault and DPA protection for lattice-based cryptography},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive cloud bundle provisioning and multi-workflow
scheduling via coalition reinforcement learning. <em>TC</em>,
<em>72</em>(4), 1041–1054. (<a
href="https://doi.org/10.1109/TC.2022.3191733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient cloud resource provisioning for the execution of complex workflow applications has always been one of the important research issues. Most of the existing approaches focus on the resource provisioning of single-type virtual machine (VM) instances for the single or multiple workflows, while few consider the situation of provisioning multi-type VM instances simultaneously. As a result, the executing performance of complex workflows degrades. Different from the existing work, this paper proposes an adaptive cloud bundle provisioning and multi-workflow scheduling model to dynamically perform both the horizontal and vertical cloud resource scaling on multi-type VM instances for the execution of complex workflows. Among the model, a depth-first-search coalition reinforcement learning (DFSCRL) provisioning policy is presented to realize the resource scaling, which integrates the physical machine (PM) coalition formation with the Q-learning algorithm, then dynamically generates an optimal multi-type VM instance bundle from the PM coalition, and finally provisions these instances to the concurrent execution of multiple workflows. The theoretical proofs and various experiments with the multifaceted metrics demonstrate that the performance of the proposed algorithms is superior to that of the state-of-the-art relevant policies.},
  archive      = {J_TC},
  author       = {Xiaogang Wang and Jian Cao and Rajkumar Buyya},
  doi          = {10.1109/TC.2022.3191733},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1041-1054},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive cloud bundle provisioning and multi-workflow scheduling via coalition reinforcement learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ARETE: Accurate error assessment via machine learning-guided
dynamic-timing analysis. <em>TC</em>, <em>72</em>(4), 1026–1040. (<a
href="https://doi.org/10.1109/TC.2022.3191966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nanometer circuits are increasingly prone to timing errors, escalating the need for fault injection frameworks to accurately evaluate their impact on applications. In this paper, we propose ARETE, a novel cross-layer, fault-injection framework that combines dynamic-binary instrumentation with machine learning-guided dynamic-timing analysis. ARETE enables accurate fault-injection into any application by estimating the location of the injecting errors via dynamic-timing analysis. To accelerate fault-injection, we develop a novel, data-aware, machine learning-based mechanism that dynamically pre-selects the error-prone instructions and limits the application of the costly dynamic-timing analysis only to them. To evaluate ARETE&#39;s accuracy, our fully automated toolflow is configured to support fault-injection based on detailed post-layout gate-level simulations as well as via existing workload-agnostic error models. Our results for various workloads, including an autonomous-driving library, show that the location and time of injected errors performed by ARETE, is 89.9\% consistent with fault-injection based on full gate-level simulation. On average, ARETE executes 84.6× faster than gate-level simulation and at a cost of 3.4\% loss in the program output quality estimation. When compared to the existing statistical fault-injection tools that are based on workload-agnostic error models, ARETE improves the accuracy of fault-injection rate and output quality estimation by 143.9\% and 40.4\% on average, respectively.},
  archive      = {J_TC},
  author       = {Ioannis Tsiokanos and Styliani Tompazi and Giorgis Georgakoudis and Lev Mukhanov and Georgios Karakonstantis},
  doi          = {10.1109/TC.2022.3191966},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1026-1040},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ARETE: Accurate error assessment via machine learning-guided dynamic-timing analysis},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling fast and memory-efficient acceleration for pattern
matching workloads: The lightweight automata processing engine.
<em>TC</em>, <em>72</em>(4), 1011–1025. (<a
href="https://doi.org/10.1109/TC.2022.3187338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Growing pattern matching applications are employing finite automata as their basic processing model. These applications match tens to thousands of patterns on a large amount of data, which brings a great challenge to conventional processors. Therefore hardware-based solutions have emerged frequently and achieved high throuphput automata processing. However, existing methods are generally difficult to achieve both processing speed and storage efficiency, and are often too heavy to be integrated into a small chip and have to rely on off-chip DRAMs or other high capacity memories even on some simple data sets, leading to the potential area and power consumption issues. In this paper, we focus on building a more lightweight automata processing engine, hoping to store the whole automata model into on-chip memory and run effectively and independently. We propose LAP, a lightweight automata processing engine. Powered with a novel automata model (A-DFA) and efficient packing algorithms, extremely high storage efficiency compared with traditional DFA is achieved in LAP. Meanwhile, we identify the key parallelization factors in the A-DFA model and then propose a specialized microarchitecture with novel instructions to further accelerate the state transition process. As a result, LAP can obtain more effective trade-off between processing speed and storage efficiency. Evaluation results show that LAP achieves extremely high storage efficiency on simple data sets, exceeding IBM&#39;s RegX by 8×, and achieves significant improvements in processing speed ranging from 1.32× to 1.91× compared with previous lightweight hardware implementations. Moreover, LAP has good scalability in hardware architecture. It is easy to build an acceleration system with higher throughput by increasing the number of cores. We prototype a 16-core system into Xilinx ZC702 FPGA and a 64-core system into Xilinx ZCU102 FPGA respectively. The prototype system on ZC702 on average achieves 3.5 GB/s throughput on simple data sets, and the prototype system on ZCU102 can obtain higher throughput and compute density values on part of large datasets in ANMLZoo compared with modern in-memory NFA-based solutions.},
  archive      = {J_TC},
  author       = {Lei Gong and Chao Wang and Haojun Xia and Xianglan Chen and Xi Li and Xuehai Zhou},
  doi          = {10.1109/TC.2022.3187338},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {1011-1025},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling fast and memory-efficient acceleration for pattern matching workloads: The lightweight automata processing engine},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). V-WAFA: An endurance variation aware fine-grained allocator
for persistent memory. <em>TC</em>, <em>72</em>(4), 998–1010. (<a
href="https://doi.org/10.1109/TC.2022.3197086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from numerous attractive characteristics, persistent memory (PM) has become a promising substitute for DRAM. Unfortunately, PM suffers from limited endurance of memory cells, especially with the unbalanced wears induced by applications. Even though several wear-leveling aware allocators have been proposed to prolong the lifespan of PM, they fail to perceive the endurance variation among memory cells, which may severely damage the weak cells. In this paper, we propose V-WAFA, a fine-grained PM allocator that considers endurance variation to improve the lifespan of PM. First, V-WAFA adopts a priority-based wear-leveling strategy to mitigate the side-effect of endurance variation among pages. Second, V-WAFA utilizes fine-grained space management and allocation strategy inside pages. Briefly, V-WAFA divides pages into basic units and allocates the basic units of a page in a rotational manner to evenly distribute fine-grained updates on memory cells. We implement V-WAFA in Linux 4.4.4 and evaluate it by running standard workloads on Redis, a typical in-memory key-value store. Evaluation results show that the wear-leveling effect of V-WAFA exceeds that of malloc, nvm_malloc, WAlloc, NVMalloc, and WAFA for 7.02×, 6.91×, 5.7×, 5.68×, and 3.78×, respectively. V-WAFA also achieves similar performance with state-of-the-art PM allocators on Redis running YCSB workloads.},
  archive      = {J_TC},
  author       = {Xiaoliu Feng and Xianzhang Chen and Qingfeng Zhuge and Duo Liu and Edwin H.-M. Sha and Chun Jason Xue},
  doi          = {10.1109/TC.2022.3197086},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {998-1010},
  shortjournal = {IEEE Trans. Comput.},
  title        = {V-WAFA: An endurance variation aware fine-grained allocator for persistent memory},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and accurate error simulation for CNNs against soft
errors. <em>TC</em>, <em>72</em>(4), 984–997. (<a
href="https://doi.org/10.1109/TC.2022.3184274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The great quest for adopting AI-based computation for safety-/mission-critical applications motivates the interest towards methods for assessing the robustness of the application w.r.t. not only its training/tuning but also errors due to faults, in particular soft errors, affecting the underlying hardware. Two strategies exist: architecture-level fault injection and application-level functional error simulation. We present a framework for the reliability analysis of Convolutional Neural Networks (CNNs) via an error simulation engine that exploits a set of validated error models extracted from a detailed fault injection campaign. These error models are defined based on the corruption patterns of the output of the CNN operators induced by faults and bridge the gap between fault injection and error simulation, exploiting the advantages of both approaches. We compared our methodology against SASSIFI for the accuracy of functional error simulation w.r.t. fault injection, and against TensorFI in terms of speedup for the error simulation strategy. Experimental results show that our methodology achieves about 99\% accuracy of the fault effects w.r.t. SASSIFI, and a speedup ranging from 44x up to 63x w.r.t. TensorFI, that only implements a limited set of error models.},
  archive      = {J_TC},
  author       = {Cristiana Bolchini and Luca Cassano and Antonio Miele and Alessandro Toschi},
  doi          = {10.1109/TC.2022.3184274},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {984-997},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fast and accurate error simulation for CNNs against soft errors},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of thermal covert channel attacks based on
classification of components of the thermal signal features.
<em>TC</em>, <em>72</em>(4), 971–983. (<a
href="https://doi.org/10.1109/TC.2022.3189578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to growing security challenges facing many-core systems imposed by thermal covert channel (TCC) attacks, a number of threshold-based detection methods have been proposed. In this paper, we show that these threshold-based detection methods are inadequate to detect TCCs that harness advanced signaling and specific modulation techniques. Since the frequency representation of a TCC signal is found to have multiple side lobes, this important feature shall be explored to enhance the TCC detection capability. To this end, we present a pattern-classification-based TCC detection method using an artificial neural network that is trained with a large volume of spectrum traces of TCC signals. After proper training, this classifier is applied at runtime to infer TCCs, should they exist. The proposed detection method is able to achieve a detection accuracy of 99\%, even in the presence of the stealthiest TCCs ever discovered. Because of its low runtime overhead ( $&amp;lt; 0.187\%$ ) and low energy overhead ( $&amp;lt; 0.072\%$ ), this proposed detection method can be indispensable in fighting against TCC attacks in many-core systems. With such a high accuracy in detecting TCCs, powerful countermeasures, like the ones based on dynamic voltage and frequency scaling (DVFS), can be rightfully applied to neutralize any malicious core participating in a TCC attack.},
  archive      = {J_TC},
  author       = {Xiaohang Wang and Hengli Huang and Ruolin Chen and Yingtao Jiang and Amit Kumar Singh and Mei Yang and Letian Huang},
  doi          = {10.1109/TC.2022.3189578},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {971-983},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Detection of thermal covert channel attacks based on classification of components of the thermal signal features},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced machine learning sketches for network measurements.
<em>TC</em>, <em>72</em>(4), 957–970. (<a
href="https://doi.org/10.1109/TC.2022.3185560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network monitoring and management require accurate statistics of a variety of flow-level metrics such as flow sizes, top- $k$ flows, and number of flows. Arguably, the current best technique to measure these metrics is sketches. While a significant amount of work has already been done on sketching techniques, there is still a lot of room for improvement because the accuracy of existing sketches varies with changing characteristics of network traffic. In this paper, we propose the idea of using machine learning to improve the accuracy of sketches, and propose a generic machine learning framework to reduce the dependence of accuracy of sketches on network traffic characteristics. We further present three case studies, where we applied our machine learning framework on sketches for measuring three flow-level network metrics, namely flow sizes, top- $k$ flows, and number of flows. We implemented and extensively evaluated this framework for these three metrics using both real-world and synthetic traffic traces. To the best of our knowledge, this is the first work that uses machine learning to reduce the dependence of sketching techniques on the characteristics of network traffic. We have released all our traces and implementation codes at Github.},
  archive      = {J_TC},
  author       = {Hengrui Wang and Huiping Lin and Zheng Zhong and Tong Yang and Muhammad Shahzad},
  doi          = {10.1109/TC.2022.3185560},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {957-970},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enhanced machine learning sketches for network measurements},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning-based microarchitecture- level power
modeling of CPUs. <em>TC</em>, <em>72</em>(4), 941–956. (<a
href="https://doi.org/10.1109/TC.2022.3185572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy efficiency has emerged as a key concern for modern processor design, especially when it comes to embedded and mobile devices. It is vital to accurately quantify the power consumption of different micro-architectural components in a CPU. Traditional RTL or gate-level power estimation is too slow for early design-space exploration studies. By contrast, existing architecture-level power models suffer from large inaccuracies. Recently, advanced machine learning techniques have been proposed for accurate power modeling. However, existing approaches still require slow RTL simulations, have large training overheads or have only been demonstrated for fixed-function accelerators and simple in-order cores with predictable behavior. In this work, we present a novel machine learning-based approach for microarchitecture-level power modeling of complex CPUs. Our approach requires only high-level activity traces obtained from microarchitecture simulations. We extract representative features and develop low-complexity learning formulations for different types of CPU-internal structures. Cycle-accurate models at the sub-component level are trained from a small number of gate-level simulations and hierarchically composed to build power models for complete CPUs. We apply our approach to both in-order and out-of-order RISC-V cores. Cross-validation results show that our models predict cycle-by-cycle power consumption to within 3\% of a gate-level power estimation on average. In addition, our power model for the Berkeley Out-of-Order (BOOM) core trained on micro-benchmarks can predict the cycle-by-cycle power of real-world applications with less than 3.6\% mean absolute error.},
  archive      = {J_TC},
  author       = {Ajay Krishna Ananda Kumar and Sami Al-Salamin and Hussam Amrouch and Andreas Gerstlauer},
  doi          = {10.1109/TC.2022.3185572},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {941-956},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Machine learning-based microarchitecture- level power modeling of CPUs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards high performance and efficient memory deduplication
via mixed pages. <em>TC</em>, <em>72</em>(4), 926–940. (<a
href="https://doi.org/10.1109/TC.2022.3191742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large pages are widely supported in modern hardware and OSes to reduce the overhead of TLB misses. However, memory deduplication can be inefficient with large pages, leading to low memory utilization. To simultaneously enjoy the benefits of high performance by accessing memory with large pages (e.g., 2 MB pages) and high deduplication rate by managing memory with base pages (e.g., 4 KB pages), we propose Smart M emory D eduplciation (SmartMD), which is an adaptive and efficient memory management scheme via mixed pages. Specifically, we propose lightweight schemes to periodically monitor pages’ access frequency and repetition rate, and present an adaptive conversion scheme to selectively split or reconstruct large pages. We further optimize SmartMD by developing SmartMD $^{+}$ , which dynamically adjusts the page scanning cycle by monitoring the TLB miss cost, and reconstructs the split large pages in an on-demand way so as to reduce the CPU overhead of SmartMD. We further implement a prototype system and conduct extensive experiments with various workloads under different system settings. Experiment results show that SmartMD and SmartMD $^{+}$ can simultaneously achieve high access performance similar to systems using large pages, and achieve a deduplication rate similar to that applying aggressive deduplication scheme (i.e., KSM) on base pages.},
  archive      = {J_TC},
  author       = {Lulu Yao and Yongkun Li and Fan Guo and Si Wu and Yinlong Xu and John C.S. Lui},
  doi          = {10.1109/TC.2022.3191742},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {926-940},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards high performance and efficient memory deduplication via mixed pages},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GRIP: A graph neural network accelerator architecture.
<em>TC</em>, <em>72</em>(4), 914–925. (<a
href="https://doi.org/10.1109/TC.2022.3197083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present GRIP, a graph neural network accelerator architecture designed for low-latency inference. Accelerating GNNs is challenging because they combine two distinct types of computation: arithmetic-intensive vertex-centric operations and memory-intensive edge-centric operations. GRIP splits GNN inference into a three edge- and vertex-centric execution phases that can be implemented in hardware. GRIP specializes each unit for the unique computational structure found in each phase. For vertex-centric phases, GRIP uses a high performance matrix multiply engine coupled with a dedicated memory subsystem for weights to improve reuse. For edge-centric phases, GRIP use multiple parallel prefetch and reduction engines to alleviate the irregularity in memory accesses. Finally, GRIP supports several GNN optimizations, including an optimization called vertex-tiling that increases the reuse of weight data. We evaluate GRIP by performing synthesis and place and route for a $28 \;\mathrm{n}\mathrm{m}$ implementation capable of executing inference for several widely-used GNN models (GCN, GraphSAGE, G-GCN, and GIN). Across several benchmark graphs, it reduces 99th percentile latency by a geometric mean of $17\times$ and $23\times$ compared to a CPU and GPU baseline, respectively, while drawing only $5 \;\mathrm{W}$ .},
  archive      = {J_TC},
  author       = {Kevin Kiningham and Philip Levis and Christopher Ré},
  doi          = {10.1109/TC.2022.3197083},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {914-925},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GRIP: A graph neural network accelerator architecture},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RUPA: A high performance, energy efficient accelerator for
rule-based password generation in heterogenous password recovery system.
<em>TC</em>, <em>72</em>(4), 900–913. (<a
href="https://doi.org/10.1109/TC.2022.3197077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a growing demand for energy efficient and high performance password recovery systems. As password generation and password validation are two integral components of any password recovery system, the former yet lags behind the latter in performance particularly for the case when the popular rule-based password generation method is applied to the heterogeneous CPU-FPGA system. In this paper, we thus present a high performance, energy efficient accelerator to speed up the rule functions in rule-based password generation. Dubbed RUPA, this proposed accelerator explores previously undiscovered computational features and memory access patterns for processing the rule functions. Specially, we show that the rule functions can be mapped to three distinct groups according to their character dependency graphs. Correspondingly, three kinds of datapath units, referred to as rule logic units, are created, and the rule functions from the same group will be processed in their shared rule logic unit. Compared with the state-of-the-art password recovery system built upon a CPU-GPU platform, the FPGA-based RUPA system achieves 5.3x speed improvement and is 33.1x more energy efficient. If RUPA is integrated into the popular password recovery tool John the Ripper (JtR), JtR&#39;s rule-based attack performance can soar by more than 48.7x.},
  archive      = {J_TC},
  author       = {Zhendong Zhang and Peng Liu and Weidong Wang and Yingtao Jiang},
  doi          = {10.1109/TC.2022.3197077},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {900-913},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RUPA: A high performance, energy efficient accelerator for rule-based password generation in heterogenous password recovery system},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-speed counter with novel LFSR state extension.
<em>TC</em>, <em>72</em>(3), 893–899. (<a
href="https://doi.org/10.1109/TC.2022.3187343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a high-speed counter architecture associated with novel LFSR state extension. By employing the proposed state extension, an m -bit LFSR counter with $(\mathrm{2}^{m}-1)$ states is modified to cover $\mathrm{2}^{m}$ states without degrading the counting rate. Based on the property that only the low-order bits are frequently switched, the proposed counter consists of two sub-counters to achieve a high counting rate and reduce the hardware complexity needed to convert an LFSR state into a binary state. The low-order sub-counter is implemented with the proposed LFSR counter, and the high-order sub-counter is designed by employing the conventional synchronous binary counter. In addition, the implemented counter takes into account the speed degradation caused by the large fan-out of the high-order sub-counter. The proposed counter designed with standard cells operates at 2.08 GHz in a 65 nm CMOS technology, and its counting rate is almost independent of the counter size.},
  archive      = {J_TC},
  author       = {Hyungjoon Bae and Yujin Hyun and Suchang Kim and Sangsoo Park and Jaeyoung Lee and Boseon Jang and Suyoung Choi and In-Cheol Park},
  doi          = {10.1109/TC.2022.3187343},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {893-899},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-speed counter with novel LFSR state extension},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DyNNamic: Dynamically reshaping, high data-reuse accelerator
for compact DNNs. <em>TC</em>, <em>72</em>(3), 880–892. (<a
href="https://doi.org/10.1109/TC.2022.3184272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional layers dominate the computation and energy costs of Deep Neural Network (DNN) inference. Recent algorithmic works attempt to reduce these bottlenecks via compact DNN structures and model compression. Likewise, state-of-the-art accelerator designs leverage spatiotemporal characteristics of convolutional layers to reduce data movement overhead and improve throughput. Although both are independently effective at reducing latency and energy costs, combining these approaches does not guarantee cumulative improvements due to inefficient mapping. This inefficiency can be attributed to (1) inflexibility of underlying hardware and (2) inherent reduction of data-reuse opportunities of compact DNN structures. To address these issues, we propose a dynamically reshaping, high data-reuse PE array accelerator, namely DyNNamic . DyNNamic leverages kernel-wise filter decomposition to partition the convolution operation into two compact stages: Shared Kernels Convolution (SKC) and Weighted Accumulation (WA). Because both stages have vastly different dimensions, DyNNamic reshapes its PE array to effectively map the algorithm to the architecture. The architecture then exploits data-reuse opportunities created by the SKC stage, further reducing data movement with negligible overhead. We evaluate our approach on various representative networks and compare against state-of-the-art accelerators. On average, DyNNamic outperforms DianNao by $8.4\times$ and $12.3\times$ in terms of inference energy and latency, respectively.},
  archive      = {J_TC},
  author       = {Edward Hanson and Shiyu Li and Xuehai Qian and Hai Helen Li and Yiran Chen},
  doi          = {10.1109/TC.2022.3184272},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {880-892},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DyNNamic: Dynamically reshaping, high data-reuse accelerator for compact DNNs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shadows: Blockchain virtualization for interoperable
computations in IIoT environments. <em>TC</em>, <em>72</em>(3), 868–879.
(<a href="https://doi.org/10.1109/TC.2022.3184271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose Shadows , a virtual blockchain (VC) for achieving parallel consensus and efficient management of data in industries by utilizing BC. Typically, industrial processes involve heterogeneous activities which require real-time consensus, managed execution, isolation, data sharing, accelerated computation, and efficient utilization of various computational resources such as CPU, RAM, and storage. Achieving these in real-time using a single conventional blockchain (BC) leads to the exertion of computational power. To achieve resource-efficient real-time consensus, we virtualize the nodes of the BC network and create different BC for various activities. Further, to virtualize BC and provide better access to data, we propose smart contracts liable for providing a unified view of a single BC, dynamically creating BCs, allocating resources to these, and making communication between the same. Through lab-scale experiments, we demonstrate that Shadows is capable of utilizing the resources efficiently and achieving real-time consensus. In particular, Shadows uses 18\% CPU and 92\% memory while reducing consensus time by 56\%, compared to a single conventional BC. Shadows also accesses the data efficiently by utilizing smart contracts and dynamically balances the load by migrating the virtual nodes. Further, Shadows reduces the number of migrations to make the balance system by 67\%.},
  archive      = {J_TC},
  author       = {Riya Tapwal and Pallav Kumar Deb and Sudip Misra and Surjya Kanta Pal},
  doi          = {10.1109/TC.2022.3184271},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {868-879},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Shadows: Blockchain virtualization for interoperable computations in IIoT environments},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LightWarner: Predicting failure of 3D NAND flash memory
using reinforcement learning. <em>TC</em>, <em>72</em>(3), 853–867. (<a
href="https://doi.org/10.1109/TC.2022.3184270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NAND flash memory has gained popularity in a wide variety of digital storage systems. Although with excellent performance, NAND flash memory suffers various reliability problems. In recent years, researchers try to predict flash failure by using machine-learning models. However, the application of machine-learning based failure prediction method faces the following problems: imbalance between robustness and portability. When applying on different flash chips, the performance of prediction model degrades with the variation of error characteristics. In order to adapt to the variation, the machine-learning model needs to be re-built to ensure performance of failure prediction. The overheads of re-building model result in challenges when adjusting prediction model to adapt to the variation of error characteristics. To overcome these challenges, we present LightWarner, an easily applicable predictor based on model-free Reinforcement learning algorithms. LightWarner learns error characteristics dynamically during flash lifetime without pre-training. We evaluate the performance of LightWarner on six types of 3D flash chips. The evaluation result shows that LightWarner achieves over 93\% F1 score on different flash chips, which is about 10\% higher than supervised machine learning methods. And LightWarner can adapt to the variation of error characteristics with low migration costs.},
  archive      = {J_TC},
  author       = {Yuqian Pan and Haichun Zhang and Runze Yu and Zhaojun Lu and Haoming Zhang and Zhenglin Liu},
  doi          = {10.1109/TC.2022.3184270},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {853-867},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LightWarner: Predicting failure of 3D NAND flash memory using reinforcement learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Telepathy: A lightweight silent data access protocol for
NVRAM+RDMA enabled distributed storage. <em>TC</em>, <em>72</em>(3),
839–852. (<a href="https://doi.org/10.1109/TC.2022.3184269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in non-volatile memory and networking technologies raise new challenges and opportunities for architecting storage systems. In this paper we propose Telepathy, a lightweight data access protocol for NVRAM+RDMA-based distributed storage systems. Telepathy is a fully distributed protocol whose I/O writes can be coordinated by any server node, and I/O reads can be served by any of the replicas. Telepathy guarantees strongly-consistent reads while providing high I/O concurrency. Hybrid RDMA operations are used to transmit data directly and efficiently to the NVRAM of target servers. The correctness of Telepathy is verified with a formal proof of consistency, and its performance is validated with YCSB benchmarks on the Chameleon cluster. Telepathy can achieve low I/O latencies and high throughput, with low CPU utilization.},
  archive      = {J_TC},
  author       = {Qingyue Liu and Peter Varman},
  doi          = {10.1109/TC.2022.3184269},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {839-852},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Telepathy: A lightweight silent data access protocol for NVRAM+RDMA enabled distributed storage},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HOME: A holistic GPU memory management framework for deep
learning. <em>TC</em>, <em>72</em>(3), 826–838. (<a
href="https://doi.org/10.1109/TC.2022.3180991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose HOlistic MEmory management (HOME), a new framework for performing tensor placements in large DNN training when GPU memory space is not enough. HOME combines tensor swapping with tensor recomputation to reduce GPU memory footprint. Different from existing work that only considers partial DNN model information, HOME takes the holistic DNN model information into account in tensor placement decisions. More specifically, HOME uses a custom-designed particle swarm optimization algorithm to achieve the globally optimized placement for each tensor of the DNN model with a greatly reduced searching space. This holistic awareness of the whole model information enables HOME to obtain high performance under the given GPU memory constraint. We implement HOME in PyTorch and conduct our experiments using six popular DNN models. Experimental results show that HOME can outperform vDNN and Capuchin by up to 5.7× and 1.3× in throughput. Furthermore, HOME can improve the maximum batch size by up to 2.8× than the original PyTorch and up to 1.3× than Capuchin.},
  archive      = {J_TC},
  author       = {Shuibing He and Ping Chen and Shuaiben Chen and Zheng Li and Siling Yang and Weijian Chen and Lidan Shou},
  doi          = {10.1109/TC.2022.3180991},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {826-838},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HOME: A holistic GPU memory management framework for deep learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WHISTLE: CPU abstractions for hardware and software memory
safety invariants. <em>TC</em>, <em>72</em>(3), 811–825. (<a
href="https://doi.org/10.1109/TC.2022.3180990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory safety invariants extracted from a program can help defend and detect against both software and hardware memory violations. For instance, by allowing only specific instructions to access certain memory locations, system can detect out-of-bound or illegal pointer dereferences that lead to correctness and security issues. In this paper, we propose CPU abstractions, called WHISTLE , to specify and check program invariants to provide defense mechanism against both software and hardware memory violations at runtime. WHISTLE ensures that the invariants must be satisfied at every memory access. We present a fast invariant address translation and retrieval scheme using a specialized cache. It stores and checks invariants related to global, stack and heap objects. The invariant checks can be performed synchronously or asynchronously. WHISTLE uses synchronous checking for high security-critical programs, while others are protected by asynchronous checking. A fast exception is proposed to alert any violations as soon as possible in order to close the gap for transient attacks. Our evaluation shows that WHISTLE can detect both software and hardware, spatial and temporal memory violations. WHISTLE incurs 53\% overhead when checking synchronously, or 15\% overhead when checking asynchronously.},
  archive      = {J_TC},
  author       = {Sungkeun Kim and Farabi Mahmud and Jiayi Huang and Pritam Majumder and Chia-Che Tsai and Abdullah Muzahid and Eun Jung Kim},
  doi          = {10.1109/TC.2022.3180990},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {811-825},
  shortjournal = {IEEE Trans. Comput.},
  title        = {WHISTLE: CPU abstractions for hardware and software memory safety invariants},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sandbox computing: A data privacy trusted sharing paradigm
via blockchain and federated learning. <em>TC</em>, <em>72</em>(3),
800–810. (<a href="https://doi.org/10.1109/TC.2022.3180968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new trusted data sharing pattern with privacy protection, the integration mechanism of blockchain and Federated Learning has attracted extensive attention. Generally, this mechanism uses blockchain technology to supervise the original data and calculation results, which ignores the supervision of the Federated Learning model and computing process. Therefore, we introduce the concepts of the sandbox and state channel to construct a new data privacy sharing paradigm via Blockchain and Federated Learning. Under this paradigm, we use state channel to connect Blockchain and Federated Learning. And state channel is used to create a “trusted sandbox” to instantiate Federated Learning tasks in the trustless edge computing environment. Meanwhile, we also mainly solve problems about data privacy sharing in Federated Learning and system performance degradation caused by data quality. The simulation results show that the proposed method has better performance and efficiency than the traditional data sharing method.},
  archive      = {J_TC},
  author       = {Shaoyong Guo and Keqin Zhang and Bei Gong and Liandong Chen and Yinlin Ren and Feng Qi and Xuesong Qiu},
  doi          = {10.1109/TC.2022.3180968},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {800-810},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Sandbox computing: A data privacy trusted sharing paradigm via blockchain and federated learning},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AXI-IC<span class="math inline"><sup>RT</sup></span> RT:
Towards a real-time AXI-interconnect for highly integrated SoCs.
<em>TC</em>, <em>72</em>(3), 786–799. (<a
href="https://doi.org/10.1109/TC.2022.3179227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern real-time heterogeneous System-on-Chips (SoCs), ensuring the predictability of interconnects is becoming increasingly important. Most of the existing interconnects are mainly designed to achieve high throughput, with their micro-architectures usually based on FIFO queues. The FIFO-based design prevents transaction prioritization based on importance and leads to occurrences of physical priority inversion. Such problems lead to difficulties in ensuring transaction predictability, especially when the system scales to a large number of elements. In this paper, we introduce AXI-Interconnect $^{\mathrm {RT}}$ ( AXI-IC$^{\mathrm {RT}}$ RT , for short) – a real-time AXI interconnect for heterogeneous SoCs, which redefines the micro-architecture of interconnects by enabling random accesses of buffered transactions and organizing transactions through compositional scheduling. This hardware-software co-design approach provides predictable and scalable real-time performance for highly integrated SoCs.},
  archive      = {J_TC},
  author       = {Zhe Jiang and Kecheng Yang and Nathan Fisher and Ian Gray and Neil C. Audsley and Zheng Dong},
  doi          = {10.1109/TC.2022.3179227},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {786-799},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AXI-IC$^{\mathrm{ RT}}$ RT: Towards a real-time AXI-interconnect for highly integrated SoCs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PR-SSD: Maximizing partial read potential by exploiting
compression and channel-level parallelism. <em>TC</em>, <em>72</em>(3),
772–785. (<a href="https://doi.org/10.1109/TC.2022.3178326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent NAND flash memories provide a partial read operation that can read a page partially and has lower latency than a normal read operation. In order to maximize the benefit of the partial read operation, compression techniques can be applied to improve performance by generating additional partial page requests via compressing pages into smaller ones. Unfortunately, existing compression-support SSDs suffer from a huge decompression latency that eventually cancels the benefit of the partial read operation. In this paper, we propose Partial Read-aware SSD (PR-SSD) for fully exploiting partial read operations. In order to mitigate the decompression latency, we propose a new compression algorithm, called Dominant Pattern Compression (DPC), which has extremely low decompression latency. Because uncompressed page requests cannot exploit the partial read operation, we propose split Flash Translation Layer (FTL) that can split the requests into smaller ones and allocate them to different channels for exploiting channel-level parallelism in SSD. Experimental results reveal that PR-SSD can reduce the read response time by 18\% on average and also the number of writes and write response time by 29\% and 24\% on average, respectively.},
  archive      = {J_TC},
  author       = {Mincheol Kang and Wonyoung Lee and Jinkwon Kim and Soontae Kim},
  doi          = {10.1109/TC.2022.3178326},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {772-785},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PR-SSD: Maximizing partial read potential by exploiting compression and channel-level parallelism},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multi-spin-flip engineering in an ising machine.
<em>TC</em>, <em>72</em>(3), 759–771. (<a
href="https://doi.org/10.1109/TC.2022.3178325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A merge process is proposed to engineer a multi-spin flip in an Ising machine. The merge process deforms the Hamiltonian (energy function) of the Ising model. We prove a theorem for the merge process and show that a single-spin flip in the deformed Hamiltonian is equivalent to a multi-spin flip in the original Hamiltonian. A merge process induces a transition within the subspace of feasible solutions. We propose a hybrid simulated annealing (SA) algorithm with the merge process. The hybrid algorithm outperforms the conventional SA algorithm, genetic algorithm, and tabu search in the binary quadratic knapsack problems (QKP) and the quadratic assignment problems (QAP). Finally, the hybrid merge process is used in a real Ising machine. The performance is improved in QKP and QAP instances. The merge process is generally applicable to existing Ising machine hardware because the deformed Hamiltonian keeps the format of the Ising model.},
  archive      = {J_TC},
  author       = {Tatsuhiko Shirai and Nozomu Togawa},
  doi          = {10.1109/TC.2022.3178325},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {759-771},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-spin-flip engineering in an ising machine},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight neural architecture search for temporal
convolutional networks at the edge. <em>TC</em>, <em>72</em>(3),
744–758. (<a href="https://doi.org/10.1109/TC.2022.3177955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS) is quickly becoming the go-to approach to optimize the structure of Deep Learning (DL) models for complex tasks such as Image Classification or Object Detection. However, many other relevant applications of DL, especially at the edge, are based on time-series processing and require models with unique features, for which NAS is less explored. This work focuses in particular on Temporal Convolutional Networks (TCNs), a convolutional model for time-series processing that has recently emerged as a promising alternative to more complex recurrent architectures. We propose the first NAS tool that explicitly targets the optimization of the most peculiar architectural parameters of TCNs, namely dilation, receptive-field and number of features in each layer. The proposed approach searches for networks that offer good trade-offs between accuracy and number of parameters/operations, enabling an efficient deployment on embedded platforms. Moreover, its fundamental feature is that of being lightweight in terms of search complexity, making it usable even with limited hardware resources. We test the proposed NAS on four real-world, edge-relevant tasks, involving audio and bio-signals: (i) PPG-based Heart-Rate Monitoring, (ii) ECG-based Arrythmia Detection, (iii) sEMG-based Hand-Gesture Recognition, and (iv) Keyword Spotting. Results show that, starting from a single seed network, our method is capable of obtaining a rich collection of Pareto optimal architectures, among which we obtain models with the same accuracy as the seed, and 15.9-152× fewer parameters. Moreover, the NAS finds solutions that Pareto-dominate state-of-the-art hand-tuned models for 3 out of the 4 benchmarks, and are Pareto-optimal on the fourth (sEMG). Compared to three state-of-the-art NAS tools, ProxylessNAS, MorphNet and FBNetV2, our method explores a larger search space for TCNs (up to $10^{12} \times$ ) and obtains superior solutions, while requiring low GPU memory and search time. We deploy our NAS outputs on two distinct edge devices, the multicore GreenWaves Technology GAP8 IoT processor and the single-core STMicroelectronics STM32H7 microcontroller. With respect to the state-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5× and 3.8× on the two targets respectively, without any accuracy loss.},
  archive      = {J_TC},
  author       = {Matteo Risso and Alessio Burrello and Francesco Conti and Lorenzo Lamberti and Yukai Chen and Luca Benini and Enrico Macii and Massimo Poncino and Daniele Jahier Pagliari},
  doi          = {10.1109/TC.2022.3177955},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {744-758},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lightweight neural architecture search for temporal convolutional networks at the edge},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MANA: Microarchitecting a temporal instruction prefetcher.
<em>TC</em>, <em>72</em>(3), 732–743. (<a
href="https://doi.org/10.1109/TC.2022.3176825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {L1 instruction (L1-I) cache misses are a source of performance bottleneck. While many instruction prefetchers have been proposed over the years, most of them leave a considerable potential uncovered. In 2011, Proactive Instruction Fetch (PIF) showed that a hardware prefetcher could effectively eliminate all instruction-cache misses. However, its enormous storage cost makes it an impractical solution. Consequently, reducing the storage cost was the main research focus in instruction prefetching in the past decade. Several instruction prefetchers, including RDIP and Shotgun, were proposed to offer PIF-level performance with significantly lower storage overhead. However, our findings show that there is a considerable performance gap between these proposals and PIF. While these proposals use different mechanisms for instruction prefetching, the performance gap is mainly not because of the mechanism, and instead, is due to not having sufficient storage. In this paper, we make the case that the key to designing a powerful and cost-effective instruction prefetcher is choosing a metadata record and microarchitecting the prefetcher to minimize the storage. We propose MANA , which offers PIF-level performance with 15.7× lower storage cost. MANA outperforms RDIP and Shotgun by 12.5 and 29\%, respectively. We also evaluate a version of MANA with no storage overhead and show that it offers 98\% of the peak performance benefits.},
  archive      = {J_TC},
  author       = {Ali Ansari and Fatemeh Golshan and Rahil Barati and Pejman Lotfi-Kamran and Hamid Sarbazi-Azad},
  doi          = {10.1109/TC.2022.3176825},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {732-743},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MANA: Microarchitecting a temporal instruction prefetcher},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scenario-based AI benchmark evaluation of distributed
cloud/edge computing systems. <em>TC</em>, <em>72</em>(3), 719–731. (<a
href="https://doi.org/10.1109/TC.2022.3176803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed cloud/edge (DCE) platform has become popular in recent years. This paper proposes a new AI benchmark suite for assessing the performance of DCE platforms in machine learning (ML) and cognitive science applications. The benchmark suite is custom-designed to satisfy scenario-based performance requirements, namely the model training time, inference speed, model accuracy, job response time, quality of service, and system reliability. These metrics are substantiated by intensive experiments with real-life AI workloads. Our work is specially tailored for supporting massive AI multitasking across distributed resources in the networking environment. Our benchmark experiments were conducted on an AI-oriented AIRS cloud built at the Chinese University of Hong Kong, Shenzhen. We have tested a large number of ML/DL programs to narrow down the inclusion of ten representative AI kernel codes in the benchmark suite. Our benchmark results reveal the advantages of using the DCE systems cost-effectively in smart cities, healthcare, community surveillance, and transportation services. Our technical contributions are in the AIRS cloud architecture, benchmark design, testing, and distributed AI computing requirements. Our work will benefit computer system designers and AI application developers on clouds, edge, and mobile devices, that are supported by 5G mobile networks and AIoT resources.},
  archive      = {J_TC},
  author       = {Tianshu Hao and Kai Hwang and Jianfeng Zhan and Yuejin Li and Yong Cao},
  doi          = {10.1109/TC.2022.3176803},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {719-731},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scenario-based AI benchmark evaluation of distributed Cloud/Edge computing systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring truss maintenance in fully dynamic graphs: A mixed
structure-based approach. <em>TC</em>, <em>72</em>(3), 707–718. (<a
href="https://doi.org/10.1109/TC.2022.3174594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are widely employed in complex system modeling, VLSI design, and social analysis. Mining cohesive subgraphs is a fundamental problem in graph analysis, while implementing cohesive subgraphs requires analysts to not only ensure cohesiveness but also consider the computational intractability. Among a variety of diverse cohesive structures, $k$ -truss exhibits a perfect trade-off between structure tightness and computational efficiency. In a $k$ -truss, each edge is present in at least $k-2$ triangles. This study aims to contribute to this growing area of truss maintenance in fully dynamic graphs by avoiding expensive re-computation. Specifically, we consider the challenging scenario of batch processing of edge and vertex insertion/deletion and propose efficient algorithms that can maintain the trusses by only searching a very small range of affected edges. Also, our algorithms allow parallel implementations to further improve the efficiency of maintenance. Extensive experiments on both real-world static and temporal graphs illustrate the efficiency and scalability of our algorithms.},
  archive      = {J_TC},
  author       = {Qi Luo and Dongxiao Yu and Xiuzhen Cheng and Hao Sheng and Weifeng Lyu},
  doi          = {10.1109/TC.2022.3174594},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {707-718},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploring truss maintenance in fully dynamic graphs: A mixed structure-based approach},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ALAMNI: Adaptive LookAside memory based near-memory
inference engine for eliminating multiplications in real-time.
<em>TC</em>, <em>72</em>(3), 693–706. (<a
href="https://doi.org/10.1109/TC.2022.3174591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CNN algorithm seeks high performance and energy efficiency in real-time inference. The costly off-chip memory accesses put additional burdens on CNN&#39;s execution. Towards avoiding off-chip accesses, we propose ALAMNI, a novel near-memory architecture that expedites the CNNs in the logic layer of the Hybrid Memory Cube. We exploit intra- and inter-vault parallelism to accelerate the highly parallel CNN operations. The proposed ALAMNI replaces costly multiplications of CNNs with lookaside memory (LAM) based searches. The proposed ALAMNI policy is effective on unseen data as it discards the data pre-profiling overhead by an adaptive LAM update policy. The ALAMNI controller keeps the most frequent triplets of weight (W), activation (A), and multiplication result (M), $\langle W, A, M \rangle$ , in the LAM to eliminate redundant computations. As an optimization, we incorporate a bitmasking concept to raise the hit rate of LAMs and further amortize computations. We also present a study on the relation between the amount of bitmasking and the loss of classification accuracy of the popular ConvNets. We keep the bitmasking as a reconfigurable feature of the ALAMNI units to achieve desired classification accuracy. Experimental results show substantial improvement in the system&#39;s performance and energy efficiency compared to the baseline and state-of-the-art.},
  archive      = {J_TC},
  author       = {Palash Das and Shashank Sharma and Hemangee K. Kapoor},
  doi          = {10.1109/TC.2022.3174591},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {693-706},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ALAMNI: Adaptive LookAside memory based near-memory inference engine for eliminating multiplications in real-time},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RISC-v galois field ISA extension for non-binary
error-correction codes and classical and post-quantum cryptography.
<em>TC</em>, <em>72</em>(3), 682–692. (<a
href="https://doi.org/10.1109/TC.2022.3174587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the recent advances in new communication standards, such as 5G New Radio and beyond 5G, and in quantum computing and communications, new requirements for integrating processors into nodes have appeared. These requirements are meant to provide flexibility in the network to reduce operational costs and support diversity in services and load balancing. They are also designed to integrate both new and classical algorithms into efficient and universal platforms, execute specific operations, and attend to tasks with lower latency. Furthermore, some cryptographic algorithms (classical and post-quantum), which are essential to portable devices, share the same arithmetic with error-correction codes. For example, Advanced Encryption Standard (AES), elliptic curve cryptography, Classic McEliece, Hamming Quasi-Cyclic, and Reed-Solomon codes use $GF(2^{m})$ arithmetic. As this arithmetic is the basis of many algorithms, a versatile RISC-V Galois field ISA extension is proposed in this work. The RISC-V instruction set extension is implemented and validated using SweRV-EL2 1.3 on a Nexys A7 FPGA. In addition, a five-times acceleration is achieved for AES, Reed-Solomon codes, and Classic McEliece (post-quantum cryptography) at the expense of increasing the logic utilization by 1.27\%.},
  archive      = {J_TC},
  author       = {Yao-Ming Kuo and Francisco García-Herrero and Oscar Ruano and Juan Antonio Maestro},
  doi          = {10.1109/TC.2022.3174587},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {682-692},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RISC-V galois field ISA extension for non-binary error-correction codes and classical and post-quantum cryptography},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximate MRAM: High-performance and power-efficient
computing with MRAM chips for error-tolerant applications. <em>TC</em>,
<em>72</em>(3), 668–681. (<a
href="https://doi.org/10.1109/TC.2022.3174584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing (AC) leverages the inherent error resilience and is used in many big-data applications from various domains such as multimedia, computer vision, signal processing, and machine learning to improve systems performance and power consumption. Like many other approximate circuits and algorithms, the memory subsystem can also be used to enhance performance and save power significantly. This paper proposes an efficient and effective systematic methodology to construct an approximate non-volatile magneto-resistive RAM (MRAM) framework using consumer-off-the-shelf (COTS) MRAM chips. In the proposed scheme, an extensive experimental characterization of memory errors is performed by manipulating the write latency of MRAM chips which exploits the inherent (intrinsic/extrinsic process variation) stochastic switching behavior of magnetic tunnel junctions (MTJs). The experimental results, involving error-resilient image compression and machine learning applications, reveal that the proposed AC framework provides a significant performance improvement and demonstrates a reduction in MRAM write energy of ${\sim }47.5\%$ on average with negligible or no loss in output quality.},
  archive      = {J_TC},
  author       = {Farah Ferdaus and B. M. S. Bahar Talukder and Md Tauhidur Rahman},
  doi          = {10.1109/TC.2022.3174584},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {668-681},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Approximate MRAM: High-performance and power-efficient computing with MRAM chips for error-tolerant applications},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online scheduling of distributed machine learning jobs for
incentivizing sharing in multi-tenant systems. <em>TC</em>,
<em>72</em>(3), 653–667. (<a
href="https://doi.org/10.1109/TC.2022.3174566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To save cost, companies usually train machine learning (ML) models on a shared multi-tenant system. In this cooperative environment, one of the fundamental challenges is how to distribute resources fairly among tenants such that each tenant is satisfied. A satisfactory allocation policy needs to meet the following properties. First, the performance of each tenant in the shared cluster is at least the same as that in its exclusive cluster partition. Second, no tenant can get more benefits by lying about its demands. Third, tenants cannot use the idle resources of others for free. Moreover, the resource allocation for ML workloads should avoid costly migration overhead. To this end, we propose a three-layer scheduling framework Astraea : i) a batch scheduling framework groups unprocessed ML jobs into multiple batches; ii) a round-by-round algorithm enables tenants to reserve their share of resources and schedule ML jobs in a non-preemptive manner; iii) one-round algorithm based on the primal-dual approach and posted pricing framework, which encourages tenants to report truthful demands and computes a schedule with maximal revenue for each ML job. Besides, our algorithm also allows tenants to trade unused resources on a paid basis. Astraea is proven to achieve performance guarantee, polynomial time complexity and some desirable properties of resource sharing, including batch sharing incentive, strategy-proofness, Pareto efficiency and gain-as-you-contribute fairness. Extensive trace-driven simulations show that our algorithm advances in both fairness and cluster efficiency by at least 20\% compared to three state-of-the-art baselines.},
  archive      = {J_TC},
  author       = {Ne Wang and Ruiting Zhou and Ling Han and Hao Chen and Zongpeng Li},
  doi          = {10.1109/TC.2022.3174566},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {653-667},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Online scheduling of distributed machine learning jobs for incentivizing sharing in multi-tenant systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Denial-of-service vulnerability of hash-based transaction
sharding: Attack and countermeasure. <em>TC</em>, <em>72</em>(3),
641–652. (<a href="https://doi.org/10.1109/TC.2022.3174560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 2016, sharding has become an auspicious solution to tackle the scalability issue in legacy blockchain systems. Despite its potential to strongly boost the blockchain throughput, sharding comes with its own security issues. To ease the process of deciding which shard to place transactions, existing sharding protocols use a hash-based transaction sharding in which the hash value of a transaction determines its output shard. Unfortunately, we show that this mechanism opens up a loophole that could be exploited to conduct a single-shard flooding attack, a type of Denial-of-Service (DoS) attack, to overwhelm a single shard that ends up reducing the performance of the system as a whole. To counter the single-shard flooding attack, we propose a countermeasure that essentially eliminates the loophole by rejecting the use of hash-based transaction sharding. The countermeasure leverages the Trusted Execution Environment (TEE) to let blockchain&#39;s validators securely execute a transaction sharding algorithm with a negligible overhead. We provide a formal specification for the countermeasure and analyze its security properties in the Universal Composability (UC) framework. Finally, a proof-of-concept is developed to demonstrate the feasibility and practicality of our solution.},
  archive      = {J_TC},
  author       = {Truc Nguyen and My T. Thai},
  doi          = {10.1109/TC.2022.3174560},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {641-652},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Denial-of-service vulnerability of hash-based transaction sharding: Attack and countermeasure},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive power shifting for power-constrained heterogeneous
systems. <em>TC</em>, <em>72</em>(3), 627–640. (<a
href="https://doi.org/10.1109/TC.2022.3174545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number and heterogeneity of compute devices, even within a single compute node, has been steadily on the rise. Since all systems must operate under a power cap, the number of discrete devices that can run simultaneously at their highest frequency is limited by the globally-imposed power cap. Current systems incorporate a centralized power management unit that statically controls the distribution of power among the devices within the node. However, such static distribution policies are unaware of the dynamic utilization profile across the devices, which leads to power allocations that end up degrading system throughput performance. The problem is particularly acute in the presence of heterogeneity since type-specific performance-boost capabilities cannot be leveraged via utilization-agnostic static power allocations. This paper proposes Adaptive Power Shifting for multi-accelerator heterogeneous systems (APS), a technique that leverages system utilization information to dynamically allocate and re-distribute power budgets across multiple discrete devices. Democratizing the power allocation based on dynamic needs results in dramatic speedup over a need-agnostic static allocation. We use APS in a real OpenPOWER compute node with 2 CPUs and 4 GPUs to demonstrate the value of on-demand, equitable power allocations. Overall, the proposed solution increases performance with respect to two state-of-the-art techniques by up to 14.9\% and 13.8\%.},
  archive      = {J_TC},
  author       = {Cristobal Ortega and Lluc Alvarez and Alper Buyuktosunoglu and Ramon Bertran and Todd Rosedahl and Pradip Bose and Miquel Moreto},
  doi          = {10.1109/TC.2022.3174545},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {627-640},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive power shifting for power-constrained heterogeneous systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adapt burstable containers to variable CPU resources.
<em>TC</em>, <em>72</em>(3), 614–626. (<a
href="https://doi.org/10.1109/TC.2022.3174480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of the cloud-native, container technology, referred as OS-level virtualization, is increasingly adopted to deploy cloud applications. Compared with virtual machines, containers are lightweight and flexible in resource management. An important quality-of-service (QoS) class in container management is burstable container, whose resource limits are higher than the actual requests allowing a container to expand whenever demands ramp up and additional resources become available. However, efficiently managing burstable containers is challenging, especially for CPU resources. On the one hand, burstable containers should maintain sufficient concurrency, in the form of threads, to utilize extendable CPU resources. On the other hand, the degree of concurrency necessary for utilizing peak CPU resources leads to suboptimal performance when a container&#39;s CPU allocation is constrained. In this paper, we recommend that the number of threads in burstable containers should always be set to the CPU limit to guarantee extensibility. However, modern operating systems (OSes) fall short of efficiently managing thread oversubscription. First, the OS CPU scheduler is inefficient for scheduling excessive threads and lacks container awareness. Second, the existing blocking synchronization supported by the OS kernel is inefficient in handling the sleep and wakeup of excessive threads. Finally, the non-blocking synchronization may waste CPUs performing busy waiting when more than one thread in the run queue. To this end, we present a user-level adaptive container scheduler and two OS mechanisms, virtual blocking and busy-waiting detection , to avoid inefficiency in managing burstable containers without requiring program code changes. Experimental results show that our approaches can keep burstable containers efficient while allowing the applications in containers to take advantage of additional CPUs. The performance gain under high system load is up to 29.7×.},
  archive      = {J_TC},
  author       = {Hang Huang and Yuqing Zhao and Jia Rao and Song Wu and Hai Jin and Duoqiang Wang and Suo Kun and Lisong Pan},
  doi          = {10.1109/TC.2022.3174480},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {614-626},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adapt burstable containers to variable CPU resources},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepWare: Imaging performance counters with deep learning to
detect ransomware. <em>TC</em>, <em>72</em>(3), 600–613. (<a
href="https://doi.org/10.1109/TC.2022.3173149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the year passed, rarely a month passes without a ransomware incident being published in a newspaper or social media. In addition to the rise in the frequency of ransomware attacks, emerging attacks are very effective as they utilize sophisticated techniques to bypass existing organizational security perimeter. To tackle this issue, this paper presents “DeepWare,” which is a ransomware detection model inspired by deep learning and hardware performance counter (HPC). Different from previous works aiming to check all HPC results returned from a single timing for every running process, DeepWare carries out a simple yet effective concept of “ imaging hardware performance counters with deep learning to detect ransomware ,” so as to identify ransomware efficiently and effectively. To be more specific, DeepWare monitors the system-wide change in the distribution of HPC data. By imaging the HPC values and restructuring the conventional CNN model, DeepWare can address HPC’s nondeterminism issue by extracting the event-specific and event-wise behavioral features, which allows it to distinguish the ransomware activity from the benign one effectively. The experiment results across ransomware families show that the proposed DeepWare is effective at detecting different classes of ransomware with the 98.6\% recall score, which is 84.41\%, 60.93\%, and 21\% improvement over RATAFIA , OC-SVM , and EGB models respectively. DeepWare achieves an average MCC score of 96.8\% and nearly zero false-positive rates by using just a 100 ms snapshot of HPC data. This timeliness of DeepWare is critical on the ground that organizations and individuals have the opportunity to take countermeasures in the first stage of the attack. Besides, the experiment conducted on unseen ransomware families such as CoronaVirus, Ryuk, and Dharma demonstrates that DeepWare has excellent potential to be a useful tool for zero-day attack detection.},
  archive      = {J_TC},
  author       = {Gaddisa Olani Ganfure and Chun-Feng Wu and Yuan-Hao Chang and Wei-Kuan Shih},
  doi          = {10.1109/TC.2022.3173149},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {600-613},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DeepWare: Imaging performance counters with deep learning to detect ransomware},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards thermal-aware workload distribution in cloud data
centers based on failure models. <em>TC</em>, <em>72</em>(2), 586–599.
(<a href="https://doi.org/10.1109/TC.2022.3158476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing workload conditions lead to a significant surge in power consumption and computing node failures in data centers. The existing workload distribution strategies focused on either thermal awareness or failure mitigation, overlooking the impact of node failures on the energy efficiency of cloud data centers. To address this issue, a new holistic model is built to characterize the impacts of workloads, computing and cooling costs, heat recirculation, and node failure on the energy efficiency of cloud data centers. Leveraging such a holistic model, we propose a novel thermal-aware workload distribution strategy called HGSA that takes node failure into accountand can improve the energy efficiency of cloud data centers. Our empirical findings confirm that (i) faulty nodes lead to a large rise in power consumption, and (ii) failure locations play a vital role in the power consumption of data centers. Experimental results unveil that HGSA is adroit at making near-optimal decisions in workload distribution strategies. In particular, HGSA cuts down the minimum inlet temperature by 5.2 $\%$ -15 $\%$ , improves the maximum air temperature of a Computer Room Air Conditioner (CRAC) model by 4.2 $\%$ -26.5 $\%$ , lowers the cooling cost by 15.4 $\%$ -50 $\%$ compared to the existing solutions. Furthermore, HGSA cuts back the total power consumption by 0.65 $\%$ -78 $\%$ .},
  archive      = {J_TC},
  author       = {Jie Li and Yuhui Deng and Yi Zhou and Zhen Zhang and Geyong Min and Xiao Qin},
  doi          = {10.1109/TC.2022.3158476},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {586-599},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards thermal-aware workload distribution in cloud data centers based on failure models},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting fault adversary models – hardware faults in
theory and practice. <em>TC</em>, <em>72</em>(2), 572–585. (<a
href="https://doi.org/10.1109/TC.2022.3164259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault injection attacks are considered as powerful techniques to successfully attack embedded cryptographic implementations since various fault injection mechanisms from simple clock glitches to more advanced techniques like laser fault injection can lead to devastating attacks. Given these critical attack vectors, researchers came up with a long list of dedicated countermeasures to thwart such attacks. However, the security validation of proposed countermeasures is mostly performed on custom adversary models that are often not tightly coupled with the actual physical behavior of available fault injection mechanisms and, hence, fail to model the reality accurately. Furthermore, using custom models complicates comparison between different designs and evaluation results. As a consequence, we aim to close this gap by proposing a simple, generic, and consolidated fault injection adversary model that can be perfectly tailored to existing fault injection mechanisms and their physical behavior in hardware. To demonstrate the advantages, we apply it to a cryptographic primitive and evaluate it based on different attack vectors. We further show that our proposed adversary model can be integrated into the state-of-the-art fault verification tool VerFI. Finally, we provide a discussion on the benefits and differences of our approach compared to already existing evaluation methods.},
  archive      = {J_TC},
  author       = {Jan Richter-Brockmann and Pascal Sasdrich and Tim Güneysu},
  doi          = {10.1109/TC.2022.3164259},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {572-585},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Revisiting fault adversary models – hardware faults in theory and practice},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AINNS: All-inclusive neural network scheduling via
accelerator formalization. <em>TC</em>, <em>72</em>(2), 559–571. (<a
href="https://doi.org/10.1109/TC.2022.3160358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the rapid development of accelerators and diverse efficiency requirements of the naturally heterogeneous neural network computation, recent years have seen increased heterogeneity in neural network accelerator systems in terms of network structures, accelerator dataflows and implementations. However, existing research fails to schedule and map the heterogeneous neural networks on heterogeneous accelerators efficiently. They rely on clumpy exhaustive search or complicated ad hoc mapping approaches due to the semantic gap between the networks and accelerators. This paper proposes a systematic method to transform various accelerators into standard parameterized containers of the neural network loops, which builds a direct connection between the computation and the underlying hardware resources. This enables us to match the neural networks with accelerators based on their essential characteristics (e.g., reuse opportunities and bandwidth requirements) without diving into the detailed architectures. To this end, we propose AINNS, an all-inclusive neural network scheduler, that automatically schedules and maps the NN computation on heterogeneous accelerators with just one universal algorithm. Our experimental results show the proposed AINNS not only performs well in the traditional neural network acceleration but also improves the system throughput and energy efficiency by 1.8x and 1.7x respectively in the most challenging heterogeneous acceleration system.},
  archive      = {J_TC},
  author       = {Jiaqi Zhang and Xiangru Chen and Sandip Ray},
  doi          = {10.1109/TC.2022.3160358},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {559-571},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AINNS: All-inclusive neural network scheduling via accelerator formalization},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SPDL: A blockchain-enabled secure and privacy-preserving
decentralized learning system. <em>TC</em>, <em>72</em>(2), 548–558. (<a
href="https://doi.org/10.1109/TC.2022.3169436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized learning involves training machine learning models over remote mobile devices, edge servers, or cloud servers while keeping data localized. Even though many studies have shown the feasibility of preserving privacy, enhancing training performance or introducing Byzantine resilience, but none of them simultaneously considers all of them. Therefore we face the following problem: how can we efficiently coordinate the decentralized learning process while simultaneously maintaining learning security and data privacy for the entire system? To address this issue, in this paper we propose SPDL, a blockchain-secured and privacy-preserving decentralized learning system. SPDL integrates blockchain, Byzantine Fault-Tolerant (BFT) consensus, BFT Gradients Aggregation Rule (GAR), and differential privacy seamlessly into one system, ensuring efficient machine learning while maintaining data privacy, Byzantine fault tolerance, transparency, and traceability. To validate our approach, we provide rigorous analysis on convergence and regret in the presence of Byzantine nodes. We also build a SPDL prototype and conduct extensive experiments to demonstrate that SPDL is effective and efficient with strong security and privacy guarantees.},
  archive      = {J_TC},
  author       = {Minghui Xu and Zongrui Zou and Ye Cheng and Qin Hu and Dongxiao Yu and Xiuzhen Cheng},
  doi          = {10.1109/TC.2022.3169436},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {548-558},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SPDL: A blockchain-enabled secure and privacy-preserving decentralized learning system},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monarch: A durable polymorphic memory for data intensive
applications. <em>TC</em>, <em>72</em>(2), 535–547. (<a
href="https://doi.org/10.1109/TC.2022.3160608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D die stacking has often been proposed to build large-scale DRAM-based caches. Unfortunately, the power and performance overheads of DRAM limit the efficiency of high-bandwidth memories. Also, DRAM is facing serious scalability challenges that make alternative technologies more appealing. This paper examines Monarch, a resistive 3D stacked memory based on a novel reconfigurable crosspoint array called XAM. The XAM array is capable of switching between random access and content-addressable modes, which enables Monarch (i) to better utilize the in-package bandwidth and (ii) to satisfy both the random access memory and associative search requirements of various applications. Moreover, the Monarch controller ensures a given target lifetime for the resistive stack. Our simulation results on a set of parallel memory-intensive applications indicate that Monarch outperforms an ideal DRAM caching by $1.21\times$ on average. For in-memory hash table and string matching workloads, Monarch improves performance up to $12\times$ over the conventional high bandwidth memories.},
  archive      = {J_TC},
  author       = {Ananth Krishna Prasad and Mahdi Nazm Bojnordi},
  doi          = {10.1109/TC.2022.3160608},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {535-547},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Monarch: A durable polymorphic memory for data intensive applications},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StreamDFP: A general stream mining framework for adaptive
disk failure prediction. <em>TC</em>, <em>72</em>(2), 520–534. (<a
href="https://doi.org/10.1109/TC.2022.3160365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore machine learning for accurately predicting imminent disk failures and hence providing proactive fault tolerance for modern large-scale storage systems. Current disk failure prediction approaches are mostly offline and assume that the disk logs required for training learning models are available a priori. However, disk logs are often continuously generated as an evolving data stream, in which the statistical patterns vary over time (also known as concept drift). Such a challenge motivates the need of online techniques that perform training and prediction on the incoming stream of disk logs in real time, while being adaptive to concept drift. We first measure and demonstrate the existence of concept drift on various disk models in production. Motivated by our study, we design StreamDFP , a general stream mining framework for disk failure prediction with concept-drift adaptation based on three key techniques, namely online labeling, concept-drift-aware training, and general prediction, with a primary objective of supporting various machine learning algorithms. We extend StreamDFP to support online transfer learning for minority disk models with concept-drift adaptation. Our evaluation shows that StreamDFP improves the prediction accuracy significantly compared to without concept-drift adaptation under various settings, and achieves reasonably high stream processing performance.},
  archive      = {J_TC},
  author       = {Shujie Han and Patrick P. C. Lee and Zhirong Shen and Cheng He and Yi Liu and Tao Huang},
  doi          = {10.1109/TC.2022.3160365},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {520-534},
  shortjournal = {IEEE Trans. Comput.},
  title        = {StreamDFP: A general stream mining framework for adaptive disk failure prediction},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-refreshable bit-cell for single-cycle refreshing of
embedded memories. <em>TC</em>, <em>72</em>(2), 513–519. (<a
href="https://doi.org/10.1109/TC.2022.3158481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power supply voltage reduction is a primary enabler for sustaining the increasing demand for ultra-low power processors. On-die memories, which are traditionally implemented by SRAM, stop functioning properly when the supply voltage is scaled down aggressively; hence, embedded DRAM (eDRAM) bit-cells are used instead. These bit-cells leak their data strongly in one direction, whereas the leakage in the opposite direction is considerably lower. Due to their intrinsic limited Data Retention Time (DRT), these memories require power-hungry refreshing, which degrades performance. In an attempt to extend the DRT of a bit-cell, theoretically to infinity, compounds of various types of storage nodes in a single bit-cell, storing the datum and its complement, were examined here. A rigorous proof shows that under realistic leakage models, there is an inherent incompleteness preventing the proper readout and decision of the stored value after a certain time. Adopting the idea of dual-polarity complementary storage nodes, a new eDRAM self-refreshable bit-cell is proposed that yields a considerably extended DRT. The dual-polarity property enables the refreshing of an entire memory array in a single clock cycle, thus almost nullifying the unavoidable performance loss occurred by row-by-row ordinary power-hungry refreshing.},
  archive      = {J_TC},
  author       = {Binyamin Frankel and Shmuel Wimer},
  doi          = {10.1109/TC.2022.3158481},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {513-519},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A self-refreshable bit-cell for single-cycle refreshing of embedded memories},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A blockchain-based decentralized, fair and authenticated
information sharing scheme in zero trust internet-of-things.
<em>TC</em>, <em>72</em>(2), 501–512. (<a
href="https://doi.org/10.1109/TC.2022.3157996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet-of-Things (IoT) are increasingly operating in the zero-trust environments where any devices and systems may be compromised and hence untrusted. In addition, data collected by and sent from IoT devices may be shared with and processed by edge computing systems, in order to reduce the reliance on centralized (cloud) servers, leading to further security and privacy issues. To cope with these challenges, this paper proposes an innovative blockchain-enabled information sharing solution in zero-trust context to guarantee anonymity yet entity authentication, data privacy yet data trustworthiness, and participant stimulation yet fairness. This new solution is able to support filtering of fabricated information through smart contracts, effective voting, and consensus mechanisms, which can prevent unauthenticated participants from sharing garbage information. We also prove that the proposed solution is secure in the universal composability framework, and further evaluate its performance over an Ethereum-based blockchain platform to demonstrate its utility.},
  archive      = {J_TC},
  author       = {Yizhi Liu and Xiaohan Hao and Wei Ren and Ruoting Xiong and Tianqing Zhu and Kim-Kwang Raymond Choo and Geyong Min},
  doi          = {10.1109/TC.2022.3157996},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {501-512},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A blockchain-based decentralized, fair and authenticated information sharing scheme in zero trust internet-of-things},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and analysis of thermal covert channel attacks in
many-core systems. <em>TC</em>, <em>72</em>(2), 494–500. (<a
href="https://doi.org/10.1109/TC.2022.3160356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a many-core chip, thermal flux and thermal correlation among the cores can be explored to create a thermal covert channel (TCC). In this paper, we provide an analytical model to quickly determine the key TCC performance metrics, in terms of bit error rate (BER), signal to noise ratio (SNR), and channel capacity, without going through lengthy computer simulation and/or physical experiments that are normally needed in current TCC performance studies. According to our model, the TCC’s BER is proportional to the square root of the transmission frequency, which can be explored quantitatively to boost the TCC’s transmission efficiency by letting the TCC’s thermal signal be transmitted at a higher frequency. In addition, our proposed model also links the jamming noise and application of Dynamic Voltage Frequency Scaling (DVFS) to TCC’s BER performance, a feature that can be explored to design/optimize the countermeasures against the TCC attacks. The TCC performance predicted by the proposed theoretical model is found in a good agreement with that obtained from computer simulations, with an average error lower than 7\%.},
  archive      = {J_TC},
  author       = {Shengjie Wang and Xiaohang Wang and Yingtao Jiang and Amit Kumar Singh and Mei Yang and Letian Huang},
  doi          = {10.1109/TC.2022.3160356},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {494-500},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Modeling and analysis of thermal covert channel attacks in many-core systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sky-sorter: A processing-in-memory architecture for
large-scale sorting. <em>TC</em>, <em>72</em>(2), 480–493. (<a
href="https://doi.org/10.1109/TC.2022.3169434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sorting is one of the most important algorithms in computer science. Conventional CPUs, GPUs, FPGAs, and ASICs running sorting are fundamentally bottlenecked by the off-chip memory bandwidth, because of their von-Neumann architecture. Processing-near-memory (PNM) designs integrate a CPU, a GPU or an ASIC upon an HBM for sorting, but their sorting throughput are still limited by the HBM bandwidth and capacity. In this paper, we propose a skyrmion racetrack memory (SRM) -based PIM accelerator, Sky-Sorter , to enhance the sorting performance of large-scale datasets. Sky-Sorter implements samplesort which involves four steps, sampling, splitting marker sorting, partitioning, and bucket sorting. An SRM-based random number generator (TRNG) is used in Sky-Sorter to randomly sample records from the dataset. Sky-Sorter divides the large dataset into many buckets based on sampled splitting markers by our proposed SRM-based partitioner. Its partitioning throughput matches the off-chip memory bandwidth. We further designed an SRM-based sorting unit (SU) to sort all records of a bucket without introducing extra CMOS logic. Our SU uses the fast in-cell insertion characteristics of SRMs to implement and perform insertsort within a bucket. Sky-Sorter employs SUs to sort all buckets simultaneously by fully utilizing large internal array bandwidth. Compared to state-of-the-art accelerators, Sky-Sorter improves the throughput per Watt by $\sim 4\times$ .},
  archive      = {J_TC},
  author       = {Farzaneh Zokaee and Fan Chen and Guangyu Sun and Lei Jiang},
  doi          = {10.1109/TC.2022.3169434},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {480-493},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Sky-sorter: A processing-in-memory architecture for large-scale sorting},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CurIAs: Current-based IC authentication by exploiting supply
current variations. <em>TC</em>, <em>72</em>(2), 466–479. (<a
href="https://doi.org/10.1109/TC.2022.3158075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical unclonable functions (PUFs) have emerged as one of the most notable hardware primitives to mitigate the ever-growing global issue of counterfeiting and cloning of integrated circuits (ICs) in recent times. PUFs exploit the intrinsic manufacturing process-induced parametric variations for generating unique chip identifiers. However, most of the existing PUF implementations require complex structures or the inclusion of additional components, which incur performance and area overheads. In this work, we introduce CurIAs , a supply current-based novel PUF implementation to authenticate ICs and to protect them from counterfeiting attacks. It exploits the dynamic current stemming from temporal switching activities in existing on-chip structures as an entropy source to generate high-quality IC-specific digital signatures. First, we investigate the source of the entropy of this PUF, i.e. , the dynamic current variations in different circuit structures, with transistor-level Monte-Carlo simulations in HSPICE. Next, to evaluate its effectiveness in Silicon, we apply this approach to map LFSR (Linear Feedback Shift Register) designs into 20 FPGA chips (fabricated in TSMC 55nm process node), perform practical measurements, and generate digital signatures. These signatures show high uniqueness, robustness, uniformity, and randomness features, and the overall implementation requires modest hardware overhead ( $&amp;lt;$ 1\%). We assess and substantiate the robustness of this approach at eight different operating points by varying supply voltage and temperature. Furthermore, we upscale the design to more extended LFSR sizes, and it exhibits a constant trend of performance improvement over the operating points. Through a judicious selection of challenge vectors, CurIAs demonstrates a high resilience against model learning attacks, with an average prediction accuracy of 50\%. These intrinsic variations in supply current across ICs for varying workloads entail unique chip-specific signatures, which are extremely difficult to clone, and can be deployed effectively against IC counterfeiting issues.},
  archive      = {J_TC},
  author       = {Shubhra Deb Paul and Swarup Bhunia},
  doi          = {10.1109/TC.2022.3158075},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {466-479},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CurIAs: Current-based IC authentication by exploiting supply current variations},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High performance hierarchical tucker tensor learning using
GPU tensor cores. <em>TC</em>, <em>72</em>(2), 452–465. (<a
href="https://doi.org/10.1109/TC.2022.3172895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting information from large-scale high-dimensional data is a fundamentally important task in high performance computing, where the hierarchical Tucker (HT) tensor learning approach (learning a tensor-tree structure) has been widely used in many applications. However, HT tensor learning algorithms are compute-intensive due to the “ curse of dimensionality ,” i.e., the time complexity grows exponentially with the order of the data tensor. The computation of HT tensor learning algorithms boils down to tensor primitives, which are amenable to computing on GPU tensor cores. Existing work does not support HT tensor learning using GPU tensor cores. There are three main challenges to address: 1) to accelerate tensor learning primitives using GPU tensor cores; 2) to implement the tensor learning algorithms using GPU tensor cores and multiple GPUs; 3) to support large-scale data tensors exceeding the GPU memory capacity. In this paper, we present efficient HT tensor learning primitives using GPU tensor cores and demonstrate three applications. First, we utilize GPU tensor cores to optimize HT tensor learning primitives, including tensor contractions, tensor matricizations and tensor singular value decomposition (SVD). We employ the optimized primitives to optimize HT tensor decomposition algorithms for Big Data analysis. Second, we propose a novel HT tensor layer for deep neural networks, whose training process only involves a forward pass without back propagation. The forward pass consists of tensor operations, thus further exploiting the computing power of GPU tensor cores. Third, we apply the optimized primitives to develop a tensor-tree structured quantum machine learning algorithm tree-tensor network (TTN) . Compared with TensorLy and TensorNetwork on NVIDIA A100 GPUs, our third-order HT tensor decomposition algorithm achieves up to $8.92 \times$ and $6.42 \times$ speedups, respectively, and our high-order case achieves up to $32.67 \times$ and $23.97 \times$ speedups, respectively. Our HT tensor layer for a fully connected neural network achieves $49.2 \times$ compression at the cost of 0.5\% drops in accuracy and $1.42 \times$ speedup compared with the implementation on CUDA cores; for the AlexNet, our HT tensor layer achieves $9.45 \times$ compression at the cost of 0.8\% drops in accuracy and $1.87 \times$ speedup compared with the implementation on CUDA cores. Our TTN algorithm achieves up to $11.17\times$ speedup compared with TensorNetwork, indicating the potential of optimized tensor learning primitives for the classical simulation of quantum machine learning algorithms.},
  archive      = {J_TC},
  author       = {Hao Huang and Xiao-Yang Liu and Weiqin Tong and Tao Zhang and Anwar Walid and Xiaodong Wang},
  doi          = {10.1109/TC.2022.3172895},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {452-465},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High performance hierarchical tucker tensor learning using GPU tensor cores},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Malware-on-the-brain: Illuminating malware byte codes with
images for malware classification. <em>TC</em>, <em>72</em>(2), 438–451.
(<a href="https://doi.org/10.1109/TC.2022.3160357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware is a piece of software that was written with the intent of doing harm to data, devices, or people. Since a number of new malware variants can be generated by reusing codes, malware attacks can be easily launched and thus become common in recent years, incurring huge losses in businesses, governments, financial institutes, health providers, etc. To defeat these attacks, malware classification is employed, which plays an essential role in anti-virus products. However, existing works that employ either static analysis or dynamic analysis have major weaknesses in complicated reverse engineering and time-consuming tasks. In this paper, we propose a visualized malware classification framework called VisMal, which provides highly efficient categorization with acceptable accuracy. VisMal converts malware samples into images and then applies a contrast-limited adaptive histogram equalization algorithm to enhance the similarity between malware image regions in the same family. We provided a proof-of-concept implementation and carried out an extensive evaluation to verify the performance of our framework. The evaluation results indicate that VisMal can classify a malware sample within 4.0 ms and have an average accuracy of 96.0\%. Moreover, VisMal provides security engineers with a simple visualization approach to further validate its performance.},
  archive      = {J_TC},
  author       = {Fangtian Zhong and Zekai Chen and Minghui Xu and Guoming Zhang and Dongxiao Yu and Xiuzhen Cheng},
  doi          = {10.1109/TC.2022.3160357},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {438-451},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Malware-on-the-brain: Illuminating malware byte codes with images for malware classification},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On-line fault protection for ReRAM-based neural networks.
<em>TC</em>, <em>72</em>(2), 423–437. (<a
href="https://doi.org/10.1109/TC.2022.3160345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging Resistive RAM (ReRAM) technology significantly boosts the performance and the energy efficiency of the deep learning accelerators (DLAs) via the Computing-in-Memory (CiM) architecture. However, ReRAM-based DLA also suffers a high occurrence rate of memory faults. How to detect and protect against the faults in ReRAM devices poses great challenges to ReRAM-based DLA design. In this work, we propose RRAMedy, an in-situ fault detection and network remedy framework for ReRAM-based DLAs. With the proposed Adversarial Example Testing, which is a lifetime on-device and on-line fault detection technique, it achieves high detection coverage of both hard faults and soft faults at a low run-time cost. In addition, it employs an edge-cloud collaborative model retraining method to tolerate the detected faults by leveraging the inherent fault-adaptive capability of DNNs. Meanwhile, to enable in-situ model remedy when the cloud assistance is absent due to security or overhead issues, we propose to accelerate the fault-masking retraining process on edge devices with parallelized Knowledge Transfer. Our experimental results show that the proposed fault detection technique achieves high fault detection accuracy and delivers real-time testing performance. Meanwhile, the proposed retraining approach greatly alleviates the accuracy degradation problem and achieves excellent performance speedups over the baselines.},
  archive      = {J_TC},
  author       = {Wen Li and Ying Wang and Cheng Liu and Yintao He and Lian Liu and Huawei Li and Xiaowei Li},
  doi          = {10.1109/TC.2022.3160345},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {423-437},
  shortjournal = {IEEE Trans. Comput.},
  title        = {On-line fault protection for ReRAM-based neural networks},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). (Adversarial) electromagnetic disturbance in the industry.
<em>TC</em>, <em>72</em>(2), 414–422. (<a
href="https://doi.org/10.1109/TC.2022.3224373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Faults occur naturally and are responsible for reliability concerns. Faults are also an interesting tool for attackers to extract sensitive information from secure chips. In particular, non-invasive fault attacks have received a fair amount of attention. One easy way to perturb a chip without altering it is the so-called Electromagnetic Fault Injection (EMFI). Such attack has been studied in great depth, and nowadays, it is part and parcel of the state-of-the-art. Indeed, new capabilities have emerged where EM experimental benches are used to cryptanalyze chips. The progress of this “field” is fast, in terms of reproducibility , accuracy , and number of use-cases . However, there is too little awareness about such advances. In this paper, we aim to expose the true harmfulness of EMFI (including reproducibility) to enable reasonable security quotations. We also analyze protections (at hardware/firmware/system levels) in light of their efficiency. We characterize the specificity of EM fault injection compared to other injection means (laser, glitch, probing).},
  archive      = {J_TC},
  author       = {Arthur Beckers and Sylvain Guilley and Philippe Maurine and Colin O&#39;Flynn and Stjepan Picek},
  doi          = {10.1109/TC.2022.3224373},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {414-422},
  shortjournal = {IEEE Trans. Comput.},
  title        = {(Adversarial) electromagnetic disturbance in the industry},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating robust DNN with resistance to bit-flip based
adversarial weight attack. <em>TC</em>, <em>72</em>(2), 401–413. (<a
href="https://doi.org/10.1109/TC.2022.3211411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rowhammer Attack, a new DRAM-based attack, was developed exploiting weak cells to alter their content. Such attacks can be launched at the user level without requiring access permission to the victim memory cells. Leveraging such attacks, a new bit-flip-based adversarial weights attack (BFA) was developed targeting deep neural network models. When BFA attackers acquire a DNN model, they manipulate the existing DNN adversarial attack into locating vulnerable bits in the target DNN model. By flipping a subset of them using Rowhammer, they can crash that model within 30 trails. In this paper, we propose a lightweight and easy-to-deploy defense mechanism in the bit-level, Randomized Rotated and Nonlinear Encoding (RREC), which generates both robustness and fault-tolerant against BFA. Since flipping the most significant bit (MSB) in quantized data is too dangerous, we introduce randomized Rotation to obfuscate the bit order of model data and efficiently hide truly vulnerable bits with less vulnerable ones. Further, RREC reduces the average bit-flipped distance by more than 3x from the nonlinear encoding. It decreases the bit-flip distance among the majority of bits (including those vulnerable bits). Theoretically, RREC minimized the impact of a single bit BFA to 1/24 compared with baseline. Experimentally, RREC tolerates more than 17x flipped bits versus baseline model and 4.8x and 5.7x more bits compared with the existing BFA defenses (4B QAT and WR) with 0.01x to 0.08x of runtime latency. Moreover, we evaluate RREC against a newly emerged attack, Targeted-BFA, and it improves the defense rate from $5\%$ to $95\%$ .},
  archive      = {J_TC},
  author       = {Liang Liu and Yanan Guo and Yueqiang Cheng and Youtao Zhang and Jun Yang},
  doi          = {10.1109/TC.2022.3211411},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {401-413},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Generating robust DNN with resistance to bit-flip based adversarial weight attack},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for design, verification, and management of SoC
access control systems. <em>TC</em>, <em>72</em>(2), 386–400. (<a
href="https://doi.org/10.1109/TC.2022.3209923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {System-on-chip (SoC) architectures are a heterogeneous mix of microprocessors, custom accelerators, memories, interfaces, peripherals, and other resources. These resources communicate using complex on-chip interconnect networks that attempt to quickly and efficiently arbitrate memory transactions whose behaviors can vary drastically depending on the current mode of operation and system operating state. Security- and safety-critical applications require access control policies that define how these resources interact to ensure that malicious and unsafe behaviors do not occur. Aker is a design and verification framework for on-chip access control. The core of Aker is the access control wrapper (ACW)–a high-performance yet efficient hardware module that dynamically arbitrates on-chip communications. Aker distributes ACWs across the SoC and programs them to perform local access control. Aker provides a firmware generation tool and a property-driven security verification methodology to ensure that the ACWs are properly integrated and configured. Aker security verification confirms that the ACW behaves properly at IP level. It verifies the hardware root of trust firmware configures the ACW correctly. And it evaluates system-level security threats due to interactions between shared resources. Aker is experimentally validated on a Xilinx UltraScale+ programmable SoC. Additionally, an Aker access control system is integrated into the OpenPULP multicore archtiecture that uses OpenTitan hardware root-of-trust for firmware configuration.},
  archive      = {J_TC},
  author       = {Francesco Restuccia and Andres Meza and Ryan Kastner and Jason Oberg},
  doi          = {10.1109/TC.2022.3209923},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {386-400},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A framework for design, verification, and management of SoC access control systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preventing coherence state side channel leaks using
TimeCache. <em>TC</em>, <em>72</em>(2), 374–385. (<a
href="https://doi.org/10.1109/TC.2022.3209922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cache side channel attacks in the presence of shared memory have been used to extract cryptographic keys and enclave data, and are used by Spectre variants for leaking speculatively loaded data. Timing side channels exist in shared caches due to the difference in response latency of cached and uncached data. In prior work, we presented TimeCache, a cache design that prevents side channel exploits from reuse of shared memory. In this work, we extend TimeCache to also defend against attacks that exploit coherence states. TimeCache allows all running applications to use the entire cache, avoiding the need for partitioning in order to effect timing isolation. A per-process caching context prevents cache hits on data filled by another process. A novel bit-serial timestamp-parallel comparison logic allows low-overhead update of stale caching contexts. The defense is suited to all caches levels, and defends against an attacker running on any core. We evaluate TimeCache using the gem5 simulator to show that it is capable of preventing both reuse attacks and an attack based on coherence state leak. The average performance overhead for SPEC2006 is 1.13\%, and for PARSEC and SPLASH is 0.46\%.},
  archive      = {J_TC},
  author       = {Divya Ojha and Sandhya Dwarkadas},
  doi          = {10.1109/TC.2022.3209922},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {374-385},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Preventing coherence state side channel leaks using TimeCache},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reverse-engineering and exploiting the frontend bus of intel
processor. <em>TC</em>, <em>72</em>(2), 360–373. (<a
href="https://doi.org/10.1109/TC.2022.3222083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The frontend of modern Intel processors will decode instructions into $\mu$ ops and stream them to the backend by the frontend bus, which is shared between two logical cores to maximize utilization without sharing mechanism fully disclosed. Taking Haswell as an example, we reverse the bus from Decoded ICache to Instruction Decode Queue and the bus from Instruction Decode Queue to backend. We find that they are dynamically shared between two logical cores, which makes it possible for observable timing differences in one another through different instructions. Based on these differences, we propose the Synthetical bus covert channel for LSD-enabled architectures like Haswell and the DI bus covert channel for LSD-disabled architectures like Cometlake. We test our covert channels in physical machines and virtual machines. The bandwidth of Synthetical bus covert channel achieves 870 Kbps with 95.69\% accuracy in physical machines and 145 Kbps with 92.83\% accuracy in virtual machines. The bandwidth of DI bus covert channel reaches 1450 Kbps with 97.2\% accuracy in physical machines and 70.33 Kbps with 92.3\% accuracy in virtual machines. We further demonstrate a new Spectre variant. Finally, we propose two possible mitigations against our covert channels due to the limitations of existing protection strategies.},
  archive      = {J_TC},
  author       = {Ke Xu and Ming Tang and Han Wang and Sylvain Guilley},
  doi          = {10.1109/TC.2022.3222083},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {360-373},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reverse-engineering and exploiting the frontend bus of intel processor},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A provably secure strong PUF based on LWE: Construction and
implementation. <em>TC</em>, <em>72</em>(2), 346–359. (<a
href="https://doi.org/10.1109/TC.2022.3207119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct a strong physical unclonable function (PUF) with provable security against machine learning (ML) attacks on both classical and quantum computers. The security is guaranteed by the cryptographic hardness of learning decryption functions of public-key cryptosystems, and the hardness of the learning-with-errors (LWE) problem defined on integer lattices. We call our construction the lattice PUF. We construct lattice PUF with a physically obfuscated key and an LWE decryption function block. To allow deployments in different scenarios, we demonstrate designs with different latency-area trade-offs. A compact design uses a highly serialized linear-feedback shift register (LFSR) and LWE decryption function, while a latency-optimized design uses an unrolled LFSR and a parallel datapath. We prototype lattice PUF designs with $2^{136}$ challenge-response pairs (CRPs) on a spartan 6 field-programmable gate array (FPGA). In addition to theoretical security guarantee, we evaluate empirical resistance to the various leading ML techniques: the prediction error remains above $49.76\%$ after 1 million training CRPs. The resource-efficient design requires only 45 slices for PUF logic proper, and 27 slices for reverse fuzzy extractor. The latency-optimized design achieves a $148X$ reduction in latency, at a $10X$ increase in PUF hardware utilization. The mean uniformity of PUF responses is $49.98\%$ , the mean uniqueness is $50.00\%$ , and the mean reliability is $1.26\%$ .},
  archive      = {J_TC},
  author       = {Xiaodan Xi and Ge Li and Ye Wang and Michael Orshansky},
  doi          = {10.1109/TC.2022.3207119},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {346-359},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A provably secure strong PUF based on LWE: Construction and implementation},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). R-HTDetector: Robust hardware-trojan detection based on
adversarial training. <em>TC</em>, <em>72</em>(2), 333–345. (<a
href="https://doi.org/10.1109/TC.2022.3222090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware Trojans (HTs) have become a serious problem, and extermination of them is strongly required for enhancing the security and safety of integrated circuits. An effective solution is to identify HTs at the gate level via machine learning techniques. However, machine learning has specific vulnerabilities, such as adversarial examples . In reality, it has been reported that adversarial modified HTs greatly degrade the performance of a machine learning-based HT detection method. Therefore, we propose a robust HT detection method using adversarial training ( R-HTDetector ). We formally describe the robustness of R-HTDetector in modifying HTs. Our work gives the world-first adversarial training for HT detection with theoretical backgrounds. We show through experiments with Trust-HUB benchmarks that R-HTDetector overcomes adversarial examples while maintaining its original accuracy.},
  archive      = {J_TC},
  author       = {Kento Hasegawa and Seira Hidano and Kohei Nozawa and Shinsaku Kiyomoto and Nozomu Togawa},
  doi          = {10.1109/TC.2022.3222090},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {333-345},
  shortjournal = {IEEE Trans. Comput.},
  title        = {R-HTDetector: Robust hardware-trojan detection based on adversarial training},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting higher-order masked comparison for lattice-based
cryptography: Algorithms and bit-sliced implementations. <em>TC</em>,
<em>72</em>(2), 321–332. (<a
href="https://doi.org/10.1109/TC.2022.3197074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Masked comparison is one of the most expensive operations in side-channel secure implementations of lattice-based post-quantum cryptography, especially for higher masking orders. First, we introduce two new masked comparison algorithms, which improve the arithmetic comparison of D’Anvers et al. (2021) and the hybrid comparison method of Coron et al. (2021) respectively. We then look into implementation-specific optimizations, and show that small specific adaptations can have a significant impact on the overall performance. Finally, we implement various state-of-the-art comparison algorithms and benchmark them on the same platform (ARM-Cortex M4) to allow a fair comparison between them. We improve on the arithmetic comparison of D’Anvers et al. with a factor $\approx 20\%$ by using Galois Field multiplications and the hybrid comparison of Coron et al. with a factor $\approx 25\%$ by streamlining the design. Our implementation-specific improvements allow a speedup of a straightforward comparison implementation of $\approx 33\%$ . We discuss the differences between the various algorithms and provide the implementations and a testing framework to ease future research.},
  archive      = {J_TC},
  author       = {Jan-Pieter D’Anvers and Michiel Van Beirendonck and Ingrid Verbauwhede},
  doi          = {10.1109/TC.2022.3197074},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {321-332},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Revisiting higher-order masked comparison for lattice-based cryptography: Algorithms and bit-sliced implementations},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-speed hardware architectures and FPGA benchmarking of
CRYSTALS-kyber, NTRU, and saber. <em>TC</em>, <em>72</em>(2), 306–320.
(<a href="https://doi.org/10.1109/TC.2022.3222954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-Quantum Cryptography (PQC) has emerged as a response of the cryptographic community to the danger of attacks performed using quantum computers. All PQC schemes can be implemented in software and hardware using conventional (non-quantum) computing systems. PQC is the biggest revolution in cryptography since the invention of public-key schemes in the mid-1970 s. Lattice-based key exchange schemes have emerged as leading candidates in the NIST PQC standardization process due to their relatively short public keys and ciphertexts. This paper presents novel high-speed hardware architectures for four lattice-based Key Encapsulation Mechanisms (KEMs) representing three NIST PQC finalists: NTRU (with two distinct variants, NTRU-HPS and NTRU-HRSS), CRYSTALS-Kyber, and Saber. We benchmark these candidates in terms of their performance and resource utilization in today&#39;s FPGAs. Our best architectures outperform the best designs from other groups reported to date in terms of the area-time product by factors ranging from 1.01 to 2.88, depending on the algorithm and security level. Additionally, our study demonstrates that CRYSTALS-Kyber and Saber have very similar hardware performance. Both outperform NTRU in terms of execution time by a factor 36-62 for key generation and 3-7 for decapsulation, assuming the same security level.},
  archive      = {J_TC},
  author       = {Viet Ba Dang and Kamyar Mohajerani and Kris Gaj},
  doi          = {10.1109/TC.2022.3222954},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {306-320},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-speed hardware architectures and FPGA benchmarking of CRYSTALS-kyber, NTRU, and saber},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: IEEE transactions on computer, special
issue on hardware security. <em>TC</em>, <em>72</em>(2), 305. (<a
href="https://doi.org/10.1109/TC.2022.3233145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The special issue includes 9 papers on various facets of hardware security, from foundational studies to industrial perspectives.},
  archive      = {J_TC},
  doi          = {10.1109/TC.2022.3233145},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {305},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Guest editorial: IEEE transactions on computer, special issue on hardware security},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graphfire: Synergizing fetch, insertion, and replacement
policies for graph analytics. <em>TC</em>, <em>72</em>(1), 291–304. (<a
href="https://doi.org/10.1109/TC.2022.3157525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their ubiquity in many important big-data applications, graph analytic kernels continue to challenge modern memory hierarchies due to their frequent, long-latency, pointer indirect accesses to vertex property data. Such accesses exhibit poor locality and variable reuse that trouble cache replacement policies, and consequently increase memory bandwidth pressure. Specialized graph-tailored prefetching mechanisms, processor designs, and memory hierarchy engines have been developed to tolerate the long latencies of such accesses. However, these approaches are either too bandwidth-intensive, require invasive hardware changes that inhibit general-purpose computation flexibility, or rely on software preprocessing that limits true speedup. This work introduces Graphfire, a flexible memory hierarchy approach that learns different access patterns in graph processing and exploits the synergy of specialized fetch, insertion, and replacement optimizations for problematic indirect accesses without relying on software or ISA support. More specifically, Graphfire identifies when these irregular accesses occur and employs tailored access granularities, data-aware insertion, and frequency-based replacement accordingly. It achieves up to a 1.79× speedup (geomean 1.3×) and these improvements scale due to bandwidth efficiency; with 64 cores, Graphfire yields up to a 71.33× speedup (geomean 63.32×) over a single baseline core and allows memory-bound graph analytic codes to scale far beyond prior work.},
  archive      = {J_TC},
  author       = {Aninda Manocha and Juan L. Aragón and Margaret Martonosi},
  doi          = {10.1109/TC.2022.3157525},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {291-304},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Graphfire: Synergizing fetch, insertion, and replacement policies for graph analytics},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating large-scale graph-based nearest neighbor search
on a computational storage platform. <em>TC</em>, <em>72</em>(1),
278–290. (<a href="https://doi.org/10.1109/TC.2022.3155956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {$K$ -nearest neighbor search is one of the fundamental tasks in various applications and the hierarchical navigable small world (HNSW) has recently drawn attention in large-scale cloud services, as it easily scales up the database while offering fast search. On the other hand, a computational storage device (CSD) that combines programmable logic and storage modules on a single board becomes popular to address the data bandwidth bottleneck of modern computing systems. In this paper, we propose a computational storage platform that can accelerate a large-scale graph-based nearest neighbor search algorithm based on SmartSSD CSD. To this end, we modify the algorithm more amenable on the hardware and implement two types of accelerators using HLS- and RTL-based methodology with various optimization methods. In addition, we scale up the proposed platform to have 4 SmartSSDs and apply graph parallelism to boost the system performance further. As a result, the proposed computational storage platform achieves 75.59 query per second throughput for the SIFT1B dataset at 258.66W power dissipation, which is 12.83x and 17.91x faster and 10.43x and 24.33x more energy efficient than the conventional CPU-based and GPU-based server platform, respectively. With multi-terabyte storage and custom acceleration capability, we believe that the proposed computational storage platform is a promising solution for cost-sensitive cloud datacenters.},
  archive      = {J_TC},
  author       = {Ji-Hoon Kim and Yeo-Reum Park and Jaeyoung Do and Soo-Young Ji and Joo-Young Kim},
  doi          = {10.1109/TC.2022.3155956},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {278-290},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating large-scale graph-based nearest neighbor search on a computational storage platform},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IVP: An intelligent video processing architecture for video
streaming. <em>TC</em>, <em>72</em>(1), 264–277. (<a
href="https://doi.org/10.1109/TC.2022.3155950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, video processing tasks, such as video enhancement and analysis, have received increasing attention from both academics and industries. However, the current video processing procedure on edge decouples the decoding phase and the subsequent video processing tasks, missing the opportunity to accelerate the procedure by orchestrating video decoding and enhancement stages. Thus, we propose an intelligent video processing workflow and architecture(IVP) for cloud-edge video streaming. For edge devices that receive compressed videos, IVP can perform direct DNN-based video enhancement, e.g., super-resolution and frame-interpolation. By leveraging the metadata motion vectors and residuals extracted from the encoded video, our architecture will significantly eliminate unnecessary frame pixels being processed by the DNNs and improve execution efficiency. The proposed IVP and workflow are proved to reduce up to 90\% of the processing latency while producing accurate and high-quality videos. Furthermore, we observe a significant portion of similar optical flow in time domain of continuous videos, which can be used to reduce the computation overhead. Thus, to utilize such temporal similarity of optical flow, the proposed IVP is upgraded to be capable of reusing previous computation results, which further improves energy efficiency of the whole system.},
  archive      = {J_TC},
  author       = {Chengsi Gao and Ying Wang and Yinhe Han and Weiwei Chen and Lei Zhang},
  doi          = {10.1109/TC.2022.3155950},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {264-277},
  shortjournal = {IEEE Trans. Comput.},
  title        = {IVP: An intelligent video processing architecture for video streaming},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FLIXR: Embedding index into flash translation layer in SSDs.
<em>TC</em>, <em>72</em>(1), 250–263. (<a
href="https://doi.org/10.1109/TC.2022.3154602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flash memory technologies rely on flash translation layer (FTL) to manage no in-place update and garbage collection. Current FTL management schemes do not exploit the semantics of the accessed data. In this paper, we explore how semantic knowledge can be exploited to build and maintain indexes for stored data automatically. Data indexing is a critical enabler to accelerate many database applications and big data analytics. Unlike traditional per-table or per-file indexes that are managed separately from the data, we propose to maintain indexes on a per-flash page basis. Our approach, called FLash IndeXeR (FLIXR), builds and maintains page-level indexes whenever a page is written into the flash. FLIXR updates the indexes alongside any data updates at page granularity. The cost of the index update is hidden in the page write delays. FLIXR stores index data for each page within the FTL entry associated with that page, thereby piggybacking index access on a page access request. FLIXR accesses the index data in each FTL entry to determine whether the associated page stores data with a given key. FLIXR achieves 52.6\% performance improvement for TPC-C and TPC-H benchmarks, compared to the conventional host-side indexing mechanism.},
  archive      = {J_TC},
  author       = {Gunjae Koo and Yunho Oh and Hung-Wei Tseng and Won Woo Ro and Murali Annavaram},
  doi          = {10.1109/TC.2022.3154602},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {250-263},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FLIXR: Embedding index into flash translation layer in SSDs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An iterative montgomery modular multiplication algorithm
with low area-time product. <em>TC</em>, <em>72</em>(1), 236–249. (<a
href="https://doi.org/10.1109/TC.2022.3154164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a highly efficient iterative Montgomery modular multiplication algorithm, wherein the computations of quotient and intermediate result in each iteration are done in parallel. This parallelism breaks the data dependency and thus reduces the computation latency. Moreover, this paper replaces required multiplications and additions in each iteration with compressions and encoding, thereby achieving a computation latency of order $d+6$ where $d=\left\lceil N/m \right\rceil +2$ is the number of iterations, $N$ denotes the bitwidth of modulus $M$ , and $m$ is the number of bits of the multiplier that are processed in each iteration of the algorithm. Hardware realization of the proposed Montgomery modular multiplication on a Xilinx Virtex-7 FPGA device shows $&amp;gt; 41\%$ computation latency saving and $&amp;gt;31\%$ area saving when $N=1,024$ and $m=8$ , compared with the best of previous state-of-art references. These savings amount to more than 63\% reduction in terms of the area-latency product metric.},
  archive      = {J_TC},
  author       = {Bo Zhang and Zeming Cheng and Massoud Pedram},
  doi          = {10.1109/TC.2022.3154164},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {236-249},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An iterative montgomery modular multiplication algorithm with low area-time product},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An exhaustive approach to detecting transient execution side
channels in RTL designs of processors. <em>TC</em>, <em>72</em>(1),
222–235. (<a href="https://doi.org/10.1109/TC.2022.3152666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware (HW) security issues have been emerging at an alarming rate in recent years. Transient execution attacks, such as Spectre and Meltdown, in particular, pose a genuine threat to the security of modern computing systems. Despite recent advances, understanding the intricate implications of microarchitectural design decisions on processor security remains a great challenge and has caused a number of update cycles in the past. This papers addresses the need for a new approach to HW sign-off verification which guarantees the security of processors at the Register Transfer Level (RTL). To this end, we introduce a formal definition of security with respect to transient execution attacks, formulated as a HW property. We present a formal proof methodology based on Unique Program Execution Checking (UPEC) which can be used to systematically detect all vulnerabilities to transient execution attacks in RTL designs. UPEC does not exploit any a priori knowledge on known attacks and can therefore detect also vulnerabilities based on new, so far unknown, types of channels. This is demonstrated by two new attack scenarios discovered in our experiments with UPEC. UPEC scales to a wide range of HW designs, including in-order processors (RocketChip), pipelines with out-of-order writeback (Ariane), and processors with deep out-of-order speculative execution (BOOM). To the best of our knowledge, UPEC is the first RTL verification technique that exhaustively covers transient execution side channels in processors of realistic complexity.},
  archive      = {J_TC},
  author       = {Mohammad Rahmani Fadiheh and Alex Wezel and Johannes Müller and Jörg Bormann and Sayak Ray and Jason M. Fung and Subhasish Mitra and Dominik Stoffel and Wolfgang Kunz},
  doi          = {10.1109/TC.2022.3152666},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {222-235},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An exhaustive approach to detecting transient execution side channels in RTL designs of processors},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EaD: ECC-assisted deduplication with high performance and
low memory overhead for ultra-low latency flash storage. <em>TC</em>,
<em>72</em>(1), 208–221. (<a
href="https://doi.org/10.1109/TC.2022.3152665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication has become a commodity feature in flash storage products to effectively reduce redundant write data and improve space efficiency. However, it also introduces computing and memory overhead to generate and store the cryptographic hash (fingerprint) in face of the moderate data redundancy in primary storage. With the advent of 3D XPoint and Z-NAND technologies, and the stronger cryptographic hash functions in use, such as SHA-256, both the computing and memory overheads are increasingly serious performance bottlenecks for inline data deduplication in these ultra-low latency flash storage. To address these problems, we propose an ECC-assisted Deduplication approach, called EaD, which exploits the ECC property and the asymmetric read-write performance characteristics of modern flash storage. EaD first identifies data similarity by leveraging the device-generated ECC values of data chunks as their fingerprints, significantly reducing the costly MD5/SHA-based cryptographic hash computing and alleviating the memory space overhead. Based on the identification results, similar data chunks and their ECCs are read from the flash to perform a byte-by-byte comparison in memory to definitively identify and remove redundant data chunks. Our experiments show that the EaD approach significantly increases I/O performance by up to 4.2 ${\times }$ , with an average of 2.5 ${\times }$ , compared with the existing MD5/SHA- and sampling-based deduplication approaches.},
  archive      = {J_TC},
  author       = {Suzhen Wu and Chunfeng Du and Weidong Zhu and Jindong Zhou and Hong Jiang and Bo Mao and Lingfang Zeng},
  doi          = {10.1109/TC.2022.3152665},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {208-221},
  shortjournal = {IEEE Trans. Comput.},
  title        = {EaD: ECC-assisted deduplication with high performance and low memory overhead for ultra-low latency flash storage},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Operating system noise in the linux kernel. <em>TC</em>,
<em>72</em>(1), 196–207. (<a
href="https://doi.org/10.1109/TC.2022.3187351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As modern network infrastructure moves from hardware-based to software-based using Network Function Virtualization, a new set of requirements is raised for operating system developers. By using the real-time kernel options and advanced CPU isolation features common to the HPC use-cases, Linux is becoming a central building block for this new architecture that aims to enable a new set of low latency networked services. Tuning Linux for these applications is not an easy task, as it requires a deep understanding of the Linux execution model and the mix of user-space tooling and tracing features. This paper discusses the internal aspects of Linux that influence the Operating System Noise from a timing perspective. It also presents Linux&#39;s osnoise tracer, an in-kernel tracer that enables the measurement of the Operating System Noise as observed by a workload, and the tracing of the sources of the noise, in an integrated manner, facilitating the analysis and debugging of the system. Finally, this paper presents a series of experiments demonstrating both Linux&#39;s ability to deliver low OS noise (in the single-digit $\mu$ s order), and the ability of the proposed tool to provide precise information about root-cause of timing-related OS noise problems.},
  archive      = {J_TC},
  author       = {Daniel Bristot de Oliveira and Daniel Casini and Tommaso Cucinotta},
  doi          = {10.1109/TC.2022.3187351},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {196-207},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Operating system noise in the linux kernel},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MINOTAuR: A timing predictable RISC-v core featuring
speculative execution. <em>TC</em>, <em>72</em>(1), 183–195. (<a
href="https://doi.org/10.1109/TC.2022.3200000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MINOTAuR, an open-source RISC-V core designed to be timing predictable, i.e., free of timing anomalies: this property enables a compositional timing analysis in a multicore context. MINOTAuR features speculative execution: thanks to a specific design of its pipeline, we formally prove that speculation does not break timing predictability while sensibly increasing performance. We propose architectural extensions that enable the use of a return address stack and of any cache replacement policy, which we implemented in the MINOTAuR core. We show that a trade-off can be found between the efficiency of these components and the overhead they incur on the die area consumption, and that using them yields a performance equivalent to that of the baseline RISC-V Ariane core, while also enforcing timing predictability.},
  archive      = {J_TC},
  author       = {Alban Gruin and Thomas Carle and Christine Rochange and Hugues Cassé and Pascal Sainrat},
  doi          = {10.1109/TC.2022.3200000},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {183-195},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MINOTAuR: A timing predictable RISC-V core featuring speculative execution},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing arm’s MPAM from the perspective of time
predictability. <em>TC</em>, <em>72</em>(1), 168–182. (<a
href="https://doi.org/10.1109/TC.2022.3202720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With heterogeneous multi-core platforms being crucial to execute the highly demanding workloads of modern applications, memory-access predictability remains a key issue for the system&#39;s safety. Many solutions have been proposed over the years, but none has been applied on a large scale. Nowadays, we are in front of an unprecedented opportunity to have an impact on commercial platforms: the Memory System Resource Partitioning and Monitoring (MPAM) specification by Arm, which describes different memory-access regulation mechanisms, presenting a valuable industrial attempt to address this issue. However, several points of the specification are described at a high level only, leaving plenty of room for interpretation to hardware manufacturers. This paper takes a close look at the memory-access regulation mechanisms in the MPAM specification and provides some detailed instantiations of such mechanisms. A fine-grained memory contention analysis is presented for each of them to finally enable a comparison of their worst-case performance.},
  archive      = {J_TC},
  author       = {Matteo Zini and Daniel Casini and Alessandro Biondi},
  doi          = {10.1109/TC.2022.3202720},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {168-182},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Analyzing arm&#39;s MPAM from the perspective of time predictability},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bounding memory access times in multi-accelerator
architectures on FPGA SoCs. <em>TC</em>, <em>72</em>(1), 154–167. (<a
href="https://doi.org/10.1109/TC.2022.3214117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern FPGA System-on-Chips (SoCs) embed large FPGA logics capable of hosting multiple hardware accelerators. Typically, hardware accelerators require direct access to the shared DRAM memory for reaching the high performance demanded by modern applications. In commercial FPGA SoCs, this goal is achieved by interconnecting the hardware accelerators on an interconnect based on AMBA AXI, which is the de-facto industrial standard for on-chip communications. The AXI standard provides great flexibility in the definition of the network topology. Nevertheless, such flexibility generates a significant unpredictability when attempting to bound the hardware accelerators’ response time when executing under contention. This work focus on bounding the worst-case memory access time of hardware accelerators deployed on commercial FPGA SoCs. We propose a modeling and analysis technique to bound the response time of the hardware accelerators and evaluate the schedulability of a system applicable to arbitrary AXI-based bus structures deployed on FPGA SoCs. Our results are validated on real execution traces collected on two popular FPGA SoCs belonging to the Xilinx ZYNQ-7000 and Zynq-Ultrascale+ families and by simulated results.},
  archive      = {J_TC},
  author       = {Francesco Restuccia and Marco Pagani and Alessandro Biondi and Mauro Marinoni and Giorgio Buttazzo},
  doi          = {10.1109/TC.2022.3214117},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {154-167},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Bounding memory access times in multi-accelerator architectures on FPGA SoCs},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fenglin-i: An open-source time-sensitive networking chip
enabling agile customization. <em>TC</em>, <em>72</em>(1), 140–153. (<a
href="https://doi.org/10.1109/TC.2022.3188179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-Sensitive Networking (TSN) technology is experiencing diverse application requirements and forming a complicated standard system. It is extremely difficult to design a one-fits-all chip for all TSN applications. Therefore, application-driven TSN chip customization is inevitable. Generally, chip customization starts from a “clean-slate”. For complicated ASIC chips, that results in significant development overhead. Inspired by RISC-V chips, an open-source template will significantly reduce the customization complexity. Along this road, we propose an open-source TSN chip named Fenglin-I. Fenglin-I includes a high-level abstraction to build a relationship between application requirements and chip implementation, source code of a real chip named FastTSN to provide reference code for chip implementation, and software tools to facilitate chip verification. Based on Fenglin-I, we further propose a TSN chip customization method that provides step-by-step guidance about customizing TSN chips agilely. To verify the effectiveness of Fenglin-I and the proposed customization method, we use FPGA arrays to prototype and verify FastTSN. The results show that FastTSN achieves microsecond-level transmission jitter for unicast and multicast time-critical traffic. Additionally, we demonstrate two domain-specific TSN chip customization cases in which the customized chips reuse at least 84 $\%$ of FastTSN code while meeting their requirements.},
  archive      = {J_TC},
  author       = {Wenwen Fu and Wei Quan and Jinli Yan and Zhigang Sun},
  doi          = {10.1109/TC.2022.3188179},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {140-153},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fenglin-I: An open-source time-sensitive networking chip enabling agile customization},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing inter-core communications under the LET paradigm
using DMA engines. <em>TC</em>, <em>72</em>(1), 127–139. (<a
href="https://doi.org/10.1109/TC.2022.3191739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern automotive applications are increasingly characterized by the need to transfer massive amounts of data in a predictable and deterministic way, possibly leveraging the Logical Execution Time (LET) paradigm. However, current proposals for LET communications are limited to core-commanded data transfers, which may result in large delays for data-intensive systems. To address this issue, we explore the use of Direct Memory Access (DMA) to handle LET communication with improved parallelism. Each DMA transfer operates on a contiguous memory area, thus calling for an optimized memory mapping to maximize performance. Modern DMA engines offer also advanced configurations, such as linked-lists of data transfers, which may provide more flexibility at the expenses of an increased (initial) programming overhead. Leveraging all such features of DMA engines, we propose a set of designs and protocols for LET communications with trade-offs between latency and space requirements. For each option we present the formulation to compute the optimal scheduling and memory allocation solution as a mixed-integer linear programming problem. Experimental results show the feasibility of the approach and a comparison of the solutions obtained using the proposed methods, showing a considerable improvement in terms of data acquisition latency when compared to LET communication without DMA.},
  archive      = {J_TC},
  author       = {Paolo Pazzaglia and Daniel Casini and Alessandro Biondi and Marco Di Natale},
  doi          = {10.1109/TC.2022.3191739},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {127-139},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing inter-core communications under the LET paradigm using DMA engines},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards hard real-time and energy-efficient virtualization
for many-core embedded systems. <em>TC</em>, <em>72</em>(1), 111–126.
(<a href="https://doi.org/10.1109/TC.2022.3207115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In safety-critical computing systems, the I/O virtualization must simultaneously satisfy different requirements, including time-predictability, performance, and energy-efficiency. However, these requirements are challenging to achieve due to complex I/O access path and resource management at the system level, lack of support from preemptive scheduling at I/O hardware level, and missing an effective energy management method. In this paper, we propose a new framework, I/O-GUARD, which reconstructs the system architecture of I/O virtualization, bringing a dedicated hardware hypervisor to handle resource management throughout the system. The hypervisor improves system real-time performance by enabling preemptive scheduling in I/O virtualization with both analytical and experimental real-time guarantees. Furthermore, we also present a dedicated energy management unit to adjust I/O-GUARD &#39;s dynamic energy using frequency scaling. Associated with that, a frequency identification algorithm is proposed to find the appropriate executing frequency at run-time. As shown in experiments, I/O-GUARD simultaneously improves the predictability, performance and energy-efficiency compared to the state-of-the-art I/O virtualization.},
  archive      = {J_TC},
  author       = {Zhe Jiang and Kecheng Yang and Yunfeng Ma and Nathan Fisher and Neil Audsley and Zheng Dong},
  doi          = {10.1109/TC.2022.3207115},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {111-126},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards hard real-time and energy-efficient virtualization for many-core embedded systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network calculus with flow prolongation – a feedforward FIFO
analysis enabled by ML. <em>TC</em>, <em>72</em>(1), 97–110. (<a
href="https://doi.org/10.1109/TC.2022.3204225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The derivation of upper bounds on data flows’ worst-case traversal times is an important task in many application areas. For accurate bounds, model simplifications should be avoided even in large networks. Network Calculus (NC) provides a modeling framework and different analyses for delay bounding. We investigate the analysis of feedforward networks where all queues implement First-In First-Out (FIFO) service. Correctly considering the effect of data flows onto each other under FIFO is already a challenging task. Yet, the fastest available NC FIFO analysis (called LUDB) suffers from limitations resulting in unnecessarily loose bounds. A feature called Flow Prolongation (FP) has been shown to improve delay bound accuracy significantly. Unfortunately, FP needs to be executed within this NC FIFO analysis very often and each time it creates an exponentially growing set of alternative networks with prolongations. FP therefore does not scale and has been out of reach for the exhaustive analysis of large networks. We introduce DeepFP, an approach to make FP scale by predicting prolongations using machine learning. In our evaluation, we show that DeepFP can improve results in FIFO networks considerably. Compared to the aforementioned LUDB analysis, DeepFP reduces delay bounds by $12.1 \;\%$ on average at negligible additional computational cost.},
  archive      = {J_TC},
  author       = {Fabien Geyer and Alexander Scheffler and Steffen Bondorf},
  doi          = {10.1109/TC.2022.3204225},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {97-110},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Network calculus with flow prolongation – a feedforward FIFO analysis enabled by ML},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparing communication paradigms in cause-effect chains.
<em>TC</em>, <em>72</em>(1), 82–96. (<a
href="https://doi.org/10.1109/TC.2022.3197082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cause-effect chain is a sequence of multi-rate real-time tasks with data dependency. Cause-effect chains are generally subject to end-to-end timing constraints, especially in safety-critical systems. Communication paradigms greatly affect the end-to-end latency of cause-effect chains. This paper compares different communication paradigms (implicit communication, LET, DBP) with regards to the end-to-end latency of cause-effect chains using them, and proposes priority assignment strategies to optimize the end-to-end latency with specific communication paradigm. Experiments with synthesized data based on an automotive benchmark and randomly generated parameters are conducted to evaluate our results.},
  archive      = {J_TC},
  author       = {Yue Tang and Xu Jiang and Nan Guan and Dong Ji and Xiantong Luo and Wang Yi},
  doi          = {10.1109/TC.2022.3197082},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {82-96},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Comparing communication paradigms in cause-effect chains},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-resilient real-time scheduling. <em>TC</em>,
<em>72</em>(1), 69–81. (<a
href="https://doi.org/10.1109/TC.2022.3202754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedded nodes in future cyber-physical systems are mostly self-powered, scavenging their required energy from the environment. The environmental sources of energy are usually variable, so that some prediction methods are employed to proactively adapt to the variable harvesting energy. However, prediction errors may surprise the system with some unpredicted changes, needing appropriate reactions. We consider an energy-harvesting real-time system with periodic tasks of multiple performance levels. An energy-resilient scheduler is proposed for the system to react to the unpredicted changes such that the system is survivable, recovers from such a change in a timely manner, and appropriately controls its performance degradation. After the recovery, however, the energy-resilient scheduler preserves the system survivability and maximizes its performance in a prediction time horizon, while it will be ready for another surprise. We provide some theoretical properties and a feasibility test which are used in the design of the energy-resilient scheduler. Our simulations show that the proposed resilient scheduler outperforms well-known performance maximization methods, effectively approximates the optimal solution, and reacts appropriately against surprises of high severity.},
  archive      = {J_TC},
  author       = {Mahmoud Shirazi and Lothar Thiele and Mehdi Kargahi},
  doi          = {10.1109/TC.2022.3202754},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {69-81},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy-resilient real-time scheduling},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic scheduling strategies for firm semi-periodic
real-time tasks. <em>TC</em>, <em>72</em>(1), 55–68. (<a
href="https://doi.org/10.1109/TC.2022.3208203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces and assesses novel strategies to schedule firm semi-periodic real-time tasks. Jobs are released periodically and have the same relative deadline. Job execution times obey an arbitrary probability distribution and can take either bounded or unbounded values. We investigate several optimization criteria, the most prominent being the Deadline Miss Ratio ( DMR ). All previous work uses some admission policies but never interrupt the execution of an admitted job before its deadline. On the contrary, we introduce three new control parameters to dynamically decide whether to interrupt a job at any given time. We derive a Markov model and use its stationary distribution to determine the best value of each control parameter. Finally we conduct an extensive simulation campaign with 16 different probability distributions. The results nicely demonstrate how the new strategies help improve system performance compared with traditional approaches. In particular, we show that (i) compared to pre-execution admission rules, the control parameters make significantly better decisions; (ii) specifically, the key control parameter is to upper bound the waiting time of each job; (iii) the best scheduling strategy decreases the DMR by up to 0.35 over traditional competitors.},
  archive      = {J_TC},
  author       = {Yiqin Gao and Guillaume Pallez and Yves Robert and Frédéric Vivien},
  doi          = {10.1109/TC.2022.3208203},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {55-68},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dynamic scheduling strategies for firm semi-periodic real-time tasks},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Precise mixed-criticality scheduling on varying-speed
multiprocessors. <em>TC</em>, <em>72</em>(1), 43–54. (<a
href="https://doi.org/10.1109/TC.2022.3197078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While traditional real-time systems analysis requires single pessimistic estimates to represent system parameters, the mixed-criticality (MC) design proposes to use multiple estimates of system parameters with different levels of pessimism, resulting in low critical workloads sacrificed at run-time in order to provide guarantees to high critical workloads. Shortcomings of the MC design were improved recently by the precise MC scheduling technique in which the processor speed is increased at run-time to provide guarantees to both low and high critical workloads. Aiming to extend the precise MC scheduling to multiprocessor computing platforms, this paper proposes three novel scheduling algorithms that are based on virtual-deadline and fluid-scheduling approaches. We prove the correctness of our proposed algorithms through schedulability analysis and also present their theoretical effectiveness via speedup bounds and approximation factor calculations. Finally, we evaluate their performance experimentally via randomly generated task sets and demonstrate that the fluid-scheduling algorithms outperform the virtual-deadline algorithm.},
  archive      = {J_TC},
  author       = {Sudharsan Vaidhun and Tianning She and Qijun Gu and Sajal K. Das and Kecheng Yang and Zhishan Guo},
  doi          = {10.1109/TC.2022.3197078},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {43-54},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Precise mixed-criticality scheduling on varying-speed multiprocessors},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A high-resilience imprecise computing architecture for
mixed-criticality systems. <em>TC</em>, <em>72</em>(1), 29–42. (<a
href="https://doi.org/10.1109/TC.2022.3202721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional mixed-criticality systems (MCS)s are designed to terminate the execution of less critical tasks in exceptional situations so that the timing properties of more critical tasks can be preserved. Such a strategy can be controversial and has proven difficult to implement in practice, as it can lead to hazards and reduced functionality due to the absence of the discarded tasks. To mitigate this issue, the imprecise mixed-critically system model (IMCS) has been proposed. In such a model, instead of completely dropping less-critical tasks, these tasks are executed as much as possible through the use of decreased computation precision. Although IMCS could effectively improve the survivability of the less-critical tasks, it also introduces three key drawbacks - run-time computation errors, real-time performance degradation, and lack of flexibility. In this paper, we present a novel IMCS framework, which can (i) mitigate the computation errors caused by imprecise computation; (ii) achieve real-time performance near to that of a conventional MCS; (iii) enhance system-level throughput; and (iv) provide flexibility for run-time configuration. We describe the design details of HIART -MCS, and then present the corresponding theoretical analysis and optimisation method for its run-time configuration. Finally, HIART -MCS is evaluated against other MCS frameworks using a variety of experimental metrics.},
  archive      = {J_TC},
  author       = {Zhe Jiang and Xiaotian Dai and Alan Burns and Neil Audsley and Zonghua Gu and Ian Gray},
  doi          = {10.1109/TC.2022.3202721},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {29-42},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A high-resilience imprecise computing architecture for mixed-criticality systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified blocking analysis for parallel tasks with spin
locks under global fixed priority scheduling. <em>TC</em>,
<em>72</em>(1), 15–28. (<a
href="https://doi.org/10.1109/TC.2022.3198634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spin locks are widely used in embedded systems to coordinate mutually exclusive accesses to shared resources from different tasks. Although the design and analysis of locking protocols have been intensively studied for sequential real-time tasks, there have been few works on this topic for parallel real-time tasks. In this paper, we study the analysis of parallel real-time tasks modeled by directed acyclic graphs (DAGs) under global fixed priority scheduling using both preemptable and non-preemptable spin locks to protect accesses to shared resources in three commonly used request serving orders (unordered, FIFO-order and priority-order). In particular, we develop a general schedulability analysis framework where the blocking time caused by resource contention is formally defined, so that the blocking analysis can be performed independently and easy to combine with the traditional interference analysis techniques. Moreover, we present a unified blocking analysis technique where the blocking time is analyzed in a scalable manner based on a linear-programming (LP) approach, making our method flexible and extendable. We conduct comprehensive experiments to evaluate our method with other the-state-of-the-art approaches for scheduling real-time parallel tasks using semaphores and spin locks.},
  archive      = {J_TC},
  author       = {Xu Jiang and Zewei Chen and Maolin Yang and Nan Guan and Yue Tang and Yi Wang},
  doi          = {10.1109/TC.2022.3198634},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {15-28},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A unified blocking analysis for parallel tasks with spin locks under global fixed priority scheduling},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Response time stochastic analysis for fixed-priority stable
real-time systems. <em>TC</em>, <em>72</em>(1), 3–14. (<a
href="https://doi.org/10.1109/TC.2022.3211421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we prove that a mean system utilization smaller than one is a necessary condition for the feasibility of real-time systems. Such systems are defined as stable . Stable systems have two distinct states: a transient state, followed by a steady-state where the same distribution of response times is repeated infinitely for each task. We prove that the Liu and Layland theorem holds for stable probabilistic real-time systems with implicit deadlines, we provide an analytical approximation of response times for each of those two states and a bound of the instant when a real-time system becomes steady.},
  archive      = {J_TC},
  author       = {Kevin Zagalo and Yasmina Abdeddaïm and Avner Bar-Hen and Liliana Cucu-Grosjean},
  doi          = {10.1109/TC.2022.3211421},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {3-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Response time stochastic analysis for fixed-priority stable real-time systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IEEE TC special issue on real-time systems. <em>TC</em>,
<em>72</em>(1), 1–2. (<a
href="https://doi.org/10.1109/TC.2022.3227228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fifteen papers in this special section focus on real-time systems. They present state-of-the-art work in theory, design, analysis, implementation, and evaluation of real-time systems. All the papers address some form of real-time requirements such as deadlines, response times or delays/latency and consider not only hard real-time systems but also time-sensitive systems in general. Following an open call for papers, authors from all over the globe sent 53 submissions on a broad range of topics. The review committee of top experts worldwide conducted rigorous professional reviews. Each paper at least 3 reviews in the first round. Approximately 50 reviews were performed in the second round to evaluate the revised submissions.},
  archive      = {J_TC},
  author       = {Enrico Bini and Tam Chantem and Bruce Childers and Daniel Mosse},
  doi          = {10.1109/TC.2022.3227228},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {1-2},
  shortjournal = {IEEE Trans. Comput.},
  title        = {IEEE TC special issue on real-time systems},
  volume       = {72},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
