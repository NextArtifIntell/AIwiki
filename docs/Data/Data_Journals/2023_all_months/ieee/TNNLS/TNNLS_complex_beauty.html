<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TNNLS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tnnls---878">TNNLS - 878</h2>
<ul>
<li><details>
<summary>
(2023). Reachable set estimation for memristive complex-valued
neural networks with disturbances. <em>TNNLS</em>, <em>34</em>(12),
11029–11034. (<a
href="https://doi.org/10.1109/TNNLS.2022.3167117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief focuses on reachable set estimation for memristive complex-valued neural networks (MCVNNs) with disturbances. Based on algebraic calculation and Gronwall–Bellman inequality, the states of MCVNNs with bounded input disturbances converge within a sphere. From this, the convergence speed is also obtained. In addition, an observer for MCVNNs is designed. Two illustrative simulations are also given to show the effectiveness of the obtained conclusions.},
  archive      = {J_TNNLS},
  author       = {Song Zhu and Yu Gao and Yuxin Hou and Chunyu Yang},
  doi          = {10.1109/TNNLS.2022.3167117},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {11029-11034},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reachable set estimation for memristive complex-valued neural networks with disturbances},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frame-level teacher–student learning with data privacy for
EEG emotion recognition. <em>TNNLS</em>, <em>34</em>(12), 11021–11028.
(<a href="https://doi.org/10.1109/TNNLS.2022.3168935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, electroencephalogram (EEG) emotion recognition has gradually attracted a lot of attention. This brief designs a novel frame-level teacher–student framework with data privacy (FLTSDP) for EEG emotion recognition. The framework first proposes a teacher–student network without prior professional information for automated filtering of useful frame-level features by a gated mechanism and extracting high-level features by using knowledge distillation to capture the results of EEG emotion recognition from a teacher network and student networks. Then, the results from subnetworks are integrated by using the novel decision module, which, motivated by the voting mechanism, adjusts the composition of feature vectors and improves the weight of accurate prediction to optimize the integration effect. During training, an innovative data privacy protection mechanism is applied for avoiding data sharing, where each student network only inherits weights from all trained networks and does not inherit the training dataset. Here, the framework can be repeatedly optimized and improved by only training the next student subnetwork on new EEG signals. Experimental results show that our framework improves the accuracy of EEG emotion recognition by more than 5\% and gets state-of-the-art performance for EEG emotion recognition in the subject-independent mode.},
  archive      = {J_TNNLS},
  author       = {Tianhao Gu and Zhe Wang and Xinlei Xu and Dongdong Li and Hai Yang and Wenli Du},
  doi          = {10.1109/TNNLS.2022.3168935},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {11021-11028},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Frame-level Teacher–Student learning with data privacy for EEG emotion recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Data-based output synchronization of multi-agent systems
with actuator faults. <em>TNNLS</em>, <em>34</em>(12), 11013–11020. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, the output synchronization of multi-agent systems (MAS) with actuator faults is studied. To detect the faults, a backward input-driven fault detection mechanism (BIFDM) is presented for MAS. Different from previous works, the system operation can be monitored without system model by the proposed BIFDM. Then to tolerate the faults, a novel fault-tolerant controller (FTC) based on reinforcement learning (RL) and backward information (BI) is proposed. Particularly, by the combination of BI, the design of additional parameters for faults is avoided. Furthermore, the proposed FTC overcomes the shortcoming that the previous FTCs cannot be applied to heterogeneous MAS. Finally, two simulation examples are given to verify the effectiveness of the proposed methods.},
  archive      = {J_TNNLS},
  author       = {Yingying Liu and Zhanshan Wang and Yuan Wang},
  doi          = {10.1109/TNNLS.2022.3160603},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {11013-11020},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-based output synchronization of multi-agent systems with actuator faults},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Von mises–fisher elliptical distribution. <em>TNNLS</em>,
<em>34</em>(12), 11006–11012. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern probabilistic learning systems mainly assume symmetric distributions, however, real-world data typically obey skewed distributions and are thus not adequately modeled through symmetric distributions. To address this issue, a generalization of symmetric distributions called elliptical distributions are increasingly used, together with further improvements based on skewed elliptical distributions. However, existing approaches are either hard to estimate or have complicated and abstract representations. To this end, we propose a novel approach based on the von-Mises–Fisher (vMF) distribution to obtain an explicit and simple probability representation of skewed elliptical distributions. The analysis shows that this not only allows us to design and implement nonsymmetric learning systems but also provides a physically meaningful and intuitive way of generalizing skewed distributions. For rigor, the proposed framework is proven to share important and desirable properties with its symmetric counterpart. The proposed vMF distribution is demonstrated to be easy to generate and stable to estimate, both theoretically and through examples.},
  archive      = {J_TNNLS},
  author       = {Shengxi Li and Danilo Mandic},
  doi          = {10.1109/TNNLS.2022.3160519},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {11006-11012},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Von Mises–Fisher elliptical distribution},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast-converging simulated annealing for ising models based
on integral stochastic computing. <em>TNNLS</em>, <em>34</em>(12),
10999–11005. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic bits (p-bits) have recently been presented as a spin (basic computing element) for the simulated annealing (SA) of Ising models. In this brief, we introduce fast-converging SA based on p-bits designed using integral stochastic computing. The stochastic implementation approximates a p-bit function, which can search for a solution to a combinatorial optimization problem at lower energy than conventional p-bits. Searching around the global minimum energy can increase the probability of finding a solution. The proposed stochastic computing-based SA method is compared with conventional SA and quantum annealing (QA) with a D-Wave Two quantum annealer on the traveling salesman, maximum cut (MAX-CUT), and graph isomorphism (GI) problems. The proposed method achieves a convergence speed a few orders of magnitude faster while dealing with an order of magnitude larger number of spins than the other methods.},
  archive      = {J_TNNLS},
  author       = {Naoya Onizawa and Kota Katsuki and Duckgyu Shin and Warren J. Gross and Takahiro Hanyu},
  doi          = {10.1109/TNNLS.2022.3159713},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10999-11005},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast-converging simulated annealing for ising models based on integral stochastic computing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized key-value memory to flexibly adjust redundancy
in memory-augmented networks. <em>TNNLS</em>, <em>34</em>(12),
10993–10998. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory-augmented neural networks enhance a neural network with an external key-value (KV) memory whose complexity is typically dominated by the number of support vectors in the key memory. We propose a generalized KV memory that decouples its dimension from the number of support vectors by introducing a free parameter that can arbitrarily add or remove redundancy to the key memory representation. In effect, it provides an additional degree of freedom to flexibly control the tradeoff between robustness and the resources required to store and compute the generalized KV memory. This is particularly useful for realizing the key memory on in-memory computing hardware where it exploits nonideal, but extremely efficient nonvolatile memory devices for dense storage and computation. Experimental results show that adapting this parameter on demand effectively mitigates up to 44\% nonidealities, at equal accuracy and number of devices, without any need for neural network retraining.},
  archive      = {J_TNNLS},
  author       = {Denis Kleyko and Geethan Karunaratne and Jan M. Rabaey and Abu Sebastian and Abbas Rahimi},
  doi          = {10.1109/TNNLS.2022.3159445},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10993-10998},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized key-value memory to flexibly adjust redundancy in memory-augmented networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiagent soft actor-critic based hybrid motion planner for
mobile robots. <em>TNNLS</em>, <em>34</em>(12), 10980–10992. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel hybrid multirobot motion planner that can be applied under no explicit communication and local observable conditions is presented. The planner is model-free and can realize the end-to-end mapping of multirobot state and observation information to final smooth and continuous trajectories. The planner is a front-end and back-end separated architecture. The design of the front-end collaborative waypoints searching module is based on the multiagent soft actor-critic (MASAC) algorithm under the centralized training with decentralized execution (CTDE) diagram. The design of the back-end trajectory optimization module is based on the minimal snap method with safety zone constraints. This module can output the final dynamic-feasible and executable trajectories. Finally, multigroup experimental results verify the effectiveness of the proposed motion planner.},
  archive      = {J_TNNLS},
  author       = {Zichen He and Lu Dong and Chunwei Song and Changyin Sun},
  doi          = {10.1109/TNNLS.2022.3172168},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10980-10992},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiagent soft actor-critic based hybrid motion planner for mobile robots},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interval bipartite synchronization of multiple neural
networks in signed graphs. <em>TNNLS</em>, <em>34</em>(12), 10970–10979.
(<a href="https://doi.org/10.1109/TNNLS.2022.3172122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval bipartite consensus of multiagents described by signed graphs has received extensive concern recently, and the rooted cycles play a critical role in stabilization, while the structurally balanced graphs are essential to achieve bipartite consensus. However, the gauge transformation used in the linear system is no longer feasible in the nonlinear case. This article addresses interval bipartite synchronization of multiple neural networks (NNs) in a signed graph via a Lyapunov-based approach, extending the existing work to a more practical but complicated case. A general matrix M in signed graphs is introduced to construct the novel Lyapunov functions, and sufficient conditions are obtained. We find that the rooted cycles and the structurally balanced graphs are essential to stabilize and achieve bipartite synchronization. More importantly, we discover that the nonrooted cycles are crucial in reaching interval bipartite synchronization, not previously mentioned. Several examples are presented to illustrate interval bipartite synchronization of multiple NNs with signed graphs.},
  archive      = {J_TNNLS},
  author       = {Wen Sun and Biwen Li and Wanli Guo and Shiping Wen and Xiaoqun Wu},
  doi          = {10.1109/TNNLS.2022.3172122},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10970-10979},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interval bipartite synchronization of multiple neural networks in signed graphs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor-CSPNet: A novel geometric deep learning framework for
motor imagery classification. <em>TNNLS</em>, <em>34</em>(12),
10955–10969. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has been widely investigated in a vast majority of applications in electroencephalography (EEG)-based brain–computer interfaces (BCIs), especially for motor imagery (MI) classification in the past five years. The mainstream DL methodology for the MI-EEG classification exploits the temporospatial patterns of EEG signals using convolutional neural networks (CNNs), which have been particularly successful in visual images. However, since the statistical characteristics of visual images depart radically from EEG signals, a natural question arises whether an alternative network architecture exists apart from CNNs. To address this question, we propose a novel geometric DL (GDL) framework called Tensor-CSPNet, which characterizes spatial covariance matrices derived from EEG signals on symmetric positive definite (SPD) manifolds and fully captures the temporospatiofrequency patterns using existing deep neural networks on SPD manifolds, integrating with experiences from many successful MI-EEG classifiers to optimize the framework. In the experiments, Tensor-CSPNet attains or slightly outperforms the current state-of-the-art performance on the cross-validation and holdout scenarios in two commonly used MI-EEG datasets. Moreover, the visualization and interpretability analyses also exhibit the validity of Tensor-CSPNet for the MI-EEG classification. To conclude, in this study, we provide a feasible answer to the question by generalizing the DL methodologies on SPD manifolds, which indicates the start of a specific GDL methodology for the MI-EEG classification.},
  archive      = {J_TNNLS},
  author       = {Ce Ju and Cuntai Guan},
  doi          = {10.1109/TNNLS.2022.3172108},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10955-10969},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tensor-CSPNet: A novel geometric deep learning framework for motor imagery classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive dynamic programming-based cooperative motion/force
control for modular reconfigurable manipulators: A joint task assignment
approach. <em>TNNLS</em>, <em>34</em>(12), 10944–10954. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a cooperative motion/force control (CMFC) scheme based on adaptive dynamic programming (ADP) for modular reconfigurable manipulators (MRMs) with the joint task assignment approach. By separating terms depending on local variables only, the dynamic model of the entire MRM system can be regarded as a set of joint modules interconnected by coupling torque. In addition, the Jacobian matrix, which reflects the interaction force of the MRM end-effector, can be mapped into each joint. Using this approach, both the motion and force tasks on the end-effector of the entire MRM system can be assigned to each joint module cooperatively. Then, by substituting the actual states of coupled joint modules with their desired ones, the norm-boundedness assumption on the interconnection of joint module can be relaxed. By using the measured input–output data of each joint module, a neural network (NN)-based robust decentralized observer, which guarantees the observation error to be asymptotically stable is established. An improved local value function is constructed for each joint module to reflect the interconnection. Then, the local Hamilton–Jacobi–Bellman equation is solved by constructing a local critic NN with a nested learning structure. Hereafter, the ADP-based CMFC is obtained by the assistance of force feedback compensation. Based on the Lyapunov stability analysis, the closed-loop MRM system is guaranteed to be uniformly ultimately bounded under the present ADP-based CMFC scheme. The simulation on a two-degree of freedom MRM system demonstrates the effectiveness of the present control approach.},
  archive      = {J_TNNLS},
  author       = {Bo Zhao and Yongwei Zhang and Derong Liu},
  doi          = {10.1109/TNNLS.2022.3171828},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10944-10954},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive dynamic programming-based cooperative Motion/Force control for modular reconfigurable manipulators: A joint task assignment approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse index tracking with k-sparsity or ϵ-deviation
constraint via ℓ0-norm minimization. <em>TNNLS</em>, <em>34</em>(12),
10930–10943. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse index tracking, as one of the passive investment strategies, is to track a benchmark financial index via constructing a portfolio with a few assets in a market index. It can be considered as parameter learning in an adaptive system, in which we periodically update the selected assets and their investment percentages based on the sliding window approach. However, many existing algorithms for sparse index tracking cannot explicitly and directly control the number of assets or the tracking error. This article formulates sparse index tracking as two constrained optimization problems and then proposes two algorithms, namely, nonnegative orthogonal matching pursuit with projected gradient descent (NNOMP-PGD) and alternating direction method of multipliers for $\ell _{0}$ -norm (ADMM- $\ell _{0}$ ). The NNOMP-PGD aims at minimizing the tracking error subject to the number of selected assets less than or equal to a predefined number. With the NNOMP-PGD, investors can directly and explicitly control the number of selected assets. The ADMM- $\ell _{0}$ aims at minimizing the number of selected assets subject to the tracking error that is upper bounded by a preset threshold. It can directly and explicitly control the tracking error. The convergence of the two proposed algorithms is also presented. With our algorithms, investors can explicitly and directly control the number of selected assets or the tracking error of the resultant portfolio. In addition, numerical experiments demonstrate that the proposed algorithms outperform the existing approaches.},
  archive      = {J_TNNLS},
  author       = {Xiao Peng Li and Zhang-Lei Shi and Chi-Sing Leung and Hing Cheung So},
  doi          = {10.1109/TNNLS.2022.3171819},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10930-10943},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sparse index tracking with K-sparsity or ϵ-deviation constraint via ℓ0-norm minimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic moore–penrose inversion with unknown derivatives:
Gradient neural network approach. <em>TNNLS</em>, <em>34</em>(12),
10919–10929. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding dynamic Moore–Penrose inverses (DMPIs) in real-time is a challenging problem due to the time-varying nature of the inverse. Traditional numerical methods for static Moore–Penrose inverse are not efficient for calculating DMPIs and are restricted by serial processing. The current state-of-the-art method for finding DMPIs is called the zeroing neural network (ZNN) method, which requires that the time derivative of the associated matrix is available all the time during the solution process. However, in practice, the time derivative of the associated dynamic matrix may not be available in a real-time manner or be subject to noises caused by differentiators. In this article, we propose a novel gradient-based neural network (GNN) method for computing DMPIs, which does not need the time derivative of the associated dynamic matrix. In particular, the neural state matrix of the proposed GNN converges to the theoretical DMPI in finite time. The finite-time convergence is kept by simply setting a large parameter when there are additive noises in the implementation of the GNN model. Simulation results demonstrate the efficacy and superiority of the proposed GNN method.},
  archive      = {J_TNNLS},
  author       = {Yinyan Zhang and Jilian Zhang and Jian Weng},
  doi          = {10.1109/TNNLS.2022.3171715},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10919-10929},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic Moore–Penrose inversion with unknown derivatives: Gradient neural network approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AFS graph: Multidimensional axiomatic fuzzy set knowledge
graph for open-domain question answering. <em>TNNLS</em>,
<em>34</em>(12), 10904–10918. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain question answering (QA) tasks require a model to retrieve inference chains associated with the answer from massive documents. The core of a QA model is the information filtering ability and reasoning ability. This article proposes a semantic knowledge reasoning graph model based on the multidimensional axiomatic fuzzy set (AFS), which can generate the knowledge graph (KG) and build reasoning paths for reading comprehension tasks through unsupervised learning. Moreover, taking advantage of the interpretable AFS framework enables the proposed model to have the ability to learn and analyze the semantic relationships between candidate documents. Meanwhile, the utilization of the multidimensional AFS acquires semantic descriptions of candidate documents more concise and flexible. The similarity degree between paragraphs is calculated according to the AFS description to generate the graph. Interpretable chains of reasoning provided by the AFS knowledge graph (AFS Graph) will serve as the basis for the answer prediction. Compared with the previous methods, the AFS Graph model presented in this article improves interpretability and reasoning ability. Experimental results show that the proposed model can achieve the state-of-the-art performance on datasets of HotpotQA, SQuAD, and Natural Questions Open.},
  archive      = {J_TNNLS},
  author       = {Qi Lang and Xiaodong Liu and Wenjuan Jia},
  doi          = {10.1109/TNNLS.2022.3171677},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10904-10918},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AFS graph: Multidimensional axiomatic fuzzy set knowledge graph for open-domain question answering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large-scale meta-heuristic feature selection based on BPSO
assisted rough hypercuboid approach. <em>TNNLS</em>, <em>34</em>(12),
10889–10903. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of prominent features for building more compact and efficient models is an important data preprocessing task in the field of data mining. The rough hypercuboid approach is an emerging technique that can be applied to eliminate irrelevant and redundant features, especially for the inexactness problem in approximate numerical classification. By integrating the meta-heuristic-based evolutionary search technique, a novel global search method for numerical feature selection is proposed in this article based on the hybridization of the rough hypercuboid approach and binary particle swarm optimization (BPSO) algorithm, namely RH-BPSO. To further alleviate the issue of high computational cost when processing large-scale datasets, parallelization approaches for calculating the hybrid feature evaluation criteria are presented by decomposing and recombining hypercuboid equivalence partition matrix via horizontal data partitioning. A distributed meta-heuristic optimized rough hypercuboid feature selection (DiRH-BPSO) algorithm is thus developed and embedded in the Apache Spark cloud computing model. Extensive experimental results indicate that RH-BPSO is promising and can significantly outperform the other representative feature selection algorithms in terms of classification accuracy, the cardinality of the selected feature subset, and execution efficiency. Moreover, experiments on distributed-memory multicore clusters show that DiRH-BPSO is significantly faster than its sequential counterpart and is perfectly capable of completing large-scale feature selection tasks that fail on a single node due to memory constraints. Parallel scalability and extensibility analysis also demonstrate that DiRH-BPSO could scale out and extend well with the growth of computational nodes and the volume of data.},
  archive      = {J_TNNLS},
  author       = {Chuan Luo and Sizhao Wang and Tianrui Li and Hongmei Chen and Jiancheng Lv and Zhang Yi},
  doi          = {10.1109/TNNLS.2022.3171614},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10889-10903},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Large-scale meta-heuristic feature selection based on BPSO assisted rough hypercuboid approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint segmentation and identification feature learning for
occlusion face recognition. <em>TNNLS</em>, <em>34</em>(12),
10875–10888. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing occlusion face recognition algorithms almost tend to pay more attention to the visible facial components. However, these models are limited because they heavily rely on existing face segmentation approaches to locate occlusions, which is extremely sensitive to the performance of mask learning. To tackle this issue, we propose a joint segmentation and identification feature learning framework for end-to-end occlusion face recognition. More particularly, unlike employing an external face segmentation model to locate the occlusion, we design an occlusion prediction module supervised by known mask labels to be aware of the mask. It shares underlying convolutional feature maps with the identification network and can be collaboratively optimized with each other. Furthermore, we propose a novel channel refinement network to cast the predicted single-channel occlusion mask into a multi-channel mask matrix with each channel owing a distinct mask map. Occlusion-free feature maps are then generated by projecting multi-channel mask probability maps onto original feature maps. Thus, it can suppress the representation of occlusion elements in both the spatial and channel dimensions under the guidance of the mask matrix. Moreover, in order to avoid misleading aggressively predicted mask maps and meanwhile actively exploit usable occlusion-robust features, we aggregate the original and occlusion-free feature maps to distill the final candidate embeddings by our proposed feature purification module. Lastly, to alleviate the scarcity of real-world occlusion face recognition datasets, we build large-scale synthetic occlusion face datasets, totaling up to 980193 face images of 10574 subjects for the training dataset and 36721 face images of 6817 subjects for the testing dataset, respectively. Extensive experimental results on the synthetic and real-world occlusion face datasets show that our approach significantly outperforms the state-of-the-art in both 1:1 face verification and 1:N face identification.},
  archive      = {J_TNNLS},
  author       = {Baojin Huang and Zhongyuan Wang and Kui Jiang and Qin Zou and Xin Tian and Tao Lu and Zhen Han},
  doi          = {10.1109/TNNLS.2022.3171604},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10875-10888},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint segmentation and identification feature learning for occlusion face recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from human demonstrations for wheel mobile
manipulator: An unscented model predictive control approach.
<em>TNNLS</em>, <em>34</em>(12), 10864–10874. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industry 4.0 requires new production models to be more flexible and efficient, which means that robots should be capable of flexible skills to adapt to different production and processing tasks. Learning from demonstration (LfD) is considered as one of the promising ways for robots to obtain motion and manipulation skills from humans. In this article, a framework that enables a wheel mobile manipulator to learn skills from humans and complete the specified tasks in an unstructured environment is developed, including a high-level trajectory learning and a low-level trajectory tracking control. First, a modified dynamic movement primitives (DMPs) model is utilized to simultaneously learn the movement trajectories of a human operator’s hand and body as reference trajectories for the mobile manipulator. Considering that the auxiliary model obtained by the nonlinear feedback is hard to accurately describe the behavior of mobile manipulator with the presence of uncertain parameters and disturbances, a novel model is established, and an unscented model predictive control (UMPC) strategy is then presented to solve the trajectory tracking control problem without violating the system constraints. Moreover, a sufficient condition guaranteeing the input to state practical stability (ISpS) of the system is obtained, and the upper bound of estimated error is also defined. Finally, the effectiveness of the proposed strategy is validated by three simulation experiments.},
  archive      = {J_TNNLS},
  author       = {Dongdong Qin and Andong Liu and Jianming Xu and Wen-An Zhang and Li Yu},
  doi          = {10.1109/TNNLS.2022.3171595},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10864-10874},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning from human demonstrations for wheel mobile manipulator: An unscented model predictive control approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative decision-reinforced self-supervision for
attributed graph clustering. <em>TNNLS</em>, <em>34</em>(12),
10851–10863. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph clustering aims to partition nodes of a graph structure into different groups. Recent works usually use variational graph autoencoder (VGAE) to make the node representations obey a specific distribution. Although they have shown promising results, how to introduce supervised information to guide the representation learning of graph nodes and improve clustering performance is still an open problem. In this article, we propose a Collaborative Decision-Reinforced Self-Supervision (CDRS) method to solve the problem, in which a pseudo node classification task collaborates with the clustering task to enhance the representation learning of graph nodes. First, a transformation module is used to enable end-to-end training of existing methods based on VGAE. Second, the pseudo node classification task is introduced into the network through multitask learning to make classification decisions for graph nodes. The graph nodes that have consistent decisions on clustering and pseudo node classification are added to a pseudo-label set, which can provide fruitful self-supervision for subsequent training. This pseudo-label set is gradually augmented during training, thus reinforcing the generalization capability of the network. Finally, we investigate different sorting strategies to further improve the quality of the pseudo-label set. Extensive experiments on multiple datasets show that the proposed method achieves outstanding performance compared with state-of-the-art methods. Our code is available at https://github.com/Jillian555/TNNLS_CDRS .},
  archive      = {J_TNNLS},
  author       = {Pengfei Zhu and Jialu Li and Yu Wang and Bin Xiao and Shuai Zhao and Qinghua Hu},
  doi          = {10.1109/TNNLS.2022.3171583},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10851-10863},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Collaborative decision-reinforced self-supervision for attributed graph clustering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pinning controller design for set reachability of
state-dependent impulsive boolean networks. <em>TNNLS</em>,
<em>34</em>(12), 10838–10850. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considered the stimulation of tumor necrosis factor as an impulsive control, an apoptosis network is modeled as a state-dependent impulsive Boolean network (SDIBN). Making cell death normally means driving the trajectory of an apoptosis network out of states that indicate cell survival. To achieve the goal, this article focuses on the pinning controller design for set reachability of SDIBNs. To begin with, the definitions of reachability and set reachability are introduced, and their relation is illustrated. For judging whether the trajectory of an SDIBN leaves undesirable states, a necessary and sufficient condition is presented according to the criteria for the set reachability. In addition, a series of algorithms is provided to find all possible sets of pinning nodes for the set reachability. Note that attractors containing in all undesirable states are studied to make SDIBNs set reachable via controlling the smallest states. For the purpose of determining pinning nodes for one-step set reachability, the Hamming distance is presented under scalar forms of states. Pinning nodes with the smallest cardinality for the set reachability are derived by deleting some redundant nodes. Compared with the existing results, the state feedback gain can be obtained without solving logical matrix equations. The computation complexity of the proposed approach is lower than that of the existing methods. Moreover, the method of designing pinning controllers is used to discuss apoptosis networks. The experimental result shows that apoptosis networks depart from undesirable states by controlling only one node.},
  archive      = {J_TNNLS},
  author       = {Yiliang Li and Jun-e Feng and Xiaodi Li and Shengyuan Xu},
  doi          = {10.1109/TNNLS.2022.3171576},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10838-10850},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pinning controller design for set reachability of state-dependent impulsive boolean networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hyperspectral and SAR image classification via multiscale
interactive fusion network. <em>TNNLS</em>, <em>34</em>(12),
10823–10837. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitations of single-source data, joint classification using multisource remote sensing data has received increasing attention. However, existing methods still have certain shortcomings when faced with feature extraction from single-source data and feature fusion between multisource data. In this article, a method based on multiscale interactive information extraction (MIFNet) for hyperspectral and synthetic aperture radar (SAR) image classification is proposed. First, a multiscale interactive information extraction (MIIE) block is designed to extract meaningful multiscale information. Compared with traditional multiscale models, it can not only obtain richer scale information but also reduce the model parameters and lower the network complexity. Furthermore, a global dependence fusion module (GDFM) is developed to fuse features from multisource data, which implements cross attention between multisource data from a global perspective and captures long-range dependence. Extensive experiments on the three datasets demonstrate the superiority of the proposed method and the necessity of each module for accuracy improvement.},
  archive      = {J_TNNLS},
  author       = {Junjie Wang and Wei Li and Yunhao Gao and Mengmeng Zhang and Ran Tao and Qian Du},
  doi          = {10.1109/TNNLS.2022.3171572},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10823-10837},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperspectral and SAR image classification via multiscale interactive fusion network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D-DFM: Anchor-free multimodal 3-d object detection with
dynamic fusion module for autonomous driving. <em>TNNLS</em>,
<em>34</em>(12), 10812–10822. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in cross-modal 3D object detection rely heavily on anchor-based methods, and however, intractable anchor parameter tuning and computationally expensive postprocessing severely impede an embedded system application, such as autonomous driving. In this work, we develop an anchor-free architecture for efficient camera–light detection and ranging (LiDAR) 3D object detection. To highlight the effect of foreground information from different modalities, we propose a dynamic fusion module (DFM) to adaptively interact images with point features via learnable filters. In addition, the 3D distance intersection-over-union (3D-DIoU) loss is explicitly formulated as a supervision signal for 3D-oriented box regression and optimization. We integrate these components into an end-to-end multimodal 3D detector termed 3D-DFM. Comprehensive experimental results on the widely used KITTI dataset demonstrate the superiority and universality of 3D-DFM architecture, with competitive detection accuracy and real-time inference speed. To the best of our knowledge, this is the first work that incorporates an anchor-free pipeline with multimodal 3D object detection.},
  archive      = {J_TNNLS},
  author       = {Chunmian Lin and Daxin Tian and Xuting Duan and Jianshan Zhou and Dezong Zhao and Dongpu Cao},
  doi          = {10.1109/TNNLS.2022.3171553},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10812-10822},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {3D-DFM: Anchor-free multimodal 3-D object detection with dynamic fusion module for autonomous driving},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nash equilibrium seeking algorithm design for distributed
nonsmooth multicluster games over weight-balanced digraphs.
<em>TNNLS</em>, <em>34</em>(12), 10802–10811. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the multicluster games over weight-balanced digraphs, where the cost functions of all players are nonsmooth. Besides, in the problem, not only are the decisions of all players constrained by heterogeneous local constraints but also the decisions of players in the same cluster are constrained by coupling constraints. Due to the nonsmooth cost functions, the coupling constraints, the general local convex constraints, and the weight-balanced digraphs, existing Nash equilibrium seeking algorithms cannot solve our problem. In order to seek the Nash equilibrium of the game, we design a distributed algorithm based on subgradient descent, differential inclusions, and projection operations. In the algorithm, a distributed learning strategy is embedded for the players to estimate the decisions of other players. Moreover, we analyze the asymptotical convergence of the algorithm via set-valued LaSalle invariance principle. Finally, a numerical simulation about electricity market games is presented to illustrate the effectiveness of our result.},
  archive      = {J_TNNLS},
  author       = {Zhenhua Deng and Yangyang Liu},
  doi          = {10.1109/TNNLS.2022.3171535},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10802-10811},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nash equilibrium seeking algorithm design for distributed nonsmooth multicluster games over weight-balanced digraphs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Adaptive decentralized control for interconnected
time-delay uncertain nonlinear systems with different unknown control
directions and deferred full-state constraints. <em>TNNLS</em>,
<em>34</em>(12), 10789–10801. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the problem of adaptive decentralized control is investigated for a class of interconnected time-delay uncertain nonlinear systems with different unknown control directions and deferred asymmetric time-varying (DTV) full-state constraints. By constructing the novel time-varying asymmetric integral barrier Lyapunov function (TVAIBLF), the conservative limitation of constant integral barrier Lyapunov function (IBLF) or symmetric IBLF is reduced and the need on the prior knowledge of control gains is also avoided, while the deferred constraints directly imposed on the states of system are achieved by introducing the shifting function into the controller design. Furthermore, based on the Nussbaum-type functions, a new adaptive decentralized control strategy for interconnected time-delay nonlinear systems with subsystems having different control directions is proposed via backstepping method. And it is proven that the proposed control method can guarantee that all signals in closed-loop system are bounded and the transform errors asymptotically converge to zero. Finally, the effectiveness of the proposed control strategy is illustrated through the simulation results.},
  archive      = {J_TNNLS},
  author       = {Liuliu Zhang and Lingchen Zhu and Changchun Hua and Cheng Qian},
  doi          = {10.1109/TNNLS.2022.3171518},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10789-10801},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive decentralized control for interconnected time-delay uncertain nonlinear systems with different unknown control directions and deferred full-state constraints},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing graph neural network with multiaspect
hilbert–schmidt independence criterion. <em>TNNLS</em>, <em>34</em>(12),
10775–10788. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph neural network (GNN) has demonstrated its superior power in various data mining tasks and has been widely applied in diversified fields. The core of GNN is the aggregation and combination functions, and mainstream GNN studies focus on the enhancement of these functions. However, GNNs face a common challenge, i.e., useless features contained in neighbor nodes may be integrated into the target node during the aggregation process. This leads to poor node embedding and undermines downstream tasks. To tackle this problem, this article proposes a novel GNN optimization framework GNN-MHSIC by introducing the nonparametric dependence method Hilbert–Schmidt independence criterion (HSIC) under the guidance of information bottleneck. HSIC is utilized to guide the information propagation among layers of a GNN from multiaspect views. GNN-MHSIC aims to achieve three main objectives: 1) minimizing the HSIC between the input features and the propagation layers; 2) maximizing the HSIC between the propagation layers and the ground truth; and 3) minimizing the HSIC between the propagation layers. With a multiaspect design, GNN-MHSIC can minimize the propagation of redundant information while preserving relevant information about the target node. We prove GNN-MHSIC’s finite upper and lower bounds theoretically and evaluate it experimentally with four classic GNN models, including the graph convolutional network, the graph attention network (GAT), the heterogeneous GAT, and the heterogeneous graph (HG) propagation network on three widely used HGs. The results illustrate the usefulness and performance of GNN-MHSIC.},
  archive      = {J_TNNLS},
  author       = {Yunfei He and Dengcheng Yan and Wenxin Xie and Yiwen Zhang and Qiang He and Yun Yang},
  doi          = {10.1109/TNNLS.2022.3171419},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10775-10788},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimizing graph neural network with multiaspect Hilbert–Schmidt independence criterion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling self-representation label correlations for textual
aspects and emojis recommendation. <em>TNNLS</em>, <em>34</em>(12),
10762–10774. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Internet services and social platforms encourages users to share their opinions. To help users give valuable comments, content providers expect the recommender system to offer appropriate suggestions, including specific features of the item described in texts and emojis, which are all considered aspects of the user reviews. Hence, the review aspect recommendation task has become significant, where the key lies in handling personal preferences and semantic correlations between suggested items. This article proposes a correlation-aware review aspect recommender (CARAR) system model by constructing self-representation correlations between different views of review aspects, including textual aspects and emojis to make a personalized recommendation. The dependencies between different textual aspects and emojis can be identified and utilized to facilitate the factorization process to learn user and item latent factors. The cross-view correlation mapping between textual aspects and emojis can be built to enhance the recommendation performance. Moreover, the additional information in the real-world environment is also applied to our model to adjust the recommendation results. We constructed experiments on five self-collected and public datasets and compared with six existing models. The results show that our model can outperform the existing models on review aspects recommendation tasks, validating the effectiveness of our approach.},
  archive      = {J_TNNLS},
  author       = {Tianjun Wei and Tommy W. S. Chow and Jianghong Ma},
  doi          = {10.1109/TNNLS.2022.3171335},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10762-10774},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modeling self-representation label correlations for textual aspects and emojis recommendation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep network with irregular convolutional kernels and
self-expressive property for classification of hyperspectral images.
<em>TNNLS</em>, <em>34</em>(12), 10747–10761. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel deep network with irregular convolutional kernels and self-expressive property (DIKS) for the classification of hyperspectral images (HSIs). Specifically, we use the principal component analysis (PCA) and superpixel segmentation to obtain a series of irregular patches, which are regarded as convolutional kernels of our network. With such kernels, the feature maps of HSIs can be adaptively computed to well describe the characteristics of each object class. After multiple convolutional layers, features exported by all convolution operations are combined into a stacked form with both shallow and deep features. These stacked features are then clustered by introducing the self-expression theory to produce final features. Unlike most traditional deep learning approaches, the DIKS method has the advantage of self-adaptability to the given HSI due to building irregular kernels. In addition, this proposed method does not require any training operations for feature extraction. Because of using both shallow and deep features, the DIKS has the advantage of being multiscale. Due to introducing self-expression, the DIKS method can export more discriminative features for HSI classification. Extensive experimental results are provided to validate that our method achieves better classification performance compared with state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Changda Xing and Yuhua Cong and Chaowei Duan and Zhisheng Wang and Meiling Wang},
  doi          = {10.1109/TNNLS.2022.3171324},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10747-10761},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep network with irregular convolutional kernels and self-expressive property for classification of hyperspectral images},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding how orthogonality of parameters improves
quantization of neural networks. <em>TNNLS</em>, <em>34</em>(12),
10737–10746. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze why the orthogonality penalty improves quantization in deep neural networks. Using results from perturbation theory as well as through extensive experiments with Resnet50, Resnet101, and VGG19 models, we mathematically and experimentally show that improved quantization accuracy resulting from orthogonality constraint stems primarily from reduced condition numbers, which is the ratio of largest to smallest singular values of weight matrices, more so than reduced spectral norms, in contrast to the explanations in previous literature. We also show that the orthogonality penalty improves quantization even in the presence of a state-of-the-art quantized retraining method. Our results show that, when the orthogonality penalty is used with quantized retraining, ImageNet Top5 accuracy loss from 4- to 8-bit quantization is reduced by up to 7\% for Resnet50, and up to 10\% for Resnet101, compared to quantized retraining with no orthogonality penalty.},
  archive      = {J_TNNLS},
  author       = {Sukru Burc Eryilmaz and Aysegul Dundar},
  doi          = {10.1109/TNNLS.2022.3171297},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10737-10746},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Understanding how orthogonality of parameters improves quantization of neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Pose and color-gamut guided generative adversarial network
for pedestrian image synthesis. <em>TNNLS</em>, <em>34</em>(12),
10724–10736. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tremendous transfer requirements in pedestrian reidentification (Re-ID) tasks have greatly promoted the remarkable success in pedestrian image synthesis, to relieve the inconsistency in poses and lighting. However, existing approaches are confined to transferring in a particular domain and are difficult to combine, since pose and color variables locate in two independent domains. To facilitate the research toward conquering this issue, we propose a pose and color-gamut guided generative adversarial network (PC-GAN) that performs joint-domain pedestrian image synthesis conditioned on certain pose and color-gamut through a delicate supervision design. The generator of the network comprises a sequence of cross-domain conversion subnets, where the local displacement estimator, color-gamut transformer, and pose transporter coordinate their learning pace to progressively synthesize images in desired pose and color-gamut. Ablation studies have demonstrated the efficacy and efficiency of the proposed network both qualitatively and quantitatively on Market-1501 and DukeMTMC. Furthermore, the proposed architecture can generate training images for person Re-ID, alleviating the data insufficiency problem.},
  archive      = {J_TNNLS},
  author       = {Xiaokai Liu and Xiang Liu and Gang Li and Sheng Bi},
  doi          = {10.1109/TNNLS.2022.3171245},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10724-10736},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pose and color-gamut guided generative adversarial network for pedestrian image synthesis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). L-BGNN: Layerwise trained bipartite graph neural networks.
<em>TNNLS</em>, <em>34</em>(12), 10711–10723. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning low-dimensional representations of bipartite graphs enables e-commerce applications, such as recommendation, classification, and link prediction. A layerwise-trained bipartite graph neural network (L-BGNN) embedding method, which is unsupervised, efficient, and scalable, is proposed in this work. To aggregate the information across and within two partitions of a bipartite graph, a customized interdomain message passing (IDMP) operation and an intradomain alignment (IDA) operation are adopted by the proposed L-BGNN method. Furthermore, we develop a layerwise training algorithm for L-BGNN to capture the multihop relationship of large bipartite networks and improve training efficiency. We conduct extensive experiments on several datasets and downstream tasks of various scales to demonstrate the effectiveness and efficiency of the L-BGNN method as compared with state-of-the-art methods. Our codes are publicly available at https://github.com/TianXieUSC/L-BGNN .},
  archive      = {J_TNNLS},
  author       = {Tian Xie and Chaoyang He and Xiang Ren and Cyrus Shahabi and C.-C. Jay Kuo},
  doi          = {10.1109/TNNLS.2022.3171199},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10711-10723},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {L-BGNN: Layerwise trained bipartite graph neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-organizing democratized learning: Toward large-scale
distributed learning systems. <em>TNNLS</em>, <em>34</em>(12),
10698–10710. (<a
href="https://doi.org/10.1109/TNNLS.2022.3170872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging cross-device artificial intelligence (AI) applications require a transition from conventional centralized learning systems toward large-scale distributed AI systems that can collaboratively perform complex learning tasks. In this regard, democratized learning (Dem-AI) lays out a holistic philosophy with underlying principles for building large-scale distributed and democratized machine learning systems. The outlined principles are meant to study a generalization in distributed learning systems that go beyond existing mechanisms such as federated learning (FL). Moreover, such learning systems rely on hierarchical self-organization of well-connected distributed learning agents who have limited and highly personalized data and can evolve and regulate themselves based on the underlying duality of specialized and generalized processes. Inspired by Dem-AI philosophy, a novel distributed learning approach is proposed in this article. The approach consists of a self-organizing hierarchical structuring mechanism based on agglomerative clustering, hierarchical generalization, and corresponding learning mechanism. Subsequently, hierarchical generalized learning problems in recursive forms are formulated and shown to be approximately solved using the solutions of distributed personalized learning problems and hierarchical update mechanisms. To that end, a distributed learning algorithm, namely DemLearn, is proposed. Extensive experiments on benchmark MNIST, Fashion-MNIST, FE-MNIST, and CIFAR-10 datasets show that the proposed algorithm demonstrates better results in the generalization performance of learning models in agents compared to the conventional FL algorithms. The detailed analysis provides useful observations to further handle both the generalization and specialization performance of the learning models in Dem-AI systems.},
  archive      = {J_TNNLS},
  author       = {Minh N. H. Nguyen and Shashi Raj Pandey and Tri Nguyen Dang and Eui-Nam Huh and Nguyen H. Tran and Walid Saad and Choong Seon Hong},
  doi          = {10.1109/TNNLS.2022.3170872},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10698-10710},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-organizing democratized learning: Toward large-scale distributed learning systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust bilinear probabilistic PCA using a matrix variate t
distribution. <em>TNNLS</em>, <em>34</em>(12), 10683–10697. (<a
href="https://doi.org/10.1109/TNNLS.2022.3170797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bilinear probabilistic principal component analysis (BPPCA) was introduced recently as a model-based dimension reduction technique on matrix data. However, BPPCA is based on the Gaussian assumption and hence is vulnerable to potential outlying matrix-valued observations. In this article, we present a new robust extension of BPPCA, called BPPCA using a matrix variate ${t}$ distribution ( ${t}$ BPPCA), that is built upon a matrix variate $t$ distribution. Like the multivariate $t$ , this distribution offers an additional robustness tuning parameter, which can downweight outliers. By introducing a Gamma distributed latent weight variable, this distribution can be represented hierarchically. With this representation, two efficient accelerated expectation–maximization (EM)-like algorithms for parameter estimation are developed. Experiments on a number of synthetic and real datasets are conducted to understand ${t}$ BPPCA and compare with several closely related competitors, including its vector-based counterpart. The results reveal that ${t}$ BPPCA is generally more robust and accurate in the presence of outliers. Moreover, the expected latent weights under ${t}$ BPPCA can be effectively used for outliers’ detection, which is much more reliable than its vector-based counterpart due to its better robustness.},
  archive      = {J_TNNLS},
  author       = {Jianhua Zhao and Xuan Ma and Lei Shi and Zhen Wang},
  doi          = {10.1109/TNNLS.2022.3170797},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10683-10697},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust bilinear probabilistic PCA using a matrix variate t distribution},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). A tensor-based online RPCA model for compressive background
subtraction. <em>TNNLS</em>, <em>34</em>(12), 10668–10682. (<a
href="https://doi.org/10.1109/TNNLS.2022.3170789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background subtraction of videos has been a fundamental research topic in computer vision in the past decades. To alleviate the computation burden and enhance the efficiency, background subtraction from online compressive measurements has recently attracted much attention. However, current methods still have limitations. First, they are all based on matrix modeling, which breaks the spatial structure within video frames. Second, they generally ignore the complex disturbance within the background, which reduces the efficiency of the low-rank assumption. To alleviate this issue, we propose a tensor-based online compressive video reconstruction and background subtraction method, abbreviated as NIOTenRPCA, by explicitly modeling the background disturbance in different frames as nonidentical but correlated noise. By virtue of such sophisticated modeling, the proposed method can well adapt to complex video scenes and, thus, perform more robustly. Extensive experiments on a series of real-world video datasets have demonstrated the effectiveness of the proposed method compared with the existing state of the arts. The code of our method is released on the website: https://github.com/crystalzina/NIOTenRPCA .},
  archive      = {J_TNNLS},
  author       = {Zina Li and Yao Wang and Qian Zhao and Shijun Zhang and Deyu Meng},
  doi          = {10.1109/TNNLS.2022.3170789},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10668-10682},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A tensor-based online RPCA model for compressive background subtraction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-frame-based deep view synchronization for
unsynchronized multicamera surveillance. <em>TNNLS</em>,
<em>34</em>(12), 10653–10667. (<a
href="https://doi.org/10.1109/TNNLS.2022.3170642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multicamera surveillance has been an active research topic for understanding and modeling scenes. Compared to a single camera, multicameras provide larger field-of-view and more object cues, and the related applications are multiview counting, multiview tracking, 3-D pose estimation or 3-D reconstruction, and so on. It is usually assumed that the cameras are all temporally synchronized when designing models for these multicamera-based tasks. However, this assumption is not always valid, especially for multicamera systems with network transmission delay and low frame rates due to limited network bandwidth, resulting in desynchronization of the captured frames across cameras. To handle the issue of unsynchronized multicameras, in this article, we propose a synchronization model that works in conjunction with existing deep neural network (DNN)-based multiview models, thus avoiding the redesign of the whole model. We consider two variants of the model, based on where in the pipeline the synchronization occurs, scene-level synchronization and camera-level synchronization. The view synchronization step and the task-specific view fusion and prediction step are unified in the same framework and trained in an end-to-end fashion. Our view synchronization models are applied to different DNNs-based multicamera vision tasks under the unsynchronized setting, including multiview counting and 3-D pose estimation, and achieve good performance compared to baselines.},
  archive      = {J_TNNLS},
  author       = {Qi Zhang and Antoni B. Chan},
  doi          = {10.1109/TNNLS.2022.3170642},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10653-10667},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Single-frame-based deep view synchronization for unsynchronized multicamera surveillance},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed energy trading and scheduling among microgrids
via multiagent reinforcement learning. <em>TNNLS</em>, <em>34</em>(12),
10638–10652. (<a
href="https://doi.org/10.1109/TNNLS.2022.3170070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Renewable energy technologies empower microgrids to generate electricity to supply themselves and trade with others. Under this paradigm, microgrids have become autonomous entities that must intelligently determine their policies for energy trading and scheduling. Many factors influence a microgrid’s decision-making, such as the complex microgrid infrastructure, the uncertain energy yield and demand, and the competition among the energy market players. These factors are usually hard to precisely model, and deriving the optimal policy for a microgrid is challenging. We propose a multiagent reinforcement learning (MARL) approach with an attention mechanism to learn the optimal policies for the microgrids without complex system modeling. We model each microgrid as an autonomous agent, which learns how to schedule energy resources and trade with others by collaborating with other agents. We adopt attention mechanism to enable intelligently selecting contextual information for the training of each agent. After training, an agent can make control decisions using only its local information, which can well preserve the microgrids’ privacy and reduce the communication overhead among microgrids to facilitate distributed control. We implement a simulation environment and evaluate the performances of our proposed method using real-world datasets. The experimental results show that our method can significantly reduce the cost of the microgrids compared with the baseline methods.},
  archive      = {J_TNNLS},
  author       = {Guanyu Gao and Yonggang Wen and Dacheng Tao},
  doi          = {10.1109/TNNLS.2022.3170070},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10638-10652},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed energy trading and scheduling among microgrids via multiagent reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting propagation delay in underwater acoustic
communication networks via deep reinforcement learning. <em>TNNLS</em>,
<em>34</em>(12), 10626–10637. (<a
href="https://doi.org/10.1109/TNNLS.2022.3170050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel deep-reinforcement learning-based medium access control (DL-MAC) protocol for underwater acoustic networks (UANs) where one agent node employing the proposed DL-MAC protocol coexists with other nodes employing traditional protocols, such as time division multiple access (TDMA) or $q$ -Aloha. The DL-MAC agent learns to exploit the large propagation delays inherent in underwater acoustic communications to improve system throughput by either a synchronous or an asynchronous transmission mode. In the sync-DL-MAC protocol, the agent action space is transmission or no transmission, while in the async-DL-MAC, the agent can also vary the start time in each transmission time slot to further exploit the spatiotemporal uncertainty of the UANs. The deep $Q$ -learning algorithm is applied to both sync-DL-MAC and async-DL-MAC agents to learn the optimal policies. A theoretical analysis and computer simulations demonstrate the performance gain obtained by both DL-MAC protocols. The async-DL-MAC protocol outperforms the sync-DL-MAC protocol significantly in sum throughput and packet success rate by adjusting the transmission start time and reducing the length of time slot.},
  archive      = {J_TNNLS},
  author       = {Xuan Geng and Yahong Rosa Zheng},
  doi          = {10.1109/TNNLS.2022.3170050},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10626-10637},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploiting propagation delay in underwater acoustic communication networks via deep reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep convolutional gated recurrent unit for CT image
reconstruction. <em>TNNLS</em>, <em>34</em>(12), 10612–10625. (<a
href="https://doi.org/10.1109/TNNLS.2022.3169569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography (CT) is one of the most important medical imaging technologies in use today. Most commercial CT products use a technique known as the filtered backprojection (FBP) that is fast and can produce decent image quality when an X-ray dose is high. However, the FBP is not good enough on low-dose X-ray CT imaging because the CT image reconstruction problem becomes more stochastic. A more effective reconstruction technique proposed recently and implemented in a limited number of CT commercial products is an iterative reconstruction (IR). The IR technique is based on a Bayesian formulation of the CT image reconstruction problem with an explicit model of the CT scanning, including its stochastic nature, and a prior model that incorporates our knowledge about what a good CT image should look like. However, constructing such prior knowledge is more complicated than it seems. In this article, we propose a novel neural network for CT image reconstruction. The network is based on the IR formulation and constructed with a recurrent neural network (RNN). Specifically, we transform the gated recurrent unit (GRU) into a neural network performing CT image reconstruction. We call it “GRU reconstruction.” This neural network conducts concurrent dual-domain learning. Many deep learning (DL)-based methods in medical imaging are single-domain learning, but dual-domain learning performs better because it learns from both the sinogram and the image domain. In addition, we propose backpropagation through stage (BPTS) as a new RNN backpropagation algorithm. It is similar to the backpropagation through time (BPTT) of an RNN; however, it is tailored for iterative optimization. Results from extensive experiments indicate that our proposed method outperforms conventional model-based methods, single-domain DL methods, and state-of-the-art DL techniques in terms of the root mean squared error (RMSE), the peak signal-to-noise ratio (PSNR), and the structure similarity (SSIM) and in terms of visual appearance.},
  archive      = {J_TNNLS},
  author       = {Masaki Ikuta and Jun Zhang},
  doi          = {10.1109/TNNLS.2022.3169569},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10612-10625},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A deep convolutional gated recurrent unit for CT image reconstruction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-constructing fuzzy neural fractional-order sliding mode
control of active power filter. <em>TNNLS</em>, <em>34</em>(12),
10600–10611. (<a
href="https://doi.org/10.1109/TNNLS.2022.3169518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a fractional-order sliding mode control (FOSMC) scheme is proposed for mitigating harmonic distortions in the power system, whereby a self-constructing recurrent fuzzy neural network (SCRFNN) is used to weaken the effect of compound nonlinearity caused by unknown uncertainties and environmental fluctuations. The fractional-order sliding mode controller (SMC) is constructed to maintain the control system to be asymptotically stable and a fractional-order calculus is introduced into an SMC to soften the sliding manifold design and realize chattering reduction. Considering parameter variations existing in the power system model, SCRFNN is adopted to approximate the unknown dynamics, which is able to dynamically update network structure by optimizing the fuzzy division, and a feedback connection is incorporated into the feedforward neural network, which is regarded as a storage unit to enhance the capability of coping with temporal problem. The control scheme combining the FOSMC with the SCRFNN can make the tracking error and its time derivative converge to zero. Experimental studies demonstrate the validity of the designed scheme, and comprehensive comparisons illustrate its superiority in harmonic suppression and high robustness.},
  archive      = {J_TNNLS},
  author       = {Juntao Fei and Zhe Wang and Qi Pan},
  doi          = {10.1109/TNNLS.2022.3169518},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10600-10611},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-constructing fuzzy neural fractional-order sliding mode control of active power filter},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synchronous spatiotemporal graph transformer: A new
framework for traffic data prediction. <em>TNNLS</em>, <em>34</em>(12),
10589–10599. (<a
href="https://doi.org/10.1109/TNNLS.2022.3169488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the spatiotemporal relationship (STR) of traffic data is important yet challenging for existing graph networks. These methods usually capture features separately in temporal and spatial dimensions or represent the spatiotemporal data by adopting multiple local spatial–temporal graphs. The first kind of method mentioned above is difficult to capture potential temporal–spatial relationships, while the other is limited for long-term feature extraction due to its local receptive field. To handle these issues, the Synchronous Spatio-Temporal grAph Transformer (S2TAT) network is proposed for efficiently modeling the traffic data. The contributions of our method include the following: 1) the nonlocal STR can be synchronously modeled by our integrated attention mechanism and graph convolution in the proposed S2TAT block; 2) the timewise graph convolution and multihead mechanism designed can handle the heterogeneity of data; and 3) we introduce a novel attention-based strategy in the output module, being able to capture more valuable historical information to overcome the shortcoming of conventional average aggregation. Extensive experiments are conducted on PeMS datasets that demonstrate the efficacy of the S2TAT by achieving a top-one accuracy but less computational cost by comparing with the state of the art.},
  archive      = {J_TNNLS},
  author       = {Tian Wang and Jiahui Chen and Jinhu Lü and Kexin Liu and Aichun Zhu and Hichem Snoussi and Baochang Zhang},
  doi          = {10.1109/TNNLS.2022.3169488},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10589-10599},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronous spatiotemporal graph transformer: A new framework for traffic data prediction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Suboptimal leader-to-coordination control for nonlinear
systems with switching topologies: A learning-based method.
<em>TNNLS</em>, <em>34</em>(12), 10578–10588. (<a
href="https://doi.org/10.1109/TNNLS.2022.3169417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the cooperative control for multiagent systems (MASs), the key issues of distributed interaction, nonlinear characteristics, and optimization should be considered simultaneously, which, however, remain intractable theoretically even to this day. Considering these factors, this article investigates leader-to-formation control and optimization for nonlinear MASs using a learning-based method. Under time-varying switching topology, a fully distributed state observer based on neural networks is designed to reconstruct the dynamics and the state trajectory of the leader signal with arbitrary precision under jointly connected topology assumption. Benefitted from the observers, formation for MASs under switching topologies is transformed into tracking control for each subsystem with continuous state generated by the observers. An augmented system with discounted infinite LQR performance index is considered to optimize the control effect. Due to the complexity of solving the Hamilton–Jacobi–Bellman equation, the optimal value function is approximated by a critic network via the integral reinforcement learning method without the knowledge of drift dynamics. Meanwhile, an actor network is also presented to assure stability. The tracking errors and estimation weighted matrices are proven to be uniformly ultimately bounded. Finally, two illustrative examples are given to show the effectiveness of this method.},
  archive      = {J_TNNLS},
  author       = {Shengbo Wang and Shiping Wen and Yin Yang and Kaibo Shi and Tingwen Huang},
  doi          = {10.1109/TNNLS.2022.3169417},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10578-10588},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Suboptimal leader-to-coordination control for nonlinear systems with switching topologies: A learning-based method},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-based continuous inverse optimal control.
<em>TNNLS</em>, <em>34</em>(12), 10563–10577. (<a
href="https://doi.org/10.1109/TNNLS.2022.3168795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of continuous inverse optimal control (over finite time horizon) is to learn the unknown cost function over the sequence of continuous control variables from expert demonstrations. In this article, we study this fundamental problem in the framework of energy-based model (EBM), where the observed expert trajectories are assumed to be random samples from a probability density function defined as the exponential of the negative cost function up to a normalizing constant. The parameters of the cost function are learned by maximum likelihood via an “analysis by synthesis” scheme, which iterates: 1) synthesis step: sample the synthesized trajectories from the current probability density using the Langevin dynamics via backpropagation through time and 2) analysis step: update the model parameters based on the statistical difference between the synthesized trajectories and the observed trajectories. Given the fact that an efficient optimization algorithm is usually available for an optimal control problem, we also consider a convenient approximation of the above learning method, where we replace the sampling in the synthesis step by optimization. Moreover, to make the sampling or optimization more efficient, we propose to train the EBM simultaneously with a top-down trajectory generator via cooperative learning, where the trajectory generator is used to fast initialize the synthesis step of the EBM. We demonstrate the proposed methods on autonomous driving tasks and show that they can learn suitable cost functions for optimal control.},
  archive      = {J_TNNLS},
  author       = {Yifei Xu and Jianwen Xie and Tianyang Zhao and Chris Baker and Yibiao Zhao and Ying Nian Wu},
  doi          = {10.1109/TNNLS.2022.3168795},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10563-10577},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Energy-based continuous inverse optimal control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning from multiple noisy annotators as a union.
<em>TNNLS</em>, <em>34</em>(12), 10552–10562. (<a
href="https://doi.org/10.1109/TNNLS.2022.3168696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing is a popular solution for large-scale data annotations. So far, various end-to-end deep learning methods have been proposed to improve the practical performance of learning from crowds. Despite their practical effectiveness, most of them have two major limitations—they do not hold learning consistency and suffer from computational inefficiency. In this article, we propose a novel method named UnionNet, which is not only theoretically consistent but also experimentally effective and efficient. Specifically, unlike existing methods that either fit a given label from each annotator independently or fuse all the labels into a reliable one, we concatenate the one-hot encoded vectors of crowdsourced labels provided by all the annotators, which takes all the labeling information as a union and coordinates multiple annotators. In this way, we can directly train an end-to-end deep neural network by maximizing the likelihood of this union with only a parametric transition matrix. We theoretically prove the learning consistency and experimentally show the effectiveness and efficiency of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Hongxin Wei and Renchunzi Xie and Lei Feng and Bo Han and Bo An},
  doi          = {10.1109/TNNLS.2022.3168696},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10552-10562},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning from multiple noisy annotators as a union},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curvature consistent network for microscope chip image
super-resolution. <em>TNNLS</em>, <em>34</em>(12), 10538–10551. (<a
href="https://doi.org/10.1109/TNNLS.2022.3168540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting hardware Trojan (HT) from a microscope chip image (MCI) is crucial for many applications, such as financial infrastructure and transport security. It takes an inordinate cost in scanning high-resolution (HR) microscope images for HT detection. It is useful when the chip image is in low-resolution (LR), which can be acquired faster and at a lower cost than its HR counterpart. However, the lost details and noises due to the electric charge effect in LR MCIs will affect the detection performance, making the problem more challenging. In this article, we address this issue by first discussing why recovering curvature information matters for HT detection and then proposing a novel MCI super-resolution (SR) method via a curvature consistent network (CCN). It consists of a homogeneous workflow and a heterogeneous workflow, where the former learns a mapping between homogeneous images, i.e., LR and HR MCIs, and the latter learns a mapping between heterogeneous images, i.e., MCIs and curvature images. Besides, a collaborative fusion strategy is used to leverage features learned from both workflows level-by-level by recovering the HR image eventually. To mitigate the issue of lacking an MCI dataset, we construct a new benchmark consisting of realistic MCIs at different resolutions, called MCI. Experiments on MCI demonstrate that the proposed CCN outperforms representative SR methods by recovering more delicate circuit lines and yields higher HT detection performance. The dataset is available at github.com/RuiZhang97/CCN.},
  archive      = {J_TNNLS},
  author       = {Mingjin Zhang and Jingwei Xin and Jing Zhang and Dacheng Tao and Xinbo Gao},
  doi          = {10.1109/TNNLS.2022.3168540},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10538-10551},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Curvature consistent network for microscope chip image super-resolution},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Keyword-based diverse image retrieval with variational
multiple instance graph. <em>TNNLS</em>, <em>34</em>(12), 10528–10537.
(<a href="https://doi.org/10.1109/TNNLS.2022.3168431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of cross-modal image retrieval has recently attracted considerable research attention. In real-world scenarios, keyword-based queries issued by users are usually short and have broad semantics. Therefore, semantic diversity is as important as retrieval accuracy in such user-oriented services, which improves user experience. However, most typical cross-modal image retrieval methods based on single point query embedding inevitably result in low semantic diversity, while existing diverse retrieval approaches frequently lead to low accuracy due to a lack of cross-modal understanding. To address this challenge, we introduce an end-to-end solution termed variational multiple instance graph (VMIG), in which a continuous semantic space is learned to capture diverse query semantics, and the retrieval task is formulated as a multiple instance learning problems to connect diverse features across modalities. Specifically, a query-guided variational autoencoder is employed to model the continuous semantic space instead of learning a single-point embedding. Afterward, multiple instances of the image and query are obtained by sampling in the continuous semantic space and applying multihead attention, respectively. Thereafter, an instance graph is constructed to remove noisy instances and align cross-modal semantics. Finally, heterogeneous modalities are robustly fused under multiple losses. Extensive experiments on two real-world datasets have well verified the effectiveness of our proposed solution in both retrieval accuracy and semantic diversity.},
  archive      = {J_TNNLS},
  author       = {Yawen Zeng and Yiru Wang and Dongliang Liao and Gongfu Li and Weijie Huang and Jin Xu and Da Cao and Hong Man},
  doi          = {10.1109/TNNLS.2022.3168431},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10528-10537},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Keyword-based diverse image retrieval with variational multiple instance graph},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PicassoNet: Searching adaptive architecture for efficient
facial landmark localization. <em>TNNLS</em>, <em>34</em>(12),
10516–10527. (<a
href="https://doi.org/10.1109/TNNLS.2022.3167743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since recent facial landmark localization methods achieve satisfying accuracy, few of them enable fast inference speed, which, however, is critical in many real-world facial applications. Existing methods typically employ complicated network structure and predict all the key points through uniform computation, which is inefficient since individual facial part might take different computation to obtain the best performance. Taking both accuracy and efficiency into consideration, we propose the PicassoNet, a lightweight cascaded facial landmark detector with adaptive computation for individual facial part. Different from the conventional cascaded methods, PicassoNet integrates refinement submodules into a single network with group convolution, where each convolution group predicts landmarks from an individual facial part. Note that the groups’ structures are flexible in the training process. Then, a novel grouping search algorithm is proposed to optimize the group division. With formulating the optimization as a network architecture search (NAS) problem, the grouping search adaptively allocates computation to each group and obtains an efficient structure. In addition, we propose a boundary-aware loss to optimize along tangent and normal of facial boundaries, instead of optimizing along horizontal and vertical as the conventional loss (L2, SmoothL1, WingLoss, and so on) do. The novel loss improves the joint locations of predicted keypoints. Experiments on three benchmark datasets AFLW, 300W, and WFLW show that the proposed method runs over $6\times $ times faster than the state of the arts and meanwhile achieves comparable accuracy.},
  archive      = {J_TNNLS},
  author       = {Tiancheng Wen and Zhonggan Ding and Yongqiang Yao and Yaxiong Wang and Xueming Qian},
  doi          = {10.1109/TNNLS.2022.3167743},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10516-10527},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PicassoNet: Searching adaptive architecture for efficient facial landmark localization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). IFL-GAN: Improved federated learning generative adversarial
network with maximum mean discrepancy model aggregation. <em>TNNLS</em>,
<em>34</em>(12), 10502–10515. (<a
href="https://doi.org/10.1109/TNNLS.2022.3167482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generative adversarial network (GAN) is usually built from the centralized, independent identically distributed (i.i.d.) training data to generate realistic-like instances. In real-world applications, however, the data may be distributed over multiple clients and hard to be gathered due to bandwidth, departmental coordination, or storage concerns. Although existing works, such as federated learning GAN (FL-GAN), adopt different distributed strategies to train GAN models, there are still limitations when data are distributed in a non-i.i.d. manner. These studies suffer from convergence difficulty, producing generated data with low quality. Fortunately, we found that these challenges are often due to the use of a federated averaging strategy to aggregate local GAN models’ updates. In this article, we propose an alternative approach to tackling this problem, which learns a globally shared GAN model by aggregating locally trained generators’ updates with maximum mean discrepancy (MMD). In this way, we term our approach improved FL-GAN (IFL-GAN). The MMD score helps each local GAN hold different weights, making the global GAN in IFL-GAN getting converged more rapidly than federated averaging. Extensive experiments on MNIST, CIFAR10, and SVHN datasets demonstrate the significant improvement of our IFL-GAN in both achieving the highest inception score and producing high-quality instances.},
  archive      = {J_TNNLS},
  author       = {Wei Li and Jinlin Chen and Zhenyu Wang and Zhidong Shen and Chao Ma and Xiaohui Cui},
  doi          = {10.1109/TNNLS.2022.3167482},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10502-10515},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IFL-GAN: Improved federated learning generative adversarial network with maximum mean discrepancy model aggregation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FastESN: Fast echo state network. <em>TNNLS</em>,
<em>34</em>(12), 10487–10501. (<a
href="https://doi.org/10.1109/TNNLS.2022.3167466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo state networks (ESNs) are reservoir computing-based recurrent neural networks widely used in pattern analysis and machine intelligence applications. In order to achieve high accuracy with large model capacity, ESNs usually contain a large-sized internal layer (reservoir), making the evaluation process too slow for some applications. In this work, we speed up the evaluation of ESN by building a reduced network called the fast ESN (fastESN) and achieve an ESN evaluation complexity independent of the original ESN size for the first time. FastESN is generated using three techniques. First, the high-dimensional state of the original ESN is approximated by a low-dimensional state through proper orthogonal decomposition (POD)-based projection. Second, the activation function evaluation number is reduced through the discrete empirical interpolation method (DEIM). Third, we show the directly generated fastESN has instability problems and provide a stabilization scheme as a solution. Through experiments on four popular benchmarks, we show that fastESN is able to accelerate the sparse storage-based ESN evaluation with a high parameter compression ratio and a fast evaluation speed.},
  archive      = {J_TNNLS},
  author       = {Hai Wang and Xingyi Long and Xue-Xin Liu},
  doi          = {10.1109/TNNLS.2022.3167466},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10487-10501},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FastESN: Fast echo state network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning is singular, and that’s good. <em>TNNLS</em>,
<em>34</em>(12), 10473–10486. (<a
href="https://doi.org/10.1109/TNNLS.2022.3167409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In singular models, the optimal set of parameters forms an analytic set with singularities, and a classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular, and thus, “dividing” by the determinant of the Hessian or employing the Laplace approximation is not appropriate. Despite its potential for addressing fundamental issues in deep learning, a singular learning theory appears to have made little inroads into the developing canon of a deep learning theory. Via a mix of theory and experiment, we present an invitation to the singular learning theory as a vehicle for understanding deep learning and suggest an important future work to make the singular learning theory directly applicable to how deep learning is performed in practice.},
  archive      = {J_TNNLS},
  author       = {Susan Wei and Daniel Murfet and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},
  doi          = {10.1109/TNNLS.2022.3167409},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10473-10486},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning is singular, and that’s good},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust passivity and control for directed and multiweighted
coupled dynamical networks. <em>TNNLS</em>, <em>34</em>(12),
10458–10472. (<a
href="https://doi.org/10.1109/TNNLS.2022.3167139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the passivity and control issues for directed uncertain coupled dynamical networks are solved. The presented model is directly coupled with multiple coupling matrices and parametric uncertainty, while previous literatures of multiweighted networks usually suppose that outer coupling matrices (OMs) are connected, undirected, and certain. The viewpoint of inner coupling matrices (IMs) in this article is added and OMs can be directed and not connected, which is a great improvement on the existing results. First, for all diagonal IMs, considering each dimension separately, we can derive if the weighted combination of multiple OMs for each dimension is strongly connected, then passivity and pinning control rules can be established. In addition, we also discuss the situation that IMs are positive definite but not diagonal. By means of the weighted combination of normalized left eigenvectors (NLEVec) corresponding to zero eigenvalue for multiple coupling matrices, we prove if the Chebyshev distance (Cheb-Dist) among these NLEVec is less than a tolerant deviation interval, then passivity, synchronization, and pinning control criteria are acquired. Moreover, a matter of adaptive coupling strengths is also settled. Examples are provided to verify the validity of established results.},
  archive      = {J_TNNLS},
  author       = {Shanrong Lin and Xiwei Liu},
  doi          = {10.1109/TNNLS.2022.3167139},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10458-10472},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust passivity and control for directed and multiweighted coupled dynamical networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuroadaptive fault-tolerant control with guaranteed
performance for euler–lagrange systems under dying power faults.
<em>TNNLS</em>, <em>34</em>(12), 10447–10457. (<a
href="https://doi.org/10.1109/TNNLS.2022.3166963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the tracking control problem for Euler–Lagrange (EL) systems subject to output constraints and extreme actuation/propulsion failures. The goal here is to design a neural network (NN)-based controller capable of guaranteeing satisfactory tracking control performance even if some of the actuators completely fail to work. This is achieved by introducing a novel fault function and rate function such that, with which the original tracking control problem is converted into a stabilization one. It is shown that the tracking error is ensured to converge to a pre-specified compact set within a given finite time and the decay rate of the tracking error can be user-designed in advance. The extreme actuation faults and the standby actuator handover time delay are explicitly addressed, and the closed signals are ensured to be globally uniformly ultimately bounded. The effectiveness of the proposed method has been confirmed through both theoretical analysis and numerical simulation.},
  archive      = {J_TNNLS},
  author       = {Zhen Gao and Yujuan Wang},
  doi          = {10.1109/TNNLS.2022.3166963},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10447-10457},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuroadaptive fault-tolerant control with guaranteed performance for Euler–Lagrange systems under dying power faults},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correntropy-based low-rank matrix factorization with
constraint graph learning for image clustering. <em>TNNLS</em>,
<em>34</em>(12), 10433–10446. (<a
href="https://doi.org/10.1109/TNNLS.2022.3166931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel low-rank matrix factorization model for semisupervised image clustering. In order to alleviate the negative effect of outliers, the maximum correntropy criterion (MCC) is incorporated as a metric to build the model. To utilize the label information to improve the clustering results, a constraint graph learning framework is proposed to adaptively learn the local structure of the data by considering the label information. Furthermore, an iterative algorithm based on Fenchel conjugate (FC) and block coordinate update (BCU) is proposed to solve the model. The convergence properties of the proposed algorithm are analyzed, which shows that the algorithm exhibits both objective sequential convergence and iterate sequential convergence. Experiments are conducted on six real-world image datasets, and the proposed algorithm is compared with eight state-of-the-art methods. The results show that the proposed method can achieve better performance in most situations in terms of clustering accuracy and mutual information.},
  archive      = {J_TNNLS},
  author       = {Nan Zhou and Kup-Sze Choi and Badong Chen and Yuanhua Du and Jun Liu and Yangyang Xu},
  doi          = {10.1109/TNNLS.2022.3166931},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10433-10446},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Correntropy-based low-rank matrix factorization with constraint graph learning for image clustering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Skeleton-based human motion prediction with privileged
supervision. <em>TNNLS</em>, <em>34</em>(12), 10419–10432. (<a
href="https://doi.org/10.1109/TNNLS.2022.3166861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing supervised methods have achieved impressive performance in forecasting skeleton-based human motion. However, they often rely on action class labels in both training and inference phases. In practice, it could be a burden to request action class labels in the inference phase, and even for the training phase, the collected labels could be incomplete for sequences with a mixture of multiple actions. In this article, we take action class labels as a kind of privileged supervision that only exists in the training phase. We design a new architecture that includes a motion classification as an auxiliary task with motion prediction. To deal with potential missing labels of motion sequence, we propose a new classification loss function to exploit their relationships with those observed labels and a perceptual loss to measure the difference between ground truth sequence and generated sequence in the classification task. Experimental results on the most challenging Human 3.6M dataset and the Carnegie Mellon University (CMU) dataset demonstrate the effectiveness of the proposed algorithm to exploit action class labels for improved modeling of human dynamics.},
  archive      = {J_TNNLS},
  author       = {Minjing Dong and Chang Xu},
  doi          = {10.1109/TNNLS.2022.3166861},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10419-10432},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Skeleton-based human motion prediction with privileged supervision},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully parallel stochastic computing hardware implementation
of convolutional neural networks for edge computing applications.
<em>TNNLS</em>, <em>34</em>(12), 10408–10418. (<a
href="https://doi.org/10.1109/TNNLS.2022.3166799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge artificial intelligence (AI) is receiving a tremendous amount of interest from the machine learning community due to the ever-increasing popularization of the Internet of Things (IoT). Unfortunately, the incorporation of AI characteristics to edge computing devices presents the drawbacks of being power and area hungry for typical deep learning techniques such as convolutional neural networks (CNNs). In this work, we propose a power-and-area efficient architecture based on the exploitation of the correlation phenomenon in stochastic computing (SC) systems. The proposed architecture solves the challenges that a CNN implementation with SC (SC-CNN) may present, such as the high resources used in binary-to-stochastic conversion, the inaccuracy produced by undesired correlation between signals, and the complexity of the stochastic maximum function implementation. To prove that our architecture meets the requirements of edge intelligence realization, we embed a fully parallel CNN in a single field-programmable gate array (FPGA) chip. The results obtained showed a better performance than traditional binary logic and other SC implementations. In addition, we performed a full VLSI synthesis of the proposed design, showing that it presents better overall characteristics than other recently published VLSI architectures.},
  archive      = {J_TNNLS},
  author       = {Christiam F. Frasser and Pablo Linares-Serrano and Iván Díez de los Ríos and Alejandro Morán and Erik S. Skibinsky-Gitlin and Joan Font-Rosselló and Vincent Canals and Miquel Roca and Teresa Serrano-Gotarredona and Josep L. Rosselló},
  doi          = {10.1109/TNNLS.2022.3166799},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10408-10418},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully parallel stochastic computing hardware implementation of convolutional neural networks for edge computing applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving tracking accuracy for repetitive learning systems
by high-order extended state observers. <em>TNNLS</em>, <em>34</em>(12),
10398–10407. (<a
href="https://doi.org/10.1109/TNNLS.2022.3166797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For systems executing repetitive tasks, how to realize the perfect tracking objective is generally desirable, for which an effective method called “iterative learning control (ILC)” emerges thanks to the incorporation of the repetitive execution of systems into an ILC design framework. However, nonrepetitive (iteration-varying) uncertainties are often inevitable in practice and greatly degrade the tracking accuracy of ILC, which has not been treated well, regardless of considerable robust ILC results. This motivates this article to develop a new design method to improve the tracking accuracy of ILC by adopting a high-order extended state observer (ESO) to address ill effects of nonrepetitive uncertainties and uncertain system models. With the designed ESO-based ILC, the robust tracking of any desired trajectory can be achieved such that the tracking error can be decreased to vary in a small bound depending continuously on the bounds of high-order variations of nonrepetitive uncertainties with respect to the iteration. It makes the tracking accuracy of ILC possible to be regulated through the design of ESO, of which the validity is demonstrated by including a simulation example.},
  archive      = {J_TNNLS},
  author       = {Jingyao Zhang and Deyuan Meng},
  doi          = {10.1109/TNNLS.2022.3166797},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10398-10407},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving tracking accuracy for repetitive learning systems by high-order extended state observers},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-based finite-time neural control for human-in-the-loop
UAV attitude systems. <em>TNNLS</em>, <em>34</em>(12), 10387–10397. (<a
href="https://doi.org/10.1109/TNNLS.2022.3166531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the event-based finite-time neural attitude consensus control problem for the six-rotor unmanned aerial vehicle (UAV) systems with unknown disturbances. It is assumed that the six-rotor UAV systems are controlled by a human operator sending command signals to the leader. A disturbance observer and radial basis function neural networks (RBF NNs) are applied to address the problems regarding external disturbances and uncertain nonlinear dynamics, respectively. In addition, the proposed finite-time command filtered (FTCF) backstepping method effectively manages the issue of “explosion of complexity,” where filtering errors are eliminated by the error compensation mechanism. In addition, an event-triggered mechanism is considered to alleviate the communication burden between the controller and the actuator in practice. It is shown that all signals of the six-rotor UAV systems are bounded and the consensus errors converge to a small neighborhood of the origin in finite time. Finally, the simulation results demonstrate the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Guohuai Lin and Hongyi Li and Choon Ki Ahn and Deyin Yao},
  doi          = {10.1109/TNNLS.2022.3166531},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10387-10397},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-based finite-time neural control for human-in-the-loop UAV attitude systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model pruning enables efficient federated learning on edge
devices. <em>TNNLS</em>, <em>34</em>(12), 10374–10386. (<a
href="https://doi.org/10.1109/TNNLS.2022.3166101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) allows model training from local data collected by edge/mobile devices while preserving data privacy, which has wide applicability to image and vision applications. A challenge is that client devices in FL usually have much more limited computation and communication resources compared to servers in a data center. To overcome this challenge, we propose PruneFL —a novel FL approach with adaptive and distributed parameter pruning, which adapts the model size during FL to reduce both communication and computation overhead and minimize the overall training time, while maintaining a similar accuracy as the original model. PruneFL includes initial pruning at a selected client and further pruning as part of the FL process. The model size is adapted during this process, which includes maximizing the approximate empirical risk reduction divided by the time of one FL round. Our experiments with various datasets on edge devices (e.g., Raspberry Pi) show that: 1) we significantly reduce the training time compared to conventional FL and various other pruning-based methods and 2) the pruned model with automatically determined size converges to an accuracy that is very similar to the original model, and it is also a lottery ticket of the original model.},
  archive      = {J_TNNLS},
  author       = {Yuang Jiang and Shiqiang Wang and Víctor Valls and Bong Jun Ko and Wei-Han Lee and Kin K. Leung and Leandros Tassiulas},
  doi          = {10.1109/TNNLS.2022.3166101},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10374-10386},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model pruning enables efficient federated learning on edge devices},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partial consistency for stabilizing undiscounted
reinforcement learning. <em>TNNLS</em>, <em>34</em>(12), 10359–10373.
(<a href="https://doi.org/10.1109/TNNLS.2022.3165941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Undiscounted return is an important setup in reinforcement learning (RL) and characterizes many real-world problems. However, optimizing an undiscounted return often causes training instability. The causes of this instability problem have not been analyzed in-depth by existing studies. In this article, this problem is analyzed from the perspective of value estimation. The analysis result indicates that the instability originates from transient traps that are caused by inconsistently selected actions. However, selecting one consistent action in the same state limits exploration. For balancing exploration effectiveness and training stability, a novel sampling method called last-visit sampling (LVS) is proposed to ensure that a part of actions is selected consistently in the same state. The LVS method decomposes the state-action value into two parts, i.e., the last-visit (LV) value and the revisit value. The decomposition ensures that the LV value is determined by consistently selected actions. We prove that the LVS method can eliminate transient traps while preserving optimality. Also, we empirically show that the method can stabilize the training processes of five typical tasks, including vision-based navigation and manipulation tasks.},
  archive      = {J_TNNLS},
  author       = {Haichuan Gao and Zhile Yang and Tian Tan and Tianren Zhang and Jinsheng Ren and Pengfei Sun and Shangqi Guo and Feng Chen},
  doi          = {10.1109/TNNLS.2022.3165941},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10359-10373},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Partial consistency for stabilizing undiscounted reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive fixed-time neural control for uncertain nonlinear
multiagent systems. <em>TNNLS</em>, <em>34</em>(12), 10346–10358. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the problem of adaptive fixed-time tracking control for a class of multiagent systems (MASs) with mismatched uncertainty. Unlike the existing methodologies that only implement the practical finite-/fixed-time stability for MASs, a newly adaptive consensus control criterion is developed to reach fixed-time stability, where the controller design includes a series of newly Lyavonov functions and modified tuning functions. Radial basis function neural networks are employed to deal with the unknown functions in each agent, and the direct adaptive strategy solves the obstacle of “explosion of complexity.” Under the performance-oriented controller, the error of the MASs converges to a predetermined interval within a fixed time. Two simulations illustrate the results obtained.},
  archive      = {J_TNNLS},
  author       = {Chengjie Huang and Zhi Liu and C. L. Philip Chen and Yun Zhang},
  doi          = {10.1109/TNNLS.2022.3165836},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10346-10358},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive fixed-time neural control for uncertain nonlinear multiagent systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining deep graph networks via input perturbation.
<em>TNNLS</em>, <em>34</em>(12), 10334–10345. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep graph networks (DGNs) are a family of machine learning models for structured data which are finding heavy application in life sciences (drug repurposing, molecular property predictions) and on social network data (recommendation systems). The privacy and safety-critical nature of such domains motivates the need for developing effective explainability methods for this family of models. So far, progress in this field has been challenged by the combinatorial nature and complexity of graph structures. In this respect, we present a novel local explanation framework specifically tailored to graph data and DGNs. Our approach leverages reinforcement learning to generate meaningful local perturbations of the input graph, whose prediction we seek an interpretation for. These perturbed data points are obtained by optimizing a multiobjective score taking into account similarities both at a structural level as well as at the level of the deep model outputs. By this means, we are able to populate a set of informative neighboring samples for the query graph, which is then used to fit an interpretable model for the predictive behavior of the deep network locally to the query graph prediction. We show the effectiveness of the proposed explainer by a qualitative analysis on two chemistry datasets, TOX21 and Estimated SOLubility (ESOL) and by quantitative results on a benchmark dataset for explanations, CYCLIQ.},
  archive      = {J_TNNLS},
  author       = {Davide Bacciu and Danilo Numeroso},
  doi          = {10.1109/TNNLS.2022.3165618},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10334-10345},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Explaining deep graph networks via input perturbation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple guidance network for industrial product surface
inspection with one labeled target sample. <em>TNNLS</em>,
<em>34</em>(12), 10324–10333. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most automatic product surface inspection methods in industry are data-hungry and task-specific. It is difficult to collect adequate labeled samples in practice due to factors including expensive data annotation cost, inadequate samples for some categories, and limitations on the initial production stage. In this article, a multiple guidance network (MGNet) is proposed to address these issues. In the network, the feature extraction machine (FEM) produces four feature maps of different functions to enhance the inspection ability of the algorithm. Also, the probability map generation (PMG) module is designed for coarse positioning of objects. Moreover, the structures of the mutual guidance and historical guidance (HG) guarantee that the network can fully utilize the information of the auxiliary dataset. Only one support sample containing the labeled objects is required for reference, and the network can determine whether the same labeled objects exist in the query images and locate them. For a comprehensive evaluation of MGNet, three experiments are carried out using three real-world datasets. Experiment results verify that the proposed method is promising for industrial product surface inspection with one labeled target sample.},
  archive      = {J_TNNLS},
  author       = {Jiaming Xu and Yu Liu},
  doi          = {10.1109/TNNLS.2022.3165575},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10324-10333},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiple guidance network for industrial product surface inspection with one labeled target sample},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). DR-GAN: Distribution regularization for text-to-image
generation. <em>TNNLS</em>, <em>34</em>(12), 10309–10323. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new text-to-image (T2I) generation model, named distribution regularization generative adversarial network (DR-GAN), to generate images from text descriptions from improved distribution learning. In DR-GAN, we introduce two novel modules: a semantic disentangling module (SDM) and a distribution normalization module (DNM). SDM combines the spatial self-attention mechanism (SSAM) and a new semantic disentangling loss (SDL) to help the generator distill key semantic information for the image generation. DNM uses a variational auto-encoder (VAE) to normalize and denoise the image latent distribution, which can help the discriminator better distinguish synthesized images from real images. DNM also adopts a distribution adversarial loss (DAL) to guide the generator to align with normalized real image distributions in the latent space. Extensive experiments on two public datasets demonstrated that our DR-GAN achieved a competitive performance in the T2I task. The code link: https://github.com/Tan-H-C/DR-GAN-Distribution-Regularization-for-Text-to-Image-Generation .},
  archive      = {J_TNNLS},
  author       = {Hongchen Tan and Xiuping Liu and Baocai Yin and Xin Li},
  doi          = {10.1109/TNNLS.2022.3165573},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10309-10323},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DR-GAN: Distribution regularization for text-to-image generation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A spatial–temporal graph model for pronunciation feature
prediction of chinese poetry. <em>TNNLS</em>, <em>34</em>(12),
10294–10308. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of artificial intelligence, speech recognition and prediction have become one of the important research domains with wild applications, such as intelligent control, education, individual identification, and emotion analysis. Chinese poetry reading contains rich features of continuous pronunciations, such as mood, emotion, rhythm schemes, lyric reading, and artistic expression. Therefore, the prediction of the pronunciation characteristics of a Chinese poetry reading is the significance for the presentation of high-level machine intelligence and has the potential to create a high-level intelligent system for teaching children to read Tang poetry. Mel frequency cepstral coefficient (MFCC) is currently used to present important speech features. Due to the complexity and high degree of nonlinearity in poetry reading, however, there is a tough challenge facing accurate pronunciation feature prediction, that is, how to model complex spatial correlations and time dynamics, such as rhyme schemes. As for many current methods, they ignore the spatial and temporal characteristics in MFCC presentation. In addition, these methods are subjected to certain limitations on prediction for long-term performance. In order to solve these problems, we propose a novel spatial–temporal graph model (STGM-MHA) based on multihead attention for the purpose of pronunciation feature prediction of Chinese poetry. The STGM-MHA is designed using an encoder–decoder structure. The encoder compresses the data into a hidden space representation, while the decoder reconstructs the hidden space representation as output. In the model, a novel gated recurrent unit (GRU) module (AGRU) based on multihead attention is proposed to extract the spatial and temporal features of MFCC data effectively. The evaluation comparison of our proposed model versus state-of-the-art methods in six datasets reveals the clear advantage of the proposed model.},
  archive      = {J_TNNLS},
  author       = {Qing Wang and Weiping Liu and Xiumei Wang and Xinghong Chen and Guannan Chen and Qingxiang Wu},
  doi          = {10.1109/TNNLS.2022.3165554},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10294-10308},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A Spatial–Temporal graph model for pronunciation feature prediction of chinese poetry},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiview subspace clustering with multilevel
representations and adversarial regularization. <em>TNNLS</em>,
<em>34</em>(12), 10279–10293. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview subspace clustering has turned into a promising technique due to its encouraging ability to discover the underlying subspace structure. In recent studies, a lot of subspace clustering methods have been developed to strengthen the clustering performance of multiview data, but these methods rarely consider simultaneously the nonlinear structure and multilevel representation (MLR) information in multiview data as well as the data distribution of latent representation. To address these problems, we develop a new Multiview Subspace Clustering with MLRs and Adversarial Regularization (MvSC-MRAR), where multiple deep auto-encoders are utilized to model nonlinear structure information of multiview data, multiple self-expressive layers are introduced into each deep auto-encoder to extract multilevel latent representations of each view data, and diversity regularizations are designed to preserve complementary information contained in different layers and different views. Furthermore, a universal discriminator based on adversarial training is developed to enforce the output of each encoder to obey a given prior distribution, so that the affinity matrix for spectral clustering (SPC) is more realistic. Comprehensive empirical evaluation with nine real-world multiview datasets indicates that our proposed MvSC-MRAR achieves significant improvements than several state-of-the-art methods in terms of clustering accuracy (ACC) and normalized mutual information (NMI).},
  archive      = {J_TNNLS},
  author       = {Guowang Du and Lihua Zhou and Kevin Lü and Hao Wu and Zhimin Xu},
  doi          = {10.1109/TNNLS.2022.3165542},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10279-10293},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview subspace clustering with multilevel representations and adversarial regularization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting sparse self-representation and particle swarm
optimization for CNN compression. <em>TNNLS</em>, <em>34</em>(12),
10266–10278. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured pruning has received ever-increasing attention as a method for compressing convolutional neural networks. However, most existing methods directly prune the network structure according to the statistical information of the parameters. Besides, these methods differentiate the pruning rates only in each pruning stage or even use the same pruning rate across all layers, rather than using learnable parameters. In this article, we propose a network redundancy elimination approach guided by the pruned model. Our proposed method can easily tackle multiple architectures and is scalable to the deeper neural networks because of the use of joint optimization during the pruning procedure. More specifically, we first construct a sparse self-representation for the filters or neurons of the well-trained model, which is useful for analyzing the relationship among filters. Then, we employ particle swarm optimization to learn pruning rates in a layerwise manner according to the performance of the pruned model, which can determine optimal pruning rates with the best performance of the pruned model. Under this criterion, the proposed pruning approach can remove more parameters without undermining the performance of the model. Experimental results demonstrate the effectiveness of our proposed method on different datasets and different architectures. For example, it can reduce 58.1\% FLOPs for ResNet50 on ImageNet with only a 1.6\% top-five error increase and 44.1\% FLOPs for FCN_ResNet50 on COCO2017 with a 3\% error increase, outperforming most state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Sijie Niu and Kun Gao and Pengfei Ma and Xizhan Gao and Hui Zhao and Jiwen Dong and Yuehui Chen and Dinggang Shen},
  doi          = {10.1109/TNNLS.2022.3165530},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10266-10278},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploiting sparse self-representation and particle swarm optimization for CNN compression},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving multispike learning with plastic synaptic delays.
<em>TNNLS</em>, <em>34</em>(12), 10254–10265. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emulating the spike-based processing in the brain, spiking neural networks (SNNs) are developed and act as a promising candidate for the new generation of artificial neural networks that aim to produce efficient cognitions as the brain. Due to the complex dynamics and nonlinearity of SNNs, designing efficient learning algorithms has remained a major difficulty, which attracts great research attention. Most existing ones focus on the adjustment of synaptic weights. However, other components, such as synaptic delays, are found to be adaptive and important in modulating neural behavior. How could plasticity on different components cooperate to improve the learning of SNNs remains as an interesting question. Advancing our previous multispike learning, we propose a new joint weight-delay plasticity rule, named TDP-DL, in this article. Plastic delays are integrated into the learning framework, and as a result, the performance of multispike learning is significantly improved. Simulation results highlight the effectiveness and efficiency of our TDP-DL rule compared to baseline ones. Moreover, we reveal the underlying principle of how synaptic weights and delays cooperate with each other through a synthetic task of interval selectivity and show that plastic delays can enhance the selectivity and flexibility of neurons by shifting information across time. Due to this capability, useful information distributed away in the time domain can be effectively integrated for a better accuracy performance, as highlighted in our generalization tasks of the image, speech, and event-based object recognitions. Our work is thus valuable and significant to improve the performance of spike-based neuromorphic computing.},
  archive      = {J_TNNLS},
  author       = {Qiang Yu and Jialu Gao and Jianguo Wei and Jing Li and Kay Chen Tan and Tiejun Huang},
  doi          = {10.1109/TNNLS.2022.3165527},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10254-10265},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving multispike learning with plastic synaptic delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate and efficient large-scale multi-label learning with
reduced feature broad learning system using label correlation.
<em>TNNLS</em>, <em>34</em>(12), 10240–10253. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning for large-scale data is a grand challenge because of a large number of labels with a complex data structure. Hence, the existing large-scale multi-label methods either have unsatisfactory classification performance or are extremely time-consuming for training utilizing a massive amount of data. A broad learning system (BLS), a flat network with the advantages of succinct structures, is appropriate for addressing large-scale tasks. However, existing BLS models are not directly applicable for large-scale multi-label learning due to the large and complex label space. In this work, a novel multi-label classifier based on BLS (called BLS-MLL) is proposed with two new mechanisms: kernel-based feature reduction module and correlation-based label thresholding. The kernel-based feature reduction module contains three layers, namely, the feature mapping layer, enhancement nodes layer, and feature reduction layer. The feature mapping layer employs elastic network regularization to solve the randomness of features in order to improve performance. In the enhancement nodes layer, the kernel method is applied for high-dimensional nonlinear conversion to achieve high efficiency. The newly constructed feature reduction layer is used to further significantly improve both the training efficiency and accuracy when facing high-dimensionality with abundant or noisy information embedded in large-scale data. The correlation-based label thresholding enables BLS-MLL to generate a label-thresholding function for effective conversion of the final decision values to logical outputs, thus, improving the classification performance. Finally, experimental comparisons among six state-of-the-art multi-label classifiers on ten datasets demonstrate the effectiveness of the proposed BLS-MLL. The results of the classification performance show that BLS-MLL outperforms the compared algorithms in 86\% of cases with better training efficiency in 90\% of cases.},
  archive      = {J_TNNLS},
  author       = {Jintao Huang and Chi-Man Vong and C. L. Philip Chen and Yimin Zhou},
  doi          = {10.1109/TNNLS.2022.3165299},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10240-10253},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accurate and efficient large-scale multi-label learning with reduced feature broad learning system using label correlation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative dictionary pair learning with
scale-constrained structured representation for image classification.
<em>TNNLS</em>, <em>34</em>(12), 10225–10239. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dictionary pair learning (DPL) model aims to design a synthesis dictionary and an analysis dictionary to accomplish the goal of rapid sample encoding. In this article, we propose a novel structured representation learning algorithm based on the DPL for image classification. It is referred to as discriminative DPL with scale-constrained structured representation (DPL-SCSR). The proposed DPL-SCSR utilizes the binary label matrix of dictionary atoms to project the representation into the corresponding label space of the training samples. By imposing a non-negative constraint, the learned representation adaptively approximates a block-diagonal structure. This innovative transformation is also capable of controlling the scale of the block-diagonal representation by enforcing the sum of within-class coefficients of each sample to 1, which means that the dictionary atoms of each class compete to represent the samples from the same class. This implies that the requirement of similarity preservation is considered from the perspective of the constraint on the sum of coefficients. More importantly, the DPL-SCSR does not need to design a classifier in the representation space as the label matrix of the dictionary can also be used as an efficient linear classifier. Finally, the DPL-SCSR imposes the $l_{2,p}$ -norm on the analysis dictionary to make the process of feature extraction more interpretable. The DPL-SCSR seamlessly incorporates the scale-constrained structured representation learning, within-class similarity preservation of representation, and the linear classifier into one regularization term, which dramatically reduces the complexity of training and parameter tuning. The experimental results on several popular image classification datasets show that our DPL-SCSR can deliver superior performance compared with the state-of-the-art (SOTA) dictionary learning methods. The MATLAB code of this article is available at https://github.com/chenzhe207/DPL-SCSR .},
  archive      = {J_TNNLS},
  author       = {Zhe Chen and Xiao-Jun Wu and Tianyang Xu and Josef Kittler},
  doi          = {10.1109/TNNLS.2022.3165217},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10225-10239},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative dictionary pair learning with scale-constrained structured representation for image classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep multimodal complementarity learning. <em>TNNLS</em>,
<em>34</em>(12), 10213–10224. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complementarity plays a significant role in the synergistic effect created by different components of a complex data object. Complementarity learning on multimodal data has fundamental challenges of representation learning because the complementarity exists along with multiple modalities and one or multiple items of each modality. Also, an appropriate metric is needed for measuring the complementarity in the representation space. Existing methods that rely on similarity-based metrics cannot adequately capture the complementarity. In this work, we propose a novel deep architecture for systematically learning the complementarity of components from multimodal multi-item data. The proposed model consists of three major modules: 1) unimodal aggregation for extracting the intramodal complementarity; 2) cross-modal fusion for extracting the intermodal complementarity at the modality level; and 3) interactive aggregation for extracting the intermodal complementarity at the item level. To quantify complementarity, we utilize the TUBE distance metric to measure the difference between the composited data object and its label in the representation space. Experiments on three real datasets show that our model outperforms the state-of-the-art by +6.8\% of mean reciprocal rank (MRR) on object classification and +3.0\% of MRR on hold-out item prediction. Qualitative analyses reveal that complementarity is significantly different from similarity.},
  archive      = {J_TNNLS},
  author       = {Daheng Wang and Tong Zhao and Wenhao Yu and Nitesh V. Chawla and Meng Jiang},
  doi          = {10.1109/TNNLS.2022.3165180},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10213-10224},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep multimodal complementarity learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model compression based on differentiable network channel
pruning. <em>TNNLS</em>, <em>34</em>(12), 10203–10212. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although neural networks have achieved great success in various fields, applications on mobile devices are limited by the computational and storage costs required for large models. The model compression (neural network pruning) technology can significantly reduce network parameters and improve computational efficiency. In this article, we propose a differentiable network channel pruning (DNCP) method for model compression. Unlike existing methods that require sampling and evaluation of a large number of substructures, our method can efficiently search for optimal substructure that meets resource constraints (e.g., FLOPs) through gradient descent. Specifically, we assign a learnable probability to each possible number of channels in each layer of the network, relax the selection of a particular number of channels to a softmax over all possible numbers of channels, and optimize the learnable probability in an end-to-end manner through gradient descent. After the network parameters are optimized, we prune the network according to the learnable probability to obtain the optimal substructure. To demonstrate the effectiveness and efficiency of DNCP, experiments are conducted with ResNet and MobileNet V2 on CIFAR, Tiny ImageNet, and ImageNet datasets.},
  archive      = {J_TNNLS},
  author       = {Yu-Jie Zheng and Si-Bao Chen and Chris H. Q. Ding and Bin Luo},
  doi          = {10.1109/TNNLS.2022.3165123},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10203-10212},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model compression based on differentiable network channel pruning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Fully decentralized multiagent communication via causal
inference. <em>TNNLS</em>, <em>34</em>(12), 10193–10202. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world tasks can be cast into multiagent (MA) reinforcement learning problems, and most algorithms in this field obey to the centralized learning and decentralized execution framework. However, enforcing centralized learning is impractical in many scenarios. Because it requires integrating the information from agents, while agents may not hope to share local information due to the issue of privacy. Thus, this article proposes a novel approach to achieve fully decentralized learning based on communication among multiple agents via reinforcement learning. Benefiting from causality analysis, an agent will choose the counterfactual that has the most significant influence on communication information of others. We find that this method can be applied in classic or complex MA scenarios and in federated learning domains, which are now attracting much attention.},
  archive      = {J_TNNLS},
  author       = {Han Wang and Yang Yu and Yuan Jiang},
  doi          = {10.1109/TNNLS.2022.3165114},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10193-10202},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully decentralized multiagent communication via causal inference},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Robust online tensor completion for IoT streaming data
recovery. <em>TNNLS</em>, <em>34</em>(12), 10178–10192. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable data measurement is considered to be one of the critical ingredients for variant Internet of Things (IoT) applications. Gaining full knowledge of measurement data is becoming increasingly crucial to ensure a satisfactory user experience. However, data missing and corruption are inevitable in practical applications, which motivates us to study how to accurately recover the missing IoT measurement data in the presence of outliers. The data recovery problem can be formulated as a tensor completion (TC) problem. Existing TC methods are built on the assumption that the rank of the underlying tensor is fixed, which is not suitable for long data sequences in practice. Consequently, based on the characteristics of IoT streaming data, we assume that the data tensor lies in time-varying subspace, and an accurate estimate of the rank is a prerequisite for filling the missing entries and achieving robustness of the variations in both rank and noise. We built up an updatable framework based on dynamic CANDECOMP/PARAFAC (CP) decomposition. In addition, an efficient algorithm, called temporal multi-aspect streaming (T-MUST), is introduced to solve the optimization problem that originates in our developed model. It is worth noting that the proposed algorithm allows time-varying tensor rank and enables the rank changes could be detected and tracked automatically. Theoretical analysis indicates that T-MUST enjoys a geometric convergence rate. Numerical experiments conducted on various synthetic and real-world datasets empirically validate the superiority of the proposed T-MUST in both efficiency and effectiveness.},
  archive      = {J_TNNLS},
  author       = {Chunsheng Liu and Tao Wu and Zhifei Li and Tao Ma and Jun Huang},
  doi          = {10.1109/TNNLS.2022.3165076},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10178-10192},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust online tensor completion for IoT streaming data recovery},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GCRec: Graph-augmented capsule network for next-item
recommendation. <em>TNNLS</em>, <em>34</em>(12), 10164–10177. (<a
href="https://doi.org/10.1109/TNNLS.2022.3164982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-item recommendation has been a hot research, which aims at predicting the next action by modeling users’ behavior sequences. While previous efforts toward this task have been made in capturing complex item transition patterns, we argue that they still suffer from three limitations: 1) they have difficulty in explicitly capturing the impact of inherent order of item transition patterns; 2) only a simple and crude embedding is insufficient to yield satisfactory long-term users’ representations from limited training sequences; and 3) they are incapable of dynamically integrating long-term and short-term user interest modeling. In this work, we propose a novel solution named graph-augmented capsule network (GCRec), which exploits sequential user behaviors in a more fine-grained manner. Specifically, we employ a linear graph convolution module to learn informative long-term representations of users. Furthermore, we devise a user-specific capsule module and a position-aware gating module, which are sensitive to the relative sequential order of the recently interacted items, to capture sequential patterns at union-level and point-level. To aggregate the long-term and short-term user interests as a representative vector, we design a dual-gating mechanism, which could decide the contribution ratio of each module given different contextual information. Through extensive experiments on four benchmarks, we validate the rationality and effectiveness of GCRec on the next-item recommendation task.},
  archive      = {J_TNNLS},
  author       = {Bin Wu and Xiangnan He and Qi Zhang and Meng Wang and Yangdong Ye},
  doi          = {10.1109/TNNLS.2022.3164982},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10164-10177},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GCRec: Graph-augmented capsule network for next-item recommendation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite-time adaptive neural control for a class of nonlinear
systems with asymmetric time-varying full-state constraints.
<em>TNNLS</em>, <em>34</em>(12), 10154–10163. (<a
href="https://doi.org/10.1109/TNNLS.2022.3164948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive finite-time tracking control scheme is developed for a category of uncertain nonlinear systems with asymmetric time-varying full-state constraints and actuator failures. First, in the control design process, the original constrained nonlinear system is transformed into an equivalent “unconstrained” one by using the uniform barrier function (UBF). Then, by introducing a new coordinate transformation and incorporating it into each recursive step of adaptive finite-time control design based on the backstepping technique, more general state constraints can be handled. In addition, since the nonlinear function in the system is unknown, neural network is employed to approximate it. Considering singularity, the virtual control signal is designed as a piecewise function to guarantee the performance of the system within a finite time. The developed finite-time control method ensures that all signals in the closed-loop system are bounded, and the output tracking error converges to a small neighborhood of the origin. At last, the simulation example illustrates the feasibility and superiority of the presented control method.},
  archive      = {J_TNNLS},
  author       = {Yan Zhang and Jian Guo and Zhengrong Xiang},
  doi          = {10.1109/TNNLS.2022.3164948},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10154-10163},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time adaptive neural control for a class of nonlinear systems with asymmetric time-varying full-state constraints},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised learning in multilayer spiking neural networks
with spike temporal error backpropagation. <em>TNNLS</em>,
<em>34</em>(12), 10141–10153. (<a
href="https://doi.org/10.1109/TNNLS.2022.3164930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain-inspired spiking neural networks (SNNs) hold the advantages of lower power consumption and powerful computing capability. However, the lack of effective learning algorithms has obstructed the theoretical advance and applications of SNNs. The majority of the existing learning algorithms for SNNs are based on the synaptic weight adjustment. However, neuroscience findings confirm that synaptic delays can also be modulated to play an important role in the learning process. Here, we propose a gradient descent-based learning algorithm for synaptic delays to enhance the sequential learning performance of single spiking neuron. Moreover, we extend the proposed method to multilayer SNNs with spike temporal-based error backpropagation. In the proposed multilayer learning algorithm, information is encoded in the relative timing of individual neuronal spikes, and learning is performed based on the exact derivatives of the postsynaptic spike times with respect to presynaptic spike times. Experimental results on both synthetic and realistic datasets show significant improvements in learning efficiency and accuracy over the existing spike temporal-based learning algorithms. We also evaluate the proposed learning method in an SNN-based multimodal computational model for audiovisual pattern recognition, and it achieves better performance compared with its counterparts.},
  archive      = {J_TNNLS},
  author       = {Xiaoling Luo and Hong Qu and Yuchen Wang and Zhang Yi and Jilun Zhang and Malu Zhang},
  doi          = {10.1109/TNNLS.2022.3164930},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10141-10153},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Supervised learning in multilayer spiking neural networks with spike temporal error backpropagation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Pinning stabilizer design for probabilistic boolean control
networks via condensation digraph. <em>TNNLS</em>, <em>34</em>(12),
10130–10140. (<a
href="https://doi.org/10.1109/TNNLS.2022.3164909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the design of pinning controllers for state feedback stabilization of probabilistic Boolean control networks (PBCNs), based on the condensation digraph method. First, two effective algorithms are presented to achieve state feedback stabilization of the considered system from the perspective of condensation digraph. One is to find the desired matrix, and the other is to search for the minimum number of pinned nodes and specific pinned nodes. Then, all the mode-independent pinning controllers can be designed based on the desired matrix and pinned nodes. Several examples are delineated to illustrate the validity of the main results.},
  archive      = {J_TNNLS},
  author       = {Lina Wang and Jiayang Liu and Yang Liu and Weihua Gui},
  doi          = {10.1109/TNNLS.2022.3164909},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10130-10140},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pinning stabilizer design for probabilistic boolean control networks via condensation digraph},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online passive-aggressive multilabel classification
algorithms. <em>TNNLS</em>, <em>34</em>(12), 10116–10129. (<a
href="https://doi.org/10.1109/TNNLS.2022.3164906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing multilabel classification methods are batch learning methods, which may suffer from expensive retraining costs when dealing with new incoming data. In order to overcome the drawbacks of batch learning, we develop a family of online multilabel classification algorithms, which can update the model instantly and efficiently, and make a timely online prediction when new data arrive. Our algorithms all take a closed-form update, which is obtained by solving a constrained optimization problem in each round of online learning. Label correlation is explicitly modeled in our optimization problem. The label thresholding function, an important component of our online classifier, can also be learned online. Our algorithms can be easily generalized to the nonlinear prediction cases using Mercer kernels. The worst case loss bounds for our algorithms are provided. The bounds are relative to the cumulative loss suffered by the best fixed predictive model that can be attained in hindsight. Finally, we corroborate the merits of our algorithms in both linear and nonlinear predictions on nine open multilabel benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Tingting Zhai and Hao Wang},
  doi          = {10.1109/TNNLS.2022.3164906},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10116-10129},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online passive-aggressive multilabel classification algorithms},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anti-attack event-triggered control for nonlinear
multi-agent systems with input quantization. <em>TNNLS</em>,
<em>34</em>(12), 10105–10115. (<a
href="https://doi.org/10.1109/TNNLS.2022.3164881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an anti-attack event-triggered secure control scheme for a class of nonlinear multi-agent systems with input quantization is developed. With the help of neural networks approximating unknown nonlinear functions, unknown states are obtained by designing an adaptive neural state observer. Then, a relative threshold event-triggered control strategy is introduced to save communication resources including network bandwidth and computational capabilities. Furthermore, a quantizer is employed to provide sufficient accuracy under the requirement of a low transmission rate, which is represented by the so-called a hysteresis quantizer. Meanwhile, to resist attacks in the multi-agent network, a predictor is designed to record whether an edge is attacked or not. Through the Lyapunov analysis, the proposed secure control protocol can ensure that all the closed-loop signals remain bounded under attacks. Finally, the effectiveness of the designed scheme is verified by simulation results.},
  archive      = {J_TNNLS},
  author       = {Yuanyuan Xu and Tieshan Li and Yue Yang and Qihe Shan and Shaocheng Tong and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2022.3164881},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10105-10115},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Anti-attack event-triggered control for nonlinear multi-agent systems with input quantization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spectrum analysis for fully connected neural networks.
<em>TNNLS</em>, <em>34</em>(12), 10091–10104. (<a
href="https://doi.org/10.1109/TNNLS.2022.3164875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the meaning of parameters of fully connected neural networks with single hidden layer from the perspective of spectrum. Under the constraints of numerical range, the corresponding relationship between parameters and the spectrum of network function can be established by the Fourier series coefficients of the activation function, which is truncated and periodically extended. This work is substantiated on the Mixed National Institute of Standards and Technology (MNIST) handwritten dataset and two illustrative examples with certain spectra. The simulations complete the conversion between spectrum and parameters with high precision and give the significance of hidden nodes to the spectrum of network function. Some algorithms derived from these properties, such as the parameter initialization method using spectrum and the pruning method by sorting amplification weights, are also presented to introduce how spectrum analysis affects neural network decision-making. Thus, spectrum analysis has great potential in network interpretation.},
  archive      = {J_TNNLS},
  author       = {Bojun Jia and Yanjun Zhang},
  doi          = {10.1109/TNNLS.2022.3164875},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10091-10104},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spectrum analysis for fully connected neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient online globalized dual heuristic programming with
an associated dual network. <em>TNNLS</em>, <em>34</em>(12),
10079–10090. (<a
href="https://doi.org/10.1109/TNNLS.2022.3164727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globalized dual heuristic programming (GDHP) is the most comprehensive adaptive critic design, which employs its critic to minimize the error with respect to both the cost-to-go and its derivatives simultaneously. Its implementation, however, confronts a dilemma of either introducing more computational load by explicitly calculating the second partial derivative term or sacrificing the accuracy by loosening the association between the cost-to-go and its derivatives. This article aims at increasing the online learning efficiency of GDHP while retaining its analytical accuracy by introducing a novel GDHP design based on a critic network and an associated dual network. This associated dual network is derived from the critic network explicitly and precisely, and its structure is in the same level of complexity as dual heuristic programming critics. Three simulation experiments are conducted to validate the learning ability, efficiency, and feasibility of the proposed GDHP critic design.},
  archive      = {J_TNNLS},
  author       = {Ye Zhou},
  doi          = {10.1109/TNNLS.2022.3164727},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10079-10090},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient online globalized dual heuristic programming with an associated dual network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convex subspace clustering by adaptive block diagonal
representation. <em>TNNLS</em>, <em>34</em>(12), 10065–10078. (<a
href="https://doi.org/10.1109/TNNLS.2022.3164540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering is a class of extensively studied clustering methods where the spectral-type approaches are its important subclass. Its key first step is to desire learning a representation coefficient matrix with block diagonal structure. To realize this step, many methods were successively proposed by imposing different structure priors on the coefficient matrix. These impositions can be roughly divided into two categories, i.e., indirect and direct. The former introduces the priors such as sparsity and low rankness to indirectly or implicitly learn the block diagonal structure. However, the desired block diagonalty cannot necessarily be guaranteed for noisy data. While the latter directly or explicitly imposes the block diagonal structure prior such as block diagonal representation (BDR) to ensure so-desired block diagonalty even if the data is noisy but at the expense of losing the convexity that the former’s objective possesses. For compensating their respective shortcomings, in this article, we follow the direct line to propose adaptive BDR (ABDR) which explicitly pursues block diagonalty without sacrificing the convexity of the indirect one. Specifically, inspired by Convex BiClustering, ABDR coercively fuses both columns and rows of the coefficient matrix via a specially designed convex regularizer, thus naturally enjoying their merits and adaptively obtaining the number of blocks. Finally, experimental results on synthetic and real benchmarks demonstrate the superiority of ABDR to the state-of-the-arts (SOTAs).},
  archive      = {J_TNNLS},
  author       = {Yunxia Lin and Songcan Chen},
  doi          = {10.1109/TNNLS.2022.3164540},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10065-10078},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convex subspace clustering by adaptive block diagonal representation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STKD: Distilling knowledge from synchronous teaching for
efficient model compression. <em>TNNLS</em>, <em>34</em>(12),
10051–10064. (<a
href="https://doi.org/10.1109/TNNLS.2022.3164264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) transfers discriminative knowledge from a large and complex model (known as teacher) to a smaller and faster one (known as student). Existing advanced KD methods, limited to fixed feature extraction paradigms that capture teacher’s structure knowledge to guide the training of the student, often fail to obtain comprehensive knowledge to the student. Toward this end, in this article, we propose a new approach, synchronous teaching knowledge distillation (STKD), to integrate online teaching and offline teaching for transferring rich and comprehensive knowledge to the student. In the online learning stage, a blockwise unit is designed to distill the intermediate-level knowledge and high-level knowledge, which can achieve bidirectional guidance of the teacher and student networks. Intermediate-level information interaction provides more supervisory information to the student network and is useful to enhance the quality of final predictions. In the offline learning stage, the STKD approach applies a pretrained teacher to further improve the performance and accelerate the training process by providing prior knowledge. Trained simultaneously, the student learns multilevel and comprehensive knowledge by incorporating online teaching and offline teaching, which combines the advantages of different KD strategies through our STKD method. Experimental results on the SVHN, CIFAR-10, CIFAR-100, and ImageNet ILSVRC 2012 real-world datasets show that the proposed method achieves significant performance improvements compared with the state-of-the-art methods, especially with satisfying accuracy and model size. Code for STKD is provided at https://github.com/nanxiaotong/STKD .},
  archive      = {J_TNNLS},
  author       = {Tongtong Su and Jinsong Zhang and Zhaoyang Yu and Gang Wang and Xiaoguang Liu},
  doi          = {10.1109/TNNLS.2022.3164264},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10051-10064},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {STKD: Distilling knowledge from synchronous teaching for efficient model compression},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Review polarity-wise recommender. <em>TNNLS</em>,
<em>34</em>(12), 10039–10050. (<a
href="https://doi.org/10.1109/TNNLS.2022.3163789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The de facto review-involved recommender systems, using review information to enhance recommendation, have received increasing interest over the past years. Thereinto, one advanced branch is to extract salient aspects from textual reviews (i.e., the item attributes that users express) and combine them with the matrix factorization (MF) technique. However, the existing approaches all ignore the fact that semantically different reviews often include opposite aspect information. In particular, positive reviews usually express aspects that users prefer, while the negative ones describe aspects that users dislike. As a result, it may mislead the recommender systems into making incorrect decisions pertaining to user preference modeling. Toward this end, in this article, we present a review polarity-wise recommender model, dubbed as RPR, to discriminately treat reviews with different polarities. To be specific, in this model, positive and negative reviews are separately gathered and used to model the user-preferred and user-rejected aspects, respectively. Besides, to overcome the imbalance of semantically different reviews, we further develop an aspect-aware importance weighting strategy to align the aspect importance for these two kinds of reviews. Extensive experiments conducted on eight benchmark datasets have demonstrated the superiority of our model when compared with several state-of-the-art review-involved baselines. Moreover, our method can provide certain explanations to real-world rating prediction scenarios.},
  archive      = {J_TNNLS},
  author       = {Han Liu and Yangyang Guo and Jianhua Yin and Zan Gao and Liqiang Nie},
  doi          = {10.1109/TNNLS.2022.3163789},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10039-10050},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Review polarity-wise recommender},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal sparse transformer network for audio-visual
speech recognition. <em>TNNLS</em>, <em>34</em>(12), 10028–10038. (<a
href="https://doi.org/10.1109/TNNLS.2022.3163771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic speech recognition (ASR) is the major human–machine interface in many intelligent systems, such as intelligent homes, autonomous driving, and servant robots. However, its performance usually significantly deteriorates in the presence of external noise, leading to limitations of its application scenes. The audio-visual speech recognition (AVSR) takes visual information as a complementary modality to enhance the performance of audio speech recognition effectively, particularly in noisy conditions. Recently, the transformer-based architectures have been used to model the audio and video sequences for the AVSR, which achieves a superior performance. However, its performance may be degraded in these architectures due to extracting irrelevant information while modeling long-term dependences. In addition, the motion feature is essential for capturing the spatio-temporal information within the lip region to best utilize visual sequences but has not been considered in the AVSR tasks. Therefore, we propose a multimodal sparse transformer network (MMST) in this article. The sparse self-attention mechanism can improve the concentration of attention on global information by selecting the most relevant parts wisely. Moreover, the motion features are seamlessly introduced into the MMST model. We subtly allow motion-modality information to flow into visual modality through the cross-modal attention module to enhance visual features, thereby further improving recognition performance. Extensive experiments conducted on different datasets validate that our proposed method outperforms several state-of-the-art methods in terms of the word error rate (WER).},
  archive      = {J_TNNLS},
  author       = {Qiya Song and Bin Sun and Shutao Li},
  doi          = {10.1109/TNNLS.2022.3163771},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10028-10038},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multimodal sparse transformer network for audio-visual speech recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive neural network control of an uncertain 2-DOF
helicopter with unknown backlash-like hysteresis and output constraints.
<em>TNNLS</em>, <em>34</em>(12), 10018–10027. (<a
href="https://doi.org/10.1109/TNNLS.2022.3163572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptive neural network (NN) control is proposed for an unknown two-degree of freedom (2-DOF) helicopter system with unknown backlash-like hysteresis and output constraint in this study. A radial basis function NN is adopted to estimate the unknown dynamics model of the helicopter, adaptive variables are employed to eliminate the effect of unknown backlash-like hysteresis present in the system, and a barrier Lyapunov function is designed to deal with the output constraint. Through the Lyapunov stability analysis, the closed-loop system is proven to be semiglobally and uniformly bounded, and the asymptotic attitude adjustment and tracking of the desired set point and trajectory are achieved. Finally, numerical simulation and experiments on a Quanser’s experimental platform verify that the control method is appropriate and effective.},
  archive      = {J_TNNLS},
  author       = {Zhijia Zhao and Jian Zhang and Zhijie Liu and Chaoxu Mu and Keum-Shik Hong},
  doi          = {10.1109/TNNLS.2022.3163572},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10018-10027},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network control of an uncertain 2-DOF helicopter with unknown backlash-like hysteresis and output constraints},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive learning control algorithms for infinite-duration
tracking. <em>TNNLS</em>, <em>34</em>(12), 10004–10017. (<a
href="https://doi.org/10.1109/TNNLS.2022.3163443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning control is applicable to systems that operate periodically or over finite time intervals. Currently, there is a lack of research results about learning control approaches to infinite-duration tracking, without requiring periodicity or repeatability. This article addresses the problem of adaptive learning control (ALC) for systems performing infinite-duration tasks. Instead of using integral adaptation, incremental adaptive mechanisms are exploited, by which the numerical integration for implementation can be avoided. The comparison with the conventional integral adaptive mechanisms indicates that the suggested methodology can be an alternative to the adaptive system designs. Using an error-tracking approach, the approximation-based backstepping design is carried out for systems in the strict-feedback form, where a novel integral Lyapunov function is shown to be efficient in the treatment of state-dependent control gain. Theoretical results for the performance analysis are presented in detail. In particular, the robust convergence of the tracking error is established, while the boundedness of the variables of the closed-loop system is characterized, with the aid of a key technical lemma. It is shown that the proposed control method can provide satisfactory tracking performance and simplify the controller designs. Numerical results are presented to demonstrate effectiveness of the learning control schemes.},
  archive      = {J_TNNLS},
  author       = {Mingxuan Sun and Shengxiang Zou},
  doi          = {10.1109/TNNLS.2022.3163443},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {10004-10017},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive learning control algorithms for infinite-duration tracking},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised continual learning in streaming environments.
<em>TNNLS</em>, <em>34</em>(12), 9992–10003. (<a
href="https://doi.org/10.1109/TNNLS.2022.3163362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep clustering network (DCN) is desired for data streams because of its aptitude in extracting natural features thus bypassing the laborious feature engineering step. While automatic construction of deep networks in streaming environments remains an open issue, it is also hindered by the expensive labeling cost of data streams rendering the increasing demand for unsupervised approaches. This article presents an unsupervised approach of DCN construction on the fly via simultaneous deep learning and clustering termed autonomous DCN (ADCN). It combines the feature extraction layer and autonomous fully connected layer in which both network width and depth are self-evolved from data streams based on the bias-variance decomposition of reconstruction loss. The self-clustering mechanism is performed in the deep embedding space of every fully connected layer, while the final output is inferred via the summation of cluster prediction score. Furthermore, a latent-based regularization is incorporated to resolve the catastrophic forgetting issue. A rigorous numerical study has shown that ADCN produces better performance compared with its counterparts while offering fully autonomous construction of ADCN structure in streaming environments in the absence of any labeled samples for model updates. To support the reproducible research initiative, codes, supplementary material, and raw results of ADCN are made available in https://github.com/andriash001/AutonomousDCN.git},
  archive      = {J_TNNLS},
  author       = {Andri Ashfahani and Mahardhika Pratama},
  doi          = {10.1109/TNNLS.2022.3163362},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9992-10003},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised continual learning in streaming environments},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ZNNs with a varying-parameter design formula for dynamic
sylvester quaternion matrix equation. <em>TNNLS</em>, <em>34</em>(12),
9981–9991. (<a
href="https://doi.org/10.1109/TNNLS.2022.3163293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to studying how to solve dynamic Sylvester quaternion matrix equation (DSQME) using the neural dynamic method. In order to solve the DSQME, the complex representation method is first adopted to derive the equivalent dynamic Sylvester complex matrix equation (DSCME) from the DSQME. It is proven that the solution to the DSCME is the same as that of the DSQME in essence. Then, a state-of-the-art neural dynamic method is presented to generate a general dynamic-varying parameter zeroing neural network (DVPZNN) model with its global stability being guaranteed by the Lyapunov theory. Specifically, when the linear activation function is utilized in the DVPZNN model, the corresponding model [termed linear DVPZNN (LDVPZNN)] achieves finite-time convergence, and a time range is theoretically calculated. When the nonlinear power-sigmoid activation function is utilized in the DVPZNN model, the corresponding model [termed power-sigmoid DVPZNN (PSDVPZNN)] achieves the better convergence compared with the LDVPZNN model, which is proven in detail. Finally, three examples are presented to compare the solution performance of different neural models for the DSQME and the equivalent DSCME, and the results verify the correctness of the theories and the superiority of the proposed two DVPZNN models.},
  archive      = {J_TNNLS},
  author       = {Lin Xiao and Wenqian Huang and Xiaopeng Li and Fuchun Sun and Qing Liao and Lei Jia and Jichun Li and Sai Liu},
  doi          = {10.1109/TNNLS.2022.3163293},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9981-9991},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ZNNs with a varying-parameter design formula for dynamic sylvester quaternion matrix equation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised multimodal anomaly detection with missing
sources for liquid rocket engine. <em>TNNLS</em>, <em>34</em>(12),
9966–9980. (<a
href="https://doi.org/10.1109/TNNLS.2022.3162949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve reliable and automatic anomaly detection (AD) for large equipment such as liquid rocket engine (LRE), multisource data are commonly manipulated in deep learning pipelines. However, current AD methods mainly aim at single source or single modality, whereas existing multimodal methods cannot effectively cope with a common issue, modality incompleteness. To this end, we propose an unsupervised multimodal method for AD with missing sources in LRE system. The proposed method handles intramodality fusion, intermodality fusion, and decision fusion in a unified framework composed of multiple deep autoencoders (AEs) and a skip-connected AE. Specifically, the first module restores missing sources to construct a complete modality, thus advancing the secondary reconstruction. Different from vanilla reconstruction-based methods, the proposed method minimizes reconstruction loss and meanwhile maximizes the dissimilarity of representations in two latent spaces. Utilizing reconstruction errors and latent representation discrepancy, the anomaly score is acquired. At decision level, the model performance can be further enhanced via anomaly score fusion. To demonstrate the effectiveness, extensive experiments are carried out on multivariate time-series data from static ignition of several LREs. The results indicate the superiority and potential of the proposed method for AD with missing sources for LRE.},
  archive      = {J_TNNLS},
  author       = {Yong Feng and Zijun Liu and Jinglong Chen and Haixin Lv and Jun Wang and Xinwei Zhang},
  doi          = {10.1109/TNNLS.2022.3162949},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9966-9980},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised multimodal anomaly detection with missing sources for liquid rocket engine},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). SIN: Semantic inference network for few-shot streaming
label learning. <em>TNNLS</em>, <em>34</em>(12), 9952–9965. (<a
href="https://doi.org/10.1109/TNNLS.2022.3162747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming label learning aims to model newly emerged labels for multilabel classification systems, which requires plenty of new label data for training. However, in changing environments, only a small amount of new label data can practically be collected. In this work, we formulate and study few-shot streaming label learning (FSLL), which models emerging new labels with only a few annotated examples by utilizing the knowledge learned from past labels. We propose a meta-learning framework, semantic inference network (SIN), which can learn and infer the semantic correlation between new labels and past labels to adapt FSLL tasks from a few examples effectively. SIN leverages label semantic representation to regularize the output space and acquires labelwise meta-knowledge based on gradient-based meta-learning. Moreover, SIN incorporates a novel label decision module with a meta-threshold loss to find the optimal confidence thresholds for each new label. Theoretically, we illustrate that the proposed semantic inference mechanism could constrain the complexity of hypotheses space to reduce the risk of overfitting and achieve better generalizability. Experimentally, extensive empirical results and ablation studies demonstrate the performance of SIN is superior to the prior state-of-the-art methods on FSLL.},
  archive      = {J_TNNLS},
  author       = {Zhen Wang and Liu Liu and Yiqun Duan and Dacheng Tao},
  doi          = {10.1109/TNNLS.2022.3162747},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9952-9965},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SIN: Semantic inference network for few-shot streaming label learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trusted-data-guided label enhancement on noisy labels.
<em>TNNLS</em>, <em>34</em>(12), 9940–9951. (<a
href="https://doi.org/10.1109/TNNLS.2022.3162316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label distribution covers a certain number of labels, representing the degree to which each label describes the instance. Label enhancement (LE) is a procedure of recovering the label distribution from the logical labels in the training data, the purpose of which is to better depict the label ambiguity through label distribution. However, data annotation inevitably introduces label noise, and it is extremely challenging to implement LE on corrupted labels. To deal with this problem, one way to recover the label distribution from the corrupted labels is to be guided by a small batch of trusted data. In this article, a novel LE method named TALEN is proposed via recovering and progressively refining label distribution guided by trusted data. Specifically, an LE process is applied to the untrusted data to select samples with a clean label. In addition, a combined loss function is designed to train the predictive model for classification. Experiments on datasets with synthetic label noise validate the feasibility of identifying clean labels via the recovered label distribution. Furthermore, experimental results on both synthetic label noise and real-world label noise on image datasets and additional experiments on text datasets show a clear advantage of TALEN over several existing noise-robust learning methods.},
  archive      = {J_TNNLS},
  author       = {Ning Xu and Jia-Yu Li and Yun-Peng Liu and Xin Geng},
  doi          = {10.1109/TNNLS.2022.3162316},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9940-9951},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Trusted-data-guided label enhancement on noisy labels},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Catastrophic interference in reinforcement learning: A
solution based on context division and knowledge distillation.
<em>TNNLS</em>, <em>34</em>(12), 9925–9939. (<a
href="https://doi.org/10.1109/TNNLS.2022.3162241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The powerful learning ability of deep neural networks enables reinforcement learning (RL) agents to learn competent control policies directly from continuous environments. In theory, to achieve stable performance, neural networks assume identically and independently distributed (i.i.d.) inputs, which unfortunately does not hold in the general RL paradigm where the training data are temporally correlated and nonstationary. This issue may lead to the phenomenon of “catastrophic interference” and the collapse in performance. In this article, we present interference-aware deep Q-learning (IQ) to mitigate catastrophic interference in single-task deep RL. Specifically, we resort to online clustering to achieve on-the-fly context division, together with a multihead network and a knowledge distillation regularization term for preserving the policy of learned contexts. Built upon deep Q networks (DQNs), IQ consistently boosts the stability and performance when compared to existing methods, verified with extensive experiments on classic control and Atari tasks. The code is publicly available at https://github.com/ Sweety-dm/Interference-aware-Deep-Q-learning.},
  archive      = {J_TNNLS},
  author       = {Tiantian Zhang and Xueqian Wang and Bin Liang and Bo Yuan},
  doi          = {10.1109/TNNLS.2022.3162241},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9925-9939},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Catastrophic interference in reinforcement learning: A solution based on context division and knowledge distillation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). SOKS: Automatic searching of the optimal kernel shapes for
stripe-wise network pruning. <em>TNNLS</em>, <em>34</em>(12), 9912–9924.
(<a href="https://doi.org/10.1109/TNNLS.2022.3162067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spite of the remarkable performance, deep convolutional neural networks (CNNs) are typically over-parameterized and computationally expensive. Network pruning has become a popular approach to reducing the storage and calculations of CNN models, which commonly prunes filters in a structured way or discards single weights without structural constraints. However, the redundancy in convolution kernels and the influence of kernel shapes on the performance of CNN models have attracted little attention. In this article, we develop a framework, termed searching of the optimal kernel shape (SOKS), to automatically search for the optimal kernel shapes and perform stripe-wise pruning (SWP). To be specific, we introduce coefficient matrices regularized by a variety of regularization terms to locate important kernel positions. The optimal kernel shapes not only provide appropriate receptive fields for each convolution layer, but also remove redundant parameters in convolution kernels. SWP is also achieved by utilizing these irregular kernels and actual inference speedups on the graphics processing unit (GPU) are obtained. Comprehensive experimental results demonstrate that SOKS searches high-efficiency kernel shapes and achieves superior performance in terms of both compression ratio and inference latency. Embedding the searched kernels into VGG-16 increases the accuracy from 93.53\% to 94.26\% on CIFAR-10, while pruning 59.27\% model parameters and reducing 27.07\% inference latency.},
  archive      = {J_TNNLS},
  author       = {Guangzhe Liu and Ke Zhang and Meibo Lv},
  doi          = {10.1109/TNNLS.2022.3162067},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9912-9924},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SOKS: Automatic searching of the optimal kernel shapes for stripe-wise network pruning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust RGB-t tracking via graph attention-based bilinear
pooling. <em>TNNLS</em>, <em>34</em>(12), 9900–9911. (<a
href="https://doi.org/10.1109/TNNLS.2022.3161969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T tracker possesses strong capability of fusing two different yet complementary target observations, thus providing a promising solution to fulfill all-weather tracking in intelligent transportation systems. Existing convolutional neural network (CNN)-based RGB-T tracking methods often consider the multisource-oriented deep feature fusion from global viewpoint, but fail to yield satisfactory performance when the target pair only contains partially useful information. To solve this problem, we propose a four-stream oriented Siamese network (FS-Siamese) for RGB-T tracking. The key innovation of our network structure lies in that we formulate multidomain multilayer feature map fusion as a multiple graph learning problem, based on which we develop a graph attention-based bilinear pooling module to explore the partial feature interaction between the RGB and the thermal targets. This can effectively avoid uninformed image blocks disturbing feature embedding fusion. To enhance the efficiency of the proposed Siamese network structure, we propose to adopt meta-learning to incorporate category information in the updating of bilinear pooling results, which can online enforce the exemplar and current target appearance obtaining similar sematic representation. Extensive experiments on grayscale-thermal object tracking (GTOT) and RGBT234 datasets demonstrate that the proposed method outperforms the state-of-the-art methods for the task of RGB-T tracking.},
  archive      = {J_TNNLS},
  author       = {Bin Kang and Dong Liang and Junxi Mei and Xiaoyang Tan and Quan Zhou and Dengyin Zhang},
  doi          = {10.1109/TNNLS.2022.3161969},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9900-9911},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust RGB-T tracking via graph attention-based bilinear pooling},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multiplier bootstrap approach to designing robust
algorithms for contextual bandits. <em>TNNLS</em>, <em>34</em>(12),
9887–9899. (<a
href="https://doi.org/10.1109/TNNLS.2022.3161806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upper confidence bound (UCB)-based contextual bandit algorithms require one to know the tail property of the reward distribution. Unfortunately, such tail property is usually unknown or difficult to specify in real-world applications. Using a tail property heavier than the ground truth leads to a slow learning speed of the contextual bandit algorithm, while using a lighter one may cause the algorithm to diverge. To address this fundamental problem, we develop an estimator (evaluated from historical rewards) for the contextual bandit UCB based on the multiplier bootstrap technique. Our proposed estimator mitigates the problem of specifying a heavier tail property by adaptively converging to the ground truth contextual bandit UCB (i.e., eliminating the impact of the specified heavier tail property) with theoretical guarantees on the convergence. The design and convergence analysis of the proposed estimator is technically nontrivial. The proposed estimator is generic and it can be applied to improve a variety of UCB-based contextual bandit algorithms. To demonstrate the versatility of the proposed estimator, we apply it to improve the linear reward contextual bandit UCB (LinUCB) algorithm resulting in our bootstrapping LinUCB (BootLinUCB) algorithm. We prove that the BootLinUCB has a sublinear regret. We conduct extensive experiments on both synthetic dataset and real-world dataset from Yahoo! to validate the benefits of our proposed estimator in reducing regret and the superior performance of BootLinUCB over the latest baseline.},
  archive      = {J_TNNLS},
  author       = {Hong Xie and Qiao Tang and Qingsheng Zhu},
  doi          = {10.1109/TNNLS.2022.3161806},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9887-9899},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A multiplier bootstrap approach to designing robust algorithms for contextual bandits},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Glove-based hand gesture recognition for diver
communication. <em>TNNLS</em>, <em>34</em>(12), 9874–9886. (<a
href="https://doi.org/10.1109/TNNLS.2022.3161682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have developed a smart dive glove that recognizes 13 static hand gestures used in diving communication. The smart glove employs five dielectric elastomer sensors to capture finger motion and implements a machine learning classifier in the onboard electronics to recognize gestures. Five basic classification algorithms are trained and assessed: the decision tree, support vector machine (SVM), logistic regression, Gaussian naïve Bayes, and multilayer perceptron. These basic classifiers were selected as they perform well in multiclass classification problems, can be trained using supervised learning, and are model-based algorithms that can be implemented on a microprocessor. The training dataset was collected from 24 participants providing for a range of different hand sizes. After training, the algorithms were evaluated in a dry environment using data collected from ten new participants to test how well they cope with new information. Furthermore, an underwater experiment was conducted to assess any impact of the underwater environment on each algorithm’s classification. The results show all classifiers performed well in a dry environment. The accuracies and F1-scores range between 0.95 and 0.98, where the logistic regressor and SVM have the highest scores for both the accuracy and F1-score (0.98). The underwater results showed that all algorithms work underwater; however, the performance drops when divers must focus on buoyancy control, breathing, and diver trim.},
  archive      = {J_TNNLS},
  author       = {Derek W. Orbaugh Antillon and Christopher R. Walker and Samuel Rosset and Iain A. Anderson},
  doi          = {10.1109/TNNLS.2022.3161682},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9874-9886},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Glove-based hand gesture recognition for diver communication},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph decoupling attention markov networks for
semisupervised graph node classification. <em>TNNLS</em>,
<em>34</em>(12), 9859–9873. (<a
href="https://doi.org/10.1109/TNNLS.2022.3161453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been ubiquitous in graph node classification tasks. Most GNN methods update the node embedding iteratively by aggregating its neighbors’ information. However, they often suffer from negative disturbances, due to edges connecting nodes with different labels. One approach to alleviate this negative disturbance is to use attention to learn the weights of aggregation, but current attention-based GNNs only consider feature similarity and suffer from the lack of supervision. In this article, we consider label dependency of graph nodes and propose a decoupling attention mechanism to learn both hard and soft attention. The hard attention is learned on labels for a refined graph structure with fewer interclass edges so that the aggregation’s negative disturbance can be reduced. The soft attention aims to learn the aggregation weights based on features over the refined graph structure to enhance information gains during message passing. Particularly, we formulate our model under the expectation–maximization (EM) framework, and the learned attention is used to guide label propagation in the M-step and feature propagation in the E-step, respectively. Extensive experiments are performed on six well-known benchmark graph datasets to verify the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jie Chen and Shouzhen Chen and Mingyuan Bai and Jian Pu and Junping Zhang and Junbin Gao},
  doi          = {10.1109/TNNLS.2022.3161453},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9859-9873},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph decoupling attention markov networks for semisupervised graph node classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DAIS: Automatic channel pruning via differentiable annealing
indicator search. <em>TNNLS</em>, <em>34</em>(12), 9847–9858. (<a
href="https://doi.org/10.1109/TNNLS.2022.3161284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convolutional neural network (CNN) has achieved great success in fulfilling computer vision tasks despite large computation overhead against efficient deployment. Channel pruning is usually applied to reduce the model redundancy while preserving the network structure, such that the pruned network can be easily deployed in practice. However, existing channel pruning methods require hand-crafted rules, which can result in a degraded model performance with respect to the tremendous potential pruning space given large neural networks. In this article, we introduce differentiable annealing indicator search (DAIS) that leverages the strength of neural architecture search in the channel pruning and automatically searches for the effective pruned model with given constraints on computation overhead. Specifically, DAIS relaxes the binarized channel indicators to be continuous and then jointly learns both indicators and model parameters via bi-level optimization. To bridge the non-negligible discrepancy between the continuous model and the target binarized model, DAIS proposes an annealing-based procedure to steer the indicator convergence toward binarized states. Moreover, DAIS designs various regularizations based on a priori structural knowledge to control the pruning sparsity and to improve model performance. Experimental results show that DAIS outperforms state-of-the-art pruning methods on CIFAR-10, CIFAR-100, and ImageNet.},
  archive      = {J_TNNLS},
  author       = {Yushuo Guan and Ning Liu and Pengyu Zhao and Zhengping Che and Kaigui Bian and Yanzhi Wang and Jian Tang},
  doi          = {10.1109/TNNLS.2022.3161284},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9847-9858},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DAIS: Automatic channel pruning via differentiable annealing indicator search},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised motion perception for spatiotemporal
representation learning. <em>TNNLS</em>, <em>34</em>(12), 9832–9846. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel pretext task and a self-supervised motion perception (SMP) method for spatiotemporal representation learning. The pretext task is defined as video playback rate perception, which utilizes temporal dilated sampling to augment video clips to multiple duplicates of different temporal resolutions. The SMP method is built upon discriminative and generative motion perception models, which capture representations related to motion dynamics and appearance from video clips of multiple temporal resolutions in a collaborative fashion. To enhance the collaboration, we further propose difference and convolution motion attention (MA), which drives the generative model focusing on motion-related appearance, and leverage multiple granularity perception (MG) to extract accurate motion dynamics. Extensive experiments demonstrate SMP’s effectiveness for video motion perception and state-of-the-art performance of self-supervised representation models upon target tasks, including action recognition and video retrieval. Code for SMP is available at github.com/yuanyao366/SMP.},
  archive      = {J_TNNLS},
  author       = {Chang Liu and Yuan Yao and Dezhao Luo and Yu Zhou and Qixiang Ye},
  doi          = {10.1109/TNNLS.2022.3160860},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9832-9846},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised motion perception for spatiotemporal representation learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymptotic tracking control for uncertain nonlinear
strict-feedback systems with unknown time-varying delays.
<em>TNNLS</em>, <em>34</em>(12), 9821–9831. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is nontrivial to achieve asymptotic tracking control for uncertain nonlinear strict-feedback systems with unknown time-varying delays. This problem becomes even more challenging if the control direction is unknown. To address such problem, the Lyapunov–Krasovskii functional (LKF) is used to deal with the time delays, and the neural network (NN) is applied to compensate for the time-delay-free yet unknown terms arising from the derivative of LKF, and then an NN-based adaptive control scheme is constructed on the basis of backstepping technique, which enables the output tracking error to converge to zero asymptotically. Besides, with a milder condition on time delay functions, the notorious singularity issue commonly encountered in coping with time delay problems is subtly settled, which makes the proposed scheme simple in structure and inexpensive in computation. Moreover, all the signals in the closed-loop system are ensured to be semiglobally uniformly ultimately bounded, and the transient performance can be improved with proper choice of design parameters. Both the theoretical analysis and numerical simulation are carried out to validate the relevance of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Hong Cheng and Xiucai Huang and Hongwei Cao},
  doi          = {10.1109/TNNLS.2022.3160803},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9821-9831},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asymptotic tracking control for uncertain nonlinear strict-feedback systems with unknown time-varying delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting and tracking of multiple mice using part proposal
networks. <em>TNNLS</em>, <em>34</em>(12), 9806–9820. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of mouse social behaviors has been increasingly undertaken in neuroscience research. However, automated quantification of mouse behaviors from the videos of interacting mice is still a challenging problem, where object tracking plays a key role in locating mice in their living spaces. Artificial markers are often applied for multiple mice tracking, which are intrusive and consequently interfere with the movements of mice in a dynamic environment. In this article, we propose a novel method to continuously track several mice and individual parts without requiring any specific tagging. First, we propose an efficient and robust deep-learning-based mouse part detection scheme to generate part candidates. Subsequently, we propose a novel Bayesian-inference integer linear programming (BILP) model that jointly assigns the part candidates to individual targets with necessary geometric constraints while establishing pair-wise association between the detected parts. There is no publicly available dataset in the research community that provides a quantitative test bed for part detection and tracking of multiple mice, and we here introduce a new challenging Multi-Mice PartsTrack dataset that is made of complex behaviors. Finally, we evaluate our proposed approach against several baselines on our new datasets, where the results show that our method outperforms the other state-of-the-art approaches in terms of accuracy. We also demonstrate the generalization ability of the proposed approach on tracking zebra and locust.},
  archive      = {J_TNNLS},
  author       = {Zheheng Jiang and Zhihua Liu and Long Chen and Lei Tong and Xiangrong Zhang and Xiangyuan Lan and Danny Crookes and Ming-Hsuan Yang and Huiyu Zhou},
  doi          = {10.1109/TNNLS.2022.3160800},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9806-9820},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Detecting and tracking of multiple mice using part proposal networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proportional–integral observer-based state estimation for
singularly perturbed complex networks with cyberattacks. <em>TNNLS</em>,
<em>34</em>(12), 9795–9805. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the asynchronous proportional–integral observer (PIO) design issue for singularly perturbed complex networks (SPCNs) subject to cyberattacks. The switching topology of SPCNs is regulated by a nonhomogeneous Markov switching process, whose time-varying transition probabilities are polytope structured. Besides, the multiple scalar Winner processes are applied to character the stochastic disturbances of the inner linking strengths. Two mutually independent Bernoulli stochastic variables are exploited to characterize the random occurrences of cyberattacks. In a practical viewpoint, by resorting to the hidden nonhomogeneous Markov model, an asynchronous PIO is formulated. Under such a framework, by applying the Lyapunov theory, sufficient conditions are established such that the augmented dynamic is mean-square exponentially ultimately bounded. Finally, the effectiveness of the theoretical results is verified by two numerical simulations.},
  archive      = {J_TNNLS},
  author       = {Lidan Liang and Jun Cheng and Jinde Cao and Zheng-Guang Wu and Wu-Hua Chen},
  doi          = {10.1109/TNNLS.2022.3160627},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9795-9805},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Proportional–Integral observer-based state estimation for singularly perturbed complex networks with cyberattacks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward efficient palmprint feature extraction by learning a
single-layer convolution network. <em>TNNLS</em>, <em>34</em>(12),
9783–9794. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a collaborative palmprint-specific binary feature learning method and a compact network consisting of a single convolution layer for efficient palmprint feature extraction. Unlike most existing palmprint feature learning methods, such as deep-learning, which usually ignore the inherent characteristics of palmprints and learn features from raw pixels of a massive number of labeled samples, palmprint-specific information, such as the direction and edge of patterns, is characterized by forming two kinds of ordinal measure vectors (OMVs). Then, collaborative binary feature codes are jointly learned by projecting double OMVs into complementary feature spaces in an unsupervised manner. Furthermore, the elements of feature projection functions are integrated into OMV extraction filters to obtain a collection of cascaded convolution templates that form a single-layer convolution network (SLCN) to efficiently obtain the binary feature codes of a new palmprint image within a single-stage convolution operation. Particularly, our proposed method can easily be extended to a general version that can efficiently perform feature extraction with more than two types of OMVs. Experimental results on five benchmark databases show that our proposed method achieves very promising feature extraction efficiency for palmprint recognition.},
  archive      = {J_TNNLS},
  author       = {Lunke Fei and Shuping Zhao and Wei Jia and Bob Zhang and Jie Wen and Yong Xu},
  doi          = {10.1109/TNNLS.2022.3160597},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9783-9794},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward efficient palmprint feature extraction by learning a single-layer convolution network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuroadaptive performance guaranteed control for multiagent
systems with power integrators and unknown measurement sensitivity.
<em>TNNLS</em>, <em>34</em>(12), 9771–9782. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive performance guaranteed tracking control problem for multiagent systems (MASs) with power integrators and measurement sensitivity. Different from the structural characteristics of existing results, the dynamic of each agent is a power exponential function. A method called adding a power integrator technique is introduced to guarantee that the consensus is achieved of the MASs with power integrators. Different from existing prescribed performance tracking control results for MASs, a new performance guaranteed control approach is proposed in this article, which can guarantee that the relative position error between neighboring agents can converge into the prescribed boundary within preassigned finite time. By utilizing the Nussbaum gain technique and neural networks, a novel control scheme is proposed to solve the unknown measurement sensitivity on the sensor, which successfully relaxes the restrictive condition that the unknown measurement sensitivity must be within a specific range. Based on the Lyapunov functional method, it is proven that the relative position error between neighboring agents can converge into the prescribed boundary within preassigned finite time. Finally, a simulation example is proposed to verify the availability of the control strategy.},
  archive      = {J_TNNLS},
  author       = {Hongjing Liang and Zhixu Du and Tingwen Huang and Yingnan Pan},
  doi          = {10.1109/TNNLS.2022.3160532},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9771-9782},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuroadaptive performance guaranteed control for multiagent systems with power integrators and unknown measurement sensitivity},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). ICVI-ARTMAP: Using incremental cluster validity indices and
adaptive resonance theory reset mechanism to accelerate validation and
achieve multiprototype unsupervised representations. <em>TNNLS</em>,
<em>34</em>(12), 9757–9770. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an adaptive resonance theory predictive mapping (ARTMAP) model, which uses incremental cluster validity indices (iCVIs) to perform unsupervised learning, namely, iCVI-ARTMAP. Incorporating iCVIs to the decision-making and many-to-one mapping capabilities of this adaptive resonance theory (ART)-based model can improve the choices of clusters to which samples are incrementally assigned. These improvements are accomplished by intelligently performing the operations of swapping sample assignments between clusters, splitting and merging clusters, and caching the values of variables when iCVI values need to be recomputed. Using recursive formulations enables iCVI-ARTMAP to considerably reduce the computational burden associated with cluster validity index (CVI)-based offline clustering. In this work, six iCVI-ARTMAP variants were realized via the integration of one information-theoretic and five sum-of-squares-based iCVIs into fuzzy ARTMAP. With proper choice of iCVI, iCVI-ARTMAP either outperformed or performed comparably to three ART-based and four non-ART-based clustering algorithms in experiments using benchmark datasets of different natures. Naturally, the performance of iCVI-ARTMAP is subject to the selected iCVI and its suitability to the data at hand; fortunately, it is a general model in which other iCVIs can be easily embedded.},
  archive      = {J_TNNLS},
  author       = {Leonardo Enzo Brito da Silva and Nagasharath Rayapati and Donald C. Wunsch},
  doi          = {10.1109/TNNLS.2022.3160381},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9757-9770},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ICVI-ARTMAP: Using incremental cluster validity indices and adaptive resonance theory reset mechanism to accelerate validation and achieve multiprototype unsupervised representations},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instance weighted incremental evolution strategies for
reinforcement learning in dynamic environments. <em>TNNLS</em>,
<em>34</em>(12), 9742–9756. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolution strategies (ESs), as a family of black-box optimization algorithms, recently emerge as a scalable alternative to reinforcement learning (RL) approaches such as Q-learning or policy gradient and are much faster when many central processing units (CPUs) are available due to better parallelization. In this article, we propose a systematic incremental learning method for ES in dynamic environments. The goal is to adjust previously learned policy to a new one incrementally whenever the environment changes. We incorporate an instance weighting mechanism with ES to facilitate its learning adaptation while retaining scalability of ES. During parameter updating, higher weights are assigned to instances that contain more new knowledge, thus encouraging the search distribution to move toward new promising areas of parameter space. We propose two easy-to-implement metrics to calculate the weights: instance novelty and instance quality. Instance novelty measures an instance’s difference from the previous optimum in the original environment, while instance quality corresponds to how well an instance performs in the new environment. The resulting algorithm, instance weighted incremental evolution strategies (IW-IESs), is verified to achieve significantly improved performance on challenging RL tasks ranging from robot navigation to locomotion. This article thus introduces a family of scalable ES algorithms for RL domains that enables rapid learning adaptation to dynamic environments.},
  archive      = {J_TNNLS},
  author       = {Zhi Wang and Chunlin Chen and Daoyi Dong},
  doi          = {10.1109/TNNLS.2022.3160173},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9742-9756},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Instance weighted incremental evolution strategies for reinforcement learning in dynamic environments},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning skill characteristics from manipulations.
<em>TNNLS</em>, <em>34</em>(12), 9727–9741. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Percutaneous coronary intervention (PCI) has increasingly become the main treatment for coronary artery disease. The procedure requires high experienced skills and dexterous manipulations. However, there are few techniques to model PCI skill so far. In this study, a learning framework with local and ensemble learning is proposed to learn skill characteristics of different skill-level subjects from their PCI manipulations. Ten interventional cardiologists (four experts and six novices) were recruited to deliver a medical guidewire to two target arteries on a porcine model for in vivo studies. Simultaneously, translation and twist manipulations of thumb, forefinger, and wrist are acquired with electromagnetic (EM) and fiber-optic bend (FOB) sensors, respectively. These behavior data are then processed with wavelet packet decomposition (WPD) under 1–10 levels for feature extraction. The feature vectors are further fed into three candidate individual classifiers in the local learning layer. Furthermore, the local learning results from different manipulation behaviors are fused in the ensemble learning layer with three rule-based ensemble learning algorithms. In subject-dependent skill characteristics learning, the ensemble learning can achieve 100\% accuracy, significantly outperforming the best local result (90\%). Furthermore, ensemble learning can also maintain 73\% accuracy in subject-independent schemes. These promising results demonstrate the great potential of the proposed method to facilitate skill learning in surgical robotics and skill assessment in clinical practice.},
  archive      = {J_TNNLS},
  author       = {Xiao-Hu Zhou and Xiao-Liang Xie and Shi-Qi Liu and Zhen-Liang Ni and Yan-Jie Zhou and Rui-Qi Li and Mei-Jiang Gui and Chen-Chen Fan and Zhen-Qiu Feng and Gui-Bin Bian and Zeng-Guang Hou},
  doi          = {10.1109/TNNLS.2022.3160159},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9727-9741},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning skill characteristics from manipulations},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tackling the challenges in scene graph generation with
local-to-global interactions. <em>TNNLS</em>, <em>34</em>(12),
9713–9726. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we seek new insights into the underlying challenges of the scene graph generation (SGG) task. Quantitative and qualitative analysis of the visual genome (VG) dataset implies: 1) ambiguity: even if interobject relationship contains the same object (or predicate), they may not be visually or semantically similar; 2) asymmetry: despite the nature of the relationship that embodied the direction, it was not well addressed in previous studies; and 3) higher-order contexts: leveraging the identities of certain graph elements can help generate accurate scene graphs. Motivated by the analysis, we design a novel SGG framework, Local-to-global interaction networks (LOGINs). Locally, interactions extract the essence between three instances of subject, object, and background, while baking direction awareness into the network by explicitly constraining the input order of subject and object. Globally, interactions encode the contexts between every graph component (i.e., nodes and edges). Finally, Attract and Repel loss is utilized to fine-tune the distribution of predicate embeddings. By design, our framework enables predicting the scene graph in a bottom-up manner, leveraging the possible complementariness. To quantify how much LOGIN is aware of relational direction, a new diagnostic task called Bidirectional Relationship Classification (BRC) is also proposed. Experimental results demonstrate that LOGIN can successfully distinguish relational direction than existing methods (in BRC task), while showing state-of-the-art results on the VG benchmark (in SGG task).},
  archive      = {J_TNNLS},
  author       = {Sangmin Woo and Junhyug Noh and Kangil Kim},
  doi          = {10.1109/TNNLS.2022.3159990},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9713-9726},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tackling the challenges in scene graph generation with local-to-global interactions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DHI-GAN: Improving dental-based human identification using
generative adversarial networks. <em>TNNLS</em>, <em>34</em>(12),
9700–9712. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a novel semisupervised framework is proposed to tackle the small-sample problem of dental-based human identification (DHI), achieving enhanced performance via a “classifying while generating” paradigm. A generative adversarial network (GAN), called the DHI-GAN, is presented to implement this idea, in which an extra classifier is also dedicatedly proposed to achieve an efficient training procedure. Considering the complex specificities of this problem, except for the noise input of the generator, an identity embedding-guided architecture is proposed to retain informative features for each individual. A parallel spatial and channel fusion attention block is innovatively designed to encourage the model to learn discriminative and informative features by focusing on different regional details and abstract concepts. The attention block is also widely applied to the overall classifier to learn identity-dependent information. A loss combination of the ArcFace and focal loss is utilized to address the small-sample problem. Two parameters are proposed to control the generated samples that are fed into the classifier during the optimization procedure. The proposed DHI-GAN framework is finally validated on a real-world dataset, and the experimental results demonstrate that it outperforms other baselines, achieving a 92.5\% top-one accuracy rate. Most importantly, the proposed GAN-based semisupervised training strategy is able to reduce the required number of training samples (individuals) and can also be incorporated into other classification models. Our code will be available at https://github.com/sculyi/MedicalImages/ .},
  archive      = {J_TNNLS},
  author       = {Yi Lin and Fei Fan and Jianwei Zhang and Jizhe Zhou and Peixi Liao and Hu Chen and Zhenhua Deng and Yi Zhang},
  doi          = {10.1109/TNNLS.2022.3159781},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9700-9712},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DHI-GAN: Improving dental-based human identification using generative adversarial networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic event-triggered formation control for heterogeneous
multiagent systems with nonautonomous leader agent. <em>TNNLS</em>,
<em>34</em>(12), 9685–9699. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the time-varying output formation issue of heterogeneous multiagent systems is investigated by the event-triggered control scheme. Only the outputs of all agents, including leader agent and follower agents, are measurable. The leader agent contains an unknown input signal to generate flexible reference trajectory. Also, only a subset of follower agents have the direct access to the leader agent. First, for each follower, the leader-state compensator is designed to estimate the state of leader. Two kinds of dynamic event-triggered (DET) mechanisms, i.e., node- and edge-based event-triggered schemes, can be equipped on the compensator to save the communication bandwidth of leader–follower and follower–follower interactions, respectively. Then, the distributed formation controller is built to drive each follower achieving formation tracking. The presented control protocol consisting of the DET state compensator and formation controller is fully distributed, which is independent of the global information of communication topology, such as the eigenvalues of Laplacian matrix of communication topology and amount of whole agents. Finally, the numerical experiments and comparison experiments are exhibited to verify the effectiveness of the presented control protocol.},
  archive      = {J_TNNLS},
  author       = {Weizhao Song and Jian Feng and Huaguang Zhang and Wei Wang},
  doi          = {10.1109/TNNLS.2022.3159669},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9685-9699},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic event-triggered formation control for heterogeneous multiagent systems with nonautonomous leader agent},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). G3SR: Global graph guided session-based recommendation.
<em>TNNLS</em>, <em>34</em>(12), 9671–9684. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation tries to make use of anonymous session data to deliver high-quality recommendations under the condition that user profiles and the complete historical behavioral data of a target user are unavailable. Previous works consider each session individually and try to capture user interests within a session. Despite their encouraging results, these models can only perceive intra-session items and cannot draw upon the massive historical relational information. To solve this problem, we propose a novel method named global graph guided session-based recommendation (G3SR). G3SR decomposes the session-based recommendation workflow into two steps. First, a global graph is built upon all session data, from which the global item representations are learned in an unsupervised manner. Then, these representations are refined on session graphs under the graph networks, and a readout function is used to generate session representations for each session. Extensive experiments on two real-world benchmark datasets show remarkable and consistent improvements of the G3SR method over the state-of-the-art methods, especially for cold items.},
  archive      = {J_TNNLS},
  author       = {Zhi-Hong Deng and Chang-Dong Wang and Ling Huang and Jian-Huang Lai and Philip S. Yu},
  doi          = {10.1109/TNNLS.2022.3159592},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9671-9684},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {G3SR: Global graph guided session-based recommendation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MuLHiTA: A novel multiclass classification framework with
multibranch LSTM and hierarchical temporal attention for early detection
of mental stress. <em>TNNLS</em>, <em>34</em>(12), 9657–9670. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental stress is an increasingly common psychological issue leading to diseases such as depression, addiction, and heart attack. In this study, an early detection framework based on electroencephalogram (EEG) data is developed for reducing the risk of these diseases. In existing frameworks, signals are often segmented into smaller sections prior to being input to a deep neural network. However, this approach ignores the fundamental nature of EEG signals as a carrier of valuable information (e.g., the integrity of frequency and phase, and temporal fluctuations of EEG components). As such, this type of segmenting may lead to information loss and a failure to effectively identify mental stress levels. Thus, we propose a novel multiclass classification framework termed multibranch LSTM and hierarchical temporal attention (MuLHiTA) for the early identification of mental stress levels. It specifically focuses on not only intraslice (within each slice) but also interslice (between different slices) samples in parallel. This was achieved by including two complementary branches, each of which integrated a specifically designed attention module into a bidirectional long short-term memory (BLSTM) network, enabling extraction of the most discriminative features from interslice and intraslice EEG signals simultaneously. The outputs of attention modules were then summed to obtain a feature representation that contributes to reduce overfitting and more effective multiclass classification. In addition, electrode positions were optimized using neural activity areas under high-stress conditions, thereby reducing computational costs by minimizing the number of critical electrodes. MuLHiTA was evaluated across one private [Montreal imaging stress task (MIST)] and two publicly available EEG datasets [EEG during mental arithmetic tasks (DMAT) and Simultaneous task EEG workload (STEW)]. These were divided into training and test sets using an 8:2 ratio, and the training data were further divided into training and validation sets using a fivefold cross-validation (CV) method, in which the model with the highest accuracy among the five was selected. The model was trained once more with the full training set, and the test data were then used to evaluate its performance. This approach achieved average classification accuracies of 93.58\%, 91.80\%, and 99.71\% for the MIST, STEW, and DMAT datasets, respectively. Experimental results showed MuLHiTA was superior to state-of-the-art algorithms, including EEGNet, BLSTM, EEGLearn, convolutional neural network (CNN)-long short-term memory (LSTM), and convolutional recurrent attention model (CRAM), for multiclass classification. This demonstrates the viability of MuLHiTA for the early detection of mental stress.},
  archive      = {J_TNNLS},
  author       = {Likun Xia and Yuan Feng and Ziheng Guo and Jinhong Ding and Yanlei Li and Yifan Li and Ming Ma and Guoxi Gan and Yehan Xu and Jingyu Luo and Zhiping Shi and Yong Guan},
  doi          = {10.1109/TNNLS.2022.3159573},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9657-9670},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MuLHiTA: A novel multiclass classification framework with multibranch LSTM and hierarchical temporal attention for early detection of mental stress},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cosine 2DPCA with weighted projection maximization.
<em>TNNLS</em>, <em>34</em>(12), 9643–9656. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-dimensional principal component analysis (2DPCA), known to be very sensitive to outliers, employs the square ${F}$ -norm as the distance metric and only satisfies the optimal objective of maximizing projection variance. However, the objective of minimizing reconstruction errors for all samples is not optimized as much as possible. To handle the problem, a novel cosine objective function is first presented for maximizing weighted projection, in which the 2-norm of vectors with an adjustable power parameter is employed as the distance metric. Not only the objective with the maximum projection distance is accomplished in the cosine objective function, but also the objective with the minimum sum of reconstruction errors is also optimized indirectly. Then, the cosine 2DPCA (Cos-2DPCA) method is proposed, and the greedy iterative algorithm to solve Cos-2DPCA is also developed. The convergence and correlation of solutions are proved theoretically and discussed in detail. Finally, the series of experiments are carried out on the artificial dataset and eight standard datasets, respectively. The results demonstrate that the performances of Cos-2DPCA are significantly improved on the reconstruction, correlation, complexity, and classification, and it outperforms most of the existing robust 2DPCA methods.},
  archive      = {J_TNNLS},
  author       = {Xiaofeng Wang and Leyan Shi and Jun Liu and Minglu Zhang},
  doi          = {10.1109/TNNLS.2022.3159011},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9643-9656},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cosine 2DPCA with weighted projection maximization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative machine learning: Schemes, robustness, and
privacy. <em>TNNLS</em>, <em>34</em>(12), 9625–9642. (<a
href="https://doi.org/10.1109/TNNLS.2022.3169347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning (ML) was originally introduced to solve a complex ML problem in a parallel way for more efficient usage of computation resources. In recent years, such learning has been extended to satisfy other objectives, namely, performing learning in situ on the training data at multiple locations and keeping the training datasets private while still allowing sharing of the model. However, these objectives have led to considerable research on the vulnerabilities of distributed learning both in terms of privacy concerns of the training data and the robustness of the learned overall model due to bad or maliciously crafted training data. This article provides a comprehensive survey of various privacy, security, and robustness issues in distributed ML.},
  archive      = {J_TNNLS},
  author       = {Junbo Wang and Amitangshu Pal and Qinglin Yang and Krishna Kant and Kaiming Zhu and Song Guo},
  doi          = {10.1109/TNNLS.2022.3169347},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9625-9642},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Collaborative machine learning: Schemes, robustness, and privacy},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perception and navigation in autonomous systems in the era
of learning: A survey. <em>TNNLS</em>, <em>34</em>(12), 9604–9624. (<a
href="https://doi.org/10.1109/TNNLS.2022.3167688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous systems possess the features of inferring their own state, understanding their surroundings, and performing autonomous navigation. With the applications of learning systems, like deep learning and reinforcement learning, the visual-based self-state estimation, environment perception, and navigation capabilities of autonomous systems have been efficiently addressed, and many new learning-based algorithms have surfaced with respect to autonomous visual perception and navigation. In this review, we focus on the applications of learning-based monocular approaches in ego-motion perception, environment perception, and navigation in autonomous systems, which is different from previous reviews that discussed traditional methods. First, we delineate the shortcomings of existing classical visual simultaneous localization and mapping (vSLAM) solutions, which demonstrate the necessity to integrate deep learning techniques. Second, we review the visual-based environmental perception and understanding methods based on deep learning, including deep learning-based monocular depth estimation, monocular ego-motion prediction, image enhancement, object detection, semantic segmentation, and their combinations with traditional vSLAM frameworks. Then, we focus on the visual navigation based on learning systems, mainly including reinforcement learning and deep reinforcement learning. Finally, we examine several challenges and promising directions discussed and concluded in related research of learning systems in the era of computer science and robotics.},
  archive      = {J_TNNLS},
  author       = {Yang Tang and Chaoqiang Zhao and Jianrui Wang and Chongzhen Zhang and Qiyu Sun and Wei Xing Zheng and Wenli Du and Feng Qian and Jürgen Kurths},
  doi          = {10.1109/TNNLS.2022.3167688},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9604-9624},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Perception and navigation in autonomous systems in the era of learning: A survey},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards personalized federated learning. <em>TNNLS</em>,
<em>34</em>(12), 9587–9603. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In parallel with the rapid adoption of artificial intelligence (AI) empowered by advances in AI research, there has been growing awareness and concerns of data privacy. Recent significant developments in the data regulation landscape have prompted a seismic shift in interest toward privacy-preserving AI. This has contributed to the popularity of Federated Learning (FL), the leading paradigm for the training of machine learning models on data silos in a privacy-preserving manner. In this survey, we explore the domain of personalized FL (PFL) to address the fundamental challenges of FL on heterogeneous data, a universal characteristic inherent in all real-world datasets. We analyze the key motivations for PFL and present a unique taxonomy of PFL techniques categorized according to the key challenges and personalization strategies in PFL. We highlight their key ideas, challenges, opportunities, and envision promising future trajectories of research toward a new PFL architectural design, realistic PFL benchmarking, and trustworthy PFL approaches.},
  archive      = {J_TNNLS},
  author       = {Alysa Ziying Tan and Han Yu and Lizhen Cui and Qiang Yang},
  doi          = {10.1109/TNNLS.2022.3160699},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {9587-9603},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Towards personalized federated learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging balanced semantic embedding for generative
zero-shot learning. <em>TNNLS</em>, <em>34</em>(11), 9575–9582. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative (generalized) zero-shot learning [(G)ZSL] models aim to synthesize unseen class features by using only seen class feature and attribute pairs as training data. However, the generated fake unseen features tend to be dominated by the seen class features and thus classified as seen classes, which can lead to inferior performances under zero-shot learning (ZSL), and unbalanced results under generalized ZSL (GZSL). To address this challenge, we tailor a novel balanced semantic embedding generative network (BSeGN), which incorporates balanced semantic embedding learning into generative learning scenarios in the pursuit of unbiased GZSL. Specifically, we first design a feature-to-semantic embedding module (FEM) to distinguish real seen and fake unseen features collaboratively with the generator in an online manner. We introduce the bidirectional contrastive and balance losses for the FEM learning, which can guarantee a balanced prediction for the interdomain features. In turn, the updated FEM can boost the learning of the generator. Next, we propose a multilevel feature integration module (mFIM) from the cycle-consistency branch of BSeGN, which can mitigate the domain bias through feature enhancement. To the best of our knowledge, this is the first work to explore embedding and generative learning jointly within the field of ZSL. Extensive evaluations on four benchmarks demonstrate the superiority of BSeGN over its state-of-the-art counterparts.},
  archive      = {J_TNNLS},
  author       = {Guo-Sen Xie and Xu-Yao Zhang and Tian-Zhu Xiang and Fang Zhao and Zheng Zhang and Ling Shao and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3208525},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9575-9582},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Leveraging balanced semantic embedding for generative zero-shot learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite-sample analysis of deep CCA-based unsupervised
post-nonlinear multimodal learning. <em>TNNLS</em>, <em>34</em>(11),
9568–9574. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Canonical correlation analysis (CCA) has been essential in unsupervised multimodal/multiview latent representation learning and data fusion. Classic CCA extracts shared information from multiple modalities of data using linear transformations. In recent years, deep neural networks-based nonlinear feature extractors were combined with CCA to come up with new variants, namely the “ DeepCCA ” line of work. These approaches were shown to have enhanced performance in many applications. However, theoretical supports of DeepCCA are often lacking. To address this challenge, the recent work of Lyu and Fu (2020) showed that, under a reasonable postnonlinear generative model, a carefully designed DeepCCA criterion provably removes unknown distortions in data generation and identifies the shared information across modalities. Nonetheless, a critical assumption used by Lyu and Fu (2020) for identifiability analysis was that unlimited data is available, which is unrealistic. This brief paper puts forth a finite-sample analysis of the DeepCCA method by Lyu and Fu (2020). The main result is that the finite-sample version of the method can still estimate the shared information with a guaranteed accuracy when the number of samples is sufficiently large. Our analytical approach is a nontrivial integration of statistical learning, numerical differentiation, and robust system identification, which may be of interest beyond the scope of DeepCCA and benefit other unsupervised learning paradigms.},
  archive      = {J_TNNLS},
  author       = {Qi Lyu and Xiao Fu},
  doi          = {10.1109/TNNLS.2022.3160407},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9568-9574},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-sample analysis of deep CCA-based unsupervised post-nonlinear multimodal learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RegNet: Self-regulated network for image classification.
<em>TNNLS</em>, <em>34</em>(11), 9562–9567. (<a
href="https://doi.org/10.1109/TNNLS.2022.3158966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ResNet and its variants have achieved remarkable successes in various computer vision tasks. Despite its success in making gradient flow through building blocks, the information communication of intermediate layers of blocks is ignored. To address this issue, in this brief, we propose to introduce a regulator module as a memory mechanism to extract complementary features of the intermediate layers, which are further fed to the ResNet. In particular, the regulator module is composed of convolutional recurrent neural networks (RNNs) [e.g., convolutional long short-term memories (LSTMs) or convolutional gated recurrent units (GRUs)], which are shown to be good at extracting spatio-temporal information. We named the new regulated network as regulated residual network (RegNet). The regulator module can be easily implemented and appended to any ResNet architecture. Experimental results on three image classification datasets have demonstrated the promising performance of the proposed architecture compared with the standard ResNet, squeeze-and-excitation ResNet, and other state-of-the-art architectures.},
  archive      = {J_TNNLS},
  author       = {Jing Xu and Yu Pan and Xinglin Pan and Steven Hoi and Zhang Yi and Zenglin Xu},
  doi          = {10.1109/TNNLS.2022.3158966},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9562-9567},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RegNet: Self-regulated network for image classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural adaptive distributed formation control of nonlinear
multi-UAVs with unmodeled dynamics. <em>TNNLS</em>, <em>34</em>(11),
9555–9561. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of neural adaptive distributed formation control is investigated for quadrotor multiple unmanned aerial vehicles (UAVs) subject to unmodeled dynamics and disturbance. The quadrotor UAV system is divided into two parts: the position subsystem and the attitude subsystem. A virtual position controller based on backstepping is designed to address the coupling constraints and generate two command signals for the attitude subsystem. By establishing the communication mechanism between the UAVs and the virtual leader, a distributed formation scheme, which uses the UAVs’ local information and makes each UAV update its position and velocity according to the information of neighboring UAVs, is proposed to form the required formation flight. By designing a neural adaptive sliding mode controller (SMC) for multi-UAVs, the compound uncertainties (including nonlinearities, unmodeled dynamics, and external disturbances) are compensated for to guarantee good tracking performance. The Lyapunov theory is used to prove that the tracking error of each UAV converges to an adjustable neighborhood of zero. Finally, the simulation results demonstrate the effectiveness of the proposed scheme.},
  archive      = {J_TNNLS},
  author       = {Yajing Yu and Jian Guo and Choon Ki Ahn and Zhengrong Xiang},
  doi          = {10.1109/TNNLS.2022.3157079},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9555-9561},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural adaptive distributed formation control of nonlinear multi-UAVs with unmodeled dynamics},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward optimal synthesis of discrete-time hopfield neural
network. <em>TNNLS</em>, <em>34</em>(11), 9549–9554. (<a
href="https://doi.org/10.1109/TNNLS.2022.3156107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research brief, the relationship between eigenvectors (with {+1, −1} components) of a synaptic weight matrix $W$ and the stable/anti-stable states of discrete-time Hopfield associative memory (HAM) is established. Also, the synthesis of $W$ with desired stable/anti-stable states using spectral representation of $W$ in even/odd dimension is discussed when the threshold vector is a non-zero vector. Freedom in choice of eigenvalues is capitalized to improve the noise immunity of the Hopfield neural network (HNN). Also, the problem of optimal synthesis of Hopfield Associative memory is formulated.},
  archive      = {J_TNNLS},
  author       = {Garimella Rama Murthy},
  doi          = {10.1109/TNNLS.2022.3156107},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9549-9554},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward optimal synthesis of discrete-time hopfield neural network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Positive consensus of fractional-order multiagent systems
over directed graphs. <em>TNNLS</em>, <em>34</em>(11), 9542–9548. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the positive consensus problem of a special kind of interconnected positive systems over directed graphs. They are composed of multiple fractional-order continuous-time positive linear systems. Unlike most existing works in the literature, we study this problem for the first time, in which the communication topology of agents is described by a directed graph containing a spanning tree. This is a more general and new scenario due to the interplay between the eigenvalues of the Laplacian matrix and the controller gains, which renders the positivity analysis fairly challenging. Based on the existing results in spectral graph theory, fractional-order systems (FOSs) theory, and positive systems theory, we derive several necessary and/or sufficient conditions on the positive consensus of fractional-order multiagent systems (PCFMAS). It is shown that the protocol, which is designed for a specific graph, can solve the positive consensus problem of agents over an additional set of directed graphs. Finally, a comprehensive comparison study of different approaches is carried out, which shows that the proposed approaches have advantages over the existing ones.},
  archive      = {J_TNNLS},
  author       = {Jason J. R. Liu and James Lam and Ka-Wai Kwok},
  doi          = {10.1109/TNNLS.2022.3152939},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9542-9548},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Positive consensus of fractional-order multiagent systems over directed graphs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On optimal learning with random features. <em>TNNLS</em>,
<em>34</em>(11), 9536–9541. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider supervised learning in a reproducing kernel Hilbert space (RKHS) using random features. We show that the optimal rate is obtained under suitable regularity conditions, and at the same time improving on the existing bounds on the number of random features required. As a straightforward extension, distributed learning in the simple setting of one-shot communication is also considered that achieves the same optimal rate.},
  archive      = {J_TNNLS},
  author       = {Jiamin Liu and Heng Lian},
  doi          = {10.1109/TNNLS.2022.3152270},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9536-9541},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On optimal learning with random features},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DualConv: Dual convolutional kernels for lightweight deep
neural networks. <em>TNNLS</em>, <em>34</em>(11), 9528–9535. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNN) architectures are generally heavy on memory and computational requirements which make them infeasible for embedded systems with limited hardware resources. We propose dual convolutional kernels (DualConv) for constructing lightweight deep neural networks. DualConv combines $3\times 3$ and $1\times 1$ convolutional kernels to process the same input feature map channels simultaneously and exploits the group convolution technique to efficiently arrange convolutional filters. DualConv can be employed in any CNN model such as VGG-16 and ResNet-50 for image classification, you only look once (YOLO) and R-CNN for object detection, or fully convolutional network (FCN) for semantic segmentation. In this work, we extensively test DualConv for classification since these network architectures form the backbone for many other tasks. We also test DualConv for image detection on YOLO-V3. Experimental results show that, combined with our structural innovations, DualConv significantly reduces the computational cost and number of parameters of deep neural networks while surprisingly achieving slightly higher accuracy than the original models in some cases. We use DualConv to further reduce the number of parameters of the lightweight MobileNetV2 by 54\% with only 0.68\% drop in accuracy on CIFAR-100 dataset. When the number of parameters is not an issue, DualConv increases the accuracy of MobileNetV1 by 4.11\% on the same dataset. Furthermore, DualConv significantly improves the YOLO-V3 object detection speed and improves its accuracy by 4.4\% on PASCAL visual object classes (VOC) dataset.},
  archive      = {J_TNNLS},
  author       = {Jiachen Zhong and Junying Chen and Ajmal Mian},
  doi          = {10.1109/TNNLS.2022.3151138},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9528-9535},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DualConv: Dual convolutional kernels for lightweight deep neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). KNNENS: A k-nearest neighbor ensemble-based method for
incremental learning under data stream with emerging new classes.
<em>TNNLS</em>, <em>34</em>(11), 9520–9527. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, we investigate the problem of incremental learning under data stream with emerging new classes (SENC). In the literature, existing approaches encounter the following problems: 1) yielding high false positive for the new class; i) having long prediction time; and 3) having access to true labels for all instances, which is unrealistic and unacceptable in real-life streaming tasks. Therefore, we propose the $k$ -Nearest Neighbor ENSemble-based method (KNNENS) to handle these problems. The KNNENS is effective to detect the new class and maintains high classification performance for known classes. It is also efficient in terms of run time and does not require true labels of new class instances for model update, which is desired in real-life streaming classification tasks. Experimental results show that the KNNENS achieves the best performance on four benchmark datasets and three real-world data streams in terms of accuracy and F1-measure and has a relatively fast run time compared to four reference methods. Codes are available at https://github.com/Ntriver/KNNENS .},
  archive      = {J_TNNLS},
  author       = {Jianjun Zhang and Ting Wang and Wing W. Y. Ng and Witold Pedrycz},
  doi          = {10.1109/TNNLS.2022.3149991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9520-9527},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {KNNENS: A k-nearest neighbor ensemble-based method for incremental learning under data stream with emerging new classes},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite-time stability control of uncertain nonlinear systems
with self-limiting control terms. <em>TNNLS</em>, <em>34</em>(11),
9514–9519. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, we define a self-limiting control term, which has the function of guaranteeing the boundedness of variables. Then, we apply it to a finite-time stability control problem. For nonstrict feedback nonlinear systems, a finite-time adaptive control scheme, which contains a piecewise differentiable function, is proposed. This scheme can eliminate the singularity of derivative of a fractional exponential function. By adding a self-limiting term to the controller and the virtual control law of each subsystem, the boundedness of the overall system state is guaranteed. Then the unknown continuous functions are estimated by neural networks (NNs). The output of the closed-loop system tracks the desired trajectory, and the tracking error converges to a small neighborhood of the equilibrium point in finite time. The theoretical results are illustrated by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Jiaming Zhu and Yuequan Yang and Tianping Zhang and Zhiqiang Cao},
  doi          = {10.1109/TNNLS.2022.3149894},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9514-9519},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time stability control of uncertain nonlinear systems with self-limiting control terms},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Game-theoretic inverse reinforcement learning: A
differential pontryagin’s maximum principle approach. <em>TNNLS</em>,
<em>34</em>(11), 9506–9513. (<a
href="https://doi.org/10.1109/TNNLS.2022.3148376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief proposes a game-theoretic inverse reinforcement learning (GT-IRL) framework, which aims to learn the parameters in both the dynamic system and individual cost function of multistage games from demonstrated trajectories. Different from the probabilistic approaches in computer science community and residual minimization solutions in control community, our framework addresses the problem in a deterministic setting by differentiating Pontryagin’s maximum principle (PMP) equations of open-loop Nash equilibrium (OLNE), which is inspired by Jin et al. (2020). The differentiated equations for a multi-player nonzero-sum multistage game are shown to be equivalent to the PMP equations for another affine-quadratic nonzero-sum multistage game and can be solved by some explicit recursions. A similar result is established for two-player zero-sum games. Simulation examples are presented to demonstrate the effectiveness of our proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Kun Cao and Lihua Xie},
  doi          = {10.1109/TNNLS.2022.3148376},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9506-9513},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Game-theoretic inverse reinforcement learning: A differential pontryagin’s maximum principle approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonlinear feature selection neural network via structured
sparse regularization. <em>TNNLS</em>, <em>34</em>(11), 9493–9505. (<a
href="https://doi.org/10.1109/TNNLS.2022.3209716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important and effective data preprocessing method, which can remove the noise and redundant features while retaining the relevant and discriminative features in high-dimensional data. In real-world applications, the relationships between data samples and their labels are usually nonlinear. However, most of the existing feature selection models focus on learning a linear transformation matrix, which cannot capture such a nonlinear structure in practice and will degrade the performance of downstream tasks. To address the issue, we propose a novel nonlinear feature selection method to select those most relevant and discriminative features in high-dimensional dataset. Specifically, our method learns the nonlinear structure of high-dimensional data by a neural network with cross entropy loss function, and then using the structured sparsity norm such as $\ell _{2,p}$ -norm to regularize the weights matrix connecting the input layer and the first hidden layer of the neural network model to learn weight of each feature. Therefore, a structural sparse weights matrix is obtained by conducting nonlinear learning based on a neural network with structured sparsity regularization. Then, we use the gradient descent method to achieve the optimal solution of the proposed model. Evaluating the experimental results on several synthetic datasets and real-world datasets shows the effectiveness and superiority of the proposed nonlinear feature selection model.},
  archive      = {J_TNNLS},
  author       = {Rong Wang and Jintang Bian and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3209716},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9493-9505},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonlinear feature selection neural network via structured sparse regularization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local embedding learning via landmark-based dynamic
connections. <em>TNNLS</em>, <em>34</em>(11), 9481–9492. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) is one of the most effective and popular methods to reduce the dimensionality of data with Gaussian assumption. However, LDA cannot handle non-Gaussian data because the center point is incompetent to represent the distribution of data. Some existing methods based on graph embedding focus on exploring local structures via pairwise relationships of data for addressing the non-Gaussian issue. Due to massive pairwise relationships, the computational complexity is high as well as the locally optimal solution is hard to find. To address these issues, we propose a novel and efficient local embedding learning via landmark-based dynamic connections (LDC) in which we leverage several landmarks to represent different subclusters in the same class and establish the connections between each point and landmark. Furthermore, in order to explore the relationship of landmarks pairwise more precisely, the relationship between each point and their corresponding neighbor landmarks are found in the optimal subspace, rather than the original space, which can avoid the negative influence of the noises. We also propose an efficient iterative algorithm to deal with the proposed ratio minimization problem. Extensive experiments conducted on several real-world datasets have demonstrated the advantages of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Canyu Zhang and Zheng Wang and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3203014},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9481-9492},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local embedding learning via landmark-based dynamic connections},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative projected clustering via unsupervised LDA.
<em>TNNLS</em>, <em>34</em>(11), 9466–9480. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on the projected clustering problem. Specifically, an efficient and parameter-free clustering model, named discriminative projected clustering (DPC), is proposed for simultaneously low-dimensional and discriminative projection learning and clustering, from the perspective of least squares regression. The proposed DPC, a constrained regression model, aims at finding both a transformation matrix and a binary indicator matrix to minimize the sum-of-squares error. Theoretically, a significant conclusion is drawn and used to reveal the connection between DPC and linear discriminant analysis (LDA). Experimentally, experiments are conducted on both toy and real-world data to validate the effectiveness and efficiency of DPC; experiments are also conducted on hyperspectral images to further verify its practicability in real-world applications. Experimental results demonstrate that DPC achieves comparable or superior results to some state-of-the-art clustering methods.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Xia Dong and Zhanxuan Hu and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3202719},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9466-9480},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative projected clustering via unsupervised LDA},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Composition-aware image steganography through adversarial
self-generated supervision. <em>TNNLS</em>, <em>34</em>(11), 9451–9465.
(<a href="https://doi.org/10.1109/TNNLS.2022.3175627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steganography is an important and prevailing information hiding tool to perform secret message transmission in an open environment. Existing steganography methods can mainly fall into two categories: predefined rule-based and data-driven methods. The former is susceptible to the statistical attack, while the latter adopts the deep convolution neural networks to promote security. However, deep learning-based methods suffer from perceptible artificial artifacts or deep steganalysis. In this article, we introduce a novel composition-aware image steganography (CAIS) to guarantee both visual security and resistance to deep steganalysis through the self-generated supervision. The key innovation is an adversarial composition estimation module, which has integrated the rule-based composition method and generative adversarial network to help synthesize steganographic images with more naturalness. We first perform a rule-based image blending method to obtain infinite synthetically data–label pairs. Then, we utilize an adversarial composition estimation branch to recognize the message feature pattern from the composite image based on these self-generated data–label pairs. Through the adversarial training, we force the steganography function to synthesize steganographic images, which can fool the composition estimation network. Thus, the proposed CAIS can achieve better information hiding and higher security to resist deep steganalysis. Furthermore, an effective global-and-part checking is designed to alleviate visual artifacts caused by hiding secret information. We conduct a comprehensive analysis of CAIS from various aspects (e.g., security and robustness) to verify the superior performance of the proposed method. Comprehensive experimental results on three large-scale widely used datasets have demonstrated the superior performance of our CAIS compared with several state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Ziqiang Zheng and Yuanmeng Hu and Yi Bin and Xing Xu and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2022.3175627},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9451-9465},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Composition-aware image steganography through adversarial self-generated supervision},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cumulant GAN. <em>TNNLS</em>, <em>34</em>(11), 9439–9450.
(<a href="https://doi.org/10.1109/TNNLS.2022.3161127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel loss function for training generative adversarial networks (GANs) aiming toward deeper theoretical understanding as well as improved stability and performance for the underlying optimization problem. The new loss function is based on cumulant generating functions (CGFs) giving rise to Cumulant GAN. Relying on a recently derived variational formula, we show that the corresponding optimization problem is equivalent to Rényi divergence minimization, thus offering a (partially) unified perspective of GAN losses: the Rényi family encompasses Kullback–Leibler divergence (KLD), reverse KLD, Hellinger distance, and $\chi ^{2}$ -divergence. Wasserstein GAN is also a member of cumulant GAN. In terms of stability, we rigorously prove the linear convergence of cumulant GAN to the Nash equilibrium for a linear discriminator, Gaussian distributions, and the standard gradient descent ascent algorithm. Finally, we experimentally demonstrate that image generation is more robust relative to Wasserstein GAN and it is substantially improved in terms of both inception score (IS) and Fréchet inception distance (FID) when both weaker and stronger discriminators are considered.},
  archive      = {J_TNNLS},
  author       = {Yannis Pantazis and Dipjyoti Paul and Michail Fasoulakis and Yannis Stylianou and Markos A. Katsoulakis},
  doi          = {10.1109/TNNLS.2022.3161127},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9439-9450},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cumulant GAN},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density distillation for fast nonparametric density
estimation. <em>TNNLS</em>, <em>34</em>(11), 9424–9438. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric density estimation has been extensively used in various application scenarios and theoretical models. However, the modeling of these powerful methods is inseparable from the sample data and comes at the cost of repeated and intensive kernel calculations, which makes their efficiency greatly affected by the sample scale, data dimension, and evaluation scale. Inspired by the knowledge distillation method, a student–teacher paradigm model named density convolutional neural network (DCNN) is proposed in this article. The method extracts the density knowledge of the samples based on the density convolution rule and transfers it to a compact and small deep neural network, in order to separate the sample data from the modeling and avoid the cumbersome kernel calculations. Experimental results show the superiority of the proposed method to various nonparametric estimation methods in terms of accuracy, stability, processing efficiency, and low-storage advantage. Especially, for the estimation speed, a univariate density estimation on 1.0E + 08 evaluation points using GPU only takes 1.57 s, and a 10-D multivariate density estimation on 1.0E + 08 evaluation points only takes 10.50 s, which makes our method very suitable for real-time and large-scale repetitive density estimation tasks.},
  archive      = {J_TNNLS},
  author       = {Bopeng Fang and Shifeng Chen and Zhurong Dong},
  doi          = {10.1109/TNNLS.2022.3160939},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9424-9438},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Density distillation for fast nonparametric density estimation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GAMMA: Graph attention model for multiple agents to solve
team orienteering problem with multiple depots. <em>TNNLS</em>,
<em>34</em>(11), 9412–9423. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present an attention-based encoder–decoder model to approximately solve the team orienteering problem with multiple depots (TOPMD). The TOPMD instance is an NP-hard combinatorial optimization problem that involves multiple agents (or autonomous vehicles) and not purely Euclidean (straight line distance) graph edge weights. In addition, to avoid tedious computations on dataset creation, we provide an approach to generate synthetic data on the fly for effectively training the model. Furthermore, to evaluate our proposed model, we conduct two experimental studies on the multi-agent reconnaissance mission planning problem formulated as TOPMD. First, we characterize the model based on the training configurations to understand the scalability of the proposed approach to unseen configurations. Second, we evaluate the solution quality of the model against several baselines—heuristics, competing machine learning (ML), and exact approaches, on several reconnaissance scenarios. The experimental results indicate that training the model with a maximum number of agents, a moderate number of targets (or nodes to visit), and moderate travel length, performs well across a variety of conditions. Furthermore, the results also reveal that the proposed approach offers a more tractable and higher quality (or competitive) solution in comparison with existing attention-based models, stochastic heuristic approach, and standard mixed-integer programming solver under the given experimental conditions. Finally, the different experimental evaluations reveal that the proposed data generation approach for training the model is highly effective.},
  archive      = {J_TNNLS},
  author       = {Prashant Sankaran and Katie McConky and Moises Sudit and Héctor Ortiz-Peña},
  doi          = {10.1109/TNNLS.2022.3159671},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9412-9423},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GAMMA: Graph attention model for multiple agents to solve team orienteering problem with multiple depots},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved stability analysis results of generalized neural
networks with time-varying delays. <em>TNNLS</em>, <em>34</em>(11),
9404–9411. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the stability problem of generalized neural networks (GNNs) with time-varying delay. The delay has two cases: the first case is that the delay’s derivative has only upper bound, the other case has no information of its derivative or itself is not differentiable. For both two cases, we provide novel stability criteria based on novel Lyapunov–Krasovskii functionals (LKFs) and new negative definite conditions (NDCs) of matrix-valued cubic polynomials. In contrast with the existing methods, in this article, the proposed criteria do not need to introduce extra state variables, and the positive-definite constraint on the novel LKF is relaxed. Moreover, based on free-matrix-based inequality (FMBI) and new NDCs, the stability conditions are expressed as linear matrix inequalities (LMIs). Eventually, the merits and efficiency of the proposed criteria are checked through some classical numerical examples.},
  archive      = {J_TNNLS},
  author       = {Zhengliang Zhai and Huaicheng Yan and Shiming Chen and Hongbing Zeng and Meng Wang},
  doi          = {10.1109/TNNLS.2022.3159625},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9404-9411},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improved stability analysis results of generalized neural networks with time-varying delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised attentive generative adversarial networks
for video anomaly detection. <em>TNNLS</em>, <em>34</em>(11), 9389–9403.
(<a href="https://doi.org/10.1109/TNNLS.2022.3159538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection (VAD) refers to the discrimination of unexpected events in videos. The deep generative model (DGM)-based method learns the regular patterns on normal videos and expects the learned model to yield larger generative errors for abnormal frames. However, DGM cannot always do so, since it usually captures the shared patterns between normal and abnormal events, which results in similar generative errors for them. In this article, we propose a novel self-supervised framework for unsupervised VAD to tackle the above-mentioned problem. To this end, we design a novel self-supervised attentive generative adversarial network (SSAGAN), which is composed of the self-attentive predictor, the vanilla discriminator, and the self-supervised discriminator. On the one hand, the self-attentive predictor can capture the long-term dependences for improving the prediction qualities of normal frames. On the other hand, the predicted frames are fed to the vanilla discriminator and self-supervised discriminator for performing true–false discrimination and self-supervised rotation detection, respectively. Essentially, the role of the self-supervised task is to enable the predictor to encode semantic information into the predicted normal frames via adversarial training, in order for the angles of rotated normal frames can be detected. As a result, our self-supervised framework lessens the generalization ability of the model to abnormal frames, resulting in larger detection errors for abnormal frames. Extensive experimental results indicate that SSAGAN outperforms other state-of-the-art methods, which demonstrates the validity and advancement of SSAGAN.},
  archive      = {J_TNNLS},
  author       = {Chao Huang and Jie Wen and Yong Xu and Qiuping Jiang and Jian Yang and Yaowei Wang and David Zhang},
  doi          = {10.1109/TNNLS.2022.3159538},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9389-9403},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised attentive generative adversarial networks for video anomaly detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FANet: A feedback attention network for improved biomedical
image segmentation. <em>TNNLS</em>, <em>34</em>(11), 9375–9388. (<a
href="https://doi.org/10.1109/TNNLS.2022.3159394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase of available large clinical and experimental datasets has contributed to a substantial amount of important contributions in the area of biomedical image analysis. Image segmentation, which is crucial for any quantitative analysis, has especially attracted attention. Recent hardware advancement has led to the success of deep learning approaches. However, although deep learning models are being trained on large datasets, existing methods do not use the information from different learning epochs effectively. In this work, we leverage the information of each training epoch to prune the prediction maps of the subsequent epochs. We propose a novel architecture called feedback attention network (FANet) that unifies the previous epoch mask with the feature map of the current training epoch. The previous epoch mask is then used to provide hard attention to the learned feature maps at different convolutional layers. The network also allows rectifying the predictions in an iterative fashion during the test time. We show that our proposed feedback attention model provides a substantial improvement on most segmentation metrics tested on seven publicly available biomedical imaging datasets demonstrating the effectiveness of FANet. The source code is available at https://github.com/nikhilroxtomar/FANet .},
  archive      = {J_TNNLS},
  author       = {Nikhil Kumar Tomar and Debesh Jha and Michael A. Riegler and Håvard D. Johansen and Dag Johansen and Jens Rittscher and Pål Halvorsen and Sharib Ali},
  doi          = {10.1109/TNNLS.2022.3159394},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9375-9388},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FANet: A feedback attention network for improved biomedical image segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust peak detection for holter ECGs by self-organized
operational neural networks. <em>TNNLS</em>, <em>34</em>(11), 9363–9374.
(<a href="https://doi.org/10.1109/TNNLS.2022.3158867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although numerous R-peak detectors have been proposed in the literature, their robustness and performance levels may significantly deteriorate in low-quality and noisy signals acquired from mobile electrocardiogram (ECG) sensors, such as Holter monitors. Recently, this issue has been addressed by deep 1-D convolutional neural networks (CNNs) that have achieved state-of-the-art performance levels in Holter monitors; however, they pose a high complexity level that requires special parallelized hardware setup for real-time processing. On the other hand, their performance deteriorates when a compact network configuration is used instead. This is an expected outcome as recent studies have demonstrated that the learning performance of CNNs is limited due to their strictly homogenous configuration with the sole linear neuron model. This has been addressed by operational neural networks (ONNs) with their heterogenous network configuration encapsulating neurons with various nonlinear operators. In this study, to further boost the peak detection performance along with an elegant computational efficiency, we propose 1-D Self-Organized ONNs (Self-ONNs) with generative neurons. The most crucial advantage of 1-D Self-ONNs over the ONNs is their self-organization capability that voids the need to search for the best operator set per neuron since each generative neuron has the ability to create the optimal operator during training. The experimental results over the China Physiological Signal Challenge-2020 (CPSC) dataset with more than one million ECG beats show that the proposed 1-D Self-ONNs can significantly surpass the state-of-the-art deep CNN with less computational complexity. Results demonstrate that the proposed solution achieves a 99.10\% F1-score, 99.79\% sensitivity, and 98.42\% positive predictivity in the CPSC dataset, which is the best R-peak detection performance ever achieved.},
  archive      = {J_TNNLS},
  author       = {Moncef Gabbouj and Serkan Kiranyaz and Junaid Malik and Muhammad Uzair Zahid and Turker Ince and Muhammad E. H. Chowdhury and Amith Khandakar and Anas Tahir},
  doi          = {10.1109/TNNLS.2022.3158867},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9363-9374},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust peak detection for holter ECGs by self-organized operational neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Embedding graph auto-encoder for graph clustering.
<em>TNNLS</em>, <em>34</em>(11), 9352–9362. (<a
href="https://doi.org/10.1109/TNNLS.2022.3158654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clustering, aiming to partition nodes of a graph into various groups via an unsupervised approach, is an attractive topic in recent years. To improve the representative ability, several graph auto-encoder (GAE) models, which are based on semisupervised graph convolution networks (GCN), have been developed and they have achieved impressive results compared with traditional clustering methods. However, all existing methods either fail to utilize the orthogonal property of the representations generated by GAE or separate the clustering and the training of neural networks. We first prove that the relaxed $k$ -means will obtain an optimal partition in the inner-product distance used space. Driven by theoretical analysis about relaxed $k$ -means, we design a specific GAE-based model for graph clustering to be consistent with the theory, namely Embedding GAE (EGAE). The learned representations are well explainable so that the representations can be also used for other tasks. To induce the neural network to produce deep features that are appropriate for the specific clustering model, the relaxed $k$ -means and GAE are learned simultaneously. Meanwhile, the relaxed $k$ -means can be equivalently regarded as a decoder that attempts to learn representations that can be linearly constructed by some centroid vectors. Accordingly, EGAE consists of one encoder and dual decoders. Extensive experiments are conducted to prove the superiority of EGAE and the corresponding theoretical analyses.},
  archive      = {J_TNNLS},
  author       = {Hongyuan Zhang and Pei Li and Rui Zhang and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3158654},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9352-9362},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Embedding graph auto-encoder for graph clustering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semisupervised cross-scale graph prototypical network for
hyperspectral image classification. <em>TNNLS</em>, <em>34</em>(11),
9337–9351. (<a
href="https://doi.org/10.1109/TNNLS.2022.3158280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, the acquirement of labeled samples for hyperspectral image (HSI) is time-consuming and labor-intensive. It frequently induces the trouble of model overfitting and performance degradation for the supervised methodologies in HSI classification (HSIC). Fortunately, semisupervised learning can alleviate this deficiency, and graph convolutional network (GCN) is one of the most effective semisupervised approaches, which propagates the node information from each other in a transductive manner. In this study, we propose a cross-scale graph prototypical network (X-GPN) to achieve semisupervised high-quality HSIC. Specifically, considering the multiscale appearance of the land covers in the same remotely captured scene, we involve the neighborhoods of different scales to construct the adjacency matrices and simultaneously design a multibranch framework to investigate the abundant spectral–spatial features through graph convolutions. Furthermore, to exploit the complementary information between different scales, we simply employ the standard 1-D convolution to excavate the dependence of the intranode and concatenate the output with the features generated from other scales. Intuitively, different branches for various samples should have different importance to predict their categories. Thus, we develop a self-branch attentional addition (SBAA) module to adaptively highlight the most critical features produced by multiple branches. In addition, different from previous GCN for HSIC, we devise an innovative prototypical layer comprising a distance-based cross-entropy (DCE) loss function and a novel temporal entropy-based regularizer (TER), which can enhance the discrimination and representativeness of the node features and prototypes actively. Extensive experiments demonstrate that the proposed X-GPN is superior to the classic and state-of-the-art (SOTA) methods in terms of the classification performance.},
  archive      = {J_TNNLS},
  author       = {Bobo Xi and Jiaojiao Li and Yunsong Li and Rui Song and Yuchao Xiao and Qian Du and Jocelyn Chanussot},
  doi          = {10.1109/TNNLS.2022.3158280},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9337-9351},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised cross-scale graph prototypical network for hyperspectral image classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A levenberg-marquardt algorithm for sparse identification of
dynamical systems. <em>TNNLS</em>, <em>34</em>(11), 9323–9336. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low complexity of a system model is essential for its use in real-time applications. However, sparse identification methods commonly have stringent requirements that exclude them from being applied in an industrial setting. In this article, we introduce a flexible method for the sparse identification of dynamical systems described by ordinary differential equations. Our method relieves many of the requirements imposed by other methods that relate to the structure of the model and the dataset, such as fixed sampling rates, full state measurements, and linearity of the model. The Levenberg–Marquardt algorithm is used to solve the identification problem. We show that the Levenberg–Marquardt algorithm can be written in a form that enables parallel computing, which greatly diminishes the time required to solve the identification problem. An efficient backward elimination strategy is presented to construct a lean system model.},
  archive      = {J_TNNLS},
  author       = {Mark Haring and Esten Ingar Grøtli and Signe Riemer-Sørensen and Katrine Seel and Kristian Gaustad Hanssen},
  doi          = {10.1109/TNNLS.2022.3157963},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9323-9336},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A levenberg-marquardt algorithm for sparse identification of dynamical systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Backstepping technology-based adaptive boundary ILC for an
input–output-constrained flexible beam. <em>TNNLS</em>, <em>34</em>(11),
9314–9322. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on vibration suppression of an Euler–Bernoulli beam which is subject to external disturbance. By integrating backstepping technique, an adaptive boundary iterative learning control (ABILC) is put forward to suppressing vibration. The adaptive law is proposed for handing the parameter uncertainty and the iterative learning term is designed to deal with periodic disturbance. An auxiliary system is utilized to compensate the effect of input nonlinearity. In addition, a barrier Lyapunov function is adopted to deal with asymmetric output constraint. With the proposed control strategy, the stability of the closed-loop system is proven based on rigorous Lyapunov analysis. In the end, the effectiveness of the proposed control is illustrated through numerical simulation results.},
  archive      = {J_TNNLS},
  author       = {Yu Liu and Xiaoqi Wu and Xiangqian Yao and Jingyi Zhao},
  doi          = {10.1109/TNNLS.2022.3157950},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9314-9322},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Backstepping technology-based adaptive boundary ILC for an Input–Output-constrained flexible beam},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical echo state network with sparse learning: A
method for multidimensional chaotic time series prediction.
<em>TNNLS</em>, <em>34</em>(11), 9302–9313. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo state network (ESN), a type of special recurrent neural network with a large-scale randomly fixed hidden layer (called a reservoir) and an adaptable linear output layer, has been widely employed in the field of time series analysis and modeling. However, when tackling the problem of multidimensional chaotic time series prediction, due to the randomly generated rules for input and reservoir weights, not only the representation of valuable variables is enriched but also redundant and irrelevant information is accumulated inevitably. To remove the redundant components, reduce the approximate collinearity among echo-state information, and improve the generalization and stability, a new method called hierarchical ESN with sparse learning (HESN-SL) is proposed. The HESN-SL mines and captures the latent evolution patterns hidden from the dynamic system by means of layer-by-layer processing in stacked reservoirs, and leverage monotone accelerated proximal gradient algorithm to train a sparse output layer with variable selection capability. Meanwhile, we further prove that the HESN-SL satisfies the echo state property, which guarantees the stability and convergence of the proposed model when applied to time series prediction. Experimental results on two synthetic chaotic systems and a real-world meteorological dataset illustrate the proposed HESN-SL outperforms both original ESN and existing hierarchical ESN-based models for multidimensional chaotic time series prediction.},
  archive      = {J_TNNLS},
  author       = {Xiaodong Na and Weijie Ren and Moran Liu and Min Han},
  doi          = {10.1109/TNNLS.2022.3157830},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9302-9313},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical echo state network with sparse learning: A method for multidimensional chaotic time series prediction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GraphHop: An enhanced label propagation method for node
classification. <em>TNNLS</em>, <em>34</em>(11), 9287–9301. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A scalable semisupervised node classification method on graph-structured data, called GraphHop, is proposed in this work. The graph contains all nodes’ attributes and link connections but labels of only a subset of nodes. Graph convolutional networks (GCNs) have provided superior performance in node label classification over the traditional label propagation (LP) methods for this problem. Nevertheless, current GCN algorithms suffer from a considerable amount of labels for training because of high model complexity or cannot be easily generalized to large-scale graphs due to the expensive cost of loading the entire graph and node embeddings. Besides, nonlinearity makes the optimization process a mystery. To this end, an enhanced LP method, called GraphHop, is proposed to tackle these problems. GraphHop can be viewed as a smoothening LP algorithm, in which each propagation alternates between two steps: label aggregation and label update. In the label aggregation step, multihop neighbor embeddings are aggregated to the center node. In the label update step, new embeddings are learned and predicted for each node based on aggregated results from the previous step. The two-step iteration improves the graph signal smoothening capacity. Furthermore, to encode attributes, links, and labels on graphs effectively under one framework, we adopt a two-stage training process, i.e., the initialization stage and the iteration stage. Thus, the smooth attribute information extracted from the initialization stage is consistently imposed in the propagation process in the iteration stage. Experimental results show that GraphHop outperforms state-of-the-art graph learning methods on a wide range of tasks in graphs of various sizes (e.g., multilabel and multiclass classification on citation networks, social graphs, and commodity consumption graphs).},
  archive      = {J_TNNLS},
  author       = {Tian Xie and Bin Wang and C.-C. Jay Kuo},
  doi          = {10.1109/TNNLS.2022.3157746},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9287-9301},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GraphHop: An enhanced label propagation method for node classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TFA-net: A deep learning-based time-frequency analysis tool.
<em>TNNLS</em>, <em>34</em>(11), 9274–9286. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, synchrosqueenzing transform (SST)-based time-frequency analysis (TFA) methods have been developed for achieving the highly concentrated TF representation (TFR). However, SST-based methods suffer from two drawbacks. The first one is that the TFRs are unsatisfactory when dealing with the multicomponent signals, the instantaneous frequencies (IFs) of which are closely adjacent or intersected. Besides, the exhaustive adjustment of window length is required for SST-based methods to obtain the optimal TFR. To tackle these problems, in this article, we first analyze the concentration of TFRs for SST-based methods. A deep learning (DL)-based end-to-end replacement scheme for SST-based methods, named TFA-Net, is then proposed, which learns complete basis functions to obtain various TF characteristics of time series. The 2-D filter kernels are subsequently used for energy concentration. Different from the two-step SST-based methods where the TF transform and energy concentration are separated, the proposed end-to-end architecture makes the basis functions used for extracting TF features more beneficial to energy concentration. The comprehensive numerical experiments are conducted to demonstrate the effectiveness of the TFA-Net. The applications of the proposed method to real-world vital signs, undersea voices and micro-Doppler signatures show its great potential in analyzing nonstationary signals.},
  archive      = {J_TNNLS},
  author       = {Pingping Pan and Yunjian Zhang and Zhenmiao Deng and Shaocan Fan and Xiaohong Huang},
  doi          = {10.1109/TNNLS.2022.3157723},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9274-9286},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TFA-net: A deep learning-based time-frequency analysis tool},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous regularization-based tensor subspace
clustering for hyperspectral band selection. <em>TNNLS</em>,
<em>34</em>(11), 9259–9273. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Band selection (BS) reduces effectively the spectral dimension of a hyperspectral image (HSI) by selecting relatively few representative bands, which allows efficient processing in subsequent tasks. Existing unsupervised BS methods based on subspace clustering are built on matrix-based models, where each band is reshaped as a vector. They encode the correlation of data only in the spectral mode (dimension) and neglect strong correlations between different modes, i.e., spatial modes and spectral mode. Another issue is that the subspace representation of bands is performed in the raw data space, where the dimension is often excessively high, resulting in a less efficient and less robust performance. To address these issues, in this article, we propose a tensor-based subspace clustering model for hyperspectral BS. Our model is developed on the well-known Tucker decomposition. The three factor matrices and a core tensor in our model encode jointly the multimode correlations of HSI, avoiding effectively to destroy the tensor structure and information loss. In addition, we propose well-motivated heterogeneous regularizations (HRs) on the factor matrices by taking into account the important local and global properties of HSI along three dimensions, which facilitates the learning of the intrinsic cluster structure of bands in the low-dimensional subspaces. Instead of learning the correlations of bands in the original domain, a common way for the matrix-based models, our model learns naturally the band correlations in a low-dimensional latent feature space, which is derived by the projections of two factor matrices associated with spatial dimensions, leading to a computationally efficient model. More importantly, the latent feature space is learned in a unified framework. We also develop an efficient algorithm to solve the resulting model. Experimental results on benchmark datasets demonstrate that our model yields improved performance compared to the state-of-the-art.},
  archive      = {J_TNNLS},
  author       = {Shaoguang Huang and Hongyan Zhang and Jize Xue and Aleksandra Pizurica},
  doi          = {10.1109/TNNLS.2022.3157711},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9259-9273},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heterogeneous regularization-based tensor subspace clustering for hyperspectral band selection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CSTNet: A dual-branch convolutional neural network for
imaging of reactive flows using chemical species tomography.
<em>TNNLS</em>, <em>34</em>(11), 9248–9258. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chemical species tomography (CST) has been widely used for in situ imaging of critical parameters, e.g., species concentration and temperature, in reactive flows. However, even with state-of-the-art computational algorithms, the method is limited due to the inherently ill-posed and rank-deficient tomographic data inversion and by high computational cost. These issues hinder its application for real-time flow diagnosis. To address them, we present here a novel convolutional neural network, namely CSTNet, for high-fidelity, rapid, and simultaneous imaging of species concentration and temperature using CST. CSTNet introduces a shared feature extractor that incorporates the CST measurements and sensor layout into the learning network. In addition, a dual-branch decoder with internal crosstalk, which automatically learns the naturally correlated distributions of species concentration and temperature, is proposed for image reconstructions. The proposed CSTNet is validated both with simulated datasets and with measured data from real flames in experiments using an industry-oriented sensor. Superior performance is found relative to previous approaches in terms of reconstruction accuracy and robustness to measurement noise. This is the first time, to the best of our knowledge, that a deep learning-based method for CST has been experimentally validated for simultaneous imaging of multiple critical parameters in reactive flows using a low-complexity optical sensor with a severely limited number of laser beams.},
  archive      = {J_TNNLS},
  author       = {Yunfan Jiang and Jingjing Si and Rui Zhang and Godwin Enemali and Bin Zhou and Hugh McCann and Chang Liu},
  doi          = {10.1109/TNNLS.2022.3157689},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9248-9258},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CSTNet: A dual-branch convolutional neural network for imaging of reactive flows using chemical species tomography},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-paced co-training of graph neural networks for
semi-supervised node classification. <em>TNNLS</em>, <em>34</em>(11),
9234–9247. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have demonstrated great success in many graph data-based applications. The impressive behavior of GNNs typically relies on the availability of a sufficient amount of labeled data for model training. However, in practice, obtaining a large number of annotations is prohibitively labor-intensive and even impossible. Co-training is a popular semi-supervised learning (SSL) paradigm, which trains multiple models based on a common training set while augmenting the limited amount of labeled data used for training each model via the pseudolabeled data generated from the prediction results of other models. Most of the existing co-training works do not control the quality of pseudolabeled data when using them. Therefore, the inaccurate pseudolabels generated by immature models in the early stage of the training process are likely to cause noticeable errors when they are used for augmenting the training data for other models. To address this issue, we propose a self-paced co-training for the GNN (SPC-GNN) framework for semi-supervised node classification. This framework trains multiple GNNs with the same or different structures on different representations of the same training data. Each GNN carries out SSL by using both the originally available labeled data and the augmented pseudolabeled data generated from other GNNs. To control the quality of pseudolabels, a self-paced label augmentation strategy is designed to make the pseudolabels generated at a higher confidence level to be utilized earlier during training such that the negative impact of inaccurate pseudolabels on training data augmentation, and accordingly, the subsequent training process can be mitigated. Finally, each of the trained GNN is evaluated on a validation set, and the best-performing one is chosen as the output. To improve the training effectiveness of the framework, we devise a pretraining followed by a two-step optimization scheme to train GNNs. Experimental results on the node classification task demonstrate that the proposed framework achieves significant improvement over the state-of-the-art SSL methods.},
  archive      = {J_TNNLS},
  author       = {Maoguo Gong and Hui Zhou and A. K. Qin and Wenfeng Liu and Zhongying Zhao},
  doi          = {10.1109/TNNLS.2022.3157688},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9234-9247},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-paced co-training of graph neural networks for semi-supervised node classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sampled-data output feedback control for nonlinear uncertain
systems using predictor-based continuous-discrete observer.
<em>TNNLS</em>, <em>34</em>(11), 9223–9233. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the problem of sampled-data robust output feedback control for a class of nonlinear uncertain systems with time-varying disturbance and measurement delay based on continuous–discrete observer. An augmented system that includes the nonlinear uncertain system and disturbance model is first found, and by using the delayed sampled-data output, we then propose a novel predictor-based continuous–discrete observer to estimate the unknown state and disturbance information. After that, in order to attenuate the undesirable influences of nonlinear uncertainties and disturbance, a sampled-data robust output feedback controller is developed based on disturbance/uncertainty estimation and attenuation technique. It shows that under the proposed control method, the states of overall hybrid nonlinear system can converge to a bounded region centered at the origin. The main benefit of the proposed control method is that in the presence of measurement delay, the influences of time-varying disturbance and nonlinear uncertainties can be effectively attenuated with the help of feedback domination method and prediction technique. Finally, the effectiveness of the proposed control method is demonstrated via the simulation results of a numerical example and a practical example.},
  archive      = {J_TNNLS},
  author       = {Jiankun Sun and Jun Yang and Zhigang Zeng and Huiming Wang},
  doi          = {10.1109/TNNLS.2022.3157649},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9223-9233},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sampled-data output feedback control for nonlinear uncertain systems using predictor-based continuous-discrete observer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Entropy minimizing matrix factorization. <em>TNNLS</em>,
<em>34</em>(11), 9209–9222. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) is a widely used data analysis technique and has yielded impressive results in many real-world tasks. Generally, existing NMF methods represent each sample with several centroids and find the optimal centroids by minimizing the sum of the residual errors. However, outliers deviating from the normal data distribution may have large residues and then dominate the objective value. In this study, an entropy minimizing matrix factorization (EMMF) framework is developed to tackle the above problem. Considering that outliers are usually much less than the normal samples, a new entropy loss function is established for matrix factorization, which minimizes the entropy of the residue distribution and allows a few samples to have large errors. In this way, the outliers do not affect the approximation of normal samples. Multiplicative updating rules for EMMF are derived, and the convergence is proven theoretically. In addition, a Graph regularized version of EMMF (G-EMMF) is also presented, which uses a data graph to capture the data relationship. Clustering results on various synthetic and real-world datasets demonstrate the advantages of the proposed models, and the effectiveness is also verified through the comparison with state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Mulin Chen and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3157148},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9209-9222},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Entropy minimizing matrix factorization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on obstacle detection and avoidance of autonomous
underwater vehicle based on forward-looking sonar. <em>TNNLS</em>,
<em>34</em>(11), 9198–9208. (<a
href="https://doi.org/10.1109/TNNLS.2022.3156907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity of the ocean environment, an autonomous underwater vehicle (AUV) is disturbed by obstacles when performing tasks. Therefore, the research on underwater obstacle detection and avoidance is particularly important. Based on the images collected by a forward-looking sonar on an AUV, this article proposes an obstacle detection and avoidance algorithm. First, a deep learning-based obstacle candidate area detection algorithm is developed. This algorithm uses the You Only Look Once (YOLO) v3 network to determine obstacle candidate areas in a sonar image. Then, in the determined obstacle candidate areas, the obstacle detection algorithm based on the improved threshold segmentation algorithm is used to detect obstacles accurately. Finally, using the obstacle detection results obtained from the sonar images, an obstacle avoidance algorithm based on deep reinforcement learning (DRL) is developed to plan a reasonable obstacle avoidance path of an AUV. Experimental results show that the proposed algorithms improve obstacle detection accuracy and processing speed of sonar images. At the same time, the proposed algorithms ensure AUV navigation safety in a complex obstacle environment.},
  archive      = {J_TNNLS},
  author       = {Xiang Cao and Lu Ren and Changyin Sun},
  doi          = {10.1109/TNNLS.2022.3156907},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9198-9208},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Research on obstacle detection and avoidance of autonomous underwater vehicle based on forward-looking sonar},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A white-box testing for deep neural networks based on neuron
coverage. <em>TNNLS</em>, <em>34</em>(11), 9185–9197. (<a
href="https://doi.org/10.1109/TNNLS.2022.3156620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the introduction of neuron coverage as a testing criterion for deep neural networks (DNNs), covering more neurons to detect more internal logic of DNNs became the main goal of many research studies. While some works had made progress, some new challenges for testing methods based on neuron coverage had been proposed, mainly as establishing better neuron selection and activation strategies influenced not only obtaining higher neuron coverage, but also more testing efficiency, validating testing results automatically, labeling generated test cases to extricate manual work, and so on. In this article, we put forward Test4Deep, an effective white-box testing DNN approach based on neuron coverage. It is based on a differential testing framework to automatically verify inconsistent DNNs’ behavior. We designed a strategy that can track inactive neurons and constantly triggered them in each iteration to maximize neuron coverage. Furthermore, we devised an optimization function that guided the DNN under testing to deviate predictions between the original input and generated test data and dominated unobservable generation perturbations to avoid manually checking test oracles. We conducted comparative experiments with two state-of-the-art white-box testing methods DLFuzz and DeepXplore. Empirical results on three popular datasets with nine DNNs demonstrated that compared to DLFuzz and DeepXplore, Test4Deep, on average, exceeded by 32.87\% and 35.69\% in neuron coverage, while reducing 58.37\% and 53.24\% testing time, respectively. In the meantime, Test4Deep also produced 58.37\% and 53.24\% more test cases with 23.81\% and 98.40\% fewer perturbations. Even compared with the two highest neuron coverage strategies of DLFuzz, Test4Deep still enhanced neuron coverage by 4.34\% and 23.23\% and achieved 94.48\% and 85.67\% higher generation time efficiency. Furthermore, Test4Deep could improve the accuracy and robustness of DNNs by merging generated test cases and retraining.},
  archive      = {J_TNNLS},
  author       = {Jing Yu and Shukai Duan and Xiaojun Ye},
  doi          = {10.1109/TNNLS.2022.3156620},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9185-9197},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A white-box testing for deep neural networks based on neuron coverage},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PWSNAS: Powering weight sharing NAS with general search
space shrinking framework. <em>TNNLS</em>, <em>34</em>(11), 9171–9184.
(<a href="https://doi.org/10.1109/TNNLS.2022.3156373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) depends heavily on an efficient and accurate performance estimator. To speed up the evaluation process, recent advances, like differentiable architecture search (DARTS) and One-Shot approaches, instead of training every model from scratch, train a weight-sharing super-network to reuse parameters among different candidates, in which all child models can be efficiently evaluated. Though these methods significantly boost search efficiency, they inherently suffer from inaccurate and unstable performance estimation. To this end, we propose a general and effective framework for powering weight-sharing NAS, namely, PWSNAS, by shrinking search space automatically, i.e., candidate operators will be discarded if they are less important. With the strategy, our approach can provide a promising search space of a smaller size by progressively simplifying the original search space, which can reduce difficulties for existing NAS methods to find superior architectures. In particular, we present two strategies to guide the shrinking process: detect redundant operators with a new angle-based metric and decrease the degree of weight sharing of a super-network by increasing parameters, which differentiates PWSNAS from existing shrinking methods. Comprehensive analysis experiments on NASBench-201 verify the superiority of our proposed metric over existing accuracy-based and magnitude-based metrics. PWSNAS can easily apply to the state-of-the-art NAS methods, e.g., single path one-shot neural architecture search (SPOS), FairNAS, ProxylessNAS, DARTS, and progressive DARTS (PDARTS). We evaluate PWSNAS and demonstrate consistent performance gains over baseline methods.},
  archive      = {J_TNNLS},
  author       = {Yiming Hu and Xingang Wang and Qingyi Gu},
  doi          = {10.1109/TNNLS.2022.3156373},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9171-9184},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PWSNAS: Powering weight sharing NAS with general search space shrinking framework},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid reinforcement learning for optimal control of
non-linear switching system. <em>TNNLS</em>, <em>34</em>(11), 9161–9170.
(<a href="https://doi.org/10.1109/TNNLS.2022.3156287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the reinforcement learning mechanism, a data-based scheme is proposed to address the optimal control problem of discrete-time non-linear switching systems. In contrast to conventional systems, in the switching systems, the control signal consists of the active mode (discrete) and the control inputs (continuous). First, the Hamilton–Jacobi–Bellman equation of the hybrid action space is derived, and a two-stage value iteration method is proposed to learn the optimal solution. In addition, a neural network structure is designed by decomposing the Q-function into the value function and the normalized advantage value function, which is quadratic with respect to the continuous control of subsystems. In this way, the Q-function and the continuous policy can be simultaneously updated at each iteration step so that the training of hybrid policies is simplified to a one-step manner. Moreover, the convergence analysis of the proposed algorithm with consideration of approximation error is provided. Finally, the algorithm is applied evaluated on three different simulation examples. Compared to the related work, the results demonstrate the potential of our method.},
  archive      = {J_TNNLS},
  author       = {Xiaofeng Li and Lu Dong and Lei Xue and Changyin Sun},
  doi          = {10.1109/TNNLS.2022.3156287},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9161-9170},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hybrid reinforcement learning for optimal control of non-linear switching system},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymptotical neuro-adaptive consensus of multi-agent systems
with a high dimensional leader and directed switching topology.
<em>TNNLS</em>, <em>34</em>(11), 9149–9160. (<a
href="https://doi.org/10.1109/TNNLS.2022.3156279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the asymptotical consensus problem for multi-agent systems (MASs) consisting of a high-dimensional leader and multiple followers with unknown nonlinear dynamics under directed switching topology by using a neural network (NN) adaptive control approach. First, we design an observer for each follower to reconstruct the states of the leader. Second, by using the idea of discontinuous control, we design a discontinuous consensus controller together with an NN adaptive law. Finally, by using the average dwell time (ADT) method and the Barbǎlat’s lemma, we show that asymptotical neuroadaptive consensus can be achieved in the considered MAS if the ADT is larger than a positive threshold. Moreover, we study the asymptotical neuroadaptive consensus problem for MASs with intermittent topology. Finally, we perform two simulation examples to validate the obtained theoretical results. In contrast to the existing works, the asymptotical neuroadaptive consensus problem for MASs is firstly solved under directed switching topology.},
  archive      = {J_TNNLS},
  author       = {Peijun Wang and Guanghui Wen and Tingwen Huang and Wenwu Yu and Yuezu Lv},
  doi          = {10.1109/TNNLS.2022.3156279},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9149-9160},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asymptotical neuro-adaptive consensus of multi-agent systems with a high dimensional leader and directed switching topology},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pruning networks with cross-layer ranking &amp; k-reciprocal
nearest filters. <em>TNNLS</em>, <em>34</em>(11), 9139–9148. (<a
href="https://doi.org/10.1109/TNNLS.2022.3156047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on filter-level network pruning. A novel pruning method, termed CLR-RNF, is proposed. We first reveal a “long-tail” pruning problem in magnitude-based weight pruning methods and then propose a computation-aware measurement for individual weight importance, followed by a cross-layer ranking (CLR) of weights to identify and remove the bottom-ranked weights. Consequently, the per-layer sparsity makes up the pruned network structure in our filter pruning. Then, we introduce a recommendation-based filter selection scheme where each filter recommends a group of its closest filters. To pick the preserved filters from these recommended groups, we further devise a ${k}$ -reciprocal nearest filter (RNF) selection scheme where the selected filters fall into the intersection of these recommended groups. Both our pruned network structure and the filter selection are nonlearning processes, which, thus, significantly reduces the pruning complexity and differentiates our method from existing works. We conduct image classification on CIFAR-10 and ImageNet to demonstrate the superiority of our CLR-RNF over the state-of-the-arts. For example, on CIFAR-10, CLR-RNF removes 74.1\% FLOPs and 95.0\% parameters from VGGNet-16 with even 0.3\% accuracy improvements. On ImageNet, it removes 70.2\% FLOPs and 64.8\% parameters from ResNet-50 with only 1.7\% top-five accuracy drops. Our project is available at https://github.com/lmbxmu/CLR-RNF .},
  archive      = {J_TNNLS},
  author       = {Mingbao Lin and Liujuan Cao and Yuxin Zhang and Ling Shao and Chia-Wen Lin and Rongrong Ji},
  doi          = {10.1109/TNNLS.2022.3156047},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9139-9148},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pruning networks with cross-layer ranking &amp; k-reciprocal nearest filters},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance optimization and fault-tolerance of highly
dynamic systems via q-learning with an incrementally attached controller
gain system. <em>TNNLS</em>, <em>34</em>(11), 9128–9138. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance and reliable control of systems that are highly dynamic and open-loop unstable is challenging but of considerable practical interest. Thus, this article investigates the performance optimization and fault tolerance of highly dynamic systems. First, an incremental control structure is proposed, where a controller gain system is attached to the predesigned controller, and by reconfiguring the controller gain system, the performance can be equivalently optimized as configuring the predesigned one. The incremental attachment of the controller gain system does not modify the existing control system, and it can be easily attached via various communication channels. Second, a structure integrating fault-tolerance strategy and hardware redundancy is proposed. Under this structure, command fusion and fault-tolerance strategies are developed where the control commands from different control units are optimally fused, and each control unit can be reconfigured w.r.t. the performance of the other ones. Furthermore, ${Q}$ -learning algorithms are developed to realize the proposed structures and strategies in real-time model-freely. As such, varying operational conditions of the highly dynamic system can be tackled. Finally, the proposed structures and algorithms are validated case by case to show their effectiveness.},
  archive      = {J_TNNLS},
  author       = {Yunsong Xu and Zhengen Zhao and Shen Yin},
  doi          = {10.1109/TNNLS.2022.3155876},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9128-9138},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Performance optimization and fault-tolerance of highly dynamic systems via Q-learning with an incrementally attached controller gain system},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-training for class-incremental semantic segmentation.
<em>TNNLS</em>, <em>34</em>(11), 9116–9127. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In class-incremental semantic segmentation, we have no access to the labeled data of previous tasks. Therefore, when incrementally learning new classes, deep neural networks suffer from catastrophic forgetting of previously learned knowledge. To address this problem, we propose to apply a self-training approach that leverages unlabeled data, which is used for rehearsal of previous knowledge. Specifically, we first learn a temporary model for the current task, and then, pseudo labels for the unlabeled data are computed by fusing information from the old model of the previous task and the current temporary model. In addition, conflict reduction is proposed to resolve the conflicts of pseudo labels generated from both the old and temporary models. We show that maximizing self-entropy can further improve results by smoothing the overconfident predictions. Interestingly, in the experiments, we show that the auxiliary data can be different from the training data and that even general-purpose, but diverse auxiliary data can lead to large performance gains. The experiments demonstrate the state-of-the-art results: obtaining a relative gain of up to 114\% on Pascal-VOC 2012 and 8.5\% on the more challenging ADE20K compared to previous state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Lu Yu and Xialei Liu and Joost van de Weijer},
  doi          = {10.1109/TNNLS.2022.3155746},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9116-9127},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-training for class-incremental semantic segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting best-selling new products in a major promotion
campaign through graph convolutional networks. <em>TNNLS</em>,
<em>34</em>(11), 9102–9115. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many e-commerce platforms, such as AliExpress, run major promotion campaigns regularly. Before such a promotion, it is important to predict potential best sellers and their respective sales volumes so that the platform can arrange their supply chains and logistics accordingly. For items with a sufficiently long sales history, accurate sales forecast can be achieved through the traditional statistical forecasting techniques. Accurately predicting the sales volume of a new item, however, is rather challenging with existing methods; time series models tend to overfit due to the very limited historical sales records of the new item, whereas models that do not utilize historical information often fail to make accurate predictions, due to the lack of strong indicators of sales volume among the item’s basic attributes. This article presents the solution deployed at Alibaba in 2019, which had been used in production to prepare for its annual “Double 11” promotion event whose total sales amount exceeded U.S. $\$ $ 38 billion in a single day. The main idea of the proposed solution is to predict the sales volume of each new item through its connections with older products with sufficiently long sales history. In other words, our solution considers the cross-selling effects between different products, which has been largely neglected in previous methods. Specifically, the proposed solution first constructs an item graph, in which each new item is connected to relevant older items. Then, a novel multitask graph convolutional neural network (GCN) is trained by a multiobjective optimization-based gradient surgery technique to predict the expected sales volumes of new items. The designs of both the item graph and the GCN exploit the fact that we only need to perform accurate sales forecasts for potential best-selling items in a major promotion, which helps reduce computational overhead. Extensive experiments on both proprietary AliExpress data and a public dataset demonstrate that the proposed solution achieves consistent performance gains compared to existing methods for sales forecast.},
  archive      = {J_TNNLS},
  author       = {Chaojie Li and Wensen Jiang and Yin Yang and Shirui Pan and Gang Huang and Lijie Guo},
  doi          = {10.1109/TNNLS.2022.3155690},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9102-9115},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Predicting best-selling new products in a major promotion campaign through graph convolutional networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A triple-double convolutional neural network for
panchromatic sharpening. <em>TNNLS</em>, <em>34</em>(11), 9088–9101. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pansharpening refers to the fusion of a panchromatic (PAN) image with a high spatial resolution and a multispectral (MS) image with a low spatial resolution, aiming to obtain a high spatial resolution MS (HRMS) image. In this article, we propose a novel deep neural network architecture with level-domain-based loss function for pansharpening by taking into account the following double-type structures, i.e., double-level, double-branch, and double-direction, called as triple-double network (TDNet). By using the structure of TDNet, the spatial details of the PAN image can be fully exploited and utilized to progressively inject into the low spatial resolution MS (LRMS) image, thus yielding the high spatial resolution output. The specific network design is motivated by the physical formula of the traditional multi-resolution analysis (MRA) methods. Hence, an effective MRA fusion module is also integrated into the TDNet. Besides, we adopt a few ResNet blocks and some multi-scale convolution kernels to deepen and widen the network to effectively enhance the feature extraction and the robustness of the proposed TDNet. Extensive experiments on reduced- and full-resolution datasets acquired by WorldView-3, QuickBird, and GaoFen-2 sensors demonstrate the superiority of the proposed TDNet compared with some recent state-of-the-art pansharpening approaches. An ablation study has also corroborated the effectiveness of the proposed approach. The code is available at https://github.com/liangjiandeng/TDNet .},
  archive      = {J_TNNLS},
  author       = {Tian-Jiang Zhang and Liang-Jian Deng and Ting-Zhu Huang and Jocelyn Chanussot and Gemine Vivone},
  doi          = {10.1109/TNNLS.2022.3155655},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9088-9101},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A triple-double convolutional neural network for panchromatic sharpening},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Globally adaptive neural network output-feedback control
for uncertain nonlinear systems. <em>TNNLS</em>, <em>34</em>(11),
9078–9087. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a globally neural-network-based adaptive control strategy with flat-zone modification is proposed for a class of uncertain output feedback systems with time-varying bounded disturbances. A high-order continuously differentiable switching function is introduced into the filter dynamics to achieve global compensation for uncertain functions, thus further to ensure that all the closed-loop signals are globally uniformity ultimately bounded (GUUB). It is proven that the output tracking error converges to the prespecified neighborhood of the origin. The effectiveness of the proposed control method is verified by two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Zhengqiang Zhang and Qiufeng Wang and Yingli Sang and Shuzhi Sam Ge},
  doi          = {10.1109/TNNLS.2022.3155635},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9078-9087},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Globally adaptive neural network output-feedback control for uncertain nonlinear systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrete robust principal component analysis via binary
weights self-learning. <em>TNNLS</em>, <em>34</em>(11), 9064–9077. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is a typical unsupervised dimensionality reduction algorithm, and one of its important weaknesses is that the squared $\ell _{2}$ -norm cannot overcome the influence of outliers. Existing robust PCA methods based on paradigm have the following two drawbacks. First, the objective function of PCA based on the $\ell _{1}$ -norm has no rotational invariance and limited robustness to outliers, and its solution mostly uses a greedy search strategy, which is expensive. Second, the robust PCA based on the $\ell _{2,1}$ -norm and the $\ell _{2,p}$ -norm is essential to learn probability weights for data, which only weakens the influence of outliers on the learning projection matrix and cannot be completely eliminated. Moreover, the ability to detect anomalies is also very poor. To solve these problems, we propose a novel discrete robust principal component analysis (DRPCA). Through self-learning binary weights, the influence of outliers on the projection matrix and data center estimation can be completely eliminated, and anomaly detection can be directly performed. In addition, an alternating iterative optimization algorithm is designed to solve the proposed problem and realize the automatic update of binary weights. Finally, our proposed model is successfully applied to anomaly detection applications, and experimental results demonstrate that the superiority of our proposed method compared with the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Sisi Wang and Zheng Wang and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3155607},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9064-9077},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discrete robust principal component analysis via binary weights self-learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust actor-critic with relative entropy regulating actor.
<em>TNNLS</em>, <em>34</em>(11), 9054–9063. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate estimation of Q-function and the enhancement of agent’s exploration ability have always been challenges of off-policy actor–critic algorithms. To address the two concerns, a novel robust actor–critic (RAC) is developed in this article. We first derive a robust policy improvement mechanism (RPIM) by using the local optimal policy about the current estimated Q-function to guide policy improvement. By constraining the relative entropy between the new policy and the previous one in policy improvement, the proposed RPIM can enhance the stability of the policy update process. The theoretical analysis shows that the incentive to increase the policy entropy is endowed when the policy is updated, which is conducive to enhancing the exploration ability of agents. Then, RAC is developed by applying the proposed RPIM to regulate the actor improvement process. The developed RAC is proven to be convergent. Finally, the proposed RAC is evaluated on some continuous-action control tasks in the MuJoCo platform and the experimental results show that RAC outperforms several state-of-the-art reinforcement learning algorithms.},
  archive      = {J_TNNLS},
  author       = {Yuhu Cheng and Longyang Huang and C. L. Philip Chen and Xuesong Wang},
  doi          = {10.1109/TNNLS.2022.3155483},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9054-9063},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust actor-critic with relative entropy regulating actor},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cell-based fast memetic algorithm for automated
convolutional neural architecture design. <em>TNNLS</em>,
<em>34</em>(11), 9040–9053. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) has attracted much attention in recent years. It automates the neural network construction for different tasks, which is traditionally addressed manually. In the literature, evolutionary optimization (EO) has been proposed for NAS due to its strong global search capability. However, despite the success enjoyed by EO, it is worth noting that existing EO algorithms for NAS are often very computationally expensive, which makes these algorithms unpractical in reality. Keeping this in mind, in this article, we propose an efficient memetic algorithm (MA) for automated convolutional neural network (CNN) architecture search. In contrast to existing EO algorithms for CNN architecture design, a new cell-based architecture search space, and new global and local search operators are proposed for CNN architecture search. To further improve the efficiency of our proposed algorithm, we develop a one-epoch-based performance estimation strategy without any pretrained models to evaluate each found architecture on the training datasets. To investigate the performance of the proposed method, comprehensive empirical studies are conducted against 34 state-of-the-art peer algorithms, including manual algorithms, reinforcement learning (RL) algorithms, gradient-based algorithms, and evolutionary algorithms (EAs), on widely used CIFAR10 and CIFAR100 datasets. The obtained results confirmed the efficacy of the proposed approach for automated CNN architecture design.},
  archive      = {J_TNNLS},
  author       = {Junwei Dong and Boyu Hou and Liang Feng and Huajin Tang and Kay Chen Tan and Yew-Soon Ong},
  doi          = {10.1109/TNNLS.2022.3155230},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9040-9053},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A cell-based fast memetic algorithm for automated convolutional neural architecture design},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning rate dropout. <em>TNNLS</em>, <em>34</em>(11),
9029–9039. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization algorithms are of great importance to efficiently and effectively train a deep neural network. However, the existing optimization algorithms show unsatisfactory convergence behavior, either slowly converging or not seeking to avoid bad local optima. Learning rate dropout (LRD) is a new gradient descent technique to motivate faster convergence and better generalization. LRD aids the optimizer to actively explore in the parameter space by randomly dropping some learning rates (to 0); at each iteration, only parameters whose learning rate is not 0 are updated. Since LRD reduces the number of parameters to be updated for each iteration, the convergence becomes easier. For parameters that are not updated, their gradients are accumulated (e.g., momentum) by the optimizer for the next update. Accumulating multiple gradients at fixed parameter positions gives the optimizer more energy to escape from the saddle point and bad local optima. Experiments show that LRD is surprisingly effective in accelerating training while preventing overfitting.},
  archive      = {J_TNNLS},
  author       = {Huangxing Lin and Weihong Zeng and Yihong Zhuang and Xinghao Ding and Yue Huang and John Paisley},
  doi          = {10.1109/TNNLS.2022.3155181},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9029-9039},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning rate dropout},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prototype-based interpretation of the functionality of
neurons in winner-take-all neural networks. <em>TNNLS</em>,
<em>34</em>(11), 9016–9028. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prototype-based learning (PbL) using a winner-take-all (WTA) network based on minimum Euclidean distance (ED-WTA) is an intuitive approach to multiclass classification. By constructing meaningful class centers, PbL provides higher interpretability and generalization than hyperplane-based learning (HbL) methods based on maximum inner product (IP-WTA) and can efficiently detect and reject samples that do not belong to any classes. In this article, we first prove the equivalence of IP-WTA and ED-WTA from a representational power perspective. Then, we show that naively using this equivalence leads to unintuitive ED-WTA networks in which the centers have high distances to data that they represent. We propose ±ED-WTA that models each neuron with two prototypes: one positive prototype, representing samples modeled by that neuron, and a negative prototype, representing the samples erroneously won by that neuron during training. We propose a novel training algorithm for the ±ED-WTA network, which cleverly switches between updating the positive and negative prototypes and is essential to the emergence of interpretable prototypes. Unexpectedly, we observed that the negative prototype of each neuron is indistinguishably similar to the positive one. The rationale behind this observation is that the training data that are mistaken for a prototype are indeed similar to it. The main finding of this article is this interpretation of the functionality of neurons as computing the difference between the distances to a positive and a negative prototype, which is in agreement with the BCM theory. Our experiments show that the proposed ±ED-WTA method constructs highly interpretable prototypes that can be successfully used for explaining the functionality of deep neural networks (DNNs), and detecting outlier and adversarial examples.},
  archive      = {J_TNNLS},
  author       = {Ramin Zarei-Sabzevar and Kamaledin Ghiasi-Shirazi and Ahad Harati},
  doi          = {10.1109/TNNLS.2022.3155174},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9016-9028},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prototype-based interpretation of the functionality of neurons in winner-take-all neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asynchronous fault detection for memristive neural networks
with dwell-time-based communication protocol. <em>TNNLS</em>,
<em>34</em>(11), 9004–9015. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the asynchronous fault detection filter problem for discrete-time memristive neural networks with a stochastic communication protocol (SCP) and denial-of-service attacks. Aiming at alleviating the occurrence of network-induced phenomena, a dwell-time-based SCP is scheduled to coordinate the packet transmission between sensors and filter, whose deterministic switching signal arranges the proper feedback switching information among the homogeneous Markov processes (HMPs) for different scenarios. A variable obeying the Bernoulli distribution is proposed to characterize the randomly occurring denial-of-service attacks, in which the attack rate is uncertain. More specifically, both dwell-time-based SCP and denial-of-service attacks are modeled by means of compensation strategy. In light of the mode mismatches between data transmission and filter, a hidden Markov model (HMM) is adopted to describe the asynchronous fault detection filter. Consequently, sufficient conditions of stochastic stability of memristive neural networks are devised with the assistance of Lyapunov theory. In the end, a numerical example is applied to show the effectiveness of the theoretical method.},
  archive      = {J_TNNLS},
  author       = {An Lin and Jun Cheng and Leszek Rutkowski and Shiping Wen and Mengzhuo Luo and Jinde Cao},
  doi          = {10.1109/TNNLS.2022.3155149},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {9004-9015},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asynchronous fault detection for memristive neural networks with dwell-time-based communication protocol},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Central attention network for hyperspectral imagery
classification. <em>TNNLS</em>, <em>34</em>(11), 8989–9003. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the intrinsic properties of hyperspectral imagery (HSI) are analyzed, and two principles for spectral–spatial feature extraction of HSI are built, including the foundation of pixel-level HSI classification and the definition of spatial information. Based on the two principles, scaled dot-product central attention (SDPCA) tailored for HSI is designed to extract spectral–spatial information from a central pixel (i.e., a query pixel to be classified) and pixels that are similar to the central pixel on an HSI patch. Then, employed with the HSI-tailored SDPCA module, a central attention network (CAN) is proposed by combining HSI-tailored dense connections of the features of the hidden layers and the spectral information of the query pixel. MiniCAN as a simplified version of CAN is also investigated. Superior classification performance of CAN and miniCAN on three datasets of different scenarios demonstrates their effectiveness and benefits compared with state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Huan Liu and Wei Li and Xiang-Gen Xia and Mengmeng Zhang and Chen-Zhong Gao and Ran Tao},
  doi          = {10.1109/TNNLS.2022.3155114},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8989-9003},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Central attention network for hyperspectral imagery classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gait quality aware network: Toward the interpretability of
silhouette-based gait recognition. <em>TNNLS</em>, <em>34</em>(11),
8978–8988. (<a
href="https://doi.org/10.1109/TNNLS.2022.3154723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition receives increasing attention since it can be conducted at a long distance in a nonintrusive way and applied to the condition of changing clothes. Most existing methods take the silhouettes of gait sequences as the input and learn a unified representation from multiple silhouettes to match probe and gallery. However, these models are all faced with the lack of interpretability, e.g., it is not clear which silhouette in a gait sequence and which part in the human body are relatively more important for recognition. In this work, we propose a gait quality aware network (GQAN) for gait recognition which explicitly assesses the quality of each silhouette and each part via two blocks: frame quality block (FQBlock) and part quality block (PQBlock). Specifically, FQBlock works in a squeeze-and-excitation style to recalibrate the features for each silhouette, and the scores of all the channels are added as frame quality indicator. PQBlock predicts a score for each part which is used to compute the weighted distance between the probe and gallery. Particularly, we propose a part quality loss (PQLoss) which enables GQAN to be trained in an end-to-end manner with only sequence-level identity annotations. This work is meaningful by moving toward the interpretability of silhouette-based gait recognition, and our method also achieves very competitive performance on CASIA-B and OUMVLP.},
  archive      = {J_TNNLS},
  author       = {Saihui Hou and Xu Liu and Chunshui Cao and Yongzhen Huang},
  doi          = {10.1109/TNNLS.2022.3154723},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8978-8988},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gait quality aware network: Toward the interpretability of silhouette-based gait recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous multidomain recommender system through
adversarial learning. <em>TNNLS</em>, <em>34</em>(11), 8965–8977. (<a
href="https://doi.org/10.1109/TNNLS.2022.3154345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the user data sparsity problem, which is the main issue in generating user preference prediction, cross-domain recommender systems transfer knowledge from one source domain with dense data to assist recommendation tasks in the target domain with sparse data. However, data are usually sparsely scattered in multiple possible source domains, and in each domain (source/target) the data may be heterogeneous, thus it is difficult for existing cross-domain recommender systems to find one source domain with dense data from multiple domains. In this way, they fail to deal with data sparsity problems in the target domain and cannot provide an accurate recommendation. In this article, we propose a novel multidomain recommender system (called HMRec) to deal with two challenging issues: 1) how to exploit valuable information from multiple source domains when no single source domain is sufficient and 2) how to ensure positive transfer from heterogeneous data in source domains with different feature spaces. In HMRec, domain-shared and domain-specific features are extracted to enable the knowledge transfer between multiple heterogeneous source and target domains. To ensure positive transfer, the domain-shared subspaces from multiple domains are maximally matched by a multiclass domain discriminator in an adversarial learning process. The recommendation in the target domain is completed by a matrix factorization module with aligned latent features from both the user and the item side. Extensive experiments on four cross-domain recommendation tasks with real-world datasets demonstrate that HMRec can effectively transfer knowledge from multiple heterogeneous domains collaboratively to increase the rating prediction accuracy in the target domain and significantly outperforms six state-of-the-art non-transfer or cross-domain baselines.},
  archive      = {J_TNNLS},
  author       = {Wenhui Liao and Qian Zhang and Bo Yuan and Guangquan Zhang and Jie Lu},
  doi          = {10.1109/TNNLS.2022.3154345},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8965-8977},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heterogeneous multidomain recommender system through adversarial learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MetaGeo: A general framework for social user geolocation
identification with few-shot learning. <em>TNNLS</em>, <em>34</em>(11),
8950–8964. (<a
href="https://doi.org/10.1109/TNNLS.2022.3154204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the geolocation of social media users is an important problem in a wide range of applications, spanning from disease outbreaks, emergency detection, local event recommendation, to fake news localization, online marketing planning, and even crime control and prevention. Researchers have attempted to propose various models by combining different sources of information, including text, social relation, and contextual data, which indeed has achieved promising results. However, existing approaches still suffer from certain constraints, such as: 1) a very few samples are available and 2) prediction models are not easy to be generalized for users from new regions—which are challenges that motivate our study. In this article, we propose a general framework for identifying user geolocation—MetaGeo, which is a meta-learning-based approach, learning the prior distribution of the geolocation task in order to quickly adapt the prediction toward users from new locations. Different from typical meta-learning settings that only learn a new concept from few-shot samples, MetaGeo improves the geolocation prediction with conventional settings by ensembling numerous mini-tasks. In addition, MetaGeo incorporates probabilistic inference to alleviate two issues inherent in training with few samples: location uncertainty and task ambiguity. To demonstrate the effectiveness of MetaGeo, we conduct extensive experimental evaluations on three real-world datasets and compare the performance with several state-of-the-art benchmark models. The results demonstrate the superiority of MetaGeo in both the settings where the predicted locations/regions are known or have not been seen during training.},
  archive      = {J_TNNLS},
  author       = {Fan Zhou and Xiuxiu Qi and Kunpeng Zhang and Goce Trajcevski and Ting Zhong},
  doi          = {10.1109/TNNLS.2022.3154204},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8950-8964},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MetaGeo: A general framework for social user geolocation identification with few-shot learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MMV-net: A multiple measurement vector network for
multifrequency electrical impedance tomography. <em>TNNLS</em>,
<em>34</em>(11), 8938–8949. (<a
href="https://doi.org/10.1109/TNNLS.2022.3154108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multifrequency electrical impedance tomography (mfEIT) is an emerging biomedical imaging modality to reveal frequency-dependent conductivity distributions in biomedical applications. Conventional model-based image reconstruction methods suffer from low spatial resolution, unconstrained frequency correlation, and high computational cost. Deep learning has been extensively applied in solving the EIT inverse problem in biomedical and industrial process imaging. However, most existing learning-based approaches deal with the single-frequency setup, which is inefficient and ineffective when extended to the multifrequency setup. This article presents a multiple measurement vector (MMV) model-based learning algorithm named MMV-Net to solve the mfEIT image reconstruction problem. MMV-Net considers the correlations between mfEIT images and unfolds the update steps of the Alternating Direction Method of Multipliers for the MMV problem (MMV-ADMM). The nonlinear shrinkage operator associated with the weighted $l_{2,1}$ regularization term of MMV-ADMM is generalized in MMV-Net with a cascade of a Spatial Self-Attention module and a Convolutional Long Short-Term Memory (ConvLSTM) module to better capture intrafrequency and interfrequency dependencies. The proposed MMV-Net was validated on our Edinburgh mfEIT Dataset and a series of comprehensive experiments. The results show superior image quality, convergence performance, noise robustness, and computational efficiency against the conventional MMV-ADMM and the state-of-the-art deep learning methods.},
  archive      = {J_TNNLS},
  author       = {Zhou Chen and Jinxi Xiang and Pierre-Olivier Bagnaninchi and Yunjie Yang},
  doi          = {10.1109/TNNLS.2022.3154108},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8938-8949},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MMV-net: A multiple measurement vector network for multifrequency electrical impedance tomography},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep PLS: A lightweight deep learning model for
interpretable and efficient data analytics. <em>TNNLS</em>,
<em>34</em>(11), 8923–8937. (<a
href="https://doi.org/10.1109/TNNLS.2022.3154090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The salient progress of deep learning is accompanied by nonnegligible deficiencies, such as: 1) interpretability problem; 2) requirement for large data amounts; 3) hard to design and tune parameters; and 4) heavy computation complexity. Despite the remarkable achievements of neural networks-based deep models in many fields, the practical applications of deep learning are still limited by these shortcomings. This article proposes a new concept called the lightweight deep model (LDM). LDM absorbs the useful ideas of deep learning and overcomes their shortcomings to a certain extent. We explore the idea of LDM from the perspective of partial least squares (PLS) by constructing a deep PLS (DPLS) model. The feasibility and merits of DPLS are proved theoretically, after that, DPLS is further generalized to a more common form (GDPLS) by adding a nonlinear mapping layer between two cascaded PLS layers in the model structure. The superiority of DPLS and GDPLS is demonstrated through four practical cases involving two regression problems and two classification tasks, in which our model not only achieves competitive performance compared with existing neural networks-based deep models but also is proven to be a more interpretable and efficient method, and we know exactly how it improves performance, how it gives correct results. Note that our proposed model can only be regarded as an alternative to fully connected neural networks at present and cannot completely replace the mature deep vision or language models.},
  archive      = {J_TNNLS},
  author       = {Xiangyin Kong and Zhiqiang Ge},
  doi          = {10.1109/TNNLS.2022.3154090},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8923-8937},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep PLS: A lightweight deep learning model for interpretable and efficient data analytics},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive event-triggered time-varying output bipartite
formation containment of multiagent systems under directed graphs.
<em>TNNLS</em>, <em>34</em>(11), 8909–8922. (<a
href="https://doi.org/10.1109/TNNLS.2022.3154028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-varying output bipartite formation containment (TVOBFC) problem for linear multiagent systems (MASs) under directed graphs is an important problem. However, the methods in existing works rely on the global information of the MASs or do not use event-triggered communication. This article investigates two kinds of TVOBFC problems for heterogeneous linear MASs under signed digraphs by event-triggered communication. For the first case where leaders have the same dynamics, the innovative fully distributed event-triggered protocol for the follower is proposed. In this case, the followers form the preset formation shape. For the second case where leaders have different dynamics, the leaders are divided into two groups. One group can directly obtain the output information of the virtual leader, while the other group cannot. In order to make leaders achieve the formation shape and track the virtual leader, two kinds of innovative observers are designed for two kinds of leaders to estimate the state of the virtual leader, and the control protocol is designed for each leader based on the designed observers. Then, the control law for each follower is designed to solve the formation containment problem. Finally, two examples are introduced to illustrate the main results.},
  archive      = {J_TNNLS},
  author       = {Juan Zhang and Huaguang Zhang and Zhongyang Ming and Yunfei Mu},
  doi          = {10.1109/TNNLS.2022.3154028},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8909-8922},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive event-triggered time-varying output bipartite formation containment of multiagent systems under directed graphs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online spatio-temporal learning in deep neural networks.
<em>TNNLS</em>, <em>34</em>(11), 8894–8908. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological neural networks are equipped with an inherent capability to continuously adapt through online learning. This aspect remains in stark contrast to learning with error backpropagation through time (BPTT) that involves offline computation of the gradients due to the need to unroll the network through time. Here, we present an alternative online learning algorithm ic framework for deep recurrent neural networks (RNNs) and spiking neural networks (SNNs), called online spatio-temporal learning (OSTL). It is based on insights from biology and proposes the clear separation of spatial and temporal gradient components. For shallow SNNs, OSTL is gradient equivalent to BPTT enabling for the first time online training of SNNs with BPTT-equivalent gradients. In addition, the proposed formulation unveils a class of SNN architectures trainable online at low time complexity. Moreover, we extend OSTL to a generic form, applicable to a wide range of network architectures, including networks comprising long short-term memory (LSTM) and gated recurrent units (GRUs). We demonstrate the operation of our algorithm ic framework on various tasks from language modeling to speech recognition and obtain results on par with the BPTT baselines.},
  archive      = {J_TNNLS},
  author       = {Thomas Bohnstingl and Stanisław Woźniak and Angeliki Pantazi and Evangelos Eleftheriou},
  doi          = {10.1109/TNNLS.2022.3153985},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8894-8908},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online spatio-temporal learning in deep neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image matting with deep gaussian process. <em>TNNLS</em>,
<em>34</em>(11), 8879–8893. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We observe a common characteristic between the classical propagation-based image matting and the Gaussian process (GP)-based regression. The former produces closer alpha matte values for pixels associated with a higher affinity, while the outputs regressed by the latter are more correlated for more similar inputs. Based on this observation, we reformulate image matting as GP and find that this novel matting-GP formulation results in a set of attractive properties. First, it offers an alternative view on and approach to propagation-based image matting. Second, an application of kernel learning in GP brings in a novel deep matting-GP technique, which is pretty powerful for encapsulating the expressive power of deep architecture on the image relative to its matting. Third, an existing scalable GP technique can be incorporated to further reduce the computational complexity to $\mathcal {O}(n)$ from $\mathcal {O}(n^{3})$ of many conventional matting propagation techniques. Our deep matting-GP provides an attractive strategy toward addressing the limit of widespread adoption of deep learning techniques to image matting for which a sufficiently large labeled dataset is lacking. A set of experiments on both synthetically composited images and real-world images show the superiority of the deep matting-GP to not only the classical propagation-based matting techniques but also modern deep learning-based approaches.},
  archive      = {J_TNNLS},
  author       = {Yuanjie Zheng and Yunshuai Yang and Tongtong Che and Sujuan Hou and Wenhui Huang and Yue Gao and Ping Tan},
  doi          = {10.1109/TNNLS.2022.3153955},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8879-8893},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Image matting with deep gaussian process},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernel error path algorithm. <em>TNNLS</em>,
<em>34</em>(11), 8866–8878. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuning the values of kernel parameters plays a vital role in the performance of kernel methods. Kernel path algorithms have been proposed for several important learning algorithms, including support vector machine and kernelized Lasso, which can fit the piecewise nonlinear solutions of kernel methods with respect to the kernel parameter in a continuous space. Although the error path algorithms have been proposed to ensure that the model with the minimum cross validation (CV) error can be found, which is usually the ultimate goal of model selection, they are limited to piecewise linear solution paths. To address this problem, in this article, we extend the classic error path algorithm to the nonlinear kernel solution paths and propose a new kernel error path algorithm (KEP) that can find the global optimal kernel parameter with the minimum CV error. Specifically, we first prove that error functions of binary classification and regression problems are piecewise constant or smooth w.r.t. the kernel parameter. Then, we propose KEP for support vector machine and kernelized Lasso and prove that it guarantees to find the model with the minimum CV error within the whole range of kernel parameter values. Experimental results on various datasets show that our KEP can find the model with minimum CV error with less time consumption. Finally, it would have better generalization error on the test set, compared with grid search and random search.},
  archive      = {J_TNNLS},
  author       = {Ziran Xiong and Charles X. Ling and Bin Gu},
  doi          = {10.1109/TNNLS.2022.3153953},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8866-8878},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Kernel error path algorithm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curriculum-based deep reinforcement learning for quantum
control. <em>TNNLS</em>, <em>34</em>(11), 8852–8865. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) has been recognized as an efficient technique to design optimal strategies for different complex systems without prior knowledge of the control landscape. To achieve a fast and precise control for quantum systems, we propose a novel DRL approach by constructing a curriculum consisting of a set of intermediate tasks defined by fidelity thresholds, where the tasks among a curriculum can be statically determined before the learning process or dynamically generated during the learning process. By transferring knowledge between two successive tasks and sequencing tasks according to their difficulties, the proposed curriculum-based DRL (CDRL) method enables the agent to focus on easy tasks in the early stage, then move onto difficult tasks, and eventually approaches the final task. Numerical comparison with the traditional methods [gradient method (GD), genetic algorithm (GA), and several other DRL methods] demonstrates that CDRL exhibits improved control performance for quantum systems and also provides an efficient way to identify optimal strategies with few control pulses.},
  archive      = {J_TNNLS},
  author       = {Hailan Ma and Daoyi Dong and Steven X. Ding and Chunlin Chen},
  doi          = {10.1109/TNNLS.2022.3153502},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8852-8865},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Curriculum-based deep reinforcement learning for quantum control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized adaptive neural inverse optimal control of
nonlinear interconnected systems. <em>TNNLS</em>, <em>34</em>(11),
8840–8851. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods on decentralized optimal control of continuous-time nonlinear interconnected systems require a complicated and time-consuming iteration on finding the solution of Hamilton–Jacobi–Bellman (HJB) equations. In order to overcome this limitation, in this article, a decentralized adaptive neural inverse approach is proposed, which ensures the optimized performance but avoids solving HJB equations. Specifically, a new criterion of inverse optimal practical stabilization is proposed, based on which a new direct adaptive neural strategy and a modified tuning functions method are proposed to design a decentralized inverse optimal controller. It is proven that all the closed-loop signals are bounded and the goal of inverse optimality with respect to the cost functional is achieved. Illustrative examples validate the performance of the methods presented.},
  archive      = {J_TNNLS},
  author       = {Kaixin Lu and Zhi Liu and Haoyong Yu and C. L. Philip Chen and Yun Zhang},
  doi          = {10.1109/TNNLS.2022.3153360},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8840-8851},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized adaptive neural inverse optimal control of nonlinear interconnected systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerated partially shared dictionary learning with
differentiable scale-invariant sparsity for multi-view clustering.
<em>TNNLS</em>, <em>34</em>(11), 8825–8839. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview dictionary learning (DL) is attracting attention in multiview clustering due to the efficient feature learning ability. However, most existing multiview DL algorithms are facing problems in fully utilizing consistent and complementary information simultaneously in the multiview data and learning the most precise representation for multiview clustering because of gaps between views. This article proposes an efficient multiview DL algorithm for multiview clustering, which uses the partially shared DL model with a flexible ratio of shared sparse coefficients to excavate both consistency and complementarity in the multiview data. In particular, a differentiable scale-invariant function is used as the sparsity regularizer, which considers the absolute sparsity of coefficients as the $\ell _{0}$ norm regularizer but is continuous and differentiable almost everywhere. The corresponding optimization problem is solved by the proximal splitting method with extrapolation technology; moreover, the proximal operator of the differentiable scale-invariant regularizer can be derived. The synthetic experiment results demonstrate that the proposed algorithm can recover the synthetic dictionary well with reasonable convergence time costs. Multiview clustering experiments include six real-world multiview datasets, and the performances show that the proposed algorithm is not sensitive to the regularizer parameter as the other algorithms. Furthermore, an appropriate coefficient sharing ratio can help to exploit consistent information while keeping complementary information from multiview data and thus enhance performances in multiview clustering. In addition, the convergence performances show that the proposed algorithm can obtain the best performances in multiview clustering among compared algorithms and can converge faster than compared multiview algorithms mostly.},
  archive      = {J_TNNLS},
  author       = {Haoli Zhao and Zhenni Li and Wuhui Chen and Zibin Zheng and Shengli Xie},
  doi          = {10.1109/TNNLS.2022.3153310},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8825-8839},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accelerated partially shared dictionary learning with differentiable scale-invariant sparsity for multi-view clustering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Do neural network weights account for classes centers?
<em>TNNLS</em>, <em>34</em>(11), 8815–8824. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploitation of deep neural networks (DNNs) as descriptors in feature learning challenges enjoys apparent popularity over the past few years. The above tendency focuses on the development of effective loss functions that ensure both high feature discrimination among different classes, as well as low geodesic distance between the feature vectors of a given class. The vast majority of the contemporary works rely their formulation on an empirical assumption about the feature space of a network’s last hidden layer, claiming that the weight vector of a class accounts for its geometrical center in the studied space. This article at hand follows a theoretical approach and indicates that the aforementioned hypothesis is not exclusively met. This fact raises stability issues regarding the training procedure of a DNN, as shown in our experimental study. Consequently, a specific symmetry is proposed and studied both analytically and empirically that satisfies the above assumption, addressing the established convergence issues. More specifically, the aforementioned symmetry suggests that all weight vectors are unit, coplanar, and their vector summation equals zero. Such a layout is proven to ensure a more stable learning curve compared against the corresponding ones succeeded by popular models in the field of feature learning.},
  archive      = {J_TNNLS},
  author       = {Ioannis Kansizoglou and Loukas Bampis and Antonios Gasteratos},
  doi          = {10.1109/TNNLS.2022.3153134},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8815-8824},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Do neural network weights account for classes centers?},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine perceptive GANs for brain MR image super-resolution in
wavelet domain. <em>TNNLS</em>, <em>34</em>(11), 8802–8814. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance (MR) imaging plays an important role in clinical and brain exploration. However, limited by factors such as imaging hardware, scanning time, and cost, it is challenging to acquire high-resolution MR images clinically. In this article, fine perceptive generative adversarial networks (FP-GANs) are proposed to produce super-resolution (SR) MR images from the low-resolution counterparts. By adopting the divide-and-conquer scheme, FP-GANs are designed to deal with the low-frequency (LF) and high-frequency (HF) components of MR images separately and parallelly. Specifically, FP-GANs first decompose an MR image into LF global approximation and HF anatomical texture subbands in the wavelet domain. Then, each subband generative adversarial network (GAN) simultaneously concentrates on super-resolving the corresponding subband image. In generator, multiple residual-in-residual dense blocks are introduced for better feature extraction. In addition, the texture-enhancing module is designed to trade off the weight between global topology and detailed textures. Finally, the reconstruction of the whole image is considered by integrating inverse discrete wavelet transformation in FP-GANs. Comprehensive experiments on the MultiRes_7T and ADNI datasets demonstrate that the proposed model achieves finer structure recovery and outperforms the competing methods quantitatively and qualitatively. Moreover, FP-GANs further show the value by applying the SR results in classification tasks.},
  archive      = {J_TNNLS},
  author       = {Senrong You and Baiying Lei and Shuqiang Wang and Charles K. Chui and Albert C. Cheung and Yong Liu and Min Gan and Guocheng Wu and Yanyan Shen},
  doi          = {10.1109/TNNLS.2022.3153088},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8802-8814},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fine perceptive GANs for brain MR image super-resolution in wavelet domain},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive neural coordinated control for multiple
euler-lagrange systems with periodic event-triggered sampling.
<em>TNNLS</em>, <em>34</em>(11), 8791–8801. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the event-triggered coordinated control problem for multiple Euler–Lagrange systems subject to parameter uncertainties and external disturbances. Based on the event-triggered technique, a distributed coordinated control scheme is first proposed, where the neural network-based estimation method is incorporated to compensate for parameter uncertainties. Then, an input-based continuous event-triggered (CET) mechanism is developed to schedule the triggering instants, which ensures that the control command is activated only when some specific events occur. After that, by analyzing the possible finite-time escape behavior of the triggering function, the real-time data sampling and event monitoring requirement in the CET strategy is tactfully ruled out, and the CET policy is further transformed into a periodic event-triggered (PET) one. In doing so, each agent only needs to monitor the triggering function at the preset periodic sampling instants, and accordingly, frequent control updating is further relieved. Besides, a parameter selection criterion is provided to specify the relationship between the control performance and the sampling period. Finally, a numerical example of attitude synchronization for multiple satellites is performed to show the effectiveness and superiority of the proposed coordinated control scheme.},
  archive      = {J_TNNLS},
  author       = {Yongxia Shi and Qinglei Hu and Xiaodong Shao and Yang Shi},
  doi          = {10.1109/TNNLS.2022.3153077},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8791-8801},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural coordinated control for multiple euler-lagrange systems with periodic event-triggered sampling},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diversified regularization enhanced training for effective
manipulator calibration. <em>TNNLS</em>, <em>34</em>(11), 8778–8790. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, robot arms have become an irreplaceable production tool, which play an important role in the industrial production. It is necessary to ensure the absolute positioning accuracy of the robot to realize automatic production. Due to the influence of machining tolerance, assembly tolerance, the robot positioning accuracy is poor. Therefore, in order to enable the precise operation of the robot, it is necessary to calibrate the robotic kinematic parameters. The least square method and Levenberg-Marquardt (LM) algorithm are commonly used to identify the positioning error of robot. However, it generally has the overfitting caused by improper regularization schemes. To solve this problem, this article discusses six regularization schemes based on its error models, i.e., $L_{1}$ , $L_{2}$ , dropout, elastic, log, and swish. Moreover, this article proposes a scheme with six regularization to obtain a reliable ensemble, which can effectively avoid overfitting. The positioning accuracy of the robot is improved significantly after calibration by enough experiments, which verifies the feasibility of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Zhibin Li and Shuai Li and Omaimah Omar Bamasag and Areej Alhothali and Xin Luo},
  doi          = {10.1109/TNNLS.2022.3153039},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8778-8790},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Diversified regularization enhanced training for effective manipulator calibration},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical multiagent formation control scheme via
actor-critic learning. <em>TNNLS</em>, <em>34</em>(11), 8764–8777. (<a
href="https://doi.org/10.1109/TNNLS.2022.3153028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a nearly optimal solution to the cooperative formation control problem for large-scale multiagent system (MAS). First, multigroup technique is widely used for the decomposition of the large-scale problem, but there is no consensus between different subgroups. Inspired by the hierarchical structure applied in the MAS, a hierarchical leader-following formation control structure with multigroup technique is constructed, where two layers and three types of agents are designed. Second, adaptive dynamic programming technique is conformed to the optimal formation control problem by the establishment of performance index function. Based on the traditional generalized policy iteration (PI) algorithm, the multistep generalized policy iteration (MsGPI) is developed with the modification of policy evaluation. The novel algorithm not only inherits the advantages of high convergence speed and low computational complexity in the generalized PI algorithm but also further accelerates the convergence speed and reduces run time. Besides, the stability analysis, convergence analysis, and optimality analysis are given for the proposed multistep PI algorithm. Afterward, a neural network-based actor-critic structure is built for approximating the iterative control policies and value functions. Finally, a large-scale formation control problem is provided to demonstrate the performance of our developed hierarchical leader-following formation control structure and MsGPI algorithm.},
  archive      = {J_TNNLS},
  author       = {Chaoxu Mu and Jiangwen Peng and Changyin Sun},
  doi          = {10.1109/TNNLS.2022.3153028},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8764-8777},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical multiagent formation control scheme via actor-critic learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MCDAL: Maximum classifier discrepancy for active learning.
<em>TNNLS</em>, <em>34</em>(11), 8753–8763. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent state-of-the-art active learning methods have mostly leveraged generative adversarial networks (GANs) for sample acquisition; however, GAN is usually known to suffer from instability and sensitivity to hyperparameters. In contrast to these methods, in this article, we propose a novel active learning framework that we call Maximum Classifier Discrepancy for Active Learning (MCDAL) that takes the prediction discrepancies between multiple classifiers. In particular, we utilize two auxiliary classification layers that learn tighter decision boundaries by maximizing the discrepancies among them. Intuitively, the discrepancies in the auxiliary classification layers’ predictions indicate the uncertainty in the prediction. In this regard, we propose a novel method to leverage the classifier discrepancies for the acquisition function for active learning. We also provide an interpretation of our idea in relation to existing GAN-based active learning methods and domain adaptation frameworks. Moreover, we empirically demonstrate the utility of our approach where the performance of our approach exceeds the state-of-the-art methods on several image classification and semantic segmentation datasets in active learning setups.},
  archive      = {J_TNNLS},
  author       = {Jae Won Cho and Dong-Jin Kim and Yunjae Jung and In So Kweon},
  doi          = {10.1109/TNNLS.2022.3152786},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8753-8763},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MCDAL: Maximum classifier discrepancy for active learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distilling a powerful student model via online knowledge
distillation. <em>TNNLS</em>, <em>34</em>(11), 8743–8752. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing online knowledge distillation approaches either adopt the student with the best performance or construct an ensemble model for better holistic performance. However, the former strategy ignores other students’ information, while the latter increases the computational complexity during deployment. In this article, we propose a novel method for online knowledge distillation, termed feature fusion and self-distillation (FFSD), which comprises two key components: FFSD, toward solving the above problems in a unified framework. Different from previous works, where all students are treated equally, the proposed FFSD splits them into a leader student set and a common student set. Then, the feature fusion module converts the concatenation of feature maps from all common students into a fused feature map. The fused representation is used to assist the learning of the leader student. To enable the leader student to absorb more diverse information, we design an enhancement strategy to increase the diversity among students. Besides, a self-distillation module is adopted to convert the feature map of deeper layers into a shallower one. Then, the shallower layers are encouraged to mimic the transformed feature maps of the deeper layers, which helps the students to generalize better. After training, we simply adopt the leader student, which achieves superior performance, over the common students, without increasing the storage or inference cost. Extensive experiments on CIFAR-100 and ImageNet demonstrate the superiority of our FFSD over existing works. The code is available at https://github.com/SJLeo/FFSD .},
  archive      = {J_TNNLS},
  author       = {Shaojie Li and Mingbao Lin and Yan Wang and Yongjian Wu and Yonghong Tian and Ling Shao and Rongrong Ji},
  doi          = {10.1109/TNNLS.2022.3152732},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8743-8752},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distilling a powerful student model via online knowledge distillation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernel proposal network for arbitrary shape text detection.
<em>TNNLS</em>, <em>34</em>(11), 8731–8742. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation-based methods have achieved great success for arbitrary shape text detection. However, separating neighboring text instances is still one of the most challenging problems due to the complexity of texts in scene images. In this article, we propose an innovative kernel proposal network (dubbed KPN) for arbitrary shape text detection. The proposed KPN can separate neighboring text instances by classifying different texts into instance-independent feature maps, meanwhile avoiding the complex aggregation process existing in segmentation-based arbitrary shape text detection methods. To be concrete, our KPN will predict a Gaussian center map for each text image, which will be used to extract a series of candidate kernel proposals (i.e., dynamic convolution kernel) from the embedding feature maps according to their corresponding keypoint positions. To enforce the independence between kernel proposals, we propose a novel orthogonal learning loss (OLL) via orthogonal constraints. Specifically, our kernel proposals contain important self-information learned by network and location information by position embedding. Finally, kernel proposals will individually convolve all embedding feature maps for generating individual embedded maps of text instances. In this way, our KPN can effectively separate neighboring text instances and improve the robustness against unclear boundaries. To the best of our knowledge, our work is the first to introduce the dynamic convolution kernel strategy to efficiently and effectively tackle the adhesion problem of neighboring text instances in text detection. Experimental results on challenging datasets verify the impressive performance and efficiency of our method. The code and model are available at https://github.com/GXYM/KPN .},
  archive      = {J_TNNLS},
  author       = {Shi-Xue Zhang and Xiaobin Zhu and Jie-Bo Hou and Chun Yang and Xu-Cheng Yin},
  doi          = {10.1109/TNNLS.2022.3152596},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8731-8742},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Kernel proposal network for arbitrary shape text detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated learning with taskonomy for non-IID data.
<em>TNNLS</em>, <em>34</em>(11), 8719–8730. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical federated learning approaches incur significant performance degradation in the presence of non-independent and identically distributed (non-IID) client data. A possible direction to address this issue is forming clusters of clients with roughly IID data. Most solutions following this direction are iterative and relatively slow, also prone to convergence issues in discovering underlying cluster formations. We introduce federated learning with taskonomy ( FLT ) that generalizes this direction by learning the task relatedness between clients for more efficient federated aggregation of heterogeneous data. In a one-off process, the server provides the clients with a pretrained (and fine-tunable) encoder to compress their data into a latent representation and transmit the signature of their data back to the server. The server then learns the task relatedness among clients via manifold learning and performs a generalization of federated averaging. FLT can flexibly handle a generic client relatedness graph, when there are no explicit clusters of clients, as well as efficiently decompose it into (disjoint) clusters for clustered federated learning. We demonstrate that FLT not only outperforms the existing state-of-the-art baselines in non-IID scenarios but also offers improved fairness across clients. Our codebase can be found at: https://github.com/hjraad/FLT/},
  archive      = {J_TNNLS},
  author       = {Hadi Jamali-Rad and Mohammad Abdizadeh and Anuj Singh},
  doi          = {10.1109/TNNLS.2022.3152581},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8719-8730},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Federated learning with taskonomy for non-IID data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stability and admissibility analysis for zero-sum games
under general value iteration formulation. <em>TNNLS</em>,
<em>34</em>(11), 8707–8718. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the general value iteration (GVI) algorithm for discrete-time zero-sum games is investigated. The theoretical analysis focuses on stability properties of the systems and also the admissibility properties of the iterative policy pair. A new criterion is established to determine the admissibility of the current policy pair. Besides, based on the admissibility criterion, the improved GVI algorithm toward zero-sum games is developed to guarantee that all iterative policy pairs are admissible if the current policy pair satisfies the criterion. On the basis of the attraction domain, we demonstrate that the state trajectory will stay in the region using the fixed or the evolving policy pair if the initial state belongs to the domain. It is emphasized that the evolving policy pair can stabilize the controlled system. These theoretical results are applied to linear and nonlinear systems via offline and online critic control design.},
  archive      = {J_TNNLS},
  author       = {Ding Wang and Mingming Zhao and Mingming Ha and Junfei Qiao},
  doi          = {10.1109/TNNLS.2022.3152268},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8707-8718},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability and admissibility analysis for zero-sum games under general value iteration formulation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gait prediction and variable admittance control for lower
limb exoskeleton with measurement delay and extended-state-observer.
<em>TNNLS</em>, <em>34</em>(11), 8693–8706. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The measurement delay of the feedback control system is a universal problem in industrial engineering, which will degrade output performance, especially causing undesirable chatter responses. In this study, a deep-Gaussian-process (DGP)-based method for operator’s gait prediction is proposed to estimate the real-time motion intention and to compensate for the measurement delay of the inertial measurement unit (IMU). On the basis of these gait prediction uncertainties quantified by the DGP method, a variable admittance controller is designed to reduce real-time human–exoskeleton interaction torque. The reference trajectory is generated by the admittance controller, which is smoothed by the two-order Bessel interpolation. Meanwhile, the admittance parameters are self-regulated based on the defined uncertainty index of gait prediction. The extend-state observer (ESO) with backstepping iteration is adopted to compensate unmeasured system state, model uncertainties, and unmodeled dynamics of lower limb exoskeleton. The effectiveness of the proposed gait prediction and control scheme is verified by both the comparative simulations and experimental results of the human–exoskeleton cooperative motion.},
  archive      = {J_TNNLS},
  author       = {Zhenlei Chen and Qing Guo and Tieshan Li and Yao Yan and Dan Jiang},
  doi          = {10.1109/TNNLS.2022.3152255},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8693-8706},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gait prediction and variable admittance control for lower limb exoskeleton with measurement delay and extended-state-observer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyperspectral anomaly detection with tensor average rank and
piecewise smoothness constraints. <em>TNNLS</em>, <em>34</em>(11),
8679–8692. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in hyperspectral images (HSIs) has attracted considerable interest in the remote-sensing domain, which aims to identify pixels with different spectral and spatial features from their surroundings. Most of the existing anomaly detection methods convert the 3-D data cube to a 2-D matrix composed of independent spectral vectors, which destroys the intrinsic spatial correlation between the pixels and their surrounding pixels, thus leading to considerable degradation in detection performance. In this article, we develop a tensor-based anomaly detection algorithm that can effectively preserve the spatial–spectral information of the original data. We first separate the 3-D HSI data into a background tensor and an anomaly tensor. Then the tensor nuclear norm based on the tensor singular value decomposition (SVD) is exploited to characterize the global low rank existing in both the spectral and spatial directions of the background tensor. In addition, the total variation (TV) regularization is incorporated due to the piecewise smoothness. For the anomaly component, the $l_{2.1}$ norm is exploited to promote the group sparsity of anomalous pixels. In order to improve the ability of the algorithm to distinguish the anomaly from the background, we design a robust background dictionary. We first split the HSI data into local clusters by leveraging their spectral similarity and spatial distance. Then we develop a simple but effective way based on the SVD to select representative pixels as atoms. The constructed background dictionary can effectively represent the background materials and eliminate anomalies. Experimental results obtained using several real hyperspectral datasets demonstrate the superiority of the proposed method compared with some state-of-the-art anomaly detection algorithms.},
  archive      = {J_TNNLS},
  author       = {Siyu Sun and Jun Liu and Xun Chen and Wei Li and Hongbin Li},
  doi          = {10.1109/TNNLS.2022.3152252},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8679-8692},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperspectral anomaly detection with tensor average rank and piecewise smoothness constraints},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data augmentation in defect detection of sanitary ceramics
in small and non-i.i.d datasets. <em>TNNLS</em>, <em>34</em>(11),
8669–8678. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a data-augmentation method is proposed to narrow the significant difference between the distribution of training and test sets when small sample sizes are concerned. Two major obstacles exist in the process of defect detection on sanitary ceramics. The first results from the high cost of sample collection, namely, the difficulty in obtaining a large number of training images required by deep-learning algorithms, which limits the application of existing algorithms in sanitary-ceramic defect detection. Second, due to the limitation of production processes, the collected defect images are often marked, thereby resulting in great differences in distribution compared with the images of test sets, which further affects the performance of detect-detection algorithms. The lack of training data and the differences in distribution between training and test sets lead to the fact that existing deep learning-based algorithms cannot be used directly in the defect detection of sanitary ceramics. The method proposed in this study, which is based on a generative adversarial network and the Gaussian mixture model, can effectively increase the number of training samples and reduce distribution differences between training and test sets, and the features of the generated images can be controlled to a certain extent. By applying this method, the accuracy is improved from approximately 75\% to nearly 90\% in almost all experiments on different classification networks.},
  archive      = {J_TNNLS},
  author       = {Xinyang Ren and Weiyang Lin and Xianqiang Yang and Xinghu Yu and Huijun Gao},
  doi          = {10.1109/TNNLS.2022.3152245},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8669-8678},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data augmentation in defect detection of sanitary ceramics in small and non-i.i.d datasets},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Theme-aware aesthetic distribution prediction with
full-resolution photographs. <em>TNNLS</em>, <em>34</em>(11), 8654–8668.
(<a href="https://doi.org/10.1109/TNNLS.2022.3151787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aesthetic quality assessment (AQA) is a challenging task due to complex aesthetic factors. Currently, it is common to conduct AQA using deep neural networks (DNNs) that require fixed-size inputs. The existing methods mainly transform images by resizing, cropping, and padding or use adaptive pooling to alternately capture the aesthetic features from fixed-size inputs. However, these transformations potentially damage aesthetic features. To address this issue, we propose a simple but effective method to accomplish full-resolution image AQA by combining image padding with region of image (RoM) pooling. Padding turns inputs into the same size. RoM pooling pools image features and discards extra padded features to eliminate the side effects of padding. In addition, the image aspect ratios are encoded and fused with visual features to remedy the shape information loss of RoM pooling. Furthermore, we observe that the same image may receive different aesthetic evaluations under different themes, which we call the theme criterion bias. Hence, a theme-aware model that uses theme information to guide model predictions is proposed. Finally, we design an attention-based feature fusion module to effectively use both the shape and theme information. Extensive experiments prove the effectiveness of the proposed method over state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Gengyun Jia and Peipei Li and Ran He},
  doi          = {10.1109/TNNLS.2022.3151787},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8654-8668},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Theme-aware aesthetic distribution prediction with full-resolution photographs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerated distributed approximate newton method.
<em>TNNLS</em>, <em>34</em>(11), 8642–8653. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed second-order optimization, as an effective strategy for training large-scale machine learning systems, has been widely investigated due to its low communication complexity. However, the existing distributed second-order optimization algorithms, including distributed approximate Newton ( DANE ), accelerated inexact DANE ( AIDE ), and statistically preconditioned accelerated gradient ( SPAG ), are all required to precisely solve an expensive subproblem up to the target precision. Therefore, this causes these algorithms to suffer from high computation costs and this hinders their development. In this article, we design a novel distributed second-order algorithm called the accelerated distributed approximate Newton ( ADAN ) method to overcome the high computation costs of the existing ones. Compared with DANE , AIDE , and SPAG , which are constructed based on the relative smooth theory, ADAN’s theoretical foundation is built upon the inexact Newton theory. The different theoretical foundations lead to handle the expensive subproblem efficiently, and steps required to solve the subproblem are independent of the target precision. At the same time, ADAN resorts to the acceleration and can effectively exploit the objective function’s curvature information, making ADAN to achieve a low communication complexity. Thus, ADAN can achieve both the communication and computation efficiencies, while DANE , AIDE , and SPAG can achieve only the communication efficiency. Our empirical study also validates the advantages of ADAN over extant distributed second-order algorithms.},
  archive      = {J_TNNLS},
  author       = {Haishan Ye and Chaoyang He and Xiangyu Chang},
  doi          = {10.1109/TNNLS.2022.3151736},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8642-8653},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accelerated distributed approximate newton method},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain neural adaptation. <em>TNNLS</em>, <em>34</em>(11),
8630–8641. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is concerned with the problem of generalizing a classification model to a target domain with little or no labeled data, by leveraging the abundant labeled data from a related source domain. The source and target domains possess different joint probability distributions, making it challenging for model generalization. In this article, we introduce domain neural adaptation (DNA): an approach that exploits nonlinear deep neural network to 1) match the source and target joint distributions in the network activation space and 2) learn the classifier in an end-to-end manner. Specifically, we employ the relative chi-square divergence to compare the two joint distributions, and show that the divergence can be estimated via seeking the maximal value of a quadratic functional over the reproducing kernel hilbert space. The analytic solution to this maximization problem enables us to explicitly express the divergence estimate as a function of the neural network mapping. We optimize the network parameters to minimize the estimated joint distribution divergence and the classification loss, yielding a classification model that generalizes well to the target domain. Empirical results on several visual datasets demonstrate that our solution is statistically better than its competitors.},
  archive      = {J_TNNLS},
  author       = {Sentao Chen and Zijie Hong and Mehrtash Harandi and Xiaowei Yang},
  doi          = {10.1109/TNNLS.2022.3151683},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8630-8641},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Domain neural adaptation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Nonzero-sum game-based voltage recovery consensus optimal
control for nonlinear microgrids system. <em>TNNLS</em>,
<em>34</em>(11), 8617–8629. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since most of the existing models based on the microgrids (MGs) are nonlinear, which could cause the controller oscillate, resulting in the excessive line loss, and the nonlinear could also lead to the controller design difficulty of MGs system. Therefore, this article researches the distributed voltage recovery consensus optimal control problem for the nonlinear MGs system with $N$ -distributed generations (DGs), in the case of providing stringent real power sharing. First, based on the distributed cooperative control concept of multiagent systems and the critic neural networks (NNs), a novel distributed secondary voltage recovery consensus optimal control protocol is constructed via applying the backstepping technique and nonzero-sum (NZS) differential game strategy to realize the voltage recovery of island MGs. Meanwhile, the model identifier is established to reconstruct the unknown NZS games systems based on a three-layer NN. Then, a critic NN weight adaptive adjustment tuning law is proposed to ensure the convergence of the cost functions and the stability of the closed-loop system. Furthermore, according to Lyapunov stability theory, it is proven that all signals are uniform ultimate boundedness in the closed loop system and the voltage recovery synchronization error converges to an arbitrarily small neighborhood of the origin near. Finally, some simulation results in MATLAB illustrate the validity of the proposed control strategy.},
  archive      = {J_TNNLS},
  author       = {Guangliang Liu and Qiuye Sun and Rui Wang and Xuguang Hu},
  doi          = {10.1109/TNNLS.2022.3151650},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8617-8629},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonzero-sum game-based voltage recovery consensus optimal control for nonlinear microgrids system},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning algorithm with knowledge division level.
<em>TNNLS</em>, <em>34</em>(11), 8602–8616. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major challenges of transfer learning algorithms is the domain drifting problem where the knowledge of source scene is inappropriate for the task of target scene. To solve this problem, a transfer learning algorithm with knowledge division level (KDTL) is proposed to subdivide knowledge of source scene and leverage them with different drifting degrees. The main properties of KDTL are three folds. First, a comparative evaluation mechanism is developed to detect and subdivide the knowledge into three kinds—the ineffective knowledge, the usable knowledge, and the efficient knowledge. Then, the ineffective and usable knowledge can be found to avoid the negative transfer problem. Second, an integrated framework is designed to prune the ineffective knowledge in the elastic layer, reconstruct the usable knowledge in the refined layer, and learn the efficient knowledge in the leveraged layer. Then, the efficient knowledge can be acquired to improve the learning performance. Third, the theoretical analysis of the proposed KDTL is analyzed in different phases. Then, the convergence property, error bound, and computational complexity of KDTL are provided for the successful applications. Finally, the proposed KDTL is tested by several benchmark problems and some real problems. The experimental results demonstrate that this proposed KDTL can achieve significant improvement over some state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Honggui Han and Hongxu Liu and Cuili Yang and Junfei Qiao},
  doi          = {10.1109/TNNLS.2022.3151646},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8602-8616},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transfer learning algorithm with knowledge division level},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Verbal-person nets: Pose-guided multi-granularity
language-to-person generation. <em>TNNLS</em>, <em>34</em>(11),
8589–8601. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person image generation conditioned on natural language allows us to personalize image editing in a user-friendly manner. This fashion, however, involves different granularities of semantic relevance between texts and visual content. Given a sentence describing an unknown person, we propose a novel pose-guided multi-granularity attention architecture to synthesize the person image in an end-to-end manner. To determine what content to draw at a global outline, the sentence-level description and pose feature maps are incorporated into a U-Net architecture to generate a coarse person image. To further enhance the fine-grained details, we propose to draw the human body parts with highly correlated textual nouns and determine the spatial positions with respect to target pose points. Our model is premised on a conditional generative adversarial network (GAN) that translates language description into a realistic person image. The proposed model is coupled with two-stream discriminators: 1) text-relevant local discriminators to improve the fine-grained appearance by identifying the region–text correspondences at the finer manipulation and 2) a global full-body discriminator to regulate the generation via a pose-weighting feature selection. Extensive experiments conducted on benchmarks validate the superiority of our method for person image generation.},
  archive      = {J_TNNLS},
  author       = {Deyin Liu and Lin Wu and Feng Zheng and Lingqiao Liu and Meng Wang},
  doi          = {10.1109/TNNLS.2022.3151631},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8589-8601},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Verbal-person nets: Pose-guided multi-granularity language-to-person generation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integral BLF-based adaptive neural constrained regulation
for switched systems with unknown bounds on control gain.
<em>TNNLS</em>, <em>34</em>(11), 8579–8588. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an integral barrier Lyapunov-function (IBLF)-based adaptive tracking controller is proposed for a class of switched nonlinear systems under the arbitrary switching rule, in which the unknown terms are approximated by radial basis function neural networks (RBFNNs). The IBLF method is used to solve the problem of state constraint. This method constrains states directly and avoids the verification of feasibility conditions. In addition, a completely unknown control gain is considered, which makes it impossible to directly apply previous existing methods. To offset the effect of the unknown control gain, the lower bound of the control gain is added into the barrier Lyapunov function, and a regulating term is introduced into the controller. The proposed control strategy realizes three control objectives: 1) all the signals in the resulting system are bounded; 2) the system output tracks the reference signal to a arbitrarily small compact set; and 3) all the constraint conditions for system states are not violated. Finally, a simulation example is used to show the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Li Tang and Kaiyue He and Yang Chen and Yan-Jun Liu and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2022.3151625},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8579-8588},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Integral BLF-based adaptive neural constrained regulation for switched systems with unknown bounds on control gain},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust mesh representation learning via efficient local
structure-aware anisotropic convolution. <em>TNNLS</em>,
<em>34</em>(11), 8566–8578. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh is a type of data structure commonly used for 3-D shapes. Representation learning for 3-D meshes is essential in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insights from CNN for 3-D shapes. However, 3-D shape data are irregular since each node’s neighbors are unordered. Various graph neural networks for 3-D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this article, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each template’s node according to its neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer—a new Transformer model for natural language processing (NLP). Since the learnable weighting matrices require large amounts of parameters for high-resolution 3-D shapes, we introduce a matrix factorization technique to notably reduce the parameter size, denoted as LSA-small. Furthermore, a residual connection with a linear transformation is introduced to improve the performance of our LSA-Conv. Comprehensive experiments demonstrate that our model produces significant improvement in 3-D shape reconstruction compared to state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Zhongpai Gao and Junchi Yan and Guangtao Zhai and Juyong Zhang and Xiaokang Yang},
  doi          = {10.1109/TNNLS.2022.3151609},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8566-8578},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust mesh representation learning via efficient local structure-aware anisotropic convolution},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient architecture search for continual learning.
<em>TNNLS</em>, <em>34</em>(11), 8555–8565. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning with neural networks, which aims to learn a sequence of tasks, is an important learning framework in artificial intelligence (AI). However, it often confronts three challenges: 1) overcome the catastrophic forgetting problem; 2) adapt the current network to new tasks; and 3) control its model complexity. To reach these goals, we propose a novel approach named continual learning with efficient architecture search (CLEAS). CLEAS works closely with neural architecture search (NAS), which leverages reinforcement learning techniques to search for the best neural architecture that fits a new task. In particular, we design a neuron-level NAS controller that decides which old neurons from previous tasks should be reused (knowledge transfer) and which new neurons should be added (to learn new knowledge). Such a fine-grained controller allows finding a very concise architecture that can fit each new task well. Meanwhile, since we do not alter the weights of the reused neurons, we perfectly memorize the knowledge learned from the previous tasks. We evaluate CLEAS on numerous sequential classification tasks, and the results demonstrate that CLEAS outperforms other state-of-the-art alternative methods, achieving higher classification accuracy while using simpler neural architectures.},
  archive      = {J_TNNLS},
  author       = {Qiang Gao and Zhipeng Luo and Diego Klabjan and Fengli Zhang},
  doi          = {10.1109/TNNLS.2022.3151511},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8555-8565},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient architecture search for continual learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep clustering and visualization for end-to-end
high-dimensional data analysis. <em>TNNLS</em>, <em>34</em>(11),
8543–8554. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data analysis for exploration and discovery includes two fundamental tasks: deep clustering and data visualization. When these two associated tasks are done separately, as is often the case thus far, disagreements can occur among the tasks in terms of geometry preservation. Namely, the clustering process is often accompanied by the corruption of the geometric structure, whereas visualization aims to preserve the data geometry for better interpretation. Therefore, how to achieve deep clustering and data visualization in an end-to-end unified framework is an important but challenging problem. In this article, we propose a novel neural network-based method, called deep clustering and visualization (DCV), to accomplish the two associated tasks end-to-end to resolve their disagreements. The DCV framework consists of two nonlinear dimensionality reduction (NLDR) transformations: 1) one from the input data space to latent feature space for clustering and 2) the other from the latent feature space to the final 2-D space for visualization. Importantly, the first NLDR transformation is mainly optimized by one Clustering Loss, allowing arbitrary corruption of the geometric structure for better clustering, while the second NLDR transformation is optimized by one Geometry-Preserving Loss to recover the corrupted geometry for better visualization. Extensive comparative results show that the DCV framework outperforms other leading clustering-visualization algorithms in terms of both quantitative evaluation metrics and qualitative visualization.},
  archive      = {J_TNNLS},
  author       = {Lirong Wu and Lifan Yuan and Guojiang Zhao and Haitao Lin and Stan Z. Li},
  doi          = {10.1109/TNNLS.2022.3151498},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8543-8554},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep clustering and visualization for end-to-end high-dimensional data analysis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning semantics-consistent stripes with self-refinement
for person re-identification. <em>TNNLS</em>, <em>34</em>(11),
8531–8542. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aligning human parts automatically is one of the most challenging problems for person re-identification (re-ID). Recently, the stripe-based methods, which equally partition the person images into the fixed stripes for aligned representation learning, have achieved great success. However, the stripes with fixed height and position cannot well handle the misalignment problems caused by inaccurate detection and occlusion and may introduce much background noise. In this article, we aim at learning adaptive stripes with foreground refinement to achieve pixel-level part alignment by only using person identity labels for person re-ID and make two contributions. 1) A semantics-consistent stripe learning method (SCS). Given an image, SCS partitions it into adaptive horizontal stripes and each stripe is corresponding to a specific semantic part. Specifically, SCS iterates between two processes: i) clustering the rows to human parts or background to generate the pseudo-part labels of rows and ii) learning a row classifier to partition a person image, which is supervised by the latest pseudo-labels. This iterative scheme guarantees the accuracy of the learned image partition. 2) A self-refinement method (SCS+) to remove the background noise in stripes. We employ the above row classifier to generate the probabilities of pixels belonging to human parts (foreground) or background, which is called the class activation map (CAM). Only the most confident areas from the CAM are assigned with foreground/background labels to guide the human part refinement. Finally, by intersecting the semantics-consistent stripes with the foreground areas, SCS+ locates the human parts at pixel-level, obtaining a more robust part-aligned representation. Extensive experiments validate that SCS+ sets the new state-of-the-art performance on three widely used datasets including Market-1501, DukeMTMC-reID, and CUHK03-NP.},
  archive      = {J_TNNLS},
  author       = {Kuan Zhu and Haiyun Guo and Songyan Liu and Jinqiao Wang and Ming Tang},
  doi          = {10.1109/TNNLS.2022.3151487},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8531-8542},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning semantics-consistent stripes with self-refinement for person re-identification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite-time and fixed-time synchronization of delayed
memristive neural networks via adaptive aperiodically intermittent
adjustment strategy. <em>TNNLS</em>, <em>34</em>(11), 8516–8530. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the finite-time and fixed-time synchronization for memristive neural networks (MNNs) with mixed time-varying delays under the adaptive aperiodically intermittent adjustment strategy. Different from previous works, this article first employs the aperiodically intermittent adjustment feedback control and adaptive control to drive the MNNs to achieve synchronization in finite time and fixed time. First of all, according to the theories of set-valued mappings and differential inclusions, the error MNNs is derived, and its finite-time and fixed-time stability problems are discussed by applying the Lyapunov function method and some LMI techniques. Moreover, by meticulously designing an effective aperiodically intermittent adjustment with adaptive updating law, sufficient conditions that guarantee the finite-time and fixed-time synchronization of the drive-response MNNs are obtained, and the settling time is explicitly estimated. Finally, three numerical examples are provided to illustrate the validity of the obtained theoretical results.},
  archive      = {J_TNNLS},
  author       = {Liyan Cheng and Fangcheng Tang and Xinli Shi and Xiangyong Chen and Jianlong Qiu},
  doi          = {10.1109/TNNLS.2022.3151478},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8516-8530},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time and fixed-time synchronization of delayed memristive neural networks via adaptive aperiodically intermittent adjustment strategy},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SLOGAN: Handwriting style synthesis for arbitrary-length and
out-of-vocabulary text. <em>TNNLS</em>, <em>34</em>(11), 8503–8515. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large amounts of labeled data are urgently required for the training of robust text recognizers. However, collecting handwriting data of diverse styles, along with an immense lexicon, is considerably expensive. Although data synthesis is a promising way to relieve data hunger, two key issues of handwriting synthesis, namely, style representation and content embedding, remain unsolved. To this end, we propose a novel method that can synthesize parameterized and controllable handwriting S tyles for arbitrary-Length and O ut-of-vocabulary text based on a G enerative A dversarial N etwork (GAN), termed SLOGAN. Specifically, we propose a style bank to parameterize specific handwriting styles as latent vectors, which are input to a generator as style priors to achieve the corresponding handwritten styles. The training of the style bank requires only writer identification of the source images, rather than attribute annotations. Moreover, we embed the text content by providing an easily obtainable printed style image, so that the diversity of the content can be flexibly achieved by changing the input printed image. Finally, the generator is guided by dual discriminators to handle both the handwriting characteristics that appear as separated characters and in a series of cursive joins. Our method can synthesize words that are not included in the training vocabulary and with various new styles. Extensive experiments have shown that high-quality text images with great style diversity and rich vocabulary can be synthesized using our method, thereby enhancing the robustness of the recognizer.},
  archive      = {J_TNNLS},
  author       = {Canjie Luo and Yuanzhi Zhu and Lianwen Jin and Zhe Li and Dezhi Peng},
  doi          = {10.1109/TNNLS.2022.3151477},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8503-8515},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SLOGAN: Handwriting style synthesis for arbitrary-length and out-of-vocabulary text},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning a world model with multitimescale memory
augmentation. <em>TNNLS</em>, <em>34</em>(11), 8493–8502. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based reinforcement learning (RL) is regarded as a promising approach to tackle the challenges that hinder model-free RL. The success of model-based RL hinges critically on the quality of the predicted dynamic models. However, for many real-world tasks involving high-dimensional state spaces, current dynamics prediction models show poor performance in long-term prediction. To that end, we propose a novel two-branch neural network architecture with multi-timescale memory augmentation to handle long-term and short-term memory differently. Specifically, we follow previous works to introduce a recurrent neural network architecture to encode history observation sequences into latent space, characterizing the long-term memory of agents. Different from previous works, we view the most recent observations as the short-term memory of agents and employ them to directly reconstruct the next frame to avoid compounding error. This is achieved by introducing a self-supervised optical flow prediction structure to model the action-conditional feature transformation at pixel level. The reconstructed observation is finally augmented by the long-term memory to ensure semantic consistency. Experimental results show that our approach is able to generate visually-realistic long-term predictions in DeepMind maze navigation games, and outperforms the prevalent state-of-the-art methods in prediction accuracy by a large margin. Furthermore, we also evaluate the usefulness of our world model by using the predicted frames to drive an imagination-augmented exploration strategy to improve the model-free RL controller.},
  archive      = {J_TNNLS},
  author       = {Wenzhe Cai and Teng Wang and Jiawei Wang and Changyin Sun},
  doi          = {10.1109/TNNLS.2022.3151412},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8493-8502},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning a world model with multitimescale memory augmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end dialogue generation using a single encoder and a
decoder cascade with a multidimension attention mechanism.
<em>TNNLS</em>, <em>34</em>(11), 8482–8492. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human dialogues often show underlying dependencies between turns, with each interlocutor influencing the queries/responses of the other. This article follows this by proposing a neural architecture for conversation modeling that looks at the dialogue history of both sides. It consists of a generative model where one encoder feeds three decoders to process three successive turns of dialogue for predicting the next utterance, with a multidimension attention mechanism aggregating the past and current contexts for a cascade effect on each decoder. As a result, a more comprehensive account of the dialogue evolution is obtained than by focusing on a single turn or the last encoder context, or on the user side alone. The response generation performance of the model is evaluated on three corpora of different sizes and topics, and a comparison is made with six recent generative neural architectures, using both automatic metrics and human judgments. Our results show that the proposed architecture equals or improves the state-of-the-art for adequacy and fluency, particularly when large open-domain corpora are used in the training. Moreover, it allows better tracking of the dialogue state evolution for response explainability.},
  archive      = {J_TNNLS},
  author       = {Billal Belainine and Fatiha Sadat and Mounir Boukadoum},
  doi          = {10.1109/TNNLS.2022.3151347},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8482-8492},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {End-to-end dialogue generation using a single encoder and a decoder cascade with a multidimension attention mechanism},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recursive reasoning with reduced complexity and
intermittency for nonequilibrium learning in stochastic games.
<em>TNNLS</em>, <em>34</em>(11), 8467–8481. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a computationally and communicationally efficient approach for decision-making in nonequilibrium stochastic games. In particular, due to the inherent complexity of computing Nash equilibria, as well as the innate tendency of agents to choose nonequilibrium strategies, we construct two models of bounded rationality based on recursive reasoning. In the first model, named level- $k$ thinking, each agent assumes that everyone else has a cognitive level immediately lower than theirs and—given such an assumption—chooses their policy to be a best response to them. In the second model, named cognitive hierarchy, each agent conjectures that the rest of the agents have a cognitive level that is lower than theirs, but follows a distribution instead of being deterministic. To explicitly compute the boundedly rational policies, a level-recursive algorithm and a level-paralleled algorithm are constructed, where the latter one can have an overall reduced computational complexity. To further reduce the complexity in the communication layer, modifications of the proposed nonequilibrium strategies are presented, which do not require the action of a boundedly rational agent to be updated at each step of the stochastic game. Simulations are performed that demonstrate our results.},
  archive      = {J_TNNLS},
  author       = {Filippos Fotiadis and Kyriakos G. Vamvoudakis},
  doi          = {10.1109/TNNLS.2022.3151250},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8467-8481},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Recursive reasoning with reduced complexity and intermittency for nonequilibrium learning in stochastic games},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predefined-time hierarchical coordinated neural control for
hypersonic reentry vehicle. <em>TNNLS</em>, <em>34</em>(11), 8456–8466.
(<a href="https://doi.org/10.1109/TNNLS.2022.3151198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the predefined-time hierarchical coordinated adaptive control on the hypersonic reentry vehicle in presence of low actuator efficiency. In order to compensate for the deficiency of rudder deflection in advantage of channel coupling, the hierarchical design is proposed for coordination of the elevator deflection and aileron deflection. Under the control scheme, the equivalent control law and switching control law are constructed with the predefined-time technology. For the dynamics uncertainty approximation, the composite learning using the tracking error and the prediction error is constructed by designing the serial-parallel estimation model. The closed-loop system stability is analyzed via the Lyapunov approach and the tracking errors are guaranteed to be uniformly ultimately bounded in a predefined time. The tracking performance and the learning accuracy of the proposed algorithm are verified via simulation tests.},
  archive      = {J_TNNLS},
  author       = {Bin Xu and Yingxin Shou and Zhongke Shi and Tian Yan},
  doi          = {10.1109/TNNLS.2022.3151198},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8456-8466},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Predefined-time hierarchical coordinated neural control for hypersonic reentry vehicle},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NPENAS: Neural predictor guided evolution for neural
architecture search. <em>TNNLS</em>, <em>34</em>(11), 8441–8455. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) adopts a search strategy to explore the predefined search space to find superior architecture with the minimum searching costs. Bayesian optimization (BO) and evolutionary algorithms (EA) are two commonly used search strategies, but they suffer from being computationally expensive, challenging to implement, and exhibiting inefficient exploration ability. In this article, we propose a neural predictor guided EA to enhance the exploration ability of EA for NAS (NPENAS) and design two kinds of neural predictors. The first predictor is a BO acquisition function for which we design a graph-based uncertainty estimation network as the surrogate model. The second predictor is a graph-based neural network that directly predicts the performance of the input neural architecture. The NPENAS using the two neural predictors are denoted as NPENAS-BO and NPENAS-NP, respectively. In addition, we introduce a new random architecture sampling method to overcome the drawbacks of the existing sampling method. Experimental results on five NAS search spaces indicate that NPENAS-BO and NPENAS-NP outperform most existing NAS algorithms, with NPENAS-NP achieving state-of-the-art performance on four of the five search spaces.},
  archive      = {J_TNNLS},
  author       = {Chen Wei and Chuang Niu and Yiping Tang and Yue Wang and Haihong Hu and Jimin Liang},
  doi          = {10.1109/TNNLS.2022.3151160},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8441-8455},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {NPENAS: Neural predictor guided evolution for neural architecture search},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to learn variational quantum algorithm.
<em>TNNLS</em>, <em>34</em>(11), 8430–8440. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational quantum algorithms (VQAs) use classical computers as the quantum outer loop optimizer and update the circuit parameters to obtain an approximate ground state. In this article, we present a meta-learning variational quantum algorithm (meta-VQA) by recurrent unit, which uses a technique called “meta-learner.” Motivated by the hybrid quantum-classical algorithms, we train classical recurrent units to assist quantum computing, learning to find approximate optima in the parameter landscape. Here, aiming to reduce the sampling number more efficiently, we use the quantum stochastic gradient descent method and introduce the adaptive learning rate. Finally, we deploy on the TensorFlow Quantum processor within approximate quantum optimization for the Ising model and variational quantum eigensolver for molecular hydrogen (H2), lithium hydride (LiH), and helium hydride cation (HeH+). Our algorithm can be expanded to larger system sizes and problem instances, which have higher performance on near-term processors.},
  archive      = {J_TNNLS},
  author       = {Rui Huang and Xiaoqing Tan and Qingshan Xu},
  doi          = {10.1109/TNNLS.2022.3151127},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8430-8440},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning to learn variational quantum algorithm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MmPose-NLP: A natural language processing approach to
precise skeletal pose estimation using mmWave radars. <em>TNNLS</em>,
<em>34</em>(11), 8418–8429. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we presented mmPose-NLP, a novel natural language processing (NLP) inspired sequence-to-sequence (Seq2Seq) skeletal key-point estimator using millimeter-wave (mmWave) radar data. To the best of our knowledge, this is the first method to precisely estimate up to 25 skeletal key points using mmWave radar data alone. Skeletal pose estimation is critical in several applications ranging from autonomous vehicles, traffic monitoring, patient monitoring, and gait analysis, to defense security forensics, and aid both preventative and actionable decision making. The use of mmWave radars for this task, over traditionally employed optical sensors, provides several advantages, primarily its operational robustness to scene lighting and adverse weather conditions, where optical sensor performance degrade significantly. The mmWave radar point-cloud (PCL) data are first voxelized (analogous to tokenization in NLP) and $N$ frames of the voxelized radar data (analogous to a text paragraph in NLP) is subjected to the proposed mmPose-NLP architecture, where the voxel indices of the 25 skeletal key points (analogous to keyword extraction in NLP) are predicted. The voxel indices are converted back to real-world 3-D coordinates using the voxel dictionary used during the tokenization process. Mean absolute error (MAE) metrics were used to measure the accuracy of the proposed system against the ground truth, with the proposed mmPose-NLP offering $N = {1,2, {\dots },10}$ . A comprehensive methodology, results, discussions, and limitations are presented in this article. All the source codes and results are made available on GitHub for further research and development in this critical yet emerging domain of skeletal key-point estimation using mmWave radars.},
  archive      = {J_TNNLS},
  author       = {Arindam Sengupta and Siyang Cao},
  doi          = {10.1109/TNNLS.2022.3151101},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8418-8429},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MmPose-NLP: A natural language processing approach to precise skeletal pose estimation using mmWave radars},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blind attention geometric restraint neural network for
single image dynamic/defocus deblurring. <em>TNNLS</em>,
<em>34</em>(11), 8404–8417. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the information loss analysis of the blur accumulation model, a novel single-image deblurring method is proposed. We apply the recurrent neural network architecture to capture the attention perception map and the generative adversarial network (GAN) architecture to yield the deblurring image. Considering that the attention mechanism has to make hard decisions about specific parts of the input image to be focused on since blurry regions are not given, we propose a new adaptive attention disentanglement model based on the variation blind source separation, which provides the global geometric restraint to reduce the large solution space, so that the generator can realistically restore details on blurry regions, and the discriminator can accurately assess the content consistency of the restored regions. Since we combine blind source separation, attention geometric restraint with GANs, we name the proposed method BAGdeblur. Extensive evaluations on quantitative and qualitative experiments show that the proposed method achieves the state-of-the-art performance on both synthetic datasets and real-world blurry images.},
  archive      = {J_TNNLS},
  author       = {Jie Zhang and Wanming Zhai},
  doi          = {10.1109/TNNLS.2022.3151099},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8404-8417},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Blind attention geometric restraint neural network for single image Dynamic/Defocus deblurring},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust supervised and semisupervised least squares
regression using ℓ2,p-norm minimization. <em>TNNLS</em>,
<em>34</em>(11), 8389–8403. (<a
href="https://doi.org/10.1109/TNNLS.2022.3150102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Least squares regression (LSR) is widely applied in statistics theory due to its theoretical solution, which can be used in supervised, semisupervised, and multiclass learning. However, LSR begins to fail and its discriminative ability cannot be guaranteed when the original data have been corrupted and noised. In reality, the noises are unavoidable and could greatly affect the error construction in LSR. To cope with this problem, a robust supervised LSR (RSLSR) is proposed to eliminate the effect of noises and outliers. The loss function adopts $\ell _{2,p}$ -norm ( $0&amp;lt; p\leq 2$ ) instead of square loss. In addition, the probability weight is added to each sample to determine whether the sample is a normal point or not. Its physical meaning is very clear, in which if the point is normal, the probability value is 1; otherwise, the weight is 0. To effectively solve the concave problem, an iterative algorithm is introduced, in which additional weights are added to penalize normal samples with large errors. We also extend RSLSR to robust semisupervised LSR (RSSLSR) to fully utilize the limited labeled samples. A large number of classification performances on corrupted data illustrate the robustness of the proposed methods.},
  archive      = {J_TNNLS},
  author       = {Jingyu Wang and Fangyuan Xie and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3150102},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8389-8403},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust supervised and semisupervised least squares regression using ℓ2,p-norm minimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). LLP-GAN: A GAN-based algorithm for learning from label
proportions. <em>TNNLS</em>, <em>34</em>(11), 8377–8388. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from label proportions (LLP) is a widespread and important learning paradigm: only the bag-level proportional information of the grouped training instances is available for the classification task, instead of the instance-level labels in the fully supervised scenario. As a result, LLP is a typical weakly supervised learning protocol and commonly exists in privacy protection circumstances due to the sensitivity in label information for real-world applications. In general, it is less laborious and more efficient to collect label proportions as the bag-level supervised information than the instance-level one. However, the hint for learning the discriminative feature representation is also limited as a less informative signal directly associated with the labels is provided, thus deteriorating the performance of the final instance-level classifier. In this article, delving into the label proportions, we bypass this weak supervision by leveraging generative adversarial networks (GANs) to derive an effective algorithm LLP-GAN. Endowed with an end-to-end structure, LLP-GAN performs approximation in the light of an adversarial learning mechanism without imposing restricted assumptions on distribution. Accordingly, the final instance-level classifier can be directly induced upon the discriminator with minor modification. Under mild assumptions, we give the explicit generative representation and prove the global optimality for LLP-GAN. In addition, compared with existing methods, our work empowers LLP solvers with desirable scalability inheriting from deep models. Extensive experiments on benchmark datasets and a real-world application demonstrate the vivid advantages of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Jiabin Liu and Bo Wang and Hanyuan Hang and Huadong Wang and Zhiquan Qi and Yingjie Tian and Yong Shi},
  doi          = {10.1109/TNNLS.2022.3149926},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8377-8388},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LLP-GAN: A GAN-based algorithm for learning from label proportions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust visual system for looming cue detection against
translating motion. <em>TNNLS</em>, <em>34</em>(11), 8362–8376. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collision detection is critical for autonomous vehicles or robots to serve human society safely. Detecting looming objects robustly and timely plays an important role in collision avoidance systems. The locust lobula giant movement detector (LGMD1) is specifically selective to looming objects which are on a direct collision course. However, the existing LGMD1 models cannot distinguish a looming object from a near and fast translatory moving object, because the latter can evoke a large amount of excitation that can lead to false LGMD1 spikes. This article presents a new visual neural system model (LGMD1) that applies a neural competition mechanism within a framework of separated ON and OFF pathways to shut off the translating response. The competition-based approach responds vigorously to monotonous ON/OFF responses resulting from a looming object. However, it does not respond to paired ON–OFF responses that result from a translating object, thereby enhancing collision selectivity. Moreover, a complementary denoising mechanism ensures reliable collision detection. To verify the effectiveness of the model, we have conducted systematic comparative experiments on synthetic and real datasets. The results show that our method exhibits more accurate discrimination between looming and translational events—the looming motion can be correctly detected. It also demonstrates that the proposed model is more robust than comparative models.},
  archive      = {J_TNNLS},
  author       = {Fang Lei and Zhiping Peng and Mei Liu and Jigen Peng and Vassilis Cutsuridis and Shigang Yue},
  doi          = {10.1109/TNNLS.2022.3149832},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8362-8376},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A robust visual system for looming cue detection against translating motion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilabel convolutional network with feature denoising and
details supplement. <em>TNNLS</em>, <em>34</em>(11), 8349–8361. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multilabel images, the changeable size, posture, and position of objects in the image will increase the difficulty of classification. Moreover, a large amount of irrelevant information interferes with the recognition of objects. Therefore, how to remove irrelevant information from the image to improve the performance of label recognition is an important problem. In this article, we propose a convolutional network based on feature denoising and details supplement (FDDS) to address this issue. In FDDS, we first design a cascade convolution module (CCM) to collect spatial details of upper features, in order to enhance the information expression of features. Second, the feature denoising module (FDM) is further put forward to reallocate the weight of the feature semantic area, in order to enrich the effective semantic information of the current feature and perform denoising operations on object-irrelevant information. Experimental results show that the proposed FDDS outperforms the existing state-of-the-art models on several benchmark datasets, especially for complex scenes.},
  archive      = {J_TNNLS},
  author       = {Tianhao Gu and Zhe Wang and Zhongli Fang and Zonghai Zhu and Hai Yang and Dongdong Li and Wenli Du},
  doi          = {10.1109/TNNLS.2022.3149760},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8349-8361},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multilabel convolutional network with feature denoising and details supplement},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive set-membership state estimation for nonlinear
systems under bit rate allocation mechanism: A neural-network-based
approach. <em>TNNLS</em>, <em>34</em>(11), 8337–8348. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the adaptive neural-network-based (NN-based) set-membership state estimation problem is studied for a class of nonlinear systems subject to bit rate constraints and unknown-but-bounded noises. The measurement output signals are transmitted from sensors to a remote estimator via a bit rate constrained communication channel. To relieve the communication burden and ameliorate the state estimation accuracy, a bit rate allocation mechanism is put forward for the sensor nodes by solving a constrained optimization problem. Subsequently, through the NN learning method, an NN-based set-membership estimator is designed to determine an ellipsoidal set that contains the system state, where the proposed estimator relies upon a prediction-correction structure. With the help of the mathematical induction technique and the set theory, sufficient conditions are obtained to ensure the existence of both the adaptive tuning parameters and the set-membership estimators, and then, the corresponding parameters and estimator gains are calculated by solving a set of optimization problems. In addition, the monotonicity of the upper bound on the squared estimation error with respect to the bit rate and the convergence of the NN weight are analyzed, respectively. Finally, an illustrative example is given to demonstrate the effectiveness of the proposed state estimation algorithm.},
  archive      = {J_TNNLS},
  author       = {Kaiqun Zhu and Zidong Wang and Guoliang Wei and Xiaohui Liu},
  doi          = {10.1109/TNNLS.2022.3149540},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8337-8348},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive set-membership state estimation for nonlinear systems under bit rate allocation mechanism: A neural-network-based approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to assess image quality like an observer.
<em>TNNLS</em>, <em>34</em>(11), 8324–8336. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human observers are the ultimate receivers and evaluators of the image visual information and have powerful perception ability of visual quality with short-term global perception and long-term regional observation. Thus, it is natural to design an image quality assessment (IQA) computational model to act like an observer for accurately predicting the human perception of image quality. Inspired by this, here, we propose a novel observer-like network (OLN) to perform IQA by jointly considering the global glimpsing information and local scanning information. Specifically, the OLN consists of a global distortion perception (GDP) module and a local distortion observation (LDO) module. The GDP module is designed to mimic the observer’s global perception of image quality through performing classification of images’ distortion categories and levels. Simultaneously, to simulate the human local observation behavior, the LDO module attempts to gather the long-term regional observation information of the distorted images by continuously tracing the human scanpath in the observer-like scanning manner. By leveraging the bilinear pooling layer to collaborate the short-term global perception with the long-term regional observation, our network precisely predicts the quality scores of distorted images, such as human observers. Comprehensive experiments on the public datasets powerfully demonstrate that the proposed OLN achieves state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Xiwen Yao and Qinglong Cao and Xiaoxu Feng and Gong Cheng and Junwei Han},
  doi          = {10.1109/TNNLS.2022.3149534},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8324-8336},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning to assess image quality like an observer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HB-DSBM: Modeling the dynamic complex networks from
community level to node level. <em>TNNLS</em>, <em>34</em>(11),
8310–8323. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of methods have been proposed for modeling and mining dynamic complex networks, in which the topological structure varies with time. As the most popular and successful network model, the stochastic block model (SBM) has been extended and applied to community detection, link prediction, anomaly detection, and evolution analysis of dynamic networks. However, all current models based on the SBM for modeling dynamic networks are designed at the community level, assuming that nodes in each community have the same dynamic behavior, which usually results in poor performance on temporal community detection and loses the modeling of node abnormal behavior. To solve the above-mentioned problem, this article proposes a hierarchical Bayesian dynamic SBM (HB-DSBM) for modeling the node-level and community-level dynamic behavior in a dynamic network synchronously. Based on the SBM, we introduce a hierarchical Dirichlet generative mechanism to associate the global community evolution with the microscopic transition behavior of nodes near-perfectly and generate the observed links across the dynamic networks. Meanwhile, an effective variational inference algorithm is developed and we can easy to infer the communities and dynamic behaviors of the nodes. Furthermore, with the two-level evolution behaviors, it can identify nodes or communities with abnormal behavior. Experiments on simulated and real-world networks demonstrate that HB-DSBM has achieved state-of-the-art performance on community detection and evolution. In addition, abnormal evolutionary behavior and events on dynamic networks can be effectively identified by our model.},
  archive      = {J_TNNLS},
  author       = {Pengfei Jiao and Tianpeng Li and Huaming Wu and Chang-Dong Wang and Dongxiao He and Wenjun Wang},
  doi          = {10.1109/TNNLS.2022.3149285},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8310-8323},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HB-DSBM: Modeling the dynamic complex networks from community level to node level},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Entity summarization via exploiting description
complementarity and salience. <em>TNNLS</em>, <em>34</em>(11),
8297–8309. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity summarization is a novel and efficient way to understand real-world facts and solve the increasing information overload problem in large-scale knowledge graphs (KG). Existing studies mainly rely on ranking independent entity descriptions as a list under a certain scoring standard such as importance. However, they often ignore the relatedness and even semantic overlap between individual descriptions. This may seriously interfere with the contribution judgment of descriptions for entity summarization. Actually, the entity summary is a whole to comprehensively integrate the main aspects of entity descriptions, which could be naturally treated as a set. Unfortunately, the exploration of these set characteristics for entity summarization is still an open issue with great challenges. To that end, we draw inspiration from a set completion perspective and propose an entity summarization method with complementarity and salience (ESCS) to deeply exploit description complementarity and salience in order to form a summary set for the target entity. Specifically, we first generate entity description representations with textual features in the description embedding module. For the purpose of learning complementary relationships within the entire summary set, we devise a bi-directional long short-term memory structure to capture global complementarity for each summary in the summary complementarity learning module. Meanwhile, in order to estimate the salience of individual descriptions, we calculate similarities between semantic embeddings of the target entity and its property-value pairs in the description salience learning module. Next, with a joint learning stage, we can optimize ESCS from a set completion perspective. Finally, a summary generation strategy is designed to infer the entire summary set step-by-step for the target entity. Extensive experiments on a public benchmark have clearly demonstrated the effectiveness of ESCS and revealed the potential of set completion in entity summarization task.},
  archive      = {J_TNNLS},
  author       = {Liyi Chen and Zhi Li and Weidong He and Gong Cheng and Tong Xu and Nicholas Jing Yuan and Enhong Chen},
  doi          = {10.1109/TNNLS.2022.3149047},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8297-8309},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Entity summarization via exploiting description complementarity and salience},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A2P-MANN: Adaptive attention inference hops pruned
memory-augmented neural networks. <em>TNNLS</em>, <em>34</em>(11),
8284–8296. (<a
href="https://doi.org/10.1109/TNNLS.2022.3148818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, to limit the number of required attention inference hops in memory-augmented neural networks, we propose an online adaptive approach called $\text{A}^{2}\text{P}$ -memory-augmented neural network (MANN). By exploiting a small neural network classifier, an adequate number of attention inference hops for the input query are determined. The technique results in the elimination of a large number of unnecessary computations in extracting the correct answer. In addition, to further lower computations in $\text{A}^{2}\text{P}$ -MANN, we suggest pruning weights of the final fully connected (FC) layers. To this end, two pruning approaches, one with negligible accuracy loss and the other with controllable loss on the final accuracy, are developed. The efficacy of the technique is assessed by applying it to two different MANN structures and two question answering (QA) datasets. The analytical assessment reveals, for the two benchmarks, on average, 50\% fewer computations compared to the corresponding baseline MANNs at the cost of less than 1\% accuracy loss. In addition, when used along with the previously published zero-skipping technique, a computation count reduction of approximately 70\% is achieved. Finally, when the proposed approach (without zero skipping) is implemented on the CPU and GPU platforms, on average, a runtime reduction of 43\% is achieved.},
  archive      = {J_TNNLS},
  author       = {Mohsen Ahmadzadeh and Mehdi Kamal and Ali Afzali-Kusha and Massoud Pedram},
  doi          = {10.1109/TNNLS.2022.3148818},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8284-8296},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A2P-MANN: Adaptive attention inference hops pruned memory-augmented neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning poisson systems and trajectories of autonomous
systems via poisson neural networks. <em>TNNLS</em>, <em>34</em>(11),
8271–8283. (<a
href="https://doi.org/10.1109/TNNLS.2022.3148734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the Poisson neural networks (PNNs) to learn Poisson systems and trajectories of autonomous systems from data. Based on the Darboux–Lie theorem, the phase flow of a Poisson system can be written as the composition of: 1) a coordinate transformation; 2) an extended symplectic map; and 3) the inverse of the transformation. In this work, we extend this result to the unknotted trajectories of autonomous systems. We employ structured neural networks with physical priors to approximate the three aforementioned maps. We demonstrate through several simulations that PNNs are capable of handling very accurately several challenging tasks, including the motion of a particle in the electromagnetic potential, the nonlinear Schrödinger equation, and pixel observations of the two-body problem.},
  archive      = {J_TNNLS},
  author       = {Pengzhan Jin and Zhen Zhang and Ioannis G. Kevrekidis and George Em Karniadakis},
  doi          = {10.1109/TNNLS.2022.3148734},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8271-8283},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning poisson systems and trajectories of autonomous systems via poisson neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven adaptive iterative learning bipartite consensus
for heterogeneous nonlinear cooperation–antagonism networks.
<em>TNNLS</em>, <em>34</em>(11), 8262–8270. (<a
href="https://doi.org/10.1109/TNNLS.2022.3148726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous dynamics, strongly nonlinear and nonaffine structures, and cooperation–antagonism networks are considered together in this work, which have been considered as challenging problems in the output consensus of multiagent systems. A heterogeneous linear data model (LDM) is presented to accommodate the nonlinear nonaffine structure of the heterogeneous agent. It also builds an I/O dynamic relationship of the agents along the iteration-dimensional direction to make it possible to learn control experience from previous iterations to improve the transient consensus performance. Then, an adaptive update algorithm is developed for the estimation of the uncertain parameters of the LDM to compensate for the unknown heterogeneous dynamics and model structures. To address the problem of cooperation and antagonism, an adaptive learning consensus protocol is proposed considering two signed graphs, which are structurally balanced and unbalanced, respectively. The learning gain can be regulated using the proposed adaptive updating law to enhance the adaptability to the uncertainties. With rigorous analysis, the bipartite consensus is proven in the case that the graph is structurally balanced, and the convergence of the agent output to zero is also proven in the case that the graph is unbalanced in its structure. The presented bipartite consensus method is data-based without the use of any explicit model information. The theoretical results are demonstrated through simulations.},
  archive      = {J_TNNLS},
  author       = {Yu Hui and Ronghu Chi and Biao Huang and Zhongsheng Hou},
  doi          = {10.1109/TNNLS.2022.3148726},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8262-8270},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven adaptive iterative learning bipartite consensus for heterogeneous nonlinear Cooperation–Antagonism networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bounded antisynchronization of multiple neural networks via
multilevel hybrid control. <em>TNNLS</em>, <em>34</em>(11), 8250–8261.
(<a href="https://doi.org/10.1109/TNNLS.2022.3148194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bounded antisynchronization (AS) problem of multiple discrete-time neural networks (NNs) based on the fuzzy model is studied, in consideration of the differences in quantity and communication among different NN groups, the variabilities of dynamics, and communication topological affected by environments. To reduce the energy consumption of communication, a cluster pinning communication mechanism is proposed, and an impulsive observer is designed to estimate the state of target NN. Then, a multilevel hybrid controller based on the impulsive observer is built including the AS controller and the bounded synchronization (BS) controller. Sufficient conditions for bounded AS are obtained by analyzing the stability of the BS augmented error (BSAE) and the AS augmented error (ASAE) based on the fuzzy-based Lyapunov functional (FBLF). Finally, a numerical example and an application example are given to verify the validity of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Fen Liu and Wei Meng and Deyin Yao},
  doi          = {10.1109/TNNLS.2022.3148194},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8250-8261},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bounded antisynchronization of multiple neural networks via multilevel hybrid control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention enhanced reinforcement learning for multi agent
cooperation. <em>TNNLS</em>, <em>34</em>(11), 8235–8249. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel method, called attention enhanced reinforcement learning (AERL), is proposed to address issues including complex interaction, limited communication range, and time-varying communication topology for multi agent cooperation. AERL includes a communication enhanced network (CEN), a graph spatiotemporal long short-term memory network (GST-LSTM), and parameters sharing multi-pseudo critic proximal policy optimization (PS-MPC-PPO). Specifically, CEN based on graph attention mechanism is designed to enlarge the agents’ communication range and to deal with complex interaction among the agents. GST-LSTM, which replaces the standard fully connected (FC) operator in LSTM with graph attention operator, is designed to capture the temporal dependence while maintaining the spatial structure learned by CEN. PS-MPC-PPO, which extends proximal policy optimization (PPO) in multi agent systems with parameters’ sharing to scale to environments with a large number of agents in training, is designed with multi-pseudo critics to mitigate the bias problem in training and accelerate the convergence process. Simulation results for three groups of representative scenarios including formation control, group containment, and predator–prey games demonstrate the effectiveness and robustness of AERL.},
  archive      = {J_TNNLS},
  author       = {Zhiqiang Pu and Huimu Wang and Zhen Liu and Jianqiang Yi and Shiguang Wu},
  doi          = {10.1109/TNNLS.2022.3146858},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8235-8249},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention enhanced reinforcement learning for multi agent cooperation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep graph translation. <em>TNNLS</em>, <em>34</em>(11),
8225–8234. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models for graphs have recently achieved great successes in modeling and generating graphs for studying networks in biology, engineering, and social sciences. However, they are typically unconditioned generative models that have no control over the target graphs given a source graph. In this article, we propose a novel graph-translation-generative-adversarial-nets (GT-GAN) model that transforms the source graphs into their target output graphs. GT-GAN consists of a graph translator equipped with innovative graph convolution and deconvolution layers to learn the translation mapping considering both global and local features. A new conditional graph discriminator is proposed to classify the target graphs by conditioning on source graphs while training. Extensive experiments on multiple synthetic and real-world datasets in the domain of cybernetworks, the Internet of Things, and neuroscience demonstrate that the proposed GT-GAN model significantly outperforms other baseline methods in terms of both effectiveness and scalability. For instance, GT-GAN outperforms the classical state-of-the-art (SOTA) methods in functional connectivity (FC) prediction of brain networks by at least 32.5\%.},
  archive      = {J_TNNLS},
  author       = {Xiaojie Guo and Lingfei Wu and Liang Zhao},
  doi          = {10.1109/TNNLS.2022.3144670},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8225-8234},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep graph translation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). MHSA-net: Multihead self-attention network for occluded
person re-identification. <em>TNNLS</em>, <em>34</em>(11), 8210–8224.
(<a href="https://doi.org/10.1109/TNNLS.2022.3144163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel person reidentification model, named multihead self-attention network (MHSA-Net), to prune unimportant information and capture key local information from person images. MHSA-Net contains two main novel components: multihead self-attention branch (MHSAB) and attention competition mechanism (ACM). The MHSAB adaptively captures key local person information and then produces effective diversity embeddings of an image for the person matching. The ACM further helps filter out attention noise and nonkey information. Through extensive ablation studies, we verified that the MHSAB and ACM both contribute to the performance improvement of the MHSA-Net. Our MHSA-Net achieves competitive performance in the standard and occluded person Re-ID tasks.},
  archive      = {J_TNNLS},
  author       = {Hongchen Tan and Xiuping Liu and Baocai Yin and Xin Li},
  doi          = {10.1109/TNNLS.2022.3144163},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8210-8224},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MHSA-net: Multihead self-attention network for occluded person re-identification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ZeRGAN: Zero-reference GAN for fusion of multispectral and
panchromatic images. <em>TNNLS</em>, <em>34</em>(11), 8195–8209. (<a
href="https://doi.org/10.1109/TNNLS.2021.3137373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a new pansharpening method, a zero-reference generative adversarial network (ZeRGAN), which fuses low spatial resolution multispectral (LR MS) and high spatial resolution panchromatic (PAN) images. In the proposed method, zero-reference indicates that it does not require paired reduced-scale images or unpaired full-scale images for training. To obtain accurate fusion results, we establish an adversarial game between a set of multiscale generators and their corresponding discriminators. Through multiscale generators, the fused high spatial resolution MS (HR MS) images are progressively produced from LR MS and PAN images, while the discriminators aim to distinguish the differences of spatial information between the HR MS images and the PAN images. In other words, the HR MS images are generated from LR MS and PAN images after the optimization of ZeRGAN. Furthermore, we construct a nonreference loss function, including an adversarial loss, spatial and spectral reconstruction losses, a spatial enhancement loss, and an average constancy loss. Through the minimization of the total loss, the spatial details in the HR MS images can be enhanced efficiently. Extensive experiments are implemented on datasets acquired by different satellites. The results demonstrate that the effectiveness of the proposed method compared with the state-of-the-art methods. The source code is publicly available at https://github.com/RSMagneto/ZeRGAN .},
  archive      = {J_TNNLS},
  author       = {Wenxiu Diao and Feng Zhang and Jiande Sun and Yinghui Xing and Kai Zhang and Lorenzo Bruzzone},
  doi          = {10.1109/TNNLS.2021.3137373},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8195-8209},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ZeRGAN: Zero-reference GAN for fusion of multispectral and panchromatic images},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based semi-supervised learning: A comprehensive
review. <em>TNNLS</em>, <em>34</em>(11), 8174–8194. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) has tremendous value in practice due to the utilization of both labeled and unlabelled data. An essential class of SSL methods, referred to as graph-based semi-supervised learning (GSSL) methods in the literature, is to first represent each sample as a node in an affinity graph, and then, the label information of unlabeled samples can be inferred based on the structure of the constructed graph. GSSL methods have demonstrated their advantages in various domains due to their uniqueness of structure, the universality of applications, and their scalability to large-scale data. Focusing on GSSL methods only, this work aims to provide both researchers and practitioners with a solid and systematic understanding of relevant advances as well as the underlying connections among them. The concentration on one class of SSL makes this article distinct from recent surveys that cover a more general and broader picture of SSL methods yet often neglect the fundamental understanding of GSSL methods. In particular, a significant contribution of this article lies in a newly generalized taxonomy for GSSL under the unified framework, with the most up-to-date references and valuable resources such as codes, datasets, and applications. Furthermore, we present several potential research directions as future work with our insights into this rapidly growing field.},
  archive      = {J_TNNLS},
  author       = {Zixing Song and Xiangli Yang and Zenglin Xu and Irwin King},
  doi          = {10.1109/TNNLS.2022.3155478},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8174-8194},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-based semi-supervised learning: A comprehensive review},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of hardware self-organizing maps. <em>TNNLS</em>,
<em>34</em>(11), 8154–8173. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-organizing feature maps (SOMs) are commonly used technique for clustering and data dimensionality reduction in many application fields. Indeed, their inherent property of topology preservation and unsupervised learning of processed data without any prior knowledge put them in the front of candidates for data reduction in the Internet of Things (IoT) and big data (BD) technologies. However, the high computational cost of SOMs limits their use to offline approaches and makes the online real-time high-performance SOM processing more challenging and mostly reserved to specific hardware implementations. In this article, we present a survey of hardware (HW) SOM implementations found in the literature so far: the most widely used computing blocks, architectures, design choices, adaptation, and optimization techniques that have been reported in the field of hardware SOMs. Moreover, we give an overview of main challenges and trends for their ubiquitous adoption as hardware accelerators in many application fields. This article is expected to be useful for researchers in the areas of artificial intelligence, hardware architecture, and system design.},
  archive      = {J_TNNLS},
  author       = {Slaviša Jovanović and Hiroomi Hikawa},
  doi          = {10.1109/TNNLS.2022.3152690},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8154-8173},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of hardware self-organizing maps},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from noisy labels with deep neural networks: A
survey. <em>TNNLS</em>, <em>34</em>(11), 8135–8153. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into five groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies.},
  archive      = {J_TNNLS},
  author       = {Hwanjun Song and Minseok Kim and Dongmin Park and Yooju Shin and Jae-Gil Lee},
  doi          = {10.1109/TNNLS.2022.3152527},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {8135-8153},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning from noisy labels with deep neural networks: A survey},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive exponential synchronization of complex networks
with nondifferentiable time-varying delay. <em>TNNLS</em>,
<em>34</em>(10), 8124–8130. (<a
href="https://doi.org/10.1109/TNNLS.2022.3145843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the adaptive exponential synchronization (AES) problem of delayed complex networks has been extensively studied. Existing results rely heavily on assuming the differentiability of the time-varying delay, which is not easy to verify in reality. Dealing with nondifferentiable delay in the field of AES is still a challenging problem. In this brief, the AES problem of complex networks with general time-varying delay is addressed, especially when the delay is nondifferentiable. A delay differential inequality is proposed to deal with the exponential stability of delayed nonlinear systems, which is more general than the widely used Halanay inequality. Next, the boundedness of the adaptive control gain is theoretically proved, which is neglected in much of the literature. Then, the AES criteria for networks with general delay are established for the first time by using the proposed inequality and the boundedness of the control gain. Finally, an example is given to demonstrate the effectiveness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Shuaibing Zhu and Jin Zhou and Quanxin Zhu and Na Li and Jun-An Lu},
  doi          = {10.1109/TNNLS.2022.3145843},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8124-8130},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive exponential synchronization of complex networks with nondifferentiable time-varying delay},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural-network-based adaptive finite-time output feedback
control for spacecraft attitude tracking. <em>TNNLS</em>,
<em>34</em>(10), 8116–8123. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief is concerned with neural network (NN)-based adaptive finite-time output feedback attitude tracking control for rigid spacecraft in the presence of actuator saturation, inertial uncertainty, and external disturbance. First, a neural state observer is designed to estimate the unknown state. Then, based on the estimated state, the adaptive neural finite-time command filtered backstepping (CFB) is applied to construct virtual control signal and controller with updating law. The finite-time command filter is given to avoid the computation complexity problem in traditional backstepping, and the compensation signals based on fractional power are constructed to remove filtering errors. Using Lyapunov stability theory, we show that the attitude tracking error (TE) can converge into the desired neighborhood of the origin in finite time and all the signals in the closed-loop system are bounded in finite time although input saturation exists. The numerical simulations are used to show the effectiveness of the given algorithm.},
  archive      = {J_TNNLS},
  author       = {Lin Zhao and Jinpeng Yu and Xinkai Chen},
  doi          = {10.1109/TNNLS.2022.3144493},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8116-8123},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based adaptive finite-time output feedback control for spacecraft attitude tracking},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ε-approximation of adaptive leaning rate optimization
algorithms for constrained nonconvex stochastic optimization.
<em>TNNLS</em>, <em>34</em>(10), 8108–8115. (<a
href="https://doi.org/10.1109/TNNLS.2022.3142726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief considers constrained nonconvex stochastic finite-sum and online optimization in deep neural networks. Adaptive-learning-rate optimization algorithms (ALROAs), such as Adam, AMSGrad, and their variants, have widely been used for these optimizations because they are powerful and useful in theory and practice. Here, it is shown that the ALROAs are $\epsilon $ -approximations for these optimizations. We provide the learning rates, mini-batch sizes, number of iterations, and stochastic gradient complexity with which to achieve $\epsilon $ -approximations of the algorithms.},
  archive      = {J_TNNLS},
  author       = {Hideaki Iiduka},
  doi          = {10.1109/TNNLS.2022.3142726},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8108-8115},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ϵ-approximation of adaptive leaning rate optimization algorithms for constrained nonconvex stochastic optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive neural asymptotic tracking of uncertain non-strict
feedback systems with full-state constraints via command filtered
technique. <em>TNNLS</em>, <em>34</em>(10), 8102–8107. (<a
href="https://doi.org/10.1109/TNNLS.2022.3141091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief addresses the adaptive neural asymptotic tracking issue for uncertain non-strict feedback systems subject to full-state constraints. By introducing the significant nonlinear transformed function (NTF), the command filtered technology, and the boundary estimation method into control design, a novel command filtered backstepping adaptive controller is proposed. The proposed control scheme is able to not only deal with full-state constraints but also avoid the “explosion of complexity” issue. By means of a Lyapunov stability analysis, we prove that: 1) the tracking error asymptotically converges to zero; 2) all the variables in the controlled systems are bounded; and 3) all the states are constrained in the asymmetric predefined sets. Finally, a numerical simulation is used to demonstrate the validity of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Chun Xin and Yuan-Xin Li and Choon Ki Ahn},
  doi          = {10.1109/TNNLS.2022.3141091},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8102-8107},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural asymptotic tracking of uncertain non-strict feedback systems with full-state constraints via command filtered technique},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty quantification in estimating blood alcohol
concentration from transdermal alcohol level with physics-informed
neural networks. <em>TNNLS</em>, <em>34</em>(10), 8094–8101. (<a
href="https://doi.org/10.1109/TNNLS.2022.3140726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an approach to estimate a blood alcohol signal from a transdermal alcohol signal using physics-informed neural networks (PINNs). Specifically, we use a generative adversarial network (GAN) with a residual-augmented loss function to estimate the distribution of unknown parameters in a diffusion equation model for transdermal transport of alcohol in the human body. We design another PINN for the deconvolution of the blood alcohol signal from the transdermal alcohol signal. Based on the distribution of the unknown parameters, this network is able to estimate the blood alcohol signal and quantify the uncertainty in the form of conservative error bands. Finally, we show how a posterior latent variable can be used to sharpen these conservative error bands. We apply the techniques to an extensive dataset of drinking episodes and demonstrate the advantages and shortcomings of this approach.},
  archive      = {J_TNNLS},
  author       = {Clemens Oszkinat and Susan E. Luczak and I. G. Rosen},
  doi          = {10.1109/TNNLS.2022.3140726},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8094-8101},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Uncertainty quantification in estimating blood alcohol concentration from transdermal alcohol level with physics-informed neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-punishment and reward backfill for deep q-learning.
<em>TNNLS</em>, <em>34</em>(10), 8086–8093. (<a
href="https://doi.org/10.1109/TNNLS.2021.3140042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) agents learn by encouraging behaviors, which maximizes their total reward, usually provided by the environment. In many environments, however, the reward is provided after a series of actions rather than each single action, leading the agent to experience ambiguity in terms of whether those actions are effective, an issue known as the credit assignment problem. In this brief, we propose two strategies inspired by behavioral psychology to enable the agent to intrinsically estimate more informative reward values for actions with no reward. The first strategy, called self-punishment (SP), discourages the agent from making mistakes that lead to undesirable terminal states. The second strategy, called the reward backfill (RB), backpropagates the rewards between two rewarded actions. We prove that, under certain assumptions and regardless of the RL algorithm used, these two strategies maintain the order of policies in the space of all possible policies in terms of their total reward and, by extension, maintain the optimal policy. Hence, our proposed strategies integrate with any RL algorithm that learns a value or action-value function through experience. We incorporated these two strategies into three popular deep RL approaches and evaluated the results on 30 Atari games. After parameter tuning, our results indicate that the proposed strategies improve the tested methods in over 65\% of tested games by up to over 25 times performance improvement.},
  archive      = {J_TNNLS},
  author       = {Mohammad Reza Bonyadi and Rui Wang and Maryam Ziaei},
  doi          = {10.1109/TNNLS.2021.3140042},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8086-8093},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-punishment and reward backfill for deep Q-learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical graph convolutional networks for structured
long document classification. <em>TNNLS</em>, <em>34</em>(10),
8071–8085. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long document classification (LDC) has been a focused interest in natural language processing (NLP) recently with the exponential increase of publications. Based on the pretrained language models, many LDC methods have been proposed and achieved considerable progression. However, most of the existing methods model long documents as sequences of text while omitting the document structure, thus limiting the capability of effectively representing long texts carrying structure information. To mitigate such limitation, we propose a novel hierarchical graph convolutional network (HGCN) for structured LDC in this article, in which a section graph network is proposed to model the macrostructure of a document and a word graph network with a decoupled graph convolutional block is designed to extract the fine-grained features of a document. In addition, an interaction strategy is proposed to integrate these two networks as a whole by propagating features between them. To verify the effectiveness of the proposed model, four structured long document datasets are constructed, and the extensive experiments conducted on these datasets and another unstructured dataset show that the proposed method outperforms the state-of-the-art related classification methods.},
  archive      = {J_TNNLS},
  author       = {Tengfei Liu and Yongli Hu and Boyue Wang and Yanfeng Sun and Junbin Gao and Baocai Yin},
  doi          = {10.1109/TNNLS.2022.3185295},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8071-8085},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical graph convolutional networks for structured long document classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymmetric feature fusion network for hyperspectral and SAR
image classification. <em>TNNLS</em>, <em>34</em>(10), 8057–8070. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint classification using multisource remote sensing data for Earth observation is promising but challenging. Due to the gap of imaging mechanism and imbalanced information between multisource data, integrating the complementary merits for interpretation is still full of difficulties. In this article, a classification method based on asymmetric feature fusion, named asymmetric feature fusion network (AsyFFNet), is proposed. First, the weight-share residual blocks are utilized for feature extraction while keeping separate batch normalization (BN) layers. In the training phase, redundancy of the current channel is self-determined by the scaling factors in BN, which is replaced by another channel when the scaling factor is less than a threshold. To eliminate unnecessary channels and improve the generalization, a sparse constraint is imposed on partial scaling factors. Besides, a feature calibration module is designed to exploit the spatial dependence of multisource features, so that the discrimination capability is enhanced. Experimental results on the three datasets demonstrate that the proposed AsyFFNet significantly outperforms other competitive approaches.},
  archive      = {J_TNNLS},
  author       = {Wei Li and Yunhao Gao and Mengmeng Zhang and Ran Tao and Qian Du},
  doi          = {10.1109/TNNLS.2022.3149394},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8057-8070},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asymmetric feature fusion network for hyperspectral and SAR image classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Filter pruning by switching to neighboring CNNs with good
attributes. <em>TNNLS</em>, <em>34</em>(10), 8044–8056. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Filter pruning is effective to reduce the computational costs of neural networks. Existing methods show that updating the previous pruned filter would enable large model capacity and achieve better performance. However, during the iterative pruning process, even if the network weights are updated to new values, the pruning criterion remains the same. In addition, when evaluating the filter importance, only the magnitude information of the filters is considered. However, in neural networks, filters do not work individually, but they would affect other filters. As a result, the magnitude information of each filter, which merely reflects the information of an individual filter itself, is not enough to judge the filter importance. To solve the above problems, we propose meta-attribute-based filter pruning (MFP). First, to expand the existing magnitude information-based pruning criteria, we introduce a new set of criteria to consider the geometric distance of filters. Additionally, to explicitly assess the current state of the network, we adaptively select the most suitable criteria for pruning via a meta-attribute, a property of the neural network at the current state. Experiments on two image classification benchmarks validate our method. For ResNet-50 on ILSVRC-2012, we could reduce more than 50\% FLOPs with only 0.44\% top-5 accuracy loss.},
  archive      = {J_TNNLS},
  author       = {Yang He and Ping Liu and Linchao Zhu and Yi Yang},
  doi          = {10.1109/TNNLS.2022.3149332},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8044-8056},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Filter pruning by switching to neighboring CNNs with good attributes},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resource-aware time series imaging classification for
wireless link layer anomalies. <em>TNNLS</em>, <em>34</em>(10),
8031–8043. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of end devices that use the last-mile wireless connectivity is dramatically increasing with the rise of smart infrastructures and requires reliable functioning to support smooth and efficient business processes. To efficiently manage such massive wireless networks, more advanced and accurate network monitoring and malfunction detection solutions are required. In this article, we perform a first-time analysis of image-based representation techniques for wireless anomaly detection using recurrence plots (RPs) and Gramian angular fields and propose a new deep learning architecture enabling accurate anomaly detection. We elaborate on the design considerations for developing a resource-aware architecture and propose a new model using time series to image transformation using RPs. We show that the proposed model: 1) outperforms the one based on Gramian angular fields by up to 14\% points; 2) outperforms classical ML models using dynamic time warping by up to 24\% points; 3) outperforms or performs on par with mainstream architectures, such as AlexNet and VGG11 while having $ &amp;lt; 10\times $ their weights and up to ≈8\% of their computational complexity; and d) outperforms the state of the art in the respective application area by up to 55\% points. Finally, we also explain on randomly chosen examples how the classifier takes decisions.},
  archive      = {J_TNNLS},
  author       = {Blaž Bertalanič and Marko Meža and Carolina Fortuna},
  doi          = {10.1109/TNNLS.2022.3149091},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8031-8043},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Resource-aware time series imaging classification for wireless link layer anomalies},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning-based DoS attack power allocation in multiprocess
systems. <em>TNNLS</em>, <em>34</em>(10), 8017–8030. (<a
href="https://doi.org/10.1109/TNNLS.2022.3148924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the denial-of-service (DoS) attack power allocation optimization in a multiprocess cyber–physical system (CPS), where sensors observe different dynamic processes and send the local estimated states to a remote estimator through wireless channels, while a DoS attacker allocates its attack power on different channels as interference to reduce the wireless transmission rates, and thus degrading the estimation accuracy of the remote estimator. We consider two attack optimization problems. One is to maximize the average estimation error of different processes, and the other is to maximize the minimal one. We formulate these problems as Markov decision processes (MDPs). Unlike the majority of existing works where the attacker is assumed to have complete knowledge of the CPS, we consider an attacker with no prior knowledge of the wireless channel model and the sensor information. To address this uncertainty issue and the curse of dimensionality, we provide a learning-based attack power allocation algorithm stemming from the double deep Q-network (DDQN) method. First, with a defined partial order, the maximal elements of the action space are determined. By investigating the characteristic of the MDP, we prove that the optimal attack allocations of both problems belong to the set of these elements. This property reduces the entire action space to a smaller subset and speeds up the learning algorithm. In addition, to further improve the data efficiency and learning performance, we propose two enhanced attack power allocation algorithms which add two auxiliary tasks of MDP transition estimation inspired by model-based reinforcement learning, i.e., the next state prediction and the current action estimation. Experimental results demonstrate the versatility and efficiency of the proposed algorithms in different system settings compared with other algorithms, such as the conventional value iteration, double Q-learning, and deep Q-network.},
  archive      = {J_TNNLS},
  author       = {Mengyu Huang and Kemi Ding and Subhrakanti Dey and Yuzhe Li and Ling Shi},
  doi          = {10.1109/TNNLS.2022.3148924},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8017-8030},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning-based DoS attack power allocation in multiprocess systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Fully distributed event-driven adaptive consensus of
unknown linear systems. <em>TNNLS</em>, <em>34</em>(10), 8007–8016. (<a
href="https://doi.org/10.1109/TNNLS.2022.3148824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the consensus problem of unknown linear multiagent systems (MASs) through adaptive event-driven control in leader–follower and leaderless networks. The proposed event-driven algorithms do not involve any global information related to the network communication structure and rely only on local information exchange to achieve consensus on MASs and are therefore fully distributed. Furthermore, the constraint of continuous communication among the agents is eliminated in terms of control law updates and triggering state monitoring. Another desirable aspect of this article is that the design process of the control algorithms is independent of the parameters of each agent’s dynamics and thus does not require precise information about the dynamics of MASs. We further exclude the Zeno behavior of each agent by proving the existence of a strict positive lower bound between any two adjacent events. Finally, the effectiveness of the proposed adaptive event-driven algorithms is verified by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Shu Liu and Jiayue Sun and Huaguang Zhang and Meina Zhai},
  doi          = {10.1109/TNNLS.2022.3148824},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {8007-8016},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully distributed event-driven adaptive consensus of unknown linear systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recurrent neural networks are universal approximators with
stochastic inputs. <em>TNNLS</em>, <em>34</em>(10), 7992–8006. (<a
href="https://doi.org/10.1109/TNNLS.2022.3148542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the approximation ability of recurrent neural networks (RNNs) with stochastic inputs in state space model form. More explicitly, we prove that open dynamical systems with stochastic inputs can be well-approximated by a special class of RNNs under some natural assumptions, and the asymptotic approximation error has also been delicately analyzed as time goes to infinity. In addition, as an important application of this result, we construct an RNN-based filter and prove that it can well-approximate finite dimensional filters which include Kalman filter (KF) and Beneš filter as special cases. The efficiency of RNN-based filter has also been verified by two numerical experiments compared with optimal KF.},
  archive      = {J_TNNLS},
  author       = {Xiuqiong Chen and Yangtianze Tao and Wenjie Xu and Stephen Shing-Toung Yau},
  doi          = {10.1109/TNNLS.2022.3148542},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7992-8006},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Recurrent neural networks are universal approximators with stochastic inputs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Meta-learning-based deep reinforcement learning for
multiobjective optimization problems. <em>TNNLS</em>, <em>34</em>(10),
7978–7991. (<a
href="https://doi.org/10.1109/TNNLS.2022.3148435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) has recently shown its success in tackling complex combinatorial optimization problems. When these problems are extended to multiobjective ones, it becomes difficult for the existing DRL approaches to flexibly and efficiently deal with multiple subproblems determined by the weight decomposition of objectives. This article proposes a concise meta-learning-based DRL approach. It first trains a meta-model by meta-learning. The meta-model is fine-tuned with a few update steps to derive submodels for the corresponding subproblems. The Pareto front is then built accordingly. Compared with other learning-based methods, our method can greatly shorten the training time of multiple submodels. Due to the rapid and excellent adaptability of the meta-model, more submodels can be derived so as to increase the quality and diversity of the found solutions. The computational experiments on multiobjective traveling salesman problems and multiobjective vehicle routing problems with time windows demonstrate the superiority of our method over most of the learning-based and iteration-based approaches.},
  archive      = {J_TNNLS},
  author       = {Zizhen Zhang and Zhiyuan Wu and Hang Zhang and Jiahai Wang},
  doi          = {10.1109/TNNLS.2022.3148435},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7978-7991},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Meta-learning-based deep reinforcement learning for multiobjective optimization problems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pinning spatiotemporal sampled-data synchronization of
coupled reaction–diffusion neural networks under deception attacks.
<em>TNNLS</em>, <em>34</em>(10), 7967–7977. (<a
href="https://doi.org/10.1109/TNNLS.2022.3148184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the pinning spatiotemporal sampled-data (SD) synchronization of coupled reaction–diffusion neural networks (CRDNNs), which are directed networks with SD in time and space communications under random deception attacks. In order to handle with the random deception attacks, we establish a directed CRDNN model, which respects the impacts of variable sampling and random deception attacks within a unified framework. Through the designed pinning spatiotemporal SD controller, sufficient conditions are obtained by linear matrix inequalities (LMIs) that guarantee the mean square exponential stability of the synchronization error system (SES) derived by utilizing inequality techniques, the stochastic analysis technique, and Lyapunov–Krasovskii functional (LKF). Finally, a numerical example is utilized to support the presented pinning spatiotemporal SD synchronization method.},
  archive      = {J_TNNLS},
  author       = {Zi-Peng Wang and Qian-Qian Li and Huai-Ning Wu and Biao Luo and Tingwen Huang},
  doi          = {10.1109/TNNLS.2022.3148184},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7967-7977},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pinning spatiotemporal sampled-data synchronization of coupled Reaction–Diffusion neural networks under deception attacks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning disentangled representation for multimodal
cross-domain sentiment analysis. <em>TNNLS</em>, <em>34</em>(10),
7956–7966. (<a
href="https://doi.org/10.1109/TNNLS.2022.3147546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal cross-domain sentiment analysis aims at transferring domain-invariant sentiment information across datasets to address the insufficiency of labeled data. Existing adaptation methods achieve well performance by remitting the discrepancies in characteristics of multiple modalities. However, the expressive styles of different datasets also contain domain-specific information, which hinders the adaptation performance. In this article, we propose a disentangled sentiment representation adversarial network (DiSRAN) to reduce the domain shift of expressive styles for multimodal cross-domain sentiment analysis. Specifically, we first align the multiple modalities and obtain the joint representation through a cross-modality attention layer. Then, we disentangle sentiment information from the multimodal joint representation that contains domain-specific expressive style by adversarial training. The obtained sentiment representation is domain-invariant, which can better facilitate the sentiment information transfer between different domains. Experimental results on two multimodal cross-domain sentiment analysis tasks demonstrate that the proposed method performs favorably against state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Yuhao Zhang and Ying Zhang and Wenya Guo and Xiangrui Cai and Xiaojie Yuan},
  doi          = {10.1109/TNNLS.2022.3147546},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7956-7966},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning disentangled representation for multimodal cross-domain sentiment analysis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Carrying out CNN channel pruning in a white box.
<em>TNNLS</em>, <em>34</em>(10), 7946–7955. (<a
href="https://doi.org/10.1109/TNNLS.2022.3147269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel pruning has been long studied to compress convolutional neural networks (CNNs), which significantly reduces the overall computation. Prior works implement channel pruning in an unexplainable manner, which tends to reduce the final classification errors while failing to consider the internal influence of each channel. In this article, we conduct channel pruning in a white box. Through deep visualization of feature maps activated by different channels, we observe that different channels have a varying contribution to different categories in image classification. Inspired by this, we choose to preserve channels contributing to most categories. Specifically, to model the contribution of each channel to differentiating categories, we develop a class-wise mask for each channel, implemented in a dynamic training manner with respect to the input image’s category. On the basis of the learned class-wise mask, we perform a global voting mechanism to remove channels with less category discrimination. Lastly, a fine-tuning process is conducted to recover the performance of the pruned model. To our best knowledge, it is the first time that CNN interpretability theory is considered to guide channel pruning. Extensive experiments on representative image classification tasks demonstrate the superiority of our White-Box over many state-of-the-arts (SOTAs). For instance, on CIFAR-10, it reduces 65.23\% floating point operations per seconds (FLOPs) with even 0.62\% accuracy improvement for ResNet-110. On ILSVRC-2012, White-Box achieves a 45.6\% FLOP reduction with only a small loss of 0.83\% in the top-1 accuracy for ResNet-50. Code is available at https://github.com/zyxxmu/White-Box .},
  archive      = {J_TNNLS},
  author       = {Yuxin Zhang and Mingbao Lin and Chia-Wen Lin and Jie Chen and Yongjian Wu and Yonghong Tian and Rongrong Ji},
  doi          = {10.1109/TNNLS.2022.3147269},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7946-7955},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Carrying out CNN channel pruning in a white box},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based self-advising for multi-agent learning.
<em>TNNLS</em>, <em>34</em>(10), 7934–7945. (<a
href="https://doi.org/10.1109/TNNLS.2022.3147221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiagent learning, one of the main ways to improve learning performance is to ask for advice from another agent. Contemporary advising methods share a common limitation that a teacher agent can only advise a student agent if the teacher has experience with an identical state. However, in highly complex learning scenarios, such as autonomous driving, it is rare for two agents to experience exactly the same state, which makes the advice less of a learning aid and more of a one-time instruction. In these scenarios, with contemporary methods, agents do not really help each other learn, and the main outcome of their back and forth requests for advice is an exorbitant communications’ overhead. In human interactions, teachers are often asked for advice on what to do in situations that students are personally unfamiliar with. In these, we generally draw from similar experiences to formulate advice. This inspired us to provide agents with the same ability when asked for advice on an unfamiliar state. Hence, we propose a model-based self-advising method that allows agents to train a model based on states similar to the state in question to inform its response. As a result, the advice given can not only be used to resolve the current dilemma but also many other similar situations that the student may come across in the future via self-advising. Compared with contemporary methods, our method brings a significant improvement in learning performance with much lower communication overheads.},
  archive      = {J_TNNLS},
  author       = {Dayong Ye and Tianqing Zhu and Congcong Zhu and Wanlei Zhou and Philip S. Yu},
  doi          = {10.1109/TNNLS.2022.3147221},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7934-7945},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-based self-advising for multi-agent learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG-based cross-subject driver drowsiness recognition with
an interpretable convolutional neural network. <em>TNNLS</em>,
<em>34</em>(10), 7921–7933. (<a
href="https://doi.org/10.1109/TNNLS.2022.3147208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of electroencephalogram (EEG)-based driver drowsiness recognition, it is still challenging to design a calibration-free system, since EEG signals vary significantly among different subjects and recording sessions. Many efforts have been made to use deep learning methods for mental state recognition from EEG signals. However, existing work mostly treats deep learning models as black-box classifiers, while what have been learned by the models and to which extent they are affected by the noise in EEG data are still underexplored. In this article, we develop a novel convolutional neural network combined with an interpretation technique that allows sample-wise analysis of important features for classification. The network has a compact structure and takes advantage of separable convolutions to process the EEG signals in a spatial-temporal sequence. Results show that the model achieves an average accuracy of 78.35\% on 11 subjects for leave-one-out cross-subject drowsiness recognition, which is higher than the conventional baseline methods of 53.40\%–72.68\% and state-of-the-art deep learning methods of 71.75\%–75.19\%. Interpretation results indicate the model has learned to recognize biologically meaningful features from EEG signals, e.g., alpha spindles, as strong indicators of drowsiness across different subjects. In addition, we also explore reasons behind some wrongly classified samples with the interpretation technique and discuss potential ways to improve the recognition accuracy. Our work illustrates a promising direction on using interpretable deep learning models to discover meaningful patterns related to different mental states from complex EEG signals.},
  archive      = {J_TNNLS},
  author       = {Jian Cui and Zirui Lan and Olga Sourina and Wolfgang Müller-Wittig},
  doi          = {10.1109/TNNLS.2022.3147208},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7921-7933},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {EEG-based cross-subject driver drowsiness recognition with an interpretable convolutional neural network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards better generalization of deep neural networks via
non-typicality sampling scheme. <em>TNNLS</em>, <em>34</em>(10),
7910–7920. (<a
href="https://doi.org/10.1109/TNNLS.2022.3147031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the generalization performance of deep neural networks (DNNs) trained by minibatch stochastic gradient descent (SGD) has raised lots of concerns from deep learning practitioners. The standard simple random sampling (SRS) scheme used in minibatch SGD treats all training samples equally in gradient estimation. In this article, we study a new data selection method based on the intrinsic property of the training set to help DNNs have better generalization performance. Our theoretical analysis suggests that this new sampling scheme, called the nontypicality sampling scheme, boosts the generalization performance of DNNs through biasing the solution toward wider minima, under certain assumptions. We confirm our findings experimentally and show that more variants of minibatch SGD can also benefit from the new sampling scheme. Finally, we discuss an extension of the nontypicality sampling scheme that holds promise to enhance both generalization performance and convergence speed of minibatch SGD.},
  archive      = {J_TNNLS},
  author       = {Xinyu Peng and Fei-Yue Wang and Li Li},
  doi          = {10.1109/TNNLS.2022.3147031},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7910-7920},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Towards better generalization of deep neural networks via non-typicality sampling scheme},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Game of drones: Multi-UAV pursuit-evasion game with online
motion planning by deep reinforcement learning. <em>TNNLS</em>,
<em>34</em>(10), 7900–7909. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the tiniest flying objects, unmanned aerial vehicles (UAVs) are often expanded as the “swarm” to execute missions. In this article, we investigate the multiquadcopter and target pursuit-evasion game in the obstacles environment. For high-quality simulation of the urban environment, we propose the pursuit-evasion scenario (PES) framework to create the environment with a physics engine, which enables quadcopter agents to take actions and interact with the environment. On this basis, we construct multiagent coronal bidirectionally coordinated with target prediction network (CBC-TP Net) with a vectorized extension of multiagent deep deterministic policy gradient (MADDPG) formulation to ensure the effectiveness of the damaged “swarm” system in pursuit-evasion mission. Unlike traditional reinforcement learning, we design a target prediction network (TP Net) innovatively in the common framework to imitate the way of human thinking: situation prediction is always before decision-making. The experiments of the pursuit-evasion game are conducted to verify the state-of-the-art performance of the proposed strategy, both in the normal and antidamaged situations.},
  archive      = {J_TNNLS},
  author       = {Ruilong Zhang and Qun Zong and Xiuyun Zhang and Liqian Dou and Bailing Tian},
  doi          = {10.1109/TNNLS.2022.3146976},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7900-7909},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Game of drones: Multi-UAV pursuit-evasion game with online motion planning by deep reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Music recommendation via hypergraph embedding.
<em>TNNLS</em>, <em>34</em>(10), 7887–7899. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, we have witnessed an ever wider spread of multimedia streaming platforms (e.g., Netflix, Spotify, and Amazon). Hence, it has become more and more essential to provide such systems with advanced recommendation facilities, in order to support users in browsing these massive collections of multimedia data according to their preferences and needs. In this context, the modeling of entities and their complex relationships (e.g., users listening to topic-based songs or authors creating different releases of their lyrics) represents the key challenge to improve the recommendation and maximize the users’ satisfaction. To this end, this is the first study to leverage the high representative power of hypergraph data structures in combination with modern graph machine learning techniques in the context of music recommendation. Specifically, we propose hypergraph embeddings for music recommendation (HEMR), a novel framework for song recommendation based on hypergraph embedding. The hypergraph data model allows us to represent seamlessly all the possible and complex interactions between users and songs with the related characteristics; meanwhile, embedding techniques provide a powerful way to infer the user–song similarities by vector mapping. We have experimented the effectiveness and efficiency of our approach with respect to the state-of-the-art most recent music recommender systems, exploiting the Million Song dataset. The results show that HEMR significantly outperforms other state-of-the-art techniques, especially in scenarios where the cold-start problem arises, thus making our system a suitable solution to embed within a music streaming platform.},
  archive      = {J_TNNLS},
  author       = {Valerio La Gatta and Vincenzo Moscato and Mirko Pennone and Marco Postiglione and Giancarlo Sperlí},
  doi          = {10.1109/TNNLS.2022.3146968},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7887-7899},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Music recommendation via hypergraph embedding},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive neural consensus for fractional-order multi-agent
systems with faults and delays. <em>TNNLS</em>, <em>34</em>(10),
7873–7886. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the consensus control for a class of fractional-order (FO) nonlinear multi-agent systems (MASs). Severe sensor/actuator faults and time-varying delays are both considered in the FO MASs. The severe faults may cause unknown control directions in MASs. A new adaptive controller, which is composed of a distributed FO Nussbaum gain, an FO filter, and an auxiliary function, is presented to deal with the severe faults. To cope with the time-varying delays, two different methods are proposed based on barrier Lyapunov function and Lyapunov–Krasovskii function, respectively. Meanwhile, the radial basis function neural network (RBF NN) is applied to approximate the unknown nonlinear functions during the design procedures. This can result in a low-complexity controller. Finally, two simulation examples are used to verify the validity of the proposed schemes.},
  archive      = {J_TNNLS},
  author       = {Xiongliang Zhang and Shiqi Zheng and Choon Ki Ahn and Yuanlong Xie},
  doi          = {10.1109/TNNLS.2022.3146889},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7873-7886},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural consensus for fractional-order multi-agent systems with faults and delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The optimization of control parameters: Finite-time
bipartite synchronization of memristive neural networks with multiple
time delays via saturation function. <em>TNNLS</em>, <em>34</em>(10),
7861–7872. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the memristive neural networks with multiple time delays (MNNsMTDs). The topology of networks is signed, which contains both cooperative and competitive relationships. Two controllers without time delays are designed to achieve finite-time bipartite synchronization (FTBS) and practical FTBS (PFTBS) of MNNsMTDs. A novel controller with a saturation function rather than a sign function is proposed to avoid chattering. Along with the Lyapunov function method, some mathematical techniques, and scaling inequalities, some sufficient conditions for FTBS and PFTBS of MNNsMTDs are attained. Besides, this article also concerns fixed-time bipartite synchronization (FXBS) and practical FXBS (PFXBS) of MNNsMTDs. An optimization model is designed to obtain some optimal control parameters. An algorithm based on particle swarm optimization (PSO) is provided to solve this model. Some numerical examples are included to demonstrate the correctness and applicability of the approaches.},
  archive      = {J_TNNLS},
  author       = {Qi Chang and Ju H. Park and Yongqing Yang},
  doi          = {10.1109/TNNLS.2022.3146832},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7861-7872},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The optimization of control parameters: Finite-time bipartite synchronization of memristive neural networks with multiple time delays via saturation function},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Fully distributed event/self-triggered bipartite output
formation-containment tracking control for heterogeneous multiagent
systems. <em>TNNLS</em>, <em>34</em>(10), 7851–7860. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the bipartite time-varying output formation-containment tracking control issue for general linear heterogeneous multiagent systems with multiple nonautonomous leaders, where the full states of agents are not available. Both cooperative interaction and antagonistic interaction between neighboring agents are taken into account. First, an observer is constructed using the output information to observe the state information. Then, based on the information between neighboring agents, an independent asynchronous fully distributed event-triggered bipartite compensator is put forward to estimate the convex hull spanned by the states of multiple leaders. Note that the compensator does not require to use of any global information. Subsequently, a formation-containment tracking control strategy based on the observer and compensator and an algorithm to determine its control parameters are given. The Zeno behavior is further proved to be excluded in any finite time. In addition, a novel self-triggered control strategy based only on the sampled information at triggering instants is also formulated, which avoids continuous communication among agents. Finally, a numerical example is given to validate the effectiveness and performance of the proposed control strategies.},
  archive      = {J_TNNLS},
  author       = {Weihua Li and Huaguang Zhang and Zhiyun Gao and Yingchun Wang and Jiayue Sun},
  doi          = {10.1109/TNNLS.2022.3146814},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7851-7860},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully distributed Event/Self-triggered bipartite output formation-containment tracking control for heterogeneous multiagent systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traffic signal control with adaptive online-learning scheme
using multiple-model neural networks. <em>TNNLS</em>, <em>34</em>(10),
7838–7850. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new traffic signal control algorithm to deal with unknown-traffic-system uncertainties and reduce delays in vehicle travel time. Unknown-traffic-system dynamics are approximated using a recurrent neural network (NN). To accurately identify the traffic system model, an online-learning scheme is developed to switch among a set of candidate NNs (i.e., multiple-model NNs) based on their estimation errors. Then, a bank of optimal signal-timing controllers is designed based on the online identification of the traffic system. Simulation studies have been carried out for the obtained control strategies using multiple-model NNs, and the desired results have been obtained. Moreover, compared with the widely used actuated traffic signal control schemes, it is shown that the proposed method can reduce vehicle travel delays and improve traffic system robustness.},
  archive      = {J_TNNLS},
  author       = {Wanshi Hong and Gang Tao and Hong Wang and Chieh Wang},
  doi          = {10.1109/TNNLS.2022.3146811},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7838-7850},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Traffic signal control with adaptive online-learning scheme using multiple-model neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and analysis of multiscroll memristive hopfield
neural network with adjustable memductance and application to image
encryption. <em>TNNLS</em>, <em>34</em>(10), 7824–7837. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memristor is an ideal electronic device used as an artificial nerve synapse due to its unique memory function. This article presents a design of a new Hopfield neural network (HNN) that can generate multiscroll attractors by utilizing a new memristor as a synapse in the HNN. Differing from the others, this memristor is constructed with hyperbolic tangent functions. Taking the memristor as a self-feedback synapse of a neuron in the HNN, the memristive HNN can yield multidouble-scroll attractors, and its parameters can be used to effectively control the number of double scrolls contained in an attractor. Interestingly, the generation of multidouble-scroll attractors is independent of the memductance function but depends only on the internal state equation. Thus, the memductance function can be adjusted to yield various complex dynamical behaviors. Moreover, amplitude control effects and quantitatively controllable multistability are revealed by numerical analysis. The accurate reproduction of some dynamical behaviors by a designed circuit verifies the correctness of the numerical analysis. Finally, based on the proposed memristive HNN, a novel image encryption scheme in the 3-D setting is designed and evaluated, demonstrating its good encryption performances.},
  archive      = {J_TNNLS},
  author       = {Qiang Lai and Zhiqiang Wan and Hui Zhang and Guanrong Chen},
  doi          = {10.1109/TNNLS.2022.3146570},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7824-7837},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design and analysis of multiscroll memristive hopfield neural network with adjustable memductance and application to image encryption},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conversation-based adaptive relational translation method
for next POI recommendation with uncertain check-ins. <em>TNNLS</em>,
<em>34</em>(10), 7810–7823. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uncertain check-ins bring challenges for current static next point-of-interest (POI) recommendation methods. Fortunately, the conversation-based recommendation has been shown the merit of integrating immediate user preference for more accurate recommendations. We, therefore, propose a conversation-based adaptive relational translation (CART) approach for the next POI recommendation over uncertain check-ins. It is equipped with recommender and conversation modules to interactively acquire users’ immediate preferences and make dynamic recommendations. Specifically, the recommender built upon the adaptive relational translation method performs location prediction via modeling both users’ historical sequential behaviors and the immediate preference received from conversations; the conversation module aims to achieve successful recommendations in fewer conversation turns by learning a conversational strategy, whereby the recommender can be updated via the user response. Extensive experiments on four real-world datasets show the superiority of our proposed CART over the state of the arts.},
  archive      = {J_TNNLS},
  author       = {Lu Zhang and Zhu Sun and Jie Zhang and Yiwen Wu and Yunwen Xia},
  doi          = {10.1109/TNNLS.2022.3146443},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7810-7823},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Conversation-based adaptive relational translation method for next POI recommendation with uncertain check-ins},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking training strategy in stereo matching.
<em>TNNLS</em>, <em>34</em>(10), 7796–7809. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In stereo matching, various learning-based approaches have shown impressive performance in solving traditional difficulties on multiple datasets. While most progress is obtained on a specific dataset with a dataset-specific network design, the performance on the single dataset and cross dataset affected by training strategy is often ignored. In this article, we analyze the relationship between different training strategies and performance by retraining some representative state-of-the-art methods (e.g., geometry and context network (GC-Net), pyramid stereo matching network (PSM-Net), and guided aggregation network (GA-Net), etc.). According to our research, it is surprising that the performance of networks on single or cross datasets is significantly improved by pre-training and data augmentation without any particular structure acquirement. Based on this discovery, we improve our previous non-local context attention network (NLCA-Net) to NLCA-Net v2 and train it with the novel strategy and rethink the training strategy of stereo matching concurrently. The quantitative experiments demonstrate that: 1) our model is capable of reaching top performance on both the single dataset and the multiple datasets with the same parameters in this study, which also won the 2nd place in the stereo task of the ECCV Robust vision Challenge 2020 (RVC 2020); and 2) on small datasets (e.g., KITTI, ETH3D, and Middlebury), the model’s generalization and robustness are significantly affected by pre-training and data augmentation, even exceeding the network structure’s influence in some cases. These observations present a challenge to the conventional wisdom of network architectures in this stage. We expect these discoveries to encourage researchers to rethink the current paradigm of “excessive attention on the performance of a single small dataset” in stereo matching.},
  archive      = {J_TNNLS},
  author       = {Zhibo Rao and Yuchao Dai and Zhelun Shen and Renjie He},
  doi          = {10.1109/TNNLS.2022.3146306},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7796-7809},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rethinking training strategy in stereo matching},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feedback stabilization of boolean control networks with
missing data. <em>TNNLS</em>, <em>34</em>(10), 7784–7795. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data loss is often random and unavoidable in realistic networks due to transmission failure or node faults. When it comes to Boolean control networks (BCNs), the model actually becomes a delayed system with unbounded time delays. It is difficult to find a suitable way to model it and transform it into a familiar form, so there have been no available results so far. In this article, the stabilization of BCNs is studied with Bernoulli-distributed missing data. First, an augmented probabilistic BCN (PBCN) is constructed to estimate the appearance of data loss items in the model form. Based on this model, some necessary and sufficient conditions are proposed based on the construction of reachable matrices and one-step state transition probability matrices. Moreover, algorithms are proposed to complete the state feedback stabilizability analysis. In addition, a constructive method is developed to design all feasible state feedback controllers. Finally, illustrative examples are given to show the effectiveness of the proposed results.},
  archive      = {J_TNNLS},
  author       = {Dingyuan Zhong and Yuanyuan Li and Jianquan Lu},
  doi          = {10.1109/TNNLS.2022.3146262},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7784-7795},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feedback stabilization of boolean control networks with missing data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feudal latent space exploration for coordinated multi-agent
reinforcement learning. <em>TNNLS</em>, <em>34</em>(10), 7775–7783. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate how multiple agents learn to coordinate to form efficient exploration in reinforcement learning. Though straightforward, independent exploration of the joint action space of multiple agents will become exponentially more difficult as the number of agents increases. To tackle this problem, we propose feudal latent-space exploration (FLE) for multi-agent reinforcement learning (MARL). FLE introduces a feudal commander to learn a low-dimensional global latent structure that instructs multiple agents to explore coordinately. Under this framework, the multi-agent policy gradient (PG) is adopted to optimize both the agent policy and latent structure end-to-end. We demonstrate the effectiveness of this method in two multi-agent environments that need explicit coordination. Experimental results validate that FLE outperforms baseline MARL approaches that use independent exploration strategy in terms of mean rewards, efficiency, and the expressiveness of coordination policies.},
  archive      = {J_TNNLS},
  author       = {Xiangyu Liu and Ying Tan},
  doi          = {10.1109/TNNLS.2022.3146201},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7775-7783},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feudal latent space exploration for coordinated multi-agent reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Soft subspace based ensemble clustering for multivariate
time series data. <em>TNNLS</em>, <em>34</em>(10), 7761–7774. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multivariate time series (MTS) clustering has gained lots of attention. However, state-of-the-art algorithms suffer from two major issues. First, few existing studies consider correlations and redundancies between variables of MTS data. Second, since different clusters usually exist in different intrinsic variables, how to efficiently enhance the performance by mining the intrinsic variables of a cluster is challenging work. To deal with these issues, we first propose a variable-weighted K-medoids clustering algorithm (VWKM) based on the importance of a variable for a cluster. In VWKM, the proposed variable weighting scheme could identify the important variables for a cluster, which can also provide knowledge and experience to related experts. Then, a Reverse nearest neighborhood-based density Peaks approach (RP) is proposed to handle the problem of initialization sensitivity of VWKM. Next, based on VWKM and the density peaks approach, an ensemble Clustering framework (SSEC) is advanced to further enhance the clustering performance. Experimental results on ten MTS datasets show that our method works well on MTS datasets and outperforms the state-of-the-art clustering ensemble approaches.},
  archive      = {J_TNNLS},
  author       = {Guoliang He and Wenjun Jiang and Rong Peng and Ming Yin and Min Han},
  doi          = {10.1109/TNNLS.2022.3146136},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7761-7774},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Soft subspace based ensemble clustering for multivariate time series data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Construction of deep ReLU nets for spatially sparse
learning. <em>TNNLS</em>, <em>34</em>(10), 7746–7760. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training an interpretable deep net to embody its theoretical advantages is difficult but extremely important in the community of machine learning. In this article, noticing the importance of spatial sparseness in signal and image processing, we develop a constructive approach to generate a deep net to capture the spatial sparseness feature. We conduct both theoretical analysis and numerical verifications to show the power of the constructive approach. Theoretically, we prove that the constructive approach can yield a deep net estimate that achieves the optimal generalization error bounds in the framework of learning theory. Numerically, we show that the constructive approach is essentially better than shallow learning in the sense that it provides better prediction accuracy with less training time.},
  archive      = {J_TNNLS},
  author       = {Xia Liu and Di Wang and Shao-Bo Lin},
  doi          = {10.1109/TNNLS.2022.3146062},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7746-7760},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Construction of deep ReLU nets for spatially sparse learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised learning in a multilayer, nonlinear chemical
neural network. <em>TNNLS</em>, <em>34</em>(10), 7734–7745. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of programmable or trainable molecular circuits is an important goal in the field of molecular programming. Multilayer, nonlinear, artificial neural networks are a powerful framework for implementing such functionality in a molecular system, as they are provably universal function approximators. Here, we present a design for multilayer chemical neural networks with a nonlinear hyperbolic tangent transfer function. We use a weight perturbation algorithm to train the neural network which uses a simple construction to directly approximate the loss derivatives required for training. We demonstrate the training of this system to learn all 16 two-input binary functions from a common starting point. This work thus introduces new capabilities in the field of adaptive and trainable chemical reaction network (CRN) design. It also opens the door to potential future experimental implementations, including DNA strand displacement reactions.},
  archive      = {J_TNNLS},
  author       = {David Arredondo and Matthew R. Lakin},
  doi          = {10.1109/TNNLS.2022.3146057},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7734-7745},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Supervised learning in a multilayer, nonlinear chemical neural network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FSAD-net: Feedback spatial attention dehazing network.
<em>TNNLS</em>, <em>34</em>(10), 7719–7733. (<a
href="https://doi.org/10.1109/TNNLS.2022.3146004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent dehazing networks learn more discriminative high-level features by designing deeper networks or introducing complicated structures, while ignoring inherent feature correlations in intermediate layers. In this article, we establish a novel and effective end-to-end dehazing method, named feedback spatial attention dehazing network (FSAD-Net). FSAD-Net is based on the recurrent structure and consists of four modules: a shallow feature extraction block (SFEB), a feedback block (FB), multiple advanced residual blocks (ARBs), and a reconstruction block (RB). FB is designed to handle feedback connections, and it can improve the dehazing performance by exploiting the dependencies of deep features across stages. ARB implements a novel attention-based estimation on a residual block to adapt to pixels with different distributions. Finally, RB helps restore haze-free images. It can be seen from the experimental results that FSAD-Net almost outperforms the state-of-the-arts in terms of five quantitative metrics. Moreover, the qualitatively comparisons on real-world images also demonstrate the superiority of the proposed FSAD-Net. Considering the efficiency and effectiveness of FSAD-Net, it can be expected to serve as a suitable image dehazing baseline in the future.},
  archive      = {J_TNNLS},
  author       = {Yu Zhou and Zhihua Chen and Ping Li and Haitao Song and C. L. Philip Chen and Bin Sheng},
  doi          = {10.1109/TNNLS.2022.3146004},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7719-7733},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FSAD-net: Feedback spatial attention dehazing network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed adaptive-neural finite-time consensus control
for stochastic nonlinear multiagent systems subject to saturated inputs.
<em>TNNLS</em>, <em>34</em>(10), 7704–7718. (<a
href="https://doi.org/10.1109/TNNLS.2022.3145975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the problem of distributed finite-time consensus control for a class of stochastic nonlinear multiagent systems (MASs) (with directed graph communication) in the presence of unknown dynamics of agents, stochastic perturbations, external disturbances (mismatched and matched), and input saturation nonlinearities is addressed and studied. By combining the backstepping control method, the command filter technique, a finite-time auxiliary system, and artificial neural networks, innovative control inputs are designed and proposed such that outputs of follower agents converge to the output of the leader agent within a finite time. Radial-basis function neural networks (RBFNNs) are employed to approximate unknown dynamics, stochastic perturbations, and external disturbances. To overcome the complexity explosion problem of the conventional backstepping method, a novel finite-time command filter approach is proposed. Then, to deal with the destructive effects of input saturation nonlinearities, the finite-time auxiliary system is designed and developed. By mathematical analysis, it is proven that the mentioned MAS (injected by the proposed control inputs) is semiglobally finite-time stable in probability (SGFSP) and all consensus tracking errors converge to a small neighborhood of the zero during a finite time. Finally, a numerical simulation onto a group of four single-link robot manipulators is carried out to illustrate the effectiveness of the suggested control scheme.},
  archive      = {J_TNNLS},
  author       = {Fatemeh Sedghi and Mohammad Mehdi Arefi and Ali Abooee and Shen Yin},
  doi          = {10.1109/TNNLS.2022.3145975},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7704-7718},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed adaptive-neural finite-time consensus control for stochastic nonlinear multiagent systems subject to saturated inputs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From pixel to patch: Synthesize context-aware features for
zero-shot semantic segmentation. <em>TNNLS</em>, <em>34</em>(10),
7689–7703. (<a
href="https://doi.org/10.1109/TNNLS.2022.3145962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) has been actively studied for image classification tasks to relieve the burden of annotating image labels. Interestingly, the semantic segmentation task requires more labor-intensive pixel-wise annotation, but zero-shot semantic segmentation has not attracted extensive research interest. Thus, we focus on zero-shot semantic segmentation that aims to segment unseen objects with only category-level semantic representations provided for unseen categories. In this article, we propose a novel context-aware feature generation network (CaGNet) that can synthesize context-aware pixel-wise visual features for unseen categories based on category-level semantic representations and pixel-wise contextual information. The synthesized features are used to fine-tune the classifier to enable segmenting of unseen objects. Furthermore, we extend pixel-wise feature generation and fine-tuning to patch-wise feature generation and fine-tuning, which additionally considers the interpixel relationship. Experimental results on Pascal-VOC, Pascal-context, and COCO-stuff show that our method significantly outperforms the existing zero-shot semantic segmentation methods.},
  archive      = {J_TNNLS},
  author       = {Zhangxuan Gu and Siyuan Zhou and Li Niu and Zihan Zhao and Liqing Zhang},
  doi          = {10.1109/TNNLS.2022.3145962},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7689-7703},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {From pixel to patch: Synthesize context-aware features for zero-shot semantic segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Langevin cooling for unsupervised domain translation.
<em>TNNLS</em>, <em>34</em>(10), 7675–7688. (<a
href="https://doi.org/10.1109/TNNLS.2022.3145812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain translation is the task of finding correspondence between two domains. Several deep neural network (DNN) models, e.g., CycleGAN and cross-lingual language models, have shown remarkable successes on this task under the unsupervised setting—the mappings between the domains are learned from two independent sets of training data in both domains (without paired samples). However, those methods typically do not perform well on a significant proportion of test samples. In this article, we hypothesize that many of such unsuccessful samples lie at the fringe—relatively low-density areas—of data distribution, where the DNN was not trained very well, and propose to perform the Langevin dynamics to bring such fringe samples toward high-density areas. We demonstrate qualitatively and quantitatively that our strategy, called Langevin cooling (L-Cool), enhances state-of-the-art methods in image translation and language translation tasks.},
  archive      = {J_TNNLS},
  author       = {Vignesh Srinivasan and Klaus-Robert Müller and Wojciech Samek and Shinichi Nakajima},
  doi          = {10.1109/TNNLS.2022.3145812},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7675-7688},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Langevin cooling for unsupervised domain translation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ETA: An efficient training accelerator for DNNs based on
hardware-algorithm co-optimization. <em>TNNLS</em>, <em>34</em>(10),
7660–7674. (<a
href="https://doi.org/10.1109/TNNLS.2022.3145850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the efficient training of deep neural networks (DNNs) on resource-constrained platforms has attracted increasing attention for protecting user privacy. However, it is still a severe challenge since the DNN training involves intensive computations and a large amount of data access. To deal with these issues, in this work, we implement an efficient training accelerator (ETA) on field-programmable gate array (FPGA) by adopting a hardware-algorithm co-optimization approach. A novel training scheme is proposed to effectively train DNNs using 8-bit precision with arbitrary batch sizes, in which a compact but powerful data format and a hardware-oriented normalization layer are introduced. Thus the computational complexity and memory accesses are significantly reduced. In the ETA, a reconfigurable processing element (PE) is designed to support various computational patterns during training while avoiding redundant calculations from nonunit-stride convolutional layers. With a flexible network-on-chip (NoC) and a hierarchical PE array, computational parallelism and data reuse can be fully exploited, and memory accesses are further reduced. In addition, a unified computing core is developed to execute auxiliary layers such as normalization and weight update (WU), which works in a time-multiplexed manner and consumes only a small amount of hardware resources. The experiments show that our training scheme achieves the state-of-the-art accuracy across multiple models, including CIFAR-VGG16, CIFAR-ResNet20, CIFAR-InceptionV3, ResNet18, and ResNet50. Evaluated on three networks (CIFAR-VGG16, CIFAR-ResNet20, and ResNet18), our ETA on Xilinx VC709 FPGA achieves 610.98, 658.64, and 811.24 GOPS in terms of throughput, respectively. Compared with the prior art, our design demonstrates a speedup of $3.65\times $ and an energy efficiency improvement of $8.54\times $ on CIFAR-ResNet20.},
  archive      = {J_TNNLS},
  author       = {Jinming Lu and Chao Ni and Zhongfeng Wang},
  doi          = {10.1109/TNNLS.2022.3145850},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7660-7674},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ETA: An efficient training accelerator for DNNs based on hardware-algorithm co-optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cluster-based input weight initialization for echo state
networks. <em>TNNLS</em>, <em>34</em>(10), 7648–7659. (<a
href="https://doi.org/10.1109/TNNLS.2022.3145565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo state networks (ESNs) are a special type of recurrent neural networks (RNNs), in which the input and recurrent connections are traditionally generated randomly, and only the output weights are trained. Despite the recent success of ESNs in various tasks of audio, image, and radar recognition, we postulate that a purely random initialization is not the ideal way of initializing ESNs. The aim of this work is to propose an unsupervised initialization of the input connections using the $K$ -means algorithm on the training data. We show that for a large variety of datasets, this initialization performs equivalently or superior than a randomly initialized ESN while needing significantly less reservoir neurons. Furthermore, we discuss that this approach provides the opportunity to estimate a suitable size of the reservoir based on prior knowledge about the data.},
  archive      = {J_TNNLS},
  author       = {Peter Steiner and Azarakhsh Jalalvand and Peter Birkholz},
  doi          = {10.1109/TNNLS.2022.3145565},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7648-7659},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cluster-based input weight initialization for echo state networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial multiview clustering networks with adaptive
fusion. <em>TNNLS</em>, <em>34</em>(10), 7635–7647. (<a
href="https://doi.org/10.1109/TNNLS.2022.3145048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing deep multiview clustering (MVC) methods are mainly based on autoencoder networks, which seek common latent variables to reconstruct the original input of each view individually. However, due to the view-specific reconstruction loss, it is challenging to extract consistent latent representations over multiple views for clustering. To address this challenge, we propose adversarial MVC (AMvC) networks in this article. The proposed AMvC generates each view’s samples conditioning on the fused latent representations among different views to encourage a more consistent clustering structure. Specifically, multiview encoders are used to extract latent descriptions from all the views, and the corresponding generators are used to generate the reconstructed samples. The discriminative networks and the mean squared loss are jointly utilized for training the multiview encoders and generators to balance the distinctness and consistency of each view’s latent representation. Moreover, an adaptive fusion layer is developed to obtain a shared latent representation, on which a clustering loss and the ${\ell _ {1,2}}$ -norm constraint are further imposed to improve clustering performance and distinguish the latent space. Experimental results on video, image, and text datasets demonstrate that the effectiveness of our AMvC is over several state-of-the-art deep MVC methods.},
  archive      = {J_TNNLS},
  author       = {Qianqian Wang and Zhiqiang Tao and Wei Xia and Quanxue Gao and Xiaochun Cao and Licheng Jiao},
  doi          = {10.1109/TNNLS.2022.3145048},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7635-7647},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial multiview clustering networks with adaptive fusion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive learning assisted-alignment for partial domain
adaptation. <em>TNNLS</em>, <em>34</em>(10), 7621–7634. (<a
href="https://doi.org/10.1109/TNNLS.2022.3145034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses unsupervised partial domain adaptation (PDA), in which classes in the target domain are a subset of the source domain. The key challenges of PDA are how to leverage source samples in the shared classes to promote positive transfer and filter out the irrelevant source samples to mitigate negative transfer. Existing PDA methods based on adversarial DA do not consider the loss of class discriminative representation. To this end, this article proposes a contrastive learning-assisted alignment (CLA) approach for PDA to jointly align distributions across domains for better adaptation and to reweight source instances to reduce the contribution of outlier instances. A contrastive learning-assisted conditional alignment (CLCA) strategy is presented for distribution alignment. CLCA first exploits contrastive losses to discover the class discriminative information in both domains. It then employs a contrastive loss to match the clusters across the two domains based on adversarial domain learning. In this respect, CLCA attempts to reduce the domain discrepancy by matching the class-conditional and marginal distributions. Moreover, a new reweighting scheme is developed to improve the quality of weights estimation, which explores information from both the source and the target domains. Empirical results on several benchmark datasets demonstrate that the proposed CLA outperforms the existing state-of-the-art PDA methods.},
  archive      = {J_TNNLS},
  author       = {Cuie Yang and Yiu-Ming Cheung and Jinliang Ding and Kay Chen Tan and Bing Xue and Mengjie Zhang},
  doi          = {10.1109/TNNLS.2022.3145034},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7621-7634},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Contrastive learning assisted-alignment for partial domain adaptation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient learning of transform-domain LMS filter using
graph laplacian. <em>TNNLS</em>, <em>34</em>(10), 7608–7620. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transform-domain least mean squares (TDLMS) adaptive filters encompass the class of learning algorithms where the input data are subjected to a data-independent unitary transform followed by a power normalization stage as preprocessing steps. Because conventional transformations are not data-dependent, this preconditioning procedure was shown theoretically to improve the convergence of the least mean squares (LMS) filter only for certain classes of input data. So, one can tailor the transformation to the class of data. However, in reality, if the class of input data is not known beforehand, it is difficult to decide which transformation to use. Thus, there is a need to devise a learning framework to obtain such a preconditioning transformation using input data prior to applying on the input data. It is hypothesized that the underlying topology of the data affects the selection of the transformation. With the input modeled as a weighted finite graph, our method, called preconditioning using graph (PrecoG), adaptively learns the desired transform by recursive estimation of the graph Laplacian matrix. We show the efficacy of the transform as a generalized split preconditioner on a linear system of equations and in Hebbian-LMS learning models. In terms of the improvement of the condition number after applying the transformation, PrecoG performs significantly better than the existing state-of-the-art techniques that involve unitary and nonunitary transforms.},
  archive      = {J_TNNLS},
  author       = {Tamal Batabyal and Daniel Weller and Jaideep Kapur and Scott T. Acton},
  doi          = {10.1109/TNNLS.2022.3144637},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7608-7620},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient learning of transform-domain LMS filter using graph laplacian},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rankness guided group sparse representation for image
restoration. <em>TNNLS</em>, <em>34</em>(10), 7593–7607. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a spotlighted nonlocal image representation model, group sparse representation (GSR) has demonstrated a great potential in diverse image restoration tasks. Most of the existing GSR-based image restoration approaches exploit the nonlocal self-similarity (NSS) prior by clustering similar patches into groups and imposing sparsity to each group coefficient, which can effectively preserve image texture information. However, these methods have imposed only plain sparsity over each individual patch of the group, while neglecting other beneficial image properties, e.g., low-rankness (LR), leads to degraded image restoration results. In this article, we propose a novel low-rankness guided group sparse representation (LGSR) model for highly effective image restoration applications. The proposed LGSR jointly utilizes the sparsity and LR priors of each group of similar patches under a unified framework. The two priors serve as the complementary priors in LGSR for effectively preserving the texture and structure information of natural images. Moreover, we apply an alternating minimization algorithm with an adaptively adjusted parameter scheme to solve the proposed LGSR-based image restoration problem. Extensive experiments are conducted to demonstrate that the proposed LGSR achieves superior results compared with many popular or state-of-the-art algorithms in various image restoration tasks, including denoising, inpainting, and compressive sensing (CS).},
  archive      = {J_TNNLS},
  author       = {Zhiyuan Zha and Bihan Wen and Xin Yuan and Jiantao Zhou and Ce Zhu and Alex Chichung Kot},
  doi          = {10.1109/TNNLS.2022.3144630},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7593-7607},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low-rankness guided group sparse representation for image restoration},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sparse model-inspired deep thresholding network for
exponential signal reconstruction—application in fast biological
spectroscopy. <em>TNNLS</em>, <em>34</em>(10), 7578–7592. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nonuniform sampling (NUS) is a powerful approach to enable fast acquisition but requires sophisticated reconstruction algorithms. Faithful reconstruction from partially sampled exponentials is highly expected in general signal processing and many applications. Deep learning (DL) has shown astonishing potential in this field, but many existing problems, such as lack of robustness and explainability, greatly limit its applications. In this work, by combining the merits of the sparse model-based optimization method and data-driven DL, we propose a DL architecture for spectra reconstruction from undersampled data, called MoDern. It follows the iterative reconstruction in solving a sparse model to build the neural network, and we elaborately design a learnable soft-thresholding to adaptively eliminate the spectrum artifacts introduced by undersampling. Extensive results on both synthetic and biological data show that MoDern enables more robust, high-fidelity, and ultrafast reconstruction than the state-of-the-art methods. Remarkably, MoDern has a small number of network parameters and is trained on solely synthetic data while generalizing well to biological data in various scenarios. Furthermore, we extend it to an open-access and easy-to-use cloud computing platform (XCloud-MoDern), contributing a promising strategy for further development of biological applications.},
  archive      = {J_TNNLS},
  author       = {Zi Wang and Di Guo and Zhangren Tu and Yihui Huang and Yirong Zhou and Jian Wang and Liubin Feng and Donghai Lin and Yongfu You and Tatiana Agback and Vladislav Orekhov and Xiaobo Qu},
  doi          = {10.1109/TNNLS.2022.3144580},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7578-7592},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A sparse model-inspired deep thresholding network for exponential signal Reconstruction—Application in fast biological spectroscopy},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust adaptive learning control of space robot for target
capturing using neural network. <em>TNNLS</em>, <em>34</em>(10),
7567–7577. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the robust adaptive learning control for space robots with target capturing. Based on the momentum conservation theory, the impact dynamics is constructed to derive the relationship of generalized velocity in the pre-impact and post-impact phase. Considering the nonlinear dynamics with contact impact, the robust control using nonsingular terminal sliding mode (NTSM) and fast NTSM is designed to achieve the fast realization of the desired states. Furthermore, for the unknown dynamics of the combination system after capturing a target, the adaptive learning control is developed based on neural network and disturbance observer. Through the serial–parallel estimation model, the prediction error is constructed for the update of adaptive law. The system signals involved in the Lyapunov function are proved to be bounded and the sliding mode surface converges in finite time. Simulation studies present the desired tracking and learning performance.},
  archive      = {J_TNNLS},
  author       = {Xia Wang and Bin Xu and Yixin Cheng and Hai Wang and Fuchun Sun},
  doi          = {10.1109/TNNLS.2022.3144569},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7567-7577},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust adaptive learning control of space robot for target capturing using neural network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive synchronization of reaction–diffusion neural
networks with nondifferentiable delay via state coupling and spatial
coupling. <em>TNNLS</em>, <em>34</em>(10), 7555–7566. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, master–slave synchronization of reaction–diffusion neural networks (RDNNs) with nondifferentiable delay is investigated via the adaptive control method. First, centralized and decentralized adaptive controllers with state coupling are designed, respectively, and a new analytical method by discussing the size of adaptive gain is proposed to prove the convergence of the adaptively controlled error system with general delay. Then, spatial coupling with adaptive gains depending on the diffusion information of the state is first proposed to achieve the master–slave synchronization of delayed RDNNs, while this coupling structure was regarded as a negative effect in most of the existing works. Finally, numerical examples are given to show the effectiveness of the proposed adaptive controllers. In comparison with the existing adaptive controllers, the proposed adaptive controllers in this article are still effective even if the network parameters are unknown and the delay is nonsmooth, and thus have a wider range of applications.},
  archive      = {J_TNNLS},
  author       = {Hao Zhang and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2022.3144222},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7555-7566},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive synchronization of Reaction–Diffusion neural networks with nondifferentiable delay via state coupling and spatial coupling},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A componentwise approach to weakly supervised semantic
segmentation using dual-feedback network. <em>TNNLS</em>,
<em>34</em>(10), 7541–7554. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent weakly supervised semantic segmentation methods generate pseudolabels to recover the lost position information in weak labels for training the segmentation network. Unfortunately, those pseudolabels often contain mislabeled regions and inaccurate boundaries due to the incomplete recovery of position information. It turns out that the result of semantic segmentation becomes determinate to a certain degree. In this article, we decompose the position information into two components: high-level semantic information and low-level physical information, and develop a componentwise approach to recover each component independently. Specifically, we propose a simple yet effective pseudolabels updating mechanism to iteratively correct mislabeled regions inside objects to precisely refine high-level semantic information. To reconstruct low-level physical information, we utilize a customized superpixel-based random walk mechanism to trim the boundaries. Finally, we design a novel network architecture, namely, a dual-feedback network (DFN), to integrate the two mechanisms into a unified model. Experiments on benchmark datasets show that DFN outperforms the existing state-of-the-art methods in terms of intersection-over-union (mIoU).},
  archive      = {J_TNNLS},
  author       = {Zhengqiang Zhang and Qinmu Peng and Sichao Fu and Wenjie Wang and Yiu-Ming Cheung and Yue Zhao and Shujian Yu and Xinge You},
  doi          = {10.1109/TNNLS.2022.3144194},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7541-7554},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A componentwise approach to weakly supervised semantic segmentation using dual-feedback network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model behavior preserving for class-incremental learning.
<em>TNNLS</em>, <em>34</em>(10), 7529–7540. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep models have shown to be vulnerable to catastrophic forgetting, a phenomenon that the recognition performance on old data degrades when a pre-trained model is fine-tuned on new data. Knowledge distillation (KD) is a popular incremental approach to alleviate catastrophic forgetting. However, it usually fixes the absolute values of neural responses for isolated historical instances, without considering the intrinsic structure of the responses by a convolutional neural network (CNN) model. To overcome this limitation, we recognize the importance of the global property of the whole instance set and treat it as a behavior characteristic of a CNN model relevant to model incremental learning. On this basis: 1) we design an instance neighborhood-preserving (INP) loss to maintain the order of pair-wise instance similarities of the old model in the feature space; 2) we devise a label priority-preserving (LPP) loss to preserve the label ranking lists within instance-wise label probability vectors in the output space; and 3) we introduce an efficient derivable ranking algorithm for calculating the two loss functions. Extensive experiments conducted on CIFAR100 and ImageNet show that our approach achieves the state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Yu Liu and Xiaopeng Hong and Xiaoyu Tao and Songlin Dong and Jingang Shi and Yihong Gong},
  doi          = {10.1109/TNNLS.2022.3144183},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7529-7540},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model behavior preserving for class-incremental learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online intention recognition with incomplete information
based on a weighted contrastive predictive coding model in wargame.
<em>TNNLS</em>, <em>34</em>(10), 7515–7528. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incomplete and imperfect essence of the battlefield situation results in a challenge to the efficiency, stability, and reliability of traditional intention recognition methods. For this problem, we propose a deep learning architecture that consists of a contrastive predictive coding (CPC) model, a variable-length long short-term memory network (LSTM) model, and an attention weight allocator for online intention recognition with incomplete information in wargame (W-CPCLSTM). First, based on the typical characteristics of intelligence data, a CPC model is designed to capture more global structures from limited battlefield information. Then, a variable-length LSTM model is employed to classify the learned representations into predefined intention categories. Next, a weighted approach to the training attention of CPC and LSTM is introduced to allow for the stability of the model. Finally, performance evaluation and application analysis of the proposed model for the online intention recognition task were carried out based on four different degrees of detection information and a perfect situation of ideal conditions in a wargame. Besides, we explored the effect of different lengths of intelligence data on recognition performance and gave application examples of the proposed model to a wargame platform. The simulation results demonstrate that our method not only contributes to the growth of recognition stability, but it also improves recognition accuracy by 7\%–11\%, 3\%–7\%, 3\%–13\%, and 3\%–7\%, the recognition speed by 6– $32\times $ , 4– $18\times $ , 13–* $\times $ , and 1– $6\times $ compared with the traditional LSTM, classical FCN, OctConv, and OctFCN models, respectively, which characterizes it as a promising reference tool for command decision-making.},
  archive      = {J_TNNLS},
  author       = {Li Chen and Xingxing Liang and Yanghe Feng and Longfei Zhang and Jing Yang and Zhong Liu},
  doi          = {10.1109/TNNLS.2022.3144171},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7515-7528},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online intention recognition with incomplete information based on a weighted contrastive predictive coding model in wargame},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A proximal neurodynamic network with fixed-time convergence
for equilibrium problems and its applications. <em>TNNLS</em>,
<em>34</em>(10), 7500–7514. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel fixed-time converging proximal neurodynamic network (FXPNN) via a proximal operator to deal with equilibrium problems (EPs). A distinctive feature of the proposed FXPNN is its better transient performance in comparison to most existing proximal neurodynamic networks. It is shown that the FXPNN converges to the solution of the corresponding EP in fixed-time under some mild conditions. It is also shown that the settling time of the FXPNN is independent of initial conditions and the fixed-time interval can be prescribed, unlike existing results with asymptotical or exponential convergence. Moreover, the proposed FXPNN is applied to solve composition optimization problems (COPs), $l_{1}$ -regularized least-squares problems, mixed variational inequalities (MVIs), and variational inequalities (VIs). It is further shown, in the case of solving COPs, that the fixed-time convergence can be established via the Polyak–Lojasiewicz condition, which is a relaxation of the more demanding convexity condition. Finally, numerical examples are presented to validate the effectiveness and advantages of the proposed neurodynamic network.},
  archive      = {J_TNNLS},
  author       = {Xingxing Ju and Chuandong Li and Hangjun Che and Xing He and Gang Feng},
  doi          = {10.1109/TNNLS.2022.3144148},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7500-7514},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A proximal neurodynamic network with fixed-time convergence for equilibrium problems and its applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stability analysis for delayed neural networks via a
generalized reciprocally convex inequality. <em>TNNLS</em>,
<em>34</em>(10), 7491–7499. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with the stability of neural networks (NNs) with time-varying delay. First, a generalized reciprocally convex inequality (RCI) is presented, providing a tight bound for reciprocally convex combinations. This inequality includes some existing ones as special case. Second, in order to cater for the use of the generalized RCI, a novel Lyapunov–Krasovskii functional (LKF) is constructed, which includes a generalized delay-product term. Third, based on the generalized RCI and the novel LKF, several stability criteria for the delayed NNs under study are put forward. Finally, two numerical examples are given to illustrate the effectiveness and advantages of the proposed stability criteria.},
  archive      = {J_TNNLS},
  author       = {Hui-Chao Lin and Hong-Bing Zeng and Xian-Ming Zhang and Wei Wang},
  doi          = {10.1109/TNNLS.2022.3144032},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7491-7499},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability analysis for delayed neural networks via a generalized reciprocally convex inequality},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Observer-based decentralized control for non-strict-feedback
fractional-order nonlinear large-scale systems with unknown dead zones.
<em>TNNLS</em>, <em>34</em>(10), 7479–7490. (<a
href="https://doi.org/10.1109/TNNLS.2022.3143901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the output-feedback decentralized control issue for the fractional-order nonlinear large-scale nonstrict-feedback systems with states immeasurable and unknown dead zones. The unknown nonlinear functions are identified by neural networks (NNs), and immeasurable states are estimated by establishing an NNs’ decentralized state observer. The algebraic loop issue is solved by using the property of NN basis functions and designing the fractional-order adaptation laws. In addition, the fractional-order dynamic surface control (FODSC) design technique is introduced in the adaptive backstepping control algorithm to avoid the issue of “explosion of complexity.” Then, by treating the nonsymmetric dead zones as the time-varying uncertain systems, an adaptive NNs’ output-feedback decentralized control scheme is developed via the fractional-order Lyapunov stability criterion. It is proven that the controlled fractional-order systems are stable, and the tracking and observer errors can converge to a small neighborhood of zero. Two simulation examples are given to confirm the validity of the put forward control scheme.},
  archive      = {J_TNNLS},
  author       = {Yongliang Zhan and Xiaomei Li and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2022.3143901},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7479-7490},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based decentralized control for non-strict-feedback fractional-order nonlinear large-scale systems with unknown dead zones},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stability of delayed reaction–diffusion neural-network
models with hybrid impulses via vector lyapunov function.
<em>TNNLS</em>, <em>34</em>(10), 7467–7478. (<a
href="https://doi.org/10.1109/TNNLS.2022.3143884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on stability analysis of delayed reaction–diffusion neural-network models with hybrid impulses based on the vector Lyapunov function. First, several properties of a vector Halanay-type inequality are given to be the key ingredient for the stability analysis. Then, the Krasovskii-type theorems are established for sufficient conditions of exponential stability, which removes the common threshold of impulses in each neuron subsystem at every impulse time. It shows that the stability of neural networks can be retained with hybrid impulses involved in neural networks, and the synchronization of neural networks can be achieved by designing an impulsive controller, which allows the existence of impulsive perturbation in some nodes and time. Finally, the effectiveness of theoretical results is verified by numerical examples with a successful application to image encryption.},
  archive      = {J_TNNLS},
  author       = {Tengda Wei and Xiaodi Li and Jinde Cao},
  doi          = {10.1109/TNNLS.2022.3143884},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7467-7478},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability of delayed Reaction–Diffusion neural-network models with hybrid impulses via vector lyapunov function},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully distributed dynamic event-triggered bipartite
formation tracking for multiagent systems with multiple nonautonomous
leaders. <em>TNNLS</em>, <em>34</em>(10), 7453–7466. (<a
href="https://doi.org/10.1109/TNNLS.2022.3143867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering that cooperative interactions and antagonistic interactions between neighboring agents may exist simultaneously in practice, this article studies the bipartite time-varying output formation tracking (BTVOFT) problems for homogeneous/heterogeneous multiagent systems with multiple nonautonomous leaders under switching communication networks. First, a full-dimensional observer-based nonsmooth distributed dynamic event-triggered (DDET) output feedback control scheme is proposed to ensure that BTVOFT is achieved, and the Zeno behavior is excluded. Note that the nonsmooth distributed control scheme requires global communication network information and may cause unexpected chattering effect, and the design cost of full-dimensional observer is relatively high. Thus, a reduced-dimensional observer-based continuous fully DDET scheme is proposed. Compared with the existing event-triggered schemes, the dynamic event-triggered scheme can ensure larger interevent times by introducing an additional internal dynamic variable. Finally, the effectiveness and performance of the theoretical results are validated by numerical simulations.},
  archive      = {J_TNNLS},
  author       = {Huaguang Zhang and Weihua Li and Juan Zhang and Yingchun Wang and Jiayue Sun},
  doi          = {10.1109/TNNLS.2022.3143867},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7453-7466},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully distributed dynamic event-triggered bipartite formation tracking for multiagent systems with multiple nonautonomous leaders},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast finite-time neural network control of stochastic
nonlinear systems. <em>TNNLS</em>, <em>34</em>(10), 7443–7452. (<a
href="https://doi.org/10.1109/TNNLS.2022.3143655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article takes a fast finite-time control of stochastic nonlinear systems into account. The presence of unknown stochastic disturbance terms makes the traditional fast finite-time control approaches unavailable. To deal with this difficulty, by establishing an auxiliary function and using Jensen’s inequality, in Lemma 6 , a new criterion of fast finite-time stability is first established for the uncertain stochastic system. Based on the approximation ability of neural networks (NNs), an innovative fast finite-time strategy is put forward for stochastic nonlinear systems. Furthermore, by adopting the presented fast finite-time stability criterion, the stability of the stochastic systems is confirmed. Finally, two simulations are implemented to validate the feasibility of the presented NN control strategy.},
  archive      = {J_TNNLS},
  author       = {Fang Wang and Zhaoyang You and Zhi Liu and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2022.3143655},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7443-7452},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A fast finite-time neural network control of stochastic nonlinear systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel value iteration scheme with adjustable convergence
rate. <em>TNNLS</em>, <em>34</em>(10), 7430–7442. (<a
href="https://doi.org/10.1109/TNNLS.2022.3143527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel value iteration scheme is developed with convergence and stability discussions. A relaxation factor is introduced to adjust the convergence rate of the value function sequence. The convergence conditions with respect to the relaxation factor are given. The stability of the closed-loop system using the control policies generated by the present VI algorithm is investigated. Moreover, an integrated VI approach is developed to accelerate and guarantee the convergence by combining the advantages of the present and traditional value iterations. Also, a relaxation function is designed to adaptively make the developed value iteration scheme possess fast convergence property. Finally, the theoretical results and the effectiveness of the present algorithm are validated by numerical examples.},
  archive      = {J_TNNLS},
  author       = {Mingming Ha and Ding Wang and Derong Liu},
  doi          = {10.1109/TNNLS.2022.3143527},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7430-7442},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel value iteration scheme with adjustable convergence rate},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Neural-network-based finite-time bipartite containment
control for fractional-order multi-agent systems. <em>TNNLS</em>,
<em>34</em>(10), 7418–7429. (<a
href="https://doi.org/10.1109/TNNLS.2022.3143494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the adaptive bipartite containment control problem for the nonaffine fractional-order multi-agent systems (FOMASs) with disturbances and completely unknown high-order dynamics. Different from the existing finite-time theory of fractional-order system, a lemma is developed that can be applied to actualize the aim of finite-time bipartite containment for the considered FOMASs, in which the settling time and convergence accuracy can be estimated. Via applying the mean-value theorem, the difficulty of the controller design generated by the nonaffine nonlinear term is overcome. A neural network (NN) is employed to approximate the ideal input signal instead of the unknown nonaffine function, then a distributed adaptive NN bipartite containment control for the FOMASs is developed under the backstepping structure. It can be proved that the bipartite containment error under the proposed control scheme can achieve finite-time convergence even though the follower agents are subjected to completely unknown dynamic and disturbances. Finally, the feasibility and validity of the obtained results are exhibited by the simulation examples.},
  archive      = {J_TNNLS},
  author       = {Yang Liu and Huaguang Zhang and Zhan Shi and Zhiyun Gao},
  doi          = {10.1109/TNNLS.2022.3143494},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7418-7429},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based finite-time bipartite containment control for fractional-order multi-agent systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-affinity style embedding network for semantic-aligned
image style transfer. <em>TNNLS</em>, <em>34</em>(10), 7404–7417. (<a
href="https://doi.org/10.1109/TNNLS.2022.3143356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image style transfer aims at synthesizing an image with the content from one image and the style from another. User studies have revealed that the semantic correspondence between style and content greatly affects subjective perception of style transfer results. While current studies have made great progress in improving the visual quality of stylized images, most methods directly transfer global style statistics without considering semantic alignment. Current semantic style transfer approaches still work in an iterative optimization fashion, which is impractically computationally expensive. Addressing these issues, we introduce a novel dual-affinity style embedding network (DaseNet) to synthesize images with style aligned at semantic region granularity. In the dual-affinity module, feature correlation and semantic correspondence between content and style images are modeled jointly for embedding local style patterns according to semantic distribution. Furthermore, the semantic-weighted style loss and the region-consistency loss are introduced to ensure semantic alignment and content preservation. With the end-to-end network architecture, DaseNet can well balance visual quality and inference efficiency for semantic style transfer. Experimental results on different scene categories have demonstrated the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Zhuoqi Ma and Tianwei Lin and Xin Li and Fu Li and Dongliang He and Errui Ding and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TNNLS.2022.3143356},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7404-7417},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual-affinity style embedding network for semantic-aligned image style transfer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient deep reinforcement learning with imitative expert
priors for autonomous driving. <em>TNNLS</em>, <em>34</em>(10),
7391–7403. (<a
href="https://doi.org/10.1109/TNNLS.2022.3142822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) is a promising way to achieve human-like autonomous driving. However, the low sample efficiency and difficulty of designing reward functions for DRL would hinder its applications in practice. In light of this, this article proposes a novel framework to incorporate human prior knowledge in DRL, in order to improve the sample efficiency and save the effort of designing sophisticated reward functions. Our framework consists of three ingredients, namely, expert demonstration, policy derivation, and RL. In the expert demonstration step, a human expert demonstrates their execution of the task, and their behaviors are stored as state-action pairs. In the policy derivation step, the imitative expert policy is derived using behavioral cloning and uncertainty estimation relying on the demonstration data. In the RL step, the imitative expert policy is utilized to guide the learning of the DRL agent by regularizing the KL divergence between the DRL agent’s policy and the imitative expert policy. To validate the proposed method in autonomous driving applications, two simulated urban driving scenarios (unprotected left turn and roundabout) are designed. The strengths of our proposed method are manifested by the training results as our method can not only achieve the best performance but also significantly improve the sample efficiency in comparison with the baseline algorithms (particularly 60\% improvement compared with soft actor-critic). In testing conditions, the agent trained by our method obtains the highest success rate and shows diverse and human-like driving behaviors as demonstrated by the human expert. We also find that using the imitative expert policy trained with the ensemble method that estimates both policy and model uncertainties, as well as increasing the training sample size, can result in better training and testing performance, especially for more difficult tasks. As a result, the proposed method has shown its potential to facilitate the applications of DRL-enabled human-like autonomous driving systems in practice. The code and supplementary videos are also provided. [ https://mczhi.github.io/Expert-Prior-RL/ ]},
  archive      = {J_TNNLS},
  author       = {Zhiyu Huang and Jingda Wu and Chen Lv},
  doi          = {10.1109/TNNLS.2022.3142822},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7391-7403},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient deep reinforcement learning with imitative expert priors for autonomous driving},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ECCA: Efficient correntropy-based clustering algorithm with
orthogonal concept factorization. <em>TNNLS</em>, <em>34</em>(10),
7377–7390. (<a
href="https://doi.org/10.1109/TNNLS.2022.3142806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the hottest topics in unsupervised learning is how to efficiently and effectively cluster large amounts of unlabeled data. To address this issue, we propose an orthogonal conceptual factorization (OCF) model to increase clustering effectiveness by restricting the degree of freedom of matrix factorization. In addition, for the OCF model, a fast optimization algorithm containing only a few low-dimensional matrix operations is given to improve clustering efficiency, as opposed to the traditional CF optimization algorithm, which involves dense matrix multiplications. To further improve the clustering efficiency while suppressing the influence of the noises and outliers distributed in real-world data, an efficient correntropy-based clustering algorithm (ECCA) is proposed in this article. Compared with OCF, an anchor graph is constructed and then OCF is performed on the anchor graph instead of directly performing OCF on the original data, which can not only further improve the clustering efficiency but also inherit the advantages of the high performance of spectral clustering. In particular, the introduction of the anchor graph makes ECCA less sensitive to changes in data dimensions and still maintains high efficiency at higher data dimensions. Meanwhile, for various complex noises and outliers in real-world data, correntropy is introduced into ECCA to measure the similarity between the matrix before and after decomposition, which can greatly improve the clustering effectiveness and robustness. Subsequently, a novel and efficient half-quadratic optimization algorithm was proposed to quickly optimize the ECCA model. Finally, extensive experiments on different real-world datasets and noisy datasets show that ECCA can archive promising effectiveness and robustness while achieving tens to thousands of times the efficiency compared with other state-of-the-art baselines.},
  archive      = {J_TNNLS},
  author       = {Ben Yang and Xuetao Zhang and Feiping Nie and Badong Chen and Fei Wang and Zhixiong Nan and Nanning Zheng},
  doi          = {10.1109/TNNLS.2022.3142806},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7377-7390},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ECCA: Efficient correntropy-based clustering algorithm with orthogonal concept factorization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven policy iteration for nonlinear optimal control
problems. <em>TNNLS</em>, <em>34</em>(10), 7365–7376. (<a
href="https://doi.org/10.1109/TNNLS.2022.3142501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of optimal control laws for nonlinear systems is tackled without knowledge of the underlying plant and of a functional description of the cost function. The proposed data-driven method is based only on real-time measurements of the state of the plant and of the (instantaneous) value of the reward signal and relies on a combination of ideas borrowed from the theories of optimal and adaptive control problems. As a result, the architecture implements a policy iteration strategy in which, hinging on the use of neural networks, the policy evaluation step and the computation of the relevant information instrumental for the policy improvement step are performed in a purely continuous-time fashion. Furthermore, the desirable features of the design method, including convergence rate and robustness properties, are discussed. Finally, the theory is validated via two benchmark numerical simulations.},
  archive      = {J_TNNLS},
  author       = {Corrado Possieri and Mario Sassano},
  doi          = {10.1109/TNNLS.2022.3142501},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7365-7376},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven policy iteration for nonlinear optimal control problems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic sparse connectivity learning for neural networks.
<em>TNNLS</em>, <em>34</em>(10), 7350–7364. (<a
href="https://doi.org/10.1109/TNNLS.2022.3141665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since sparse neural networks usually contain many zero weights, these unnecessary network connections can potentially be eliminated without degrading network performance. Therefore, well-designed sparse neural networks have the potential to significantly reduce the number of floating-point operations (FLOPs) and computational resources. In this work, we propose a new automatic pruning method—sparse connectivity learning (SCL). Specifically, a weight is reparameterized as an elementwise multiplication of a trainable weight variable and a binary mask. Thus, network connectivity is fully described by the binary mask, which is modulated by a unit step function. We theoretically prove the fundamental principle of using a straight-through estimator (STE) for network pruning. This principle is that the proxy gradients of STE should be positive, ensuring that mask variables converge at their minima. After finding Leaky ReLU, Softplus, and identity STEs can satisfy this principle, we propose to adopt identity STE in SCL for discrete mask relaxation. We find that mask gradients of different features are very unbalanced; hence, we propose to normalize mask gradients of each feature to optimize mask variable training. In order to automatically train sparse masks, we include the total number of network connections as a regularization term in our objective function. As SCL does not require pruning criteria or hyperparameters defined by designers for network layers, the network is explored in a larger hypothesis space to achieve optimized sparse connectivity for the best performance. SCL overcomes the limitations of existing automatic pruning methods. Experimental results demonstrate that SCL can automatically learn and select important network connections for various baseline network structures. Deep learning models trained by SCL outperform the state-of-the-art human-designed and automatic pruning methods in sparsity, accuracy, and FLOPs reduction.},
  archive      = {J_TNNLS},
  author       = {Zhimin Tang and Linkai Luo and Bike Xie and Yiyu Zhu and Rujie Zhao and Lvqing Bi and Chao Lu},
  doi          = {10.1109/TNNLS.2022.3141665},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7350-7364},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatic sparse connectivity learning for neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural adaptive dynamic surface asymptotic tracking control
of hydraulic manipulators with guaranteed transient performance.
<em>TNNLS</em>, <em>34</em>(10), 7339–7349. (<a
href="https://doi.org/10.1109/TNNLS.2022.3141463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel neural network (NN)-based adaptive dynamic surface asymptotic tracking controller with guaranteed transient performance is proposed for $n$ -degrees of freedom (DOF) hydraulic manipulators. To fulfill the work, the entire manipulator system model, including hydraulic actuator dynamics, is first established. Then, the neural adaptive dynamic surface controller is designed, in which the NN is utilized to approximate the unknown joint coupling dynamics, while the approximation error and uncertainties of the actuator dynamics are addressed by the nonlinear robust control law with adaptive gains. In addition, a modified funnel function that ensures the joint tracking errors remains within a predefined funnel boundary and is skillfully incorporated into the adaptive dynamic surface control (ADSC) design to achieve a guaranteed transient tracking performance. The theoretical analysis reveals that both the guaranteed transient tracking performance and asymptotic stability can be achieved with the proposed controller. Contrastive simulations are performed on a 2-DOF hydraulic manipulator to demonstrate the superiority of the proposed controller.},
  archive      = {J_TNNLS},
  author       = {Xiaowei Yang and Wenxiang Deng and Jianyong Yao},
  doi          = {10.1109/TNNLS.2022.3141463},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7339-7349},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural adaptive dynamic surface asymptotic tracking control of hydraulic manipulators with guaranteed transient performance},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic modeling for image registration using radial
basis functions: Application to cardiac motion estimation.
<em>TNNLS</em>, <em>34</em>(10), 7324–7338. (<a
href="https://doi.org/10.1109/TNNLS.2022.3141119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases (CVDs) are the leading cause of death, affecting the cardiac dynamics over the cardiac cycle. Estimation of cardiac motion plays an essential role in many medical clinical tasks. This article proposes a probabilistic framework for image registration using compact support radial basis functions (CSRBFs) to estimate cardiac motion. A variational inference-based generative model with convolutional neural networks (CNNs) is proposed to learn the probabilistic coefficients of CSRBFs used in image deformation. We designed two networks to estimate the deformation coefficients of CSRBFs: the first one solves the spatial transformation using given control points, and the second one models the transformation using drifting control points. The given-point-based network estimates the probabilistic coefficients of control points. In contrast, the drifting-point-based model predicts the probabilistic coefficients and spatial distribution of control points simultaneously. To regularize these coefficients, we derive the bending energy (BE) in the variational bound by defining the covariance of coefficients. The proposed framework has been evaluated on the cardiac motion estimation and the calculation of the myocardial strain. In the experiments, 1409 slice pairs of end-diastolic (ED) and end-systolic (ES) phase in 4-D cardiac magnetic resonance (MR) images selected from three public datasets are employed to evaluate our networks. The experimental results show that our framework outperforms the state-of-the-art registration methods concerning the deformation smoothness and registration accuracy.},
  archive      = {J_TNNLS},
  author       = {Ziyu Gan and Wei Sun and Kaimin Liao and Xuan Yang},
  doi          = {10.1109/TNNLS.2022.3141119},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7324-7338},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic modeling for image registration using radial basis functions: Application to cardiac motion estimation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive NN tracking control for uncertain MIMO nonlinear
system with time-varying state constraints and disturbances.
<em>TNNLS</em>, <em>34</em>(10), 7309–7323. (<a
href="https://doi.org/10.1109/TNNLS.2022.3141052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive neural network (NN) tracking control scheme is proposed for uncertain multi-input–multi-output (MIMO) nonlinear system in strict-feedback form subject to system uncertainties, time-varying state constraints, and bounded disturbances. The radial basis function NNs (RBFNNs) are adopted to approximate the system uncertainties. By constructing the intermediate variables, the external disturbances that cannot be directly measured are approximated by the disturbance observers. The time-varying barrier Lyapunov function (TVBLF) is constructed to guarantee the boundedness of the errors lie in the sets. To overcome the potential singularity problem that the denominator of the barrier function term approaches zero in controller design, the adaptive NN tracking control scheme with time-varying state constraints is proposed. Based on the TVBLF, the controller will be designed to guarantee tracking performance without violating the appropriate error constraints. The analysis of TVBLF shows that all closed-loop signals remain semiglobally uniformly ultimately bounded (SGUUB). The simulation results are performed to validate the validity of the proposed scheme.},
  archive      = {J_TNNLS},
  author       = {Shumin Lu and Mou Chen and Yanjun Liu and Shuyi Shao},
  doi          = {10.1109/TNNLS.2022.3141052},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7309-7323},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive NN tracking control for uncertain MIMO nonlinear system with time-varying state constraints and disturbances},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic event-based control for stochastic optimal
regulation of nonlinear networked control systems. <em>TNNLS</em>,
<em>34</em>(10), 7299–7308. (<a
href="https://doi.org/10.1109/TNNLS.2022.3140478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a dynamic event-triggered stochastic adaptive dynamic programming (ADP)-based problem is investigated for nonlinear systems with a communication network. First, a novel condition of obtaining stochastic input-to-state stability (SISS) of discrete version is skillfully established. Then, the event-triggered control strategy is devised, and a near-optimal control policy is designed using an identifier-actor–critic neural networks (NNs) with an event-sampled state vector. Above all, an adaptive static event sampling condition is designed by using the Lyapunov technique to ensure ultimate boundedness (UB) for the closed-loop system. However, since the static event-triggered rule only depends on the current state, regardless of previous values, this article presents an explicit dynamic event-triggered rule. Furthermore, we prove that the lower bound of sampling interval for the proposed dynamic event-triggered control strategy is greater than one, which avoids the so-called triviality phenomenon. Finally, the effectiveness of the proposed near-optimal control pattern is verified by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Zhongyang Ming and Huaguang Zhang and Yanhong Luo and Wei Wang},
  doi          = {10.1109/TNNLS.2022.3140478},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7299-7308},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic event-based control for stochastic optimal regulation of nonlinear networked control systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). QTT-DLSTM: A cloud-edge-aided distributed LSTM for
cyber–physical–social big data. <em>TNNLS</em>, <em>34</em>(10),
7286–7298. (<a
href="https://doi.org/10.1109/TNNLS.2022.3140238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber–physical–social systems (CPSS), an emerging cross-disciplinary research area, combines cyber–physical systems (CPS) with social networking for the purpose of providing personalized services for humans. CPSS big data, recording various aspects of human lives, should be processed to mine valuable information for CPSS services. To efficiently deal with CPSS big data, artificial intelligence (AI), an increasingly important technology, is used for CPSS data processing and analysis. Meanwhile, the rapid development of edge devices with fast processors and large memories allows local edge computing to be a powerful real-time complement to global cloud computing. Therefore, to facilitate the processing and analysis of CPSS big data from the perspective of multi-attributes, a cloud-edge-aided quantized tensor-train distributed long short-term memory (QTT-DLSTM) method is presented in this article. First, a tensor is used to represent the multi-attributes CPSS big data, which will be decomposed into the QTT form to facilitate distributed training and computing. Second, a distributed cloud-edge computing model is used to systematically process the CPSS data, including global large-scale data processing in the cloud, and local small-scale data processed at the edge. Third, a distributed computing strategy is used to improve the efficiency of training via partitioning the weight matrix and large amounts of input data in the QTT form. Finally, the performance of the proposed QTT-DLSTM method is evaluated using experiments on a public discrete manufacturing process dataset, the Li-ion battery dataset, and a public social dataset.},
  archive      = {J_TNNLS},
  author       = {Xiaokang Wang and Lei Ren and Ruixue Yuan and Laurence T. Yang and M. Jamal Deen},
  doi          = {10.1109/TNNLS.2022.3140238},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7286-7298},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {QTT-DLSTM: A cloud-edge-aided distributed LSTM for Cyber–Physical–Social big data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Residual tuning: Toward novel category discovery without
labels. <em>TNNLS</em>, <em>34</em>(10), 7271–7285. (<a
href="https://doi.org/10.1109/TNNLS.2022.3140235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering novel visual categories from a set of unlabeled images is a crucial and essential capability for intelligent vision systems since it enables them to automatically learn new concepts with no need for human-annotated supervision anymore. To tackle this problem, existing approaches first pretrain a neural network with a set of labeled images and then fine-tune the network to cluster unlabeled images into a few categorical groups. However, their unified feature representation hits a tradeoff bottleneck between feature preservation on labeled data and feature adaptation on unlabeled data. To circumvent this bottleneck, we propose a residual-tuning approach, which estimates a new residual feature from the pretrained network and adds it with a previous basic feature to compute the clustering objective together. Our disentangled representation approach facilitates adjusting visual representations for unlabeled images and overcoming forgetting old knowledge acquired from labeled images, with no need of replaying the labeled images again. In addition, residual-tuning is an efficient solution, adding few parameters and consuming modest training time. Our results on three common benchmarks show consistent and considerable gains over other state-of-the-art methods, and further reduce the performance gap to the fully supervised learning setup. Moreover, we explore two extended scenarios, including using fewer labeled classes and continually discovering more unlabeled sets, where the results further signify the advantages and effectiveness of our residual-tuning approach against previous approaches. Our code is available at https://github.com/liuyudut/ResTune .},
  archive      = {J_TNNLS},
  author       = {Yu Liu and Tinne Tuytelaars},
  doi          = {10.1109/TNNLS.2022.3140235},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7271-7285},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Residual tuning: Toward novel category discovery without labels},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disturbance observer-based adaptive neural network output
feedback control for uncertain nonlinear systems. <em>TNNLS</em>,
<em>34</em>(10), 7260–7270. (<a
href="https://doi.org/10.1109/TNNLS.2021.3140106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is devoted to the output feedback control of nonlinear system subject to unknown control directions, unknown Bouc–Wen hysteresis and unknown disturbances. During the control design process, the design obstacles caused by unknown control directions and Bouc–Wen hysteresis are eliminated by introducing linear state transformation and a new coordinate transformation, which avoids using the Nussbaum function with high-frequency oscillation to deal with the issue. Besides, to settle the issue caused by the unknown disturbances, a novel nonlinear disturbance observer is designed, which has the characteristics of simple structure, low coupling, and easy implementation. Especially, a compensation item is constructed to offset the redundant items generated in the backstepping design process. Simultaneously, using the neural network and backstepping technology, an output feedback controller is devised. The controller ensures that all closed-loop signals are bounded, and the system output, state observation error, and disturbance observation error converge to a small neighborhood of the origin. Finally, to illustrate the effectiveness of the proposed scheme, simulation verification is carried out based on a numerical example and a Nomoto ship model.},
  archive      = {J_TNNLS},
  author       = {Yuxiao Lian and Jianwei Xia and Ju H. Park and Wei Sun and Hao Shen},
  doi          = {10.1109/TNNLS.2021.3140106},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7260-7270},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disturbance observer-based adaptive neural network output feedback control for uncertain nonlinear systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clifford-valued distributed optimization based on recurrent
neural networks. <em>TNNLS</em>, <em>34</em>(10), 7248–7259. (<a
href="https://doi.org/10.1109/TNNLS.2021.3139865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the Clifford-valued distributed optimization subject to linear equality and inequality constraints. The objective function of the optimization problems is composed of the sum of convex functions defined in the Clifford domain. Based on the generalized Clifford gradient, a system of multiple Clifford-valued recurrent neural networks (RNNs) is proposed for solving the distributed optimization problems. Each Clifford-valued RNN minimizes a local objective function individually, with local interactions with others. The convergence of the neural system is rigorously proved based on the Lyapunov theory. Two illustrative examples are delineated to demonstrate the viability of the results in this article.},
  archive      = {J_TNNLS},
  author       = {Zicong Xia and Yang Liu and Kit Ian Kou and Jun Wang},
  doi          = {10.1109/TNNLS.2021.3139865},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7248-7259},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Clifford-valued distributed optimization based on recurrent neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local causal discovery in multiple manipulated datasets.
<em>TNNLS</em>, <em>34</em>(10), 7235–7247. (<a
href="https://doi.org/10.1109/TNNLS.2021.3139389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of distinguishing direct causes from direct effects of a target variable of interest from multiple manipulated datasets with unknown manipulated variables and nonidentical data distributions. Recent studies have shown that datasets attained from manipulated experiments (i.e., manipulated data) contain richer causal information than observational data for causal structure learning. Thus, in this article, we propose a new algorithm, which makes full use of the interventional properties of a causal model to discover the direct causes and direct effects of a target variable from multiple datasets with different manipulations. It is more suited to real-world cases and is also a challenge to be addressed in this article. First, we apply the backward framework to learn parents and children (PC) of a given target from multiple manipulated datasets. Second, we orient some edges connected to the target in advance through the assumption that the target variable is not manipulated and then orient the remaining undirected edges by finding invariant V-structures from multiple datasets. Third, we analyze the correctness of the proposed algorithm. To the best of our knowledge, the proposed algorithm is the first that can identify the local causal structure of a given target from multiple manipulated datasets with unknown manipulated variables. Experimental results on standard Bayesian networks validate the effectiveness of our algorithm.},
  archive      = {J_TNNLS},
  author       = {Yunxia Wang and Fuyuan Cao and Kui Yu and Jiye Liang},
  doi          = {10.1109/TNNLS.2021.3139389},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7235-7247},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local causal discovery in multiple manipulated datasets},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Nonsingular practical fixed-time adaptive output feedback
control of MIMO nonlinear systems. <em>TNNLS</em>, <em>34</em>(10),
7222–7234. (<a
href="https://doi.org/10.1109/TNNLS.2021.3139230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the nonsingular fixed-time control problem of multiple-input multiple-output (MIMO) nonlinear systems with unmeasured states for the first time. A state observer is designed to solve the problem that system states cannot be measured. Due to the existence of the unknown system nonlinear dynamics, neural networks (NNs) are introduced to approximate them. Then, through the combination of adaptive backstepping recursive technology and adding power integration technology, a nonsingular fixed-time adaptive output feedback control algorithm is proposed, which introduces a filter to avoid the complicated derivation process of the virtual control function. According to the fixed-time Lyapunov stability theory, the practical fixed-time stability of the closed-loop system is proven, which means that all signals of the closed-loop system remain bounded in a fixed time under the proposed algorithm. Finally, the effectiveness of the proposed algorithm is verified by the numerical simulation and practical simulation.},
  archive      = {J_TNNLS},
  author       = {Hao Xu and Dengxiu Yu and Shuai Sui and Yin-Ping Zhao and C. L. Philip Chen and Zhen Wang},
  doi          = {10.1109/TNNLS.2021.3139230},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7222-7234},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonsingular practical fixed-time adaptive output feedback control of MIMO nonlinear systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed actor–critic algorithms for multiagent
reinforcement learning over directed graphs. <em>TNNLS</em>,
<em>34</em>(10), 7210–7221. (<a
href="https://doi.org/10.1109/TNNLS.2021.3139138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Actor–critic (AC) cooperative multiagent reinforcement learning (MARL) over directed graphs is studied in this article. The goal of the agents in MARL is to maximize the globally averaged return in a distributed way, i.e., each agent can only exchange information with its neighboring agents. AC methods proposed in the literature require the communication graphs to be undirected and the weight matrices to be doubly stochastic (more precisely, the weight matrices are row stochastic and their expectation are column stochastic). Differently from these methods, we propose a distributed AC algorithm for MARL over directed graph with fixed topology that only requires the weight matrix to be row stochastic. Then, we also study the MARL over directed graphs (possibly not connected) with changing topologies, proposing a different distributed AC algorithm based on the push-sum protocol that only requires the weight matrices to be column stochastic. Convergence of the proposed algorithms is proven for linear function approximation of the action value function. Simulations are presented to demonstrate the effectiveness of the proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Pengcheng Dai and Wenwu Yu and He Wang and Simone Baldi},
  doi          = {10.1109/TNNLS.2021.3139138},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7210-7221},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed Actor–Critic algorithms for multiagent reinforcement learning over directed graphs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain adaptation as optimal transport on grassmann
manifolds. <em>TNNLS</em>, <em>34</em>(10), 7196–7209. (<a
href="https://doi.org/10.1109/TNNLS.2021.3139119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation in the Euclidean space is a challenging task on which researchers recently have made great progress. However, in practice, there are rich data representations that are not Euclidean. For example, many high-dimensional data in computer vision are in general modeled by a low-dimensional manifold. This prompts the demand of exploring domain adaptation between non-Euclidean manifold spaces. This article is concerned with domain adaption over the classic Grassmann manifolds. An optimal transport-based domain adaptation model on Grassmann manifolds has been proposed. The model implements the adaption between datasets by minimizing the Wasserstein distances between the projected source data and the target data on Grassmann manifolds. Four regularization terms are introduced to keep task-related consistency in the adaptation process. Furthermore, to reduce the computational cost, a simplified model preserving the necessary adaption property and its efficient algorithm is proposed and tested. The experiments on several publicly available datasets prove the proposed model outperforms several relevant baseline domain adaptation methods.},
  archive      = {J_TNNLS},
  author       = {Tianhang Long and Yanfeng Sun and Junbin Gao and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TNNLS.2021.3139119},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7196-7209},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Domain adaptation as optimal transport on grassmann manifolds},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partial-node-based state estimation for delayed complex
networks under intermittent measurement outliers: A
multiple-order-holder approach. <em>TNNLS</em>, <em>34</em>(10),
7181–7195. (<a
href="https://doi.org/10.1109/TNNLS.2021.3138979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the partial-node-based (PNB) state estimation problem for delayed complex networks (DCNs) subject to intermittent measurement outliers (IMOs). In order to describe the intermittent nature of outliers, several sequences of shifted gate functions are adopted to model the occurrence moments and the disappearing moments of IMOs. Two outlier-related indices, namely, minimum and maximum interval lengths, are employed to parameterize the “occurrence frequency” of IMOs. The norm of the addressed outlier is allowed to be greater than a certain fixed threshold, and this distinguishes the outlier from the extensively studied norm-bounded noise. By adopting the input–output models of the considered complex network, a novel multiple-order-holder (MOH) approach is developed to resist the effects of IMOs by dedicatedly designing a weighted average of certain non-IMO measurements, and then, a PNB state estimator is constructed based on the outputs of the MOHs. Sufficient conditions are proposed to ensure the exponentially ultimate boundedness (EUB) of the resultant estimation error, and the estimator gain matrices are subsequently obtained by solving a constrained optimization problem. Finally, two simulation examples are provided to demonstrate the effectiveness of our developed outlier-resistant PNB state estimation scheme.},
  archive      = {J_TNNLS},
  author       = {Lei Zou and Zidong Wang and Jun Hu and Hongli Dong},
  doi          = {10.1109/TNNLS.2021.3138979},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7181-7195},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Partial-node-based state estimation for delayed complex networks under intermittent measurement outliers: A multiple-order-holder approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local synchronization of directed lur’e networks with
coupling delay via distributed impulsive control subject to actuator
saturation. <em>TNNLS</em>, <em>34</em>(10), 7170–7180. (<a
href="https://doi.org/10.1109/TNNLS.2021.3138997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the local exponential synchronization synthesis problem of directed Lur’e networks with coupling time-varying delay under the distributed impulsive control subject to actuator saturation. First, by utilizing proof by contradiction, impulsive comparison principle, and latest improved convex hull representation of saturation function, some delay-independent sufficient criteria for local exponential synchronization are presented in the form of bilinear matrix inequalities. Meanwhile, a novel method with less conservatism is developed to estimate the domain of attraction, which is radically different from the traditional method by means of contractive invariant set. Second, optimization problems constrained by the transformed linear matrix inequalities are established to acquire the maximum estimates of both the domain of attraction and average impulsive interval (AII), which are conveniently solved by the YALMIP toolbox in MATLAB software. Finally, a numerical simulation is rendered to demonstrate the effectiveness and advantages of the proposed theoretical results.},
  archive      = {J_TNNLS},
  author       = {Xiaoxiao Lv and Jinde Cao and Xiaodi Li and Yiping Luo},
  doi          = {10.1109/TNNLS.2021.3138997},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7170-7180},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local synchronization of directed lur’e networks with coupling delay via distributed impulsive control subject to actuator saturation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive lexical and semantic graphs for semisupervised
relation extraction. <em>TNNLS</em>, <em>34</em>(10), 7158–7169. (<a
href="https://doi.org/10.1109/TNNLS.2021.3138956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of relation extraction (RE) is hindered by the lack of sufficient labeled data. Semisupervised methods can offer to help hands with this problem by augmenting high-quality unlabeled samples into the training data. However, existing semisupervised RE methods either need a set of manually defined rules or rely on the classifier trained on the small labeled data, i.e., the former requires the heavy intervention of human knowledge, and the latter is bound to the number and the quality of the labeled data. In this article, we present a novel semisupervised RE method that involves small human efforts and is robust to the size of the initial set of labeled data. Specifically, we adopt only two simple rules to build the lexical and semantic graphs which connect the labeled samples with the unlabeled ones. In this way, the graphs are much easier to construct yet keep the ability to transfer knowledge from labeled samples to unlabeled ones. We then develop a graph interaction module to fully exploit the reference information in lexical and semantic graphs, which is used to jointly recognize the high-quality unlabeled samples with the classifier. We conduct extensive experimental results on two public datasets. The results demonstrate that our proposed method significantly outperforms the state-of-the-art baselines.},
  archive      = {J_TNNLS},
  author       = {Wanli Li and Tieyun Qian and Ming Zhong and Xu Chen},
  doi          = {10.1109/TNNLS.2021.3138956},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7158-7169},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interactive lexical and semantic graphs for semisupervised relation extraction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Action mapping: A reinforcement learning method for
constrained-input systems. <em>TNNLS</em>, <em>34</em>(10), 7145–7157.
(<a href="https://doi.org/10.1109/TNNLS.2021.3138924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing approaches to constrained-input optimal control problems mainly focus on systems with input saturation, whereas other constraints, such as combined inequality constraints and state-dependent constraints, are seldom discussed. In this article, a reinforcement learning (RL)-based algorithm is developed for constrained-input optimal control of discrete-time (DT) systems. The deterministic policy gradient (DPG) is introduced to iteratively search the optimal solution to the Hamilton–Jacobi–Bellman (HJB) equation. To deal with input constraints, an action mapping (AM) mechanism is proposed. The objective of this mechanism is to transform the exploration space from the subspace generated by the given inequality constraints to the standard Cartesian product space, which can be searched effectively by existing algorithms. By using the proposed architecture, the learned policy can output control signals satisfying the given constraints, and the original reward function can be kept unchanged. In our study, the convergence analysis is given. It is shown that the iterative algorithm is convergent to the optimal solution of the HJB equation. In addition, the continuity of the iterative estimated $Q$ -function is investigated. Two numerical examples are provided to demonstrate the effectiveness of our approach.},
  archive      = {J_TNNLS},
  author       = {Xin Yuan and Yuanda Wang and Jian Liu and Changyin Sun},
  doi          = {10.1109/TNNLS.2021.3138924},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7145-7157},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Action mapping: A reinforcement learning method for constrained-input systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and analysis of a self-adaptive zeroing neural
network for solving time-varying quadratic programming. <em>TNNLS</em>,
<em>34</em>(10), 7135–7144. (<a
href="https://doi.org/10.1109/TNNLS.2021.3138900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the time-varying quadratic programming (TVQP) problem more effectively, a new self-adaptive zeroing neural network (ZNN) is designed and analyzed in this article by using the Takagi–Sugeno fuzzy logic system (TSFLS) and thus called the Takagi–Sugeno (T-S) fuzzy ZNN (TSFZNN). Specifically, a multiple-input–single-output TSFLS is designed to generate a self-adaptive convergence factor to construct the TSFZNN model. In order to obtain finite- or predefined-time convergence, four novel activation functions (AFs) [namely, power-bi-sign AF (PBSAF), tanh-bi-sign AF (TBSAF), exp-bi-sign AF (EBSAF), and sinh-bi-sign AF (SBSAF)] are developed and applied in the TSFZNN model for solving the TVQP problem. Both theoretical proofs and experimental simulations show that the TSFZNN model using PBSAF or TBSAF has the property of converging in a finite time, and the TSFZNN model using EBSAF or SBSAF has the property of converging in a predefined time, which have superior convergence performance compared to the traditional ZNN model.},
  archive      = {J_TNNLS},
  author       = {Jianhua Dai and Xing Yang and Lin Xiao and Lei Jia and Xinwang Liu and Yaonan Wang},
  doi          = {10.1109/TNNLS.2021.3138900},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7135-7144},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design and analysis of a self-adaptive zeroing neural network for solving time-varying quadratic programming},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online deterministic annealing for classification and
clustering. <em>TNNLS</em>, <em>34</em>(10), 7125–7134. (<a
href="https://doi.org/10.1109/TNNLS.2021.3138676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inherent in virtually every iterative machine learning algorithm is the problem of hyperparameter tuning, which includes three major design parameters: 1) the complexity of the model, e.g., the number of neurons in a neural network; 2) the initial conditions, which heavily affect the behavior of the algorithm; and 3) the dissimilarity measure used to quantify its performance. We introduce an online prototype-based learning algorithm that can be viewed as a progressively growing competitive-learning neural network architecture for classification and clustering. The learning rule of the proposed approach is formulated as an online gradient-free stochastic approximation algorithm that solves a sequence of appropriately defined optimization problems, simulating an annealing process. The annealing nature of the algorithm contributes to avoiding poor local minima, offers robustness with respect to the initial conditions, and provides a means to progressively increase the complexity of the learning model, through an intuitive bifurcation phenomenon. The proposed approach is interpretable, requires minimal hyperparameter tuning, and allows online control over the performance-complexity tradeoff. Finally, we show that Bregman divergences appear naturally as a family of dissimilarity measures that play a central role in both the performance and the computational complexity of the learning algorithm.},
  archive      = {J_TNNLS},
  author       = {Christos N. Mavridis and John S. Baras},
  doi          = {10.1109/TNNLS.2021.3138676},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7125-7134},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online deterministic annealing for classification and clustering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-sensor fusion boolean bayesian filtering for
stochastic boolean networks. <em>TNNLS</em>, <em>34</em>(10), 7114–7124.
(<a href="https://doi.org/10.1109/TNNLS.2021.3138132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic Boolean networks (SBNs) take process noise into account, so it is better to fit the actual situation and has a wider application background than Boolean networks (BNs). However, the presence of noise influences us to estimate the real state of the system. To minimize the inaccuracies caused by the presence of noise, an optimal state estimation problem is studied in this article. The multi-sensor fusion Boolean Bayesian filtering is proposed and a recursive algorithm is provided to calculate the prior and posterior belief of system state by fusing multi-sensor measurements based on the algebraic form of the SBN and Bayesian law. Then, the optimal state estimator is obtained, which minimizes the mean-square estimation error. Finally, a simulation example is carried out to demonstrate the performance of the proposed methodology. It has been shown through the simulation experiment that it increases the confidence level of the state estimation and improves the estimation performance using multi-sensor fusion compared with using single sensor.},
  archive      = {J_TNNLS},
  author       = {Fangfei Li and Yang Tang},
  doi          = {10.1109/TNNLS.2021.3138132},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7114-7124},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-sensor fusion boolean bayesian filtering for stochastic boolean networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). SmartDeal: Remodeling deep network weights for efficient
inference and training. <em>TNNLS</em>, <em>34</em>(10), 7099–7113. (<a
href="https://doi.org/10.1109/TNNLS.2021.3138056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The record-breaking performance of deep neural networks (DNNs) comes with heavy parameter budgets, which leads to external dynamic random access memory (DRAM) for storage. The prohibitive energy of DRAM accesses makes it nontrivial for DNN deployment on resource-constrained devices, calling for minimizing the movements of weights and data in order to improve the energy efficiency. Driven by this critical bottleneck, we present SmartDeal, a hardware-friendly algorithm framework to trade higher-cost memory storage/access for lower-cost computation, in order to aggressively boost the storage and energy efficiency, for both DNN inference and training. The core technique of SmartDeal is a novel DNN weight matrix decomposition framework with respective structural constraints on each matrix factor, carefully crafted to unleash the hardware-aware efficiency potential. Specifically, we decompose each weight tensor as the product of a small basis matrix and a large structurally sparse coefficient matrix whose nonzero elements are readily quantized to the power-of-2. The resulting sparse and readily quantized DNNs enjoy greatly reduced energy consumption in data movement as well as weight storage, while incurring minimal overhead to recover the original weights thanks to the required sparse bit-operations and cost-favorable computations. Beyond inference, we take another leap to embrace energy-efficient training, by introducing several customized techniques to address the unique roadblocks arising in training while preserving the SmartDeal structures. We also design a dedicated hardware accelerator to fully utilize the new weight structure to improve the real energy efficiency and latency performance. We conduct experiments on both vision and language tasks, with nine models, four datasets, and three settings (inference-only, adaptation, and fine-tuning). Our extensive results show that 1) being applied to inference, SmartDeal achieves up to $2.44\times $ improvement in energy efficiency as evaluated using real hardware implementations and 2) being applied to training, SmartDeal can lead to $10.56\times $ and $4.48\times $ reduction in the storage and the training energy cost, respectively, with usually negligible accuracy loss, compared to state-of-the-art training baselines. Our source codes are available at: https://github.com/VITA-Group/SmartDeal .},
  archive      = {J_TNNLS},
  author       = {Xiaohan Chen and Yang Zhao and Yue Wang and Pengfei Xu and Haoran You and Chaojian Li and Yonggan Fu and Yingyan Lin and Zhangyang Wang},
  doi          = {10.1109/TNNLS.2021.3138056},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7099-7113},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SmartDeal: Remodeling deep network weights for efficient inference and training},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance improvement of active suspension constrained
system via neural network identification. <em>TNNLS</em>,
<em>34</em>(10), 7089–7098. (<a
href="https://doi.org/10.1109/TNNLS.2021.3137883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust adaptive control method for a certain type of quarter active suspension system (ASS) is proposed in this work. The constraint issue of ASS is put into consideration primarily. Due to the limitation of the traditional barrier Lyapunov functions (BLFs), the integral barrier Lyapunov function (iBLF) is introduced to exert direct constraints on state variables in each stage under the backstepping frame, and neural networks (NNs) are applied to identify those unknown functions. Then, an adaptive law based on the projection operator is defined to eliminate the influence caused by the actuator failure. It is widely known that only the vertical displacement and velocity constraints are not violated, can the ASSs become stable and secure. It can be ultimately confirmed that all signals in the closed-loop system are bounded, and the control goals are satisfied. Last but not least, the feasibility of the approach is illustrated directly through a contrast simulation example.},
  archive      = {J_TNNLS},
  author       = {Lei Liu and Changqi Zhu and Yan-Jun Liu and Rui Wang and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2021.3137883},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7089-7098},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Performance improvement of active suspension constrained system via neural network identification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering spatially correlated functional data with
multiple scalar covariates. <em>TNNLS</em>, <em>34</em>(10), 7074–7088.
(<a href="https://doi.org/10.1109/TNNLS.2021.3137795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a probabilistic model for clustering spatially correlated functional data with multiple scalar covariates. The motivating application is to partition the 29 provinces of the Chinese mainland into a few groups characterized by the epidemic severity of COVID-19, while the spatial dependence and effects of risk factors are considered. It can be regarded as an extension of mixture models, which allows different subsets of covariates to influence the component weights and the component densities by modeling the parameters of the mixture as functions of the covariates. In this way, provinces with similar spatial factors are a priori more likely to be clustered together. Posterior predictive inference in this model formalizes the desired prediction. Further, the identifiability of the proposed model is analyzed, and sufficient conditions to guarantee “generic” identifiability are provided. An $L_{1}$ -penalized estimator is developed to assist variable selection and robust estimation when the number of explanatory covariates is large. An efficient expectation-minimization algorithm is presented for parameter estimation. Simulation studies and real-data examples are presented to investigate the empirical performance of the proposed method. Finally, it is worth noting that the proposed model has a wide range of practical applications, e.g., health management, environmental science, ecological studies, and so on.},
  archive      = {J_TNNLS},
  author       = {Hui Wu and Yan-Fu Li},
  doi          = {10.1109/TNNLS.2021.3137795},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7074-7088},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Clustering spatially correlated functional data with multiple scalar covariates},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning-based model-free controller for
feedback stabilization of robotic systems. <em>TNNLS</em>,
<em>34</em>(10), 7059–7073. (<a
href="https://doi.org/10.1109/TNNLS.2021.3137548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a reinforcement learning (RL) algorithm for achieving model-free control of robotic applications. The RL functions are adapted with the least-square temporal difference (LSTD) learning algorithms to develop a model-free state feedback controller by establishing linear quadratic regulator (LQR) as a baseline controller. The classical least-square policy iteration technique is adapted to establish the boundary conditions for complexities incurred by the learning algorithm. Furthermore, the use of exact and approximate policy iterations estimates the parameters of the learning functions for a feedback policy. To assess the operation of the proposed controller, the trajectory tracking and balancing control problems of unmanned helicopters and balancer robotic applications are solved for real-time experiment. The results showed the robustness of the proposed approach in achieving trajectory tracking and balancing control.},
  archive      = {J_TNNLS},
  author       = {Rupam Singh and Bharat Bhushan},
  doi          = {10.1109/TNNLS.2021.3137548},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7059-7073},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning-based model-free controller for feedback stabilization of robotic systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive bipartite event-triggered time-varying output
formation tracking of heterogeneous linear multi-agent systems under
signed directed graph. <em>TNNLS</em>, <em>34</em>(10), 7049–7058. (<a
href="https://doi.org/10.1109/TNNLS.2021.3137393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the adaptive bipartite event-triggered time-varying output formation tracking for heterogeneous linear multi-agent systems (MASs) under signed directed communication topology. Both cooperative communication and antagonistic communication among agents are considered. The fully distributed bipartite compensator based on the novel composite event-triggered transmission mechanism is first put forward to estimate the state of the leader. Compared with the existing methods, our compensator can save communication resources using event-triggered transmission mechanism; is independent of the global information of the network graph; and is applicable for the signed directed graph. With the developed compensator, the distributed control protocol is designed to achieve the time-varying output formation tracking. Moreover, the case that the networked systems subject to external disturbances is also considered. To estimate the state of leader with disturbance, the fully distributed bipartite compensator based on an innovative composite event-triggered mechanism is presented. And the novel distributed control protocol is proposed to address the output formation tracking issue for linear MASs with heterogeneous dynamics and external disturbances. It is shown that the Zeno-behavior can be excluded in both transmission mechanisms. Finally, the effectiveness of the developed control methods is illustrated through three simulation examples.},
  archive      = {J_TNNLS},
  author       = {Yuliang Cai and Huaguang Zhang and Zhiyun Gao and Liu Yang and Qiang He},
  doi          = {10.1109/TNNLS.2021.3137393},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7049-7058},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive bipartite event-triggered time-varying output formation tracking of heterogeneous linear multi-agent systems under signed directed graph},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate time series prediction based on temporal change
information learning method. <em>TNNLS</em>, <em>34</em>(10), 7034–7048.
(<a href="https://doi.org/10.1109/TNNLS.2021.3137178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multivariate time series prediction tasks, the impact information of all nonpredictive time series on the predictive target series is difficult to be extracted at different time stages. Through the emphasis on optimal-related sequences in the target series, the deep learning model with the attention mechanism achieves a good predictive performance. However, temporal change information in the objective function and optimization algorithm is completely ignored in these models. To this end, a temporal change information learning (CIL) method is proposed in this article. First, mean absolute error (MAE) and mean squared error (MSE) losses are contained in the objective function to evaluate different amplitude errors. Meanwhile, the second-order difference technology is used in the correlation terms of the objective function to adaptively capture the impact of the abrupt and slow change information in each series on the target series. Second, the long short-term memory (LSTM) network with the transformation mechanism is used in the method so that temporal dependence information can be fully extracted (i.e., avoiding the supersaturation region). Third, to effectively obtain the optimal model parameters, the current and historical moment estimation information is adaptively memorized without the introduction of additional hyperparameters, and therefore, the acquisition ability of temporal change information in the error gradient flow is greatly enhanced by the proposed optimization algorithm. Finally, three datasets with different scales are used to verify the advantages of the CIL method in computational overhead and prediction effect.},
  archive      = {J_TNNLS},
  author       = {Wendong Zheng and Jun Hu},
  doi          = {10.1109/TNNLS.2021.3137178},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7034-7048},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multivariate time series prediction based on temporal change information learning method},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel data augmentation method based on CoralGAN for
prediction of part surface roughness. <em>TNNLS</em>, <em>34</em>(10),
7024–7033. (<a
href="https://doi.org/10.1109/TNNLS.2021.3137172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning networks can be applied to the field of intelligent prediction of part surface roughness. However, the surface roughness samples of parts have the problems of high collection cost, unbalanced categories, and complicated data distribution, which inevitably limit the application of deep learning network models in the field of intelligent prediction of part surface roughness. To solve these problems, this article proposes a novel data augmentation method based on CoralGAN for prediction of part surface roughness, which introduces the domain adaptive method deep coral function to help optimize the network parameters of the generator of generative adversarial network (GAN). Specifically, the vibration signal collected during processing is converted into frequency spectrum data and input into CoralGAN. The training of the generator is guided by coral loss, that is, the distance between the covariances of the real samples and generated samples features, not just the statistical consistency of the traditional GAN. Experiments have been carried out on the three-axis vertical machining center. Research shows that the proposed method can improve the prediction accuracy of part surface roughness to 95.5\%.},
  archive      = {J_TNNLS},
  author       = {Yongqing Wang and Mengmeng Niu and Kuo Liu and Mingrui Shen and Bo Qin and Honghui Wang},
  doi          = {10.1109/TNNLS.2021.3137172},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7024-7033},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel data augmentation method based on CoralGAN for prediction of part surface roughness},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MGEL: Multigrained representation analysis and ensemble
learning for text moderation. <em>TNNLS</em>, <em>34</em>(10),
7014–7023. (<a
href="https://doi.org/10.1109/TNNLS.2021.3137045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we describe our efforts in addressing two typical challenges involved in the popular text classification methods when they are applied to text moderation: the representation of multibyte characters and word obfuscations. Specifically, a multihot byte-level scheme is developed to significantly reduce the dimension of one-hot character-level encoding caused by the multiplicity of instance-scarce non-ASCII characters. In addition, we introduce a simple yet effective weighting approach for fusing n-gram features to empower the classical logistic regression. Surprisingly, it outperforms well-tuned representative neural networks greatly. As a continual effort toward text moderation, we endeavor to analyze the current state-of-the-art (SOTA) algorithm bidirectional encoder representations from transformers (BERT), which works well in context understanding but performs poorly on intentional word obfuscations. To resolve this crux, we then develop an enhanced variant and remedy this drawback by integrating byte and character decomposition. It advances the SOTA performance on the largest abusive language datasets as demonstrated by our comprehensive experiments. Our work offers a feasible and effective framework to tackle word obfuscations.},
  archive      = {J_TNNLS},
  author       = {Fei Tan and Changwei Hu and Yifan Hu and Kevin Yen and Zhi Wei and Aasish Pappu and Serim Park and Keqian Li},
  doi          = {10.1109/TNNLS.2021.3137045},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7014-7023},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MGEL: Multigrained representation analysis and ensemble learning for text moderation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time leak location of long-distance pipeline using
adaptive dynamic programming. <em>TNNLS</em>, <em>34</em>(10),
7004–7013. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional leak location methods, the position of the leak point is located through the time difference of pressure change points of both ends of the pipeline. The inaccurate estimation of pressure change points leads to the wrong leak location result. To address it, adaptive dynamic programming is proposed to solve the pipeline leak location problem in this article. First, a pipeline model is proposed to describe the pressure change along pipeline, which is utilized to reflect the iterative situation of the logarithmic form of pressure change. Then, under the Bellman optimality principle, a value iteration (VI) scheme is proposed to provide the optimal sequence of the nominal parameter and obtain the pipeline leak point. Furthermore, neural networks are built as the VI scheme structure to ensure the iterative performance of the proposed method. By transforming into the dynamic optimization problem, the proposed method adopts the estimation of the logarithmic form of pressure changes of both ends of the pipeline to locate the leak point, which avoids the wrong results caused by unclear pressure change points. Thus, it could be applied for real-time leak location of long-distance pipeline. Finally, the experiment cases are given to illustrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Xuguang Hu and Huaguang Zhang and Dazhong Ma and Rui Wang and Tianbiao Wang and Xiangpeng Xie},
  doi          = {10.1109/TNNLS.2021.3136939},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {7004-7013},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Real-time leak location of long-distance pipeline using adaptive dynamic programming},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of recurrent neural network-based methods in
computational physiology. <em>TNNLS</em>, <em>34</em>(10), 6983–7003.
(<a href="https://doi.org/10.1109/TNNLS.2022.3145365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence and machine learning techniques have progressed dramatically and become powerful tools required to solve complicated tasks, such as computer vision, speech recognition, and natural language processing. Since these techniques have provided promising and evident results in these fields, they emerged as valuable methods for applications in human physiology and healthcare. General physiological recordings are time-related expressions of bodily processes associated with health or morbidity. Sequence classification, anomaly detection, decision making, and future status prediction drive the learning algorithms to focus on the temporal pattern and model the nonstationary dynamics of the human body. These practical requirements give birth to the use of recurrent neural networks (RNNs), which offer a tractable solution in dealing with physiological time series and provide a way to understand complex time variations and dependencies. The primary objective of this article is to provide an overview of current applications of RNNs in the area of human physiology for automated prediction and diagnosis within different fields. Finally, we highlight some pathways of future RNN developments for human physiology.},
  archive      = {J_TNNLS},
  author       = {Shitong Mao and Ervin Sejdić},
  doi          = {10.1109/TNNLS.2022.3145365},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6983-7003},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A review of recurrent neural network-based methods in computational physiology},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of deep learning techniques for underwater image
classification. <em>TNNLS</em>, <em>34</em>(10), 6968–6982. (<a
href="https://doi.org/10.1109/TNNLS.2022.3143887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been an enormous interest in using deep learning to classify underwater images to identify various objects, such as fishes, plankton, coral reefs, seagrass, submarines, and gestures of sea divers. This classification is essential for measuring the water bodies’ health and quality and protecting the endangered species. Furthermore, it has applications in oceanography, marine economy and defense, environment protection, underwater exploration, and human-robot collaborative tasks. This article presents a survey of deep learning techniques for performing underwater image classification. We underscore the similarities and differences of several methods. We believe that underwater image classification is one of the killer application that would test the ultimate success of deep learning techniques. Toward realizing that goal, this survey seeks to inform researchers about state-of-the-art on deep learning on underwater images and also motivate them to push its frontiers forward.},
  archive      = {J_TNNLS},
  author       = {Sparsh Mittal and Srishti Srivastava and J. Phani Jayanth},
  doi          = {10.1109/TNNLS.2022.3143887},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6968-6982},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of deep learning techniques for underwater image classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). InOR-net: Incremental 3-d object recognition network for
point cloud representation. <em>TNNLS</em>, <em>34</em>(10), 6955–6967.
(<a href="https://doi.org/10.1109/TNNLS.2023.3247490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3-D object recognition has successfully become an appealing research topic in the real world. However, most existing recognition models unreasonably assume that the categories of 3-D objects cannot change over time in the real world. This unrealistic assumption may result in significant performance degradation for them to learn new classes of 3-D objects consecutively due to the catastrophic forgetting on old learned classes. Moreover, they cannot explore which 3-D geometric characteristics are essential to alleviate the catastrophic forgetting on old classes of 3-D objects. To tackle the above challenges, we develop a novel Incremental 3-D Object Recognition Network (i.e., InOR-Net), which could recognize new classes of 3-D objects continuously by overcoming the catastrophic forgetting on old classes. Specifically, category-guided geometric reasoning is proposed to reason local geometric structures with distinctive 3-D characteristics of each class by leveraging intrinsic category information. We then propose a novel critic-induced geometric attention mechanism to distinguish which 3-D geometric characteristics within each class are beneficial to overcome the catastrophic forgetting on old classes of 3-D objects while preventing the negative influence of useless 3-D characteristics. In addition, a dual adaptive fairness compensations’ strategy is designed to overcome the forgetting brought by class imbalance by compensating biased weights and predictions of the classifier. Comparison experiments verify the state-of-the-art performance of the proposed InOR-Net model on several public point cloud datasets.},
  archive      = {J_TNNLS},
  author       = {Jiahua Dong and Yang Cong and Gan Sun and Lixu Wang and Lingjuan Lyu and Jun Li and Ender Konukoglu},
  doi          = {10.1109/TNNLS.2023.3247490},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6955-6967},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {InOR-net: Incremental 3-D object recognition network for point cloud representation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HealthNet: A health progression network via heterogeneous
medical information fusion. <em>TNNLS</em>, <em>34</em>(10), 6940–6954.
(<a href="https://doi.org/10.1109/TNNLS.2022.3202305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous electronic health records (EHRs) offer valuable opportunities for understanding patients’ health status at different stages, namely health progression. Extracting the health progression patterns allows researchers to perform accurate predictive analysis of patient outcomes. However, most existing works on this task suffer from the following two limitations: 1) the diverse dependencies among heterogeneous medical entities are overlooked, which leads to the one-sided modeling of patients’ status and 2) the extraction granularity of patient’s health progression patterns is coarse, limiting the model’s ability to accurately infer the patient’s future status. To address these challenges, a pretrained Health progression network via heterogeneous medical information fusion, HealthNet, is proposed in this article. Specifically, a global heterogeneous graph in HealthNet is built to integrate heterogeneous medical entities and the dependencies among them. In addition, the proposed health progression network is designed to model hierarchical medical event sequences. By this method, the fine-grained health progression patterns of patients’ health can be captured. The experimental results on real disease datasets demonstrate that HealthNet outperforms the state-of-the-art models for both diagnosis prediction task and mortality prediction task.},
  archive      = {J_TNNLS},
  author       = {Fuqiang Yu and Lizhen Cui and Huanhuan Chen and Yiming Cao and Ning Liu and Weiming Huang and Yonghui Xu and Hua Lu},
  doi          = {10.1109/TNNLS.2022.3202305},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6940-6954},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HealthNet: A health progression network via heterogeneous medical information fusion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Building fake review detection model based on sentiment
intensity and PU learning. <em>TNNLS</em>, <em>34</em>(10), 6926–6939.
(<a href="https://doi.org/10.1109/TNNLS.2023.3234427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake review detection has the characteristics of huge stream data processing scale, unlimited data increment, dynamic change, and so on. However, the existing fake review detection methods mainly target limited and static review data. In addition, deceptive fake reviews have always been a difficult point in fake review detection due to their hidden and diverse characteristics. To solve the above problems, this article proposes a fake review detection model based on sentiment intensity and PU learning (SIPUL), which can continuously learn the prediction model from the constantly arriving streaming data. First, when the streaming data arrive, the sentiment intensity is introduced to divide the reviews into different subsets (i.e., strong sentiment set and weak sentiment set). Then, the initial positive and negative samples are extracted from the subset using the marking mechanism of selection completely at random (SCAR) and Spy technology. Second, building a semi-supervised positive-unlabeled (PU) learning detector based on the initial sample to detect fake reviews in the data stream iteratively. According to the detection results, the data of initial samples and the PU learning detector are continuously updated. Finally, the old data are continually deleted according to the historical record points, so that the training sample data are within a manageable size and prevent overfitting. Experimental results show that the model can effectively detect fake reviews, especially deceptive reviews.},
  archive      = {J_TNNLS},
  author       = {Zhang Shunxiang and Zhu Aoqiang and Zhu Guangli and Wei Zhongliang and Li KuanChing},
  doi          = {10.1109/TNNLS.2023.3234427},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6926-6939},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Building fake review detection model based on sentiment intensity and PU learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Bidirectional spatial-temporal adaptive transformer for
urban traffic flow forecasting. <em>TNNLS</em>, <em>34</em>(10),
6913–6925. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban traffic forecasting is the cornerstone of the intelligent transportation system (ITS). Existing methods focus on spatial-temporal dependency modeling, while two intrinsic properties of the traffic forecasting problem are overlooked. First, the complexity of diverse forecasting tasks is nonuniformly distributed across various spaces (e.g., suburb versus downtown) and times (e.g., rush hour versus off-peak). Second, the recollection of past traffic conditions is beneficial to the prediction of future traffic conditions. Based on these properties, we propose a bidirectional spatial-temporal adaptive transformer (Bi-STAT) for accurate traffic forecasting. Bi-STAT adopts an encoder–decoder architecture, where both the encoder and the decoder maintain a spatial-adaptive transformer and a temporal-adaptive transformer structure. Inspired by the first property, each transformer is designed to dynamically process the traffic streams according to their task complexities. Specifically, we realize this by the recurrent mechanism with a novel dynamic halting module (DHM). Each transformer performs iterative computation with shared parameters until DHM emits a stopping signal. Motivated by the second property, Bi-STAT utilizes one decoder to perform the present $\rightarrow $ past recollection task and the other decoder to perform the present $\rightarrow $ future prediction task. The recollection task supplies complementary information to assist and regularize the prediction task for a better generalization. Through extensive experiments, we show the effectiveness of each module in Bi-STAT and demonstrate the superiority of Bi-STAT over the state-of-the-art baselines on four benchmark datasets. The code is available at https://github.com/chenchl19941118/Bi-STAT.git .},
  archive      = {J_TNNLS},
  author       = {Changlu Chen and Yanbin Liu and Ling Chen and Chengqi Zhang},
  doi          = {10.1109/TNNLS.2022.3183903},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6913-6925},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bidirectional spatial-temporal adaptive transformer for urban traffic flow forecasting},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interval dominance-based feature selection for
interval-valued ordered data. <em>TNNLS</em>, <em>34</em>(10),
6898–6912. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dominance-based rough approximation discovers inconsistencies from ordered criteria and satisfies the requirement of the dominance principle between single-valued domains of condition attributes and decision classes. When the ordered decision system (ODS) is no longer single-valued, how to utilize the dominance principle to deal with multivalued ordered data is a promising research direction, and it is the most challenging step to design a feature selection algorithm in interval-valued ODS (IV-ODS). In this article, we first present novel thresholds of interval dominance degree (IDD) and interval overlap degree (IOD) between interval values to make the dominance principle applicable to an IV-ODS, and then, the interval-valued dominance relation in the IV-ODS is constructed by utilizing the above two developed parameters. Based on the proposed interval-valued dominance relation, the interval-valued dominance-based rough set approach (IV-DRSA) and their corresponding properties are investigated. Moreover, the interval dominance-based feature selection rules based on IV-DRSA are provided, and the relevant algorithms for deriving the interval-valued dominance relation and the feature selection methods are established in IV-ODS. To illustrate the effectiveness of the parameters variation on feature selection rules, experimental evaluation is performed using 12 datasets coming from the University of California-Irvine (UCI) repository.},
  archive      = {J_TNNLS},
  author       = {Wentao Li and Haoxiang Zhou and Weihua Xu and Xi-Zhao Wang and Witold Pedrycz},
  doi          = {10.1109/TNNLS.2022.3184120},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6898-6912},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interval dominance-based feature selection for interval-valued ordered data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty estimation with neural processes for
meta-continual learning. <em>TNNLS</em>, <em>34</em>(10), 6887–6897. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to evaluate uncertainties in evolving data streams has become equally, if not more, crucial than building a static predictor. For instance, during the pandemic, a model should consider possible uncertainties such as governmental policies, meteorological features, and vaccination schedules. Neural process families (NPFs) have recently shone a light on predicting such uncertainties by bridging Gaussian processes (GPs) and neural networks (NNs). Their abilities to output average predictions and the acceptable variances, i.e., uncertainties, made them suitable for predictions with insufficient data, such as meta-learning or few-shot learning. However, existing models have not addressed continual learning which imposes a stricter constraint on the data access. Regarding this, we introduce a member meta-continual learning with neural process (MCLNP) for uncertainty estimation. We enable two levels of uncertainty estimations: the local uncertainty on certain points and the global uncertainty $p(\boldsymbol {z})$ that represents the function evolution in dynamic environments. To facilitate continual learning, we hypothesize that the previous knowledge can be applied to the current task, hence adopt a coreset as a memory buffer to alleviate catastrophic forgetting. The relationships between the degree of global uncertainties with the intratask diversity and model complexity are discussed. We have estimated prediction uncertainties with multiple evolving types including abrupt/gradual/recurrent shifts. The applications encompass meta-continual learning in the 1-D, 2-D datasets, and a novel spatial–temporal COVID dataset. The results show that our method outperforms the baselines on the likelihood and can rebound quickly even for heavily evolved data streams.},
  archive      = {J_TNNLS},
  author       = {Xuesong Wang and Lina Yao and Xianzhi Wang and Hye-Young Paik and Sen Wang},
  doi          = {10.1109/TNNLS.2022.3215633},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6887-6897},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Uncertainty estimation with neural processes for meta-continual learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Difficult novel class detection in semisupervised streaming
data. <em>TNNLS</em>, <em>34</em>(10), 6872–6886. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming data mining can be applied in many practical applications, such as social media, market analysis, and sensor networks. Most previous efforts assume that all training instances except for the novel class have been completely labeled for novel class detection in streaming data. However, a more realistic situation is that only a few instances in the data stream are labeled. In addition, most existing algorithms are potentially dependent on the strong cohesion between known classes or the greater separation between novel class and known classes in the feature space. Unfortunately, this potential dependence is usually not an inherent characteristic of streaming data. Therefore, to classify data streams and detect novel classes, the proposed algorithm should satisfy: 1) it can handle any degree of separation between novel class and known classes (both easy and difficult novel class detection) and 2) it can use limited labeled instances to build algorithm models. In this article, we tackle these issues by a new framework called semisupervised streaming learning for difficult novel class detection (SSLDN), which consists of three major components: an effective novel class detector based on random trees, a classifier by using the information of nearest neighbors, and an efficient updating process. Empirical studies on several datasets validate that SSLDN can accurately handle different degrees of separation between the novel and known classes in semisupervised streaming data.},
  archive      = {J_TNNLS},
  author       = {Peng Zhou and Ni Wang and Shu Zhao and Yanping Zhang and Xindong Wu},
  doi          = {10.1109/TNNLS.2022.3213682},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6872-6886},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Difficult novel class detection in semisupervised streaming data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor-empowered adaptive learning for few-shot streaming
tasks. <em>TNNLS</em>, <em>34</em>(10), 6861–6871. (<a
href="https://doi.org/10.1109/TNNLS.2022.3227267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various stream learning methods are emerging in an endless stream to provide a wealth of solutions for artificial intelligence in streaming data scenarios. However, when each data stream is oriented to a different target space, it forces stream learning approaches oriented to the same task to be no longer applicable. Due to inconsistent target spaces for different tasks, the previous approaches fail on the new streaming tasks or it is impracticable to be trained from scratch with few labeled samples at the beginning. To this end, we have proposed an adaptive learning scheme for few-shot streaming tasks with the contributions of tensor and meta-learning. This adaptive scheme is conducive to mitigating the domain shift when a new task has few labeled samples. We elaborate a novel tensor-empowered attention mechanism derived from nonlocal neural networks, which enables to capture long-range dependency and preserve the high-dimensional structure to refine the global features of streaming tasks. Furthermore, we develop a fine-grained similarity computing approach, which is prone to better characterize the difference across few-shot streaming tasks. To show the superiority of our method, we have carried out extensive experiments on three popular few-shot datasets to simulate streaming tasks and evaluate the performance of adaptation. The results show that our proposed method has achieved competitive performance for few-shot streaming tasks compared with the state-of-the-art (SOTA).},
  archive      = {J_TNNLS},
  author       = {Bocheng Ren and Laurence T. Yang and Qingchen Zhang and Jun Feng and Xin Nie},
  doi          = {10.1109/TNNLS.2022.3227267},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6861-6871},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tensor-empowered adaptive learning for few-shot streaming tasks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiagent reinforcement learning with heterogeneous graph
attention network. <em>TNNLS</em>, <em>34</em>(10), 6851–6860. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recent research on multiagent reinforcement learning (MARL) has explored how to deploy cooperative policies for homogeneous agents. However, realistic multiagent environments may contain heterogeneous agents that have different attributes or tasks. The heterogeneity of the agents and the diversity of relationships cause the learning of policy excessively tough. To tackle this difficulty, we present a novel method that employs a heterogeneous graph attention network to model the relationships between heterogeneous agents. The proposed method can generate an integrated feature representation for each agent by hierarchically aggregating latent feature information of neighbor agents, with the importance of the agent level and the relationship level being entirely considered. The method is agnostic to specific MARL methods and can be flexibly integrated with diverse value decomposition methods. We conduct experiments in predator–prey and StarCraft Multiagent Challenge (SMAC) environments, and the empirical results demonstrate that the performance of our method is superior to existing methods in several heterogeneous scenarios.},
  archive      = {J_TNNLS},
  author       = {Wei Du and Shifei Ding and Chenglong Zhang and Zhongzhi Shi},
  doi          = {10.1109/TNNLS.2022.3215774},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6851-6860},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiagent reinforcement learning with heterogeneous graph attention network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous cross domain adaptation under extreme label
scarcity. <em>TNNLS</em>, <em>34</em>(10), 6839–6850. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cross domain multistream classification is a challenging problem calling for fast domain adaptations to handle different but related streams in never-ending and rapidly changing environments. Notwithstanding that existing multistream classifiers assume no labeled samples in the target stream, they still incur expensive labeling costs since they require fully labeled samples of the source stream. This article aims to attack the problem of extreme label shortage in the cross domain multistream classification problems where only very few labeled samples of the source stream are provided before process runs. Our solution, namely, Learning Streaming Process from Partial Ground Truth (LEOPARD), is built upon a flexible deep clustering network where its hidden nodes, layers, and clusters are added and removed dynamically with respect to varying data distributions. A deep clustering strategy is underpinned by a simultaneous feature learning and clustering technique leading to clustering-friendly latent spaces. A domain adaptation strategy relies on the adversarial domain adaptation technique where a feature extractor is trained to fool a domain classifier by classifying source and target streams. Our numerical study demonstrates the efficacy of LEOPARD where it delivers improved performances compared to prominent algorithms in 15 of 24 cases. Source codes of LEOPARD are shared in https://github.com/wengweng001/LEOPARD.git to enable further study.},
  archive      = {J_TNNLS},
  author       = {Weiwei Weng and Mahardhika Pratama and Choiru Za’in and Marcus de Carvalho and Rakaraddi Appan and Andri Ashfahani and Edward Yapp Kien Yee},
  doi          = {10.1109/TNNLS.2022.3183356},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6839-6850},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Autonomous cross domain adaptation under extreme label scarcity},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multicomponent adversarial domain adaptation: A general
framework. <em>TNNLS</em>, <em>34</em>(10), 6824–6838. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) aims to transfer knowledge from one source domain to another different but related target domain. The mainstream approach embeds adversarial learning into deep neural networks (DNNs) to either learn domain-invariant features to reduce the domain discrepancy or generate data to fill in the domain gap. However, these adversarial DA (ADA) approaches mainly consider the domain-level data distributions, while ignoring the differences among components contained in different domains. Therefore, components that are not related to the target domain are not filtered out. This can cause a negative transfer. In addition, it is difficult to make full use of the relevant components between the source and target domains to enhance DA. To address these limitations, we propose a general two-stage framework, named multicomponent ADA (MCADA). This framework trains the target model by first learning a domain-level model and then fine-tuning that model at the component-level. In particular, MCADA constructs a bipartite graph to find the most relevant component in the source domain for each component in the target domain. Since the nonrelevant components are filtered out for each target component, fine-tuning the domain-level model can enhance positive transfer. Extensive experiments on several real-world datasets demonstrate that MCADA has significant advantages over state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Chang’an Yi and Haotian Chen and Yonghui Xu and Huanhuan Chen and Yong Liu and Haishu Tan and Yuguang Yan and Han Yu},
  doi          = {10.1109/TNNLS.2023.3270359},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6824-6838},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multicomponent adversarial domain adaptation: A general framework},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IWDA: Importance weighting for drift adaptation in streaming
supervised learning problems. <em>TNNLS</em>, <em>34</em>(10),
6813–6823. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distribution drift is an important issue for practical applications of machine learning (ML). In particular, in streaming ML, the data distribution may change over time, yielding the problem of concept drift, which affects the performance of learners trained with outdated data. In this article, we focus on supervised problems in an online nonstationary setting, introducing a novel learner-agnostic algorithm for drift adaptation, namely importance weighting for drift adaptation (IWDA), with the goal of performing efficient retraining of the learner when drift is detected. IWDA incrementally estimates the joint probability density of input and target for the incoming data and, as soon as drift is detected, retrains the learner using importance-weighted empirical risk minimization. The importance weights are computed for all the samples observed so far, employing the estimated densities, thus, using all available information efficiently. After presenting our approach, we provide a theoretical analysis in the abrupt drift setting. Finally, we present numerical simulations that illustrate how IWDA competes and often outperforms state-of-the-art stream learning techniques, including adaptive ensemble methods, on both synthetic and real-world data benchmarks.},
  archive      = {J_TNNLS},
  author       = {Filippo Fedeli and Alberto Maria Metelli and Francesco Trovò and Marcello Restelli},
  doi          = {10.1109/TNNLS.2023.3265524},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6813-6823},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IWDA: Importance weighting for drift adaptation in streaming supervised learning problems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-way concept-cognitive learning via concept movement
viewpoint. <em>TNNLS</em>, <em>34</em>(10), 6798–6812. (<a
href="https://doi.org/10.1109/TNNLS.2023.3235800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation and learning of concepts are critical problems in data science and cognitive science. However, the existing research about concept learning has one prevalent disadvantage: incomplete and complex cognitive. Meanwhile, as a practical mathematical tool for concept representation and concept learning, two-way learning (2WL) also has some issues leading to the stagnation of its related research: the concept can only learn from specific information granules and lacks a concept evolution mechanism. To overcome these challenges, we propose the two-way concept-cognitive learning (TCCL) method for enhancing the flexibility and evolution ability of 2WL for concept learning. We first analyze the fundamental relationship between two-way granule concepts in the cognitive system to build a novel cognitive mechanism. Furthermore, the movement three-way decision (M-3WD) method is introduced to 2WL to study the concept evolution mechanism via the concept movement viewpoint. Unlike the existing 2WL method, the primary consideration of TCCL is two-way concept evolution rather than information granules transformation. Finally, to interpret and help understand TCCL, an example analysis and some experiments on various datasets are carried out to demonstrate our method’s effectiveness. The results show that TCCL is more flexible and less time-consuming than 2WL, and meanwhile, TCCL can also learn the same concept as the latter method in concept learning. In addition, from the perspective of concept learning ability, TCCL is more generalization of concepts than the granule concept cognitive learning model (CCLM).},
  archive      = {J_TNNLS},
  author       = {Weihua Xu and Doudou Guo and Jusheng Mi and Yuhua Qian and Keyin Zheng and Weiping Ding},
  doi          = {10.1109/TNNLS.2023.3235800},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6798-6812},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two-way concept-cognitive learning via concept movement viewpoint},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal batch-wise change detection. <em>TNNLS</em>,
<em>34</em>(10), 6783–6797. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of detecting distribution changes in a novel batch-wise and multimodal setup. This setup is characterized by a stationary condition where batches are drawn from potentially different modalities among a set of distributions in $\mathbb {R}^{d}$ represented in the training set. Existing change detection (CD) algorithms assume that there is a unique—possibly multipeaked—distribution characterizing stationary conditions, and in batch-wise multimodal context exhibit either low detection power or poor control of false positives. We present MultiModal QuantTree (MMQT), a novel CD algorithm that uses a single histogram to model the batch-wise multimodal stationary conditions. During testing, MMQT automatically identifies which modality has generated the incoming batch and detects changes by means of a modality-specific statistic. We leverage the theoretical properties of QuantTree to: 1) automatically estimate the number of modalities in a training set and 2) derive a principled calibration procedure that guarantees false-positive control. Our experiments show that MMQT achieves high detection power and accurate control over false positives in synthetic and real-world multimodal CD problems. Moreover, we show the potential of MMQT in Stream Learning applications, where it proves effective at detecting concept drifts and the emergence of novel classes by solely monitoring the input distribution.},
  archive      = {J_TNNLS},
  author       = {Diego Stucchi and Luca Magri and Diego Carrera and Giacomo Boracchi},
  doi          = {10.1109/TNNLS.2023.3294846},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6783-6797},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multimodal batch-wise change detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptation for automated drift detection in
electromechanical machine monitoring. <em>TNNLS</em>, <em>34</em>(10),
6768–6782. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical machine learning applications for streaming data can involve concept drift (the change in statistical properties of data over time), one-shot or few-shot learning (starting with only one or a few examples for each class), a scarcity of representative training data, and extreme verification latency (only the initial dataset has ground-truth labels). This work presents a framework for organizing signal processing and machine learning techniques to provide adaptive classification and drift detection. Nonintrusive load monitoring (NILM) serves as an ideal case study, as modern sensing solutions provide a wellspring of electromechanical data sources. There is a lack of training datasets that generalize across different load and fault scenarios. Accordingly, training must be accomplished with a limited set of data when deploying a NILM to a new power system. Also, loads can exhibit concept drift over time either due to faults or normal variation. NILM field data is used as an illustrative case study to demonstrate the proposed framework for adaptation and drift tracking.},
  archive      = {J_TNNLS},
  author       = {Daisy H. Green and Aaron W. Langham and Rebecca A. Agustin and Devin W. Quinn and Steven B. Leeb},
  doi          = {10.1109/TNNLS.2022.3184011},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6768-6782},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptation for automated drift detection in electromechanical machine monitoring},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online extra trees regressor. <em>TNNLS</em>,
<em>34</em>(10), 6755–6767. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data production has followed an increased growth in the last years, to the point that traditional or batch machine-learning (ML) algorithms cannot cope with the sheer volume of generated data. Stream or online ML presents itself as a viable solution to deal with the dynamic nature of streaming data. Besides coping with the inherent challenges of streaming data, online ML solutions must be accurate, fast, and bear a reduced memory footprint. We propose a new decision tree-based ensemble algorithm for online ML regression named online extra trees (OXT). Our proposal takes inspiration from the batch learning extra trees (XT) algorithm, a popular and faster alternative to random forest (RF). While speed and memory costs might not be a central concern in most batch applications, they become crucial in data stream data learning. Our proposal combines subbagging (sampling without replacement), random tree split points, and model trees to deliver competitive prediction errors and reduced computational costs. Throughout an extensive experimental evaluation comprising 22 real-world and synthetic datasets, we compare OXT against the state-of-the-art adaptive RF (ARF) and other incremental regressors. OXT is generally more accurate than its competitors while running significantly faster than ARF and expending significantly less memory.},
  archive      = {J_TNNLS},
  author       = {Saulo Martiello Mastelini and Felipe Kenji Nakano and Celine Vens and André Carlos Ponce de Leon Ferreira de Carvalho},
  doi          = {10.1109/TNNLS.2022.3212859},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6755-6767},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online extra trees regressor},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection in the data stream based on incremental
markov boundary learning. <em>TNNLS</em>, <em>34</em>(10), 6740–6754.
(<a href="https://doi.org/10.1109/TNNLS.2023.3249767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the proliferation of techniques for streaming data mining to meet the demands of many real-time systems, where high-dimensional streaming data are generated at high speed, increasing the burden on both hardware and software. Some feature selection algorithms for streaming data are proposed to tackle this issue. However, these algorithms do not consider the distribution shift due to nonstationary scenarios, leading to performance degradation when the underlying distribution changes in the data stream. To solve this problem, this article investigates feature selection in streaming data through incremental Markov boundary (MB) learning and proposes a novel algorithm. Different from existing algorithms focusing on prediction performance on off-line data, the MB is learned by analyzing conditional dependence/independence in data, which uncovers the underlying mechanism and is naturally more robust against the distribution shift. To learn MB in the data stream, the proposal transforms the learned information in previous data blocks to prior knowledge and employs them to assist MB discovery in current data blocks, where the likelihood of distribution shift and reliability of conditional independence test are monitored to avoid the negative impact from invalid prior information. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Xingyu Wu and Bingbing Jiang and Xiangyu Wang and Taiyu Ban and Huanhuan Chen},
  doi          = {10.1109/TNNLS.2023.3249767},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6740-6754},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feature selection in the data stream based on incremental markov boundary learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online passive-aggressive active learning for trapezoidal
data streams. <em>TNNLS</em>, <em>34</em>(10), 6725–6739. (<a
href="https://doi.org/10.1109/TNNLS.2022.3178880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea of combining the active query strategy and the passive-aggressive (PA) update strategy in online learning can be credited to the PA active (PAA) algorithm, which has proven to be effective in learning linear classifiers from datasets with a fixed feature space. We propose a novel family of online active learning algorithms, named PAA learning for trapezoidal data streams (PAATS) and multiclass PAATS (MPAATS) (and their variants), for binary and multiclass online classification tasks on trapezoidal data streams where the feature space may expand over time. Under the context of an ever-changing feature space, we provide the theoretical analysis of the mistake bounds for both PAATS and MPAATS. Our experiments on a wide variety of benchmark datasets have confirm that the combination of the instance-regulated active query strategy and the PA update strategy is much more effective in learning from trapezoidal data streams. We have also compared PAATS with online learning with streaming features (OLSF)—the state-of-the-art approach in learning linear classifiers from trapezoidal data streams. PAATS could achieve much better classification accuracy, especially for large-scale real-world data streams.},
  archive      = {J_TNNLS},
  author       = {Yanfang Liu and Xiaocong Fan and Wenbin Li and Yang Gao},
  doi          = {10.1109/TNNLS.2022.3178880},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6725-6739},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online passive-aggressive active learning for trapezoidal data streams},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). An online active broad learning approach for real-time
safety assessment of dynamic systems in nonstationary environments.
<em>TNNLS</em>, <em>34</em>(10), 6714–6724. (<a
href="https://doi.org/10.1109/TNNLS.2022.3222265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time safety assessment of the complex dynamic systems in nonstationary environments is of great significance for avoiding the potential hazards. In this case, the update procedure with high assessment accuracy and training speed is crucial and meaningful in the dynamic streaming setting. Generally, the performance of most online learning approaches will be negatively affected by limited annotated samples in such a setting. Moreover, the time cost of advanced conventional methods with retaining procedures is relatively high, constraining their practicality. In this article, a novel online active broad learning approach, termed OABL, is proposed. In detail, the effectiveness of the broad learning system in the framework of online active learning is first revealed and verified. A reasonable dynamic asymmetric query strategy is then designed with a limited annotation budget to actively annotate the relatively valuable samples, which is beneficial to mitigating the negative effects of class imbalance. In this context, the advantage of the human-in-the-loop characteristic is also effectively used to control the evolution direction of the learner during the incremental update, which makes it better able to adapt to complex and nonstationary environments. Several related experiments are conducted with the realistic data of JiaoLong deep-sea manned submersible. Results show the effectiveness and practicality of the proposal compared with the existing advanced approaches.},
  archive      = {J_TNNLS},
  author       = {Zeyi Liu and Yi Zhang and Zhongjun Ding and Xiao He},
  doi          = {10.1109/TNNLS.2022.3222265},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6714-6724},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An online active broad learning approach for real-time safety assessment of dynamic systems in nonstationary environments},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A multilayer framework for online metric learning.
<em>TNNLS</em>, <em>34</em>(10), 6701–6713. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online metric learning (OML) has been widely applied in classification and retrieval. It can automatically learn a suitable metric from data by restricting similar instances to be separated from dissimilar instances with a given margin. However, the existing OML algorithms have limited performance in real-world classifications, especially, when data distributions are complex. To this end, this article proposes a multilayer framework for OML to capture the nonlinear similarities among instances. Different from the traditional OML, which can only learn one metric space, the proposed multilayer OML (MLOML) takes an OML algorithm as a metric layer and learns multiple hierarchical metric spaces, where each metric layer follows a nonlinear layer for the complicated data distribution. Moreover, the forward propagation (FP) strategy and backward propagation (BP) strategy are employed to train the hierarchical metric layers. To build a metric layer of the proposed MLOML, a new Mahalanobis-based OML (MOML) algorithm is presented based on the passive-aggressive strategy and one-pass triplet construction strategy. Furthermore, in a progressively and nonlinearly learning way, MLOML has a stronger learning ability than traditional OML in the case of limited available training data. To make the learning process more explainable and theoretically guaranteed, theoretical analysis is provided. The proposed MLOML enjoys several nice properties, indeed learns a metric progressively, and performs better on the benchmark datasets. Extensive experiments with different settings have been conducted to verify these properties of the proposed MLOML.},
  archive      = {J_TNNLS},
  author       = {Wenbin Li and Yanfang Liu and Jing Huo and Yinghuan Shi and Yang Gao and Lei Wang and Jiebo Luo},
  doi          = {10.1109/TNNLS.2022.3213511},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6701-6713},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A multilayer framework for online metric learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Incremental cluster validity index-guided online learning
for performance and robustness to presentation order. <em>TNNLS</em>,
<em>34</em>(10), 6686–6700. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In streaming data applications, the incoming samples are processed and discarded, and therefore, intelligent decision-making is crucial for the performance of lifelong learning systems. In addition, the order in which the samples arrive may heavily affect the performance of incremental learners. The recently introduced incremental cluster validity indices (iCVIs) provide valuable aid in addressing such class of problems. Their primary use case has been cluster quality monitoring; nonetheless, they have been recently integrated in a streaming clustering method. In this context, the work presented, here, introduces the first adaptive resonance theory (ART)-based model that uses iCVIs for unsupervised and semi-supervised online learning. Moreover, it shows how to use iCVIs to regulate ART vigilance via an iCVI-based match tracking mechanism. The model achieves improved accuracy and robustness to ordering effects by integrating an online iCVI module as module B of a topological ART predictive mapping (TopoARTMAP)—thereby being named iCVI-TopoARTMAP—and using iCVI-driven postprocessing heuristics at the end of each learning step. The online iCVI module provides assignments of input samples to clusters at each iteration in accordance to any of the several iCVIs. The iCVI-TopoARTMAP maintains useful properties shared by the ART predictive mapping (ARTMAP) models, such as stability, immunity to catastrophic forgetting, and the many-to-one mapping capability via the map field module. The performance and robustness to the presentation order of iCVI-TopoARTMAP were evaluated via experiments with synthetic and real-world datasets.},
  archive      = {J_TNNLS},
  author       = {Leonardo Enzo Brito da Silva and Nagasharath Rayapati and Donald C. Wunsch},
  doi          = {10.1109/TNNLS.2022.3212345},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6686-6700},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Incremental cluster validity index-guided online learning for performance and robustness to presentation order},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Special issue on stream learning.
<em>TNNLS</em>, <em>34</em>(10), 6683–6685. (<a
href="https://doi.org/10.1109/TNNLS.2023.3304146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, learning from streaming data, commonly known as stream learning, has enjoyed tremendous growth and shown a wealth of development at both the conceptual and application levels. Stream learning is highly visible in both the machine learning and data science fields and has become a hot new direction in research. Advancements in stream learning include learning with concept drift detection, that includes whether a drift has occurred; understanding where, when, and how a drift occurs; adaptation by actively or passively updating models; and online learning, active learning, incremental learning, and reinforcement learning in data streaming situations.},
  archive      = {J_TNNLS},
  author       = {Jie Lu and Joao Gama and Xin Yao and Leandro Minku},
  doi          = {10.1109/TNNLS.2023.3304146},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6683-6685},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial: Special issue on stream learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comments on “convergence analysis of adaptive exponential
functional link network” [feb 21 882-891]. <em>TNNLS</em>,
<em>34</em>(9), 6677–6678. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is to comment on the derivation of the weight-update stability of in-parameter-linear nonlinear learning system with the gradient descent learning rule in the above article. Our comments are not to disqualify the commented article’s whole contribution; however, the issues should be pointed out to avoid their proliferation.},
  archive      = {J_TNNLS},
  author       = {Ivo Bukovsky and Gejza Dohnal and Noriyasu Homma},
  doi          = {10.1109/TNNLS.2021.3123540},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6677-6678},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Comments on “Convergence analysis of adaptive exponential functional link network” [Feb 21 882-891]},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Iterative learning control of constrained systems with
varying trial lengths under alignment condition. <em>TNNLS</em>,
<em>34</em>(9), 6670–6676. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief is concerned with iterative learning control (ILC) of constrained multi-input multi-output (MIMO) nonlinear systems under the state alignment condition with varying trial lengths. A modified reference trajectory is constructed to meet the alignment condition by adjusting the reference trajectory to be spatially closed. Resorting to the barrier composite energy function (BCEF) approach, an adaptive ILC scheme is built to guarantee the bounded convergence of the resultant closed-loop system. Illustrative examples are presented to verify the validity of the proposed iteration scheme.},
  archive      = {J_TNNLS},
  author       = {Mouquan Shen and Xingzheng Wu and Ju H. Park and Yang Yi and Yonghui Sun},
  doi          = {10.1109/TNNLS.2021.3135504},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6670-6676},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative learning control of constrained systems with varying trial lengths under alignment condition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Observer-based event-triggered adaptive control for
nonlinear multiagent systems with unknown states and disturbances.
<em>TNNLS</em>, <em>34</em>(9), 6663–6669. (<a
href="https://doi.org/10.1109/TNNLS.2021.3133440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on radial basis function neural networks (RBF NNs) and backstepping techniques, this brief considers the consensus tracking problem for nonlinear semi-strict-feedback multiagent systems with unknown states and disturbances. The adaptive event-triggered control scheme is introduced to decrease the update times of the controller so as to save the limited communication resources. To detect the unknown state, external disturbance, and reduce calculation workload, the state observer and disturbance observer as well as the first-order filter are first jointly constructed. It is shown that all the output signals of followers can uniformly track the reference signal of the leader and all the error signals are uniformly bounded. A simulation example is carried out to further prove the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Ning Pang and Xin Wang and Ziming Wang},
  doi          = {10.1109/TNNLS.2021.3133440},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6663-6669},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based event-triggered adaptive control for nonlinear multiagent systems with unknown states and disturbances},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). QC_SANE: Robust control in DRL using quantile critic with
spiking actor and normalized ensemble. <em>TNNLS</em>, <em>34</em>(9),
6656–6662. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently introduced deep reinforcement learning (DRL) techniques in discrete-time have resulted in significant advances in online games, robotics, and so on. Inspired from recent developments, we have proposed an approach referred to as Quantile Critic with Spiking Actor and Normalized Ensemble (QC_SANE) for continuous control problems, which uses quantile loss to train critic and a spiking neural network (NN) to train an ensemble of actors. The NN does an internal normalization using a scaled exponential linear unit (SELU) activation function and ensures robustness. The empirical study on multijoint dynamics with contact (MuJoCo)-based environments shows improved training and test results than the state-of-the-art approach: population coded spiking actor network (PopSAN).},
  archive      = {J_TNNLS},
  author       = {Surbhi Gupta and Gaurav Singal and Deepak Garg and Sarangapani Jagannathan},
  doi          = {10.1109/TNNLS.2021.3129525},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6656-6662},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {QC_SANE: Robust control in DRL using quantile critic with spiking actor and normalized ensemble},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Learning deep blind quality assessment for cartoon images.
<em>TNNLS</em>, <em>34</em>(9), 6650–6655. (<a
href="https://doi.org/10.1109/TNNLS.2021.3127720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the cartoon industry has developed rapidly in recent years, few studies pay special attention to cartoon image quality assessment (IQA). Unfortunately, applying blind natural IQA algorithms directly to cartoons often leads to inconsistent results with subjective visual perception. Hence, this brief proposes a blind cartoon IQA method based on convolutional neural networks (CNNs). Note that training a robust CNN depends on manually labeled training sets. However, for a large number of cartoon images, it is very time-consuming and costly to manually generate enough mean opinion scores (MOSs). Therefore, this brief first proposes a full reference (FR) cartoon IQA metric based on cartoon-texture decomposition and then uses the estimated FR index to guide the no-reference IQA network. Moreover, in order to improve the robustness of the proposed network, a large-scale dataset is established in the training stage, and a stochastic degradation strategy is presented, which randomly implements different degradations with random parameters. Experimental results on both synthetic and real-world cartoon image datasets demonstrate the effectiveness and robustness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Yuan Chen and Yang Zhao and Li Cao and Wei Jia and Xiaoping Liu},
  doi          = {10.1109/TNNLS.2021.3127720},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6650-6655},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning deep blind quality assessment for cartoon images},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density-based distance preserving graph: Theoretical and
practical analyses. <em>TNNLS</em>, <em>34</em>(9), 6642–6649. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief aims to provide theoretical guarantee and practical guidance on constructing a type of graphs from input data via distance preserving criterion. Unlike the graphs constructed by other methods, the targeted graphs are hidden through estimating a density function of latent variables such that the pairwise distances in both the input space and the latent space are retained, and they have been successfully applied to various learning scenarios. However, previous work heuristically treated the multipliers in the dual as the graph weights, so the interpretation of this graph from a theoretical perspective is still missing. In this brief, we fill up this gap by presenting a detailed interpretation based on optimality conditions and their connections to neighborhood graphs. We further provide a systematic way to set up proper hyperparameters to prevent trivial graphs and achieve varied levels of sparsity. Three extensions are explored to leverage different measure functions, refine/reweigh an initial graph, and reduce computation cost for medium-sized graph. Extensive experiments on both synthetic and real datasets were conducted and experimental results verify our theoretical findings and the showcase of the studied graph in semisupervised learning provides competitive results to those of compared methods with their best graph.},
  archive      = {J_TNNLS},
  author       = {Li Wang and Haian Yin and Jin Zhang},
  doi          = {10.1109/TNNLS.2021.3123089},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6642-6649},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Density-based distance preserving graph: Theoretical and practical analyses},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Ensemble feature selection with block-regularized m × 2
cross-validation. <em>TNNLS</em>, <em>34</em>(9), 6628–6641. (<a
href="https://doi.org/10.1109/TNNLS.2021.3128173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble feature selection (EFS) has attracted significant interest in the literature due to its great potential in reducing the discovery rate of noise features and stabilizing the feature selection results. In view of the superior performance of block-regularized $m \,\times \,2$ cross-validation on generalization performance and algorithm comparison, a novel EFS technology based on block-regularized $m \,\times \, 2$ cross-validation is proposed in this study. Contrary to the traditional ensemble learning with a binomial distribution, the distribution of feature selection frequency in the proposed technique is approximated by a beta distribution more accurately. Furthermore, theoretical analysis of the proposed technique shows that it yields a higher selection probability for important features, lower selected risk for noise features, more true positives, and fewer false positives. Finally, the above conclusions are verified by the simulated and real data experiments.},
  archive      = {J_TNNLS},
  author       = {Xingli Yang and Yu Wang and Ruibo Wang and Jihong Li},
  doi          = {10.1109/TNNLS.2021.3128173},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6628-6641},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ensemble feature selection with block-regularized m × 2 cross-validation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supplement and suppression: Both boundary and nonboundary
are helpful for salient object detection. <em>TNNLS</em>,
<em>34</em>(9), 6615–6627. (<a
href="https://doi.org/10.1109/TNNLS.2021.3127959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current methods aggregate multilevel features from the backbone and introduce edge information to get more refined saliency maps. However, little attention is paid to how to suppress the regions with similar saliency appearances in the background. These regions usually exist in the vicinity of salient objects and have high contrast with the background, which is easy to be misclassified as foreground. To solve this problem, we propose a gated feature interaction network (GFINet) to integrate multiple saliency features, which can utilize nonboundary features with background information to suppress pseudosalient objects and simultaneously apply boundary features to supplement edge details. Different from previous methods that only consider the complementarity between saliency and boundary, the proposed network introduces nonboundary features into the decoder to filter the pseudosalient objects. Specifically, GFINet consists of global features aggregation branch (GFAB), boundary and nonboundary features’ perception branch (B&amp;NFPB), and gated feature interaction module (GFIM). According to the global features generated by GFAB, boundary and nonboundary features produced by B&amp;NFPB and GFIM employ a gate structure to adaptively optimize the saliency information interchange between abovementioned features and, thus, predict the final saliency maps. Besides, due to the imbalanced distribution between the boundary pixels and nonboundary ones, the binary cross-entropy (BCE) loss is difficult to predict the pixels near the boundary. Therefore, we design a border region aware (BRA) loss to further boost the quality of boundary and nonboundary, which can guide the network to focus more on the hard pixels near the boundary by assigning different weights to different positions. Compared with 12 counterparts, experimental results on five benchmark datasets show that our method has better generalization and improves the state-of-the-art approach by 4.85\% averagely in terms of the regional and boundary evaluation measures. In addition, our model is more efficient with an inference speed of 50.3 FPS when processing a $320 \times 320$ image. Code has been made available at https://github.com/lesonly/GFINet .},
  archive      = {J_TNNLS},
  author       = {Ge Zhu and Jinbao Li and Yahong Guo},
  doi          = {10.1109/TNNLS.2021.3127959},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6615-6627},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Supplement and suppression: Both boundary and nonboundary are helpful for salient object detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global plus local jointly regularized support vector data
description for novelty detection. <em>TNNLS</em>, <em>34</em>(9),
6602–6614. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practice application, the cost for acquiring abnormal data is quite expensive, thus the one-class classification (OCC) problem attracts great attention. As one of the solutions, support vector data description (SVDD) gains a continuous focus in outlier detection since it is based on the data description. For the sphere obtained by SVDD, both the center and the volume (or radius) strongly depend on the support vectors, while the support vectors are sensitive to the tradeoff parameter ${C}$ . Hence, how to select this parameter is a rather challenging problem. In order to address this problem, we define several distance metrics relative to the image region in Gaussian kernel space. With the distance metrics, two probability densities relative to the global region and the local region are designed, respectively. Then, the information quantity and the information entropy are developed for regularizing the tradeoff parameter. This novel SVDD is called global plus local jointly regularized support vector data description (GL-SVDD), in which both the global region information and the local image region information jointly penalize the images as possible outliers. Finally, we use the UCI dataset and the hyperspectral data of cherry fruit to evaluate the performance of several OCC approaches. Experimental results show that GL-SVDD is encouraging.},
  archive      = {J_TNNLS},
  author       = {Wenjun Hu and Tianjie Hu and Yuzhen Wei and Jungang Lou and Shitong Wang},
  doi          = {10.1109/TNNLS.2021.3129321},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6602-6614},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global plus local jointly regularized support vector data description for novelty detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic parallel pyramid networks for scene recognition.
<em>TNNLS</em>, <em>34</em>(9), 6591–6601. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene recognition is considered a challenging task of image recognition, mainly due to the presence of multiscale information of global layout and local objects in a given scene. Recent convolutional neural networks (CNNs) that can learn multiscale features have achieved remarkable progress in scene recognition. They have two limitations: 1) the receptive field (RF) size is fixed even though a scene may have large-scale variations and 2) they are computing and memory intensive, partially due to the representation of multiscales. To address these limitations, we propose a lightweight dynamic scene recognition approach based on a novel architectural unit, namely, a dynamic parallel pyramid (DPP) block, that can adaptively select RF size based on multiscale information from the input regarding channel dimensions. We encode multiscale features by applying different convolutional (CONV) kernels on different input tensor channels and then dynamically merge their output using a group attention mechanism followed by channel shuffling to generate the parallel feature pyramid. DPP can be easily incorporated with existing CNNs to develop new deep models, called DPP networks (DPP-Nets). Extensive experiments on large-scale scene image datasets, Places365 standard, Places365 challenge, the Massachusetts Institute of Technology (MIT) Indoor67, and Sun397 confirmed that the proposed method provides significant performance improvement compared with current state-of-the-art (SOTA) approaches. We also verified general applicability from compelling results on lightweight models of MobileNetV2 and ShuffleNetV2 on ImageNet-1k and small object centralized benchmarks on CIFAR-10 and CIFAR-100.},
  archive      = {J_TNNLS},
  author       = {Kai Liu and Seungbin Moon},
  doi          = {10.1109/TNNLS.2021.3129227},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6591-6601},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic parallel pyramid networks for scene recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial intelligence enhanced reliability assessment
methodology with small samples. <em>TNNLS</em>, <em>34</em>(9),
6578–6590. (<a
href="https://doi.org/10.1109/TNNLS.2021.3128514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high price of the product and the limitation of laboratory conditions, reliability tests often get a small number of failed samples. If the data are not handled properly, the reliability evaluation results will incur grave errors. In order to solve this problem, this work proposes an artificial intelligence (AI) enhanced reliability assessment methodology by combining Bayesian neural networks (BNNs) and differential evolution (DE) algorithms. First, a single hidden layer BNN model is constructed by fusing small samples and prior information to obtain the 95\% confidence interval (CI) of the posterior distribution. Then, the DE algorithm is used to iteratively generate optimal virtual samples based on the 95\% CI and small samples trends. A reliability assessment model is reconstructed based on double hidden layers BNN model by combining virtual samples and test samples in the last stage. In order to verify the effectiveness of the proposed method, an accelerated life test (ALT) of the subsurface electronic control unit (S-ECU) was carried out. The verification test results show that the proposed method can accurately evaluate the reliability life of a product. And compared with the two existing methods, the results show that this method can effectively improve the accuracy of the reliability assessment of a test product.},
  archive      = {J_TNNLS},
  author       = {Baoping Cai and Chaoyang Sheng and Chuntan Gao and Yonghong Liu and Mingwei Shi and Zengkai Liu and Qiang Feng and Guijie Liu},
  doi          = {10.1109/TNNLS.2021.3128514},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6578-6590},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Artificial intelligence enhanced reliability assessment methodology with small samples},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A second-order projected primal-dual dynamical system for
distributed optimization and learning. <em>TNNLS</em>, <em>34</em>(9),
6568–6577. (<a
href="https://doi.org/10.1109/TNNLS.2021.3127883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on developing distributed optimization strategies for a class of machine learning problems over a directed network of computing agents. In these problems, the global objective function is an addition function, which is composed of local objective functions. Such local objective functions are convex and only endowed by the corresponding computing agent. A second-order Nesterov accelerated dynamical system with time-varying damping coefficient is developed to address such problems. To effectively deal with the constraints in the problems, the projected primal-dual method is carried out in the Nesterov accelerated system. By means of the cocoercive maximal monotone operator, it is shown that the trajectories of the Nesterov accelerated dynamical system can reach consensus at the optimal solution, provided that the damping coefficient and gains meet technical conditions. In the end, the validation of the theoretical results is demonstrated by the email classification problem and the logistic regression problem in machine learning.},
  archive      = {J_TNNLS},
  author       = {Xiaoxuan Wang and Shaofu Yang and Zhenyuan Guo and Tingwen Huang},
  doi          = {10.1109/TNNLS.2021.3127883},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6568-6577},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A second-order projected primal-dual dynamical system for distributed optimization and learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-/event-triggered adaptive neural asymptotic tracking
control of nonlinear interconnected systems with unmodeled dynamics and
prescribed performance. <em>TNNLS</em>, <em>34</em>(9), 6557–6567. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes two adaptive asymptotic tracking control schemes for a class of interconnected systems with unmodeled dynamics and prescribed performance. By applying an inherent property of radial basis function (RBF) neural networks (NNs), the design difficulties aroused from the unknown interactions among subsystems and unmodeled dynamics are overcome. Then, in order to ensure that the tracking errors can be suppressed in the specified range, the constrained control problem is transformed into the stabilization problem by using an auxiliary function. Based on the adaptive backstepping method, a time-triggered controller is constructed. It is proven that under the framework of Barbalat’s lemma, all the variables in the closed-loop system are bounded and the tracking errors are further ensured to converge to zero asymptotically. Furthermore, the event-triggered strategy with a variable threshold is adopted to make more precise control such that the better system performance can be obtained, which reduces the system communication burden under the condition of limited communication resources. Finally, an illustrative example is provided to demonstrate the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Ting-Ting Cheng and Ben Niu and Jia-Ming Zhang and Ding Wang and Zhen-Hua Wang},
  doi          = {10.1109/TNNLS.2021.3129228},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6557-6567},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Time-/Event-triggered adaptive neural asymptotic tracking control of nonlinear interconnected systems with unmodeled dynamics and prescribed performance},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural network-based cooperative trajectory tracking control
for a mobile dual flexible manipulator. <em>TNNLS</em>, <em>34</em>(9),
6545–6556. (<a
href="https://doi.org/10.1109/TNNLS.2021.3128404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a mobile dual flexible manipulator (MDFM) system, this article focuses on the problem of cooperative trajectory tracking under unknown dynamics and time-varying trajectories. The dynamic model of the wheeled mobile manipulator system in 2-D space is established. Taking into account the unmodeled dynamics of the system, unknown terms of the system are approximated by integrating the radial basis function neural network (RBFNN) structure. By introducing the servo system, the cooperative trajectory tracking control (CTTC) strategy is designed, which realizes the system’s cooperative operation, time-varying trajectory tracking, and vibration suppression. The performance of the proposed control scheme is verified through theoretical analysis and numerical simulations.},
  archive      = {J_TNNLS},
  author       = {Shuang Zhang and Yue Wu and Xiuyu He and Jingyuan Wang},
  doi          = {10.1109/TNNLS.2021.3128404},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6545-6556},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based cooperative trajectory tracking control for a mobile dual flexible manipulator},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based dissimilarity measurement for cluster analysis
of any-type-attributed data. <em>TNNLS</em>, <em>34</em>(9), 6530–6544.
(<a href="https://doi.org/10.1109/TNNLS.2022.3202700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous attribute data composed of attributes with different types of values are quite common in a variety of real-world applications. As data annotation is usually expensive, clustering has provided a promising way for processing unlabeled data, where the adopted similarity measure plays a key role in determining the clustering accuracy. However, it is a very challenging task to appropriately define the similarity between data objects with heterogeneous attributes because the values from heterogeneous attributes are generally with very different characteristics. Specifically, numerical attributes are with quantitative values, while categorical attributes are with qualitative values. Furthermore, categorical attributes can be categorized into nominal and ordinal ones according to the order information of their values. To circumvent the awkward gap among the heterogeneous attributes, this article will propose a new dissimilarity metric for cluster analysis of such data. We first study the connections among the heterogeneous attributes and build graph representations for them. Then, a metric is proposed, which computes the dissimilarities between attribute values under the guidance of the graph structures. Finally, we develop a new $k$ -means-type clustering algorithm associated with this proposed metric. It turns out that the proposed method is competent to perform cluster analysis of datasets composed of an arbitrary combination of numerical, nominal, and ordinal attributes. Experimental results show its efficacy in comparison with its counterparts.},
  archive      = {J_TNNLS},
  author       = {Yiqun Zhang and Yiu-Ming Cheung},
  doi          = {10.1109/TNNLS.2022.3202700},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6530-6544},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-based dissimilarity measurement for cluster analysis of any-type-attributed data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FastAdaBelief: Improving convergence rate for belief-based
adaptive optimizers by exploiting strong convexity. <em>TNNLS</em>,
<em>34</em>(9), 6515–6529. (<a
href="https://doi.org/10.1109/TNNLS.2022.3143554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AdaBelief, one of the current best optimizers, demonstrates superior generalization ability over the popular Adam algorithm by viewing the exponential moving average of observed gradients. AdaBelief is theoretically appealing in which it has a data-dependent $O(\sqrt {T})$ regret bound when objective functions are convex, where $T$ is a time horizon. It remains, however, an open problem whether the convergence rate can be further improved without sacrificing its generalization ability. To this end, we make the first attempt in this work and design a novel optimization algorithm called FastAdaBelief that aims to exploit its strong convexity in order to achieve an even faster convergence rate. In particular, by adjusting the step size that better considers strong convexity and prevents fluctuation, our proposed FastAdaBelief demonstrates excellent generalization ability and superior convergence. As an important theoretical contribution, we prove that FastAdaBelief attains a data-dependent $O(\log T)$ regret bound, which is substantially lower than AdaBelief in strongly convex cases. On the empirical side, we validate our theoretical analysis with extensive experiments in scenarios of strong convexity and nonconvexity using three popular baseline models. Experimental results are very encouraging: FastAdaBelief converges the quickest in comparison to all mainstream algorithms while maintaining an excellent generalization ability, in cases of both strong convexity or nonconvexity. FastAdaBelief is, thus, posited as a new benchmark model for the research community.},
  archive      = {J_TNNLS},
  author       = {Yangfan Zhou and Kaizhu Huang and Cheng Cheng and Xuguang Wang and Amir Hussain and Xin Liu},
  doi          = {10.1109/TNNLS.2022.3143554},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6515-6529},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FastAdaBelief: Improving convergence rate for belief-based adaptive optimizers by exploiting strong convexity},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). System stability of learning-based linear optimal control
with general discounted value iteration. <em>TNNLS</em>, <em>34</em>(9),
6504–6514. (<a
href="https://doi.org/10.1109/TNNLS.2021.3137524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For discounted optimal regulation design, the stability of the controlled system is affected by the discount factor. If an inappropriate discount factor is employed, the optimal control policy might be unstabilizing. Therefore, in this article, the effect of the discount factor on the stabilization of control strategies is discussed. We develop the system stability criterion and the selection rules of the discount factor with respect to the linear quadratic regulator problem under the general discounted value iteration algorithm. Based on the monotonicity of the value function sequence, the method to judge the stability of the controlled system is established during the iteration process. In addition, once some stability conditions are satisfied at a certain iteration step, all control policies after this iteration step are stabilizing. Furthermore, combined with the undiscounted optimal control problem, the practical rule of how to select an appropriate discount factor is constructed. Finally, several simulation examples with physical backgrounds are conducted to demonstrate the present theoretical results.},
  archive      = {J_TNNLS},
  author       = {Ding Wang and Jin Ren and Mingming Ha and Junfei Qiao},
  doi          = {10.1109/TNNLS.2021.3137524},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6504-6514},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {System stability of learning-based linear optimal control with general discounted value iteration},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Chance-constrained h∞ state estimation for recursive neural
networks under deception attacks and energy constraints: The
finite-horizon case. <em>TNNLS</em>, <em>34</em>(9), 6492–6503. (<a
href="https://doi.org/10.1109/TNNLS.2021.3137426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the chance-constrained $H_{\infty }$ state estimation problem is investigated for a class of time-varying neural networks subject to measurements degradation and randomly occurring deception attacks. A novel energy-constrained deception attack model is proposed, in which both the occurrence of the attack and the selection of released faked packet are random and the energy of the deception attack is introduced, calculated, and analyzed quantitatively. The main purpose of the addressed problem is to design an $H_{\infty }$ estimator such that the prefixed probabilistic constraints of the system error dynamics are satisfied and the $H_{\infty }$ performance is also ensured. Subsequently, the explicit expression of the estimator gains is derived by solving a minimization problem subjected to certain recursive inequality constraints. Finally, a numerical example and a practical three-tank system are utilized to demonstrate the correctness and effectiveness of the proposed estimation scheme.},
  archive      = {J_TNNLS},
  author       = {Fanrong Qu and Engang Tian and Xia Zhao},
  doi          = {10.1109/TNNLS.2021.3137426},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6492-6503},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Chance-constrained h∞ state estimation for recursive neural networks under deception attacks and energy constraints: The finite-horizon case},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered distributed stochastic mirror descent for
convex optimization. <em>TNNLS</em>, <em>34</em>(9), 6480–6491. (<a
href="https://doi.org/10.1109/TNNLS.2021.3137010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the distributed convex constrained optimization over a time-varying multiagent network in the non-Euclidean sense, where the bandwidth limitation of the network is considered. To save the network resources so as to reduce the communication costs, we apply an event-triggered strategy (ETS) in the information interaction of all the agents over the network. Then, an event-triggered distributed stochastic mirror descent (ET-DSMD) algorithm, which utilizes the Bregman divergence as the distance-measuring function, is presented to investigate the multiagent optimization problem subject to a convex constraint set. Moreover, we also analyze the convergence of the developed ET-DSMD algorithm. An upper bound for the convergence result of each agent is established, which is dependent on the trigger threshold. It shows that a sublinear upper bound can be guaranteed if the trigger threshold converges to zero as time goes to infinity. Finally, a distributed logistic regression example is provided to prove the feasibility of the developed ET-DSMD algorithm.},
  archive      = {J_TNNLS},
  author       = {Menghui Xiong and Baoyong Zhang and Daniel W. C. Ho and Deming Yuan and Shengyuan Xu},
  doi          = {10.1109/TNNLS.2021.3137010},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6480-6491},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered distributed stochastic mirror descent for convex optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An interacting multiple model for trajectory prediction of
intelligent vehicles in typical road traffic scenario. <em>TNNLS</em>,
<em>34</em>(9), 6468–6479. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an interacting multiple model (IMM) for short-term prediction and long-term trajectory prediction of an intelligent vehicle. This model is based on vehicle’s physics model and maneuver recognition model. The long-term trajectory prediction is challenging due to the dynamical nature of the system and large uncertainties. The vehicle physics model is composed of kinematics and dynamics models, which could guarantee the accuracy of short-term prediction. The maneuver recognition model is realized by means of hidden Markov model, which could guarantee the accuracy of long-term prediction, and an IMM is adopted to guarantee the accuracy of both short-term prediction and long-term prediction. The experiment results of a real vehicle are presented to show the effectiveness of the prediction method.},
  archive      = {J_TNNLS},
  author       = {Hongbo Gao and Yechen Qin and Chuan Hu and Yuchao Liu and Keqiang Li},
  doi          = {10.1109/TNNLS.2021.3136866},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6468-6479},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An interacting multiple model for trajectory prediction of intelligent vehicles in typical road traffic scenario},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MCTAN: A novel multichannel temporal attention-based network
for industrial health indicator prediction. <em>TNNLS</em>,
<em>34</em>(9), 6456–6467. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health indicator prediction, such as remaining useful life prediction and product quality prediction, is an important aspect of industrial intelligence. It is essential to process the massive multichannel industrial time series collected from the Industrial Internet of Things for the industrial health indicator prediction. At present, there are still three issues that need to be considered for industrial health indicator prediction. First, it is difficult to directly connect the distant positions in the industrial time series to extract the temporal relations, which decreases the efficiency of extracting the potential long-distance temporal relations and training networks. Second, it should be fully considered that data from different channels have different contributions. Equally dealing with the contributions of each channel will weaken the representational ability of prediction networks. Third, the loss function deals with early predictions and delay predictions equally, which will lead to high risks caused by delay predictions. In this article, for these issues, a novel multichannel temporal attention-based network (MCTAN) is proposed for industrial health indicator prediction, which can weigh contributions of different channels through the channel attention while avoiding the loss of the temporal information and directly connect each time series position to the local fields of the sequence through the multi-head local attention mechanism to efficiently extract potential long-distance temporal relations. Then, a weighted mean square error loss function differently dealing with early predictions and delay predictions by setting dynamic weights is presented to reduce delay predictions. Next, to deal with the above-mentioned issues systematically, a framework combining data preprocessing and MCTAN collaboratively is introduced to predict industrial health indicators through multichannel time series. Finally, the experiments are carried out on the commercial modular aero-propulsion system simulation dataset to measure the performances, including the accuracy of industrial health indicator predictions and the inference speed.},
  archive      = {J_TNNLS},
  author       = {Lei Ren and Yuxin Liu and Di Huang and Keke Huang and Chunhua Yang},
  doi          = {10.1109/TNNLS.2021.3136768},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6456-6467},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MCTAN: A novel multichannel temporal attention-based network for industrial health indicator prediction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dense semantic forecasting in video by joint regression of
features and feature motion. <em>TNNLS</em>, <em>34</em>(9), 6443–6455.
(<a href="https://doi.org/10.1109/TNNLS.2021.3136624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense semantic forecasting anticipates future events in the video by inferring pixel-level semantics of an unobserved future image. We present a novel approach that is applicable to various single-frame architectures and tasks. Our approach consists of two modules. The feature-to-motion (F2M) module forecasts a dense deformation field that warps past features into their future positions. The feature-to-feature (F2F) module regresses the future features directly and is, therefore, able to account for emergent scenery. The compound F2MF model decouples the effects of motion from the effects of novelty in a task-agnostic manner. We aim to apply F2MF forecasting to the most subsampled and the most abstract representation of the desired single-frame model. Our design takes advantage of deformable convolutions and spatial correlation coefficients across neighboring time instants. We perform experiments on three dense prediction tasks: semantic segmentation, instance-level segmentation, and panoptic segmentation. The results reveal state-of-the-art forecasting accuracy across three dense prediction tasks.},
  archive      = {J_TNNLS},
  author       = {Josip Šarić and Sacha Vražić and Siniša Šegvić},
  doi          = {10.1109/TNNLS.2021.3136624},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6443-6455},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dense semantic forecasting in video by joint regression of features and feature motion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-organizing interval type-2 fuzzy neural network using
information aggregation method. <em>TNNLS</em>, <em>34</em>(9),
6428–6442. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval type-2 fuzzy neural networks (IT2FNNs) usually stack adequate fuzzy rules to identify nonlinear systems with high-dimensional inputs, which may result in an explosion of fuzzy rules. To cope with this problem, a self-organizing IT2FNN, based on the information aggregation method (IA-SOIT2FNN), is developed to avoid the explosion of fuzzy rules in this article. First, a relation-aware strategy is proposed to construct rotatable type-2 fuzzy rules (RT2FRs). This strategy uses the individual RT2FR, instead of multiple standard fuzzy rules, to interpret interactive features of high-dimensional inputs. Second, a comprehensive information evaluation mechanism, associated with the interval information and rotation information of RT2FR, is developed to direct the structural adjustment of IA-SOIT2FNN. This mechanism can achieve a compact structure of IA-SOIT2FNN by growing and pruning RT2FRs. Third, a multicriteria-based optimization algorithm is designed to optimize the parameters of IA-SOIT2FNN. The algorithm can simultaneously update the rotatable parameters and the conventional parameters of RT2FR, and further maintain the accuracy of IA-SOIT2FNN. Finally, the experiments showcase that the proposed IA-SOIT2FNN can compete with the state-of-the-art approaches in terms of identification performance.},
  archive      = {J_TNNLS},
  author       = {Honggui Han and Chenxuan Sun and Xiaolong Wu and Hongyan Yang and Junfei Qiao},
  doi          = {10.1109/TNNLS.2021.3136678},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6428-6442},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-organizing interval type-2 fuzzy neural network using information aggregation method},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive iterative learning control for a class of nonlinear
strict-feedback systems with unknown state delays. <em>TNNLS</em>,
<em>34</em>(9), 6416–6427. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive iterative learning control scheme is presented for a class of nonlinear parametric strict-feedback systems with unknown state delays, aiming to achieve the point-wise tracking of desired trajectory in a finite interval. The appropriate Lyapunov–Krasovskii functions are established to compensate the influence of time-delay uncertainties on the control systems. As the main features, the proposed approach integrates the command filter into the backstepping procedure to avoid the differential explosion problem that may occur with the increase of system order, and introduces the hyperbolic tangent functions into the learning controller to handle the singularity problem thus maintaining the continuity of input signal. The results of theoretical analysis and numerical simulation demonstrate that the tracking errors at the entire period will converge to a compact set along the iteration axis. Compared with the existing works, the proposed control scheme is promising to manifest the better performance and practicability owing to the learning mechanism, the dynamic model, as well as the implementation of controller.},
  archive      = {J_TNNLS},
  author       = {Yong Chen and Deqing Huang and Na Qin and Yanhui Zhang},
  doi          = {10.1109/TNNLS.2021.3136644},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6416-6427},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive iterative learning control for a class of nonlinear strict-feedback systems with unknown state delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid reinforcement learning for power transmission network
self-healing considering wind power. <em>TNNLS</em>, <em>34</em>(9),
6405–6415. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transmission network self-healing considering uncertain wind power becomes crucial with increasing penetration of wind power. A hybrid reinforcement learning (HRL) method combining offline self-learning with online Monte Carlo tree search (MCTS) is designed to deal with the strong uncertainty induced by wind power restoration. The HRL method trains a policy network with offline self-learning based on historical wind and transmission system data. It then applies the policy network to guide MCTS to realize step-by-step transmission network self-healing based on real-time and forecast data in different wind power scenarios. Besides, a model predictive control method for active power dispatch is proposed to improve wind power generation credibility during self-healing. Simulation results of both test and real-life power systems demonstrate that the proposed method can realize online transmission system self-healing reliably. Comparisons among different reinforcement learning methods indicate that the number of scenarios dominated by HRL is more than twice that dominated by MCTS and a dozen times that dominated by deep Q-network. Meanwhile, the online method is more flexible in uncertain wind power scenarios than optimization methods.},
  archive      = {J_TNNLS},
  author       = {Runjia Sun and Yutian Liu},
  doi          = {10.1109/TNNLS.2021.3136554},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6405-6415},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hybrid reinforcement learning for power transmission network self-healing considering wind power},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepSMOTE: Fusing deep learning and SMOTE for imbalanced
data. <em>TNNLS</em>, <em>34</em>(9), 6390–6404. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite over two decades of progress, imbalanced data is still considered a significant challenge for contemporary machine learning models. Modern advances in deep learning have further magnified the importance of the imbalanced data problem, especially when learning from images. Therefore, there is a need for an oversampling method that is specifically tailored to deep learning models, can work on raw images while preserving their properties, and is capable of generating high-quality, artificial images that can enhance minority classes and balance the training set. We propose Deep synthetic minority oversampling technique (SMOTE), a novel oversampling algorithm for deep learning models that leverages the properties of the successful SMOTE algorithm. It is simple, yet effective in its design. It consists of three major components: 1) an encoder/decoder framework; 2) SMOTE-based oversampling; and 3) a dedicated loss function that is enhanced with a penalty term. An important advantage of DeepSMOTE over generative adversarial network (GAN)-based oversampling is that DeepSMOTE does not require a discriminator, and it generates high-quality artificial images that are both information-rich and suitable for visual inspection. DeepSMOTE code is publicly available at https://github.com/dd1github/DeepSMOTE .},
  archive      = {J_TNNLS},
  author       = {Damien Dablain and Bartosz Krawczyk and Nitesh V. Chawla},
  doi          = {10.1109/TNNLS.2021.3136503},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6390-6404},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DeepSMOTE: Fusing deep learning and SMOTE for imbalanced data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable design of reservoir computing networks using
realization theory. <em>TNNLS</em>, <em>34</em>(9), 6379–6389. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reservoir computing networks (RCNs) have been successfully employed as a tool in learning and complex decision-making tasks. Despite their efficiency and low training cost, practical applications of RCNs rely heavily on empirical design. In this article, we develop an algorithm to design RCNs using the realization theory of linear dynamical systems. In particular, we introduce the notion of $\alpha $ -stable realization and provide an efficient approach to prune the size of a linear RCN without deteriorating the training accuracy. Furthermore, we derive a necessary and sufficient condition on the irreducibility of the number of hidden nodes in linear RCNs based on the concepts of controllability and observability from systems theory. Leveraging the linear RCN design, we provide a tractable procedure to realize RCNs with nonlinear activation functions. We present numerical experiments on forecasting time-delay systems and chaotic systems to validate the proposed RCN design methods and demonstrate their efficacy.},
  archive      = {J_TNNLS},
  author       = {Wei Miao and Vignesh Narayanan and Jr-Shin Li},
  doi          = {10.1109/TNNLS.2021.3136495},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6379-6389},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interpretable design of reservoir computing networks using realization theory},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conservative policy construction using variational
autoencoders for logged data with missing values. <em>TNNLS</em>,
<em>34</em>(9), 6368–6378. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-stakes applications of data-driven decision-making such as healthcare, it is of paramount importance to learn a policy that maximizes the reward while avoiding potentially dangerous actions when there is uncertainty. There are two main challenges usually associated with this problem. First, learning through online exploration is not possible due to the critical nature of such applications. Therefore, we need to resort to observational datasets with no counterfactuals. Second, such datasets are usually imperfect, additionally cursed with missing values in the attributes of features. In this article, we consider the problem of constructing personalized policies using logged data when there are missing values in the attributes of features in both training and test data. The goal is to recommend an action (treatment) when $\tilde { \boldsymbol {X}}$ , a degraded version of $\boldsymbol {X}$ with missing values, is observed. We consider three strategies for dealing with missingness. In particular, we introduce the conservative strategy where the policy is designed to safely handle the uncertainty due to missingness. In order to implement this strategy, we need to estimate posterior distribution $p(\boldsymbol {X}| \tilde { \boldsymbol {X}})$ and use a variational autoencoder to achieve this. In particular, our method is based on partial variational autoencoders (PVAEs) that are designed to capture the underlying structure of features with missing values.},
  archive      = {J_TNNLS},
  author       = {Mahed Abroshan and Kai Hou Yip and Cem Tekin and Mihaela van der Schaar},
  doi          = {10.1109/TNNLS.2021.3136385},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6368-6378},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Conservative policy construction using variational autoencoders for logged data with missing values},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A discrete-time event-driven near-optimal second-order SMC
for multirobotic system formation prone to network uncertainties.
<em>TNNLS</em>, <em>34</em>(9), 6354–6367. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel stochastic event-driven near-optimal sliding-mode controller design for addressing the consensus of a multiagent system in a network. The system is prone to external disturbances and network uncertainties, such as losses and delays of data packets. The randomness of network uncertainties introduces stochasticity in the system. The design starts with the formulation of control-affine dynamics based on a single integrator robot model, formation error, and sliding surface dynamics. An event-triggering condition is then derived for an update of control input for each agent. These input updates guarantee desired consensus in finite time with reaching time of each agent’s sliding surface having an upper bound. The admissibility of event-driven near-optimal control updates is also ensured for each agent. The near-optimal control design for each agent has achieved through neural-network-based actor–critic architecture. The implementation of Pioneer P3-DX mobile robots illustrates threefold efficacy of the proposed design: 1) advantages of event-driven approach and higher order sliding mode controller; 2) robustness to network uncertainties; and 3) near-optimality in system performance.},
  archive      = {J_TNNLS},
  author       = {Anuj Nandanwar and Narendra Kumar Dhar and Laxmidhar Behera and Saeid Nahavandi and Rajesh Sinha},
  doi          = {10.1109/TNNLS.2021.3135952},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6354-6367},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A discrete-time event-driven near-optimal second-order SMC for multirobotic system formation prone to network uncertainties},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent fault diagnosis of gearbox under variable
working conditions with adaptive intraclass and interclass convolutional
neural network. <em>TNNLS</em>, <em>34</em>(9), 6339–6353. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The industrial gearboxes usually work in harsh and variable conditions, which results in partial failure of gears or bearings. Accordingly, the continuous irregular fluctuations of gearbox under variable conditions maybe increase the intraclass difference and reduce the interclass difference for the monitored samples. To this end, a new intelligent fault diagnosis method of gearbox based on adaptive intraclass and interclass convolutional neural network (AIICNN) under variable working conditions is proposed. The core of the proposed algorithm is to apply the designed intraclass and interclass constraints to improve the distribution differences of samples. Meanwhile, the adaptive activation function is added into the 1-D convolutional neural network (1dCNN) to enlarge the heterogeneous distance and narrow the homogeneous distance of samples. Specifically, the training sample subset with intraclass and interclass spacing fluctuations under variable conditions is first converted into frequency domain through the fast Fourier transform (FFT), and the designed AIICNN algorithm is employed for model training. Afterward, the testing subset is provided to the trained AIICNN algorithm for fault diagnosis. The experimental data of the planetary gearbox test rig verify the feasibility of the proposed diagnosis method and algorithm. Compared with other methods, this method can eliminate the difference of sample distribution under variable conditions and improve its diagnostic generalization.},
  archive      = {J_TNNLS},
  author       = {Xiaoli Zhao and Jianyong Yao and Wenxiang Deng and Peng Ding and Yifei Ding and Minping Jia and Zheng Liu},
  doi          = {10.1109/TNNLS.2021.3135877},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6339-6353},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intelligent fault diagnosis of gearbox under variable working conditions with adaptive intraclass and interclass convolutional neural network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Global predefined-time adaptive neural network control for
disturbed pure-feedback nonlinear systems with zero tracking error.
<em>TNNLS</em>, <em>34</em>(9), 6328–6338. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a global adaptive neural-network-based control algorithm for disturbed pure-feedback nonlinear systems to achieve zero tracking error in a predefined time. Different from the traditional works that only solve the semiglobal bounded tracking problem for pure-feedback systems, this work not only achieves that the tracking error globally converges to zero but also guarantees that the convergence time can be predefined according to the user specification. In order to get the desired predefined-time controller, first, a mild semibound assumption for nonaffine functions is skillfully proposed so that the design difficulty caused by the structure of pure feedback can be easily solved. Then, we apply the property of radial basis function (RBF) neural networks (NNs) and Young’s inequality to derive the upper bound of the term that contains the unknown nonlinear function and external disturbances, and the designed adaptive parameters decide the derived upper and robust control gain. Finally, the predefined-time virtual control inputs are presented whose derivatives are further estimated by utilizing finite-time differentiators. It is strictly proved that the proposed novel predefined-time controller can guarantee that the tracking error globally converges to zero within predefined time and a practical example is shown to verify the effectiveness and practicability of the proposed predefined-time control method.},
  archive      = {J_TNNLS},
  author       = {Yu Zhang and Ben Niu and Xudong Zhao and Peiyong Duan and Huanqing Wang and Baozhong Gao},
  doi          = {10.1109/TNNLS.2021.3135582},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6328-6338},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global predefined-time adaptive neural network control for disturbed pure-feedback nonlinear systems with zero tracking error},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive CL-BFGS algorithms for complex-valued neural
networks. <em>TNNLS</em>, <em>34</em>(9), 6313–6327. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex-valued limited-memory BFGS (CL-BFGS) algorithm is efficient for the training of complex-valued neural networks (CVNNs). As an important parameter, the memory size represents the number of saved vector pairs and would essentially affect the performance of the algorithm. However, the determination of a suitable memory size for the CL-BFGS algorithm remains challenging. To deal with this issue, an adaptive method is proposed in which the memory size is allowed to vary during the iteration process. Basically, at each iteration, with the help of multistep quasi-Newton method, an appropriate memory size is chosen from a variable set ${1,2, {\dots }, M}$ by approximating complex Hessian matrix as close as possible. To reduce the computational complexity and ensure desired performance, the upper bound $M$ is adjustable according to the moving average of memory sizes found in previous iterations. The proposed adaptive CL-BFGS (ACL-BFGS) algorithm can be efficiently applied for the training of CVNNs. Moreover, it is suggested to take multiple memory sizes to construct the search direction, which further improves the performance of the ACL-BFGS algorithm. Experimental results on some benchmark problems including the pattern classification, complex function approximation, and nonlinear channel equalization problems are given to illustrate the advantages of the developed algorithms over some previous ones.},
  archive      = {J_TNNLS},
  author       = {Yongliang Zhang and He Huang and Gangxiang Shen},
  doi          = {10.1109/TNNLS.2021.3135553},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6313-6327},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive CL-BFGS algorithms for complex-valued neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep clustering analysis via dual variational autoencoder
with spherical latent embeddings. <em>TNNLS</em>, <em>34</em>(9),
6303–6312. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, clustering methods based on deep generative models have received great attention in various unsupervised applications, due to their capabilities for learning promising latent embeddings from original data. This article proposes a novel clustering method based on variational autoencoder (VAE) with spherical latent embeddings. The merits of our clustering method can be summarized as follows. First, instead of considering the Gaussian mixture model (GMM) as the prior over latent space as in a variety of existing VAE-based deep clustering methods, the von Mises–Fisher mixture model prior is deployed in our method, leading to spherical latent embeddings that can explicitly control the balance between the capacity of decoder and the utilization of latent embedding in a principled way. Second, a dual VAE structure is leveraged to impose the reconstruction constraint for the latent embedding and its corresponding noise counterpart, which embeds the input data into a hyperspherical latent space for clustering. Third, an augmented loss function is proposed to enhance the robustness of our model, which results in a self-supervised manner through the mutual guidance between the original data and the augmented ones. The effectiveness of the proposed deep generative clustering method is validated through comparisons with state-of-the-art deep clustering methods on benchmark datasets. The source code of the proposed model is available at https://github.com/fwt-team/DSVAE .},
  archive      = {J_TNNLS},
  author       = {Lin Yang and Wentao Fan and Nizar Bouguila},
  doi          = {10.1109/TNNLS.2021.3135460},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6303-6312},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep clustering analysis via dual variational autoencoder with spherical latent embeddings},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal mutual information maximization: A novel approach
for unsupervised deep cross-modal hashing. <em>TNNLS</em>,
<em>34</em>(9), 6289–6302. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we adopt the maximizing mutual information (MI) approach to tackle the problem of unsupervised learning of binary hash codes for efficient cross-modal retrieval. We proposed a novel method, dubbed cross-modal info-max hashing (CMIMH). First, to learn informative representations that can preserve both intramodal and intermodal similarities, we leverage the recent advances in estimating variational lower bound of MI to maximizing the MI between the binary representations and input features and between binary representations of different modalities. By jointly maximizing these MIs under the assumption that the binary representations are modeled by multivariate Bernoulli distributions, we can learn binary representations, which can preserve both intramodal and intermodal similarities, effectively in a mini-batch manner with gradient descent. Furthermore, we find out that trying to minimize the modality gap by learning similar binary representations for the same instance from different modalities could result in less informative representations. Hence, balancing between reducing the modality gap and losing modality-private information is important for the cross-modal retrieval tasks. Quantitative evaluations on standard benchmark datasets demonstrate that the proposed method consistently outperforms other state-of-the-art cross-modal retrieval methods.},
  archive      = {J_TNNLS},
  author       = {Tuan Hoang and Thanh-Toan Do and Tam V. Nguyen and Ngai-Man Cheung},
  doi          = {10.1109/TNNLS.2021.3135420},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6289-6302},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multimodal mutual information maximization: A novel approach for unsupervised deep cross-modal hashing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Adaptive critic for event-triggered unknown nonlinear
optimal tracking design with wastewater treatment applications.
<em>TNNLS</em>, <em>34</em>(9), 6276–6288. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an event-based near-optimal tracking control algorithm is developed for a class of nonaffine systems. First, in order to gain the tracking control strategy, the costate function is established through the iterative dual heuristic dynamic programming (DHP) algorithm. Then, the event-based control method is employed to improve the utilization efficiency of resources and ensure that the closed-loop system has an excellent control performance. Meanwhile, the input-to-state stability (ISS) is proven for the event-based tracking plant. In addition, three kinds of neural networks are used in the event-based DHP algorithm, which aims to identify the nonaffine nonlinear system, estimate the costate function, and approximate the tracking control law. Finally, a numerical experimental simulation is conducted to verify the effectiveness of the proposed scheme. Moreover, in order to further validate the feasibility, the algorithm is applied to the wastewater treatment plant to effectively control the concentrations of dissolved oxygen and nitrate nitrogen.},
  archive      = {J_TNNLS},
  author       = {Ding Wang and Lingzhi Hu and Mingming Zhao and Junfei Qiao},
  doi          = {10.1109/TNNLS.2021.3135405},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6276-6288},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive critic for event-triggered unknown nonlinear optimal tracking design with wastewater treatment applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning deep generative clustering via mutual information
maximization. <em>TNNLS</em>, <em>34</em>(9), 6263–6275. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering refers to joint representation learning and clustering using deep neural networks. Existing methods can be mainly categorized into two types: discriminative and generative methods. The former learns representations for clustering with discriminative mechanisms directly, and the latter estimate the latent distribution of each cluster for generating data points and then infers cluster assignments. Although generative methods have the advantage of estimating the latent distributions of clusters, their performances still significantly fall behind discriminative methods. In this work, we argue that this performance gap might be partly due to the overlap of data distribution of different clusters. In fact, there is little guarantee of generative methods to separate the distributions of different clusters in the data space. To tackle these problems, we theoretically prove that mutual information maximization promotes the separation of different clusters in the data space, which provides a theoretical justification for deep generative clustering with mutual information maximization. Our theoretical analysis directly leads to a model which integrates a hierarchical generative adversarial network and mutual information maximization. Moreover, we further propose three techniques and empirically show their effects to stabilize and enhance the model. The proposed approach notably outperforms other generative models for deep clustering on public benchmarks.},
  archive      = {J_TNNLS},
  author       = {Xiaojiang Yang and Junchi Yan and Yu Cheng and Yizhe Zhang},
  doi          = {10.1109/TNNLS.2021.3135375},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6263-6275},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning deep generative clustering via mutual information maximization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal disentanglement: A generalized bearing fault
diagnostic framework in continuous degradation mode. <em>TNNLS</em>,
<em>34</em>(9), 6250–6262. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the identification of out-of-distribution faults has become a hot topic in the field of intelligent diagnosis. Existing researches usually adopt domain adaptation methods to complete the generalization of diagnostic knowledge with the aid of target domain data, but the acquisition of fault samples in real industries is extremely time-consuming and costly. Moreover, most researches focus on samples with fixed fault levels, ignoring the fact that system degradation is a continuous process. In response to the above intractable problems, this article proposed a causal disentanglement network (CDN) to realize cross-machine knowledge generalization and continuous degradation mode diagnosis. In CDN, multitask instance normalization and batch normalization structure was proposed to learn task-specific knowledge and enhance the informativeness of the extracted features. On this basis, a causal disentanglement loss was proposed, which minimized the mutual information of features between subtask structures and captured the causal invariant fault information for better generalization. The experimental results proved the superiority and generalization ability of CDN, and the visualization results proved the performance of CDN in causality mining.},
  archive      = {J_TNNLS},
  author       = {Jie Li and Yu Wang and Yanyang Zi and Haijun Zhang and Zhiguo Wan},
  doi          = {10.1109/TNNLS.2021.3135036},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6250-6262},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Causal disentanglement: A generalized bearing fault diagnostic framework in continuous degradation mode},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimal pinning control for oscillatority of boolean
networks. <em>TNNLS</em>, <em>34</em>(9), 6237–6249. (<a
href="https://doi.org/10.1109/TNNLS.2021.3134960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, minimal pinning control for oscillatority (i.e., instability) of Boolean networks (BNs) under algebraic state space representations method is studied. First, two criteria for oscillatority of BNs are obtained from the aspects of state transition matrix (STM) and network structure (NS) of BNs, respectively. A distributed pinning control (DPC) from these two aspects is proposed: one is called STM-based DPC and the other one is called NS-based DPC, both of which are only dependent on local in-neighbors. As for STM-based DPC, one arbitrary node can be chosen to be controlled, based on certain solvability of several equations, meanwhile a hybrid pinning control (HPC) combining DPC and conventional pinning control (CPC) is also proposed. In addition, as for NS-based DPC, pinning control nodes (PCNs) can be found using the information of NS, which efficiently reduces the high computational complexity. The proposed STM-based DPC and NS-based DPC in this article are shown to be simple and concise, which provide a new direction to dramatically reduce control costs and computational complexity. Finally, gene networks are simulated to discuss the effectiveness of theoretical results.},
  archive      = {J_TNNLS},
  author       = {Jie Zhong and Qinyao Pan and Bowen Li and Jianquan Lu},
  doi          = {10.1109/TNNLS.2021.3134960},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6237-6249},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Minimal pinning control for oscillatority of boolean networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gated spiking neural p systems for time series forecasting.
<em>TNNLS</em>, <em>34</em>(9), 6227–6236. (<a
href="https://doi.org/10.1109/TNNLS.2021.3134792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural P (SNP) systems are a class of neural-like computing models, abstracted by the mechanism of spiking neurons. This article proposes a new variant of SNP systems, called gated spiking neural P (GSNP) systems, which are composed of gated neurons. Two gated mechanisms are introduced in the nonlinear spiking mechanism of GSNP systems, consisting of a reset gate and a consumption gate. The two gates are used to control the updating of states in neurons. Based on gated neurons, a prediction model for time series is developed, known as the GSNP model. Several benchmark univariate and multivariate time series are used to evaluate the proposed GSNP model and to compare several state-of-the-art prediction models. The comparison results demonstrate the availability and effectiveness of GSNP for time series forecasting.},
  archive      = {J_TNNLS},
  author       = {Qian Liu and Lifan Long and Hong Peng and Jun Wang and Qian Yang and Xiaoxiao Song and Agustín Riscos-Núñez and Mario J. Pérez-Jiménez},
  doi          = {10.1109/TNNLS.2021.3134792},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6227-6236},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gated spiking neural p systems for time series forecasting},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exponential signal reconstruction with deep hankel matrix
factorization. <em>TNNLS</em>, <em>34</em>(9), 6214–6226. (<a
href="https://doi.org/10.1109/TNNLS.2021.3134717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exponential function is a basic form of temporal signals, and how to fast acquire this signal is one of the fundamental problems and frontiers in signal processing. To achieve this goal, partial data may be acquired but result in severe artifacts in its spectrum, which is the Fourier transform of exponentials. Thus, reliable spectrum reconstruction is highly expected in the fast data acquisition in many applications, such as chemistry, biology, and medical imaging. In this work, we propose a deep learning method whose neural network structure is designed by imitating the iterative process in the model-based state-of-the-art exponentials’ reconstruction method with the low-rank Hankel matrix factorization. With the experiments on synthetic data and realistic biological magnetic resonance signals, we demonstrate that the new method yields much lower reconstruction errors and preserves the low-intensity signals much better than compared methods.},
  archive      = {J_TNNLS},
  author       = {Yihui Huang and Jinkui Zhao and Zi Wang and Vladislav Orekhov and Di Guo and Xiaobo Qu},
  doi          = {10.1109/TNNLS.2021.3134717},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6214-6226},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential signal reconstruction with deep hankel matrix factorization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalization analysis of CNNs for classification on
spheres. <em>TNNLS</em>, <em>34</em>(9), 6200–6213. (<a
href="https://doi.org/10.1109/TNNLS.2021.3134675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based on deep convolutional neural networks (CNNs) is extremely efficient in solving classification problems in speech recognition, computer vision, and many other fields. But there is no enough theoretical understanding about this topic, especially the generalization ability of the induced CNN algorithms. In this article, we develop some generalization analysis of a deep CNN algorithm for binary classification with data on spheres. An essential property of the classification problem is the lack of continuity or high smoothness of the target function associated with a convex loss function such as the hinge loss. This motivates us to consider the approximation of functions in the $L_{p}$ space with $1\leq p \leq \infty $ . We provide rates of $L_{p}$ -approximation when the approximated function lies in a Sobolev space and then present generalization bounds and learning rates for the excess misclassification error of the deep CNN classification algorithm. Our novel analysis is based on efficient cubature formulae on spheres and other tools from spherical analysis and approximation theory.},
  archive      = {J_TNNLS},
  author       = {Han Feng and Shuo Huang and Ding-Xuan Zhou},
  doi          = {10.1109/TNNLS.2021.3134675},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6200-6213},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalization analysis of CNNs for classification on spheres},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Open set domain adaptation via joint alignment and category
separation. <em>TNNLS</em>, <em>34</em>(9), 6186–6199. (<a
href="https://doi.org/10.1109/TNNLS.2021.3134673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prevalent domain adaptation approaches are suitable for a close-set scenario where the source domain and the target domain are assumed to share the same data categories. However, this assumption is often violated in real-world conditions where the target domain usually contains samples of categories that are not presented in the source domain. This setting is termed as open set domain adaptation (OSDA). Most existing domain adaptation approaches do not work well in this situation. In this article, we propose an effective method, named joint alignment and category separation (JACS), for OSDA. Specifically, JACS learns a latent shared space, where the marginal and conditional divergence of feature distributions for commonly known classes across domains is alleviated (Joint Alignment), the distribution discrepancy between the known classes and the unknown class is enlarged, and the distance between different known classes is also maximized (Category Separation). These two aspects are unified into an objective to reinforce the optimization of each part simultaneously. The classifier is achieved based on the learned new feature representations by minimizing the structural risk in the reproducing kernel Hilbert space. Extensive experiment results verify that our method outperforms other state-of-the-art approaches on several benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Jieyan Liu and Mengmeng Jing and Jingjing Li and Ke Lu and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2021.3134673},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6186-6199},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Open set domain adaptation via joint alignment and category separation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two recurrent neural networks with reduced model complexity
for constrained l₁-norm optimization. <em>TNNLS</em>, <em>34</em>(9),
6173–6185. (<a
href="https://doi.org/10.1109/TNNLS.2021.3133836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the robustness and sparsity performance of least absolute deviation (LAD or $l_{1}$ ) optimization, developing effective solution methods becomes an important topic. Recurrent neural networks (RNNs) are reported to be capable of effectively solving constrained $l_{1}$ -norm optimization problems, but their convergence speed is limited. To accelerate the convergence, this article introduces two RNNs, in form of continuous- and discrete-time systems, for solving $l_{1}$ -norm optimization problems with linear equality and inequality constraints. The RNNs are theoretically proven to be globally convergent to optimal solutions without any condition. With reduced model complexity, the two RNNs can significantly expedite constrained $l_{1}$ -norm optimization. Numerical simulation results show that the two RNNs spend much less computational time than related RNNs and numerical optimization algorithms for linearly constrained $l_{1}$ -norm optimization.},
  archive      = {J_TNNLS},
  author       = {Youshen Xia and Jun Wang and Zhenyu Lu and Liqing Huang},
  doi          = {10.1109/TNNLS.2021.3133836},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6173-6185},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two recurrent neural networks with reduced model complexity for constrained l₁-norm optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Semisupervised boundary detection for aluminum grains
combined with transfer learning and region growing. <em>TNNLS</em>,
<em>34</em>(9), 6158–6172. (<a
href="https://doi.org/10.1109/TNNLS.2021.3133760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the manufacturing process of aluminum alloy, the size, distribution, and shape of microscopic grains indicate the mechanical characteristics and product quality. However, for metallographic images that can reveal microstructures, the cost of expert labeling at pixel level is high. To solve the problem, we propose a semisupervised learning strategy for grain boundary detection with a few labeled images and abundant unlabeled samples. To expand the helpful information, transfer learning and rule-based region growing are considered. Specifically, a deep network used for extracting multiscale features is designed. With constant training, through a few labeled metallographic images and abundant transferred natural images, pseudo annotations are generated gradually for unlabeled metallographic images iteratively by feature similarity and boundary region growing. The increased unlabeled samples with their pseudo annotations would be involved in the following training process in semisupervised self-training mode to improve the generalization ability of model, together with the domain adaptation block. In experiments, the proposed two methods named semiricher convolutional features-generative adversarial networks (SemiRCF-GAN) and semiricher convolutional features-maximum mean discrepancy (SemiRCF-MMD) can effectively detect grain boundaries with only one labeled metallographic image, and achieve F1 scores of 0.73 and 0.72, respectively, which surpass typical methods.},
  archive      = {J_TNNLS},
  author       = {Mingchun Li and Dali Chen and Shixin Liu and Fang Liu},
  doi          = {10.1109/TNNLS.2021.3133760},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6158-6172},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised boundary detection for aluminum grains combined with transfer learning and region growing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversary agnostic robust deep reinforcement learning.
<em>TNNLS</em>, <em>34</em>(9), 6146–6157. (<a
href="https://doi.org/10.1109/TNNLS.2021.3133537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) policies have been shown to be deceived by perturbations (e.g., random noise or intensional adversarial attacks) on state observations that appear at test time but are unknown during training. To increase the robustness of DRL policies, previous approaches assume that explicit adversarial information can be added into the training process, to achieve generalization ability on these perturbed observations as well. However, such approaches not only make robustness improvement more expensive but may also leave a model prone to other kinds of attacks in the wild. In contrast, we propose an adversary agnostic robust DRL paradigm that does not require learning from predefined adversaries. To this end, we first theoretically show that robustness could indeed be achieved independently of the adversaries based on a policy distillation (PD) setting. Motivated by this finding, we propose a new PD loss with two terms: 1) a prescription gap maximization (PGM) loss aiming to simultaneously maximize the likelihood of the action selected by the teacher policy and the entropy over the remaining actions and 2) a corresponding Jacobian regularization (JR) loss that minimizes the magnitude of gradients with respect to the input state. The theoretical analysis substantiates that our distillation loss guarantees to increase the prescription gap and hence improves the adversarial robustness. Furthermore, experiments on five Atari games firmly verify the superiority of our approach compared to the state-of-the-art baselines.},
  archive      = {J_TNNLS},
  author       = {Xinghua Qu and Abhishek Gupta and Yew-Soon Ong and Zhu Sun},
  doi          = {10.1109/TNNLS.2021.3133537},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6146-6157},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversary agnostic robust deep reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning context restrained correlation tracking filters via
adversarial negative instance generation. <em>TNNLS</em>,
<em>34</em>(9), 6132–6145. (<a
href="https://doi.org/10.1109/TNNLS.2021.3133441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tracking performance of discriminative correlation filters (DCFs) is often subject to unwanted boundary effects. Many attempts have already been made to address the above issue by enlarging searching regions over the last years. However, introducing excessive background information makes the discriminative filter prone to learn from the surrounding context rather than the target. In this article, we propose a novel context restrained correlation tracking filter (CRCTF) that can effectively suppress background interference via incorporating high-quality adversarial generative negative instances. Concretely, we first construct an adversarial context generation network to simulate the central target area with surrounding background information at the initial frame. Then, we suggest a coarse background estimation network to accelerate the background generation in subsequent frames. By introducing a suppression convolution term, we utilize generative background patches to reformulate the original ridge regression objective through circulant property of correlation and a cropping operator. Finally, our tracking filter is efficiently solved by the alternating direction method of multipliers (ADMM). CRCTF demonstrates the accuracy performance on par with several well-established and highly optimized baselines on multiple challenging tracking datasets, verifying the effectiveness of our proposed approach.},
  archive      = {J_TNNLS},
  author       = {Bo Huang and Tingfa Xu and Jianan Li and Fei Luo and Qingwang Qin and Junjie Chen},
  doi          = {10.1109/TNNLS.2021.3133441},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6132-6145},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning context restrained correlation tracking filters via adversarial negative instance generation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wide-area composite load parameter identification based on
multi-residual deep neural network. <em>TNNLS</em>, <em>34</em>(9),
6121–6131. (<a
href="https://doi.org/10.1109/TNNLS.2021.3133350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and practical load modeling plays a critical role in the power system studies including stability, control, and protection. Recently, wide-area measurement systems (WAMSs) are utilized to model the static and dynamic behavior of the load consumption pattern in real-time, simultaneously. In this article, a WAMS-based load modeling method is established based on a multi-residual deep learning structure. To do so, a comprehensive and efficient load model founded on combination of impedance–current–power and induction motor (IM) is constructed at the first step. Then, a deep learning-based framework is developed to understand the time-varying and complex behavior of the composite load model (CLM). To do so, a residual convolutional neural network (ResCNN) is developed to capture the spatial features of the load at different location of the large-scale power system. Then, gated recurrent unit (GRU) is used to fully understand the temporal features from highly variant time-domain signals. It is essential to provide a balance between fast and slow variant parameters. Thus, the designed structure is implemented in a parallel manner to fulfill the balance and moreover, weighted fusion method is used to estimate the parameters, as well. Consequently, an error-based loss function is reformulated to improve the training process as well as robustness in the noisy conditions. The numerical experiments on IEEE 68-bus and Iranian 95-bus systems verify the effectiveness and robustness of the proposed load modeling approach. Furthermore, a comparative study with some relevant methods demonstrates the superiority of the proposed structure. The obtained results in the worst-case scenario show error lower than 0.055\% considering noisy condition and at least 50\% improvement comparing the several state-of-art methods.},
  archive      = {J_TNNLS},
  author       = {Shahabodin Afrasiabi and Mousa Afrasiabi and Mohammad Amin Jarrahi and Mohammad Mohammadi and Jamshid Aghaei and Mohammad Sadegh Javadi and Miadreza Shafie-Khah and João P. S. Catalão},
  doi          = {10.1109/TNNLS.2021.3133350},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6121-6131},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Wide-area composite load parameter identification based on multi-residual deep neural network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward unique and unbiased causal effect estimation from
data with hidden variables. <em>TNNLS</em>, <em>34</em>(9), 6108–6120.
(<a href="https://doi.org/10.1109/TNNLS.2021.3133337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal effect estimation from observational data is a crucial but challenging task. Currently, only a limited number of data-driven causal effect estimation methods are available. These methods either provide only a bound estimation of causal effects of treatment on the outcome or generate a unique estimation of the causal effect but making strong assumptions on data and having low efficiency. In this article, we identify a problem setting with the Cause Or Spouse of the treatment Only (COSO) variable assumption and propose an approach to achieving a unique and unbiased estimation of causal effects from data with hidden variables. For the approach, we have developed the theorems to support the discovery of the proper covariate sets for confounding adjustment (adjustment sets). Based on the theorems, two algorithms are proposed for finding the proper adjustment sets from data with hidden variables to obtain unbiased and unique causal effect estimation. Experiments with synthetic datasets generated using five benchmark Bayesian networks and four real-world datasets have demonstrated the efficiency and effectiveness of the proposed algorithms, indicating the practicability of the identified problem setting and the potential of the proposed approach in real-world applications.},
  archive      = {J_TNNLS},
  author       = {Debo Cheng and Jiuyong Li and Lin Liu and Kui Yu and Thuc Duy Le and Jixue Liu},
  doi          = {10.1109/TNNLS.2021.3133337},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6108-6120},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward unique and unbiased causal effect estimation from data with hidden variables},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptively customizing activation functions for various
layers. <em>TNNLS</em>, <em>34</em>(9), 6096–6107. (<a
href="https://doi.org/10.1109/TNNLS.2021.3133263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the nonlinearity of neural networks and increase their mapping abilities between the inputs and response variables, activation functions play a crucial role to model more complex relationships and patterns in the data. In this work, a novel methodology is proposed to adaptively customize activation functions only by adding very few parameters to the traditional activation functions such as Sigmoid, Tanh, and rectified linear unit (ReLU). To verify the effectiveness of the proposed methodology, some theoretical and experimental analysis on accelerating the convergence and improving the performance is presented, and a series of experiments are conducted based on various network models (such as AlexNet, VggNet, GoogLeNet, ResNet and DenseNet), and various datasets (such as CIFAR10, CIFAR100, miniImageNet, PASCAL VOC, and COCO). To further verify the validity and suitability in various optimization strategies and usage scenarios, some comparison experiments are also implemented among different optimization strategies (such as SGD, Momentum, AdaGrad, AdaDelta, and ADAM) and different recognition tasks such as classification and detection. The results show that the proposed methodology is very simple but with significant performance in convergence speed, precision, and generalization, and it can surpass other popular methods such as ReLU and adaptive functions such as Swish in almost all experiments in terms of overall performance.},
  archive      = {J_TNNLS},
  author       = {Haigen Hu and Aizhu Liu and Qiu Guan and Hanwang Qian and Xiaoxin Li and Shengyong Chen and Qianwei Zhou},
  doi          = {10.1109/TNNLS.2021.3133263},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6096-6107},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptively customizing activation functions for various layers},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based class-imbalance learning with label enhancement.
<em>TNNLS</em>, <em>34</em>(9), 6081–6095. (<a
href="https://doi.org/10.1109/TNNLS.2021.3133262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a common issue in the community of machine learning and data mining. The class-imbalance distribution can make most classical classification algorithms neglect the significance of the minority class and tend toward the majority class. In this article, we propose a label enhancement method to solve the class-imbalance problem in a graph manner, which estimates the numerical label and trains the inductive model simultaneously. It gives a new perspective on the class-imbalance learning based on the numerical label rather than the original logical label. We also present an iterative optimization algorithm and analyze the computation complexity and its convergence. To demonstrate the superiority of the proposed method, several single-label and multilabel datasets are applied in the experiments. The experimental results show that the proposed method achieves a promising performance and outperforms some state-of-the-art single-label and multilabel class-imbalance learning methods.},
  archive      = {J_TNNLS},
  author       = {Guodong Du and Jia Zhang and Min Jiang and Jinyi Long and Yaojin Lin and Shaozi Li and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2021.3133262},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6081-6095},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-based class-imbalance learning with label enhancement},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CEModule: A computation efficient module for lightweight
convolutional neural networks. <em>TNNLS</em>, <em>34</em>(9),
6069–6080. (<a
href="https://doi.org/10.1109/TNNLS.2021.3133127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight convolutional neural networks (CNNs) rely heavily on the design of lightweight convolutional modules (LCMs). For an LCM, lightweight design based on repetitive feature maps (LoR) is currently one of the most effective approaches. An LoR mainly involves an extraction of feature maps from convolutional layers (CE) and feature map regeneration through cheap operations (RO). However, existing LoR approaches carry out lightweight improvements only from the aspect of RO but ignore the problems of poor generalization, low stability, and high computation workload incurred in the CE part. To alleviate these problems, this article introduces the concept of key features from a CNN model interpretation perspective. Subsequently, it presents a novel LCM, namely CEModule, focusing on the CE part. CEModule increases the number of key features to maintain a high level of accuracy in classification. In the meantime, CEModule employs a group convolution strategy to reduce floating-point operations (FLOPs) incurred in the training process. Finally, this article brings forth a dynamic adaptation algorithm ( $\alpha $ -DAM) to enhance the generalization of CEModule-enabled lightweight CNN models, including the developed CENet in dealing with datasets of different scales. Compared with the state-of-the-art results, CEModule reduces FLOPs by up to 54\% on CIFAR-10 while maintaining a similar level of accuracy in classification. On ImageNet, CENet increases accuracy by 1.2\% following the same FLOPs and training strategies.},
  archive      = {J_TNNLS},
  author       = {Yu Liang and Maozhen Li and Changjun Jiang and Guanjun Liu},
  doi          = {10.1109/TNNLS.2021.3133127},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6069-6080},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CEModule: A computation efficient module for lightweight convolutional neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multitask learning for classification problem via new tight
relaxation of rank minimization. <em>TNNLS</em>, <em>34</em>(9),
6055–6068. (<a
href="https://doi.org/10.1109/TNNLS.2021.3132918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask learning (MTL) is a joint learning paradigm, which fuses multiple related tasks together to achieve the better performance than single-task learning methods. It has been observed by many researchers that different tasks with certain similarities share a low-dimensional common yet latent subspace. In order to get the low-rank structure shared across tasks, trace norm has been used as a convex relaxation of the rank minimization problem. However, trace norm is not a tight approximation for the rank function. To address this important issue, we propose two novel regularization-based models to approximate the rank minimization problem by minimizing the $k$ minimal singular values. For our new models, if the minimal singular values are suppressed to zeros, the rank would also be reduced. Compared with the standard trace norm, our new regularization-based models are the tighter approximations, which can help our models capture the low-dimensional subspace among multiple tasks better. Besides, it is an NP-hard problem to directly solve the exact rank minimization problem for our models. In this article, we proposed two simple but effective strategies to optimize our models, which tactically solves the exact rank minimization problem by setting a large penalizing parameter. Experimental results performed on synthetic and real-world benchmark datasets demonstrate that the proposed models have the ability of learning the low-rank structure shared across tasks and the better performance than other classical MTL methods.},
  archive      = {J_TNNLS},
  author       = {Wei Chang and Feiping Nie and Yijie Zhi and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3132918},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6055-6068},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitask learning for classification problem via new tight relaxation of rank minimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed finite-time optimization of second-order
multiagent systems with unknown velocities and disturbances.
<em>TNNLS</em>, <em>34</em>(9), 6042–6054. (<a
href="https://doi.org/10.1109/TNNLS.2021.3132658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the distributed finite-time optimization problem is investigated for second-order multiagent systems with unknown velocities, disturbances, and quadratic local cost functions. To solve this problem, by combining finite-time observers (FTOs), the homogeneous systems theory, and distributed finite-time estimator techniques together, an output feedback-based feedforward-feedback composite distributed control scheme is proposed. Specifically, the control scheme consists of three parts. First, some FTOs are developed for the agents to estimate their unknown velocities and the disturbances together. Second, based on the velocity and disturbance estimates, the homogeneous system theory, and some global information on all the local cost functions’ gradients, Hessian matrices, and the velocity estimates, a kind of centralized finite-time optimization controllers is designed. Third, by designing some distributed finite-time estimators and using their estimates to replace the global terms employed in the centralized optimization controllers, the distributed finite-time optimization controllers are derived. These controllers achieve the distributed finite-time optimization goal. Simulations illustrate the effectiveness and superiority of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Xiangyu Wang and Wei Xing Zheng and Guodong Wang},
  doi          = {10.1109/TNNLS.2021.3132658},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6042-6054},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed finite-time optimization of second-order multiagent systems with unknown velocities and disturbances},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OSNN: An online semisupervised neural network for
nonstationary data streams. <em>TNNLS</em>, <em>34</em>(9), 6029–6041.
(<a href="https://doi.org/10.1109/TNNLS.2021.3132584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from data streams that emerge from nonstationary environments has many real-world applications and poses various challenges. A key characteristic of such a task is the varying nature of target functions and data distributions over time (concept drifts). Most existing work relies solely on labeled data to adapt to concept drifts in classification problems. However, labeling all instances in a potentially life-long data stream is frequently prohibitively expensive, hindering such approaches. Therefore, we propose a novel algorithm to exploit unlabeled instances, which are typically plentiful and easily obtained. The algorithm is an online semisupervised radial basis function neural network (OSNN) with manifold-based training to exploit unlabeled data while tackling concept drifts in classification problems. OSNN employs a novel semisupervised learning vector quantization (SLVQ) to train network centers and learn meaningful data representations that change over time. It uses manifold learning on dynamic graphs to adjust the network weights. Our experiments confirm that OSNN can effectively use unlabeled data to elucidate underlying structures of data streams while its dynamic topology learning provides robustness to concept drifts.},
  archive      = {J_TNNLS},
  author       = {Rodrigo G. F. Soares and Leandro L. Minku},
  doi          = {10.1109/TNNLS.2021.3132584},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6029-6041},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {OSNN: An online semisupervised neural network for nonstationary data streams},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Interaction-aware graph neural networks for fault diagnosis
of complex industrial processes. <em>TNNLS</em>, <em>34</em>(9),
6015–6028. (<a
href="https://doi.org/10.1109/TNNLS.2021.3132376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis of complex industrial processes becomes a challenging task due to various fault patterns in sensor signals and complex interactions between different units. However, how to explore the interactions and integrate with sensor signals remains an open question. Considering that the sensor signals and their interactions in an industrial process with the form of nodes and edges can be represented as a graph, this article proposes a novel interaction-aware and data fusion method for fault diagnosis of complex industrial processes, named interaction-aware graph neural networks (IAGNNs). First, to describe the complex interactions in an industrial process, the sensor signals are transformed into a heterogeneous graph with multiple edge types, and the edge weights are learned by the attention mechanism, adaptively. Then, multiple independent graph neural network (GNN) blocks are employed to extract the fault feature for each subgraph with one edge type. Finally, each subgraph feature is concatenated or fused by a weighted summation function to generate the final graph embedding. Therefore, the proposed method can learn multiple interactions between sensor signals and extract the fault feature from each subgraph by message passing operation of GNNs. The final fault feature contains the information from raw data and implicit interactions between sensor signals. The experimental results on the three-phase flow facility and power system (PS) demonstrate the reliable and superior performance of the proposed method for fault diagnosis of complex industrial processes.},
  archive      = {J_TNNLS},
  author       = {Dongyue Chen and Ruonan Liu and Qinghua Hu and Steven X. Ding},
  doi          = {10.1109/TNNLS.2021.3132376},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6015-6028},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interaction-aware graph neural networks for fault diagnosis of complex industrial processes},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale visual-attribute co-attention for zero-shot image
recognition. <em>TNNLS</em>, <em>34</em>(9), 6003–6014. (<a
href="https://doi.org/10.1109/TNNLS.2021.3132366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot image recognition aims to classify data from unseen classes, by exploring the association between visual features and the semantic representations of each class. Most existing approaches focus on learning a shared single-scale embedding space (often at the output layer of the network) for both visual and semantic features, ignoring a fact that different-scale visual features exhibit different semantics. In this article, we propose a multi-scale visual-attribute co-attention (mVACA) model, considering both visual-semantic alignment and visual discrimination at multiple scales. At each scale, a hybrid visual attention is realized by attribute-related attention and visual self-attention. The attribute-related attention is guided by a pseudo attribute vector inferred via a mutual information regularization (MIR). The visual self-attentive features further influence the attribute attention to emphasize visual-associated attributes. Leveraging multiscale visual discrimination, mVACA unifies standard zero-shot learning (ZSL) and generalized ZSL tasks in one framework, achieving state-of-the-art or competitive performance on several commonly used benchmarks of both setups. To better understand the interaction between images and attributes in mVACA, we also provide visualized analysis.},
  archive      = {J_TNNLS},
  author       = {Hao Zhang and Long Tian and Zhengjue Wang and Yishi Xu and Pengyu Cheng and Ke Bai and Bo Chen},
  doi          = {10.1109/TNNLS.2021.3132366},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {6003-6014},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiscale visual-attribute co-attention for zero-shot image recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attentive siamese networks for automatic modulation
classification based on multitiming constellation diagrams.
<em>TNNLS</em>, <em>34</em>(9), 5988–6002. (<a
href="https://doi.org/10.1109/TNNLS.2021.3132341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic modulation classification (AMC) is an essential part in a cognitive radio receiver. Benefited from the discriminative constellation characteristics among most modulations, AMC methods based on constellation diagrams usually achieve pleasant performance. However, in noncooperation communication systems, constellation diagrams expressing modulations explicitly are difficult to obtain via blind symbol timing synchronization, especially in complicated wireless channels. Therefore, this article proposes a novel constellation diagram-based AMC architecture called attentive Siamese networks (ASNs) by considering multitiming constellation diagrams (MCDs) and selecting the proper symbol timings at the feature level, which is a more robust way than the conventional signal-level symbol timing synchronization. In detail, convolutional neural networks sharing the same parameters first extract deep feature vectors for MCDs. Then, an attention inference module weights all the deep feature vectors. Finally, AMC is realized based on the weighted feature vectors. Moreover, the ASN architecture can be trained end-to-end. Comparing with the state-of-the-art methods that take diverse representations of received baseband signals as input, experimental results based on the RadioML 2018.01A dataset and non-Gaussian noise dataset demonstrate that ASN achieves a remarkable improvement, whose classification accuracy goes over 99\% when the signal-to-noise ratio (SNR) &gt; 10 dB.},
  archive      = {J_TNNLS},
  author       = {Yu Mao and Yang-Yang Dong and Ting Sun and Xian Rao and Chun-Xi Dong},
  doi          = {10.1109/TNNLS.2021.3132341},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5988-6002},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attentive siamese networks for automatic modulation classification based on multitiming constellation diagrams},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite-time passivity for coupled fractional-order neural
networks with multistate or multiderivative couplings. <em>TNNLS</em>,
<em>34</em>(9), 5976–5987. (<a
href="https://doi.org/10.1109/TNNLS.2021.3132069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article mainly delves into the finite-time passivity (FTP) for coupled fractional-order neural networks with multistate couplings (CFNNMSCs) or coupled fractional-order neural networks with multiderivative couplings (CFNNMDCs). Distinguishing from the traditional FTP definitions, several concepts of FTP for fractional-order systems are given. On one hand, we present several sufficient conditions to ensure the FTP for CFNNMSCs by artfully designing a state-feedback controller and an adaptive state-feedback controller. On the other hand, by utilizing some inequality techniques, two sets of FTP criteria for CFNNMDCs are also established on the basis of the state-feedback and adaptive state-feedback controllers. Finally, numerical examples are used to demonstrate the validity of the derived FTP criteria.},
  archive      = {J_TNNLS},
  author       = {Chen-Guang Liu and Jin-Liang Wang and Huai-Ning Wu},
  doi          = {10.1109/TNNLS.2021.3132069},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5976-5987},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time passivity for coupled fractional-order neural networks with multistate or multiderivative couplings},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Why is everyone training very deep neural network with skip
connections? <em>TNNLS</em>, <em>34</em>(9), 5961–5975. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep neural networks (DNNs) with several layers of feature representations rely on some form of skip connections to simultaneously circumnavigate optimization problems and improve generalization performance. However, the operations of these models are still not clearly understood, especially in comparison to DNNs without skip connections referred to as plain networks (PlainNets) that are absolutely untrainable beyond some depth. As such, the exposition of this article is the theoretical analysis of the role of skip connections in training very DNNs using concepts from linear algebra and random matrix theory. In comparison with PlainNets, the results of our investigation directly unravel the following: 1) why DNNs with skip connections are easier to optimize and 2) why DNNs with skip connections exhibit improved generalization. Our investigation results concretely show that the hidden representations of PlainNets progressively suffer from information loss via singularity problems with depth increase, thus making their optimization difficult. In contrast, as model depth increases, the hidden representations of DNNs with skip connections circumnavigate singularity problems to retain full information that reflects in improved optimization and generalization. For theoretical analysis, this article studies in relation to PlainNets two popular skip connection-based DNNs that are residual networks (ResNets) and residual network with aggregated features (ResNeXt).},
  archive      = {J_TNNLS},
  author       = {Oyebade K. Oyedotun and Kassem Al Ismaeil and Djamila Aouada},
  doi          = {10.1109/TNNLS.2021.3131813},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5961-5975},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Why is everyone training very deep neural network with skip connections?},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed extended state estimation for complex networks
with nonlinear uncertainty. <em>TNNLS</em>, <em>34</em>(9), 5952–5960.
(<a href="https://doi.org/10.1109/TNNLS.2021.3131661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the distributed state estimation issue for complex networks with nonlinear uncertainty. The extended state approach is used to deal with the nonlinear uncertainty. The distributed state predictor is designed based on the extended state system model, and the distributed state estimator is designed by using the measurement of the corresponding node. The prediction error and the estimation error are derived. The prediction error covariance (PEC) is obtained in terms of the recursive Riccati equation, and the upper bound of the PEC is minimized by designing an optimal estimator gain. With the vectorization approach, a sufficient condition concerning stability of the upper bound is developed. Finally, a numerical example is presented to illustrate the effectiveness of the designed extended state estimator.},
  archive      = {J_TNNLS},
  author       = {Hui Peng and Boru Zeng and Lixin Yang and Yong Xu and Renquan Lu},
  doi          = {10.1109/TNNLS.2021.3131661},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5952-5960},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed extended state estimation for complex networks with nonlinear uncertainty},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model compression for communication efficient federated
learning. <em>TNNLS</em>, <em>34</em>(9), 5937–5951. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the many advantages of using deep neural networks over shallow networks in various machine learning tasks, their effectiveness is compromised in a federated learning setting due to large storage sizes and high computational resource requirements for training. A large model size can potentially require infeasible amounts of data to be transmitted between the server and clients for training. To address these issues, we investigate the traditional and novel compression techniques to construct sparse models from dense networks whose storage and bandwidth requirements are significantly lower. We do this by separately considering compression techniques for the server model to address downstream communication and the client models to address upstream communication. Both of these play a crucial role in developing and maintaining sparsity across communication cycles. We empirically demonstrate the efficacy of the proposed schemes by testing their performance on standard datasets and verify that they outperform various state-of-the-art baseline schemes in terms of accuracy and communication volume.},
  archive      = {J_TNNLS},
  author       = {Suhail Mohmad Shah and Vincent K. N. Lau},
  doi          = {10.1109/TNNLS.2021.3131614},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5937-5951},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model compression for communication efficient federated learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent control of flexible hypersonic flight dynamics
with input dead zone using singular perturbation decomposition.
<em>TNNLS</em>, <em>34</em>(9), 5926–5936. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the robust intelligent control for the longitudinal dynamics of flexible hypersonic flight vehicle with input dead zone. Considering the different time-scale characteristics among the system states, the singular perturbation decomposition is employed to transform the rigid-elastic coupling model into the slow dynamics and the fast dynamics. For the slow dynamics with unknown system nonlinearities, the robust neural control is constructed using the switching mechanism to achieve the coordination between robust design and neural learning. For the time-varying control gain caused by unknown dead-zone input, the stable control is presented with an adaptive estimation design. For the fast dynamics, the sliding mode control is constructed to make the elastic modes stable and convergent. The elevator deflection is obtained by combining the two control signals. The stability of the dynamics is analyzed through the Lyapunov approach and the system tracking errors are bounded. The simulation is conducted to demonstrate the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Bin Xu and Xia Wang and Fuchun Sun and Zhongke Shi},
  doi          = {10.1109/TNNLS.2021.3131578},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5926-5936},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intelligent control of flexible hypersonic flight dynamics with input dead zone using singular perturbation decomposition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synchronization of generally uncertain markovian inertial
neural networks with random connection weight strengths and image
encryption application. <em>TNNLS</em>, <em>34</em>(9), 5911–5925. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the synchronization problem of delayed inertial neural networks (INNs) with generally uncertain Markovian jumping and their applications in image encryption. The random connection weight strengths and generally uncertain Markovian are discussed in the INNs model. Compared with most existing INNs models that have constant connection weight strengths, our model is more practical because connection weight strengths of INNs may randomly vary due to the external and internal environment and human factor. The delay-range-dependent synchronization conditions (DRDSCs) could be obtained by adopting the delay-product-term Lyapunov–Krasovskii functional (DPTLKF) and higher order polynomial-based relaxed inequality (HOPRII). In addition, the desired controllers are obtained by solving a set of linear matrix inequalities. Finally, two examples are shown to demonstrate the effectiveness of the proposed results.},
  archive      = {J_TNNLS},
  author       = {Junyi Wang and Zewen Ji and Huaguang Zhang and Zhanshan Wang and Qinggang Meng},
  doi          = {10.1109/TNNLS.2021.3131512},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5911-5925},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of generally uncertain markovian inertial neural networks with random connection weight strengths and image encryption application},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Adaptive neural network-based observer design for switched
systems with quantized measurements. <em>TNNLS</em>, <em>34</em>(9),
5897–5910. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is concerned with the adaptive neural network (NN) observer design problem for continuous-time switched systems via quantized output signals. A novel NN observer is presented in which the adaptive laws are constructed using quantized measurements. Then, persistent dwell time (PDT) switching is considered in the observer design to describe fast and slow switching in a unified framework. Accurate estimations of state and actuator efficiency factor can be obtained by the proposed observer technique despite actuator degradation. Finally, a simulation example is provided to illustrate the effectiveness of the developed NN observer design approach.},
  archive      = {J_TNNLS},
  author       = {Liheng Chen and Yanzheng Zhu and Choon Ki Ahn},
  doi          = {10.1109/TNNLS.2021.3131412},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5897-5910},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network-based observer design for switched systems with quantized measurements},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recursion newton-like algorithm for l2,0-ReLU deep neural
networks. <em>TNNLS</em>, <em>34</em>(9), 5882–5896. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rectified linear unit (ReLU) deep neural network (DNN) is a classical model in deep learning and has achieved great success in many applications. However, this model is characterized by too many parameters, which not only requires huge memory but also imposes unbearable computation burden. The $l_{2,0}$ regularization has become a useful technique to cope with this trouble. In this article, we design a recursion Newton-like algorithm (RNLA) to simultaneously train and compress ReLU-DNNs with $l_{2,0}$ regularization. First, we reformulate the multicomposite training model into a constrained optimization problem by explicitly introducing the network nodes as the variables of the optimization. Based on the penalty function of the reformulation, we obtain two types of minimization subproblems. Second, we build the first-order optimality conditions for acquiring P-stationary points of the two subproblems, and these P-stationary points enable us to equivalently derive two sequences of stationary equations, which are piecewise linear matrix equations. We solve these equations by the column Newton-like method in group sparse subspace with lower computational scale and cost. Finally, numerical experiments are conducted on real datasets, and the results demonstrate that the proposed method RNLA is effective and applicable.},
  archive      = {J_TNNLS},
  author       = {Hui Zhang and Zhengpeng Yuan and Naihua Xiu},
  doi          = {10.1109/TNNLS.2021.3131406},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5882-5896},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Recursion newton-like algorithm for l2,0-ReLU deep neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guaranteeing global stability for neuro-adaptive control of
unknown pure-feedback nonaffine systems via barrier functions.
<em>TNNLS</em>, <em>34</em>(9), 5869–5881. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing approximation-based adaptive control (AAC) approaches for unknown pure-feedback nonaffine systems retain a dilemma that all closed-loop signals are semiglobally uniformly bounded (SGUB) rather than globally uniformly bounded (GUB). To achieve the GUB stability result, this article presents a neuro-adaptive backstepping control approach by blending the mean value theorem (MVT), the barrier Lyapunov functions (BLFs), and the technique of neural approximation. Specifically, we first resort the MVT to acquire the intermediate and actual control inputs from the nonaffine structures directly. Then, neural networks (NNs) are adopted to approximate the unknown nonlinear functions, in which the compact sets for maintaining the approximation capabilities of NNs are predetermined actively through the BLFs. It is shown that, with the developed neuro-adaptive control scheme, global stability of the resulting closed-loop system is ensured. Simulations are conducted to verify and clarify the developed approach.},
  archive      = {J_TNNLS},
  author       = {Yong-Hua Liu and Yu-Fa Liu and Chun-Yi Su and Yang Liu and Qi Zhou and Renquan Lu},
  doi          = {10.1109/TNNLS.2021.3131364},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5869-5881},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guaranteeing global stability for neuro-adaptive control of unknown pure-feedback nonaffine systems via barrier functions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse bayesian learning with weakly informative hyperprior
and extended predictive information criterion. <em>TNNLS</em>,
<em>34</em>(9), 5856–5868. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the regression problem with sparse Bayesian learning (SBL) when the number of weights $P$ is larger than the data size $N$ , i.e., $P\gg N$ . The situation induces overfitting and makes regression tasks, such as prediction and basis selection, challenging. We show a strategy to address this problem. Our strategy consists of two steps. The first is to apply an inverse gamma hyperprior with a shape parameter close to zero over the noise precision of automatic relevance determination (ARD) prior. This hyperprior is associated with the concept of a weakly informative prior in terms of enhancing sparsity. The model sparsity can be controlled by adjusting a scale parameter of inverse gamma hyperprior, leading to the prevention of overfitting. The second is to select an optimal scale parameter. We develop an extended predictive information criterion (EPIC) for optimal selection. We investigate the strategy through relevance vector machine (RVM) with a multiple-kernel scheme dealing with highly nonlinear data, including smooth and less smooth regions. This setting is one form of the regression task with SBL in the $P\gg N$ situation. As an empirical evaluation, regression analyses on four artificial datasets and eight real datasets are performed. We see that the overfitting is prevented, while predictive performance may be not drastically superior to comparative methods. Our methods allow us to select a small number of nonzero weights while keeping the model sparse. Thus, the methods are expected to be useful for basis and variable selection.},
  archive      = {J_TNNLS},
  author       = {Kazuaki Murayama and Shuichi Kawano},
  doi          = {10.1109/TNNLS.2021.3131357},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5856-5868},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sparse bayesian learning with weakly informative hyperprior and extended predictive information criterion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HybridSNN: Combining bio-machine strengths by boosting
adaptive spiking neural networks. <em>TNNLS</em>, <em>34</em>(9),
5841–5855. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs), inspired by the neuronal network in the brain, provide biologically relevant and low-power consuming models for information processing. Existing studies either mimic the learning mechanism of brain neural networks as closely as possible, for example, the temporally local learning rule of spike-timing-dependent plasticity (STDP), or apply the gradient descent rule to optimize a multilayer SNN with fixed structure. However, the learning rule used in the former is local and how the real brain might do the global-scale credit assignment is still not clear, which means that those shallow SNNs are robust but deep SNNs are difficult to be trained globally and could not work so well. For the latter, the nondifferentiable problem caused by the discrete spike trains leads to inaccuracy in gradient computing and difficulties in effective deep SNNs. Hence, a hybrid solution is interesting to combine shallow SNNs with an appropriate machine learning (ML) technique not requiring the gradient computing, which is able to provide both energy-saving and high-performance advantages. In this article, we propose a HybridSNN, a deep and strong SNN composed of multiple simple SNNs, in which data-driven greedy optimization is used to build powerful classifiers, avoiding the derivative problem in gradient descent. During the training process, the output features (spikes) of selected weak classifiers are fed back to the pool for the subsequent weak SNN training and selection. This guarantees HybridSNN not only represents the linear combination of simple SNNs, as what regular AdaBoost algorithm generates, but also contains neuron connection information, thus closely resembling the neural networks of a brain. HybridSNN has the benefits of both low power consumption in weak units and overall data-driven optimizing strength. The network structure in HybridSNN is learned from training samples, which is more flexible and effective compared with existing fixed multilayer SNNs. Moreover, the topological tree of HybridSNN resembles the neural system in the brain, where pyramidal neurons receive thousands of synaptic input signals through their dendrites. Experimental results show that the proposed HybridSNN is highly competitive among the state-of-the-art SNNs.},
  archive      = {J_TNNLS},
  author       = {Jiangrong Shen and Yu Zhao and Jian K. Liu and Yueming Wang},
  doi          = {10.1109/TNNLS.2021.3131356},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5841-5855},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HybridSNN: Combining bio-machine strengths by boosting adaptive spiking neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding how pretraining regularizes deep learning
algorithms. <em>TNNLS</em>, <em>34</em>(9), 5828–5840. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning algorithms have led to a series of breakthroughs in computer vision, acoustical signal processing, and others. However, they have only been popularized recently due to the groundbreaking techniques developed for training deep architectures. Understanding the training techniques is important if we want to further improve them. Through extensive experimentation, Erhan et al. (2010) empirically illustrated that unsupervised pretraining has an effect of regularization for deep learning algorithms. However, theoretical justifications for the observation remain elusive. In this article, we provide theoretical supports by analyzing how unsupervised pretraining regularizes deep learning algorithms. Specifically, we interpret deep learning algorithms as the traditional Tikhonov-regularized batch learning algorithms that simultaneously learn predictors in the input feature spaces and the parameters of the neural networks to produce the Tikhonov matrices. We prove that unsupervised pretraining helps in learning meaningful Tikhonov matrices, which will make the deep learning algorithms uniformly stable and the learned predictor will generalize fast w.r.t. the sample size. Unsupervised pretraining, therefore, can be interpreted as to have the function of regularization.},
  archive      = {J_TNNLS},
  author       = {Yu Yao and Baosheng Yu and Chen Gong and Tongliang Liu},
  doi          = {10.1109/TNNLS.2021.3131377},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5828-5840},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Understanding how pretraining regularizes deep learning algorithms},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive diffusion pairwise fused lasso LMS algorithm over
networks. <em>TNNLS</em>, <em>34</em>(9), 5816–5827. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of identification for sparse vector in a distributed way has triggered great interest in the area of adaptive filtering. Grouping components in the sparse vector has been validated to be an efficient way for enhancing identification performance for sparse parameter. The technique of pairwise fused lasso, which can promote similarity between each possible pair of nonnegligible components in the sparse vector, does not require that the nonnegligible components have to be distributed in one or multiple clusters. In other words, the nonnegligible components may be randomly scattered in the unknown sparse vector. In this article, based on the technique of pairwise fused lasso, we propose the novel pairwise fused lasso diffusion least mean-square (PFL-DLMS) algorithm, to identify sparse vector. The objective function we construct consists of three terms, i.e., the mean-square error (MSE) term, the regularizing term promoting the sparsity of all components, and the regularizing term promoting the sparsity of difference between each pair of components in the unknown sparse vector. After investigating mean stability condition of mean-square behavior in theoretical analysis, we propose the strategy of variable regularizing coefficients to overcome the difficulty that the optimal regularizing coefficients are usually unknown. Finally, numerical experiments are conducted to verify the effectiveness of the PFL-DLMS algorithm in identifying and tracking sparse parameter vector.},
  archive      = {J_TNNLS},
  author       = {Wei Huang and Haojie Shan and Jinshan Xu and Xinwei Yao},
  doi          = {10.1109/TNNLS.2021.3131335},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5816-5827},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive diffusion pairwise fused lasso LMS algorithm over networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust admittance control of optimized robot–environment
interaction using reference adaptation. <em>TNNLS</em>, <em>34</em>(9),
5804–5815. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a robust control scheme is proposed for robots to achieve an optimal performance in the process of interacting with external forces from environments. The environmental dynamics are defined as a linear model, and the interaction performance is evaluated by a defined cost function, which is composed of trajectory errors and force regulation. Based on admittance control, the reference adaptation method is used to minimize the cost function and achieve the optimal interaction performance. To make the trajectory tracking controller robust to the unknown disturbance of internal system dynamics, an auxiliary system is defined and the approximation optimal controller is designed. Experiments on the Baxter robot are conducted to verify the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Guangzhu Peng and C. L. Philip Chen and Chenguang Yang},
  doi          = {10.1109/TNNLS.2021.3131261},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5804-5815},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust admittance control of optimized Robot–Environment interaction using reference adaptation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaussian process regression with interpretable sample-wise
feature weights. <em>TNNLS</em>, <em>34</em>(9), 5789–5803. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process regression (GPR) is a fundamental model used in machine learning (ML). Due to its accurate prediction with uncertainty and versatility in handling various data structures via kernels, GPR has been successfully used in various applications. However, in GPR, how the features of an input contribute to its prediction cannot be interpreted. Here, we propose GPR with local explanation, which reveals the feature contributions to the prediction of each sample while maintaining the predictive performance of GPR. In the proposed model, both the prediction and explanation for each sample are performed using an easy-to-interpret locally linear model. The weight vector of the locally linear model is assumed to be generated from multivariate Gaussian process priors. The hyperparameters of the proposed models are estimated by maximizing the marginal likelihood. For a new test sample, the proposed model can predict the values of its target variable and weight vector, as well as their uncertainties, in a closed form. Experimental results on various benchmark datasets verify that the proposed model can achieve predictive performance comparable to those of GPR and superior to that of existing interpretable models and can achieve higher interpretability than them, both quantitatively and qualitatively.},
  archive      = {J_TNNLS},
  author       = {Yuya Yoshikawa and Tomoharu Iwata},
  doi          = {10.1109/TNNLS.2021.3131234},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5789-5803},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gaussian process regression with interpretable sample-wise feature weights},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online orthogonal dictionary learning based on frank–wolfe
method. <em>TNNLS</em>, <em>34</em>(9), 5774–5788. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dictionary learning is a widely used unsupervised learning method in signal processing and machine learning. Most existing works on dictionary learning adopt an off-line approach, and there are two main off-line ways of conducting it. One is to alternately optimize both the dictionary and the sparse code, while the other is to optimize the dictionary by restricting it over the orthogonal group. The latter, called orthogonal dictionary learning (ODL), has a lower implementation complexity and, hence, is more favorable for low-cost devices. However, existing schemes for ODL only work with batch data and cannot be implemented online, making them inapplicable for real-time applications. This article, thus, proposes a novel online orthogonal dictionary scheme to dynamically learn the dictionary from streaming data, without storing the historical data. The proposed scheme includes a novel problem formulation and an efficient online algorithm design with convergence analysis. In the problem formulation, we relax the orthogonal constraint to enable an efficient online algorithm. We then propose the design of a new Frank–Wolfe-based online algorithm with a convergence rate of $\mathcal {O}(\ln t/t^{1/4})$ . The convergence rate in terms of key system parameters is also derived. Experiments with synthetic data and real-world Internet of things (IoT) sensor readings demonstrate the effectiveness and efficiency of the proposed online ODL scheme.},
  archive      = {J_TNNLS},
  author       = {Ye Xue and Vincent K. N. Lau},
  doi          = {10.1109/TNNLS.2021.3131181},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5774-5788},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online orthogonal dictionary learning based on Frank–Wolfe method},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effects of motion-relevant knowledge from unlabeled video to
human–object interaction detection. <em>TNNLS</em>, <em>34</em>(9),
5760–5773. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing works on human–object interaction (HOI) detection usually rely on expensive large-scale labeled image datasets. However, in real scenes, labeled data may be insufficient, and some rare HOI categories have few samples. This poses great challenges for deep-learning-based HOI detection models. Existing works tackle it by introducing compositional learning or word embedding but still need large-scale labeled data or extremely rely on the well-learned knowledge. In contrast, the freely available unlabeled videos contain rich motion-relevant information that can help infer rare HOIs. In this article, we creatively propose a multitask learning (MTL) perspective to assist in HOI detection with the aid of motion-relevant knowledge learning on unlabeled videos. Specifically, we design the appearance reconstruction loss (ARL) and sequential motion mining module in a self-supervised manner to learn more generalizable motion representations for promoting the detection of rare HOIs. Moreover, to better transfer motion-related knowledge from unlabeled videos to HOI images, a domain discriminator is introduced to decrease the domain gap between two domains. Extensive experiments on the HICO-DET dataset with rare categories and the V-COCO dataset with minimum supervision demonstrate the effectiveness of motion-aware knowledge implied in unlabeled videos for HOI detection.},
  archive      = {J_TNNLS},
  author       = {Xue Lin and Qi Zou and Xixia Xu and Yaping Huang and Ding Ding},
  doi          = {10.1109/TNNLS.2021.3131154},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5760-5773},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effects of motion-relevant knowledge from unlabeled video to Human–Object interaction detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low complexity gradient computation techniques to accelerate
deep neural network training. <em>TNNLS</em>, <em>34</em>(9), 5745–5759.
(<a href="https://doi.org/10.1109/TNNLS.2021.3130991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) training is an iterative process of updating network weights, called gradient computation, where (mini-batch) stochastic gradient descent (SGD) algorithm is generally used. Since SGD inherently allows gradient computations with noise, the proper approximation of computing weight gradients within SGD noise can be a promising technique to save energy/time consumptions during DNN training. This article proposes two novel techniques to reduce the computational complexity of the gradient computations for the acceleration of SGD-based DNN training. First, considering that the output predictions of a network (confidence) change with training inputs, the relation between the confidence and the magnitude of the weight gradient can be exploited to skip the gradient computations without seriously sacrificing the accuracy, especially for high confidence inputs. Second, the angle diversity-based approximations of intermediate activations for weight gradient calculation are also presented. Based on the fact that the angle diversity of gradients is small (highly uncorrelated) in the early training epoch, the bit precision of activations can be reduced to 2-/4-/8-bit depending on the resulting angle error between the original gradient and quantized gradient. The simulations show that the proposed approach can skip up to 75.83\% of gradient computations with negligible accuracy degradation for CIFAR-10 dataset using ResNet-20. Hardware implementation results using 65-nm CMOS technology also show that the proposed training accelerator achieves up to $1.69\times $ energy efficiency compared with other training accelerators.},
  archive      = {J_TNNLS},
  author       = {Dongyeob Shin and Geonho Kim and Joongho Jo and Jongsun Park},
  doi          = {10.1109/TNNLS.2021.3130991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5745-5759},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low complexity gradient computation techniques to accelerate deep neural network training},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AMITE: A novel polynomial expansion for analyzing neural
network nonlinearities. <em>TNNLS</em>, <em>34</em>(9), 5732–5744. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polynomial expansions are important in the analysis of neural network nonlinearities. They have been applied thereto addressing well-known difficulties in verification, explainability, and security. Existing approaches span classical Taylor and Chebyshev methods, asymptotics, and many numerical approaches. We find that, while these have useful properties individually, such as exact error formulas, adjustable domain, and robustness to undefined derivatives, there are no approaches that provide a consistent method, yielding an expansion with all these properties. To address this, we develop an analytically modified integral transform expansion (AMITE), a novel expansion via integral transforms modified using derived criteria for convergence. We show the general expansion and then demonstrate an application for two popular activation functions: hyperbolic tangent and rectified linear units. Compared with existing expansions (i.e., Chebyshev, Taylor, and numerical) employed to this end, AMITE is the first to provide six previously mutually exclusive desired expansion properties, such as exact formulas for the coefficients and exact expansion errors. We demonstrate the effectiveness of AMITE in two case studies. First, a multivariate polynomial form is efficiently extracted from a single hidden layer black-box multilayer perceptron (MLP) to facilitate equivalence testing from noisy stimulus–response pairs. Second, a variety of feedforward neural network (FFNN) architectures having between three and seven layers are range bounded using Taylor models improved by the AMITE polynomials and error formulas. AMITE presents a new dimension of expansion methods suitable for the analysis/approximation of nonlinearities in neural networks, opening new directions and opportunities for the theoretical analysis and systematic testing of neural networks.},
  archive      = {J_TNNLS},
  author       = {Mauro J. Sanchirico and Xun Jiao and C. Nataraj},
  doi          = {10.1109/TNNLS.2021.3130904},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5732-5744},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AMITE: A novel polynomial expansion for analyzing neural network nonlinearities},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Population-based hyperparameter tuning with multitask
collaboration. <em>TNNLS</em>, <em>34</em>(9), 5719–5731. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population-based optimization methods are widely used for hyperparameter (HP) tuning for a given specific task. In this work, we propose the population-based hyperparameter tuning with multitask collaboration (PHTMC), which is a general multitask collaborative framework with parallel and sequential phases for population-based HP tuning methods. In the parallel HP tuning phase, a shared population for all tasks is kept and the intertask relatedness is considered to both yield a better generalization ability and avoid data bias to a single task. In the sequential HP tuning phase, a surrogate model is built for each new-added task so that the metainformation from the existing tasks can be extracted and used to help the initialization for the new task. Experimental results show significant improvements in generalization abilities yielded by neural networks trained using the PHTMC and better performances achieved by multitask metalearning. Moreover, a visualization of the solution distribution and the autoencoder’s reconstruction of both the PHTMC and a single-task population-based HP tuning method is compared to analyze the property with the multitask collaboration.},
  archive      = {J_TNNLS},
  author       = {Wendi Li and Ting Wang and Wing W. Y. Ng},
  doi          = {10.1109/TNNLS.2021.3130896},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5719-5731},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Population-based hyperparameter tuning with multitask collaboration},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and smooth composite local learning-based adaptive
control. <em>TNNLS</em>, <em>34</em>(9), 5708–5718. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model structure representation and fast estimation of perturbations are two key research aspects in adaptive control. This work proposes a composite local learning adaptive control framework, which possesses fast and flexible approximation to system uncertainties and meanwhile smoothens control inputs. Local learning, which is a nonparametric regression approach, is able to automatically adjust the structure of approximator based on data distribution from the local region, but it is sensitive to the outliers and measurement noises. To tackle this problem, the regression filter technique is employed to attenuate the adverse effect of noises by smoothing the output response and state features. In addition, the stable integral adaptation is integrated into local learning framework to further enhance the system robustness and smoothness of the estimation. Through the online elimination of uncertainties, the nominal control performance is recovered when the plant encounters violent perturbations. Stability analysis and numerical simulations are performed to demonstrate the effectiveness and benefits of the proposed control method. The proposed approach exhibits a promising performance in terms of rapid perturbation elimination and accurate tracking control.},
  archive      = {J_TNNLS},
  author       = {Tao Jiang and Jiangshuai Huang and Xiaojie Su},
  doi          = {10.1109/TNNLS.2021.3130812},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5708-5718},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast and smooth composite local learning-based adaptive control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SCANet: Securing the weights with superparamagnetic-MTJ
crossbar array networks. <em>TNNLS</em>, <em>34</em>(9), 5693–5707. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) form a critical infrastructure supporting various systems, spanning from the iPhone neural engine to imaging satellites and drones. The design of these neural cores is often proprietary or a military secret. Nevertheless, they remain vulnerable to model replication attacks that seek to reverse engineer the network’s synaptic weights. In this article, we propose SCANet ( S uperparamagnetic-MTJ C rossbar A rray Net works), a novel defense mechanism against such model stealing attacks by utilizing the innate stochasticity in superparamagnets. When used as the synapse in DNNs, superparamagnetic magnetic tunnel junctions (s-MTJs) are shown to be significantly more secure than prior memristor-based solutions. The thermally induced telegraphic switching in the s-MTJs is robust and uncontrollable, thus thwarting the attackers from obtaining sensitive data from the network. Using a mixture of both superparamagnetic and conventional MTJs in the neural network (NN), the designer can optimize the time period between the weight updation and the power consumed by the system. Furthermore, we propose a modified NN architecture that can prevent replication attacks while minimizing power consumption. We investigate the effect of the number of layers in the deep network and the number of neurons in each layer on the sharpness of accuracy degradation when the network is under attack. We also explore the efficacy of SCANet in real-time scenarios, using a case study on object detection.},
  archive      = {J_TNNLS},
  author       = {Dinesh Rajasekharan and Nikhil Rangarajan and Satwik Patnaik and Ozgur Sinanoglu and Yogesh Singh Chauhan},
  doi          = {10.1109/TNNLS.2021.3130884},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5693-5707},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SCANet: Securing the weights with superparamagnetic-MTJ crossbar array networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameter efficient neural networks with singular value
decomposed kernels. <em>TNNLS</em>, <em>34</em>(9), 5682–5692. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, neural networks are viewed from the perspective of connected neuron layers represented as matrix multiplications. We propose to compose these weight matrices from a set of orthogonal basis matrices by approaching them as elements of the real matrices vector space under addition and multiplication. Making use of the Kronecker product for vectors, this composition is unified with the singular value decomposition (SVD) of the weight matrix. The orthogonal components of this SVD are trained with a descent curve on the Stiefel manifold using the Cayley transform. Next, update equations for the singular values and initialization routines are derived. Finally, acceleration for stochastic gradient descent optimization using this formulation is discussed. Our proposed method allows more parameter-efficient representations of weight matrices in neural networks. These decomposed weight matrices achieve maximal performance in both standard and more complicated neural architectures. Furthermore, the more parameter-efficient decomposed layers are shown to be less dependent on optimization and better conditioned. As a tradeoff, training time is increased up to a factor of 2. These observations are consequently attributed to the properties of the method and choice of optimization over the manifold of orthogonal matrices.},
  archive      = {J_TNNLS},
  author       = {David Vander Mijnsbrugge and Femke Ongenae and Sofie Van Hoecke},
  doi          = {10.1109/TNNLS.2021.3130756},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5682-5692},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameter efficient neural networks with singular value decomposed kernels},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RDLNet: A regularized descriptor learning network.
<em>TNNLS</em>, <em>34</em>(9), 5669–5681. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local image descriptor learning has been instrumental in various computer vision tasks. Recent innovations lie with similarity measurement of descriptor vectors with metric learning for randomly selected Siamese or triplet patches. Local image descriptor learning focuses more on hard samples since easy samples do not contribute much to optimization. However, few studies focus on hard samples of image patches from the perspective of loss functions and design appropriate learning algorithms to obtain a more compact descriptor representation. This article proposes a regularized descriptor learning network (RDLNet) that makes the network focus on the learning of hard samples and compact descriptor with triplet networks. A novel hard sample mining strategy is designed to select the hardest negative samples in mini-batch. Then batch margin loss concerned with hard samples is adopted to optimize the distance of extreme cases. Finally, for a more stable network and preventing network collapsing, orthogonal regularization is designed to constrain convolutional kernels and obtain rich deep features. RDLNet provides a compact discriminative low-dimensional representation and can be embedded in other pipelines easily. This article gives extensive experimental results for large benchmarks in multiple scenarios and generalization in matching applications with significant improvements.},
  archive      = {J_TNNLS},
  author       = {Jun Zhang and Licheng Jiao and Wenping Ma and Fang Liu and Xu Liu and Lingling Li and Hao Zhu},
  doi          = {10.1109/TNNLS.2021.3130655},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5669-5681},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RDLNet: A regularized descriptor learning network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SocialSift: Target query discovery on online social media
with deep reinforcement learning. <em>TNNLS</em>, <em>34</em>(9),
5654–5668. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the prohibitively large volume of posts (e.g., tweets in Twitter) on online social networks (OSNs), how to design effective queries to explore the ones of interest is a pressing problem. There are two main challenges to address the problem. First, given public application programming interfaces (APIs) for querying posts related to keywords from an extremely large vocabulary, how to infer the keywords relevant to our target interest using as few queries as possible? Second, how to deal with the agnostics of OSN’s API? i.e., as different social networks typically have different running mechanisms, even with some randomness in returning results, how to build the knowledge of the API returns w.r.t. target interests from scratches? To address the above two challenges, we propose a target query discovery framework based on a deep reinforcement learning approach, named SocialSift. SocialSift intelligently interacts with OSNs’ keyword-based API and develops its own knowledge in searching the optimal queries w.r.t. the target interests as well as OSN APIs. Specifically, to address the first challenge, we are inspired by the human searching experience, and recognize learning to query with context awareness to reduce the searching space, by qualifying keywords from returned results and keeping the tracks of the query trial history, or say contexts. As for addressing the second challenge, we treat OSNs’ APIs as black boxes and probabilistically quantify query-interest pairs guided by rewards, which is a well-curated indicator w.r.t. target interests. Empirical results on three popular OSNs: Twitter, Reddit, and Amazon demonstrate our SocialSift significantly outperforms the state-of-the-art baselines by 12\% in retrieving target posts.},
  archive      = {J_TNNLS},
  author       = {Changyu Wang and Pinghui Wang and Tao Qin and Chenxu Wang and Suhansanu Kumar and Xiaohong Guan and Jun Liu and Kevin Chen-Chuan Chang},
  doi          = {10.1109/TNNLS.2021.3130587},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5654-5668},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SocialSift: Target query discovery on online social media with deep reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved non-negative latent factor model for missing
data estimation via extragradient-based alternating direction method.
<em>TNNLS</em>, <em>34</em>(9), 5640–5653. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an improved double factorization-based symmetric and non-negative latent factor (Im-DF-SNLF) model is proposed to make the estimation for missing data in symmetric, high-dimensional, and sparse (SHiDS) matrices. The main idea of the Im-DF-SNLF model is fourfold: 1) considering the data variety in the practical engineering, non-negative latent factors (NLFs) in different cases are considered to better reflect the latent relationships between entries; 2) the $l_{2}$ -norm regularization and the Lagrangian multiplier technique are simultaneously adopted to handle the overfitting and satisfy the non-negative constraint for latent factors (LFs); 3) the extragradient-based alternating direction (EGAD) method is utilized to accelerate the model training and rigidly guarantee the non-negativity of LFS; and 4) a rigorous proof is provided to show that, under the given assumption that the objective function is smooth and has a Lipschitz continuous gradient, the designed algorithm can find an $\epsilon $ -optimal solution within $O(1/\epsilon )$ , and the upper bound of the learning rate is given by 1/2. Finally, experimental results on public datasets are given to demonstrate the effectiveness of our proposed Im-DF-SNLF model with EGAD.},
  archive      = {J_TNNLS},
  author       = {Ming Li and Yan Song},
  doi          = {10.1109/TNNLS.2021.3130289},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5640-5653},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An improved non-negative latent factor model for missing data estimation via extragradient-based alternating direction method},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed optimization for second-order discrete-time
multiagent systems with set constraints. <em>TNNLS</em>, <em>34</em>(9),
5629–5639. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization problem of second-order discrete-time multiagent systems with set constraints is studied in this article. In particular, the involved agents cooperatively search an optimal solution of a global objective function summed by multiple local ones within the intersection of multiple constrained sets. We also consider that each pair of local objective function and constrained set is exclusively accessible to the respective agent, and each agent just interacts with its local neighbors. By borrowing from the consensus idea, a projection-based distributed optimization algorithm resorting to an auxiliary dynamics is first proposed without interacting the gradient information of local objective functions. Next, by considering the local objective functions being strongly convex, selection criteria of step size and algorithm parameter are built such that the unique solution to the concerned optimization problem is obtained. Moreover, by fixing a unit step size, it is also shown that the optimization result can be relaxed to the case with just convex local objective functions given a properly chosen algorithm parameter. Finally, practical and numerical examples are taken to verify the proposed optimization results.},
  archive      = {J_TNNLS},
  author       = {Yao Zou and Kewei Xia and Bomin Huang and Ziyang Meng},
  doi          = {10.1109/TNNLS.2021.3130173},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5629-5639},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed optimization for second-order discrete-time multiagent systems with set constraints},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image manipulation localization using attentional
cross-domain CNN features. <em>TNNLS</em>, <em>34</em>(9), 5614–5628.
(<a href="https://doi.org/10.1109/TNNLS.2021.3130168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the advancement of manipulation technologies, image modification is becoming increasingly convenient and imperceptible. To tackle the challenging image tampering detection problem, this article presents an attentional cross-domain deep architecture, which can be trained end-to-end. This architecture is composed of three convolutional neural network (CNN) streams to extract three types of features, including visual perception, resampling, and local inconsistency features, from spatial and frequency domains. The multitype and cross-domain features are then combined to formulate hybrid features to distinguish manipulated regions from nonmanipulated parts. Compared with other deep architectures, the proposed one spans a more complementary and discriminative feature space by integrating richer types of features from different domains in a unified end-to-end trainable framework and thus can better capture artifacts caused by different types of manipulations. In addition, we design and train a module called tampering discriminative attention network (TDA-Net) to highlight suspicious parts. These part-level representations are then integrated with the global ones to further enhance the discriminating capability of the hybrid features. To adequately train the proposed architecture, we synthesize a large dataset containing various types of manipulations based on DRESDEN and COCO. Experiments on four public datasets demonstrate that the proposed model can localize various manipulations and achieve the state-of-the-art performance. We also conduct ablation studies to verify the effectiveness of each stream and the TDA-Net module.},
  archive      = {J_TNNLS},
  author       = {Shuaibo Li and Shibiao Xu and Wei Ma and Qiu Zong},
  doi          = {10.1109/TNNLS.2021.3130168},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5614-5628},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Image manipulation localization using attentional cross-domain CNN features},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-h∞ controls for unknown input-interference nonlinear
system with reinforcement learning. <em>TNNLS</em>, <em>34</em>(9),
5601–5613. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the multi- $\text{H}\infty $ controls for the input-interference nonlinear systems via adaptive dynamic programming (ADP) method, which allows for multiple inputs to have the individual selfish component of the strategy to resist weighted interference. In this line, the ADP scheme is used to learn the Nash-optimization solutions of the input-interference nonlinear system such that multiple $\text{H}\infty $ performance indices can reach the defined Nash equilibrium. First, the input-interference nonlinear system is given and the Nash equilibrium is defined. An adaptive neural network (NN) observer is introduced to identify the input-interference nonlinear dynamics. Then, the critic NNs are used to learn the multiple $\text{H}\infty $ performance indices. A novel adaptive law is designed to update the critic NN weights by minimizing the Hamiltonian–Jacobi–Isaacs (HJI) equation, which can be used to directly calculate the multi- $\text{H}\infty $ controls effectively by using input–output data such that the actor structure is avoided. Moreover, the control system stability and updated parameter convergence are proved. Finally, two numerical examples are simulated to verify the proposed ADP scheme for the input-interference nonlinear system.},
  archive      = {J_TNNLS},
  author       = {Yongfeng Lv and Jing Na and Xiaowei Zhao and Yingbo Huang and Xuemei Ren},
  doi          = {10.1109/TNNLS.2021.3130092},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5601-5613},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-h∞ controls for unknown input-interference nonlinear system with reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered prescribed settling time consensus
compensation control for a class of uncertain nonlinear systems with
actuator failures. <em>TNNLS</em>, <em>34</em>(9), 5590–5600. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a class of uncertain nonlinear systems with actuator failures, the event-triggered prescribed settling time consensus adaptive compensation control method is proposed. The unknown form of actuator failures may occur in practical applications, resulting in system instability or even control failure. In order to effectively deal with the above problems, a neural network adaptive control method is developed to ensure that the system states rapidly converge in the event of failure and compensate for the failures of actuator. Meanwhile, a nonlinear transformation function is introduced to make sure that the tracking error converges for the predefined interval within a prescribed settling time, which makes that the convergence time can be preset. Furthermore, a finite-time event-triggered compensation control strategy is established by the backstepping technology. Under this strategy, the system not only can rapidly stabilize in finite time but also can effectively save network bandwidth. In addition, the states of the system are globally uniformly bounded. Finally, the theoretical analysis and simulation experiments validate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jianhui Wang and Qijuan Gong and Kunfeng Huang and Zhi Liu and C. L. Philip Chen and Jie Liu},
  doi          = {10.1109/TNNLS.2021.3129816},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5590-5600},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered prescribed settling time consensus compensation control for a class of uncertain nonlinear systems with actuator failures},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distance transform pooling neural network for LiDAR depth
completion. <em>TNNLS</em>, <em>34</em>(9), 5580–5589. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering dense depth maps from sparse depth sensors, such as LiDAR, is a recently proposed task with many computer vision and robotics applications. Previous works have identified input sparsity as the key challenge of this task. To solve the sparsity challenge, we propose a recurrent distance transform pooling (DTP) module that aggregates multi-level nearby information prior to the backbone neural network. The intuition of this module is originated from the observation that most pixels within the receptive field of the network are zero. This indicates a deep and heavy network structure has to be used to enlarge the receptive field aiming at capturing enough useful information as most processed signals are uninformative zeros. Our recurrent DTP module can fill in empty pixels with the nearest value in a local patch and recurrently transform distance to reach farther nearest points. The output of the proposed DTP module is a collection of multi-level semi-dense depth maps from original sparse to almost full. Processing this collection of semi-dense depth maps alleviates the network from the input sparsity, which helps a lightweight simplified ResNet-18 with 1M parameters achieve state-of-the-art performance on the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) depth completion benchmark with LiDAR only. Besides the sparsity, the input LiDAR map also contains some incorrect values due to the sensor error. Thus, we further enhance the DTP with an error correction (EC) module to avoid the spreading of the incorrect input values. At last, we discuss the benefit of only using LiDAR for nighttime driving and the potential extension of the proposed method for sensor fusion and the indoor scenario. The code has been released online at https://github.com/placeforyiming/DistanceTransform-DepthCompletion .},
  archive      = {J_TNNLS},
  author       = {Yiming Zhao and Mahdi Elhousni and Ziming Zhang and Xinming Huang},
  doi          = {10.1109/TNNLS.2021.3129801},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5580-5589},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distance transform pooling neural network for LiDAR depth completion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting miRNA–disease associations through deep
autoencoder with multiple kernel learning. <em>TNNLS</em>,
<em>34</em>(9), 5570–5579. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining microRNA (miRNA)–disease associations (MDAs) is an integral part in the prevention, diagnosis, and treatment of complex diseases. However, wet experiments to discern MDAs are inefficient and expensive. Hence, the development of reliable and efficient data integrative models for predicting MDAs is of significant meaning. In the present work, a novel deep learning method for predicting MDAs through deep autoencoder with multiple kernel learning (DAEMKL) is presented. Above all, DAEMKL applies multiple kernel learning (MKL) in miRNA space and disease space to construct miRNA similarity network and disease similarity network, respectively. Then, for each disease or miRNA, its feature representation is learned from the miRNA similarity network and disease similarity network via the regression model. After that, the integrated miRNA feature representation and disease feature representation are input into deep autoencoder (DAE). Furthermore, the novel MDAs are predicted through reconstruction error. Ultimately, the AUC results show that DAEMKL achieves outstanding performance. In addition, case studies of three complex diseases further prove that DAEMKL has excellent predictive performance and can discover a large number of underlying MDAs. On the whole, our method DAEMKL is an effective method to identify MDAs.},
  archive      = {J_TNNLS},
  author       = {Feng Zhou and Meng-Meng Yin and Cui-Na Jiao and Jing-Xiu Zhao and Chun-Hou Zheng and Jin-Xing Liu},
  doi          = {10.1109/TNNLS.2021.3129772},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5570-5579},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Predicting miRNA–Disease associations through deep autoencoder with multiple kernel learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Balancing learning model privacy, fairness, and accuracy
with early stopping criteria. <em>TNNLS</em>, <em>34</em>(9), 5557–5569.
(<a href="https://doi.org/10.1109/TNNLS.2021.3129592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep learning models mature, one of the most prescient questions we face is: what is the ideal tradeoff between accuracy, fairness, and privacy (AFP)? Unfortunately, both the privacy and the fairness of a model come at the cost of its accuracy. Hence, an efficient and effective means of fine-tuning the balance between this trinity of needs is critical. Motivated by some curious observations in privacy–accuracy tradeoffs with differentially private stochastic gradient descent (DP-SGD), where fair models sometimes result, we conjecture that fairness might be better managed as an indirect byproduct of this process. Hence, we conduct a series of analyses, both theoretical and empirical, on the impacts of implementing DP-SGD in deep neural network models through gradient clipping and noise addition. The results show that, in deep learning, the number of training epochs is central to striking a balance between AFP because DP-SGD makes the training less stable, providing the possibility of model updates at a low discrimination level without much loss in accuracy. Based on this observation, we designed two different early stopping criteria to help analysts choose the optimal epoch at which to stop training a model so as to achieve their ideal tradeoff. Extensive experiments show that our methods can achieve an ideal balance between AFP.},
  archive      = {J_TNNLS},
  author       = {Tao Zhang and Tianqing Zhu and Kun Gao and Wanlei Zhou and Philip S. Yu},
  doi          = {10.1109/TNNLS.2021.3129592},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5557-5569},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Balancing learning model privacy, fairness, and accuracy with early stopping criteria},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonautoregressive encoder–decoder neural framework for
end-to-end aspect-based sentiment triplet extraction. <em>TNNLS</em>,
<em>34</em>(9), 5544–5556. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment triplet extraction (ASTE) aims at recognizing the joint triplets from texts, i.e., aspect terms, opinion expressions, and correlated sentiment polarities. As a newly proposed task, ASTE depicts the complete sentiment picture from different perspectives to better facilitate real-world applications. Unfortunately, several major challenges, such as the overlapping issue and long-distance dependency, have not been addressed effectively by the existing ASTE methods, which limits the performance of the task. In this article, we present an innovative encoder–decoder framework for end-to-end ASTE. Specifically, the ASTE task is first modeled as an unordered triplet set prediction problem, which is satisfied with a nonautoregressive decoding paradigm with a pointer network. Second, a novel high-order aggregation mechanism is proposed for fully integrating the underlying interactions between the overlapping structure of aspect and opinion terms. Third, a bipartite matching loss is introduced for facilitating the training of our nonautoregressive system. Experimental results on benchmark datasets show that our proposed framework significantly outperforms the state-of-the-art methods. Further analysis demonstrates the advantages of the proposed framework in handling the overlapping issue, relieving long-distance dependency and decoding efficiency.},
  archive      = {J_TNNLS},
  author       = {Hao Fei and Yafeng Ren and Yue Zhang and Donghong Ji},
  doi          = {10.1109/TNNLS.2021.3129483},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5544-5556},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonautoregressive Encoder–Decoder neural framework for end-to-end aspect-based sentiment triplet extraction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedAUX: Leveraging unlabeled auxiliary data in federated
learning. <em>TNNLS</em>, <em>34</em>(9), 5531–5543. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated distillation (FD) is a popular novel algorithmic paradigm for Federated learning (FL), which achieves training performance competitive to prior parameter averaging-based methods, while additionally allowing the clients to train different model architectures, by distilling the client predictions on an unlabeled auxiliary set of data into a student model. In this work, we propose FedAUX, an extension to FD, which, under the same set of assumptions, drastically improves the performance by deriving maximum utility from the unlabeled auxiliary data. FedAUX modifies the FD training procedure in two ways: First, unsupervised pre-training on the auxiliary data is performed to find a suitable model initialization for the distributed training. Second, $(\varepsilon, \delta)$ -differentially private certainty scoring is used to weight the ensemble predictions on the auxiliary data according to the certainty of each client model. Experiments on large-scale convolutional neural networks (CNNs) and transformer models demonstrate that our proposed method achieves remarkable performance improvements over state-of-the-art FL methods, without adding appreciable computation, communication, or privacy cost. For instance, when training ResNet8 on non-independent identically distributed (i.i.d.) subsets of CIFAR10, FedAUX raises the maximum achieved validation accuracy from 30.4\% to 78.1\%, further closing the gap to centralized training performance. Code is available at https://github.com/fedl-repo/fedaux .},
  archive      = {J_TNNLS},
  author       = {Felix Sattler and Tim Korjakow and Roman Rischke and Wojciech Samek},
  doi          = {10.1109/TNNLS.2021.3129371},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5531-5543},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FedAUX: Leveraging unlabeled auxiliary data in federated learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonsmooth optimization-based model and algorithm for
semisupervised clustering. <em>TNNLS</em>, <em>34</em>(9), 5517–5530.
(<a href="https://doi.org/10.1109/TNNLS.2021.3129370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a nonconvex nonsmooth optimization approach, we introduce a model for semisupervised clustering (SSC) with pairwise constraints. In this model, the objective function is represented as a sum of three terms: the first term reflects the clustering error for unlabeled data points, the second term expresses the error for data points with must-link (ML) constraints, and the third term represents the error for data points with cannot-link (CL) constraints. This function is nonconvex and nonsmooth. To find its optimal solutions, we introduce an adaptive SSC (A-SSC) algorithm. This algorithm is based on the combination of the nonsmooth optimization method and an incremental approach, which involves the auxiliary SSC problem. The algorithm constructs clusters incrementally starting from one cluster and gradually adding one cluster center at each iteration. The solutions to the auxiliary SSC problem are utilized as starting points for solving the nonconvex SSC problem. The discrete gradient method (DGM) of nonsmooth optimization is applied to solve the underlying nonsmooth optimization problems. This method does not require subgradient evaluations and uses only function values. The performance of the A-SSC algorithm is evaluated and compared with four benchmarking SSC algorithms on one synthetic and 12 real-world datasets. Results demonstrate that the proposed algorithm outperforms the other four algorithms in identifying compact and well-separated clusters while satisfying most constraints.},
  archive      = {J_TNNLS},
  author       = {Adil M. Bagirov and Sona Taheri and Fusheng Bai and Fangying Zheng},
  doi          = {10.1109/TNNLS.2021.3129370},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5517-5530},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonsmooth optimization-based model and algorithm for semisupervised clustering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning in visual tracking: A review. <em>TNNLS</em>,
<em>34</em>(9), 5497–5516. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has made breakthroughs in many computer vision tasks and also in visual tracking. From the beginning of the research on the automatic acquisition of high abstract feature representation, DL has gone deep into all aspects of tracking to date, to name a few, similarity metric, data association, and bounding box estimation. Also, pure DL-based trackers have obtained the state-of-the-art performance after the community’s constant research. We believe that it is time to comprehensively review the development of DL research in visual tracking. In this article, we overview the critical improvements brought to the field by DL: deep feature representations, network architecture, and four crucial issues in visual tracking (spatiotemporal information integration, target-specific classification, target information update, and bounding box estimation). The scope of the survey of DL-based tracking covers two primary subtasks for the first time, single-object tracking and multiple-object tracking. Also, we analyze the performance of DL-based approaches and give meaningful conclusions. Finally, we provide several promising directions and tasks in visual tracking and relevant fields.},
  archive      = {J_TNNLS},
  author       = {Licheng Jiao and Dan Wang and Yidong Bai and Puhua Chen and Fang Liu},
  doi          = {10.1109/TNNLS.2021.3136907},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5497-5516},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning in visual tracking: A review},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive review of continuous-/discontinuous-time
fractional-order multidimensional neural networks. <em>TNNLS</em>,
<em>34</em>(9), 5476–5496. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamical study of continuous-/discontinuous-time fractional-order neural networks (FONNs) has been thoroughly explored, and several publications have been made available. This study is designed to give an exhaustive review of the dynamical studies of multidimensional FONNs in continuous/discontinuous time, including Hopfield NNs (HNNs), Cohen–Grossberg NNs, and bidirectional associative memory NNs, and similar models are considered in real ( $\mathbb {R}$ ), complex ( $\mathbb {C}$ ), quaternion ( $\mathbb {Q}$ ), and octonion ( $\mathbb {O}$ ) fields. Since, in practice, delays are unavoidable, theoretical findings from multidimensional FONNs with various types of delays are thoroughly evaluated. Some required and adequate stability and synchronization requirements are also mentioned for fractional-order NNs without delays.},
  archive      = {J_TNNLS},
  author       = {Jinde Cao and K. Udhayakumar and R. Rakkiyappan and Xiaodi Li and Jianquan Lu},
  doi          = {10.1109/TNNLS.2021.3129829},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5476-5496},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A comprehensive review of continuous-/Discontinuous-time fractional-order multidimensional neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered cardinality-constrained cooling and
electrical load dispatch based on collaborative neurodynamic
optimization. <em>TNNLS</em>, <em>34</em>(9), 5464–5475. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses event-triggered optimal load dispatching based on collaborative neurodynamic optimization. Two cardinality-constrained global optimization problems are formulated and two event-triggering functions are defined for event-triggered load dispatching in thermal energy and electric power systems. An event-triggered dispatching method is developed in the collaborative neurodynamic optimization framework with multiple projection neural networks and a meta-heuristic updating rule. Experimental results are elaborated to demonstrate the efficacy and superiority of the approach against many existing methods for optimal load dispatching in air conditioning systems and electric power generation systems.},
  archive      = {J_TNNLS},
  author       = {Zhongying Chen and Jun Wang and Qing-Long Han},
  doi          = {10.1109/TNNLS.2022.3160645},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5464-5475},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered cardinality-constrained cooling and electrical load dispatch based on collaborative neurodynamic optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multifingered robot hand compliant manipulation based on
vision-based demonstration and adaptive force control. <em>TNNLS</em>,
<em>34</em>(9), 5452–5463. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multifingered hand dexterous manipulation is quite challenging in the domain of robotics. One remaining issue is how to achieve compliant behaviors. In this work, we propose a human-in-the-loop learning-control approach for acquiring compliant grasping and manipulation skills of a multifinger robot hand. This approach takes the depth image of the human hand as input and generates the desired force commands for the robot. The markerless vision-based teleoperation system is used for the task demonstration, and an end-to-end neural network model (i.e., TeachNet) is trained to map the pose of the human hand to the joint angles of the robot hand in real-time. To endow the robot hand with compliant human-like behaviors, an adaptive force control strategy is designed to predict the desired force control commands based on the pose difference between the robot hand and the human hand during the demonstration. The force controller is derived from a computational model of the biomimetic control strategy in human motor learning, which allows adapting the control variables (impedance and feedforward force) online during the execution of the reference joint angles. The simultaneous adaptation of the impedance and feedforward profiles enables the robot to interact with the environment compliantly. Our approach has been verified in both simulation and real-world task scenarios based on a multifingered robot hand, that is, the Shadow Hand, and has shown more reliable performances than the current widely used position control mode for obtaining compliant grasping and manipulation behaviors.},
  archive      = {J_TNNLS},
  author       = {Chao Zeng and Shuang Li and Zhaopeng Chen and Chenguang Yang and Fuchun Sun and Jianwei Zhang},
  doi          = {10.1109/TNNLS.2022.3184258},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5452-5463},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multifingered robot hand compliant manipulation based on vision-based demonstration and adaptive force control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive hybrid optimization learning-based accurate motion
planning of multi-joint arm. <em>TNNLS</em>, <em>34</em>(9), 5440–5451.
(<a href="https://doi.org/10.1109/TNNLS.2023.3262109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion planning is important to the automatic operation of the manipulator. It is difficult for traditional motion planning algorithms to achieve efficient online motion planning in a rapidly changing environment and high-dimensional planning space. The neural motion planning (NMP) algorithm based on reinforcement learning provides a new way to solve the above-mentioned task. Aiming to overcome the difficulty of training the neural network in high-accuracy planning tasks, this article proposes to combine the artificial potential field (APF) method and reinforcement learning. The neural motion planner can avoid obstacles in a wide range; meanwhile, the APF method is exploited to adjust the partial position. Considering that the action space of the manipulator is high-dimensional and continuous, the soft-actor-critic (SAC) algorithm is adopted to train the neural motion planner. By training and testing with different accuracy values in a simulation engine, it is verified that, in the high-accuracy planning tasks, the success rate of the proposed hybrid method is better than using the two algorithms alone. Finally, the feasibility of directly transferring the learned neural network to the real manipulator is verified by a dynamic obstacle-avoidance task.},
  archive      = {J_TNNLS},
  author       = {Chengchao Bai and Jiawei Zhang and Jifeng Guo and C. Patrick Yue},
  doi          = {10.1109/TNNLS.2023.3262109},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5440-5451},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive hybrid optimization learning-based accurate motion planning of multi-joint arm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unpaired artistic portrait style transfer via asymmetric
double-stream GAN. <em>TNNLS</em>, <em>34</em>(9), 5427–5439. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of image style transfer technologies, portrait style transfer has attracted growing attention in this research community. In this article, we present an asymmetric double-stream generative adversarial network (ADS-GAN) to solve the problems that caused by cartoonization and other style transfer techniques when they are applied to portrait photos, such as facial deformation, contours missing, and stiff lines. By observing the characteristics between source and target images, we propose an edge contour retention (ECR) regularized loss to constrain the local and global contours of generated portrait images to avoid the portrait deformation. In addition, a content-style feature fusion module is introduced for further learning of the target image style, which uses a style attention mechanism to integrate features and embeds style features into content features of portrait photos according to the attention weights. Finally, a guided filter is introduced in content encoder to smooth the textures and specific details of source image, thereby eliminating its negative impact on style transfer. We conducted overall unified optimization training on all components and got an ADS-GAN for unpaired artistic portrait style transfer. Qualitative comparisons and quantitative analyses demonstrate that the proposed method generates superior results than benchmark work in preserving the overall structure and contours of portrait; ablation and parameter study demonstrate the effectiveness of each component in our framework.},
  archive      = {J_TNNLS},
  author       = {Fanmin Kong and Yuanyuan Pu and Ivan Lee and Rencan Nie and Zhengpeng Zhao and Dan Xu and Wenhua Qian and Hong Liang},
  doi          = {10.1109/TNNLS.2023.3263846},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5427-5439},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unpaired artistic portrait style transfer via asymmetric double-stream GAN},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dimensionality-reducible operational optimal control for
wastewater treatment process. <em>TNNLS</em>, <em>34</em>(9), 5418–5426.
(<a href="https://doi.org/10.1109/TNNLS.2022.3192246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operational optimal control (OOC) is an essential component of wastewater treatment process (WWTP). The control variables usually are high-dimensional, nonlinear, and strongly coupled, which can easily fail traditional optimization control methods. Mathematically, these operational variables usually are in the unknown low-dimensional space embedded in the high-dimensional space. Therefore, the OOC problem of WWTP can be resolved as an optimization challenge involving low-dimensional space, and the unknown low-dimensional space is presented in the form of a set of controlled variables in a high-dimensional space, which is normal in real-world industries. Here, a dimension-reducible data-driven optimization control framework for WWTP is proposed. Considering the difficulty in elucidating the whole space of set points, a neural network is designed to approximate the constraint relationship between control variables. The search process is based on optimization methods in low-dimensional space embedded into Euclidean spaces. Furthermore, the convergence of the process is ensured via mathematical analysis. Finally, the experimental simulation of wastewater treatment revealed that this approach is effective for an optimal solution in control systems.},
  archive      = {J_TNNLS},
  author       = {Qili Chen and Junfang Fan and Wenbai Chen and Ancai Zhang and Guangyuan Pan},
  doi          = {10.1109/TNNLS.2022.3192246},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5418-5426},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A dimensionality-reducible operational optimal control for wastewater treatment process},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Noah: Reinforcement-learning-based rate limiter for
microservices in large-scale e-commerce services. <em>TNNLS</em>,
<em>34</em>(9), 5403–5417. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern large-scale online service providers typically deploy microservices into containers to achieve flexible service management. One critical problem in such container-based microservice architectures is to control the arrival rate of requests in the containers to avoid containers from being overloaded. In this article, we present our experience of rate limit for the containers in Alibaba, one of the largest e-commerce services in the world. Given the highly diverse characteristics of containers in Alibaba, we point out that the existing rate limit mechanisms cannot meet our demand. Thus, we design Noah, a dynamic rate limiter that can automatically adapt to the specific characteristic of each container without human efforts. The key idea of Noah is to use deep reinforcement learning (DRL) that automatically infers the most suitable configuration for each container. To fully embrace the advantages of DRL in our context, Noah addresses two technical challenges. First, Noah uses a lightweight system monitoring mechanism to collect container status. In this way, it minimizes the monitoring overhead while ensuring a timely reaction to system load changes. Second, Noah injects synthetic extreme data when training its models. Thus, its model gains knowledge on unseen special events and hence remains highly available in extreme scenarios. To guarantee model convergence with the injected training data, Noah adopts task-specific curriculum learning to train the model from normal data to extreme data gradually. Noah has been deployed in the production of Alibaba for two years, serving more than 50000 containers and around 300 types of microservice applications. Experimental results show that Noah can well adapt to three common scenarios in the production environment. It effectively achieves better system availability and shorter request response time compared with four state-of-the-art rate limiters.},
  archive      = {J_TNNLS},
  author       = {Zhao Li and Haifeng Sun and Zheng Xiong and Qun Huang and Zehong Hu and Ding Li and Shasha Ruan and Hai Hong and Jie Gui and Jintao He and Zebin Xu and Yang Fang},
  doi          = {10.1109/TNNLS.2023.3264038},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5403-5417},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Noah: Reinforcement-learning-based rate limiter for microservices in large-scale E-commerce services},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven tabulation for chemistry integration using
recurrent neural networks. <em>TNNLS</em>, <em>34</em>(9), 5392–5402.
(<a href="https://doi.org/10.1109/TNNLS.2022.3175301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the wide range of time scales involved in the ordinary differential equations (ODEs) describing chemical reaction kinetics, multidimensional numerical simulation of chemical reactive flows using detailed combustion mechanisms is computationally expensive. To confront this issue, this article presents an economic data-driven tabulation algorithm for fast combustion chemistry integration. It uses the recurrent neural networks (RNNs) to construct the tabulation from a series of current and past states to the next state, which takes full advantage of RNN in handling long-term dependencies of time series data. The training data are first generated from direct numerical integrations to form an initial state space, which is divided into several subregions by the K-means algorithm. The centroid of each cluster is also determined at the same time. Next, an Elman RNN is constructed in each of these subregions to approximate the expensive direct integration, in which the integration routine obtained from the centroid is regarded as the basis for a storing and retrieving solution to ODEs. Finally, the alpha-shape metrics with principal component analysis (PCA) are used to generate a set of reduced-order geometric constraints that characterize the applicable range of these RNN approximations. For online implementation, geometric constraints are frequently verified to determine which RNN network to be used to approximate the integration routine. The advantage of the proposed algorithm is to use a set of RNNs to replace the expensive direct integration, which allows to reduce both the memory consumption and computational cost. Numerical simulations of a H2/CO-air combustion process are performed to demonstrate the effectiveness of the proposed algorithm compared to the existing ODE solver.},
  archive      = {J_TNNLS},
  author       = {Yu Zhang and Qingguo Lin and Wenli Du and Feng Qian},
  doi          = {10.1109/TNNLS.2022.3175301},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5392-5402},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven tabulation for chemistry integration using recurrent neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An entropy weighted nonnegative matrix factorization
algorithm for feature representation. <em>TNNLS</em>, <em>34</em>(9),
5381–5391. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) has been widely used to learn low-dimensional representations of data. However, NMF pays the same attention to all attributes of a data point, which inevitably leads to inaccurate representations. For example, in a human-face dataset, if an image contains a hat on a head, the hat should be removed or the importance of its corresponding attributes should be decreased during matrix factorization. This article proposes a new type of NMF called entropy weighted NMF (EWNMF), which uses an optimizable weight for each attribute of each data point to emphasize their importance. This process is achieved by adding an entropy regularizer to the cost function and then using the Lagrange multiplier method to solve the problem. Experimental results with several datasets demonstrate the feasibility and effectiveness of the proposed method. The code developed in this study is available at https://github.com/Poisson-EM/Entropy-weighted-NMF .},
  archive      = {J_TNNLS},
  author       = {Jiao Wei and Can Tong and Bingxue Wu and Qiang He and Shouliang Qi and Yudong Yao and Yueyang Teng},
  doi          = {10.1109/TNNLS.2022.3184286},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5381-5391},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An entropy weighted nonnegative matrix factorization algorithm for feature representation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metalearning-based alternating minimization algorithm for
nonconvex optimization. <em>TNNLS</em>, <em>34</em>(9), 5366–5380. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel solution for nonconvex problems of multiple variables, especially for those typically solved by an alternating minimization (AM) strategy that splits the original optimization problem into a set of subproblems corresponding to each variable and then iteratively optimizes each subproblem using a fixed updating rule. However, due to the intrinsic nonconvexity of the original optimization problem, the optimization can be trapped into a spurious local minimum even when each subproblem can be optimally solved at each iteration. Meanwhile, learning-based approaches, such as deep unfolding algorithms, have gained popularity for nonconvex optimization; however, they are highly limited by the availability of labeled data and insufficient explainability. To tackle these issues, we propose a meta-learning based alternating minimization (MLAM) method that aims to minimize a part of the global losses over iterations instead of carrying minimization on each subproblem, and it tends to learn an adaptive strategy to replace the handcrafted counterpart resulting in advance on superior performance. The proposed MLAM maintains the original algorithmic principle, providing certain interpretability. We evaluate the proposed method on two representative problems, namely, bilinear inverse problem: matrix completion and nonlinear problem: Gaussian mixture models. The experimental results validate the proposed approach outperforms AM-based methods.},
  archive      = {J_TNNLS},
  author       = {Jing-Yuan Xia and Shengxi Li and Jun-Jie Huang and Zhixiong Yang and Imad M. Jaimoukha and Deniz Gündüz},
  doi          = {10.1109/TNNLS.2022.3165627},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5366-5380},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Metalearning-based alternating minimization algorithm for nonconvex optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local learning enabled iterative linear quadratic regulator
for constrained trajectory planning. <em>TNNLS</em>, <em>34</em>(9),
5354–5365. (<a
href="https://doi.org/10.1109/TNNLS.2022.3165846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory planning is one of the indispensable and critical components in robotics and autonomous systems. As an efficient indirect method to deal with the nonlinear system dynamics in trajectory planning tasks over the unconstrained state and control space, the iterative linear quadratic regulator (iLQR) has demonstrated noteworthy outcomes. In this article, a local-learning-enabled constrained iLQR algorithm is herein presented for trajectory planning based on hybrid dynamic optimization and machine learning. Rather importantly, this algorithm attains the key advantage of circumventing the requirement of system identification, and the trajectory planning task is achieved with a simultaneous refinement of the optimal policy and the neural network system in an iterative framework. The neural network can be designed to represent the local system model with a simple architecture, and thus it leads to a sample-efficient training pipeline. In addition, in this learning paradigm, the constraints of the general form that are typically encountered in trajectory planning tasks are preserved. Several illustrative examples on trajectory planning are scheduled as part of the test itinerary to demonstrate the effectiveness and significance of this work.},
  archive      = {J_TNNLS},
  author       = {Jun Ma and Zilong Cheng and Xiaoxue Zhang and Ziyu Lin and Frank L. Lewis and Tong Heng Lee},
  doi          = {10.1109/TNNLS.2022.3165846},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5354-5365},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local learning enabled iterative linear quadratic regulator for constrained trajectory planning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized nonconvex nonsmooth low-rank matrix recovery
framework with feasible algorithm designs and convergence analysis.
<em>TNNLS</em>, <em>34</em>(9), 5342–5353. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposing data matrix into low-rank plus additive matrices is a commonly used strategy in pattern recognition and machine learning. This article mainly studies the alternating direction method of multiplier (ADMM) with two dual variables, which is used to optimize the generalized nonconvex nonsmooth low-rank matrix recovery problems. Furthermore, the minimization framework with a feasible optimization procedure is designed along with the theoretical analysis, where the variable sequences generated by the proposed ADMM can be proved to be bounded. Most importantly, it can be concluded from the Bolzano–Weierstrass theorem that there must exist a subsequence converging to a critical point, which satisfies the Karush–Kuhn–Tucher (KKT) conditions. Meanwhile, we further ensure the local and global convergence properties of the generated sequence relying on constructing the potential objective function. Particularly, the detailed convergence analysis would be regarded as one of the core contributions besides the algorithm designs and the model generality. Finally, the numerical simulations and the real-world applications are both provided to verify the consistence of the theoretical results, and we also validate the superiority in performance over several mostly related solvers to the tasks of image inpainting and subspace clustering.},
  archive      = {J_TNNLS},
  author       = {Hengmin Zhang and Feng Qian and Peng Shi and Wenli Du and Yang Tang and Jianjun Qian and Chen Gong and Jian Yang},
  doi          = {10.1109/TNNLS.2022.3183970},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5342-5353},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized nonconvex nonsmooth low-rank matrix recovery framework with feasible algorithm designs and convergence analysis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A thompson sampling algorithm with logarithmic regret for
unimodal gaussian bandit. <em>TNNLS</em>, <em>34</em>(9), 5332–5341. (<a
href="https://doi.org/10.1109/TNNLS.2023.3295360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a Thompson sampling algorithm with Gaussian prior for unimodal bandit under Gaussian reward setting, where the expected reward is unimodal over the partially ordered arms. To exploit the unimodal structure better, at each step, instead of exploration from the entire decision space, the proposed algorithm makes decisions according to posterior distribution only in the arm’s neighborhood with the highest empirical mean estimate. We theoretically prove that the asymptotic regret of our algorithm reaches $\mathcal {O}(\log T)$ , i.e., it shares the same regret order with asymptotic optimal algorithms, which is comparable to extensive existing state-of-the-art unimodal multiarm bandit (U-MAB) algorithms. Finally, we use extensive experiments to demonstrate the effectiveness of the proposed algorithm on both synthetic datasets and real-world applications.},
  archive      = {J_TNNLS},
  author       = {Long Yang and Zhao Li and Zehong Hu and Shasha Ruan and Gang Pan},
  doi          = {10.1109/TNNLS.2023.3295360},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5332-5341},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A thompson sampling algorithm with logarithmic regret for unimodal gaussian bandit},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven state transition algorithm for fuzzy
chance-constrained dynamic optimization. <em>TNNLS</em>, <em>34</em>(9),
5322–5331. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many actual industrial production processes are dynamic and uncertain. When uncertain information are described by subjective experience and experts’ knowledge based on scanty or vague information, fuzzy uncertainty exists. Fuzzy chance-constrained dynamic programming are applicable to industrial production modeling accompanied by fuzzy uncertainty and dynamics, where constraints need not or cannot be completely satisfied. In this article, a fuzzy chance-constrained dynamic optimization (FCCDO) formulation on the basis of credibility theory is established, in which, the credibility is used to measure the fuzzy uncertainty level of constraints. To solve the FCCDO problem (FCCDOP), an improved fuzzy simulation technique based on Hammersley sequence sampling is raised to transform fuzzy chance constraints to their deterministic equivalents, and then a data-driven state transition algorithm (DDSTA) using deep neural networks (DNNs) is put forward to achieve a stable, global and robust optimization performance. Finally, the successful applications of the FCCDO method to industrial studies demonstrate its advantages.},
  archive      = {J_TNNLS},
  author       = {Feifan Lin and Xiaojun Zhou and Chaojie Li and Tingwen Huang and Chunhua Yang},
  doi          = {10.1109/TNNLS.2022.3186475},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5322-5331},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven state transition algorithm for fuzzy chance-constrained dynamic optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed stochastic gradient tracking algorithm with
variance reduction for non-convex optimization. <em>TNNLS</em>,
<em>34</em>(9), 5310–5321. (<a
href="https://doi.org/10.1109/TNNLS.2022.3170944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a distributed stochastic algorithm with variance reduction for general smooth non-convex finite-sum optimization, which has wide applications in signal processing and machine learning communities. In distributed setting, a large number of samples are allocated to multiple agents in the network. Each agent computes local stochastic gradient and communicates with its neighbors to seek for the global optimum. In this article, we develop a modified variance reduction technique to deal with the variance introduced by stochastic gradients. Combining gradient tracking and variance reduction techniques, this article proposes a distributed stochastic algorithm, gradient tracking algorithm with variance reduction (GT-VR), to solve large-scale non-convex finite-sum optimization over multiagent networks. A complete and rigorous proof shows that the GT-VR algorithm converges to the first-order stationary points with $O({1}/{k})$ convergence rate. In addition, we provide the complexity analysis of the proposed algorithm. Compared with some existing first-order methods, the proposed algorithm has a lower $\mathcal {O}(PM\epsilon ^{-1})$ gradient complexity under some mild condition. By comparing state-of-the-art algorithms and GT-VR in numerical simulations, we verify the efficiency of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Xia Jiang and Xianlin Zeng and Jian Sun and Jie Chen},
  doi          = {10.1109/TNNLS.2022.3170944},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5310-5321},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed stochastic gradient tracking algorithm with variance reduction for non-convex optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improvement of reinforcement learning with supermodularity.
<em>TNNLS</em>, <em>34</em>(9), 5298–5309. (<a
href="https://doi.org/10.1109/TNNLS.2023.3244024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) is a promising approach to tackling learning and decision-making problems in a dynamic environment. Most studies on RL focus on the improvement of state evaluation or action evaluation. In this article, we investigate how to reduce action space by using supermodularity. We consider the decision tasks in the multistage decision process as a collection of parameterized optimization problems, where state parameters dynamically vary along with the time or stage. The optimal solutions of these parameterized optimization problems correspond to the optimal actions in RL. For a given Markov decision process (MDP) with supermodularity, the monotonicity of the optimal action set and the optimal selection with respect to state parameters can be obtained by using the monotone comparative statics. Accordingly, we propose a monotonicity cut to remove unpromising actions from the action space. Taking bin packing problem (BPP) as an example, we show how the supermodularity and monotonicity cut work in RL. Finally, we evaluate the monotonicity cut on the benchmark datasets reported in the literature and compare the proposed RL with some popular baseline algorithms. The results show that the monotonicity cut can effectively improve the performance of RL.},
  archive      = {J_TNNLS},
  author       = {Ying Meng and Fengyuan Shi and Lixin Tang and Defeng Sun},
  doi          = {10.1109/TNNLS.2023.3244024},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5298-5309},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improvement of reinforcement learning with supermodularity},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Just-in-time precast production scheduling using dominance
rule-based genetic algorithm. <em>TNNLS</em>, <em>34</em>(9), 5283–5297.
(<a href="https://doi.org/10.1109/TNNLS.2022.3217318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this study is to provide a model and method to assist fabricators in making appropriate precast production plans, coordinating factories prefabrication and on-site assembly in construction based on the just-in-time (JIT) philosophy. We propose a JIT precast production scheduling model for the precast production of steel box girders in the Hong Kong–Zhuhai–Macau (HZM) bridge construction project. In order to minimize the total early/tardy costs for this JIT scheduling model, we first present a job batching algorithm (JBA) to group series of jobs into several batches. Then, we model the batch cost function as a piecewise linear convex function and derive its optimal solution, which guides us to propose an optimal shifting algorithm (OSA) to locate the optimal starting time of each batch and minimize the batch cost. In order to get the best job sequence for minimizing the total early/tardy costs and improve search efficiency, we introduce a dominance rule for early/tardy scheduling problem and propose a dominance rule-based genetic algorithm (DBGA) embedded with an optimal timing algorithm, which can find the best job sequence as well as the corresponding optimal schedule. The real-world case study of HZM bridge project demonstrates that our proposed model and algorithm can assist construction practitioners to make better decision on precast production scheduling compared to empirical rule in current engineering practice. In addition, numerical studies prove that our proposed algorithm has a better performance on both effectiveness and efficiency due to the dominance rule.},
  archive      = {J_TNNLS},
  author       = {Yong Xie and Hongwei Wang and Gang Liu and Hui Lu},
  doi          = {10.1109/TNNLS.2022.3217318},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5283-5297},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Just-in-time precast production scheduling using dominance rule-based genetic algorithm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic optimal control for multivariable dynamical
systems using expectation maximization. <em>TNNLS</em>, <em>34</em>(9),
5268–5282. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory optimization is a fundamental stochastic optimal control (SOC) problem. This article deals with a trajectory optimization approach for dynamical systems subject to measurement noise that can be fitted into linear time-varying stochastic models. Exact/complete solutions to these kind of control problems have been deemed analytically intractable in literature because they come under the category of partially observable Markov decision processes (MDPs). Therefore, effective solutions with reasonable approximations are widely sought for. We propose a reformulation of stochastic control in a reinforcement learning setting. This type of formulation assimilates the benefits of conventional optimal control procedure, with the advantages of maximum likelihood approaches. Finally, an iterative trajectory optimization paradigm called as SOC—expectation maximization (SOC-EM) is put forth. This trajectory optimization procedure exhibits better performance in terms of reduction in cumulative cost-to-go which is proven both theoretically and empirically. Furthermore, we also provide novel theoretical work which is related to uniqueness of control parameter estimates. Analysis of the control covariance matrix is presented, which handles stochasticity through efficiently balancing exploration and exploitation.},
  archive      = {J_TNNLS},
  author       = {Prakash Mallick and Zhiyong Chen},
  doi          = {10.1109/TNNLS.2022.3190246},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5268-5282},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stochastic optimal control for multivariable dynamical systems using expectation maximization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Policy-iteration-based finite-horizon approximate dynamic
programming for continuous-time nonlinear optimal control.
<em>TNNLS</em>, <em>34</em>(9), 5255–5267. (<a
href="https://doi.org/10.1109/TNNLS.2022.3225090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hamilton–Jacobi–Bellman (HJB) equation serves as the necessary and sufficient condition for the optimal solution to the continuous-time (CT) optimal control problem (OCP). Compared with the infinite-horizon HJB equation, the solving of the finite-horizon (FH) HJB equation has been a long-standing challenge, because the partial time derivative of the value function is involved as an additional unknown term. To address this problem, this study first-time bridges the link between the partial time derivative and the terminal-time utility function, and thus it facilitates the use of the policy iteration (PI) technique to solve the CT FH OCPs. Based on this key finding, the FH approximate dynamic programming (ADP) algorithm is proposed leveraging an actor–critic framework. It is shown that the algorithm exhibits important properties in terms of convergence and optimality. Rather importantly, with the use of multilayer neural networks (NNs) in the actor–critic architecture, the algorithm is suitable for CT FH OCPs toward more general nonlinear and complex systems. Finally, the effectiveness of the proposed algorithm is demonstrated by conducting a series of simulations on both a linear quadratic regulator (LQR) problem and a nonlinear vehicle tracking problem.},
  archive      = {J_TNNLS},
  author       = {Ziyu Lin and Jingliang Duan and Shengbo Eben Li and Haitong Ma and Jie Li and Jianyu Chen and Bo Cheng and Jun Ma},
  doi          = {10.1109/TNNLS.2022.3225090},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5255-5267},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Policy-iteration-based finite-horizon approximate dynamic programming for continuous-time nonlinear optimal control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid design of fault detection for nonlinear systems
based on dynamic optimization. <em>TNNLS</em>, <em>34</em>(9),
5244–5254. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure the safety of an automation system, fault detection (FD) has become an active research topic. With the development of artificial intelligence, model-free FD strategies have been widely investigated over the past 20 years. In this work, a hybrid FD design approach that combines data-driven and model-based is developed for nonlinear dynamic systems whose information is not known beforehand. With the aid of a Takagi–Sugeno (T-S) fuzzy model, the nonlinear system can be identified through a group of least-squares-based optimization. The associated modeling errors are taken into account when designing residual generators. In addition, statistical learning is adopted to obtain an upper bound of modeling errors, based on which an optimization problem is formulated to determine a reliable FD threshold. In the online FD decision, an event-triggered strategy is also involved in saving computational costs and network resources. The effectiveness and feasibility of the proposed hybrid FD method are illustrated through two simulation studies on nonlinear systems.},
  archive      = {J_TNNLS},
  author       = {Guangtao Ran and Hongtian Chen and Chuanjiang Li and Guangfu Ma and Bin Jiang},
  doi          = {10.1109/TNNLS.2022.3174822},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5244-5254},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid design of fault detection for nonlinear systems based on dynamic optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convex temporal convolutional network-based distributed
cooperative learning control for multiagent systems. <em>TNNLS</em>,
<em>34</em>(9), 5234–5243. (<a
href="https://doi.org/10.1109/TNNLS.2022.3216327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its great efficiency, scalability, and inclusivity, distributed cooperative learning control has gotten a lot of attention. For complex uncertain multiagent systems, it is challenging to model the uncertainties and exploit the cooperative learning ability of the systems. To address these issues, we proposed a novel convex temporal convolutional network-based distributed cooperative learning control for uncertain discrete-time nonlinear multiagent systems. A new concept of using a convex temporal convolutional network (CTCNet) is proposed for estimating the uncertain agent dynamics in a cooperative way. Unlike previous methods that require adjustment of network weights for different control tasks, the proposed CTCNet can map the high-dimensional input-output space into a deep space spanned by basis features that represent the inherent properties of the system, so it has good robustness for different tasks. Consequently, to improve the control performance, a CTCNet-based distributed cooperative learning control method that shares learned knowledge through the communication topology among adaptive laws of CTCNet is proposed. Furthermore, the asymptotic convergence of system tracking errors to an arbitrarily small neighborhood of the origin is strictly proved. Finally, the simulation results are given to illustrate that our suggested method has higher control accuracy, stronger robustness, and anti-interference ability than the existing methods.},
  archive      = {J_TNNLS},
  author       = {Shaofeng Chen and Yu Kang and Jian Di and Pengfei Li and Yang Cao},
  doi          = {10.1109/TNNLS.2022.3216327},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5234-5243},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convex temporal convolutional network-based distributed cooperative learning control for multiagent systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Special issue on theory, algorithms, and
applications for hybrid intelligent dynamic optimization.
<em>TNNLS</em>, <em>34</em>(9), 5231–5233. (<a
href="https://doi.org/10.1109/TNNLS.2023.3285456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic optimization problems are pervasive in various fields, ranging from chemical process control to aerospace, autonomous driving, physics, robotics, and beyond. These problems involve optimizing a dynamic system considering inputs, parameters, constraints, and a cost function. For dynamic optimization, two broad classes of strategies emerge: deterministic and heuristic methods. Deterministic optimization methods leverage the analytical properties of the problem, generating a sequence of points that converge to the optimal solution. These techniques are suitable when explicit models and constraints are available and easy to evaluate. On the other hand, heuristic approaches treat the problem as a black box, relying on iterative improvements to a fitness function. They are employed for complex problems with challenging system models or significant uncertainty.},
  archive      = {J_TNNLS},
  author       = {Jun Fu and Junfei Qiao and Kok Lay Teo and Rolf Findeisen},
  doi          = {10.1109/TNNLS.2023.3285456},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5231-5233},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial: Special issue on theory, algorithms, and applications for hybrid intelligent dynamic optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A globally stable LPNN model for sparse approximation.
<em>TNNLS</em>, <em>34</em>(8), 5218–5226. (<a
href="https://doi.org/10.1109/TNNLS.2021.3126730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of compressive sampling is to determine a sparse vector from an observation vector. This brief describes an analog neural method to achieve the objective. Unlike previous analog neural models which either resort to the $\ell _{1}$ -norm approximation or are with local convergence only, the proposed method avoids any approximation of the $\ell _{1}$ -norm term and is probably capable of leading to the optimum solution. Moreover, its computational complexity is lower than that of the other three comparison analog models. Simulation results show that the error performance of the proposed model is comparable to several state-of-the-art digital algorithms and analog models and that its convergence is faster than that of the comparison analog neural models.},
  archive      = {J_TNNLS},
  author       = {Hao Wang and Ruibin Feng and Chi-Sing Leung and John Sum and Anthony G. Constantinides},
  doi          = {10.1109/TNNLS.2021.3126730},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5218-5226},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A globally stable LPNN model for sparse approximation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A weighted heterogeneous graph-based dialog system.
<em>TNNLS</em>, <em>34</em>(8), 5212–5217. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-based dialog systems have attracted increasing research interest in diverse applications. However, for disease diagnosis, the widely used knowledge graph (KG) is hard to represent the symptom–symptom and symptom–disease relations since the edges of traditional KG are unweighted. Most research on disease diagnosis dialog systems highly relies on data-driven methods and statistical features, lacking profound comprehension of symptom–symptom and symptom–disease relations. To tackle this issue, this work presents a weighted heterogeneous graph-based dialog system for disease diagnosis. Specifically, we build a weighted heterogeneous graph based on symptom co-occurrence and the proposed symptom frequency-inverse disease frequency. Then, this work proposes a graph-based deep $Q$ -network (graph-DQN) for dialog management. By combining graph convolutional network (GCN) with DQN to learn the embeddings of diseases and symptoms from both the structural and attribute information in the weighted heterogeneous graph, graph-DQN could capture the symptom–disease relations and symptom–symptom relations better. Experimental results show that the proposed dialog system rivals the state-of-the-art models. More importantly, the proposed dialog system can complete the task with fewer dialog turns and possess a better distinguishing capability on diseases with similar symptoms.},
  archive      = {J_TNNLS},
  author       = {Xinyan Zhao and Liangwei Chen and Huanhuan Chen},
  doi          = {10.1109/TNNLS.2021.3124640},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5212-5217},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A weighted heterogeneous graph-based dialog system},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiclass sparse centroids with application to fast time
series classification. <em>TNNLS</em>, <em>34</em>(8), 5206–5211. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose an efficient multiclass classification scheme based on sparse centroids classifiers. The proposed strategy exhibits linear complexity with respect to both the number of classes and the cardinality of the feature space. The classifier we introduce is based on binary space partitioning, performed by a decision tree where the assignation law at each node is defined via a sparse centroid classifier. We apply the presented strategy to the time series classification problem, showing by experimental evidence that it achieves performance comparable to that of state-of-the-art methods, but with a significantly lower classification time. The proposed technique can be an effective option in resource-constrained environments where the classification time and the computational cost are critical or, in scenarios, where real-time classification is necessary.},
  archive      = {J_TNNLS},
  author       = {Tommaso Bradde and Giulia Fracastoro and Giuseppe C. Calafiore},
  doi          = {10.1109/TNNLS.2021.3124300},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5206-5211},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiclass sparse centroids with application to fast time series classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spiking deep residual networks. <em>TNNLS</em>,
<em>34</em>(8), 5200–5205. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have received significant attention for their biological plausibility. SNNs theoretically have at least the same computational power as traditional artificial neural networks (ANNs). They possess the potential of achieving energy-efficient machine intelligence while keeping comparable performance to ANNs. However, it is still a big challenge to train a very deep SNN. In this brief, we propose an efficient approach to build deep SNNs. Residual network (ResNet) is considered a state-of-the-art and fundamental model among convolutional neural networks (CNNs). We employ the idea of converting a trained ResNet to a network of spiking neurons named spiking ResNet (S-ResNet). We propose a residual conversion model that appropriately scales continuous-valued activations in ANNs to match the firing rates in SNNs and a compensation mechanism to reduce the error caused by discretization. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet 2012 with low latency. This work is the first time to build an asynchronous SNN deeper than 100 layers, with comparable performance to its original ANN.},
  archive      = {J_TNNLS},
  author       = {Yangfan Hu and Huajin Tang and Gang Pan},
  doi          = {10.1109/TNNLS.2021.3119238},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5200-5205},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spiking deep residual networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Visual perception generalization for vision-and-language
navigation via meta-learning. <em>TNNLS</em>, <em>34</em>(8), 5193–5199.
(<a href="https://doi.org/10.1109/TNNLS.2021.3122579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-and-language navigation (VLN) is a challenging task that requires an agent to navigate in real-world environments by understanding natural language instructions and visual information received in real time. Prior works have implemented VLN tasks on continuous environments or physical robots, all of which use a fixed-camera configuration due to the limitations of datasets, such as 1.5-m height, 90° horizontal field of view (HFOV), and so on. However, real-life robots with different purposes have multiple camera configurations, and the huge gap in visual information makes it difficult to directly transfer the learned navigation skills between various robots. In this brief, we propose a visual perception generalization strategy based on meta-learning, which enables the agent to fast adapt to a new camera configuration. In the training phase, we first locate the generalization problem to the visual perception module and then compare two meta-learning algorithms for better generalization in seen and unseen environments. One of them uses the model-agnostic meta-learning (MAML) algorithm that requires few-shot adaptation, and the other refers to a metric-based meta-learning method with a feature-wise affine transformation (AT) layer. The experimental results on the VLN-CE dataset demonstrate that our strategy successfully adapts the learned navigation skills to new camera configurations, and the two algorithms show their advantages in seen and unseen environments respectively.},
  archive      = {J_TNNLS},
  author       = {Ting Wang and Zongkai Wu and Donglin Wang},
  doi          = {10.1109/TNNLS.2021.3122579},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5193-5199},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Visual perception generalization for vision-and-language navigation via meta-learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Letter on convergence of in-parameter-linear nonlinear
neural architectures with gradient learnings. <em>TNNLS</em>,
<em>34</em>(8), 5189–5192. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter summarizes and proves the concept of bounded-input bounded-state (BIBS) stability for weight convergence of a broad family of in-parameter-linear nonlinear neural architectures (IPLNAs) as it generally applies to a broad family of incremental gradient learning algorithms. A practical BIBS convergence condition results from the derived proofs for every individual learning point or batches for real-time applications.},
  archive      = {J_TNNLS},
  author       = {Ivo Bukovsky and Gejza Dohnal and Peter M. Benes and Kei Ichiji and Noriyasu Homma},
  doi          = {10.1109/TNNLS.2021.3123533},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5189-5192},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Letter on convergence of in-parameter-linear nonlinear neural architectures with gradient learnings},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AudioVisual video summarization. <em>TNNLS</em>,
<em>34</em>(8), 5181–5188. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio and vision are two main modalities in video data. Multimodal learning, especially for audiovisual learning, has drawn considerable attention recently, which can boost the performance of various computer vision tasks. However, in video summarization, most existing approaches just exploit the visual information while neglecting the audio information. In this brief, we argue that the audio modality can assist vision modality to better understand the video content and structure and further benefit the summarization process. Motivated by this, we propose to jointly exploit the audio and visual information for the video summarization task and develop an audiovisual recurrent network (AVRN) to achieve this. Specifically, the proposed AVRN can be separated into three parts: 1) the two-stream long-short term memory (LSTM) is used to encode the audio and visual feature sequentially by capturing their temporal dependency; 2) the audiovisual fusion LSTM is used to fuse the two modalities by exploring the latent consistency between them; and 3) the self-attention video encoder is adopted to capture the global dependency in the video. Finally, the fused audiovisual information and the integrated temporal and global dependencies are jointly used to predict the video summary. Practically, the experimental results on the two benchmarks, i.e., SumMe and TVsum, have demonstrated the effectiveness of each part and the superiority of AVRN compared with those approaches just exploiting visual information for video summarization.},
  archive      = {J_TNNLS},
  author       = {Bin Zhao and Maoguo Gong and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3119969},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5181-5188},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AudioVisual video summarization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive fuzzy neural network command filtered impedance
control of constrained robotic manipulators with disturbance observer.
<em>TNNLS</em>, <em>34</em>(8), 5171–5180. (<a
href="https://doi.org/10.1109/TNNLS.2021.3113044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an adaptive fuzzy neural network (NN) command filtered impedance control for constrained robotic manipulators with disturbance observers. First, barrier Lyapunov functions are introduced to handle the full-state constraints. Second, the adaptive fuzzy NN is introduced to handle the unknown system dynamics and a disturbance observer is designed to eliminate the effect of unknown bound disturbance. Then, a modified auxiliary system is designed to suppress the input saturation effect. In addition, the command filtered technique and error compensation mechanism are used to directly obtain the derivative of the virtual control law and improve the control accuracy. The barrier Lyapunov theory is used to prove that all the signals in the closed-loop system are semiglobally uniformly ultimately bounded. Finally, simulation studies are performed to illustrate the effectiveness of the proposed control method.},
  archive      = {J_TNNLS},
  author       = {Gang Li and Jinpeng Yu and Xinkai Chen},
  doi          = {10.1109/TNNLS.2021.3113044},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5171-5180},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive fuzzy neural network command filtered impedance control of constrained robotic manipulators with disturbance observer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Faster convergence in deep-predictive-coding networks to
learn deeper representations. <em>TNNLS</em>, <em>34</em>(8), 5156–5170.
(<a href="https://doi.org/10.1109/TNNLS.2021.3115698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-predictive-coding networks (DPCNs) are hierarchical, generative models. They rely on feed-forward and feedback connections to modulate latent feature representations of stimuli in a dynamic and context-sensitive manner. A crucial element of DPCNs is a forward-backward inference procedure to uncover sparse, invariant features. However, this inference is a major computational bottleneck. It severely limits the network depth due to learning stagnation. Here, we prove why this bottleneck occurs. We then propose a new forward-inference strategy based on accelerated proximal gradients. This strategy has faster theoretical convergence guarantees than the one used for DPCNs. It overcomes learning stagnation. We also demonstrate that it permits constructing deep and wide predictive-coding networks. Such convolutional networks implement receptive fields that capture well the entire classes of objects on which the networks are trained. This improves the feature representations compared with our lab’s previous nonconvolutional and convolutional DPCNs. It yields unsupervised object recognition that surpass convolutional autoencoders and is on par with convolutional networks trained in a supervised manner.},
  archive      = {J_TNNLS},
  author       = {Isaac J. Sledge and José C. Príncipe},
  doi          = {10.1109/TNNLS.2021.3115698},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5156-5170},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Faster convergence in deep-predictive-coding networks to learn deeper representations},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Learning high-order graph convolutional networks via
adaptive layerwise aggregation combination. <em>TNNLS</em>,
<em>34</em>(8), 5144–5155. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks have attracted wide attention for their expressiveness and empirical success on graph-structured data. However, deeper graph convolutional networks with access to more information can often perform worse because their low-order Chebyshev polynomial approximation cannot learn adaptive and structure-aware representations. To solve this problem, many high-order graph convolution schemes have been proposed. In this article, we study the reason why high-order schemes have the ability to learn structure-aware representations. We first prove that these high-order schemes are generalized Weisfeiler–Lehman (WL) algorithm and conduct spectral analysis on these schemes to show that they correspond to polynomial filters in the graph spectral domain. Based on our analysis, we point out twofold limitations of existing high-order models: 1) lack mechanisms to generate individual feature combinations for each node and 2) fail to properly model the relationship between information from different distances. To enable a node-specific combination scheme and capture this interdistance relationship for each node efficiently, we propose a new adaptive feature combination method inspired by the squeeze-and-excitation module that can recalibrate features from different distances by explicitly modeling interdependencies between them. Theoretical analysis shows that models with our new approach can effectively learn structure-aware representations, and extensive experimental results show that our new approach can achieve significant performance gain compared with other high-order schemes.},
  archive      = {J_TNNLS},
  author       = {Tianqi Zhang and Qitian Wu and Junchi Yan},
  doi          = {10.1109/TNNLS.2021.3119958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5144-5155},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning high-order graph convolutional networks via adaptive layerwise aggregation combination},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Output feedback control for stochastic nonlinear systems
with nondifferentiable measurement function and input saturation.
<em>TNNLS</em>, <em>34</em>(8), 5133–5143. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the problem of output feedback control for a class of stochastic nonlinear systems in the presence of nondifferentiable measurement function and input saturation is studied. A novel power-auxiliary system is introduced to handle the adverse effects of input saturation. What is more, the common growth assumptions of nonlinear terms can be eliminated by a key lemma. Then, an output feedback controller is constructed to ensure that all the signals in the closed-loop system are globally bounded almost surely. Finally, a simulation shows that the control strategy is effective.},
  archive      = {J_TNNLS},
  author       = {Qingtan Meng and Qian Ma},
  doi          = {10.1109/TNNLS.2021.3123637},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5133-5143},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Output feedback control for stochastic nonlinear systems with nondifferentiable measurement function and input saturation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SFA-net: A selective features absorption network for object
detection in rainy weather conditions. <em>TNNLS</em>, <em>34</em>(8),
5122–5132. (<a
href="https://doi.org/10.1109/TNNLS.2021.3125679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, object detection approaches using deep convolutional neural networks (CNNs) have derived major advances in normal images. However, such success is hardly achieved with rainy images due to lack of visibility. Aiming to bridge this gap, in this article, we present a novel selective features absorption network (SFA-Net) to improve the performance of object detection not only in rainy weather conditions but also in favorable weather conditions. SFA-Net accomplishes this objective by utilizing three subnetworks, where the feature selection subnetwork is concatenated with the object detection subnetwork through the feature absorption subnetwork to form a unified model. To promote further advancement in object detection impaired by rain, we propose a large-scale rainy image dataset, named srRain, which contains both synthetic rainy images and real-world rainy images for training and testing purposes. srRain is comprised of 25 900 rainy images depicting diverse driving scenarios in the presence of rain with a total of 181 164 instances interpreting five common object categories. Experimental results display that our SFA-Net reaches the highest mean average precision (mAP) of 77.53\% on a normal image set, 62.52\% on a synthetic rainy image set, 37.34\% on a collected natural rainy image set, and 32.86\% on a published real rainy image set, surpassing current state-of-the-art object detectors and the combination of image deraining and object detection models while retaining a high speed.},
  archive      = {J_TNNLS},
  author       = {Shih-Chia Huang and Quoc-Viet Hoang and Trung-Hieu Le},
  doi          = {10.1109/TNNLS.2021.3125679},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5122-5132},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SFA-net: A selective features absorption network for object detection in rainy weather conditions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the imaginary wings: Text-assisted complex-valued fusion
network for fine-grained visual classification. <em>TNNLS</em>,
<em>34</em>(8), 5112–5121. (<a
href="https://doi.org/10.1109/TNNLS.2021.3126046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification (FGVC) is challenging due to the interclass similarity and intraclass variation in datasets. In this work, we explore the great merit of complex values in introducing an imaginary part for modeling data uncertainty (e.g., different points on the complex plane can describe the same state) and graph convolutional networks (GCNs) in learning interdependently among classes to simultaneously tackle the above two major challenges. To the end, we propose a novel approach, termed text-assisted complex-valued fusion network (TA-CFN). Specifically, we expand each feature from 1-D real values to 2-D complex value by disassembling feature maps, thereby enabling the extension of traditional deep convolutional neural networks over the complex domain. Then, we fuse the real and imaginary parts of complex features through complex projection and modulus operation. Finally, we build an undirected graph over the object labels with the assistance of a text corpus, and a GCN is learned to map this graph into a set of classifiers. The benefits are in two folds: 1) complex features allow for a richer algebraic structure to better model the large variation within the same category and 2) leveraging the interclass dependencies brought by the GCN to capture key factors of the slight variation among different categories. We conduct extensive experiments to verify that our proposed model can achieve the state-of-the-art performance on two widely used FGVC datasets.},
  archive      = {J_TNNLS},
  author       = {Xiang Guan and Yang Yang and Jingjing Li and Xiaofeng Zhu and Jingkuan Song and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2021.3126046},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5112-5121},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the imaginary wings: Text-assisted complex-valued fusion network for fine-grained visual classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Logical topology inference via CPGCN joint optimizing with
pedestrian re-id. <em>TNNLS</em>, <em>34</em>(8), 5099–5111. (<a
href="https://doi.org/10.1109/TNNLS.2021.3125368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of artificial intelligence, deep learning has become the main research method of pedestrian recognition re-identification (re-id). However, most of the existing researches usually just determine the retrieval order based on the geographical location of cameras, which ignore the spatio-temporal logic characteristics of pedestrian flow. Furthermore, most of these methods rely on common object detection to detect and match pedestrians directly, which will separate the logical connection between videos from different cameras. In this research, a novel pedestrian re-identification model assisted by logical topological inference is proposed, which includes: 1) a joint optimization mechanism of pedestrian re-identification and multicamera logical topology inference, which makes the multicamera logical topology provides the retrieval order and the confidence for re-identification. And meanwhile, the results of pedestrian re-identification as a feedback modify logical topological inference; 2) a dynamic spatio-temporal information driving logical topology inference method via conditional probability graph convolution network (CPGCN) with random forest-based transition activation mechanism (RF-TAM) is proposed, which focuses on the pedestrian’s walking direction at different moments; and 3) a pedestrian group cluster graph convolution network (GC-GCN) is designed to measure the correlation between embedded pedestrian features. Some experimental analyses and real scene experiments on datasets CUHK-SYSU, PRW, SLP, and UJS-reID indicate that the designed model can achieve a better logical topology inference with an accuracy of 87.3\% and achieve the top-1 accuracy of 77.4\% and the mAP accuracy of 74.3\% for pedestrian re-identification.},
  archive      = {J_TNNLS},
  author       = {Keyang Cheng and Qing Liu and Rabia Tahir and Liangmin Wang and Maozhen Li},
  doi          = {10.1109/TNNLS.2021.3125368},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5099-5111},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Logical topology inference via CPGCN joint optimizing with pedestrian re-id},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuroadaptive impulsive control on consensus of uncertain
multiagent systems using continuous and sampled information.
<em>TNNLS</em>, <em>34</em>(8), 5086–5098. (<a
href="https://doi.org/10.1109/TNNLS.2021.3126531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the consensus problem of uncertain multiagent systems, which is addressed by neuroadaptive impulsive control schemes. The proposed control schemes indicate that the communication among agents only occurs impulsively, while the dynamics uncertainty is addressed by adaptive schemes using neural networks. Based on such approaches, two specific control schemes are designed. One is that with impulsive feedback, the control scheme uses continuous-time information, which implies that the adaptive process is continuous over time. Another is that by adopting sampled information, the update of all systems, including the feedbacks on agents, the update of neural networks, and the estimation for uncertainty, can be executed only at impulsive instants. The latter case can reduce the energy cost for communication and control, but extra assistant systems are required. The estimation and consensus prove to be achieved with errors if some conditions are fulfilled. Numerical simulations, including a practical system example, are presented.},
  archive      = {J_TNNLS},
  author       = {Yiyan Han and Qiang Xiao and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2021.3126531},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5086-5098},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuroadaptive impulsive control on consensus of uncertain multiagent systems using continuous and sampled information},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuroadaptive fault-tolerant control with unsynchronized
event triggering for actuation updating and parameter adaptation.
<em>TNNLS</em>, <em>34</em>(8), 5076–5085. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication and computation resources are normally limited in remote/networked control systems, and thus, saving either of them could substantially contribute to cost reduction and life-span increasing as well as reliability enhancement for such systems. This article investigates the event-triggered control method to save both communication and computation resources for a class of uncertain nonlinear systems in the presence of actuator failures and full-state constraints. By introducing the triggering mechanisms for actuation updating and parameter adaptation, and with the aid of the unified constraining functions, a neuroadaptive and fault-tolerant event-triggered control scheme is developed with several salient features: 1) online computation and communication resources are substantially reduced due to the utilization of unsynchronized (uncorrelated) event-triggering pace for control updating and parameter adaptation; 2) systems with and without constraints can be addressed uniformly without involving feasibility conditions on virtual controllers; and 3) the output tracking error converges to a prescribed precision region in the presence of actuation faults and state constraints. Both theoretical analysis and numerical simulation verify the benefits and efficiency of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Hongwei Cao and Yongduan Song},
  doi          = {10.1109/TNNLS.2021.3123946},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5076-5085},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuroadaptive fault-tolerant control with unsynchronized event triggering for actuation updating and parameter adaptation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrepancy-guided domain-adaptive data augmentation.
<em>TNNLS</em>, <em>34</em>(8), 5064–5075. (<a
href="https://doi.org/10.1109/TNNLS.2021.3128401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation has been observed playing a crucial role in achieving better generalization in many machine learning tasks, especially in unsupervised domain adaptation (DA). It is particularly effective on visual object recognition tasks as images are high-dimensional with an enormous range of variations that can be simulated. Existing data augmentation techniques, however, are not explicitly designed to address the differences between different domains. Expert knowledge about the data is required, as well as manual efforts in finding the optimal parameters. In this article, we propose a novel domain-adaptive augmentation method by making use of a state-of-the-art style transfer method and domain discrepancy measurement. Specifically, we measure the discrepancy between source and target domains, and use it as a guide to augment the original source samples using style transferred source-to-target samples. The proposed domain-adaptive augmentation method is data and model agnostic that can be easily incorporated with state-of-the-art DA algorithms. We show empirically that, by using this domain-adaptive augmentation, we are able to gradually reduce the discrepancy between the source and target samples, and further boost the adaptation performance using different DA algorithms on three popular domain adaption datasets.},
  archive      = {J_TNNLS},
  author       = {Jian Gao and Yang Hua and Guosheng Hu and Chi Wang and Neil M. Robertson},
  doi          = {10.1109/TNNLS.2021.3128401},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5064-5075},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discrepancy-guided domain-adaptive data augmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge-based reasoning network for relation detection.
<em>TNNLS</em>, <em>34</em>(8), 5051–5063. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of large-scale knowledge bases (KBs), knowledge base question answering (KBQA) has attracted increasing attention recently. Relation detection plays an important role in the KBQA system, which finds a compatible answer by analyzing the semantics of questions and querying and reasoning with multiple KB triples. Significant progress has been made by deep neural networks. However, existing methods often concern on detecting single-hop relation without path reasoning, and a few of these methods exploit the multihop relation reasoning, which involves the answer reasoning from the noisy and abundant relational paths in the KB. Meanwhile, the relatedness between question and answer candidates has received little attention and remains unsolved. This article proposes a novel knowledge-based reasoning network (KRN) for relation detection, including both single-hop relation and multihop relation. To address the semantic gap problem in question-answer interaction, we first learn attentive question representations according to the influence of answer aspects. Then, we learn the single-hop relation sequence through different levels of abstraction and adopt the KB entity and structure information to denoise the multihop relation detection task. Finally, we adopt a Siamese network to measure the similarity between question representation and relation representation for both single-hop and multihop relation KBQA tasks. We conduct experiments on two well-known benchmarks, SimpleQuestions and WebQSP, and the results show the superiority of our approach over the state-of-the-art models for both single-hop and multihop relation detection. Our model also achieves a significant improvement over existing methods on KBQA end task. Further analysis demonstrates the robustness and the applicability of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Ying Shen and Min Yang and Yaliang Li and Dong Wang and Haitao Zheng and Daoyuan Chen},
  doi          = {10.1109/TNNLS.2021.3123751},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5051-5063},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge-based reasoning network for relation detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robotic manipulation in dynamic scenarios via
bounding-box-based hindsight goal generation. <em>TNNLS</em>,
<em>34</em>(8), 5037–5050. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By relabeling past experience with heuristic or curriculum goals, state-of-the-art reinforcement learning (RL) algorithms such as hindsight experience replay (HER), hindsight goal generation (HGG), and graph-based HGG (G-HGG) have been able to solve challenging robotic manipulation tasks in multigoal settings with sparse rewards. HGG outperforms HER in challenging tasks in which goals are difficult to explore by learning from a curriculum, in which intermediate goals are selected based on the Euclidean distance to target goals. G-HGG enhances HGG by selecting intermediate goals from a precomputed graph representation of the environment, which enables its applicability in an environment with stationary obstacles. However, G-HGG is not applicable to manipulation tasks with dynamic obstacles, since its graph representation is only valid in static scenarios and fails to provide any correct information to guide the exploration. In this article, we propose bounding-box-based HGG (Bbox-HGG), an extension of G-HGG selecting hindsight goals with the help of image observations of the environment, which makes it applicable to tasks with dynamic obstacles. We evaluate Bbox-HGG on four challenging manipulation tasks, where significant enhancements in both sample efficiency and overall success rate are shown over state-of-the-art algorithms. The videos can be viewed at https://videoviewsite.wixsite.com/bbhgg .},
  archive      = {J_TNNLS},
  author       = {Zhenshan Bing and Erick Álvarez and Long Cheng and Fabrice O. Morin and Rui Li and Xiaojie Su and Kai Huang and Alois Knoll},
  doi          = {10.1109/TNNLS.2021.3124366},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5037-5050},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robotic manipulation in dynamic scenarios via bounding-box-based hindsight goal generation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ORCCA: Optimal randomized canonical correlation analysis.
<em>TNNLS</em>, <em>34</em>(8), 5024–5036. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random features approach has been widely used for kernel approximation in large-scale machine learning. A number of recent studies have explored data-dependent sampling of features, modifying the stochastic oracle from which random features are sampled. While proposed techniques in this realm improve the approximation, their suitability is often verified on a single learning task. In this article, we propose a task-specific scoring rule for selecting random features, which can be employed for different applications with some adjustments. We restrict our attention to canonical correlation analysis (CCA) and provide a novel, principled guide for finding the score function maximizing the canonical correlations. We prove that this method, called optimal randomized CCA (ORCCA), can outperform (in expectation) the corresponding kernel CCA with a default kernel. Numerical experiments verify that ORCCA is significantly superior to other approximation techniques in the CCA task.},
  archive      = {J_TNNLS},
  author       = {Yinsong Wang and Shahin Shahrampour},
  doi          = {10.1109/TNNLS.2021.3124868},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5024-5036},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ORCCA: Optimal randomized canonical correlation analysis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Tradeoff analysis between control time and energy
consumption for delayed neural networks with discontinuous activation
functions. <em>TNNLS</em>, <em>34</em>(8), 5012–5023. (<a
href="https://doi.org/10.1109/TNNLS.2021.3125827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies finite-time stabilization of delayed neural networks (DNNs) whose activation functions are discontinuous. Several sufficient conditions for guaranteeing finite-time stabilization of considered DNNs are obtained by constructing appropriate controllers with giving upper bounds of control time. Subsequently, based on the existing definition of energy consumption, the required energy to achieve stabilization is estimated. To quantify the cost of control, an evaluation index function is constructed to analyze the tradeoff between control time and consumed energy. Ultimately, acquired results are verified by simulating two numerical examples.},
  archive      = {J_TNNLS},
  author       = {Chongyang Chen and Song Zhu and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2021.3125827},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5012-5023},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tradeoff analysis between control time and energy consumption for delayed neural networks with discontinuous activation functions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deterministic learning-based adaptive neural control for
nonlinear full-state constrained systems. <em>TNNLS</em>,
<em>34</em>(8), 5002–5011. (<a
href="https://doi.org/10.1109/TNNLS.2021.3126320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive neural learning method is introduced for a category of nonlinear strict-feedback systems with time-varying full-state constraints. The two challenging problems of state constraints and learning capability are investigated and solved in a unified framework. To obtain the learning of unknown functions and satisfy full-state constraints, three main steps are considered. First, an adaptive dynamic surface controller (DSC) based on barrier Lyapunov functions (BLFs) is structured to implement that the closed-loop systems signals are bounded and full-state variables remain within the prescribed time-varying intervals. Moreover, the radial basis function neural networks (RBF NNs) are used to identify unknown functions. The output of the first-order filter, instead of virtual control derivatives, is used to simplify the complexity of the RBF NN input variables. Second, the state transformation is used to obtain a class of linear time-varying subsystems with small perturbations such that the recurrence of the RBF NN input variables and the partial persistent excitation condition are actualized. Therefore, the unknown functions can be accurately approximated, and the learned knowledge is kept as constant NN weights. Third, the obtained constant weights are borrowed into an adaptive learning scheme to achieve the batter control performance. Finally, simulation studies illustrate the advantage of the reported adaptive learning method on higher tracking accuracy, faster convergence rate, and lower computational expense by reusing learned knowledge.},
  archive      = {J_TNNLS},
  author       = {Dapeng Li and Honggui Han and Junfei Qiao},
  doi          = {10.1109/TNNLS.2021.3126320},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {5002-5011},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deterministic learning-based adaptive neural control for nonlinear full-state constrained systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Breast tumor segmentation in DCE-MRI with tumor sensitive
synthesis. <em>TNNLS</em>, <em>34</em>(8), 4990–5001. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting breast tumors from dynamic contrast-enhanced magnetic resonance (DCE-MR) images is a critical step for early detection and diagnosis of breast cancer. However, variable shapes and sizes of breast tumors, as well as inhomogeneous background, make it challenging to accurately segment tumors in DCE-MR images. Therefore, in this article, we propose a novel tumor-sensitive synthesis module and demonstrate its usage after being integrated with tumor segmentation. To suppress false-positive segmentation with similar contrast enhancement characteristics to true breast tumors, our tumor-sensitive synthesis module can feedback differential loss of the true and false breast tumors. Thus, by following the tumor-sensitive synthesis module after the segmentation predictions, the false breast tumors with similar contrast enhancement characteristics to the true ones will be effectively reduced in the learned segmentation model. Moreover, the synthesis module also helps improve the boundary accuracy while inaccurate predictions near the boundary will lead to higher loss. For the evaluation, we build a very large-scale breast DCE-MR image dataset with 422 subjects from different patients, and conduct comprehensive experiments and comparisons with other algorithms to justify the effectiveness, adaptability, and robustness of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Shuai Wang and Kun Sun and Li Wang and Liangqiong Qu and Fuhua Yan and Qian Wang and Dinggang Shen},
  doi          = {10.1109/TNNLS.2021.3129781},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4990-5001},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Breast tumor segmentation in DCE-MRI with tumor sensitive synthesis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the existence of the exact solution of quaternion-valued
neural networks based on a sequence of approximate solutions.
<em>TNNLS</em>, <em>34</em>(8), 4981–4989. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical applications, it is difficult or impossible to obtain the exact solution of the mathematical model due to the limitations of solving methods and the complexity of the neural network itself. A natural problem is given as follows: does the exact solution of quaternion-valued neural networks (QVNNs) exist when successively improved approximate solutions can be obtained? Fortunately, the Hyers–Ulam stability happens to be one of the important means to deal with this problem. In this article, the issue of Hyers–Ulam stability of QVNNs with time-varying delays is addressed. First, inspired by the Hyers–Ulam stability of general functional equations, the concept of the Hyers–Ulam stability of QVNNs is proposed along with the QVNNs model. Then, by utilizing the successive approximation method, both delay-dependent and delay-independent Hyers–Ulam stability criteria are obtained to ensure the Hyers–Ulam stability of the QVNNs considered. Finally, a simulation example is given to verify the effectiveness of the derived results.},
  archive      = {J_TNNLS},
  author       = {Dongyuan Lin and Xiaofeng Chen and Zhongshan Li and Bing Li and Xujun Yang},
  doi          = {10.1109/TNNLS.2021.3129269},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4981-4989},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the existence of the exact solution of quaternion-valued neural networks based on a sequence of approximate solutions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A scalable open-set ECG identification system based on
compressed CNNs. <em>TNNLS</em>, <em>34</em>(8), 4966–4980. (<a
href="https://doi.org/10.1109/TNNLS.2021.3127497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) is known for its excellence in feature learning and its ability to deliver high-accuracy results. Its application to ECG biometric recognition has received increasing interest but is also accompanied by several deficiencies. In this study, we focus on applying DL, especially convolutional neural networks (CNNs), to ECG biometric identification to address these deficiencies. Using prestored user-specific feature vectors, the proposed scheme can exclude unregistered subjects to realize “open-set” identification. With the help of its scalable structure and “transfer learning,” new subjects can be enrolled in an existing system without the need for storing the ECGs of those previously enrolled. Finally, schemes based on the quantum evolutionary algorithm (QEA) are presented to prune unnecessary filters in the proposed CNN model. The performance of the proposed scheme was evaluated using the ECGs of 285 subjects from the PTB dataset. The experimental results demonstrate an identification rate of more than 99\% in closed-set identification. Although incorporating the proposed method for unregistered subject exclusion degraded the identification performance slightly, the ability of the approach to resist a dictionary attack was evident. Finally, using the QEA-based filter pruning method and its two-stage extension reduced the number of floating-point operations required to complete one identity recognition to 1.20\% and 0.22\% of the original value without significantly impacting the identification accuracy.},
  archive      = {J_TNNLS},
  author       = {Shun-Chi Wu and Shih-Ying Wei and Chun-Shun Chang and A. Lee Swindlehurst and Jui-Kun Chiu},
  doi          = {10.1109/TNNLS.2021.3127497},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4966-4980},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A scalable open-set ECG identification system based on compressed CNNs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two new zhang neural networks for solving time-varying
linear equations and inequalities systems. <em>TNNLS</em>,
<em>34</em>(8), 4957–4965. (<a
href="https://doi.org/10.1109/TNNLS.2021.3126114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Xu et al. solved a class of time-varying linear equations and inequalities systems (LEIESs) by using a Zhang neural network (ZNN) model through introducing a nonnegative relaxation vector. However, the introduction of this unknown nonnegative slack vector will increase the size and complexity of the model, thereby increasing the cost of computation. In this article, we propose two new ZNN models (called traditional Zhang neural network (TZNN) and variant Zhang neural network (VZNN) models, respectively) in which no additional relaxation vector is needed. The convergence analysis of these two new models are performed, and two simulation experiments are given to illustrate their efficiency and effectiveness for solving the time-varying LEIESs, including the applicability of our proposed models to robot manipulator.},
  archive      = {J_TNNLS},
  author       = {Wenqi Wu and Bing Zheng},
  doi          = {10.1109/TNNLS.2021.3126114},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4957-4965},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two new zhang neural networks for solving time-varying linear equations and inequalities systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HpGAN: Sequence search with generative adversarial networks.
<em>TNNLS</em>, <em>34</em>(8), 4944–4956. (<a
href="https://doi.org/10.1109/TNNLS.2021.3126944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequences play an important role in many engineering applications. Searching sequences with desired properties has long been an intriguing but also challenging research topic. This article proposes a novel method, called HpGAN, to search desired sequences algorithmically using generative adversarial networks (GANs). HpGAN is based on the idea of zero-sum game to train a generative model, which can generate sequences with characteristics similar to the training sequences. In HpGAN, we design the Hopfield network as an encoder to avoid the limitations of GAN in generating discrete data. Compared with traditional sequence construction by algebraic tools, HpGAN is particularly suitable for complex problems which are intractable by mathematical analysis. We demonstrate the search capabilities of HpGAN in two applications: 1) HpGAN successfully found many different mutually orthogonal complementary sequence sets (MOCSSs) and optimal odd-length binary Z-complementary pairs (OB-ZCPs) which are not part of the training set. In the literature, both MOCSSs and OB-ZCPs have found wide applications in wireless communications and 2) HpGAN found new sequences which achieve a four-times increase of signal-to-interference ratio—benchmarked against the well-known Legendre sequences—of a mismatched filter (MMF) estimator in pulse compression radar systems. These sequences outperform those found by AlphaSeq.},
  archive      = {J_TNNLS},
  author       = {Mingxing Zhang and Zhengchun Zhou and Lanping Li and Zilong Liu and Meng Yang and Yanghe Feng},
  doi          = {10.1109/TNNLS.2021.3126944},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4944-4956},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HpGAN: Sequence search with generative adversarial networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach to detecting muscle fatigue based on sEMG
by using neural architecture search framework. <em>TNNLS</em>,
<em>34</em>(8), 4932–4943. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Muscle fatigue detection is of great significance to human physiological activities, but many complex factors increase the difficulty of this task. In this article, we integrate several effective techniques to distinguish muscle states under fatigue and nonfatigue conditions via surface electromyography (sEMG) signals. First, we perform an isometric contraction experiment of biceps brachii to collect sEMG signals. Second, we propose a neural architecture search (NAS) framework based on reinforcement learning to autogenerate neural networks. Finally, we present an effective two-step training strategy to improve the performance by combining CNN with three types of commonly used statistical algorithms. Meanwhile, we propose a data enhancement algorithm based on empirical mode decomposition (EMD) to generate time-series data for expanding the dataset. The results show that this search algorithm can hunt for high-performing networks, and the accuracy of the best-selected model combined with support vector machine (SVM) for the group is 96.5\%. With the same architecture, the average accuracy in individual models is 97.8\%. The proposed data enhancement technique can effectively improve the fatigue detection performance, which allows further implementations in the human–exoskeleton interaction systems.},
  archive      = {J_TNNLS},
  author       = {Shurun Wang and Hao Tang and Bin Wang and Jia Mo},
  doi          = {10.1109/TNNLS.2021.3124330},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4932-4943},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel approach to detecting muscle fatigue based on sEMG by using neural architecture search framework},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiplane convex proximal support vector machine.
<em>TNNLS</em>, <em>34</em>(8), 4918–4931. (<a
href="https://doi.org/10.1109/TNNLS.2021.3125955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective method for XOR problems, generalized eigenvalue proximal support vector machine (GEPSVM) recently has gained widespread attention accompanied with many variants proposed. Although these variants strengthen the classification performance to different extents, the number of fitting hyperplanes, similar to GEPSVM, for each class is still limited to just one. Intuitively, using single hyperplane seems not enough, especially for the datasets with complex feature structures. Therefore, this article mainly focuses on extending the fitting hyperplanes for each class from single one to multiple ones. However, such an extension from the original GEPSVM is not trivial even though, if possible, the elegant solution via generalized eigenvalues will also not be guaranteed. To address this issue, we first make a simple yet crucial transformation for the optimization problem of GEPSVM and then propose a novel multiplane convex proximal support vector machine (MCPSVM), where a set of hyperplanes determined by the features of the data are learned for each class. We adopt a strictly (geodesically) convex objective to characterize this optimization problem; thus, a more elegant closed-form solution is obtained, which only needs a few lines of MATLAB codes. Besides, MCPSVM is more flexible in form and can be naturally and seamlessly extended to the feature weighting learning, whereas GEPSVM and its variants can hardly straightforwardly work like this. Extensive experiments on benchmark and large-scale image datasets indicate the advantages of our MCPSVM.},
  archive      = {J_TNNLS},
  author       = {Chuanxing Geng and Songcan Chen},
  doi          = {10.1109/TNNLS.2021.3125955},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4918-4931},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiplane convex proximal support vector machine},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Amortized bayesian model comparison with evidential deep
learning. <em>TNNLS</em>, <em>34</em>(8), 4903–4917. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparing competing mathematical models of complex processes is a shared goal among many branches of science. The Bayesian probabilistic framework offers a principled way to perform model comparison and extract useful metrics for guiding decisions. However, many interesting models are intractable with standard Bayesian methods, as they lack a closed-form likelihood function or the likelihood is computationally too expensive to evaluate. In this work, we propose a novel method for performing Bayesian model comparison using specialized deep learning architectures. Our method is purely simulation-based and circumvents the step of explicitly fitting all alternative models under consideration to each observed dataset. Moreover, it requires no hand-crafted summary statistics of the data and is designed to amortize the cost of simulation over multiple models, datasets, and dataset sizes. This makes the method especially effective in scenarios where model fit needs to be assessed for a large number of datasets, so that case-based inference is practically infeasible. Finally, we propose a novel way to measure epistemic uncertainty in model comparison problems. We demonstrate the utility of our method on toy examples and simulated data from nontrivial models from cognitive science and single-cell neuroscience. We show that our method achieves excellent results in terms of accuracy, calibration, and efficiency across the examples considered in this work. We argue that our framework can enhance and enrich model-based analysis and inference in many fields dealing with computational models of natural processes. We further argue that the proposed measure of epistemic uncertainty provides a unique proxy to quantify absolute evidence even in a framework which assumes that the true data-generating model is within a finite set of candidate models.},
  archive      = {J_TNNLS},
  author       = {Stefan T. Radev and Marco D’Alessandro and Ulf K. Mertens and Andreas Voss and Ullrich Köthe and Paul-Christian Bürkner},
  doi          = {10.1109/TNNLS.2021.3124052},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4903-4917},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Amortized bayesian model comparison with evidential deep learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fixed-time system identification using concurrent learning.
<em>TNNLS</em>, <em>34</em>(8), 4892–4902. (<a
href="https://doi.org/10.1109/TNNLS.2021.3125145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a fixed-time (FxT) system identifier for continuous-time nonlinear systems. A novel adaptive update law with discontinuous gradient flows of the identification errors is presented, which leverages concurrent learning (CL) to guarantee the learning of uncertain nonlinear dynamics in a fixed time, as opposed to asymptotic or exponential time. More specifically, the CL approach retrieves a batch of samples stored in a memory, and the update law simultaneously minimizes the identification error for the current stream of samples and past memory samples. Rigorous analyses are provided based on FxT Lyapunov stability to certify FxT convergence to the stable equilibria of the gradient descent flow of the system identification error under easy-to-verify rank conditions. The performance of the proposed method in comparison with the existing methods is illustrated in the simulation results.},
  archive      = {J_TNNLS},
  author       = {Farzaneh Tatari and Majid Mazouchi and Hamidreza Modares},
  doi          = {10.1109/TNNLS.2021.3125145},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4892-4902},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fixed-time system identification using concurrent learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bicriteria sparse nonnegative matrix factorization via
two-timescale duplex neurodynamic optimization. <em>TNNLS</em>,
<em>34</em>(8), 4881–4891. (<a
href="https://doi.org/10.1109/TNNLS.2021.3125457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, sparse nonnegative matrix factorization (SNMF) is formulated as a mixed-integer bicriteria optimization problem for minimizing matrix factorization errors and maximizing factorized matrix sparsity based on an exact binary representation of $l_{0}$ matrix norm. The binary constraints of the problem are then equivalently replaced with bilinear constraints to convert the problem to a biconvex problem. The reformulated biconvex problem is finally solved by using a two-timescale duplex neurodynamic approach consisting of two recurrent neural networks (RNNs) operating collaboratively at two timescales. A Gaussian score (GS) is defined as to integrate the bicriteria of factorization errors and sparsity of resulting matrices. The performance of the proposed neurodynamic approach is substantiated in terms of low factorization errors, high sparsity, and high GS on four benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Hangjun Che and Jun Wang and Andrzej Cichocki},
  doi          = {10.1109/TNNLS.2021.3125457},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4881-4891},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bicriteria sparse nonnegative matrix factorization via two-timescale duplex neurodynamic optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-based motion segmentation with spatio-temporal graph
cuts. <em>TNNLS</em>, <em>34</em>(8), 4868–4880. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying independently moving objects is an essential task for dynamic scene understanding. However, traditional cameras used in dynamic scenes may suffer from motion blur or exposure artifacts due to their sampling principle. By contrast, event-based cameras are novel bio-inspired sensors that offer advantages to overcome such limitations. They report pixel-wise intensity changes asynchronously, which enables them to acquire visual information at exactly the same rate as the scene dynamics. We develop a method to identify independently moving objects acquired with an event-based camera, that is, to solve the event-based motion segmentation problem. We cast the problem as an energy minimization one involving the fitting of multiple motion models. We jointly solve two sub-problems, namely event-cluster assignment (labeling) and motion model fitting, in an iterative manner by exploiting the structure of the input event data in the form of a spatio-temporal graph. Experiments on available datasets demonstrate the versatility of the method in scenes with different motion patterns and number of moving objects. The evaluation shows state-of-the-art results without having to predetermine the number of expected moving objects. We release the software and dataset under an open source license to foster research in the emerging topic of event-based motion segmentation.},
  archive      = {J_TNNLS},
  author       = {Yi Zhou and Guillermo Gallego and Xiuyuan Lu and Siqi Liu and Shaojie Shen},
  doi          = {10.1109/TNNLS.2021.3124580},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4868-4880},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-based motion segmentation with spatio-temporal graph cuts},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Progressive enhancement of label distributions for partial
multilabel learning. <em>TNNLS</em>, <em>34</em>(8), 4856–4867. (<a
href="https://doi.org/10.1109/TNNLS.2021.3125366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial multi-label learning (PML) aims to learn a multilabel predictive model from the PML training examples, each of which is associated with a set of candidate labels where only a subset is valid. The common strategy to induce a predictive model is identifying the valid labels in each candidate label set. Nonetheless, this strategy ignores considering the essential label distribution corresponding to each instance as label distributions are not explicitly available in the training dataset. In this article, a novel partial multilabel learning method is proposed to recover the latent label distribution and progressively enhance it for predictive model induction. Specifically, the label distribution is recovered by considering the observation model for logical labels and the sharing topological structure from feature space to label distribution space. Besides, the latent label distribution is progressively enhanced by recovering latent labeling information and supervising predictive model training alternatively to make the label distribution appropriate for the induced predictive model. Experimental results on PML datasets clearly validate the effectiveness of the proposed method for solving partial multilabel learning problems. In addition, further experiments show the high quality of the recovered label distributions and the effectiveness of adopting label distributions for partial multilabel learning.},
  archive      = {J_TNNLS},
  author       = {Ning Xu and Yun-Peng Liu and Yan Zhang and Xin Geng},
  doi          = {10.1109/TNNLS.2021.3125366},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4856-4867},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Progressive enhancement of label distributions for partial multilabel learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Distributed neural-network-based cooperation control for
teleoperation of multiple mobile manipulators under round-robin
protocol. <em>TNNLS</em>, <em>34</em>(8), 4841–4855. (<a
href="https://doi.org/10.1109/TNNLS.2021.3125004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the distributed cooperative control design for a class of sampled-data teleoperation systems with multiple slave mobile manipulators grasping an object in the presence of communication bandwidth limitation and time delays. Discrete-time information transmission with time-varying delays is assumed, and the Round-Robin (RR) scheduling protocol is used to regulate the data transmission from the multiple slaves to the master. The control task is to guarantee the task-space position synchronization between the master and the grasped object with the mobile bases in a fixed formation. A fully distributed control strategy including neural-network-based task-space synchronization controllers and neural-network-based null-space formation controllers is proposed, where the radial basis function (RBF) neural networks with adaptive estimation of approximation errors are used to compensate the dynamical uncertainties. The stability and the synchronization/formation features of the single-master–multiple-slaves (SMMS) teleoperation system are analyzed, and the relationship among the control parameters, the upper bound of the time delays, and the maximum allowable sampling interval is established. Experiments are implemented to validate the effectiveness of the proposed control algorithm.},
  archive      = {J_TNNLS},
  author       = {Yuling Li and Liping Wang and Kun Liu and Wei He and Yixin Yin and Rolf Johansson},
  doi          = {10.1109/TNNLS.2021.3125004},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4841-4855},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed neural-network-based cooperation control for teleoperation of multiple mobile manipulators under round-robin protocol},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An alternating-direction-method of multipliers-incorporated
approach to symmetric non-negative latent factor analysis.
<em>TNNLS</em>, <em>34</em>(8), 4826–4840. (<a
href="https://doi.org/10.1109/TNNLS.2021.3125774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale undirected weighted networks are frequently encountered in big-data-related applications concerning interactions among a large unique set of entities. Such a network can be described by a Symmetric, High-Dimensional, and Incomplete (SHDI) matrix whose symmetry and incompleteness should be addressed with care. However, existing models fail in either correctly representing its symmetry or efficiently handling its incomplete data. For addressing this critical issue, this study proposes an Alternating-Direction-Method of Multipliers (ADMM)-based Symmetric Non-negative Latent Factor Analysis (ASNL) model. It adopts fourfold ideas: 1) implementing the data density-oriented modeling for efficiently representing an SHDI matrix’s incomplete and imbalanced data; 2) separating the non-negative constraints from the decision parameters to avoid truncations during the training process; 3) incorporating the ADMM principle into its learning scheme for fast model convergence; and 4) parallelizing the training process with load balance considerations for high efficiency. Empirical studies on four SHDI matrices demonstrate that ASNL significantly outperforms several state-of-the-art models in both prediction accuracy for missing data of an SHDI and computational efficiency. It is a promising model for handling large-scale undirected networks raised in real applications.},
  archive      = {J_TNNLS},
  author       = {Xin Luo and Yurong Zhong and Zidong Wang and Maozhen Li},
  doi          = {10.1109/TNNLS.2021.3125774},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4826-4840},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An alternating-direction-method of multipliers-incorporated approach to symmetric non-negative latent factor analysis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empowering the diversity and individuality of option:
Residual soft option critic framework. <em>TNNLS</em>, <em>34</em>(8),
4816–4825. (<a
href="https://doi.org/10.1109/TNNLS.2021.3128666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting temporal abstraction (option), which empowers the action space, is a crucial challenge in hierarchical reinforcement learning. Under a well-structured action space, decision-making agents can probe more deeply in the searching or plan efficiently through pruning irrelevant action candidates. However, automatically capturing a well-performed temporal abstraction is a nontrivial challenge due to its insufficient exploration and inadequate functionality. We consider alleviating this challenge from two perspectives, i.e., diversity and individuality. For the aspect of diversity, we propose a maximum entropy model based on ensembled options to encourage exploration. For the aspect of individuality, we propose to distinguish each option accurately, utilizing mutual formation minimization, so that each option can better express and function. We name our framework as an ensemble with soft option (ESO) critics. Furthermore, the residual algorithm (RA) with a bidirectional target network is introduced to stabilize bootstrapping, yielding a residual version of ESO. We provide detailed analysis for extensive experiments, which shows that our method boosts performance in commonly used continuous control tasks.},
  archive      = {J_TNNLS},
  author       = {Anjie Zhu and Feiyu Chen and Hui Xu and Deqiang Ouyang and Jie Shao},
  doi          = {10.1109/TNNLS.2021.3128666},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4816-4825},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Empowering the diversity and individuality of option: Residual soft option critic framework},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-adaptive crowd counting via high-quality image
translation and density reconstruction. <em>TNNLS</em>, <em>34</em>(8),
4803–4815. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, crowd counting using supervised learning achieves a remarkable improvement. Nevertheless, most counters rely on a large amount of manually labeled data. With the release of synthetic crowd data, a potential alternative is transferring knowledge from them to real data without any manual label. However, there is no method to effectively suppress domain gaps and output elaborate density maps during the transferring. To remedy the above problems, this article proposes a domain-adaptive crowd counting (DACC) framework, which consists of a high-quality image translation and density map reconstruction. To be specific, the former focuses on translating synthetic data to realistic images, which prompts the translation quality by segregating domain-shared/independent features and designing content-aware consistency loss. The latter aims at generating pseudo labels on real scenes to improve the prediction quality. Next, we retrain a final counter using these pseudo labels. Adaptation experiments on six real-world datasets demonstrate that the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Junyu Gao and Tao Han and Yuan Yuan and Qi Wang},
  doi          = {10.1109/TNNLS.2021.3124272},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4803-4815},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Domain-adaptive crowd counting via high-quality image translation and density reconstruction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semisupervised network embedding with differentiable deep
quantization. <em>TNNLS</em>, <em>34</em>(8), 4791–4802. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning accurate low-dimensional embeddings for a network is a crucial task as it facilitates many downstream network analytics tasks. For large networks, the trained embeddings often require a significant amount of space to store, making storage and processing a challenge. Building on our previous work on semisupervised network embedding, we develop d-SNEQ, a differentiable DNN-based quantization method for network embedding. d-SNEQ incorporates a rank loss to equip the learned quantization codes with rich high-order information and is able to substantially compress the size of trained embeddings, thus reducing storage footprint and accelerating retrieval speed. We also propose a new evaluation metric, path prediction, to fairly and more directly evaluate the model performance on the preservation of high-order information. Our evaluation on four real-world networks of diverse characteristics shows that d-SNEQ outperforms a number of state-of-the-art embedding methods in link prediction, path prediction, node classification, and node recommendation while being far more space- and time-efficient.},
  archive      = {J_TNNLS},
  author       = {Tao He and Lianli Gao and Jingkuan Song and Yuan-Fang Li},
  doi          = {10.1109/TNNLS.2021.3129280},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4791-4802},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised network embedding with differentiable deep quantization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational dynamic for self-supervised exploration in deep
reinforcement learning. <em>TNNLS</em>, <em>34</em>(8), 4776–4790. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient exploration remains a challenging problem in reinforcement learning, especially for tasks where extrinsic rewards from environments are sparse or even totally disregarded. Significant advances based on intrinsic motivation show promising results in simple environments but often get stuck in environments with multimodal and stochastic dynamics. In this work, we propose a variational dynamic model based on the conditional variational inference to model the multimodality and stochasticity. We consider the environmental state–action transition as a conditional generative process by generating the next-state prediction under the condition of the current state, action, and latent variable, which provides a better understanding of the dynamics and leads to a better performance in exploration. We derive an upper bound of the negative log likelihood of the environmental transition and use such an upper bound as the intrinsic reward for exploration, which allows the agent to learn skills by self-supervised exploration without observing extrinsic rewards. We evaluate the proposed method on several image-based simulation tasks and a real robotic manipulating task. Our method outperforms several state-of-the-art environment model-based exploration approaches.},
  archive      = {J_TNNLS},
  author       = {Chenjia Bai and Peng Liu and Kaiyu Liu and Lingxiao Wang and Yingnan Zhao and Lei Han and Zhaoran Wang},
  doi          = {10.1109/TNNLS.2021.3129160},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4776-4790},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Variational dynamic for self-supervised exploration in deep reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized contextual bandits with latent features:
Algorithms and applications. <em>TNNLS</em>, <em>34</em>(8), 4763–4775.
(<a href="https://doi.org/10.1109/TNNLS.2021.3124603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual bandit is a popular sequential decision-making framework to balance the exploration and exploitation tradeoff in many applications such as recommender systems, search engines, etc. Motivated by two important factors in real-world applications: 1) latent contexts (or features) often exist and 2) feedbacks often have humans in the loop leading to human biases, we formulate a generalized contextual bandit framework with latent contexts. Our proposed framework includes a two-layer probabilistic interpretable model for the feedbacks from human with latent features. We design a GCL-PS algorithm for the proposed framework, which utilizes posterior sampling to balance the exploration and exploitation tradeoff. We prove a sublinear regret upper bound for GCL-PS, and prove a lower bound for the proposed bandit framework revealing insights on the optimality of GCL-PS. To further improve the computational efficiency of GCL-PS, we propose a Markov Chain Monte Carlo (MCMC) algorithm to generate approximate samples, resulting in our GCL-PSMC algorithm. We not only prove a sublinear Bayesian regret upper bound for our GCL-PSMC algorithm, but also reveal insights into the tradeoff between computational efficiency and sequential decision accuracy. Finally, we apply the proposed framework to hotel recommendations and news article recommendations, and show its superior performance over a variety of baselines via experiments on two public datasets.},
  archive      = {J_TNNLS},
  author       = {Xiongxiao Xu and Hong Xie and John C. S. Lui},
  doi          = {10.1109/TNNLS.2021.3124603},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4763-4775},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized contextual bandits with latent features: Algorithms and applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered neural-network adaptive control for
strict-feedback nonlinear systems: Selections on valid compact sets.
<em>TNNLS</em>, <em>34</em>(8), 4750–4762. (<a
href="https://doi.org/10.1109/TNNLS.2021.3120620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies neural-network (NN) adaptive control for strict-feedback nonlinear systems with matched uncertainties and event-triggered communication. Radial basis function NNs (RBFNNs) are used in the backstepping design approach to compensate for nonlinear uncertain functions. The concept of valid compact sets for RBFNN adaptive controllers is proposed, where a local RBFNN approximator is defined and the closed-loop state can remain. To guarantee the existence of such valid compact sets, a new property on RBFNNs is presented, which shows that, in some properly designed RBFNNs, the norm of their ideal weight vectors can always become arbitrarily small. By utilizing this property, the selections on valid compact sets are investigated, resulting in rigorous proof on RBFNN adaptive controllers to solve a local tracking problem with a given smooth enough reference signal. Subsequently, to save limited communication resources, a Zeno-free event-triggering mechanism in controller-to-actuator channels is proposed. Under this event-triggered adaptive controller, the corresponding tradeoff among the tracking performance, computational burden, and communication consumption is analyzed. Furthermore, two extensions are made to the general local function approximator, which is in the form of a weight vector multiplying a group of basis functions, and to the communication in sensor-to-controller channels. Finally, several simulation results are provided to illustrate the efficiency and feasibility of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Hao Yu and Tongwen Chen},
  doi          = {10.1109/TNNLS.2021.3120620},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4750-4762},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered neural-network adaptive control for strict-feedback nonlinear systems: Selections on valid compact sets},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive neural network output-constraint control for a
variable-length rotary arm with input backlash nonlinearity.
<em>TNNLS</em>, <em>34</em>(8), 4741–4749. (<a
href="https://doi.org/10.1109/TNNLS.2021.3117251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the problem of deformation reduction and attitude tracking for a rotated and extended flexible crane arm with input backlash-saturation and output asymmetrical constraint. By employing Halmilton’s principle, the arm system model is formulated by a set of partial and ordinary differential equations (ODEs). Given the modeling inaccuracy, a radial neural network (RNN) is used to approximate system parameters. To better design the controllers, the backstepping technique is applied to the control design. For input nonlinearities with backlash and saturation, we reversely transform them as an asymmetric saturation constraint via a virtual input. A barrier Lyapunov function (BLF) containing logarithmic terms is constructed to guarantee the asymmetric output constraints and the uniformly ultimate boundedness and stability of the arm system are proved. Finally, to testify the effectiveness of the proposed controllers, numerical simulations are carried out, and responding simulation diagrams are displayed.},
  archive      = {J_TNNLS},
  author       = {Yanfang Mei and Yu Liu and Huan Wang},
  doi          = {10.1109/TNNLS.2021.3117251},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4741-4749},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network output-constraint control for a variable-length rotary arm with input backlash nonlinearity},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural network output-feedback consensus fault-tolerant
control for nonlinear multiagent systems with intermittent actuator
faults. <em>TNNLS</em>, <em>34</em>(8), 4728–4740. (<a
href="https://doi.org/10.1109/TNNLS.2021.3117364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the distributed adaptive neural network (NN) consensus fault-tolerant control (FTC) problem is studied for nonstrict-feedback nonlinear multiagent systems (NMASs) subjected to intermittent actuator faults. The NNs are applied to approximate nonlinear functions, and a NN state-observer is developed to estimate the unmeasured states. Then, to compensate for the influence of intermittent actuator faults, a novel distributed output-feedback adaptive FTC is then designed by co-designing the last virtual controller, and the problem of “algebraic-loop” can be solved. The stability of the closed-loop system is proven by using the Lyapunov theory. Finally, the effectiveness of the proposed FTC approach is validated by numerical and practical examples.},
  archive      = {J_TNNLS},
  author       = {Wei Wu and Yongming Li and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2021.3117364},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4728-4740},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network output-feedback consensus fault-tolerant control for nonlinear multiagent systems with intermittent actuator faults},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered practical prescribed time output feedback
neuroadaptive tracking control under saturated actuation.
<em>TNNLS</em>, <em>34</em>(8), 4717–4727. (<a
href="https://doi.org/10.1109/TNNLS.2021.3118089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on the issue of event-triggered practical prescribed time tracking control for a type of uncertain nonlinear systems subject to actuator saturation and unmeasurable states as well as time-varying unknown control coefficients. First, a state observer with simple structure is constructed by means of neural network technology to estimate the unmeasurable system states under time-varying control coefficients. Then, with the help of one-to-one nonlinear mapping of the tracking error, an event-triggered output feedback control scheme is developed to steer the tracking error into a residual set of predefined accuracy within a preassigned settling time. Unlike existing related control methods, there is no need to involve finite-time state observer or fractional power feedback of system states, and thus, the control solution presented here is less complex and more acceptable. The key technique in control design lies in the establishment of an alternative first-order auxiliary system for dealing with the impact arisen from the input saturation. In our proposed approach, a new bounded function related to auxiliary variable and new dynamics of the auxiliary system are skillfully utilized such that the upper bound of the difference between actual input and designed input signal is not involved in implementation of the controller.},
  archive      = {J_TNNLS},
  author       = {Shuyan Zhou and Yongduan Song and Changyun Wen},
  doi          = {10.1109/TNNLS.2021.3118089},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4717-4727},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered practical prescribed time output feedback neuroadaptive tracking control under saturated actuation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FastHyMix: Fast and parameter-free hyperspectral image mixed
noise removal. <em>TNNLS</em>, <em>34</em>(8), 4702–4716. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decrease in the widths of spectral bands in hyperspectral imaging leads to a decrease in signal-to-noise ratio (SNR) of measurements. The decreased SNR reduces the reliability of measured features or information extracted from hyperspectral images (HSIs). Furthermore, the image degradations linked with various mechanisms also result in different types of noise, such as Gaussian noise, impulse noise, deadlines, and stripes. This article introduces a fast and parameter-free hyperspectral image mixed noise removal method (termed FastHyMix), which characterizes the complex distribution of mixed noise by using a Gaussian mixture model and exploits two main characteristics of hyperspectral data, namely, low rankness in the spectral domain and high correlation in the spatial domain. The Gaussian mixture model enables us to make a good estimation of Gaussian noise intensity and the locations of sparse noise. The proposed method takes advantage of the low rankness using subspace representation and the spatial correlation of HSIs by adding a powerful deep image prior, which is extracted from a neural denoising network. An exhaustive array of experiments and comparisons with state-of-the-art denoisers was carried out. The experimental results show significant improvement in both synthetic and real datasets. A MATLAB demo of this work is available at https://github.com/LinaZhuang for the sake of reproducibility.},
  archive      = {J_TNNLS},
  author       = {Lina Zhuang and Michael K. Ng},
  doi          = {10.1109/TNNLS.2021.3112577},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4702-4716},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FastHyMix: Fast and parameter-free hyperspectral image mixed noise removal},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven finite-horizon h∞ tracking control with
event-triggered mechanism for the continuous-time nonlinear systems.
<em>TNNLS</em>, <em>34</em>(8), 4687–4701. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the neural network (NN)-based adaptive dynamic programming (ADP) event-triggered control method is presented to obtain the near-optimal control policy for the model-free finite-horizon $H_\infty $ optimal tracking control problem with constrained control input. First, using available input–output data, a data-driven model is established by a recurrent NN (RNN) to reconstruct the unknown system. Then, an augmented system with event-triggered mechanism is obtained by a tracking error system and a command generator. We present a novel event-triggering condition without Zeno behavior. On this basis, the relationship between event-triggered Hamilton–Jacobi–Isaacs (HJI) equation and time-triggered HJI equation is given in Theorem 3 . Since the solution of the HJI equation is time-dependent for the augmented system, the time-dependent activation functions of NNs are considered. Moreover, an extra error is incorporated to satisfy the terminal constraints of cost function. This adaptive control pattern finds, in real time, approximations of the optimal value while also ensuring the uniform ultimate boundedness of the closed-loop system. Finally, the effectiveness of the proposed near-optimal control pattern is verified by two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Huaguang Zhang and Zhongyang Ming and Yuqing Yan and Wei Wang},
  doi          = {10.1109/TNNLS.2021.3116464},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4687-4701},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven finite-horizon h∞ tracking control with event-triggered mechanism for the continuous-time nonlinear systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Intermittent control to stabilization of stochastic highly
non-linear coupled systems with multiple time delays. <em>TNNLS</em>,
<em>34</em>(8), 4674–4686. (<a
href="https://doi.org/10.1109/TNNLS.2021.3113508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the stabilization of stochastic highly non-linear coupled systems (SHNCSs) with multiple time delays by using periodically intermittent control (PIC). It is worth noting that coefficients in SHNCSs dissatisfy the linear growth condition, which weakens the previous stability conditions. In addition, PIC and multiple time delays are first introduced into the study of highly nonlinear systems, which leads to the existing methods being inapplicable to investigate the stability of SHNCSs with multiple time delays. Therefore, a novel Halanay-type differential inequality is established, which can be employed to deal with highly nonlinear systems with PIC. Based on the Lyapunov method, the graph theory, and the novel differential inequality, SHNCSs with multiple time delays are first studied, and stability criteria are presented. Next, the theoretical results can be applied to modified FitzHugh–Nagumo models. At last, a numerical example is presented to show the effectiveness of our results.},
  archive      = {J_TNNLS},
  author       = {Yan Liu and Yi-Min Li and Jin-Liang Wang},
  doi          = {10.1109/TNNLS.2021.3113508},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4674-4686},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intermittent control to stabilization of stochastic highly non-linear coupled systems with multiple time delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the classification performance of dendrite
morphological neurons. <em>TNNLS</em>, <em>34</em>(8), 4659–4673. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dendrite morphological neurons (DMNs) are neural models for pattern classification, where dendrites are represented by a geometric shape enclosing patterns of the same class. This study evaluates the impact of three dendrite geometries—namely, box, ellipse, and sphere—on pattern classification. In addition, we propose using smooth maximum and minimum functions to reduce the coarseness of decision boundaries generated by typical DMNs, and a softmax layer is attached at the DMN output to provide posterior probabilities from weighted dendrites responses. To adjust the number of dendrites per class automatically, a tuning algorithm based on an incremental–decremental procedure is introduced. The classification performance assessment is conducted on nine synthetic and 49 real-world datasets. Meanwhile, 12 DMN variants are evaluated in terms of accuracy and model complexity. The DMN reaches its highest potential by combining spherical dendrites with smooth activation functions and a learnable softmax layer. It attained the highest accuracy, uses the simplest geometric shape, is insensitive to variables with zero variance, and its structural complexity diminishes by using the smooth maximum function. Furthermore, this DMN configuration performed competitively or even better than other well-established classifiers in terms of accuracy, such as support vector machine, multilayer perceptron, radial basis function network, $k$ -nearest neighbors, and random forest. Thus, the proposed DMN is an attractive alternative for pattern classification in real-world problems.},
  archive      = {J_TNNLS},
  author       = {Wilfrido Gómez-Flores and Humberto Sossa},
  doi          = {10.1109/TNNLS.2021.3116519},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4659-4673},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving the classification performance of dendrite morphological neurons},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ELM-based adaptive faster fixed-time control of robotic
manipulator systems. <em>TNNLS</em>, <em>34</em>(8), 4646–4658. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of fast fixed-time tracking control for robotic manipulator systems subject to model uncertainties and disturbances. First, on the basis of a newly constructed fixed-time stable system, a novel faster nonsingular fixed-time sliding mode (FNFTSM) surface is developed to ensure a faster convergence rate, and the settling time of the proposed surface is independent of initial values of system states. Subsequently, an extreme learning machine (ELM) algorithm is utilized to suppress the negative influence of system uncertainties and disturbances. By incorporating fixed-time stable theory and the ELM learning technique, an adaptive fixed-time sliding mode control scheme without knowing any information of system parameters is synthesized, which can circumvent chattering phenomenon and ensure that the tracking errors converge to a small region in fixed time. Finally, the superior of the proposed control strategy is substantiated with comparison simulation results.},
  archive      = {J_TNNLS},
  author       = {Miaomiao Gao and Lijian Ding and Xiaozheng Jin},
  doi          = {10.1109/TNNLS.2021.3116958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4646-4658},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ELM-based adaptive faster fixed-time control of robotic manipulator systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Manifold interpolation for large-scale multiobjective
optimization via generative adversarial networks. <em>TNNLS</em>,
<em>34</em>(8), 4631–4645. (<a
href="https://doi.org/10.1109/TNNLS.2021.3113158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multiobjective optimization problems (LSMOPs) are characterized as optimization problems involving hundreds or even thousands of decision variables and multiple conflicting objectives. To solve LSMOPs, some algorithms designed a variety of strategies to track Pareto-optimal solutions (POSs) by assuming that the distribution of POSs follows a low-dimensional manifold. However, traditional genetic operators for solving LSMOPs have some deficiencies in dealing with the manifold, which often results in poor diversity, local optima, and inefficient searches. In this work, a generative adversarial network (GAN)-based manifold interpolation framework is proposed to learn the manifold and generate high-quality solutions on the manifold, thereby improving the optimization performance of evolutionary algorithms. We compare the proposed approach with several state-of-the-art algorithms on various large-scale multiobjective benchmark functions. The experimental results demonstrate that significant improvements have been achieved by the proposed framework in solving LSMOPs.},
  archive      = {J_TNNLS},
  author       = {Zhenzhong Wang and Haokai Hong and Kai Ye and Guang-En Zhang and Min Jiang and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2021.3113158},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4631-4645},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Manifold interpolation for large-scale multiobjective optimization via generative adversarial networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Event-triggered synchronization of multiple
fractional-order recurrent neural networks with time-varying delays.
<em>TNNLS</em>, <em>34</em>(8), 4620–4630. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the synchronization of multiple fractional-order recurrent neural networks (RNNs) with time-varying delays under event-triggered communications. Based on the assumption of the existence of strong connectivity or a spanning tree in the communication digraph, two sets of sufficient conditions are derived for achieving event-triggered synchronization. Moreover, an additional condition is derived to preclude Zeno behaviors. As a generalization of existing results, the criteria herein are also applicable to the event-triggered synchronization of multiple integer-order RNNs with or without delays. Two numerical examples are elaborated to illustrate the new results.},
  archive      = {J_TNNLS},
  author       = {Peng Liu and Jun Wang and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2021.3116382},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4620-4630},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered synchronization of multiple fractional-order recurrent neural networks with time-varying delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RAN-GNNs: Breaking the capacity limits of graph neural
networks. <em>TNNLS</em>, <em>34</em>(8), 4610–4619. (<a
href="https://doi.org/10.1109/TNNLS.2021.3118450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have become a staple in problems addressing learning and analysis of data defined over graphs. However, several results suggest an inherent difficulty in extracting better performance by increasing the number of layers. Recent works attribute this to a phenomenon peculiar to the extraction of node features in graph-based tasks, i.e., the need to consider multiple neighborhood sizes at the same time and adaptively tune them. In this article, we investigate the recently proposed randomly wired architectures in the context of GNNs. Instead of building deeper networks by stacking many layers, we prove that employing a randomly wired architecture can be a more effective way to increase the capacity of the network and obtain richer representations. We show that such architectures behave like an ensemble of paths, which are able to merge contributions from receptive fields of varied size. Moreover, these receptive fields can also be modulated to be wider or narrower through the trainable weights over the paths. We also provide extensive experimental evidence of the superior performance of randomly wired architectures over multiple tasks and five graph convolution definitions, using recent benchmarking frameworks that address the reliability of previous testing methodologies.},
  archive      = {J_TNNLS},
  author       = {Diego Valsesia and Giulia Fracastoro and Enrico Magli},
  doi          = {10.1109/TNNLS.2021.3118450},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4610-4619},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RAN-GNNs: Breaking the capacity limits of graph neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inverse reinforcement learning for adversarial apprentice
games. <em>TNNLS</em>, <em>34</em>(8), 4596–4609. (<a
href="https://doi.org/10.1109/TNNLS.2021.3114612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes new inverse reinforcement learning (RL) algorithms to solve our defined Adversarial Apprentice Games for nonlinear learner and expert systems. The games are solved by extracting the unknown cost function of an expert by a learner using demonstrated expert’s behaviors. We first develop a model-based inverse RL algorithm that consists of two learning stages: an optimal control learning and a second learning based on inverse optimal control. This algorithm also clarifies the relationships between inverse RL and inverse optimal control. Then, we propose a new model-free integral inverse RL algorithm to reconstruct the unknown expert cost function. The model-free algorithm only needs online demonstration of the expert and learner’s trajectory data without knowing system dynamics of either the learner or the expert. These two algorithms are further implemented using neural networks (NNs). In Adversarial Apprentice Games, the learner and the expert are allowed to suffer from different adversarial attacks in the learning process. A two-player zero-sum game is formulated for each of these two agents and is solved as a subproblem for the learner in inverse RL. Furthermore, it is shown that the cost functions that the learner learns to mimic the expert’s behavior are stabilizing and not unique. Finally, simulations and comparisons show the effectiveness and the superiority of the proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Bosen Lian and Wenqian Xue and Frank L. Lewis and Tianyou Chai},
  doi          = {10.1109/TNNLS.2021.3114612},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4596-4609},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Inverse reinforcement learning for adversarial apprentice games},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning-based fixed-time trajectory tracking
control for uncertain robotic manipulators with input saturation.
<em>TNNLS</em>, <em>34</em>(8), 4584–4595. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fixed-time trajectory tracking control method for uncertain robotic manipulators with input saturation based on reinforcement learning (RL) is studied. The designed RL control algorithm is implemented by a radial basis function (RBF) neural network (NN), in which the actor NN is used to generate the control strategy and the critic NN is used to evaluate the execution cost. A new nonsingular fast terminal sliding mode technique is used to ensure the convergence of tracking error in fixed time, and the upper bound of convergence time is estimated. To solve the saturation problem of an actuator, a nonlinear antiwindup compensator is designed to compensate for the saturation effect of the joint torque actuator in real time. Finally, the stability of the closed-loop system based on the Lyapunov candidate is analyzed, and the timing convergence of the closed-loop system is proven. Simulation and experimental results show the effectiveness and superiority of the proposed control law.},
  archive      = {J_TNNLS},
  author       = {Shengjie Cao and Liang Sun and Jingjing Jiang and Zongyu Zuo},
  doi          = {10.1109/TNNLS.2021.3116713},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4584-4595},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning-based fixed-time trajectory tracking control for uncertain robotic manipulators with input saturation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic neural network for bicriteria weighted control of
robot manipulators. <em>TNNLS</em>, <em>34</em>(8), 4570–4583. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, bicriteria optimization schemes for manipulator control have become preferred by researchers, given their satisfactory performance. In this article, a bicriteria weighted (BCW) scheme to remedy joint drift and minimize the infinity norm of joint velocity is proposed. The scheme adopts a novel repetitive motion index that can theoretically decouple the joint error and the position error, which many conventional cyclic motion generation schemes cannot achieve. Subsequently, through transformation, the BCW scheme is converted into a time-varying quadratic programming (QP) problem. Then, a dynamic neural network (DNN) system with a new Fisher–Burmeister function is proposed to address the resulting QP problem. It is proven that the proposed DNN system is free of residual errors, which means that the actual solution is able to converge to the theoretical solution. Another essential feature of the DNN system is that it has a suppression effect on noise. To demonstrate the convergence and robustness of the proposed DNN system, comparative simulations are carried out in nominal and noisy environments. Finally, experiments on Franka Emika Panda are conducted to elucidate the availability of the BCW scheme addressed by the DNN system.},
  archive      = {J_TNNLS},
  author       = {Mei Liu and Li He and Mingsheng Shang},
  doi          = {10.1109/TNNLS.2021.3116321},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4570-4583},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic neural network for bicriteria weighted control of robot manipulators},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial autoencoder network for hyperspectral unmixing.
<em>TNNLS</em>, <em>34</em>(8), 4555–4569. (<a
href="https://doi.org/10.1109/TNNLS.2021.3114203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral unmixing (SU), which refers to extracting basic features (i.e., endmembers) at the subpixel level and calculating the corresponding proportion (i.e., abundances), has become a major preprocessing technique for the hyperspectral image analysis. Since the unmixing procedure can be explained as finding a set of low-dimensional representations that reconstruct the data with their corresponding bases, autoencoders (AEs) have been effectively designed to address unsupervised SU problems. However, their ability to exploit the prior properties remains limited, and noise and initialization conditions will greatly affect the performance of unmixing. In this article, we propose a novel technique network for unsupervised unmixing which is based on the adversarial AE, termed as adversarial autoencoder network (AAENet), to address the above problems. First, the image to be unmixed is assumed to be partitioned into homogeneous regions. Then, considering the spatial correlation between local pixels, the pixels in the same region are assumed to share the same statistical properties (means and covariances) and abundance can be modeled to follow an appropriate prior distribution. Then the adversarial training procedure is adapted to transfer the spatial information into the network. By matching the aggregated posterior of the abundance with a certain prior distribution to correct the weight of unmixing, the proposed AAENet exhibits a more accurate and interpretable unmixing performance. Compared with the traditional AE method, our approach can greatly enhance the performance and robustness of the model by using the adversarial procedure and adding the abundance prior to the framework. The experiments on both the simulated and real hyperspectral data demonstrate that the proposed algorithm can outperform the other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Qiwen Jin and Yong Ma and Fan Fan and Jun Huang and Xiaoguang Mei and Jiayi Ma},
  doi          = {10.1109/TNNLS.2021.3114203},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4555-4569},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial autoencoder network for hyperspectral unmixing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive neural consensus tracking control for nonlinear
multiagent systems using integral barrier lyapunov functionals.
<em>TNNLS</em>, <em>34</em>(8), 4544–4554. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the adaptive tracking control scheme of nonlinear multiagent systems under a directed graph and state constraints. In this article, the integral barrier Lyapunov functionals (iBLFs) are introduced to overcome the conservative limitation of the barrier Lyapunov function with error variables, relax the feasibility conditions, and simultaneously solve state constrained and coupling terms of the communication errors between agents. An adaptive distributed controller was designed based on iBLF and backstepping method, and iBLF was differentiated by means of the integral mean value theorem. At the same time, the properties of neural network are used to approximate the unknown terms, and the stability of the systems is proven by the Lyapunov stability theory. This scheme can not only ensure that the output of all the followers meets the output trajectory of the leader but also make the state variables not violate the constraint bounds, and all the closed-loop signals are bounded. Finally, the efficiency of the proposed controller is revealed.},
  archive      = {J_TNNLS},
  author       = {Fengyi Yuan and Yan-Jun Liu and Lei Liu and Jie Lan and Dapeng Li and Shaocheng Tong and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3112763},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4544-4554},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural consensus tracking control for nonlinear multiagent systems using integral barrier lyapunov functionals},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Policy gradient-based core placement optimization for
multichip many-core systems. <em>TNNLS</em>, <em>34</em>(8), 4529–4543.
(<a href="https://doi.org/10.1109/TNNLS.2021.3117878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As many deep neural network models become deeper and more complex, processing devices with stronger computing performance and communication capability are required. Following this trend, the dependence on multichip many-core systems that have high parallelism and reasonable transmission costs is on the rise. In this work, in order to improve routing performance of the system, such as routing runtime and power consumption, we propose a reinforcement learning (RL)- based core placement optimization approach, considering application constraints, such as deadlock caused by multicast paths. We leverage the capability of deep RL from indirect supervision as a direct nonlinear optimizer, and the parameters of the policy network are updated by proximal policy optimization. We treat the routing topology as a network graph, so we utilize a graph convolutional network to embed the features into the policy network. One step size environment is designed, so all cores are placed simultaneously. To handle large dimensional action space, we use continuous values matching with the number of cores as the output of the policy network and discretize them again for obtaining the new placement. For multichip system mapping, we developed a community detection algorithm. We use several datasets of multilayer perceptron and convolutional neural networks to evaluate our agent. We compare the optimal results obtained by our agent with other baselines under different multicast conditions. Our approach achieves a significant reduction of routing runtime, communication cost, and average traffic load, along with deadlock-free performance for inner chip data transmission. The traffic of interchip routing is also significantly reduced after integrating the community detection algorithm to our agent.},
  archive      = {J_TNNLS},
  author       = {Wooshik Myung and Donghyun Lee and Chenhang Song and Guanrui Wang and Cheng Ma},
  doi          = {10.1109/TNNLS.2021.3117878},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4529-4543},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Policy gradient-based core placement optimization for multichip many-core systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correlated chained gaussian processes for datasets with
multiple annotators. <em>TNNLS</em>, <em>34</em>(8), 4514–4528. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The labeling process within a supervised learning task is usually carried out by an expert, which provides the ground truth (gold standard) for each sample. However, in many real-world applications, we typically have access to annotations provided by crowds holding different and unknown expertise levels. Learning from crowds (LFC) intends to configure machine learning paradigms in the presence of multilabelers, residing on two key assumptions: the labeler’s performance does not depend on the input space, and independence among the annotators is imposed. Here, we propose the correlated chained Gaussian processes from the multiple annotators (CCGPMA) approach, which models each annotator’s performance as a function of the input space and exploits the correlations among experts. Experimental results associated with classification and regression tasks show that our CCGPMA performs better modeling of the labelers’ behavior, indicating that it consistently outperforms other state-of-the-art LFC approaches.},
  archive      = {J_TNNLS},
  author       = {J. Gil-González and Juan-José Giraldo and A. M. Álvarez-Meza and A. Orozco-Gutiérrez and M. A. Álvarez},
  doi          = {10.1109/TNNLS.2021.3116943},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4514-4528},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Correlated chained gaussian processes for datasets with multiple annotators},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BaGFN: Broad attentive graph fusion network for high-order
feature interactions. <em>TNNLS</em>, <em>34</em>(8), 4499–4513. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling feature interactions is of crucial significance to high-quality feature engineering on multifiled sparse data. At present, a series of state-of-the-art methods extract cross features in a rather implicit bitwise fashion and lack enough comprehensive and flexible competence of learning sophisticated interactions among different feature fields. In this article, we propose a new broad attentive graph fusion network (BaGFN) to better model high-order feature interactions in a flexible and explicit manner. On the one hand, we design an attentive graph fusion module to strengthen high-order feature representation under graph structure. The graph-based module develops a new bilinear-cross aggregation function to aggregate the graph node information, employs the self-attention mechanism to learn the impact of neighborhood nodes, and updates the high-order representation of features by multihop fusion steps. On the other hand, we further construct a broad attentive cross module to refine high-order feature interactions at a bitwise level. The optimized module designs a new broad attention mechanism to dynamically learn the importance weights of cross features and efficiently conduct the sophisticated high-order feature interactions at the granularity of feature dimensions. The final experimental results demonstrate the effectiveness of our proposed model.},
  archive      = {J_TNNLS},
  author       = {Zhifeng Xie and Wenling Zhang and Bin Sheng and Ping Li and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3116209},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4499-4513},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BaGFN: Broad attentive graph fusion network for high-order feature interactions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuroadaptive control for complicated underactuated systems
with simultaneous output and velocity constraints exerted on both
actuated and unactuated states. <em>TNNLS</em>, <em>34</em>(8),
4488–4498. (<a
href="https://doi.org/10.1109/TNNLS.2021.3115960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to limited workspace and safety requirements for practical underactuated mechanical systems, it is necessary to restrict all to-be-controlled variables and their velocities within preset ranges, avoid collisions/overshoots, and improve braking performance. However, due to fewer available control inputs, it is quite challenging to ensure error elimination and full-state constraints for both actuated/unactuated variables, including displacements/angles and their derivatives (i.e., velocity signals) together. To handle the above issues, this article designs a new adaptive full-state constraint controller for a class of uncertain multi-input–multi-output (MIMO) underactuated systems. First, different output constraint-related auxiliary functions are constructed in the Lyapunov function candidate to generate nonlinear displacement-/angle-limited terms to control all state variables. Then, this article handles velocity constraints in a new manner, where the elaborately designed velocity constraint-related terms are directly introduced into the presented controller (instead of the Lyapunov function candidate), and strict theoretical analysis is provided by utilizing reduction to absurdity. Hence, both actuated and unactuated velocity constraints are ensured to further improve transient performance. In addition, the impact of model uncertainties is addressed online to realize accurate positioning control for all state variables. Compared with current studies of underactuated systems, this article presents the first adaptive controller to address output and velocity constraints for actuated and unactuated variables together; moreover, their asymptotic convergence is proven by strict stability analysis, which is important both theoretically and practically. In the end, the feasibility and robustness of the proposed controller are verified by hardware experiments.},
  archive      = {J_TNNLS},
  author       = {Tong Yang and Ning Sun and Yongchun Fang},
  doi          = {10.1109/TNNLS.2021.3115960},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4488-4498},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuroadaptive control for complicated underactuated systems with simultaneous output and velocity constraints exerted on both actuated and unactuated states},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-performance acceleration of 2-d and 3-d CNNs on FPGAs
using static block floating point. <em>TNNLS</em>, <em>34</em>(8),
4473–4487. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, 2-D convolutional neural networks (CNNs) have demonstrated their great success in a wide range of 2-D computer vision applications, such as image classification and object detection. At the same time, 3-D CNNs, as a variant of 2-D CNNs, have shown their excellent ability to analyze 3-D data, such as video and geometric data. However, the heavy algorithmic complexity of 2-D and 3-D CNNs imposes a substantial overhead over the speed of these networks, which limits their deployment in real-life applications. Although various domain-specific accelerators have been proposed to address this challenge, most of them only focus on accelerating 2-D CNNs, without considering their computational efficiency on 3-D CNNs. In this article, we propose a unified hardware architecture to accelerate both 2-D and 3-D CNNs with high hardware efficiency. Our experiments demonstrate that the proposed accelerator can achieve up to 92.4\% and 85.2\% multiply-accumulate efficiency on 2-D and 3-D CNNs, respectively. To improve the hardware performance, we propose a hardware-friendly quantization approach called static block floating point (BFP), which eliminates the frequent representation conversions required in traditional dynamic BFP arithmetic. Comparing with the integer linear quantization using zero-point, the static BFP quantization can decrease the logic resource consumption of the convolutional kernel design by nearly 50\% on a field-programmable gate array (FPGA). Without time-consuming retraining, the proposed static BFP quantization is able to quantize the precision to 8-bit mantissa with negligible accuracy loss. As different CNNs on our reconfigurable system require different hardware and software parameters to achieve optimal hardware performance and accuracy, we also propose an automatic tool for parameter optimization. Based on our hardware design and optimization, we demonstrate that the proposed accelerator can achieve 3.8–5.6 times higher energy efficiency than graphics processing unit (GPU) implementation. Comparing with the state-of-the-art FPGA-based accelerators, our design achieves higher generality and up to 1.4–2.2 times higher resource efficiency on both 2-D and 3-D CNNs.},
  archive      = {J_TNNLS},
  author       = {Hongxiang Fan and Shuanglong Liu and Zhiqiang Que and Xinyu Niu and Wayne Luk},
  doi          = {10.1109/TNNLS.2021.3116302},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4473-4487},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {High-performance acceleration of 2-D and 3-D CNNs on FPGAs using static block floating point},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smoothness-driven consensus based on compact representation
for robust feature matching. <em>TNNLS</em>, <em>34</em>(8), 4460–4472.
(<a href="https://doi.org/10.1109/TNNLS.2021.3118409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For robust feature matching, a popular and particularly effective method is to recover smooth functions from the data to differentiate the true correspondences (inliers) from false correspondences (outliers). In the existing works, the well-established regularization theory has been extensively studied and exploited to estimate the functions while controlling its complexity to enforce the smoothness constraint, which has shown prominent advantages in this task. However, despite the theoretical optimality properties, the high complexities in both time and space are induced and become the main obstacle of their application. In this article, we propose a novel method for multivariate regression and point matching, which exploits the sparsity structure of smooth functions. Specifically, we use compact Fourier bases for constructing the function, which inherently allows a coarse-to-fine representation. The smoothness constraint can be explicitly imposed by adopting a few low-frequency bases for representation, resulting in reduced computational complexities of the induced multivariate regression algorithm. To cope with potential gross outliers, we formulate the learning problem into a Bayesian framework with latent variables indicating the inliers and outliers and a mixture model accounting for the distribution of data, where a fast expectation–maximization solution can be derived. Extensive experiments are conducted on synthetic data and real-world image matching, and point set registration datasets, which demonstrates the advantages of our method against the current state-of-the-art methods in terms of both scalability and robustness.},
  archive      = {J_TNNLS},
  author       = {Aoxiang Fan and Xingyu Jiang and Yong Ma and Xiaoguang Mei and Jiayi Ma},
  doi          = {10.1109/TNNLS.2021.3118409},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4460-4472},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Smoothness-driven consensus based on compact representation for robust feature matching},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient dynamic optimization algorithm for
path-constrained switched systems. <em>TNNLS</em>, <em>34</em>(8),
4451–4459. (<a
href="https://doi.org/10.1109/TNNLS.2021.3113345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic optimization is one of the model-based adaptive reinforcement learning methods, which has been widely used in industrial systems with switching mechanisms. This article presents an efficient dynamic optimization strategy to locate an optimal input and switch times for switched systems with guaranteed satisfaction for path constraints during the whole time period. In this article, we propose a single-level algorithm where, at each iteration, gradients of the objective function with respect to switch times and the system input are evaluated by solving adjoint systems and sensitivity equations, respectively. Then the optimization of the input is performed at the same iteration with that of the switch time vector, which greatly reduces the number of nonlinear programs (NLPs) and computational burden compared with multistage algorithms. The feasibility of the optimal solution is guaranteed by adapting a new policy iteration method proposed to switched systems. It is proven that the proposed algorithm terminates finitely, and converges to a solution which satisfies the Karush–Kuhn–Tucker (KKT) conditions to specified tolerances. Numerical case studies are provided to illustrate that the proposed algorithm has less expensive computational time than the bi-level algorithm.},
  archive      = {J_TNNLS},
  author       = {Chi Zhang and Jun Fu},
  doi          = {10.1109/TNNLS.2021.3113345},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4451-4459},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient dynamic optimization algorithm for path-constrained switched systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LRPRNet: Lightweight deep network by low-rank pointwise
residual convolution. <em>TNNLS</em>, <em>34</em>(8), 4440–4450. (<a
href="https://doi.org/10.1109/TNNLS.2021.3117685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has become popular in recent years primarily due to powerful computing devices such as graphics processing units (GPUs). However, it is challenging to deploy these deep models to multimedia devices, smartphones, or embedded systems with limited resources. To reduce the computation and memory costs, we propose a novel lightweight deep learning module by low-rank pointwise residual (LRPR) convolution, called LRPRNet. Essentially, LRPR aims at using a low-rank approximation in pointwise convolution to further reduce the module size while keeping depthwise convolutions as the residual module to rectify the LRPR module. This is critical when the low-rankness undermines the convolution process. Moreover, our LRPR is quite general and can be directly applied to many existing network architectures such as MobileNetv1, ShuffleNetv2, MixNet, and so on. Experiments on visual recognition tasks, including image classification and face alignment on popular benchmarks, show that our LRPRNet achieves competitive performance but with a significant reduction of Flops and memory cost compared to the state-of-the-art deep lightweight models.},
  archive      = {J_TNNLS},
  author       = {Bin Sun and Jun Li and Ming Shao and Yun Fu},
  doi          = {10.1109/TNNLS.2021.3117685},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4440-4450},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LRPRNet: Lightweight deep network by low-rank pointwise residual convolution},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative metric learning for partial label learning.
<em>TNNLS</em>, <em>34</em>(8), 4428–4439. (<a
href="https://doi.org/10.1109/TNNLS.2021.3118362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One simple strategy to deal with ambiguity in partial label learning (PLL) is to regard all candidate labels equally as the ground-truth label, and then solve the PLL problem using existing multiclass classification algorithms. However, due to the noisy false-positive labels in the candidate set, these approaches are readily mislead and do not generalize well in testing. Consequently, the method of identifying the ground-truth label straight from the candidate label set has grown popular and effective. When the labeling information in PLL is ambiguous, we ought to take advantage of the data’s underlying structure, such as label and feature interdependencies, to conduct disambiguation. Furthermore, while metric learning is an excellent method for supervised learning classification that takes feature and label interdependencies into account, it cannot be used to solve the weekly supervised learning PLL problem directly due to the ambiguity of labeling information in the candidate label set. In this article, we propose an effective PLL paradigm called discriminative metric learning for partial label learning (DML-PLL), which aims to learn a Mahanalobis distance metric discriminatively while identifying the ground-truth label iteratively for PLL. We also design an efficient algorithm to alternatively optimize the metric parameter and the latent ground-truth label in an iterative way. Besides, we prove the convergence of the designed algorithms by two proposed lemmas. We additionally study the computational complexity of the proposed DML-PLL in terms of training and testing time for each iteration. Extensive experiments on both controlled UCI datasets and real-world PLL datasets from diverse domains demonstrate that the proposed DML-PLL regularly outperforms the compared approaches in terms of prediction accuracy.},
  archive      = {J_TNNLS},
  author       = {Xiuwen Gong and Dong Yuan and Wei Bao},
  doi          = {10.1109/TNNLS.2021.3118362},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4428-4439},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative metric learning for partial label learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A low-power DNN accelerator enabled by a novel staircase
RRAM array. <em>TNNLS</em>, <em>34</em>(8), 4416–4427. (<a
href="https://doi.org/10.1109/TNNLS.2021.3118451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing the ubiquitous sensors and connected devices with computational abilities to realize visions of the Internet of Things (IoT) requires the development of robust, compact, and low-power deep neural network accelerators. Analog in-memory matrix–matrix multiplications enabled by emerging memories can significantly reduce the accelerator energy budget while resulting in compact accelerators. In this article, we design a hardware-aware deep neural network (DNN) accelerator that combines a planar-staircase resistive random access memory (RRAM) array with a variation-tolerant in-memory compute methodology to enhance the peak power efficiency by $5.64\times $ and area efficiency by $4.7\times $ over state-of-the-art DNN accelerators. Pulse application at the bottom electrodes of the staircase array generates a concurrent input shift, which eliminates the input unfolding, and regeneration required for convolution execution within typical crossbar arrays. Our in-memory compute method operates in charge domain and facilitates high-accuracy floating-point computations with low RRAM states, device requirement. This work provides a path toward fast hardware accelerators that use low power and low area.},
  archive      = {J_TNNLS},
  author       = {Hasita Veluri and Umesh Chand and Yida Li and Baoshan Tang and Aaron Voon-Yew Thean},
  doi          = {10.1109/TNNLS.2021.3118451},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4416-4427},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A low-power DNN accelerator enabled by a novel staircase RRAM array},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Morphological feature visualization of alzheimer’s disease
via multidirectional perception GAN. <em>TNNLS</em>, <em>34</em>(8),
4401–4415. (<a
href="https://doi.org/10.1109/TNNLS.2021.3118369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosis of early stages of Alzheimer’s disease (AD) is essential for timely treatment to slow further deterioration. Visualizing the morphological features for early stages of AD is of great clinical value. In this work, a novel multidirectional perception generative adversarial network (MP-GAN) is proposed to visualize the morphological features indicating the severity of AD for patients of different stages. Specifically, by introducing a novel multidirectional mapping mechanism into the model, the proposed MP-GAN can capture the salient global features efficiently. Thus, using the class discriminative map from the generator, the proposed model can clearly delineate the subtle lesions via MR image transformations between the source domain and the predefined target domain. Besides, by integrating the adversarial loss, classification loss, cycle consistency loss, and ${L}1$ penalty, a single generator in MP-GAN can learn the class discriminative maps for multiple classes. Extensive experimental results on Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset demonstrate that MP-GAN achieves superior performance compared with the existing methods. The lesions visualized by MP-GAN are also consistent with what clinicians observe.},
  archive      = {J_TNNLS},
  author       = {Wen Yu and Baiying Lei and Shuqiang Wang and Yong Liu and Zhiguang Feng and Yong Hu and Yanyan Shen and Michael K. Ng},
  doi          = {10.1109/TNNLS.2021.3118369},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4401-4415},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Morphological feature visualization of alzheimer’s disease via multidirectional perception GAN},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Brain-inspired search engine assistant based on knowledge
graph. <em>TNNLS</em>, <em>34</em>(8), 4386–4400. (<a
href="https://doi.org/10.1109/TNNLS.2021.3113026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Search engines can quickly respond to a hyperlink list according to query keywords. However, when a query is complex, developers need to repeatedly refine search keywords and open a large number of web pages to find and summarize answers. Many research works of question and answering (Q&amp;A) system attempt to assist search engines by providing simple, accurate, and understandable answers. However, without original semantic contexts, these answers lack explainability, making them difficult for users to trust and adopt. In this article, a brain-inspired search engine assistant named DeveloperBot based on knowledge graph is proposed, which aligns to the cognitive process of humans and has the capacity to answer complex queries with explainability. Specifically, DeveloperBot first constructs a multilayer query graph by splitting a complex multiconstraint query into several ordered constraints. Then, it models a constraint reasoning process as a subgraph search process inspired by a spreading activation model of cognitive science. In the end, novel features of the subgraph are extracted for decision-making. The corresponding reasoning subgraph and answer confidence are derived as explanations. The results of the decision-making demonstrate that DeveloperBot can estimate answers and answer confidences with high accuracy. We implement a prototype and conduct a user study to evaluate whether and how the direct answers and the explanations provided by DeveloperBot can assist developers’ information needs.},
  archive      = {J_TNNLS},
  author       = {Xuejiao Zhao and Huanhuan Chen and Zhenchang Xing and Chunyan Miao},
  doi          = {10.1109/TNNLS.2021.3113026},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4386-4400},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Brain-inspired search engine assistant based on knowledge graph},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multiview orthonormalized partial least squares:
Regularizations and deep extensions. <em>TNNLS</em>, <em>34</em>(8),
4371–4385. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we establish a family of subspace-based learning methods for multiview learning using least squares as the fundamental basis. Specifically, we propose a novel unified multiview learning framework called multiview orthonormalized partial least squares (MvOPLSs) to learn a classifier over a common latent space shared by all views. The regularization technique is further leveraged to unleash the power of the proposed framework by providing three types of regularizers on its basic ingredients, including model parameters, decision values, and latent projected points. With a set of regularizers derived from various priors, we not only recast most existing multiview learning methods into the proposed framework with properly chosen regularizers but also propose two novel models. To further improve the performance of the proposed framework, we propose to learn nonlinear transformations parameterized by deep networks. Extensive experiments are conducted on multiview datasets in terms of both feature extraction and cross-modal retrieval. Results show that the subspace-based learning for a common latent space is effective and its nonlinear extension can further boost performance, and more importantly, one of two proposed methods with nonlinear extension can achieve better results than all compared methods.},
  archive      = {J_TNNLS},
  author       = {Li Wang and Ren-Cang Li and Wen-Wei Lin},
  doi          = {10.1109/TNNLS.2021.3116784},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4371-4385},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview orthonormalized partial least squares: Regularizations and deep extensions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Late fusion multiple kernel clustering with proxy graph
refinement. <em>TNNLS</em>, <em>34</em>(8), 4359–4370. (<a
href="https://doi.org/10.1109/TNNLS.2021.3117403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel clustering (MKC) optimally utilizes a group of pre-specified base kernels to improve clustering performance. Among existing MKC algorithms, the recently proposed late fusion MKC methods demonstrate promising clustering performance in various applications and enjoy considerable computational acceleration. However, we observe that the kernel partition learning and late fusion processes are separated from each other in the existing mechanism, which may lead to suboptimal solutions and adversely affect the clustering performance. In this article, we propose a novel late fusion multiple kernel clustering with proxy graph refinement (LFMKC-PGR) framework to address these issues. First, we theoretically revisit the connection between late fusion kernel base partition and traditional spectral embedding. Based on this observation, we construct a proxy self-expressive graph from kernel base partitions. The proxy graph in return refines the individual kernel partitions and also captures partition relations in graph structure rather than simple linear transformation. We also provide theoretical connections and considerations between the proposed framework and the multiple kernel subspace clustering. An alternate algorithm with proved convergence is then developed to solve the resultant optimization problem. After that, extensive experiments are conducted on 12 multi-kernel benchmark datasets, and the results demonstrate the effectiveness of our proposed algorithm. The code of the proposed algorithm is publicly available at https://github.com/wangsiwei2010/graphlatefusion_MKC .},
  archive      = {J_TNNLS},
  author       = {Siwei Wang and Xinwang Liu and Li Liu and Sihang Zhou and En Zhu},
  doi          = {10.1109/TNNLS.2021.3117403},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4359-4370},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Late fusion multiple kernel clustering with proxy graph refinement},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quaternion factorization machines: A lightweight solution to
intricate feature interaction modeling. <em>TNNLS</em>, <em>34</em>(8),
4345–4358. (<a
href="https://doi.org/10.1109/TNNLS.2021.3118706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the sparsity of available features in web-scale predictive analytics, combinatorial features become a crucial means for deriving accurate predictions. As a well-established approach, a factorization machine (FM) is capable of automatically learning high-order interactions among features to make predictions without the need for manual feature engineering. With the prominent development of deep neural networks (DNNs), there is a recent and ongoing trend of enhancing the expressiveness of FM-based models with DNNs. However, though better results are obtained with DNN-based FM variants, such performance gain is paid off by an enormous amount (usually millions) of excessive model parameters on top of the plain FM. Consequently, the heavy parameterization impedes the real-life practicality of those deep models, especially efficient deployment on resource-constrained Internet of Things (IoT) and edge devices. In this article, we move beyond the traditional real space where most deep FM-based models are defined and seek solutions from quaternion representations within the hypercomplex space. Specifically, we propose the quaternion factorization machine (QFM) and quaternion neural factorization machine (QNFM), which are two novel lightweight and memory-efficient quaternion-valued models for sparse predictive analytics. By introducing a brand new take on FM-based models with the notion of quaternion algebra, our models not only enable expressive inter-component feature interactions but also significantly reduce the parameter size due to lower degrees of freedom in the hypercomplex Hamilton product compared with real-valued matrix multiplication. Extensive experimental results on three large-scale datasets demonstrate that QFM achieves 4.36\% performance improvement over the plain FM without introducing any extra parameters, while QNFM outperforms all baselines with up to two magnitudes’ parameter size reduction in comparison to state-of-the-art peer methods.},
  archive      = {J_TNNLS},
  author       = {Tong Chen and Hongzhi Yin and Xiangliang Zhang and Zi Huang and Yang Wang and Meng Wang},
  doi          = {10.1109/TNNLS.2021.3118706},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4345-4358},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quaternion factorization machines: A lightweight solution to intricate feature interaction modeling},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep deterministic policy gradient with compatible critic
network. <em>TNNLS</em>, <em>34</em>(8), 4332–4344. (<a
href="https://doi.org/10.1109/TNNLS.2021.3117790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep deterministic policy gradient (DDPG) is a powerful reinforcement learning algorithm for large-scale continuous controls. DDPG runs the back-propagation from the state-action value function to the actor network’s parameters directly, which raises a big challenge for the compatibility of the critic network. This compatibility emphasizes that the policy evaluation is compatible with the policy improvement. As proved in deterministic policy gradient, the compatible function guarantees the convergence ability but restricts the form of the critic network tightly. The complexities and limitations of the compatible function impede its development in DDPG. This article introduces neural networks’ similarity indices with gradients to measure the compatibility concretely. Represented as kernel matrices, we consider the actor network’s and the critic network’s training dataset, trained parameters, and gradients. With the sketching trick, the calculation time of the similarity index decreases hugely. The centered kernel alignment index and the normalized Bures similarity index provide us with consistent compatibility scores empirically. Moreover, we demonstrate the necessity of the compatible critic network in DDPG from three aspects: 1) analyzing the policy improvement/evaluation steps; 2) conducting the theoretic analysis; and 3) showing the experimental results. Following our research, we remodel the compatible function with an energy function model, enabling it suitable to the sizeable state-action space problem. The critic network has higher compatibility scores and better performance by introducing the policy change information into the critic-network optimization process. Besides, based on our experiment observations, we propose a light-computation overestimation solution. To prove our algorithm’s performance and validate the compatibility of the critic network, we compare our algorithm with six state-of-the-art algorithms using seven PyBullet robotics environments.},
  archive      = {J_TNNLS},
  author       = {Di Wang and Mengqi Hu},
  doi          = {10.1109/TNNLS.2021.3117790},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4332-4344},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep deterministic policy gradient with compatible critic network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empirical study on adaptive inference for pretrained
language model. <em>TNNLS</em>, <em>34</em>(8), 4321–4331. (<a
href="https://doi.org/10.1109/TNNLS.2021.3114188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive inference has been proven to improve bidirectional encoder representations from transformers (BERT)’s inference speed with minimal loss of accuracy. However, current work only focuses on the BERT model and lacks exploration of other pretrained language models (PLMs). Therefore, this article conducts an empirical study on the application of adaptive inference mechanism in various PLMs, including generative pretraining (GPT), GCNN, ALBERT, and TinyBERT. This mechanism is verified on both English and Chinese benchmarks, and experimental results demonstrated that it is able to speed up by a wide range from 1 to 10 times if given different speed thresholds. In addition, its application on ALBERT shows that adaptive inference can work with parameter sharing, achieving model compression and acceleration simultaneously, while the application on TinyBERT proves that it can further accelerate the distilled small model. As for the problem that too many labels make adaptive inference invalid, this article also proposes a solution, namely label reduction. Finally, this article open-sources an easy-to-use toolkit called FastPLM to help developers adopt pretrained models with adaptive inference capabilities in their applications.},
  archive      = {J_TNNLS},
  author       = {Weijie Liu and Xin Zhao and Zhe Zhao and Qi Ju and Xuefeng Yang and Wei Lu},
  doi          = {10.1109/TNNLS.2021.3114188},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4321-4331},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An empirical study on adaptive inference for pretrained language model},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Hopfield neural networks with delays driven by colored
noise. <em>TNNLS</em>, <em>34</em>(8), 4308–4320. (<a
href="https://doi.org/10.1109/TNNLS.2021.3117040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, Hopfield neural networks system with time-varying delays driven by nonlinear colored noise is introduced. The existence and globally exponential stability of stationary solutions are investigated for such random delay neural networks systems, which may be regarded as a generalization for the case of the constant equilibrium point in the literature. Moreover, the synchronization behavior of linearly coupled delay Hopfield neural networks driven by nonlinear colored noise is investigated at the level of the random attractor. Finally, illustrative examples and numerical simulations are provided to show the effectiveness of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Zhang Chen and Dandan Yang and Shitao Zhong},
  doi          = {10.1109/TNNLS.2021.3117040},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4308-4320},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hopfield neural networks with delays driven by colored noise},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SHNE: Semantics and homophily preserving network embedding.
<em>TNNLS</em>, <em>34</em>(8), 4296–4307. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have achieved great success in many applications and have caught significant attention in both academic and industrial domains. However, repeatedly employing graph convolutional layers would render the node embeddings indistinguishable. For the sake of avoiding oversmoothing, most GCN-based models are restricted in a shallow architecture. Therefore, the expressive power of these models is insufficient since they ignore information beyond local neighborhoods. Furthermore, existing methods either do not consider the semantics from high-order local structures or neglect the node homophily (i.e., node similarity), which severely limits the performance of the model. In this article, we take above problems into consideration and propose a novel Semantics and Homophily preserving Network Embedding (SHNE) model. In particular, SHNE leverages higher order connectivity patterns to capture structural semantics. To exploit node homophily, SHNE utilizes both structural and feature similarity to discover potential correlated neighbors for each node from the whole graph; thus, distant but informative nodes can also contribute to the model. Moreover, with the proposed dual-attention mechanisms, SHNE learns comprehensive embeddings with additional information from various semantic spaces. Furthermore, we also design a semantic regularizer to improve the quality of the combined representation. Extensive experiments demonstrate that SHNE outperforms state-of-the-art methods on benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Ziyang Zhang and Chuan Chen and Yaomin Chang and Weibo Hu and Xingxing Xing and Yuren Zhou and Zibin Zheng},
  doi          = {10.1109/TNNLS.2021.3116936},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4296-4307},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SHNE: Semantics and homophily preserving network embedding},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust and collision-free formation control of multiagent
systems with limited information. <em>TNNLS</em>, <em>34</em>(8),
4286–4295. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the collision-free cooperative formation control problem for second-order multiagent systems with unknown velocity, dynamics uncertainties, and limited reference information. An observer-based sliding mode control law is proposed to ensure both the convergence of the system’s tracking error and the boundedness of the relative distance between each pair of agents. First, two new finite-time neural-based observer designs are introduced to estimate both the agent velocity and the system uncertainty. The sliding mode differentiator is then employed for every agent to approximate the unknown derivatives of the formation reference to further construct the limited-information-based sliding mode controller. To ensure that the system is collision-free, artificial potential fields are introduced along with a time-varying topology. An example of a multiple omnidirectional robot system is used to conduct numerical simulations, and necessary comparisons are made to justify the effectiveness of the proposed limited-information-based control scheme.},
  archive      = {J_TNNLS},
  author       = {Yang Fei and Peng Shi and Cheng-Chew Lim},
  doi          = {10.1109/TNNLS.2021.3112679},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4286-4295},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust and collision-free formation control of multiagent systems with limited information},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Learning hierarchical document graphs from multilevel
sentence relations. <em>TNNLS</em>, <em>34</em>(8), 4273–4285. (<a
href="https://doi.org/10.1109/TNNLS.2021.3113297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizing the implicit topology of a document as a graph, and further performing feature extraction via the graph convolutional network (GCN), has proven effective in document analysis. However, existing document graphs are often restricted to expressing single-level relations, which are predefined and independent of downstream learning. A set of learnable hierarchical graphs are built to explore multilevel sentence relations, assisted by a hierarchical probabilistic topic model. Based on these graphs, multiple parallel GCNs are used to extract multilevel semantic features, which are aggregated by an attention mechanism for different document-comprehension tasks. Equipped with variational inference, the graph construction and GCN are learned jointly, allowing the graphs to evolve dynamically to better match the downstream task. The effectiveness and efficiency of the proposed multilevel sentence relation graph convolutional network (MuserGCN) is demonstrated via experiments on document classification, abstractive summarization, and matching.},
  archive      = {J_TNNLS},
  author       = {Hao Zhang and Chaojie Wang and Zhengjue Wang and Zhibin Duan and Bo Chen and Mingyuan Zhou and Ricardo Henao and Lawrence Carin},
  doi          = {10.1109/TNNLS.2021.3113297},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4273-4285},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning hierarchical document graphs from multilevel sentence relations},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vibration suppression of a high-rise building with adaptive
iterative learning control. <em>TNNLS</em>, <em>34</em>(8), 4261–4272.
(<a href="https://doi.org/10.1109/TNNLS.2021.3120838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the design of an adaptive iterative learning controller for high-rise buildings with active mass dampers (AMDs). High-rise buildings in this article are seen as distributed parameter systems, in which the characteristics of every point in buildings should be considered. Two partial differential equations (PDEs) and several ordinary differential equations are used to describe the model of buildings. To achieve the control target that is to suppress the vibration induced by high winds, an adaptive iterative learning controller is proposed for the flexible building system with boundary disturbance. The convergency of the adaptive iterative learning control (AILC) approach is proven by serious theory analysis. In simulations and experiments, this article uses both the analysis of figures and quantitative analysis (root-mean-square values) to illustrate the efficiency of the AILC scheme.},
  archive      = {J_TNNLS},
  author       = {Jiali Feng and Zhijie Liu and Xiuyu He and Qing Li and Wei He},
  doi          = {10.1109/TNNLS.2021.3120838},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4261-4272},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Vibration suppression of a high-rise building with adaptive iterative learning control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning for load shedding against
short-term voltage instability in large power systems. <em>TNNLS</em>,
<em>34</em>(8), 4249–4260. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an innovative solution approach to the challenging dynamic load-shedding problem which directly affects the stability of large power grid. Our proposed deep Q-network for load-shedding (DQN-LS) determines optimal load-shedding strategy to maintain power system stability by taking into account both spatial and temporal information of a dynamically operating power system, using a convolutional long-short-term memory (ConvLSTM) network to automatically capture dynamic features that are translation-invariant in short-term voltage instability, and by introducing a new design of the reward function. The overall goal for the proposed DQN-LS is to provide real-time, fast, and accurate load-shedding decisions to increase the quality and probability of voltage recovery. To demonstrate the efficacy of our proposed approach and its scalability to large-scale, complex dynamic problems, we utilize the China Southern Grid (CSG) to obtain our test results, which clearly show superior voltage recovery performance by employing the proposed DQN-LS under different and uncertain power system fault conditions. What we have developed and demonstrated in this study, in terms of the scale of the problem, the load-shedding performance obtained, and the DQN-LS approach, have not been demonstrated previously.},
  archive      = {J_TNNLS},
  author       = {Jingyi Zhang and Yonghong Luo and Boya Wang and Chao Lu and Jennie Si and Jie Song},
  doi          = {10.1109/TNNLS.2021.3121757},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4249-4260},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep reinforcement learning for load shedding against short-term voltage instability in large power systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuro-optimal trajectory tracking with value iteration of
discrete-time nonlinear dynamics. <em>TNNLS</em>, <em>34</em>(8),
4237–4248. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel neuro-optimal tracking control approach is developed toward discrete-time nonlinear systems. By constructing a new augmented plant, the optimal trajectory tracking design is transformed into an optimal regulation problem. For discrete-time nonlinear dynamics, the steady control input corresponding to the reference trajectory is given. Then, the value-iteration-based tracking control algorithm is provided and the convergence of the value function sequence is established. Therein, the approximation error between the iterative value function and the optimal cost is estimated. The uniformly ultimately bounded stability of the closed-loop system is also discussed in detail. Moreover, the iterative heuristic dynamic programming (HDP) algorithm is implemented by involving the critic and action components, where some new updating rules of the action network are provided. Finally, two examples are used to demonstrate the optimality of the present controller as well as the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Ding Wang and Mingming Ha and Long Cheng},
  doi          = {10.1109/TNNLS.2021.3123444},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4237-4248},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuro-optimal trajectory tracking with value iteration of discrete-time nonlinear dynamics},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implicit weight learning for multi-view clustering.
<em>TNNLS</em>, <em>34</em>(8), 4223–4236. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting different representations, or views, of the same object for better clustering has become very popular these days, which is conventionally called multi-view clustering. In general, it is essential to measure the importance of each individual view, due to some noises, or inherent capacities in the description. Many previous works model the view importance as weight, which is simple but effective empirically. In this article, instead of following the traditional thoughts, we propose a new weight learning paradigm in the context of multi-view clustering in virtue of the idea of the reweighted approach, and we theoretically analyze its working mechanism. Meanwhile, as a carefully achieved example, all of the views are connected by exploring a unified Laplacian rank constrained graph, which will be a representative method to compare with other weight learning approaches in experiments. Furthermore, the proposed weight learning strategy is much suitable for multi-view data, and it can be naturally integrated with many existing clustering learners. According to the numerical experiments, the proposed implicit weight learning approach is proven effective and practical to use in multi-view clustering.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Shaojun Shi and Jing Li and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3121246},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4223-4236},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Implicit weight learning for multi-view clustering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Efficient sparse representation for learning with
high-dimensional data. <em>TNNLS</em>, <em>34</em>(8), 4208–4222. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the capability of effectively learning intrinsic structures from high-dimensional data, techniques based on sparse representation have begun to display an impressive impact on several fields, such as image processing, computer vision, and pattern recognition. Learning sparse representations isoften computationally expensive due to the iterative computations needed to solve convex optimization problems in which the number of iterations is unknown before convergence. Moreover, most sparse representation algorithms focus only on determining the final sparse representation results and ignore the changes in the sparsity ratio (SR) during iterative computations. In this article, two algorithms are proposed to learn sparse representations based on locality-constrained linear representation learning with probabilistic simplex constraints. Specifically, the first algorithm, called approximated local linear representation (ALLR), obtains a closed-form solution from individual locality-constrained sparse representations. The second algorithm, called ALLR with symmetric constraints (ALLRSC), further obtains a symmetric sparse representation result with a limited number of computations; notably, the sparsity and convergence of sparse representations can be guaranteed based on theoretical analysis. The steady decline in the SR during iterative computations is a critical factor in practical applications. Experimental results based on public datasets demonstrate that the proposed algorithms perform better than several state-of-the-art algorithms for learning with high-dimensional data.},
  archive      = {J_TNNLS},
  author       = {Jie Chen and Shengxiang Yang and Zhu Wang and Hua Mao},
  doi          = {10.1109/TNNLS.2021.3119278},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4208-4222},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient sparse representation for learning with high-dimensional data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Limbic system-inspired performance-guaranteed control for
nonlinear multi-agent systems with uncertainties. <em>TNNLS</em>,
<em>34</em>(8), 4196–4207. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a performance-guaranteed limbic system-inspired control (LISIC) strategy for nonlinear multi-agent systems (MASs) with uncertain high-order dynamics and external perturbations, where each agent in the MAS incorporates a LISIC structure to support the consensus controller. This novel approach, which we call double integrator LISIC (DILISIC), is designed to imitate double integrator dynamics after closing the agent-specific control loop, allowing the control designer to apply consensus techniques specifically formulated for double integrator agents. The objective of each DILISIC structure is then to identify and compensate model differences between the theoretical assumptions considered when tuning the consensus protocol and the actual conditions encountered in the real-time system to be controlled. A Lyapunov analysis is provided to demonstrate the stability of the closed-loop MAS enhanced with the DILISIC. Additionally, the stabilization of a complex system via DILISIC is addressed in a synthetic scenario: the consensus control of a team of flexible single-link arms. The dynamics of these agents are of fourth order, contain uncertainties, and are subject to external perturbations. The numerical results validate the applicability of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Ignacio Rubio Scola and Luis Rodolfo Garcia Carrillo and João P. Hespanha},
  doi          = {10.1109/TNNLS.2021.3121232},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4196-4207},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Limbic system-inspired performance-guaranteed control for nonlinear multi-agent systems with uncertainties},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning smooth representation for unsupervised domain
adaptation. <em>TNNLS</em>, <em>34</em>(8), 4181–4195. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typical adversarial-training-based unsupervised domain adaptation (UDA) methods are vulnerable when the source and target datasets are highly complex or exhibit a large discrepancy between their data distributions. Recently, several Lipschitz-constraint-based methods have been explored. The satisfaction of Lipschitz continuity guarantees a remarkable performance on a target domain. However, they lack a mathematical analysis of why a Lipschitz constraint is beneficial to UDA and usually perform poorly on large-scale datasets. In this article, we take the principle of utilizing a Lipschitz constraint further by discussing how it affects the error bound of UDA. A connection between them is built, and an illustration of how Lipschitzness reduces the error bound is presented. A local smooth discrepancy is defined to measure the Lipschitzness of a target distribution in a pointwise way. When constructing a deep end-to-end model, to ensure the effectiveness and stability of UDA, three critical factors are considered in our proposed optimization strategy, i.e., the sample amount of a target domain, dimension, and batchsize of samples. Experimental results demonstrate that our model performs well on several standard benchmarks. Our ablation study shows that the sample amount of a target domain, the dimension, and batchsize of samples, indeed, greatly impact Lipschitz-constraint-based methods’ ability to handle large-scale datasets. Code is available at https://github.com/CuthbertCai/SRDA .},
  archive      = {J_TNNLS},
  author       = {Guanyu Cai and Lianghua He and Mengchu Zhou and Hesham Alhumade and Die Hu},
  doi          = {10.1109/TNNLS.2021.3119889},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4181-4195},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning smooth representation for unsupervised domain adaptation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Extremely sparse networks via binary augmented pruning for
fast image classification. <em>TNNLS</em>, <em>34</em>(8), 4167–4180.
(<a href="https://doi.org/10.1109/TNNLS.2021.3120409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network pruning and binarization have been demonstrated to be effective in neural network accelerator design for high speed and energy efficiency. However, most existing pruning approaches achieve a poor tradeoff between accuracy and efficiency, which on the other hand, has limited the progress of neural network accelerators. At the same time, binary networks are highly efficient, however, a large accuracy gap exists between binary networks and their full-precision counterparts. In this article, we investigate the merits of extremely sparse networks with binary connections for image classification through software-hardware codesign. More specifically, we first propose a binary augmented extremely pruning method that can achieve ~98\% sparsity with small accuracy degradation. Then we design the hardware architecture based on the resulting sparse and binary networks, which extensively explores the benefits of extreme sparsity with negligible resource consumption introduced by binary branch. Experiments on large-scale ImageNet classification and field-programmable gate array (FPGA) demonstrate that the proposed software-hardware architecture can achieve a prominent tradeoff between accuracy and efficiency.},
  archive      = {J_TNNLS},
  author       = {Peisong Wang and Fanrong Li and Gang Li and Jian Cheng},
  doi          = {10.1109/TNNLS.2021.3120409},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4167-4180},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Extremely sparse networks via binary augmented pruning for fast image classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HIN-RNN: A graph representation learning neural network for
fraudster group detection with no handcrafted features. <em>TNNLS</em>,
<em>34</em>(8), 4153–4166. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social reviews are indispensable resources for modern consumers’ decision making. To influence the reviews, for financial gains, some companies may choose to pay groups of fraudsters rather than individuals to demote or promote products and services. This is because consumers are more likely to be misled by a large amount of similar reviews, produced by a group of fraudsters. Semantic relation such as content similarity (CS) and polarity similarity is an important factor characterizing solicited group frauds. Recent approaches on fraudster group detection employed handcrafted features of group behaviors that failed to capture the semantic relation of review text from the reviewers. In this article, we propose the first neural approach, HIN-RNN, a heterogeneous information network (HIN) compatible recurrent neural network (RNN) for fraudster group detection that makes use of semantic similarity and requires no handcrafted features. The HIN-RNN provides a unifying architecture for representation learning of each reviewer, with the initial vector as the sum of word embeddings (SoWEs) of all review text written by the same reviewer, concatenated by the ratio of negative reviews. Given a co-review network representing reviewers who have reviewed the same items with similar ratings and the reviewers’ vector representation, a collaboration matrix is captured through the HIN-RNN training. The proposed approach is demonstrated to be effective with marked improvement over state-of-the-art approaches on both the Yelp (22\% and 12\% in terms of recall and F1-value, respectively) and Amazon (4\% and 2\% in terms of recall and F1-value, respectively) datasets.},
  archive      = {J_TNNLS},
  author       = {Saeedreza Shehnepoor and Roberto Togneri and Wei Liu and Mohammed Bennamoun},
  doi          = {10.1109/TNNLS.2021.3123876},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4153-4166},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HIN-RNN: A graph representation learning neural network for fraudster group detection with no handcrafted features},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global dissipativity and quasi-mittag–leffler
synchronization of fractional-order discontinuous complex-valued neural
networks. <em>TNNLS</em>, <em>34</em>(8), 4139–4152. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with fractional-order discontinuous complex-valued neural networks (FODCNNs). Based on a new fractional-order inequality, such system is analyzed as a compact entirety without any decomposition in the complex domain which is different from a common method in almost all literature. First, the existence of global Filippov solution is given in the complex domain on the basis of the theories of vector norm and fractional calculus. Successively, by virtue of the nonsmooth analysis and differential inclusion theory, some sufficient conditions are developed to guarantee the global dissipativity and quasi-Mittag–Leffler synchronization of FODCNNs. Furthermore, the error bounds of quasi-Mittag–Leffler synchronization are estimated without reference to the initial values. Especially, our results include some existing integer-order and fractional-order ones as special cases. Finally, numerical examples are given to show the effectiveness of the obtained theories.},
  archive      = {J_TNNLS},
  author       = {Zhixia Ding and Hao Zhang and Zhigang Zeng and Le Yang and Sai Li},
  doi          = {10.1109/TNNLS.2021.3119647},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4139-4152},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global dissipativity and quasi-Mittag–Leffler synchronization of fractional-order discontinuous complex-valued neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Initialization-based k-winners-take-all neural network model
using modified gradient descent. <em>TNNLS</em>, <em>34</em>(8),
4130–4138. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The $k$ -winners-take-all ( $k$ -WTA) problem refers to the selection of $k$ winners with the first $k$ largest inputs over a group of $n$ neurons, where each neuron has an input. In existing $k$ -WTA neural network models, the positive integer $k$ is explicitly given in the corresponding mathematical models. In this article, we consider another case where the number $k$ in the $k$ -WTA problem is implicitly specified by the initial states of the neurons. Based on the constraint conversion for a classical optimization problem formulation of the $k$ -WTA, via modifying the traditional gradient descent, we propose an initialization-based $k$ -WTA neural network model with only $n$ neurons for $n$ -dimensional inputs, and the dynamics of the neural network model is described by parameterized gradient descent. Theoretical results show that the state vector of the proposed $k$ -WTA neural network model globally asymptotically converges to the theoretical $k$ -WTA solution under mild conditions. Simulative examples demonstrate the effectiveness of the proposed model and indicate that its convergence can be accelerated by readily setting two design parameters.},
  archive      = {J_TNNLS},
  author       = {Yinyan Zhang and Shuai Li and Guanggang Geng},
  doi          = {10.1109/TNNLS.2021.3123240},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4130-4138},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Initialization-based k-winners-take-all neural network model using modified gradient descent},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resilient model free adaptive distributed LFC for multi-area
power systems against jamming attacks. <em>TNNLS</em>, <em>34</em>(8),
4120–4129. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with distributed resilient load frequency control (LFC) for multi-area power interconnection systems against jamming attacks. First, considering uncertainties and high dimension nonlinearity, the model-free adaptive control (MFAC) model is adopted for the power system, in which only input and output (I/O) data are used. Second, jamming attacks are modeled in a stochastic process, and a multistep predictive compensation algorithm is developed to mitigate the impact of jamming attacks. Then, the distributed MFAC protocol with predictive compensation algorithm is designed such that the frequency tracking errors under the predictive compensation algorithm of multi-area power interconnection systems converge consensually into a small neighborhood of origin in the mean square sense. Simulation results show the effectiveness of the approach.},
  archive      = {J_TNNLS},
  author       = {Xiaojie Qiu and Yingchun Wang and Huaguang Zhang and Xiangpeng Xie},
  doi          = {10.1109/TNNLS.2021.3123235},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4120-4129},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Resilient model free adaptive distributed LFC for multi-area power systems against jamming attacks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gradient monitored reinforcement learning. <em>TNNLS</em>,
<em>34</em>(8), 4106–4119. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel neural network training approach for faster convergence and better generalization abilities in deep reinforcement learning (RL). Particularly, we focus on the enhancement of training and evaluation performance in RL algorithms by systematically reducing gradient’s variance and, thereby, providing a more targeted learning process. The proposed method, which we term gradient monitoring (GM), is a method to steer the learning in the weight parameters of a neural network based on the dynamic development and feedback from the training process itself. We propose different variants of the GM method that we prove to increase the underlying performance of the model. One of the proposed variants, momentum with GM (M-WGM), allows for a continuous adjustment of the quantum of backpropagated gradients in the network based on certain learning parameters. We further enhance the method with the adaptive M-WGM (AM-WGM) method, which allows for automatic adjustment between focused learning of certain weights versus more dispersed learning depending on the feedback from the rewards collected. As a by-product, it also allows for automatic derivation of the required deep network sizes during training as the method automatically freezes trained weights. The method is applied to two discrete (real-world multirobot coordination problems and Atari games) and one continuous control task (MuJoCo) using advantage actor–critic (A2C) and proximal policy optimization (PPO), respectively. The results obtained particularly underline the applicability and performance improvements of the methods in terms of generalization capability.},
  archive      = {J_TNNLS},
  author       = {Mohammed Sharafath Abdul Hameed and Gavneet Singh Chadha and Andreas Schwung and Steven X. Ding},
  doi          = {10.1109/TNNLS.2021.3119853},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4106-4119},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gradient monitored reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust similarity measurement based on a novel time filter
for SSVEPs detection. <em>TNNLS</em>, <em>34</em>(8), 4096–4105. (<a
href="https://doi.org/10.1109/TNNLS.2021.3118468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The steady-state visual evoked potential (SSVEP)-based brain-computer interface (BCI) has received extensive attention in research for the less training time, excellent recognition performance, and high information translate rate. At present, most of the powerful SSVEPs detection methods are similarity measurements based on spatial filters and Pearson’s correlation coefficient. Among them, the task-related component analysis (TRCA)-based method and its variant, the ensemble TRCA (eTRCA)-based method, are two methods with high performance and great potential. However, they have a defect, that is, they can only suppress certain kinds of noise, but not more general noises. To solve this problem, a novel time filter was designed by introducing the temporally local weighting into the objective function of the TRCA-based method and using the singular value decomposition. Based on this, the time filter and (e)TRCA-based similarity measurement methods were proposed, which can perform a robust similarity measure to enhance the detection ability of SSVEPs. A benchmark dataset recorded from 35 subjects was used to evaluate the proposed methods and compare them with the (e)TRCA-based methods. The results indicated that the proposed methods performed significantly better than the (e)TRCA-based methods. Therefore, it is believed that the proposed time filter and the similarity measurement methods have promising potential for SSVEPs detection.},
  archive      = {J_TNNLS},
  author       = {Jing Jin and Zhiqiang Wang and Ren Xu and Chang Liu and Xingyu Wang and Andrzej Cichocki},
  doi          = {10.1109/TNNLS.2021.3118468},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4096-4105},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust similarity measurement based on a novel time filter for SSVEPs detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal optimal transport for treatment effect estimation.
<em>TNNLS</em>, <em>34</em>(8), 4083–4095. (<a
href="https://doi.org/10.1109/TNNLS.2021.3118542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment effect estimation helps answer questions, such as whether a specific treatment affects the outcome of interest. One fundamental issue in this research is to alleviate the treatment assignment bias among those treated units and controlled units. Classical causal inference methods resort to the propensity score estimation, which unfortunately tends to be misspecified when only limited overlapping exists between the treated and the controlled units. Moreover, existing supervised methods mainly consider the treatment assignment information underlying the factual space, and thus, their performance of counterfactual inference may be degraded due to overfitting of the factual results. To alleviate those issues, we build on the optimal transport theory and propose a novel causal optimal transport (CausalOT) model to estimate an individual treatment effect (ITE). With the proposed propensity measure, CausalOT can infer the counterfactual outcome by solving a novel regularized optimal transport problem, which allows the utilization of global information on observational covariates to alleviate the issue of limited overlapping. In addition, a novel counterfactual loss is designed for CausalOT to align the factual outcome distribution with the counterfactual outcome distribution. Most importantly, we prove the theoretical generalization bound for the counterfactual error of CausalOT. Empirical studies on benchmark datasets confirm that the proposed CausalOT outperforms state-of-the-art causal inference methods.},
  archive      = {J_TNNLS},
  author       = {Qian Li and Zhichao Wang and Shaowu Liu and Gang Li and Guandong Xu},
  doi          = {10.1109/TNNLS.2021.3118542},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4083-4095},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Causal optimal transport for treatment effect estimation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fusion of centroid-based clustering with graph clustering:
An expectation-maximization-based hybrid clustering. <em>TNNLS</em>,
<em>34</em>(8), 4068–4082. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article extends the expectation-maximization (EM) formulation for the Gaussian mixture model (GMM) with a novel weighted dissimilarity loss. This extension results in the fusion of two different clustering methods, namely, centroid-based clustering and graph clustering in the same framework in order to leverage their advantages. The fusion of centroid-based clustering and graph clustering results in a simple “soft” asynchronous hybrid clustering method. The proposed algorithm may start as a pure centroid-based clustering algorithm (e.g., $k$ -means), and as the time evolves, it may eventually and gradually turn into a pure graph clustering algorithm [e.g., basic greedy asynchronous distributed interference avoidance (GADIA) (Babadi and Tarokh, 2010)] as the algorithm converges and vice versa. The “hard” version of the proposed hybrid algorithm includes the standard Hopfield neural networks (and, thus, Bruck’s Ln algorithm by (Bruck, 1990) and the Ising model in statistical mechanics), Babadi and Tarokh’s basic GADIA in 2010, and the standard ${k}$ -means (Steinhaus, 1956), (MacQueen, 1967) [i.e., the Lloyd algorithm (Lloyd, 1957, 1982)] as its special cases. We call the “hard version” of the proposed clustering as “hybrid–nongreedy asynchronous clustering (H-NAC).” We apply the H-NAC to various clustering problems using well-known benchmark datasets. The computer simulations confirm the superior performance of the H-NAC compared to the $k$ -means clustering, $k$ -GADIA, spectral clustering, and a very recent clustering algorithm structured graph learning (SGL) by Kang et al. (2021), which represents one of the state-of-the-art clustering algorithms.},
  archive      = {J_TNNLS},
  author       = {Zekeriya Uykan},
  doi          = {10.1109/TNNLS.2021.3121224},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4068-4082},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fusion of centroid-based clustering with graph clustering: An expectation-maximization-based hybrid clustering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural-network-based adaptive constrained control for
switched systems under state-dependent switching law. <em>TNNLS</em>,
<em>34</em>(8), 4057–4067. (<a
href="https://doi.org/10.1109/TNNLS.2021.3120999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the adaptive tracking control problem for switched uncertain nonlinear systems with state constraints via the multiple Lyapunov function approach. The system functions are considered unknown and approximated by radial basis function neural networks (RBFNNs). For the state constraint problem, the barrier Lyapunov functions (BLFs) are chosen to ensure the satisfaction of the constrained properties. Moreover, a state-dependent switching law is designed, which does not require stability for individual subsystems. Then, using the backstepping technique, an adaptive NN controller is constructed such that all signals in the resulting system are bounded, the system output can track the reference signal to a compact set, and the constraint conditions for states are not violated under the designed state-dependent switching signal. Finally, simulation results show the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Li Tang and Xin-Yu Zhang and Yan-Jun Liu and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2021.3120999},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4057-4067},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based adaptive constrained control for switched systems under state-dependent switching law},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metaparametric neural networks for survival analysis.
<em>TNNLS</em>, <em>34</em>(8), 4047–4056. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis is a critical tool for the modeling of time-to-event data, such as life expectancy after a cancer diagnosis or optimal maintenance scheduling for complex machinery. However, current neural network models provide an imperfect solution for survival analysis as they either restrict the shape of the target probability distribution or restrict the estimation to predetermined times. As a consequence, current survival neural networks lack the ability to estimate a generic function without prior knowledge of its structure. In this article, we present the metaparametric neural network framework that encompasses the existing survival analysis methods and enables their extension to solve the aforementioned issues. This framework allows survival neural networks to satisfy the same independence of generic function estimation from the underlying data structure that characterizes their regression and classification counterparts. Furthermore, we demonstrate the application of the metaparametric framework using both simulated and large real-world datasets and show that it outperforms the current state-of-the-art methods in: 1) capturing nonlinearities and 2) identifying temporal patterns, leading to more accurate overall estimations while placing no restrictions on the underlying function structure.},
  archive      = {J_TNNLS},
  author       = {Fabio Luis De Mello and J. Mark Wilkinson and Visakan Kadirkamanathan},
  doi          = {10.1109/TNNLS.2021.3119510},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4047-4056},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Metaparametric neural networks for survival analysis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploration with task information for meta reinforcement
learning. <em>TNNLS</em>, <em>34</em>(8), 4033–4046. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta reinforcement learning (meta-RL) is a promising technique for fast task adaptation by leveraging prior knowledge from previous tasks. Recently, context-based meta-RL has been proposed to improve data efficiency by applying a principled framework, dividing the learning procedure into task inference and task execution. However, the task information is not adequately leveraged in this approach, thus leading to inefficient exploration. To address this problem, we propose a novel context-based meta-RL framework with an improved exploration mechanism. For the existing exploration and execution problem in context-based meta-RL, we propose a novel objective that employs two exploration terms to encourage better exploration in action and task embedding space, respectively. The first term pushes for improving the diversity of task inference, while the second term, named action information, works as sharing or hiding task information in different exploration stages. We divide the meta-training procedure into task-independent exploration and task-relevant exploration stages according to the utilization of action information. By decoupling task inference and task execution and proposing the respective optimization objectives in the two exploration stages, we can efficiently learn policy and task inference networks. We compare our algorithm with several popular meta-RL methods on MuJoco benchmarks with both dense and sparse reward settings. The empirical results show that our method significantly outperforms baselines on the benchmarks in terms of sample efficiency and task performance.},
  archive      = {J_TNNLS},
  author       = {Peng Jiang and Shiji Song and Gao Huang},
  doi          = {10.1109/TNNLS.2021.3121432},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4033-4046},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploration with task information for meta reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel multistage wide neural network. <em>TNNLS</em>,
<em>34</em>(8), 4019–4032. (<a
href="https://doi.org/10.1109/TNNLS.2021.3120331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning networks have achieved great success in many areas, such as in large-scale image processing. They usually need large computing resources and time and process easy and hard samples inefficiently in the same way. Another undesirable problem is that the network generally needs to be retrained to learn new incoming data. Efforts have been made to reduce the computing resources and realize incremental learning by adjusting architectures, such as scalable effort classifiers, multi-grained cascade forest (gcForest), conditional deep learning (CDL), tree CNN, decision tree structure with knowledge transfer (ERDK), forest of decision trees with radial basis function (RBF) networks, and knowledge transfer (FDRK). In this article, a parallel multistage wide neural network (PMWNN) is presented. It is composed of multiple stages to classify different parts of data. First, a wide radial basis function (WRBF) network is designed to learn features efficiently in the wide direction. It can work on both vector and image instances and can be trained in one epoch using subsampling and least squares (LS). Second, successive stages of WRBF networks are combined to make up the PMWNN. Each stage focuses on the misclassified samples of the previous stage. It can stop growing at an early stage, and a stage can be added incrementally when new training data are acquired. Finally, the stages of the PMWNN can be tested in parallel, thus speeding up the testing process. To sum up, the proposed PMWNN network has the advantages of: 1) optimized computing resources; 2) incremental learning; and 3) parallel testing with stages. The experimental results with the MNIST data, a number of large hyperspectral remote sensing data, and different types of data in different application areas, including many image and nonimage datasets, show that the WRBF and PMWNN can work well on both image and nonimage data and have very competitive accuracy compared to learning models, such as stacked autoencoders, deep belief nets, support vector machine (SVM), multilayer perceptron (MLP), LeNet-5, RBF network, recently proposed CDL, broad learning, gcForest, ERDK, and FDRK.},
  archive      = {J_TNNLS},
  author       = {Jiangbo Xi and Okan K. Ersoy and Jianwu Fang and Tianjun Wu and Xin Wei and Chaoying Zhao},
  doi          = {10.1109/TNNLS.2021.3120331},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4019-4032},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parallel multistage wide neural network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cluster synchronization of multiple fractional-order
recurrent neural networks with time-varying delays. <em>TNNLS</em>,
<em>34</em>(8), 4007–4018. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the cluster synchronization of multiple fractional-order recurrent neural networks (FNNs) with time-varying delays. Sufficient criteria are deduced for realizing cluster synchronization of multiple FNNs via a pinning control by applying an extended Halanay inequality applicable for time-delayed fractional-order differential equations. Moreover, an adaptive control applicable for the synchronization of fractional-order systems with time-varying delays is proposed, under which sufficient criteria are derived for realizing cluster synchronization of multiple FNNs with time-varying delays. Finally, two examples are presented to illustrate the effectiveness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Peng Liu and Minglin Xu and Junwei Sun and Shiping Wen},
  doi          = {10.1109/TNNLS.2021.3121516},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4007-4018},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cluster synchronization of multiple fractional-order recurrent neural networks with time-varying delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Intermittent event-triggered optimal leader-following
consensus for nonlinear multi-agent systems via actor-critic algorithm.
<em>TNNLS</em>, <em>34</em>(8), 3992–4006. (<a
href="https://doi.org/10.1109/TNNLS.2021.3122458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the intermittent event-triggered optimal leader-following consensus for nonlinear multi-agent systems (MASs) utilizing the actor-critic algorithm. First, we propose a novel distributed intermittent event-triggered control strategy, and a sufficient criterion is obtained to guarantee the leader-following consensus of MASs by establishing a novel piecewise differential inequality. Next, the intermittent event-triggered optimal control strategy is delicately given. Remarkably, the optimality of MASs is proven based on policy iteration and the convergence of the closed-loop system is also proved based on the Lyapunov stability theory. Then, the intermittent event-triggered approximate optimal control strategy is designed via an actor-critic network whose weights are only updated at the trigger instants. Furthermore, the Zeno behavior can be excluded in this article. Finally, two simulation examples further verify the effectiveness of the proposed scheme.},
  archive      = {J_TNNLS},
  author       = {Chen Liu and Lei Liu and Jinde Cao and Mahmoud Abdel-Aty},
  doi          = {10.1109/TNNLS.2021.3122458},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3992-4006},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intermittent event-triggered optimal leader-following consensus for nonlinear multi-agent systems via actor-critic algorithm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed fault-tolerant containment control protocols for
the discrete-time multiagent systems via reinforcement learning method.
<em>TNNLS</em>, <em>34</em>(8), 3979–3991. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the model-free fault-tolerant containment control problem for multiagent systems (MASs) with time-varying actuator faults. Depending on the relative state information of neighbors, a distributed containment control method based on reinforcement learning (RL) is adopted to achieve containment control objective without prior knowledge on the system dynamics. First, based on the information of agent itself and its neighbors, a containment error system is established. Then, the optimal containment control problem is transformed into an optimal regulation problem for the containment error system. Furthermore, the RL-based policy iteration method is employed to deal with the corresponding optimal regulation problem, and the nominal controller is proposed for the original fault-free system. Based on the nominal controller, a fault-tolerant controller is further developed to compensate for the influence of actuator faults on MAS. Meanwhile, the uniform boundedness of the containment errors can be guaranteed by using the presented control scheme. Finally, numerical simulations are given to show the effectiveness and advantages of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Tieshan Li and Weiwei Bai and Qi Liu and Yue Long and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3121403},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3979-3991},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed fault-tolerant containment control protocols for the discrete-time multiagent systems via reinforcement learning method},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered communication network with limited-bandwidth
constraint for multi-agent reinforcement learning. <em>TNNLS</em>,
<em>34</em>(8), 3966–3978. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communicating agents with each other in a distributed manner and behaving as a group are essential in multi-agent reinforcement learning. However, real-world multi-agent systems suffer from restrictions on limited bandwidth communication. If the bandwidth is fully occupied, some agents are not able to send messages promptly to others, causing decision delay and impairing cooperative effects. Recent related work has started to address the problem but still fails in maximally reducing the consumption of communication resources. In this article, we propose an event-triggered communication network (ETCNet) to enhance communication efficiency in multi-agent systems by communicating only when necessary. For different task requirements, two paradigms of the ETCNet framework, event-triggered sending network (ETSNet) and event-triggered receiving network (ETRNet), are proposed for learning efficient sending and receiving protocols, respectively. Leveraging the information theory, the limited bandwidth is translated to the penalty threshold of an event-triggered strategy, which determines whether an agent at each step participates in communication or not. Then, the design of the event-triggered strategy is formulated as a constrained Markov decision problem and reinforcement learning finds the feasible and optimal communication protocol that satisfies the limited bandwidth constraint. Experiments on typical multi-agent tasks demonstrate that ETCNet outperforms other methods in reducing bandwidth occupancy and still preserves the cooperative performance of multi-agent systems at the most.},
  archive      = {J_TNNLS},
  author       = {Guangzheng Hu and Yuanheng Zhu and Dongbin Zhao and Mengchen Zhao and Jianye Hao},
  doi          = {10.1109/TNNLS.2021.3121546},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3966-3978},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered communication network with limited-bandwidth constraint for multi-agent reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning data streams with changing distributions and
temporal dependency. <em>TNNLS</em>, <em>34</em>(8), 3952–3965. (<a
href="https://doi.org/10.1109/TNNLS.2021.3122531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a data stream, concept drift refers to unpredictable distribution changes over time, which violates the identical-distribution assumption required by conventional machine learning methods. Current concept drift adaptation techniques mostly focus on a data stream with changing distributions. However, since each variable of a data stream is a time series, these variables normally have temporal dependency problems in the real world. How to solve concept drift and temporal dependency problems at the same time is rarely discussed in the concept-drift literature. To solve this situation, this article proves and validates that the testing error decreases faster if a predictor is trained on a temporally reconstructed space when drift occurs. Based on this theory, a novel drift adaptation regression (DAR) framework is designed to predict the label variable for data streams with concept drift and temporal dependency. A new statistic called local drift degree (LDD+) is proposed and used as a drift adaptation technique in the DAR framework to discard outdated instances in a timely way, thereby guaranteeing that the most relevant instances will be selected during the training process. The performance of DAR is demonstrated by a set of experimental evaluations on both synthetic data and real-world data streams.},
  archive      = {J_TNNLS},
  author       = {Yiliao Song and Jie Lu and Haiyan Lu and Guangquan Zhang},
  doi          = {10.1109/TNNLS.2021.3122531},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3952-3965},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning data streams with changing distributions and temporal dependency},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mode-dependent adaptive event-triggered control for
stabilization of markovian memristor-based reaction–diffusion neural
networks. <em>TNNLS</em>, <em>34</em>(8), 3939–3951. (<a
href="https://doi.org/10.1109/TNNLS.2021.3122143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the design of a mode- dependent adaptive event-triggered control (AETC) scheme for the stabilization of Markovian memristor-based reaction–diffusion neural networks (RDNNs). Different from the existing works with completely known transition probabilities, partly unknown transition probabilities (PUTPs) are considered here. The switching conditions and values of memristive connection weights are all correlated with Markovian jumping. A mode-dependent AETC scheme is newly proposed, in which different adaptive event-triggered mechanisms will be applied for different Markovian jumping modes and memristor switching modes. For each given mode, the corresponding event-triggered mechanism can efficiently reduce the number of transmission signals by adaptively adjusting the threshold. Thus, the mode-dependent AETC scheme can effectively save the limited network communication resources for the considered system. Based on the proposed control scheme, a new stabilization criterion is set up for Markovian memristor-based RDNNs with PUTPs. Meanwhile, a memristor-dependent AETC scheme is devised for memristor-based RDNNs. Finally, simulation results are presented to verify the effectiveness and superiority of the analysis results.},
  archive      = {J_TNNLS},
  author       = {Ruimei Zhang and Hongxia Wang and Ju H. Park and Kaibo Shi and Peisong He},
  doi          = {10.1109/TNNLS.2021.3122143},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3939-3951},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mode-dependent adaptive event-triggered control for stabilization of markovian memristor-based Reaction–Diffusion neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GPENs: Graph data learning with graph propagation-embedding
networks. <em>TNNLS</em>, <em>34</em>(8), 3925–3938. (<a
href="https://doi.org/10.1109/TNNLS.2021.3120100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compact representation of graph data is a fundamental problem in pattern recognition and machine learning area. Recently, graph neural networks (GNNs) have been widely studied for graph-structured data representation and learning tasks, such as graph semi-supervised learning, clustering, and low-dimensional embedding. In this article, we present graph propagation-embedding networks (GPENs), a new model for graph-structured data representation and learning problem. GPENs are mainly motivated by 1) revisiting of traditional graph propagation techniques for graph node context-aware feature representation and 2) recent studies on deeply graph embedding and neural network architecture. GPENs integrate both feature propagation on graph and low-dimensional embedding simultaneously into a unified network using a novel propagation-embedding architecture. GPENs have two main advantages. First, GPENs can be well-motivated and explained from feature propagation and deeply learning architecture. Second, the equilibrium representation of the propagation-embedding operation in GPENs has both exact and approximate formulations, both of which have simple closed-form solutions. This guarantees the compactivity and efficiency of GPENs. Third, GPENs can be naturally extended to multiple GPENs (M-GPENs) to address the data with multiple graph structures. Experiments on various semi-supervised learning tasks on several benchmark datasets demonstrate the effectiveness and benefits of the proposed GPENs and M-GPENs.},
  archive      = {J_TNNLS},
  author       = {Bo Jiang and Leiling Wang and Jian Cheng and Jin Tang and Bin Luo},
  doi          = {10.1109/TNNLS.2021.3120100},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3925-3938},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GPENs: Graph data learning with graph propagation-embedding networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TADC: A topic-aware dynamic convolutional neural network for
aspect extraction. <em>TNNLS</em>, <em>34</em>(8), 3912–3924. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect extraction is one of the key tasks in fine-grained sentiment analysis. This task aims to identify explicit opinion targets from user-generated documents. Currently, the mainstream methods for aspect extraction are built on recurrent neural networks (RNNs), which are difficult to parallelize. To accelerate the training/testing process, convolutional neural network (CNN)-based methods are introduced. However, such models usually utilize the same set of filters to convolve all input documents, and hence, the unique information inherent in each document may not be fully captured. To alleviate this issue, we propose a CNN-based model that employs a set of dynamic filters. Specifically, the proposed model extracts the aspects in a document using the filters generated from the aspect information intrinsic in the document. With the dynamically generated filters, our model is capable of learning more important features concerning aspects, thus promoting the effectiveness of aspect extraction. Furthermore, considering that aspects can be grouped into certain topics that conversely indicate the target words that need to be extracted, we naturally introduce a neural topic model (NTM) and integrate latent topics into the CNN-based module to help identify aspects. Experiments on two benchmark datasets demonstrate that the joint model is able to effectively identify aspects and produce interpretable topics.},
  archive      = {J_TNNLS},
  author       = {Zusheng Zhang and Yanghui Rao and Hanjiang Lai and Jiahai Wang and Jian Yin},
  doi          = {10.1109/TNNLS.2021.3119026},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3912-3924},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TADC: A topic-aware dynamic convolutional neural network for aspect extraction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-scale progressive collaborative attention network
for remote sensing fusion classification. <em>TNNLS</em>,
<em>34</em>(8), 3897–3911. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of remote sensing technology, panchromatic images (PANs) and multispectral images (MSs) can be easily obtained. PAN has higher spatial resolution, while MS has more spectral information. So how to use the two kinds of images’ characteristics to design a network has become a hot research field. In this article, a multi-scale progressive collaborative attention network (MPCA-Net) is proposed for PAN and MS’s fusion classification. Compared to the traditional multi-scale convolution operations, we adopt an adaptive dilation rate selection strategy (ADR-SS) to adaptively select the dilation rate to deal with the problem of category area’s excessive scale differences. For the traditional pixel-by-pixel sliding window sampling strategy, the patches which are generated by adjacent pixels but belonging to different categories contain a considerable overlap of information. So we change original sampling strategy and propose a center pixel migration (CPM) strategy. It migrates the center pixel to the most similar position of the neighborhood information for classification, which reduces network confusion and increases its stability. Moreover, due to the different spatial and spectral characteristics of PAN and MS, the same network structure for the two branches ignores their respective advantages. For a certain branch, as the network deepens, characteristic has different representations in different stages, so using the same module in multiple feature extraction stages is inappropriate. Thus we carefully design different modules for each feature extraction stage of the two branches. Between the two branches, because the strong mapping methods of directly cascading their features are too rough, we design collaborative progressive fusion modules to eliminate the differences. The experimental results verify that our proposed method can achieve competitive performance.},
  archive      = {J_TNNLS},
  author       = {Wenping Ma and Yating Li and Hao Zhu and Haoxiang Ma and Licheng Jiao and Jianchao Shen and Biao Hou},
  doi          = {10.1109/TNNLS.2021.3121490},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3897-3911},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A multi-scale progressive collaborative attention network for remote sensing fusion classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural-network-based event-triggered sliding mode control
for networked switched linear systems with the unknown nonlinear
disturbance. <em>TNNLS</em>, <em>34</em>(8), 3885–3896. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The event-triggered sliding mode control (SMC) problem for uncertain networked switched systems with the external unknown nonlinear disturbance is investigated. A neural network (NN) receiving the triggered state is utilized to approximate the external unknown nonlinear disturbance. First, a novel adaptive mode-dependent continuous-time event-triggering scheme (ETS) based on NN weights’ estimations is proposed to reduce the burden of the network bandwidth. Then, using the time-varying Lyapunov function method, a novel adaptive NN event-triggered sliding mode controller is established and a dwell-time switching law is obtained, which can guarantee ultimate boundedness, and attain the sliding region around the specified sliding surface for switched systems. Further, a new integral sliding surface that depends on the system states at switching instants and includes the exponential term is proposed. Obtaining the boundary of the sliding mode region relies on the exponential term for continuous-time systems. Moreover, the Zeno behavior can be avoided under the proposed continuous-time ETS by dividing event-triggering signals and switching signals. Finally, a comparative example and a switched Chua’s Circuit example are given to illustrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Yuzhong Wang and Jun Zhao},
  doi          = {10.1109/TNNLS.2021.3119665},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3885-3896},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based event-triggered sliding mode control for networked switched linear systems with the unknown nonlinear disturbance},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A background knowledge revising and incorporating dialogue
model. <em>TNNLS</em>, <em>34</em>(8), 3874–3884. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, dialogue systems have attracted increasing research interest. In particular, background knowledge is incorporated to improve the performance of dialogue systems. Existing dialogue systems mostly assume that the background knowledge is correct and comprehensive. However, low-quality background knowledge is common in real-world applications. On the other hand, dialogue datasets with manual labeled background knowledge are often insufficient. To tackle these challenges, this article presents an algorithm to revise low-quality background knowledge, named background knowledge revising transformer (BKR-Transformer). By innovatively formulating the knowledge revising task as a sequence-to-sequence (Seq2Seq) problem, BKR-Transformer generates the revised background knowledge based on the original background knowledge and dialogue history. More importantly, to alleviate the effect of insufficient training data, BKR-Transformer introduces the ideas of parameter sharing and tensor decomposition, which could significantly reduce the number of model parameters. Furthermore, this work presents a background knowledge revising and incorporating dialogue model that combines the background knowledge revision with response selection in a unified model. Empirical analyses on real-world applications demonstrate that the proposed background knowledge revising and incorporating dialogue system (BKRI) could revise most low-quality background knowledge and substantially outperforms previous dialogue models.},
  archive      = {J_TNNLS},
  author       = {Xinyan Zhao and Xiao Feng and Huanhuan Chen},
  doi          = {10.1109/TNNLS.2021.3123128},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3874-3884},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A background knowledge revising and incorporating dialogue model},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bridging the theoretical bound and deep algorithms for open
set domain adaptation. <em>TNNLS</em>, <em>34</em>(8), 3859–3873. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the unsupervised open set domain adaptation (UOSDA), the target domain contains unknown classes that are not observed in the source domain. Researchers in this area aim to train a classifier to accurately: 1) recognize unknown target data (data with unknown classes) and 2) classify other target data. To achieve this aim, a previous study has proven an upper bound of the target-domain risk, and the open set difference, as an important term in the upper bound, is used to measure the risk on unknown target data. By minimizing the upper bound, a shallow classifier can be trained to achieve the aim. However, if the classifier is very flexible [e.g., deep neural networks (DNNs)], the open set difference will converge to a negative value when minimizing the upper bound, which causes an issue where most target data are recognized as unknown data. To address this issue, we propose a new upper bound of target-domain risk for UOSDA, which includes four terms: source-domain risk, $\epsilon $ -open set difference ( $\Delta _\epsilon $ ), distributional discrepancy between domains, and a constant. Compared with the open set difference, $\Delta _\epsilon $ is more robust against the issue when it is being minimized, and thus we are able to use very flexible classifiers (i.e., DNNs). Then, we propose a new principle-guided deep UOSDA method that trains DNNs via minimizing the new upper bound. Specifically, source-domain risk and $\Delta _\epsilon $ are minimized by gradient descent, and the distributional discrepancy is minimized via a novel open set conditional adversarial training strategy. Finally, compared with the existing shallow and deep UOSDA methods, our method shows the state-of-the-art performance on several benchmark datasets, including digit recognition [modified National Institute of Standards and Technology database (MNIST), the Street View House Number (SVHN), U.S. Postal Service (USPS)], object recognition (Office-31, Office-Home), and face recognition [pose, illumination, and expression (PIE)].},
  archive      = {J_TNNLS},
  author       = {Li Zhong and Zhen Fang and Feng Liu and Bo Yuan and Guangquan Zhang and Jie Lu},
  doi          = {10.1109/TNNLS.2021.3119965},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3859-3873},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bridging the theoretical bound and deep algorithms for open set domain adaptation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain graph convolutions for adversarial unsupervised
domain adaptation. <em>TNNLS</em>, <em>34</em>(8), 3847–3858. (<a
href="https://doi.org/10.1109/TNNLS.2021.3122899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) has attracted increasing attention in recent years, which adapts classifiers to an unlabeled target domain by exploiting a labeled source domain. To reduce the discrepancy between source and target domains, adversarial learning methods are typically selected to seek domain-invariant representations by confusing the domain discriminator. However, classifiers may not be well adapted to such a domain-invariant representation space, as the sample- and class-level data structures could be distorted during adversarial learning. In this article, we propose a novel transferable feature learning approach on graphs (TFLG) for unsupervised adversarial domain adaptation (DA), which jointly incorporates sample- and class-level structure information across two domains. TFLG first constructs graphs for minibatch samples and identifies the classwise correspondence across domains. A novel cross-domain graph convolutional operation is designed to jointly align the sample- and class-level structures in two domains. Moreover, a memory bank is designed to further exploit the class-level information. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach compared to the state-of-the-art UDA methods.},
  archive      = {J_TNNLS},
  author       = {Ronghang Zhu and Xiaodong Jiang and Jiasen Lu and Sheng Li},
  doi          = {10.1109/TNNLS.2021.3122899},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3847-3858},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cross-domain graph convolutions for adversarial unsupervised domain adaptation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Automatic design of convolutional neural network
architectures under resource constraints. <em>TNNLS</em>,
<em>34</em>(8), 3832–3846. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of various smart electronics and mobile/edge devices, many existing high-accuracy convolutional neural network (CNN) models are difficult to be applied in practice due to the limited resources, such as memory capacity, power consumption, and spectral efficiency. In order to meet these constraints, researchers have carefully designed some lightweight networks. Meanwhile, to reduce the reliance on manual design on expert experience, some researchers also work to improve neural architecture search (NAS) algorithms to automatically design small networks, exploiting the multiobjective approaches that consider both accuracy and other important goals during optimization. However, simply searching for smaller network models is not consistent with the current research belief of “the deeper the better” and may affect the effectiveness of the model and thus waste the limited resources available. In this article, we propose an automatic method for designing CNNs architectures under constraint handling, which can search for optimal network models meeting the preset constraint. Specifically, an adaptive penalty algorithm is used for fitness evaluation, and a selective repair operation is developed for infeasible individuals to search for feasible CNN architectures. As a case study, we set the complexity (the number of parameters) as a resource constraint and perform multiple experiments on CIFAR-10 and CIFAR-100, to demonstrate the effectiveness of the proposed method. In addition, the proposed algorithm is compared with a state-of-the-art algorithm, NSGA-Net, and several manual-designed models. The experimental results show that the proposed algorithm can successfully solve the problem of the uncertain size of the optimal CNN model under the random search strategy, and the automatically designed CNN model can satisfy the predefined resource constraint while achieving better accuracy.},
  archive      = {J_TNNLS},
  author       = {Siyi Li and Yanan Sun and Gary G. Yen and Mengjie Zhang},
  doi          = {10.1109/TNNLS.2021.3123105},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3832-3846},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatic design of convolutional neural network architectures under resource constraints},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of vehicle detection techniques for intelligent
vehicles. <em>TNNLS</em>, <em>34</em>(8), 3811–3831. (<a
href="https://doi.org/10.1109/TNNLS.2021.3128968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust and efficient vehicle detection is an important task of environment perception of intelligent vehicles, which directly affects the behavior decision-making and motion planning of intelligent vehicles. Due to the rapid development of sensor and computer technology, the algorithm and technology of vehicle detection have been updated rapidly. But, there are few reviews on vehicle detection of intelligent vehicles, especially covering all kinds of sensors and algorithms in recent years. This article presents a comprehensive review of vehicle detection approaches and their applications in intelligent vehicle systems to analyze the development of vehicle detection, with a specific focus on sensor types and algorithm classification. First, more than 300 research contributions are summarized in this review, including all kinds of vehicle detection sensors (machine vision, millimeter-wave radar, lidar, and multisensor fusion), and the performance of the classic and latest algorithms was compared in detail. Then, the application scenarios of vehicle detection with different sensors and algorithms were analyzed according to their performance and applicability. Moreover, we also systematically summarized the methods of vehicle detection in adverse weather. Finally, the remaining challenges and future research trends were analyzed according to the development of intelligent vehicle sensors and algorithms.},
  archive      = {J_TNNLS},
  author       = {Zhangu Wang and Jun Zhan and Chunguang Duan and Xin Guan and Pingping Lu and Kai Yang},
  doi          = {10.1109/TNNLS.2021.3128968},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3811-3831},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A review of vehicle detection techniques for intelligent vehicles},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning for mobile robotics exploration: A
survey. <em>TNNLS</em>, <em>34</em>(8), 3796–3810. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient exploration of unknown environments is a fundamental precondition for modern autonomous mobile robot applications. Aiming to design robust and effective robotic exploration strategies, suitable to complex real-world scenarios, the academic community has increasingly investigated the integration of robotics with reinforcement learning (RL) techniques. This survey provides a comprehensive review of recent research works that use RL to design unknown environment exploration strategies for single and multirobots. The primary purpose of this study is to facilitate future research by compiling and analyzing the current state of works that link these two knowledge domains. This survey summarizes: what are the employed RL algorithms and how they compose the so far proposed mobile robot exploration strategies; how robotic exploration solutions are addressing typical RL problems like the exploration-exploitation dilemma, the curse of dimensionality, reward shaping, and slow learning convergence; and what are the performed experiments and software tools used for learning and testing. Achieved progress is described, and a discussion about remaining limitations and future perspectives is presented.},
  archive      = {J_TNNLS},
  author       = {Luíza Caetano Garaffa and Maik Basso and Andréa Aparecida Konzen and Edison Pignaton de Freitas},
  doi          = {10.1109/TNNLS.2021.3124466},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3796-3810},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning for mobile robotics exploration: A survey},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning for cyber security.
<em>TNNLS</em>, <em>34</em>(8), 3779–3795. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scale of Internet-connected systems has increased considerably, and these systems are being exposed to cyberattacks more than ever. The complexity and dynamics of cyberattacks require protecting mechanisms to be responsive, adaptive, and scalable. Machine learning, or more specifically deep reinforcement learning (DRL), methods have been proposed widely to address these issues. By incorporating deep learning into traditional RL, DRL is highly capable of solving complex, dynamic, and especially high-dimensional cyber defense problems. This article presents a survey of DRL approaches developed for cyber security. We touch on different vital aspects, including DRL-based security methods for cyber–physical systems, autonomous intrusion detection techniques, and multiagent DRL-based game theory simulations for defense strategies against cyberattacks. Extensive discussions and future research directions on DRL-based cyber security are also given. We expect that this comprehensive review provides the foundations for and facilitates future studies on exploring the potential of emerging DRL to cope with increasingly complex cyber security problems.},
  archive      = {J_TNNLS},
  author       = {Thanh Thi Nguyen and Vijay Janapa Reddi},
  doi          = {10.1109/TNNLS.2021.3121870},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3779-3795},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep reinforcement learning for cyber security},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural-network-based adaptive control of uncertain MIMO
singularly perturbed systems with full-state constraints.
<em>TNNLS</em>, <em>34</em>(7), 3764–3774. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the tracking control problem for a class of nonlinear multi-input–multi-output (MIMO) uncertain singularly perturbed systems (SPSs) with full-state constraints. The underlying issues become more challenging because two-time-scale characteristics and full state constraints are involved. To this end, first, the adaptive neural network (NN) control method is designed to handle system uncertainties in the design process. Second, the nonlinear state-dependent coordinate transformation functions are employed to avoid the violation of full-state constraints and feasibility conditions for intermediate controllers. Furthermore, by introducing an appropriate $\varepsilon $ -dependent Lyapunov function, the potential ill-conditioned numerical problems in the design process of SPSs are avoided, and the stability of the nonlinear SPSs is proven. Finally, two examples are presented to illustrate the validity of the proposed adaptive NN control scheme.},
  archive      = {J_TNNLS},
  author       = {Hao Wang and Chunyu Yang and Xiaomin Liu and Linna Zhou},
  doi          = {10.1109/TNNLS.2021.3123361},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3764-3774},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based adaptive control of uncertain MIMO singularly perturbed systems with full-state constraints},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local stability and convergence analysis of neural network
controllers with error integral inputs. <em>TNNLS</em>, <em>34</em>(7),
3751–3763. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the local stability and local convergence of a class of neural network (NN) controllers with error integrals as inputs for reference tracking. It is formally proved that if the input of the NN controller consists exclusively of error terms, the control system shows a non-zero steady-state error for any constant reference except for one specific point, for both single-layer and multi-layer NN controllers. It is further proved that adding error integrals to the input of the (single- and multi-layers) NN controller is one sufficient way to remove the steady-state error for any constant reference. Due to the nonlinearity of the NN controllers, the NN control systems are linearized at the equilibrium points. We provide proof that if all the eigenvalues of the linearized NN control system have negative real parts, local asymptotic stability and local exponential convergence are guaranteed. Two case studies were explored to verify the theoretical results: a single-layer NN controller in a 1-D system and a four-layer NN controller in a 2-D system applied to renewable energy integration. Simulations demonstrate that when NN controllers and the corresponding generalized proportional-integral (PI) controllers have the same eigenvalues, all control systems exhibit almost the same responses in a small neighborhood of their respective equilibrium points.},
  archive      = {J_TNNLS},
  author       = {Xingang Fu and Shuhui Li and Donald C. Wunsch and Eduardo Alonso},
  doi          = {10.1109/TNNLS.2021.3116189},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3751-3763},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local stability and convergence analysis of neural network controllers with error integral inputs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-constraint latent representation learning for
prognosis analysis using multi-modal data. <em>TNNLS</em>,
<em>34</em>(7), 3737–3750. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox proportional hazard model has been widely applied to cancer prognosis prediction. Nowadays, multi-modal data, such as histopathological images and gene data, have advanced this field by providing histologic phenotype and genotype information. However, how to efficiently fuse and select the complementary information of high-dimensional multi-modal data remains challenging for Cox model, as it generally does not equip with feature fusion/selection mechanism. Many previous studies typically perform feature fusion/selection in the original feature space before Cox modeling. Alternatively, learning a latent shared feature space that is tailored for Cox model and simultaneously keeps sparsity is desirable. In addition, existing Cox-based models commonly pay little attention to the actual length of the observed time that may help to boost the model’s performance. In this article, we propose a novel Cox-driven multi-constraint latent representation learning framework for prognosis analysis with multi-modal data. Specifically, for efficient feature fusion, a multi-modal latent space is learned via a bi-mapping approach under ranking and regression constraints. The ranking constraint utilizes the log-partial likelihood of Cox model to induce learning discriminative representations in a task-oriented manner. Meanwhile, the representations also benefit from regression constraint, which imposes the supervision of specific survival time on representation learning. To improve generalization and alleviate overfitting, we further introduce similarity and sparsity constraints to encourage extra consistency and sparseness. Extensive experiments on three datasets acquired from The Cancer Genome Atlas (TCGA) demonstrate that the proposed method is superior to state-of-the-art Cox-based models.},
  archive      = {J_TNNLS},
  author       = {Zhenyuan Ning and Zehui Lin and Qing Xiao and Denghui Du and Qianjin Feng and Wufan Chen and Yu Zhang},
  doi          = {10.1109/TNNLS.2021.3112194},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3737-3750},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-constraint latent representation learning for prognosis analysis using multi-modal data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing chinese character representation with
lattice-aligned attention. <em>TNNLS</em>, <em>34</em>(7), 3727–3736.
(<a href="https://doi.org/10.1109/TNNLS.2021.3114378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word–character lattice models have been proved to be effective for some Chinese natural language processing (NLP) tasks, in which word boundary information is fused into character sequences. However, due to the inherently unidirectional sequential nature, prior approaches have only learned sequential interactions of character–word instances but fail to capture fine-grained correlations in word–character spaces. In this article, we propose a lattice-aligned attention network (LAN) that aims to model dense interactions over word–character lattice structure for enhancing character representations. By carefully combining cross-lattice module, gated word–character semantic fusion unit, and self-lattice attention module, the network can explicitly capture fine-grained correlations across different spaces (e.g., word-to-character and character-to-character), thus significantly improving model performance. Experimental results on three Chinese NLP benchmark tasks demonstrate that LAN obtains state-of-the-art results compared to several competitive approaches.},
  archive      = {J_TNNLS},
  author       = {Shan Zhao and Minghao Hu and Zhiping Cai and Zhanjun Zhang and Tongqing Zhou and Fang Liu},
  doi          = {10.1109/TNNLS.2021.3114378},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3727-3736},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhancing chinese character representation with lattice-aligned attention},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hessian-aided random perturbation (HARP) using noisy
zeroth-order oracles. <em>TNNLS</em>, <em>34</em>(7), 3717–3726. (<a
href="https://doi.org/10.1109/TNNLS.2021.3117999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In stochastic optimization problems where only noisy zeroth-order (ZO) oracles are available, the Kiefer-Wolfowitz algorithm and its randomized counterparts are widely used as gradient estimators. Existing algorithms generate the random perturbations from certain distributions with a zero mean and an isotropic (either identity or scalar) covariance matrix. In contrast, this work considers the generalization where the perturbations may have an anisotropic covariance based on the ZO oracle history. We propose to feed the second-order approximation into the covariance matrix of the random perturbation, so it is dubbed as Hessian-aided random perturbation (HARP). HARP collects two or more (depending on the specific estimator form) ZO oracle calls per iteration to construct the gradient and the Hessian estimators. We prove HARP’s almost-surely convergence and derive its convergence rate under standard assumptions. We demonstrate, with theoretical guarantees and numerical experiments, that HARP is less sensitive to ill-conditioning and more query-efficient than other gradient approximation schemes whose random perturbations have an isotropic covariance.},
  archive      = {J_TNNLS},
  author       = {Jingyi Zhu},
  doi          = {10.1109/TNNLS.2021.3117999},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3717-3726},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hessian-aided random perturbation (HARP) using noisy zeroth-order oracles},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TRUST-TECH-based systematic search for multiple local optima
in deep neural nets. <em>TNNLS</em>, <em>34</em>(7), 3706–3716. (<a
href="https://doi.org/10.1109/TNNLS.2021.3115710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep neural networks (DNNs) rested heavily on efficient local solvers. Due to their local property, local solvers are sensitive to initialization and hyperparameters. In this article, a systematical method for finding multiple high-quality local optimal DNNs, based on the transformation under stability-retaining equilibria characterization (TRUST-TECH) method, is introduced. Our goal is to systematically search for multiple local optimal parameters for large models, such as DNNs, trained on large datasets. To achieve this, a dynamic searching path (DSP) method is proposed to provide improved search guidance used in TRUST-TECH. By integrating the DSP method with the TRUST-TECH (DSP-TT) method, multiple optimal training solutions with higher quality than randomly initialized ones can be obtained. To take advantage of these optimal solutions, a DSP-TT ensemble method is further developed. Experiments on various test cases show that the proposed DSP-TT method achieves considerable improvement over other ensemble methods developed for deep architectures. The DSP-TT ensemble method also shows diversity advantages over other ensemble methods.},
  archive      = {J_TNNLS},
  author       = {Zhiyong Hao and Hsiao-Dong Chiang and Bin Wang},
  doi          = {10.1109/TNNLS.2021.3115710},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3706-3716},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TRUST-TECH-based systematic search for multiple local optima in deep neural nets},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature distillation in deep attention network against
adversarial examples. <em>TNNLS</em>, <em>34</em>(7), 3691–3705. (<a
href="https://doi.org/10.1109/TNNLS.2021.3113342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are easily fooled by adversarial examples. Most existing defense strategies defend against adversarial examples based on full information of whole images. In reality, one possible reason as to why humans are not sensitive to adversarial perturbations is that the human visual mechanism often concentrates on most important regions of images. A deep attention mechanism has been applied in many computer fields and has achieved great success. Attention modules are composed of an attention branch and a trunk branch. The encoder/decoder architecture in the attention branch has potential of compressing adversarial perturbations. In this article, we theoretically prove that attention modules can compress adversarial perturbations by destroying potential linear characteristics of DNNs. Considering the distribution characteristics of adversarial perturbations in different frequency bands, we design and compare three types of attention modules based on frequency decomposition and reorganization to defend against adversarial examples. Moreover, we find that our designed attention modules can obtain high classification accuracies on clean images by locating attention regions more accurately. Experimental results on the CIFAR and ImageNet dataset demonstrate that frequency reorganization in attention modules can not only achieve good robustness to adversarial perturbations, but also obtain comparable, even higher classification, accuracies on clean images. Moreover, our proposed attention modules can be integrated with existing defense strategies as components to further improve adversarial robustness.},
  archive      = {J_TNNLS},
  author       = {Xin Chen and Jian Weng and Xiaoling Deng and Weiqi Luo and Yubin Lan and Qi Tian},
  doi          = {10.1109/TNNLS.2021.3113342},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3691-3705},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feature distillation in deep attention network against adversarial examples},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning on autonomous driving policy
with auxiliary critic network. <em>TNNLS</em>, <em>34</em>(7),
3680–3690. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) is a machine learning method based on rewards, which can be extended to solve some complex and realistic decision-making problems. Autonomous driving needs to deal with a variety of complex and changeable traffic scenarios, so the application of DRL in autonomous driving presents a broad application prospect. In this article, an end-to-end autonomous driving policy learning method based on DRL is proposed. On the basis of proximal policy optimization (PPO), we combine a curiosity-driven method called recurrent neural network (RNN) to generate an intrinsic reward signal to encounter the agent to explore its environment, which improves the efficiency of exploration. We introduce an auxiliary critic network on the original actor–critic framework and choose the lower estimate which is predicted by the dual critic network when the network update to avoid the overestimation bias. We test our method on the lane- keeping task and overtaking task in the open racing car simulator (TORCS) driving simulator and compare with other DRL methods, experimental results show that our proposed method can improve the training efficiency and control performance in driving tasks.},
  archive      = {J_TNNLS},
  author       = {Yuanqing Wu and Siqin Liao and Xiang Liu and Zhihang Li and Renquan Lu},
  doi          = {10.1109/TNNLS.2021.3116063},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3680-3690},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep reinforcement learning on autonomous driving policy with auxiliary critic network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale curvelet scattering network. <em>TNNLS</em>,
<em>34</em>(7), 3665–3679. (<a
href="https://doi.org/10.1109/TNNLS.2021.3118221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature representation has received more and more attention in image classification. Existing methods always directly extract features via convolutional neural networks (CNNs). Recent studies have shown the potential of CNNs when dealing with images’ edges and textures, and some methods have been explored to further improve the representation process of CNNs. In this article, we propose a novel classification framework called the multiscale curvelet scattering network (MSCCN). Using the multiscale curvelet-scattering module (CCM), image features can be effectively represented. There are two parts in MSCCN, which are the multiresolution scattering process and the multiscale curvelet module. According to multiscale geometric analysis, curvelet features are utilized to improve the scattering process with more effective multiscale directional information. Specifically, the scattering process and curvelet features are effectively formulated into a unified optimization structure, with features from different scale levels being efficiently aggregated and learned. Furthermore, a one-level CCM, which can essentially improve the quality of feature representation, is constructed to be embedded into other existing networks. Extensive experimental results illustrate that MSCCN achieves better classification accuracy when compared with state-of-the-art techniques. Eventually, the convergence, insight, and adaptability are evaluated by calculating the trend of loss function’s values, visualizing some feature maps, and performing generalization analysis.},
  archive      = {J_TNNLS},
  author       = {Jie Gao and Licheng Jiao and Fang Liu and Shuyuan Yang and Biao Hou and Xu Liu},
  doi          = {10.1109/TNNLS.2021.3118221},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3665-3679},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiscale curvelet scattering network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial–spectral unified adaptive probability graph
convolutional networks for hyperspectral image classification.
<em>TNNLS</em>, <em>34</em>(7), 3650–3664. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In hyperspectral image (HSI) classification task, semisupervised graph convolutional network (GCN)-based methods have received increasing attention. However, two problems still need to be addressed. The first is that the initial graph structure in the GCN-based methods is not sufficiently flexible to encode the homogenous structure similarity of HSI pixels when facing the complex scenarios induced by the spatial variability. Another problem is that the input (graph structure) and output (output features) of the GCN-based methods are separated with a “single pass” procedure, which is a suboptimal problem for HSI classification because it does not flexibly optimize the graph construction with a feedback method via output features. In this article, a novel spatial–spectral unified adaptive probability GCN (SSAPGCN) method is proposed for HSI classification. First, considering the homogeneous structural similarity of the pairwise relationships of HSI pixels, this article combines the inherent spectral information and spatial coordinates to obtain the spatial–spectral adaptive probability graph (SSAPG) structure, which can capture the probabilistic connectivity between each pair of the homogeneous HSI pixels. Second, the SSAPG structure and GCN model are combined into a unified framework to a daptively learn both the graph structure and the output features simultaneously with feedback. Finally, the proposed SSAPGCN method with two layers is evaluated on four public HSI datasets to demonstrate its superiority over different classification methods in terms of two evaluation metrics, the overall accuracy (OA) and kappa coefficient (KC), especially with small training sample sizes.},
  archive      = {J_TNNLS},
  author       = {Yun Ding and Yanwen Chong and Shaoming Pan and Yujie Wang and Congchong Nie},
  doi          = {10.1109/TNNLS.2021.3112268},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3650-3664},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatial–Spectral unified adaptive probability graph convolutional networks for hyperspectral image classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Keep and select: Improving hierarchical context modeling for
multi-turn response generation. <em>TNNLS</em>, <em>34</em>(7),
3636–3649. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical context modeling plays an important role in the response generation for multi-turn conversational systems. Previous methods mainly model context as multiple independent utterances and rely on attention mechanisms to obtain the context representation. They tend to ignore the explicit responds-to relationships between adjacent utterances and the special role that the user’s latest utterance (the query) plays in determining the success of a conversation. To deal with this, we propose a multi-turn response generation model named KS-CQ, which contains two crucial components, the Keep and the Select modules, to produce a neighbor-aware context representation and a context-enriched query representation. The Keep module recodes each utterance of context by attentively introducing semantics from its prior and posterior neighboring utterances. The Select module treats the context as background information and selectively uses it to enrich the query representing process. Extensive experiments on two benchmark multi-turn conversation datasets demonstrate the effectiveness of our proposal compared with the state-of-the-art baselines in terms of both automatic and human evaluations.},
  archive      = {J_TNNLS},
  author       = {Yanxiang Ling and Fei Cai and Jun Liu and Honghui Chen and Maarten de Rijke},
  doi          = {10.1109/TNNLS.2021.3112700},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3636-3649},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Keep and select: Improving hierarchical context modeling for multi-turn response generation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synchronization of uncertain coupled neural networks with
time-varying delay of unknown bound via distributed delayed impulsive
control. <em>TNNLS</em>, <em>34</em>(7), 3624–3635. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the issue of synchronization for a type of uncertain coupled neural networks (CNNs) involving time-varying delay with unmeasured or unknown bound by delayed impulsive control with distributed delay. A new Halanay-like delayed differential inequality is presented, and both cases of impulsive control and impulsive perturbation are well-considered. Stemmed from this new inequality and techniques of linear matrix inequalities (LMIs), some sufficient criteria are obtained to achieve both dynamically and statically global $\mu $ -synchronization of the delayed CNNs, and a distributed-delay-dependent impulsive controller is designed. A numerical simulation is provided to demonstrate the validity of the obtained theoretical results.},
  archive      = {J_TNNLS},
  author       = {Xiaoyu Zhang and Chuandong Li and Hongfei Li and Zhengran Cao},
  doi          = {10.1109/TNNLS.2021.3116069},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3624-3635},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of uncertain coupled neural networks with time-varying delay of unknown bound via distributed delayed impulsive control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining knowledge graph and word embeddings for spherical
topic modeling. <em>TNNLS</em>, <em>34</em>(7), 3609–3623. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic topic models are considered as an effective framework for text analysis that uncovers the main topics in an unlabeled set of documents. However, the inferred topics by traditional topic models are often unclear and not easy to interpret because they do not account for semantic structures in language. Recently, a number of topic modeling approaches tend to leverage domain knowledge to enhance the quality of the learned topics, but they still assume a multinomial or Gaussian document likelihood in the Euclidean space, which often results in information loss and poor performance. In this article, we propose a Bayesian embedded spherical topic model (ESTM) that combines both knowledge graph and word embeddings in a non-Euclidean curved space, the hypersphere, for better topic interpretability and discriminative text representations. Extensive experimental results show that our proposed model successfully uncovers interpretable topics and learns high-quality text representations useful for common natural language processing (NLP) tasks across multiple benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Hafsa Ennajari and Nizar Bouguila and Jamal Bentahar},
  doi          = {10.1109/TNNLS.2021.3112045},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3609-3623},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Combining knowledge graph and word embeddings for spherical topic modeling},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale hybrid fusion network for single image
deraining. <em>TNNLS</em>, <em>34</em>(7), 3594–3608. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have been able to generate rain-free images effectively, but the extension of these methods to complex rain conditions where rain streaks show various blurring degrees, shapes, and densities has remained an open problem. Among the major challenges are the capacity to encode the rain streaks and the sheer difficulty of learning multi-scale context features that preserve both global color coherence and exactness of detail. To address the first problem, we design a non-local fusion module (NFM) and an attention fusion module (AFM), and construct the multi-level pyramids’ architecture to explore the local and global correlations of rain information from the rain image pyramid. More specifically, we apply the non-local operation to fully exploit the self-similarity of rain streaks and perform the fusion of multi-scale features along the image pyramid. To address the latter challenge, we additionally design a residual learning branch that is capable of adaptively bridging the gaps (e.g., texture and color information) between the predicted rain-free image and the clean background via a hybrid embedding representation. Extensive results have demonstrated that our proposed method is able to generate much better rain-free images on several benchmark datasets than the state-of-the-art algorithms. Moreover, we conduct the joint evaluation experiments with respect to deraining performance and the detection/segmentation accuracy to further verify the effectiveness of our deraining method for downstream vision tasks/applications. The source code is available at https://github.com/kuihua/MSHFN .},
  archive      = {J_TNNLS},
  author       = {Kui Jiang and Zhongyuan Wang and Peng Yi and Chen Chen and Guangcheng Wang and Zhen Han and Junjun Jiang and Zixiang Xiong},
  doi          = {10.1109/TNNLS.2021.3112235},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3594-3608},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-scale hybrid fusion network for single image deraining},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Looking at boundary: Siamese densely cooperative fusion for
salient object detection. <em>TNNLS</em>, <em>34</em>(7), 3580–3593. (<a
href="https://doi.org/10.1109/TNNLS.2021.3113657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though deep learning-based saliency detection methods have achieved gratifying performance recently, the predicted saliency maps still suffer from the boundary challenge. From the perspective of foreground–background separation, this article attempts to extract the edge information of objects by exploiting the difference between different color channels in the RGB color space and establishes a novel multicolor contrast extraction (MCE) mechanism to improve the learning ability of exquisite boundary information of the network. To make full use of the MCE outputs and RGB colors, and well depict and capture the complementary information between them, we devise a novel Siamese densely cooperative fusion (DCF) network (SDFNet) for saliency detection, which consists of two effective components: boundary-directed feature learning (BDFL) and DCF. The BDFL provides joint learning for both MCE and RGB modalities through a Siamese network, while the DCF module is devised for complementary feature discovery, in order to effectively combine the features learned from two modalities. Experiments on five well-known benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art approaches in terms of different evaluation metrics. We provide a detailed analysis of these results and indicate that our joint modeling of MCE and RGB colors helps to better capture the object details, especially in the object boundaries.},
  archive      = {J_TNNLS},
  author       = {Junxia Li and Ziyang Wang and Zefeng Pan and Qingshan Liu and Dongyan Guo},
  doi          = {10.1109/TNNLS.2021.3113657},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3580-3593},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Looking at boundary: Siamese densely cooperative fusion for salient object detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Direct-optimization-based DC dictionary learning with the
MCP regularizer. <em>TNNLS</em>, <em>34</em>(7), 3568–3579. (<a
href="https://doi.org/10.1109/TNNLS.2021.3114400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct-optimization-based dictionary learning has attracted increasing attention for improving computational efficiency. However, the existing direct optimization scheme can only be applied to limited dictionary learning problems, and it remains an open problem to prove that the whole sequence obtained by the algorithm converges to a critical point of the objective function. In this article, we propose a novel direct-optimization-based dictionary learning algorithm using the minimax concave penalty (MCP) as a sparsity regularizer that can enforce strong sparsity and obtain accurate estimation. For solving the corresponding optimization problem, we first decompose the nonconvex MCP into two convex components. Then, we employ the difference of the convex functions algorithm and the nonconvex proximal-splitting algorithm to process the resulting subproblems. Thus, the direct optimization approach can be extended to a broader class of dictionary learning problems, even if the sparsity regularizer is nonconvex. In addition, the convergence guarantee for the proposed algorithm can be theoretically proven. Our numerical simulations demonstrate that the proposed algorithm has good convergence performances in different cases and robust dictionary-recovery capabilities. When applied to sparse approximations, the proposed approach can obtain sparser and less error estimation than the different sparsity regularizers in existing methods. In addition, the proposed algorithm has robustness in image denoising and key-frame extraction.},
  archive      = {J_TNNLS},
  author       = {Zhenni Li and Zuyuan Yang and Haoli Zhao and Shengli Xie},
  doi          = {10.1109/TNNLS.2021.3114400},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3568-3579},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Direct-optimization-based DC dictionary learning with the MCP regularizer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven h∞ optimal output feedback control for linear
discrete-time systems based on off-policy q-learning. <em>TNNLS</em>,
<em>34</em>(7), 3553–3567. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops two novel output feedback (OPFB) $Q$ -learning algorithms, on-policy $Q$ -learning and off-policy $Q$ -learning, to solve $H_{\infty }$ static OPFB control problem of linear discrete-time (DT) systems. The primary contribution of the proposed algorithms lies in a newly developed OPFB control algorithm form for completely unknown systems. Under the premise of satisfying disturbance attenuation conditions, the conditions for the existence of the optimal OPFB solution are given. The convergence of the proposed $Q$ -learning methods, and the difference and equivalence of two algorithms are rigorously proven. Moreover, considering the effects brought by probing noise for the persistence of excitation (PE), the proposed off-policy $Q$ -learning method has the advantage of being immune to probing noise and avoiding biasedness of solution. Simulation results are presented to verify the effectiveness of the proposed approaches.},
  archive      = {J_TNNLS},
  author       = {Li Zhang and Jialu Fan and Wenqian Xue and Victor G. Lopez and Jinna Li and Tianyou Chai and Frank L. Lewis},
  doi          = {10.1109/TNNLS.2021.3112457},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3553-3567},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven h∞ optimal output feedback control for linear discrete-time systems based on off-policy Q-learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intelligent method for predicting the pressure
coefficient curve of airfoil-based conditional generative adversarial
networks. <em>TNNLS</em>, <em>34</em>(7), 3538–3552. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, extensive studies have focused on analyzing aerodynamic performance due to its important impact on aircraft design. Most of these works compute the aerodynamic coefficient of the airfoil through computational fluid dynamics (CFD) simulation, which is too time-consuming. To reduce the computational time required, some intelligence-based methods have been presented. However, these methods also suffer from certain issues. First, most of them directly implement existing machine learning methods used to predict the aerodynamic coefficient without adding any improvements. Second, some methods convert the airfoil shape and aerodynamic curves into images, which may lead to curve distortion and the introduction of noise. Third, some methods learn the relationship between the airfoil shape and aerodynamic coefficients but ignore the influence of initial inflow conditions. Accordingly, to address these issues, we propose an intelligent method for predicting the pressure coefficients (Cp) of airfoil based on a conditional generative adversarial network (cGAN). More specifically, we first present a two-step data augmentation strategy designed to expand the original airfoil dataset. Subsequently, we design a novel cGAN-based neural network to predict the Cp curve. To the best of our knowledge, this is the first work to apply generative adversarial network (GAN) to aerodynamic coefficient prediction. Moreover, we design a new loss function to train our network. Extensive experimental results demonstrate that the Cp curve predicted by our method is very close to that generated via CFD simulation. More importantly, our method achieves a speedup close to 1000x compared with CFD simulation.},
  archive      = {J_TNNLS},
  author       = {Yueqing Wang and Liang Deng and Yunbo Wan and Zhigong Yang and Wenxiang Yang and Cheng Chen and Dan Zhao and Fang Wang and Yang Guo},
  doi          = {10.1109/TNNLS.2021.3111911},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3538-3552},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An intelligent method for predicting the pressure coefficient curve of airfoil-based conditional generative adversarial networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Highly robust vehicle lateral localization using multilevel
robust network. <em>TNNLS</em>, <em>34</em>(7), 3527–3537. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based vehicle lateral localization has been extensively studied in the literature. However, it faces great challenges when dealing with occlusion situations where the road is frequently occluded by moving/static objects. To address the occlusion problem, we propose a highly robust lateral localization framework called multilevel robust network (MLRN) in this article. MLRN utilizes three deep neural networks (DNNs) to reduce the impact of occluding objects on localization performance from the object, feature, and decision levels, respectively, which shows strong robustness to varying degrees of road occlusion. At the object level, an attention-guided network (AGNet) is designed to achieve accurate road detection by paying more attention to the interested road area. Then, at the feature level, a lateral-connection fully convolutional denoising autoencoder (LC-FCDAE) is proposed to learn robust location features from the road area. Finally, at the decision level, a long short-term memory (LSTM) network is used to enhance the prediction accuracy of lateral position by establishing the temporal correlations of positioning decisions. Experimental results validate the effectiveness of the proposed framework in improving the reliability and accuracy of vehicle lateral localization.},
  archive      = {J_TNNLS},
  author       = {Zhiyong Zheng and Xu Li and Jianxiao Zhu and Jianhua Yuan and Linqi Wu},
  doi          = {10.1109/TNNLS.2021.3116433},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3527-3537},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Highly robust vehicle lateral localization using multilevel robust network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved finite-time and fixed-time stable
synchronization of coupled discontinuous neural networks.
<em>TNNLS</em>, <em>34</em>(7), 3516–3526. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the finite-time and fixed-time synchronization of a class of coupled discontinuous neural networks, which can be viewed as a combination of the Hindmarsh–Rose model and the Kuramoto model. To this end, under the framework of Filippov solution, a new finite-time and fixed-time stable theorem is established for nonlinear systems whose right-hand sides may be discontinuous. Moreover, the high-precise settling time is given. Furthermore, by designing a discontinuous control law and using the theory of differential inclusions, some new sufficient conditions are derived to guarantee the synchronization of the addressed coupled networks achieved within a finite-time or fixed-time. These interesting results can be seemed as the supplement and expansion of the previous references. Finally, the derived theoretical results are supported by examples with numerical simulations.},
  archive      = {J_TNNLS},
  author       = {Qizhen Xiao and Hongliang Liu and Yinkun Wang},
  doi          = {10.1109/TNNLS.2021.3116320},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3516-3526},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An improved finite-time and fixed-time stable synchronization of coupled discontinuous neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Relaxed exponential stabilization for coupled memristive
neural networks with connection fault and multiple delays via optimized
elastic event-triggered mechanism. <em>TNNLS</em>, <em>34</em>(7),
3501–3515. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of relaxed exponential stabilization for coupled memristive neural networks (CMNNs) with connection fault and multiple delays via an optimized elastic event-triggered mechanism (OEEM). The connection fault of the two or some nodes can result in the connection fault of other nodes and cause iterative faults in the CMNNs. Therefore, the method of backup resources is considered to improve the fault-tolerant capability and survivability of the CMNNs. In order to improve the robustness of the event-triggered mechanism and enhance the ability of the event-triggered mechanism to process noise signals, the time-varying bounded noise threshold matrices, time-varying decreased exponential threshold functions, and adaptive functions are simultaneously introduced to design the OEEM. In addition, the appropriate Lyapunov–Krasovskii functionals (LKFs) with some improved delay-product-type terms are constructed, and the relaxed exponential stabilization and globally uniformly ultimately bounded (GUUB) conditions are derived for the CMNNs with connection fault and multiple delays by means of some inequality processing techniques. Finally, two numerical examples are provided to illustrate the effectiveness of the results.},
  archive      = {J_TNNLS},
  author       = {Xiangxiang Wang and Yongbin Yu and Jingye Cai and Shouming Zhong and Nijing Yang and Kaibo Shi and Kwabena Adu and Nyima Tashi},
  doi          = {10.1109/TNNLS.2021.3112068},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3501-3515},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Relaxed exponential stabilization for coupled memristive neural networks with connection fault and multiple delays via optimized elastic event-triggered mechanism},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiresolution discriminative mixup network for
fine-grained visual categorization. <em>TNNLS</em>, <em>34</em>(7),
3488–3500. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual categorization (FGVC) is a challenging task because there are many hard examples existing between fine-grained classes which differ subtly in particular local regions. To address this issue, many methods have recourse to high-resolution source images and others adopt effective regularization like “mixup” or “between class learning.” Despite their promising achievements, mixup tends to cause the manifold intrusion problem which would result in under-fitting and degradation of the model performance and high-resolution input inevitably leads to high computational costs. In view of this, we present a multiresolution discriminative mixup network (MRDMN). Different from standard mixup, the proposed discriminative mixup strategy mixes discriminative regions linearly instead of entire images to avoid manifold intrusion, which makes it learn the local detail features more effectively and contributes to more precise categorization. Furthermore, an innovative resolution-based distillation strategy is designed to transfer the multiresolution detail feature representations to a low-resolution network, which speeds up the testing and boosts the categorization accuracy simultaneously. Extensive experiments demonstrate that our proposed MRDMN remarkably outperforms most competitive approaches with less computation time on the CUB-200-2011, Stanford-Cars, Stanford-Dogs, Food-101, and iNaturalist 2017 datasets. The codes are in https://github.com/aztc/MRDMN .},
  archive      = {J_TNNLS},
  author       = {Kunran Xu and Rui Lai and Lin Gu and Yishi Li},
  doi          = {10.1109/TNNLS.2021.3112768},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3488-3500},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiresolution discriminative mixup network for fine-grained visual categorization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute augmented network embedding based on generative
adversarial nets. <em>TNNLS</em>, <em>34</em>(7), 3473–3487. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding is to learn low-dimensional representations of nodes while preserving necessary information for network analysis tasks. Though representations preserving both structure and attribute features have achieved in many real-world applications, learning these representations for networks with attribute information is difficult due to the heterogeneity between structure and attribute information. Many existing methods have been proposed to preserve explicit proximities between nodes, with optimization limited to node pairs with large structure and attribute proximities, which may lead to overfitting. To address the above problems, we adopt an attribute augmented network to represent attribute and structure information in a unified framework. Specifically, we study the problem of attribute augmented network embedding that exploits the strength of generative adversarial nets (ANGANs) in capturing the latent distribution of data to learn robust and informative representations of nodes. The ANGAN method obtains the low-dimensional representations of nodes through adversarial learning between the generative and discriminative models. The generative model approximates the underlying connectivity and attributes distributions of nodes by using the distributions generated from the learned representations. It is implemented by utilizing the properties of the attribute augmented network to improve the traditional Skip-gram model. The discriminative model is designed as a binary classifier to distinguish the truly connected node pairs from the generated ones. The pre-training algorithm and the teacher forcing approach are adopted to improve training efficiency and stability. Empirical results show that ANGAN generally outperforms state-of-the-art methods in various real-world applications, which demonstrates the effectiveness and generality of our method.},
  archive      = {J_TNNLS},
  author       = {Conghui Zheng and Li Pan and Peng Wu},
  doi          = {10.1109/TNNLS.2021.3116419},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3473-3487},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attribute augmented network embedding based on generative adversarial nets},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal tracking in switched systems with free final time
and fixed mode sequence using approximate dynamic programming.
<em>TNNLS</em>, <em>34</em>(7), 3460–3472. (<a
href="https://doi.org/10.1109/TNNLS.2021.3113801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal tracking in switched systems with fixed mode sequence and free final time is studied in this article. In the optimal control problem formulation, the switching times and the final time are treated as parameters. For solving the optimal control problem, approximate dynamic programming (ADP) is used. The ADP solution uses an inner loop to converge to the optimal policy at each time step. In order to decrease the computational burden of the solution, a new method is introduced, which uses evolving suboptimal policies (not the optimal policies), to learn the optimal solution. The effectiveness of the proposed solutions is evaluated through numerical simulations.},
  archive      = {J_TNNLS},
  author       = {Tohid Sardarmehni and Xingyong Song},
  doi          = {10.1109/TNNLS.2021.3113801},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3460-3472},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal tracking in switched systems with free final time and fixed mode sequence using approximate dynamic programming},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of sim-to-real transfer techniques applied to
reinforcement learning for bioinspired robots. <em>TNNLS</em>,
<em>34</em>(7), 3444–3459. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art reinforcement learning (RL) techniques have made innumerable advancements in robot control, especially in combination with deep neural networks (DNNs), known as deep reinforcement learning (DRL). In this article, instead of reviewing the theoretical studies on RL, which were almost fully completed several decades ago, we summarize some state-of-the-art techniques added to commonly used RL frameworks for robot control. We mainly review bioinspired robots (BIRs) because they can learn to locomote or produce natural behaviors similar to animals and humans. With the ultimate goal of practical applications in real world, we further narrow our review scope to techniques that could aid in sim-to-real transfer. We categorized these techniques into four groups: 1) use of accurate simulators; 2) use of kinematic and dynamic models; 3) use of hierarchical and distributed controllers; and 4) use of demonstrations. The purposes of these four groups of techniques are to supply general and accurate environments for RL training, improve sampling efficiency, divide and conquer complex motion tasks and redundant robot structures, and acquire natural skills. We found that, by synthetically using these techniques, it is possible to deploy RL on physical BIRs in actuality.},
  archive      = {J_TNNLS},
  author       = {Wei Zhu and Xian Guo and Dai Owaki and Kyo Kutsuzawa and Mitsuhiro Hayashibe},
  doi          = {10.1109/TNNLS.2021.3112718},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3444-3459},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of sim-to-real transfer techniques applied to reinforcement learning for bioinspired robots},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). High-fidelity permeability and porosity prediction using
deep learning with the self-attention mechanism. <em>TNNLS</em>,
<em>34</em>(7), 3429–3443. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate estimation of reservoir parameters (e.g., permeability and porosity) helps to understand the movement of underground fluids. However, reservoir parameters are usually expensive and time-consuming to obtain through petrophysical experiments of core samples, which makes a fast and reliable prediction method highly demanded. In this article, we propose a deep learning model that combines the 1-D convo- lutional layer and the bidirectional long short-term memory network to predict reservoir permeability and porosity. The mapping relationship between logging data and reservoir parameters is established by training a network with a combination of nonlinear and linear modules. Optimization algorithms, such as layer normalization, recurrent dropout, and early stopping, can help obtain a more accurate training model. Besides, the self-attention mechanism enables the network to better allocate weights to improve the prediction accuracy. The testing results of the well-trained network in blind wells of three different regions show that our proposed method is accurate and robust in the reservoir parameters prediction task.},
  archive      = {J_TNNLS},
  author       = {Liuqing Yang and Shoudong Wang and Xiaohong Chen and Wei Chen and Omar M. Saad and Xu Zhou and Nam Pham and Zhicheng Geng and Sergey Fomel and Yangkang Chen},
  doi          = {10.1109/TNNLS.2022.3157765},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3429-3443},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {High-fidelity permeability and porosity prediction using deep learning with the self-attention mechanism},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-attention fully convolutional DenseNets for automatic
salt segmentation. <em>TNNLS</em>, <em>34</em>(7), 3415–3428. (<a
href="https://doi.org/10.1109/TNNLS.2022.3175419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3-D salt segmentation is important for many research topics spanning from exploration geophysics to structural geology. In seismic exploration, 3-D salt segmentation is directly related to the velocity modeling building that affects many processing steps, such as seismic migration and full waveform inversion. Manually picking the salt boundary becomes prohibitively time-consuming when the data size is too large. Here, we develop a highly generalized fully convolutional DenseNet for automatic salt segmentation. A squeeze-and-excitation network is used as a self-attention mechanism for guiding the proposed network to extract the most significant information related to the salt signals and discard the others. The proposed framework is a supervised technique and shows robust performance when applied to a new dataset using transfer learning and a small amount of training data. We test the robustness of the proposed framework on the Kaggle TGS salt segmentation dataset. To demonstrate the generalization ability of the framework, we further apply the trained model to an independent dataset synthesized from the 3-D SEAM model. We apply transfer learning to finely tune the trained model from the TGS dataset using only a small percentage of data from the 3-D SEAM dataset and obtain satisfactory results.},
  archive      = {J_TNNLS},
  author       = {Omar M. Saad and Wei Chen and Fangxue Zhang and Liuqing Yang and Xu Zhou and Yangkang Chen},
  doi          = {10.1109/TNNLS.2022.3175419},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3415-3428},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-attention fully convolutional DenseNets for automatic salt segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A self-supervised deep learning method for seismic data
deblending using a blind-trace network. <em>TNNLS</em>, <em>34</em>(7),
3405–3414. (<a
href="https://doi.org/10.1109/TNNLS.2022.3188915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simultaneous-source technology for high-density seismic acquisition is a key solution to efficient seismic surveying. It is a cost-effective method when blended subsurface responses are recorded within a short time interval using multiple seismic sources. A following deblending process, however, is needed to separate signals contributed by individual sources. Recent advances in deep learning and its data-driven approach toward feature engineering have led to many new applications for a variety of seismic processing problems. It is still a challenge, though, to collect enough labeled data and avoid model overfitting and poor generalization performance over different datasets with a low resemblance from each other. In this article, we propose a novel self-supervised learning method to solve the deblending problem without labeled training datasets. Using a blind-trace deep neural network and a carefully crafted blending loss function, we demonstrate that the individual source-response pairs can be accurately separated under three different blended-acquisition designs.},
  archive      = {J_TNNLS},
  author       = {Shirui Wang and Wenyi Hu and Pengyu Yuan and Xuqing Wu and Qunshan Zhang and Prashanth Nadukandi and German Ocampo Botero and Jiefu Chen},
  doi          = {10.1109/TNNLS.2022.3188915},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3405-3414},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A self-supervised deep learning method for seismic data deblending using a blind-trace network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepSeg: Deep segmental denoising neural network for seismic
data. <em>TNNLS</em>, <em>34</em>(7), 3397–3404. (<a
href="https://doi.org/10.1109/TNNLS.2022.3205421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise attenuation is a crucial phase in seismic signal processing. Enhancing the signal-to-noise ratio (SNR) of registered seismic signals improves subsequent processing and, eventually, data analysis and interpretation. In this work, a novel noise reduction framework based on an intelligent deep convolutional neural network is proposed that works on segments of the time-frequency domain and, hence named as DeepSeg. The proposed network is efficient in learning sparse representation of the data simultaneously in the time-frequency domain and adaptively capturing seismic signals corrupted with noise. DeepSeg is able to achieve impressive denoising performance even when seismic signal shares common frequency band with noise. The proposed approach properly tackles a variety of correlated (color) and uncorrelated noise, and other nonseismic signals. DeepSeg can boost the SNR considerably even in extremely noisy environments with minimal changes to the signal of interest. The effectiveness of the proposed methodology is demonstrated in enhancing passive seismic event detection/denoising. However, there are other obvious applications of the DeepSeg in active and passive seismic fields, e.g., seismic imaging, preprocessing of ambient noise data, and microseismic event monitoring. It is worth pointing out here that the deep neural network is trained exclusively using synthetic seismic data, negating the need for real data during the training phase. Furthermore, the proposed setup is general and its potential applications are not confined to passive event denoising or even seismic. The method proposed is also adaptable to other diverse signals in different settings, like medical images/signals [magnetic resonance imaging (MRI), electroencephalogram (EEG) signals, electrocardiograms (ECG) signals, and retinal images, to name a few], radar signals, speech signals, fault detection in electrical/mechanical systems, daily life images, etc. Experiments on synthetic and real seismic data reveal the efficacy and supremacy of the proposed method in terms of SNR improvement and required training data when compared to the state-of-the-art deep neural network-based denoising technique.},
  archive      = {J_TNNLS},
  author       = {Naveed Iqbal},
  doi          = {10.1109/TNNLS.2022.3205421},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3397-3404},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DeepSeg: Deep segmental denoising neural network for seismic data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond correlations: Deep learning for seismic
interferometry. <em>TNNLS</em>, <em>34</em>(7), 3385–3396. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passive seismic interferometry is a vastly generalized blind deconvolution question, where different paths through the Earth correspond to different channels called Green’s functions; the sources are completely incoherent and not shared by the channels, and the question is to estimate paths (channels) that are not present in the dataset. SI, turning noise to signal, has numerous applications, from monitoring industrial activities to crustal structure investigation. No standard method of signal processing will solve SI. Instead, domain scientists resort to a simple cross-correlation operation, a.k.a. correlogram, which can retrieve the Green’s function directly, but only under restrictive assumptions of ergodicity (energy equipartitioning) of the random process generating the seismic source. However, in practice, correlograms are not equal to the empirical Green’s function, because these assumptions are generally far from being satisfied in realistic situations. In the framework of supervised learning, we propose to train deep neural networks (NNs) to overcome two limitations of correlation-based SI: the temporal limitation of passive recordings and the spatial limitation of the random source distribution. Deep NNs are trained to implicitly find the relationship between the empirical Green’s functions and the correlograms and then used to extract the correct Green’s functions from ambient noise. The input of the network is correlograms (a virtual shot gather), and the desired output is the empirical Green’s function (the active shot gather). The NN can often retrieve Green’s functions from 5-min passive recordings with acceptable accuracy in our synthetic example. Although an exact estimation of the source locations may not be necessary, a prior knowledge of the source directionality (through a preliminary beamforming step) is helpful when training the NN to mitigate the challenges associated with inhomogeneous source distributions (directional wave fields). In this work, all the numerical examples are based on the retrieval of P-wave reflections in the exploration scale and are conducted on synthetic data. We use a modified ResNet in our numerical experiments.},
  archive      = {J_TNNLS},
  author       = {Hongyu Sun and Laurent Demanet},
  doi          = {10.1109/TNNLS.2022.3172385},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3385-3396},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Beyond correlations: Deep learning for seismic interferometry},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-supervised deep learning approach for blind denoising
and waveform coherence enhancement in distributed acoustic sensing data.
<em>TNNLS</em>, <em>34</em>(7), 3371–3384. (<a
href="https://doi.org/10.1109/TNNLS.2021.3132832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fiber-optic distributed acoustic sensing (DAS) is an emerging technology for vibration measurements with numerous applications in seismic signal analysis, including microseismicity detection, ambient noise tomography, earthquake source characterization, and active source seismology. Using laser-pulse techniques, DAS turns (commercial) fiber-optic cables into seismic arrays with a spatial sampling density of the order of meters and a time sampling rate up to one thousand Hertz. The versatility of DAS enables dense instrumentation of traditionally inaccessible domains, such as urban, glaciated, and submarine environments. This in turn opens up novel applications such as traffic density monitoring and maritime vessel tracking. However, these new environments also introduce new challenges in handling various types of recorded noise, impeding the application of traditional data analysis workflows. In order to tackle the challenges posed by noise, new denoising techniques need to be explored that are tailored to DAS. In this work, we propose a Deep Learning approach that leverages the spatial density of DAS measurements to remove spatially incoherent noise with unknown characteristics. This approach is entirely self-supervised, so no noise-free ground truth is required, and it makes no assumptions regarding the noise characteristics other than that it is spatio-temporally incoherent. We apply our approach to both synthetic and real-world DAS data to demonstrate its excellent performance, even when the signals of interest are well below the noise level. Our proposed methods can be readily incorporated into conventional data processing workflows to facilitate subsequent seismological analyses.},
  archive      = {J_TNNLS},
  author       = {Martijn van den Ende and Itzhak Lior and Jean-Paul Ampuero and Anthony Sladen and André Ferrari and Cédric Richard},
  doi          = {10.1109/TNNLS.2021.3132832},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3371-3384},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A self-supervised deep learning approach for blind denoising and waveform coherence enhancement in distributed acoustic sensing data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physics-guided generative adversarial networks for sea
subsurface temperature prediction. <em>TNNLS</em>, <em>34</em>(7),
3357–3370. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sea subsurface temperature, an essential component of aquatic wildlife, underwater dynamics, and heat transfer with the sea surface, is affected by global warming in climate change. Existing research is commonly based on either physics-based numerical models or data-based models. Physical modeling and machine learning are traditionally considered as two unrelated fields for the sea subsurface temperature prediction task, with very different scientific paradigms (physics-driven and data-driven). However, we believe that both methods are complementary to each other. Physical modeling methods can offer the potential for extrapolation beyond observational conditions, while data-driven methods are flexible in adapting to data and are capable of detecting unexpected patterns. The combination of both approaches is very attractive and offers potential performance improvement. In this article, we propose a novel framework based on a generative adversarial network (GAN) combined with a numerical model to predict sea subsurface temperature. First, a GAN-based model is used to learn the simplified physics between the surface temperature and the target subsurface temperature in the numerical model. Then, observation data are used to calibrate the GAN-based model parameters to obtain a better prediction. We evaluate the proposed framework by predicting daily sea subsurface temperature in the South China Sea. Extensive experiments demonstrate the effectiveness of the proposed framework compared to existing state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yuxin Meng and Eric Rigall and Xueen Chen and Feng Gao and Junyu Dong and Sheng Chen},
  doi          = {10.1109/TNNLS.2021.3123968},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3357-3370},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Physics-guided generative adversarial networks for sea subsurface temperature prediction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for deep learning emulation of numerical models
with a case study in satellite remote sensing. <em>TNNLS</em>,
<em>34</em>(7), 3345–3356. (<a
href="https://doi.org/10.1109/TNNLS.2022.3169958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical models based on physics represent the state of the art in Earth system modeling and comprise our best tools for generating insights and predictions. Despite rapid growth in computational power, the perceived need for higher model resolutions overwhelms the latest generation computers, reducing the ability of modelers to generate simulations for understanding parameter sensitivities and characterizing variability and uncertainty. Thus, surrogate models are often developed to capture the essential attributes of the full-blown numerical models. Recent successes of machine learning methods, especially deep learning (DL), across many disciplines offer the possibility that complex nonlinear connectionist representations may be able to capture the underlying complex structures and nonlinear processes in Earth systems. A difficult test for DL-based emulation, which refers to function approximation of numerical models, is to understand whether they can be comparable to traditional forms of surrogate models in terms of computational efficiency while simultaneously reproducing model results in a credible manner. A DL emulation that passes this test may be expected to perform even better than simple models with respect to capturing complex processes and spatiotemporal dependencies. Here, we examine, with a case study in satellite-based remote sensing, the hypothesis that DL approaches can credibly represent the simulations from a surrogate model with comparable computational efficiency. Our results are encouraging in that the DL emulation reproduces the results with acceptable accuracy and often even faster performance. We discuss the broader implications of our results in light of the pace of improvements in high-performance implementations of DL and the growing desire for higher resolution simulations in the Earth sciences.},
  archive      = {J_TNNLS},
  author       = {Kate Duffy and Thomas J. Vandal and Weile Wang and Ramakrishna R. Nemani and Auroop R. Ganguly},
  doi          = {10.1109/TNNLS.2022.3169958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3345-3356},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A framework for deep learning emulation of numerical models with a case study in satellite remote sensing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A kernel-based multi-featured rock modeling and detection
framework for a mars rover. <em>TNNLS</em>, <em>34</em>(7), 3335–3344.
(<a href="https://doi.org/10.1109/TNNLS.2021.3131206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents two kernel-based rock detection methods for a Mars rover. Rock detection on planetary surfaces is particularly pivotal for planetary vehicles regarding navigation and obstacle avoidance. However, the diverse morphologies of Martian rocks, the sparsity of pixel-wise features, and engineering constraints are great challenges to current pixel-wise object detection methods, resulting in inaccurate and delayed object location and recognition. We therefore propose a region-wise rock detection framework and design two detection algorithms, kernel principle component analysis (KPCA)-based rock detection (KPRD) and kernel low-rank representation (KLRR)-based rock detection (KLRD), using hypotheses of feature and sub-spatial separability. KPRD is based on KPCA and is expert in real-time detection yet with less accurate performance. KLRD is based on KPRD with KLRR which can generate more precise rock detection results with less delay. To validate the efficiency of the proposed methods, we build a small-scale Martian rock dataset, MarsData, containing various rocks. Preliminary experimental results show that our methods are efficient in dealing with complex images containing rocks, shadows, and gravel. The code and data are available at: https://github.com/CVIR-Lab/MarsData .},
  archive      = {J_TNNLS},
  author       = {Xueming Xiao and Meibao Yao and Haiqiang Liu and Jiake Wang and Lei Zhang and Yuegang Fu},
  doi          = {10.1109/TNNLS.2021.3131206},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3335-3344},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A kernel-based multi-featured rock modeling and detection framework for a mars rover},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Class-wise subspace alignment-based unsupervised adaptive
land cover classification in scene-level using deep siamese network.
<em>TNNLS</em>, <em>34</em>(7), 3323–3334. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an unsupervised domain adaptation strategy has been investigated using a deep Siamese neural network in scene-level land cover classification using remotely sensed images. At the onset, the soft class label and probability scores of each target sample have been obtained using a pretrained model of a deep convolutional neural network. Thereafter, a semiautomatic threshold selection algorithm along with a graph-based approach has been explored to obtain the “most-confident” target samples. Furthermore, the deep Siamese network has been incorporated by training the source and “most-confident” target samples to generate the classwise cross domain common subspace. To assess the effectiveness of the proposed framework, experiments are carried out using three aerial image datasets. The results are found to be encouraging for the proposed scheme in comparison with the other state-of-art techniques.},
  archive      = {J_TNNLS},
  author       = {Indrajit Kalita and Moumita Roy},
  doi          = {10.1109/TNNLS.2022.3149292},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3323-3334},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Class-wise subspace alignment-based unsupervised adaptive land cover classification in scene-level using deep siamese network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aerial images meet crowdsourced trajectories: A new approach
to robust road extraction. <em>TNNLS</em>, <em>34</em>(7), 3308–3322.
(<a href="https://doi.org/10.1109/TNNLS.2022.3141821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land remote-sensing analysis is a crucial research in earth science. In this work, we focus on a challenging task of land analysis, i.e., automatic extraction of traffic roads from remote-sensing data, which has widespread applications in urban development and expansion estimation. Nevertheless, conventional methods either only utilized the limited information of aerial images, or simply fused multimodal information (e.g., vehicle trajectories), thus cannot well recognize unconstrained roads. To facilitate this problem, we introduce a novel neural network framework termed cross-modal message propagation network (CMMPNet), which fully benefits the complementary different modal data (i.e., aerial images and crowdsourced trajectories). Specifically, CMMPNet is composed of two deep autoencoders for modality-specific representation learning and a tailor-designed dual enhancement module for cross-modal representation refinement. In particular, the complementary information of each modality is comprehensively extracted and dynamically propagated to enhance the representation of another modality. Extensive experiments on three real-world benchmarks demonstrate the effectiveness of our CMMPNet for robust road extraction benefiting from blending different modal data, either using image and trajectory data or image and light detection and ranging (LiDAR) data. From the experimental results, we observe that the proposed approach outperforms current state-of-the-art methods by large margins. Our source code is resealed on the project page http://lingboliu.com/multimodal_road_extraction.html .},
  archive      = {J_TNNLS},
  author       = {Lingbo Liu and Zewei Yang and Guanbin Li and Kuo Wang and Tianshui Chen and Liang Lin},
  doi          = {10.1109/TNNLS.2022.3141821},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3308-3322},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Aerial images meet crowdsourced trajectories: A new approach to robust road extraction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A vision transformer model for convolution-free multilabel
classification of satellite imagery in deforestation monitoring.
<em>TNNLS</em>, <em>34</em>(7), 3299–3307. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the dynamics of deforestation and land uses of neighboring areas is of vital importance for the design and development of appropriate forest conservation and management policies. In this article, we approach deforestation as a multilabel classification (MLC) problem in an endeavor to capture the various relevant land uses from satellite images. To this end, we propose a multilabel vision transformer model, ForestViT, which leverages the benefits of the self-attention mechanism, obviating any convolution operations involved in commonly used deep learning models utilized for deforestation detection. Experimental evaluation in open satellite imagery datasets yields promising results in the case of MLC, particularly for imbalanced classes, and indicates ForestViT’s superiority compared with well-established convolutional structures (ResNET, VGG, DenseNet, and ModileNet neural networks). This superiority is more evident for minority classes.},
  archive      = {J_TNNLS},
  author       = {Maria Kaselimi and Athanasios Voulodimos and Ioannis Daskalopoulos and Nikolaos Doulamis and Anastasios Doulamis},
  doi          = {10.1109/TNNLS.2022.3144791},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3299-3307},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A vision transformer model for convolution-free multilabel classification of satellite imagery in deforestation monitoring},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StateNet: Deep state learning for robust feature matching of
remote sensing images. <em>TNNLS</em>, <em>34</em>(7), 3284–3298. (<a
href="https://doi.org/10.1109/TNNLS.2021.3120768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seeking good correspondences between two images is a fundamental and challenging problem in the remote sensing (RS) community, and it is a critical prerequisite in a wide range of feature-based visual tasks. In this article, we propose a flexible and general deep state learning network for both rigid and nonrigid feature matching, which provides a mechanism to change the state of matches into latent canonical forms, thereby weakening the degree of randomness in matching patterns. Different from the current conventional strategies (i.e., imposing a global geometric constraint or designing additional handcrafted descriptor), the proposed StateNet is designed to perform alternating two steps: 1) recalibrates matchwise feature responses in the spatial domain and 2) leverages the spatially local correlation across two sets of feature points for transformation update. For this purpose, our network contains two novel operations: adaptive dual-aggregation convolution (ADAConv) and point rendering layer (PRL). These two operations are differentiable, so our network can be inserted into the existing classification architecture to reduce the cost of establishing reliable correspondences. To demonstrate the robustness and universality of our approach, extensive experiments on various real image pairs for feature matching are conducted. Experiments reveal the superiority of our StateNet significantly over the state-of-the-art alternatives.},
  archive      = {J_TNNLS},
  author       = {Jiaxuan Chen and Shuang Chen and Xiaoxian Chen and Yang Yang and Yujing Rao},
  doi          = {10.1109/TNNLS.2021.3120768},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3284-3298},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {StateNet: Deep state learning for robust feature matching of remote sensing images},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An end-to-end framework for joint denoising and
classification of hyperspectral images. <em>TNNLS</em>, <em>34</em>(7),
3269–3283. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising and classification are typically conducted separately and sequentially according to their respective objectives. In such a setup, where the two tasks are decoupled, the denoising operation does not optimally serve the classification task and sometimes even deteriorates it. We introduce here a unified deep learning framework for joint denoising and classification of high-dimensional images, and we particularly apply it in the framework of hyperspectral imaging. Earlier works on joint image denoising and classification are very scarce, and to the best of our knowledge, no deep learning models were proposed or studied yet for this type of multitask image processing. A key component in our joint learning model is a compound loss function, designed in such a way that the denoising and classification operations benefit each other iteratively during the learning process. Hyperspectral images (HSIs) are particularly challenging for both denoising and classification due to their high dimensionality and varying noise statistics across the bands. We argue that a well-designed end-to-end deep learning framework for joint denoising and classification is superior to current deep learning approaches for processing HSI data, and we substantiate this by results on real HSI images in remote sensing. We experimentally show that the proposed joint learning framework substantially improves the classification performance compared to the common deep learning approaches in HSI processing, and as a by-product, the denoising results are enhanced as well, especially in terms of the semantic content, benefiting from the classification.},
  archive      = {J_TNNLS},
  author       = {Xian Li and Mingli Ding and Yanfeng Gu and Aleksandra Pižurica},
  doi          = {10.1109/TNNLS.2023.3264587},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3269-3283},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An end-to-end framework for joint denoising and classification of hyperspectral images},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DnRCNN: Deep recurrent convolutional neural network for HSI
destriping. <em>TNNLS</em>, <em>34</em>(7), 3255–3268. (<a
href="https://doi.org/10.1109/TNNLS.2022.3142425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spite of achieving promising results in hyperspectral image (HSI) restoration, deep-learning-based methodologies still face the problem of spectral or spatial information loss due to neglecting the inner correlation of HSI. To address this issue, we propose an innovative deep recurrent convolution neural network (DnRCNN) model for HSI destriping. To the best of our knowledge, this is the first study on HSI destriping from the perspective of inner band and interband correlation explorations with the recurrent convolution neural network. In the novel DnRCNN, a selective recurrent memory unit (SRMU) is designed to respectively extract the correlative features involved in spectral and spatial domains. Moreover, an innovative recurrent fusion (RF) strategy incorporated with group concatenation is further proposed to remove strip noise and preserve scene details using the complementary features from SRMU. Experimental results on extensive HSI datasets validated that the proposed method achieves a new state-of-the-art (SOTA) HSI destriping performance.},
  archive      = {J_TNNLS},
  author       = {Juntao Guan and Rui Lai and Huanan Li and Yintang Yang and Lin Gu},
  doi          = {10.1109/TNNLS.2022.3142425},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3255-3268},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DnRCNN: Deep recurrent convolutional neural network for HSI destriping},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal interpolation of geostationary satellite imagery
with optical flow. <em>TNNLS</em>, <em>34</em>(7), 3245–3254. (<a
href="https://doi.org/10.1109/TNNLS.2021.3101742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications of satellite data in areas such as weather tracking and modeling, ecosystem monitoring, wildfire detection, and land-cover change are heavily dependent on the tradeoffs to spatial, spectral, and temporal resolutions of observations. In weather tracking, high-frequency temporal observations are critical and used to improve forecasts, study severe events, and extract atmospheric motion, among others. However, while the current generation of geostationary (GEO) satellites has hemispheric coverage at 10–15-min intervals, higher temporal frequency observations are ideal for studying mesoscale severe weather events. In this work, we present a novel application of deep learning-based optical flow to temporal upsampling of GEO satellite imagery. We apply this technique to 16 bands of the GOES-R/Advanced Baseline Imager mesoscale dataset to temporally enhance full-disk hemispheric snapshots of different spatial resolutions from 10 to 1 min. Experiments show the effectiveness of task-specific optical flow and multiscale blocks for interpolating high-frequency severe weather events relative to bilinear and global optical flow baselines. Finally, we demonstrate strong performance in capturing variability during convective precipitation events.},
  archive      = {J_TNNLS},
  author       = {Thomas J. Vandal and Ramakrishna R. Nemani},
  doi          = {10.1109/TNNLS.2021.3101742},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3245-3254},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Temporal interpolation of geostationary satellite imagery with optical flow},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial special issue on deep learning for earth and
planetary geosciences. <em>TNNLS</em>, <em>34</em>(7), 3242–3244. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Earth and planetary geosciences are essential for understanding and addressing many societal challenges and scientific questions. Increased availability of geoscience data creates an opportunity for deep learning to advance the methods and scientific understanding for tackling these challenges. However, the complex characteristics of geoscience problems and datasets necessitate the development of novel approaches and frameworks. This Special Issue aims at collecting new ideas and deep learning formulations to gain new earth and planetary insights. The contributions cover a wide range of topics, such as satellite and hyperspectral imaging, land monitoring, geophysical imaging, and subsurface analysis.},
  archive      = {J_TNNLS},
  author       = {Antonio Paiva and Weichang Li and Chris A. Mattmann and Youzuo Lin and Maarten V. De Hoop},
  doi          = {10.1109/TNNLS.2023.3282150},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3242-3244},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial special issue on deep learning for earth and planetary geosciences},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameter-free loss for class-imbalanced deep learning in
image classification. <em>TNNLS</em>, <em>34</em>(6), 3234–3240. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current state-of-the-art class-imbalanced loss functions for deep models require exhaustive tuning on hyperparameters for high model performance, resulting in low training efficiency and impracticality for nonexpert users. To tackle this issue, a parameter-free loss (PF-loss) function is proposed, which works for both binary and multiclass-imbalanced deep learning for image classification tasks. PF-loss provides three advantages: 1) training time is significantly reduced due to NO tuning on hyperparameter(s); 2) it dynamically pays more attention on minority classes (rather than outliers compared to the existing loss functions) with NO hyperparameters in the loss function; and 3) higher accuracy can be achieved since it adapts to the changes of data distribution in each mini-batch instead of the fixed hyperparameters in the existing methods during training, especially when the data are highly skewed. Experimental results on some classical image datasets with different imbalance ratios (IR, up to 200) show that PF-loss reduces the training time down to 1/148 of that spent by compared state-of-the-art losses and simultaneously achieves comparable or even higher accuracy in terms of both G-mean and area under receiver operating characteristic (ROC) curve (AUC) metrics, especially when the data are highly skewed.},
  archive      = {J_TNNLS},
  author       = {Jie Du and Yanhong Zhou and Peng Liu and Chi-Man Vong and Tianfu Wang},
  doi          = {10.1109/TNNLS.2021.3110885},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3234-3240},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameter-free loss for class-imbalanced deep learning in image classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synchronization and control for multiweighted and directed
complex networks. <em>TNNLS</em>, <em>34</em>(6), 3226–3233. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of complex networks with multiweights (CNMWs) has been a hot topic recently. For a network with a single weight, previous studies have shown that they can promote synchronization, but for CNMWs, there are no rigorous analyses about the role of coupling matrices. In this brief, the complex network is allowed to be directed, which is the main difference with previous studies and may make the synchronization analysis difficult for multiple couplings. At first, we prove that if the inner coupling matrices are all diagonal, then synchronization can be realized only if the weighted sum (or union) of multiple coupling matrices is strongly connected, which bridges the gap between single-weighted and multiweighted networks. Moreover, we also consider the case that inner coupling matrices are positive definite but not diagonal. We design two techniques for this hard problem. One technique is to decompose inner coupling matrices into diagonal matrices and residual matrices. The other one is to measure the similarity between outer coupling matrices. In virtue of the normalized left eigenvectors (NLEVecs) corresponding to the zero eigenvalue of coupling matrices, we prove that if the Chebyshev distance between NLEVec is less than some value, defined as the allowable deviation bound, then the synchronization and control will be realized with sufficiently large coupling strengths. Furthermore, adaptive rules are also designed for coupling strength.},
  archive      = {J_TNNLS},
  author       = {Xiwei Liu},
  doi          = {10.1109/TNNLS.2021.3110681},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3226-3233},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization and control for multiweighted and directed complex networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stability analysis of delayed recurrent neural networks via
a quadratic matrix convex combination approach. <em>TNNLS</em>,
<em>34</em>(6), 3220–3225. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief addresses the stability analysis problem of a class of delayed recurrent neural networks (DRNNs). In previously published studies, the slope information of activation function (SIAF) is just reflected in three slope information matrices, i.e., the upper and lower boundary matrices and the maximum norm matrix. In practice, there are $2^{n}$ possible combination cases on the slope information matrices. To exploit more information about SIAF, first, an activation function separation method is proposed to derive $n$ slope-information-based uncertainties (SIBUs) containing SIAF; second, a quadratic matrix convex combination approach is proposed to dispose $n$ SIBUs using $2^{n}$ combination slope information matrices. Third, a stability criterion with less conservatism is established based on the proposed approach. Finally, two simulation examples are used to testify the validity of theoretical results.},
  archive      = {J_TNNLS},
  author       = {Shasha Xiao and Zhanshan Wang and Yufeng Tian},
  doi          = {10.1109/TNNLS.2021.3107427},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3220-3225},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability analysis of delayed recurrent neural networks via a quadratic matrix convex combination approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LMFFNet: A well-balanced lightweight network for fast and
accurate semantic segmentation. <em>TNNLS</em>, <em>34</em>(6),
3205–3219. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time semantic segmentation is widely used in autonomous driving and robotics. Most previous networks achieved great accuracy based on a complicated model involving mass computing. The existing lightweight networks generally reduce the parameter sizes by sacrificing the segmentation accuracy. It is critical to balance the parameters and accuracy for real-time semantic segmentation. In this article, we propose a lightweight multiscale-feature-fusion network (LMFFNet) mainly composed of three types of components: split-extract-merge bottleneck (SEM-B) block, feature fusion module (FFM), and multiscale attention decoder (MAD), where the SEM-B block extracts sufficient features with fewer parameters. FFMs fuse multiscale semantic features to effectively improve the segmentation accuracy and the MAD well recovers the details of the input images through the attention mechanism. Without pretraining, LMFFNet-3-8 achieves 75.1\% mean intersection over union (mIoU) with 1.4 M parameters at 118.9 frames/s using RTX 3090 GPU. More experiments are investigated extensively on various resolutions on other three datasets of CamVid, KITTI, and WildDash2. The experiments verify that the proposed LMFFNet model makes a decent tradeoff between segmentation accuracy and inference speed for real-time tasks. The source code is publicly available at https://github.com/Greak-1124/LMFFNet .},
  archive      = {J_TNNLS},
  author       = {Min Shi and Jialin Shen and Qingming Yi and Jian Weng and Zunkai Huang and Aiwen Luo and Yicong Zhou},
  doi          = {10.1109/TNNLS.2022.3176493},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3205-3219},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LMFFNet: A well-balanced lightweight network for fast and accurate semantic segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal adaptive control of uncertain nonlinear
continuous-time systems with input and state delays. <em>TNNLS</em>,
<em>34</em>(6), 3195–3204. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an actor-critic neural network (NN)-based online optimal adaptive regulation of a class of nonlinear continuous-time systems with known state and input delays and uncertain system dynamics is introduced. The temporal difference error (TDE), which is dependent upon state and input delays, is derived using actual and estimated value function and via integral reinforcement learning. The NN weights of the critic are tuned at every sampling instant as a function of the instantaneous integral TDE. A novel identifier, which is introduced to estimate the control coefficient matrices, is utilized to obtain the estimated control policy. The boundedness of the state vector, critic NN weights, identification error, and NN identifier weights are shown through the Lyapunov analysis. Simulation results are provided to illustrate the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Rohollah Moghadam and Sarangapani Jagannathan},
  doi          = {10.1109/TNNLS.2021.3112566},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3195-3204},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal adaptive control of uncertain nonlinear continuous-time systems with input and state delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge distillation classifier generation network for
zero-shot learning. <em>TNNLS</em>, <em>34</em>(6), 3183–3194. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a conceptually simple but effective framework called knowledge distillation classifier generation network (KDCGN) for zero-shot learning (ZSL), where the learning agent requires recognizing unseen classes that have no visual data for training. Different from the existing generative approaches that synthesize visual features for unseen classifiers’ learning, the proposed framework directly generates classifiers for unseen classes conditioned on the corresponding class-level semantics. To ensure the generated classifiers to be discriminative to the visual features, we borrow the knowledge distillation idea to both supervise the classifier generation and distill the knowledge with, respectively, the visual classifiers and soft targets trained from a traditional classification network. Under this framework, we develop two, respectively, strategies, i.e., class augmentation and semantics guidance, to facilitate the supervision process from the perspectives of improving visual classifiers. Specifically, the class augmentation strategy incorporates some additional categories to train the visual classifiers, which regularizes the visual classifier weights to be compact, under supervision of which the generated classifiers will be more discriminative. The semantics-guidance strategy encodes the class semantics into the visual classifiers, which would facilitate the supervision process by minimizing the differences between the generated and the real-visual classifiers. To evaluate the effectiveness of the proposed framework, we have conducted extensive experiments on five datasets in image classification, i.e., AwA1, AwA2, CUB, FLO, and APY. Experimental results show that the proposed approach performs best in the traditional ZSL task and achieves a significant performance improvement on four out of the five datasets in the generalized ZSL task.},
  archive      = {J_TNNLS},
  author       = {Yunlong Yu and Bin Li and Zhong Ji and Jungong Han and Zhongfei Zhang},
  doi          = {10.1109/TNNLS.2021.3112229},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3183-3194},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge distillation classifier generation network for zero-shot learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DIET-SNN: A low-latency spiking neural network with direct
input encoding and leakage and threshold optimization. <em>TNNLS</em>,
<em>34</em>(6), 3174–3182. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bioinspired spiking neural networks (SNNs), operating with asynchronous binary signals (or spikes) distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. The state-of-the-art SNNs suffer from high inference latency, resulting from inefficient input encoding and suboptimal settings of the neuron parameters (firing threshold and membrane leak). We propose DIET-SNN, a low-latency deep spiking network trained with gradient descent to optimize the membrane leak and the firing threshold along with other network parameters (weights). The membrane leak and threshold of each layer are optimized with end-to-end backpropagation to achieve competitive accuracy at reduced latency. The input layer directly processes the analog pixel values of an image without converting it to spike train. The first convolutional layer converts analog inputs into spikes where leaky-integrate-and-fire (LIF) neurons integrate the weighted inputs and generate an output spike when the membrane potential crosses the trained firing threshold. The trained membrane leak selectively attenuates the membrane potential, which increases activation sparsity in the network. The reduced latency combined with high activation sparsity provides massive improvements in computational efficiency. We evaluate DIET-SNN on image classification tasks from CIFAR and ImageNet datasets on VGG and ResNet architectures. We achieve top-1 accuracy of 69\% with five timesteps (inference latency) on the ImageNet dataset with $12\times $ less compute energy than an equivalent standard artificial neural network (ANN). In addition, DIET-SNN performs 20– $500\times $ faster inference compared to other state-of-the-art SNN models.},
  archive      = {J_TNNLS},
  author       = {Nitin Rathi and Kaushik Roy},
  doi          = {10.1109/TNNLS.2021.3111897},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3174-3182},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DIET-SNN: A low-latency spiking neural network with direct input encoding and leakage and threshold optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven bipartite formation for a class of nonlinear
MIMO multiagent systems. <em>TNNLS</em>, <em>34</em>(6), 3161–3173. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bipartite formation control for the nonlinear discrete-time multiagent systems with signed digraph is considered in this article, in which the dynamics of the agents are completely unknown and multi-input multi-output (MIMO). First, the unknown nonlinear dynamic is converted into the compact-form dynamic linearization (CFDL) data model with a pseudo-Jacobian matrix (PJM). Based on the structurally balanced signed graph, a distance-based formation term is constructed and a bipartite formation model-free adaptive control (MFAC) protocol is designed. By employing the measured input and output data of the agents, the theoretical analysis is developed to prove the bounded-input bounded-output stability and the asymptotic convergence of the formation tracking error. Finally, the effectiveness of the proposed protocol is verified by two numerical examples.},
  archive      = {J_TNNLS},
  author       = {Jiaqi Liang and Xuhui Bu and Lizhi Cui and Zhongsheng Hou},
  doi          = {10.1109/TNNLS.2021.3111893},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3161-3173},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven bipartite formation for a class of nonlinear MIMO multiagent systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Training provably robust models by polyhedral envelope
regularization. <em>TNNLS</em>, <em>34</em>(6), 3146–3160. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training certifiable neural networks enables us to obtain models with robustness guarantees against adversarial attacks. In this work, we introduce a framework to obtain a provable adversarial-free region in the neighborhood of the input data by a polyhedral envelope, which yields more fine-grained certified robustness than existing methods. We further introduce polyhedral envelope regularization (PER) to encourage larger adversarial-free regions and thus improve the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks of different architectures and with general activation functions. Compared with state of the art, PER has negligible computational overhead; it achieves better robustness guarantees and accuracy on the clean data in various settings.},
  archive      = {J_TNNLS},
  author       = {Chen Liu and Mathieu Salzmann and Sabine Süsstrunk},
  doi          = {10.1109/TNNLS.2021.3111892},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3146-3160},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Training provably robust models by polyhedral envelope regularization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COMIRE: A consistence-based mislabeled instances removal
method. <em>TNNLS</em>, <em>34</em>(6), 3135–3145. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training neural network classifiers (NNCs) usually requires all instances to be correctly labeled, which is difficult and/or expensive to satisfy in some practical applications. When label noise is present, mislabeled data will severely mislead the training of NNCs, resulting in poor generalization performance. In this work, we address the label noise issue by removing mislabeled instances from the training data. A COnsistence-based Mislabeled Instances REmoval (COMIRE) method is proposed. The main idea is based on the observation that during the training of the NNC, the training loss and the model’s prediction uncertainty of correctly labeled instances show similar trends, while those of mislabeled instances have quite different trends. Thus, the consistency between the two trends can be used to distinguish correctly labeled instances from mislabeled ones. On this basis, an iteration scheme is introduced to further increase the separability between the two types of data. Experimental results show that COMIRE can effectively identify the mislabeled instances. Moreover, the classification performance is significantly improved after removing the identified instances from the noisy training data.},
  archive      = {J_TNNLS},
  author       = {Xiaokun Pu and Chunguang Li and Hui-Liang Shen},
  doi          = {10.1109/TNNLS.2021.3111871},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3135-3145},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {COMIRE: A consistence-based mislabeled instances removal method},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intermittent learning through operant conditioning for
cyber-physical systems. <em>TNNLS</em>, <em>34</em>(6), 3124–3134. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel scheme, namely, an intermittent learning scheme based on Skinner’s operant conditioning techniques that approximates the optimal policy while decreasing the usage of the communication buses transferring information. While traditional reinforcement learning schemes continuously evaluate and subsequently improve, every action taken by a specific learning agent based on received reinforcement signals, this form of continuous transmission of reinforcement signals and policy improvement signals can cause overutilization of the system’s inherently limited resources. Moreover, the highly complex nature of the operating environment for cyber-physical systems (CPSs) creates a gap for malicious individuals to corrupt the signal transmissions between various components. The proposed schemes will increase uncertainty in the learning rate and the extinction rate of the acquired behavior of the learning agents. In this article, we investigate the use of fixed/variable interval and fixed/variable ratio schedules in CPSs along with their rate of success and loss in their optimal behavior incurred during intermittent learning. Simulation results show the efficacy of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Prachi Pratyusha Sahoo and Aris Kanellopoulos and Kyriakos G. Vamvoudakis},
  doi          = {10.1109/TNNLS.2021.3111826},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3124-3134},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intermittent learning through operant conditioning for cyber-physical systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian matrix factorization for semibounded data.
<em>TNNLS</em>, <em>34</em>(6), 3111–3123. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian non-negative matrix factorization (BNMF) has been widely used in different applications. In this article, we propose a novel BNMF technique dedicated to semibounded data where each entry of the observed matrix is supposed to follow an Inverted Beta distribution. The model has two parameter matrices with the same size as the observation matrix which we factorize into a product of excitation and basis matrices. Entries of the corresponding basis and excitation matrices follow a Gamma prior. To estimate the parameters of the model, variational Bayesian inference is used. A lower bound approximation for the objective function is used to find an analytically tractable solution for the model. An online extension of the algorithm is also proposed for more scalability and to adapt to streaming data. The model is evaluated on five different applications: part-based decomposition, collaborative filtering, market basket analysis, transactions prediction and items classification, topic mining, and graph embedding on biomedical networks.},
  archive      = {J_TNNLS},
  author       = {Oumayma Dalhoumi and Nizar Bouguila and Manar Amayri and Wentao Fan},
  doi          = {10.1109/TNNLS.2021.3111824},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3111-3123},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bayesian matrix factorization for semibounded data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatiotemporal sequence prediction with point processes and
self-organizing decision trees. <em>TNNLS</em>, <em>34</em>(6),
3097–3110. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the spatiotemporal prediction problem and introduce a novel point-process-based prediction algorithm. Spatiotemporal prediction is extensively studied in machine learning literature due to its critical real-life applications, such as crime, earthquake, and social event prediction. Despite these thorough studies, specific problems inherent to the application domain are not yet fully explored. Here, we address the nonstationary spatiotemporal prediction problem on both densely and sparsely distributed sequences. We introduce a probabilistic approach that partitions the spatial domain into subregions and models the event arrivals in each region with interacting point processes. Our algorithm can jointly learn the spatial partitioning and the interaction between these regions through a gradient-based optimization procedure. Finally, we demonstrate the performance of our algorithm on both simulated data and two real-life datasets. We compare our approach with baseline and state-of-the-art deep learning-based approaches, where we achieve significant performance improvements. Moreover, we also show the effect of using different parameters on the overall performance through empirical results and explain the procedure for choosing the parameters.},
  archive      = {J_TNNLS},
  author       = {Oguzhan Karaahmetoglu and Suleyman Serdar Kozat},
  doi          = {10.1109/TNNLS.2021.3111817},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3097-3110},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatiotemporal sequence prediction with point processes and self-organizing decision trees},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Globally localized multisource domain adaptation for
cross-domain fault diagnosis with category shift. <em>TNNLS</em>,
<em>34</em>(6), 3082–3096. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has demonstrated splendid performance in mechanical fault diagnosis on condition that source and target data are identically distributed. In engineering practice, however, the domain shift between source and target domains significantly limits the further application of intelligent algorithms. Despite various transfer techniques proposed, either they focus on single-source domain adaptation (SDA) or they utilize multisource domain globally or locally, which both cannot address the cross-domain diagnosis effectively, especially with category shift. To this end, we propose globally localized multisource DA for cross-domain fault diagnosis with category shift. Specifically, we construct a GlocalNet to fuse multisource information comprehensively, which consists of a feature generator and three classifiers. By optimizing the Wasserstein discrepancy of classifiers locally and accumulative higher order multisource moment globally, multisource DA is achieved from domain and class levels thus to reduce the shift on domain and category. To refine the classifier at sample level, a distilling strategy is presented. Finally, an adaptive weighting policy is employed for reliable result. To evaluate the effectiveness, the proposed method is compared with multiple methods on four bearing vibration datasets. Experimental results indicate the superiority and practicability of the proposed method for cross-domain fault diagnosis.},
  archive      = {J_TNNLS},
  author       = {Yong Feng and Jinglong Chen and Shuilong He and Tongyang Pan and Zitong Zhou},
  doi          = {10.1109/TNNLS.2021.3111732},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3082-3096},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Globally localized multisource domain adaptation for cross-domain fault diagnosis with category shift},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Joint feature selection and extraction with sparse
unsupervised projection. <em>TNNLS</em>, <em>34</em>(6), 3071–3081. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection and feature extraction, in the field of data dimensionality reduction, are the two main strategies. Nevertheless, each of these two strategies has its own advantages and disadvantages. The features chosen by feature selection method have complete physical meaning. However, feature selection cannot reveal the implicit structural information of the samples. In this article, the methods proposed by us combine both feature selection and feature extraction, called joint feature selection and extraction with sparse unsupervised projection (SUP) and graph optimization SUP (GOSUP). A constraint on the number of nonzero rows of the projection matrix is added, which ensures the sparsity of the projection matrix, and only the features corresponding to the nonzero rows of the projection matrix are selected for the feature extraction procedure. We invoke a newly proposed algorithm to tackle this constrained optimization problem. A new concept of “purification matrix” is invented, the use of which could better eliminate meaningless information of samples in subspace. The performance on several datasets verifies the effectiveness of the proposed method for data dimensionality reduction.},
  archive      = {J_TNNLS},
  author       = {Jingyu Wang and Lin Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3111714},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3071-3081},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint feature selection and extraction with sparse unsupervised projection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). QBox: Partial transfer learning with active querying for
object detection. <em>TNNLS</em>, <em>34</em>(6), 3058–3070. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection requires plentiful data annotated with bounding boxes for model training. However, in many applications, it is difficult or even impossible to acquire a large set of labeled examples for the target task due to the privacy concern or lack of reliable annotators. On the other hand, due to the high-quality image search engines, such as Flickr and Google , it is relatively easy to obtain resource-rich unlabeled datasets, whose categories are a superset of those of target data. In this article, to improve the target model with cost-effective supervision from source data, we propose a partial transfer learning approach QBox to actively query labels for bounding boxes of source images. Specifically, we design two criteria, i.e., informativeness and transferability, to measure the potential utility of a bounding box for improving the target model. Based on these criteria, QBox actively queries the labels of the most useful boxes from the source domain and, thus, requires fewer training examples to save the labeling cost. Furthermore, the proposed query strategy allows annotators to simply labeling a specific region, instead of the whole image, and, thus, significantly reduces the labeling difficulty. Extensive experiments are performed on various partial transfer benchmarks and a real COVID-19 detection task. The results validate that QBox improves the detection accuracy with lower labeling cost compared to state-of-the-art query strategies for object detection.},
  archive      = {J_TNNLS},
  author       = {Ying-Peng Tang and Xiu-Shen Wei and Borui Zhao and Sheng-Jun Huang},
  doi          = {10.1109/TNNLS.2021.3111621},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3058-3070},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {QBox: Partial transfer learning with active querying for object detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilabel feature selection: A local causal structure
learning approach. <em>TNNLS</em>, <em>34</em>(6), 3044–3057. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel feature selection plays an essential role in high-dimensional multilabel learning tasks. Existing multilabel feature selection approaches mainly either explore the feature–label and feature–feature correlations or the label–label and feature–feature correlations. A few of them are able to deal with all three types of correlations simultaneously. To address this problem, in this article, we formulate multilabel feature selection as a local causal structure learning problem and propose a novel algorithm, M2LC. By learning the local causal structure of each class label, M2LC considers three types of feature relationships simultaneously and is scalable to high-dimensional datasets as well. To tackle false discoveries caused by the label–label correlations, M2LC consists of two novel error-correction subroutines to correct those false discoveries. Through local causal structure learning, M2LC learns the causal mechanism behind data, and thus, it can select causally informative features and visualize common features shared by class labels and specific features owned by an individual class label using the learned causal structures. Extensive experiments have been conducted to evaluate M2LC in comparison with the state-of-the-art multilabel feature selection algorithms.},
  archive      = {J_TNNLS},
  author       = {Kui Yu and Mingzhu Cai and Xingyu Wu and Lin Liu and Jiuyong Li},
  doi          = {10.1109/TNNLS.2021.3111288},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3044-3057},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multilabel feature selection: A local causal structure learning approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite-time synchronization of neural networks with infinite
discrete time-varying delays and discontinuous activations.
<em>TNNLS</em>, <em>34</em>(6), 3034–3043. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates finite-time synchronization of neural networks (NNs) with infinite discrete time-varying delays and discontinuous activations (DDNNs). By virtue of theory of differential inclusions, comparison strategies, and inequality techniques, finite-time synchronization of the underlying DDNNs can be developed via a discontinuous state feedback control law, and the synchronous settling time can be estimated. The delayed state feedback controller and finite-time stability theorem are not employed during the analysis. As a special case, finite-time synchronization of NNs with bounded delays and discontinuous activations is given. Finally, two examples are provided to illustrate the validity of the theories.},
  archive      = {J_TNNLS},
  author       = {Yin Sheng and Zhigang Zeng and Tingwen Huang},
  doi          = {10.1109/TNNLS.2021.3110880},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3034-3043},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time synchronization of neural networks with infinite discrete time-varying delays and discontinuous activations},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy enhancing machine learning via removal of unwanted
dependencies. <em>TNNLS</em>, <em>34</em>(6), 3019–3033. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid rise of IoT and Big Data has facilitated copious data-driven applications to enhance our quality of life. However, the omnipresent and all-encompassing nature of the data collection can generate privacy concerns. Hence, there is a strong need to develop techniques that ensure the data serve only the intended purposes, giving users control over the information they share. To this end, this article studies new variants of supervised and adversarial learning methods, which remove the sensitive information in the data before they are sent out for a particular application. The explored methods optimize privacy-preserving feature mappings and predictive models simultaneously in an end-to-end fashion. Additionally, the models are built with an emphasis on placing little computational burden on the user side so that the data can be desensitized on device in a cheap manner. Experimental results on mobile sensing and face datasets demonstrate that our models can successfully maintain the utility performances of predictive models while causing sensitive predictions to perform poorly.},
  archive      = {J_TNNLS},
  author       = {Mert Al and Semih Yagli and Sun-Yuan Kung},
  doi          = {10.1109/TNNLS.2021.3110831},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3019-3033},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Privacy enhancing machine learning via removal of unwanted dependencies},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Jerk-level zhang neurodynamics equivalency of bound
constraints, equation constraints, and objective indices for cyclic
motion of robot-arm systems. <em>TNNLS</em>, <em>34</em>(6), 3005–3018.
(<a href="https://doi.org/10.1109/TNNLS.2021.3110777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Equivalency is a powerful approach that can transform an original problem into another problem that is relatively more ready to be resolved. In recent years, Zhang neurodynamics equivalency (ZNE), in the form of neurodynamics or recurrent neural networks (RNNs), has been investigated, abstracted, and proposed as a process that can equivalently solve equations at different levels. After long-term research, we have noticed that the ZNE can not only work with equations, but also inequations. Thus, the ZNE of inequation type is proposed, proved, and applied in this study. The ZNE of inequation type can transform different-level bound constraints into unified-level bound constraints. Applications of the jerk-level ZNE of bound constraints, equation constraints, and objective indices ultimately build up effective time-varying quadratic-programming schemes for cyclic motion planning and control (CMPC) of single and dual robot-arm systems. In addition, as an effective time-varying quadratic-programming solver, a projection neural network (PNN) is introduced. Experimental results with single and dual robot-arm systems substantiate the correctness and efficacy of ZNE and especially the ZNE of inequation type. Comparisons with conventional methods also exhibit the superiorities of ZNE.},
  archive      = {J_TNNLS},
  author       = {Yunong Zhang and Zhenyu Li and Min Yang and Liangjie Ming and Jinjin Guo},
  doi          = {10.1109/TNNLS.2021.3110777},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {3005-3018},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Jerk-level zhang neurodynamics equivalency of bound constraints, equation constraints, and objective indices for cyclic motion of robot-arm systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered impulsive fault-tolerant control for
memristor-based RDNNs with actuator faults. <em>TNNLS</em>,
<em>34</em>(6), 2993–3004. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on designing an event-triggered impulsive fault-tolerant control strategy for the stabilization of memristor-based reaction–diffusion neural networks (RDNNs) with actuator faults. Different from the existing memristor-based RDNNs with fault-free environments, actuator faults are considered here. A hybrid event-triggered and impulsive (HETI) control scheme, which combines the advantages of event-triggered control and impulsive control, is newly proposed. The hybrid control scheme can effectively accommodate the actuator faults, save the limited communication resources, and achieve the desired system performance. Unlike the existing Lyapunov–Krasovskii functionals (LKFs) constructed on sampling intervals or required to be continuous, the introduced LKF here is directly constructed on event-triggered intervals and can be discontinuous. Based on the LKF and the HETI control scheme, new stabilization criteria are derived for memristor-based RDNNs. Finally, numerical simulations are presented to verify the effectiveness of the obtained results and the merits of the HETI control method.},
  archive      = {J_TNNLS},
  author       = {Ruimei Zhang and Hongxia Wang and Ju H. Park and Peisong He and Xiangpeng Xie},
  doi          = {10.1109/TNNLS.2021.3110756},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2993-3004},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered impulsive fault-tolerant control for memristor-based RDNNs with actuator faults},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FTA-GAN: A computation-efficient accelerator for GANs with
fast transformation algorithm. <em>TNNLS</em>, <em>34</em>(6),
2978–2992. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, generative adversarial network (GAN) is making continuous breakthroughs in many machine learning tasks. The popular GANs usually involve computation-intensive deconvolution operations, leading to limited real-time applications. Prior works have brought several accelerators for deconvolution, but all of them suffer from severe problems, such as computation imbalance and large memory requirements. In this article, we first introduce a novel fast transformation algorithm (FTA) for deconvolution computation, which well solves the computation imbalance problem and removes the extra memory requirement for overlapped partial sums. Besides, it can reduce the computation complexity for various types of deconvolutions significantly. Based on FTA, we develop a fast computing core (FCC) and the corresponding computing array so that the deconvolution can be efficiently computed. We next optimize the dataflow and storage scheme to further reuse on-chip memory and improve the computation efficiency. Finally, we present a computation-efficient hardware architecture for GANs and validate it on several GAN benchmarks, such as deep convolutional GAN (DCGAN), energy-based GAN (EBGAN), and Wasserstein GAN (WGAN). The experimental results show that our design can reach 2211 GOPS under 185-MHz working frequency on Intel Stratix 10SX field-programmable gate array (FPGA) board with satisfactory visual results. In brief, the proposed design can achieve more than $2\times $ hardware efficiency improvement over previous designs, and it can reduce the storage requirement drastically.},
  archive      = {J_TNNLS},
  author       = {Wendong Mao and Peixiang Yang and Zhongfeng Wang},
  doi          = {10.1109/TNNLS.2021.3110728},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2978-2992},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FTA-GAN: A computation-efficient accelerator for GANs with fast transformation algorithm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Batch-based learning consensus of multiagent systems with
faded neighborhood information. <em>TNNLS</em>, <em>34</em>(6),
2965–2977. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the batch-based learning consensus for linear and nonlinear multiagent systems (MASs) with faded neighborhood information. The motivation comes from the observation that agents exchange information via wireless networks, which inevitably introduces random fading effect and channel additive noise to the transmitted signals. It is therefore of great significance to investigate how to ensure the precise consensus tracking to a given reference leader using heavily contaminated information. To this end, a novel distributed learning consensus scheme is proposed, which consists of a classic distributed control structure, a preliminary correction mechanism, and a separated design of learning gain and regulation matrix. The influence of biased and unbiased randomness is discussed in detail according to the convergence rate and consensus performance. The iterationwise asymptotic consensus tracking is strictly established for linear MAS first to demonstrate the inherent principles for the effectiveness of the proposed scheme. Then, the results are extended to nonlinear systems with nonidentical initialization condition and diverse gain design. The obtained results show that the distributed learning consensus scheme can achieve high-precision tracking performance for an MAS under unreliable communications. The theoretical results are verified by two illustrative simulations.},
  archive      = {J_TNNLS},
  author       = {Ganggui Qu and Dong Shen and Xinghuo Yu},
  doi          = {10.1109/TNNLS.2021.3110684},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2965-2977},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Batch-based learning consensus of multiagent systems with faded neighborhood information},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning social spatio-temporal relation graph in the wild
and a video benchmark. <em>TNNLS</em>, <em>34</em>(6), 2951–2964. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social relations are ubiquitous and form the basis of social structure in our daily life. However, existing studies mainly focus on recognizing social relations from still images and movie clips, which are different from real-world scenarios. For example, movie-based datasets define the task as the video classification, only recognizing one relation in the scene. In this article, we aim to study the problem of social relation recognition in an open environment. To close the gap, we provide the first video dataset collected from real-life scenarios, named social relation in the wild (SRIW), where the number of people can be huge and vary, and each pair of relations needs to be recognized. To overcome new challenges, we propose a spatio-temporal relation graph convolutional network (STRGCN) architecture, utilizing correlative visual features to recognize social relations intuitively. Our method decouples the task into two classification tasks: person-level and pair-level relation recognition. Specifically, we propose a person behavior and character module to encode moving and static features in two explicit ways. Then we take them as node features to build a relation graph with meaningful edges in a scene. Based on the relation graph, we introduce the graph convolutional network (GCN) and local GCN to encode social relation features which are used for both recognitions. Experimental results demonstrate the effectiveness of the proposed framework, achieving 83.1\% and 40.8\% mAP in person-level and pair-level classification. Moreover, the study also contributes to the practicality in this field.},
  archive      = {J_TNNLS},
  author       = {Haoran Wang and Licheng Jiao and Fang Liu and Lingling Li and Xu Liu and Deyi Ji and Weihao Gan},
  doi          = {10.1109/TNNLS.2021.3110682},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2951-2964},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning social spatio-temporal relation graph in the wild and a video benchmark},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synchronization of delayed complex networks on time scales
via aperiodically intermittent control using matrix-based convex
combination method. <em>TNNLS</em>, <em>34</em>(6), 2938–2950. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reconsiders synchronization problem of linear complex networks with time-varying delay on time scales. For different types of time scales, aperiodically intermittent control scheme is established by using a matrix-based convex combination method, which has great potential in reducing control consumption and saving communication bandwidth. By employing a common Lyapunov function, aperiodically intermittent controllers are utilized successfully to achieve synchronization of linear delayed complex networks on special time scales onto an isolated node. Next, by constructing a special Lyapunov function with time-varying coefficients, sufficient criteria that consist of two linear matrix inequalities are demonstrated to make linear delayed complex networks on general time scales synchronized onto an isolated system with an exponential convergence rate given in advance. Due to delayed complex networks in this article defined on time scales, the proposed control schemes are applicable to continuous-time networks, their discrete-time forms, and any combination of them. Four numerical examples are offered to highlight the effectiveness and superiority of the proposed aperiodically intermittent control schemes at last.},
  archive      = {J_TNNLS},
  author       = {Peng Wan and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2021.3110321},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2938-2950},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of delayed complex networks on time scales via aperiodically intermittent control using matrix-based convex combination method},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Primal–dual fixed point algorithms based on adapted metric
for distributed optimization. <em>TNNLS</em>, <em>34</em>(6), 2923–2937.
(<a href="https://doi.org/10.1109/TNNLS.2021.3110295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers distributed optimization by a group of agents over an undirected network. The objective is to minimize the sum of a twice differentiable convex function and two possibly nonsmooth convex functions, one of which is composed of a bounded linear operator. A novel distributed primal–dual fixed point algorithm is proposed based on an adapted metric method, which exploits the second-order information of the differentiable convex function. Furthermore, by incorporating a randomized coordinate activation mechanism, we propose a randomized asynchronous iterative distributed algorithm that allows each agent to randomly and independently decide whether to perform an update or remain unchanged at each iteration, and thus alleviates the communication cost. Moreover, the proposed algorithms adopt nonidentical stepsizes to endow each agent with more independence. Numerical simulation results substantiate the feasibility of the proposed algorithms and the correctness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Huaqing Li and Zuqing Zheng and Qingguo Lü and Zheng Wang and Lan Gao and Guo-Cheng Wu and Lianghao Ji and Huiwei Wang},
  doi          = {10.1109/TNNLS.2021.3110295},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2923-2937},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Primal–Dual fixed point algorithms based on adapted metric for distributed optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-unaware adversarial multi-armed bandits with switching
costs. <em>TNNLS</em>, <em>34</em>(6), 2908–2922. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a family of adversarial (a.k.a. nonstochastic) multi-armed bandit (MAB) problems, wherein not only the player cannot observe the reward on the played arm (self-unaware player) but also it incurs switching costs when shifting to another arm. We study two cases: In Case 1 , at each round, the player is able to either play or observe the chosen arm, but not both. In Case 2, the player can choose an arm to play and, at the same round, choose another arm to observe. In both cases, the player incurs a cost for consecutive arm switching due to playing or observing the arms. We propose two novel online learning-based algorithms each addressing one of the aforementioned MAB problems. We theoretically prove that the proposed algorithms for Case 1 and Case 2 achieve sublinear regret of $O(\sqrt [{4}]{KT^{3}\ln K})$ and $O(\sqrt [{3}]{(K-1)T^{2}\ln K})$ , respectively, where the latter regret bound is order-optimal in time, $K$ is the number of arms, and $T$ is the total number of rounds. In Case 2, we extend the player’s capability to multiple $m&amp;gt;1$ observations and show that more observations do not necessarily improve the regret bound due to incurring switching costs. However, we derive an upper bound for switching cost as $c \leq 1/\sqrt [{3}]{m^{2}}$ for which the regret bound is improved as the number of observations increases. Finally, through this study, we found that a generalized version of our approach gives an interesting sublinear regret upper bound result of $\tilde {O}\left ({T^{\frac {s+1}{s+2}}}\right)$ for any self-unaware bandit player with $s$ number of binary decision dilemma before taking the action. To further validate and complement the theoretical findings, we conduct extensive performance evaluations over synthetic data constructed by nonstochastic MAB environment simulations and wireless spectrum measurement data collected in a real-world experiment.},
  archive      = {J_TNNLS},
  author       = {Amir Alipour-Fanid and Monireh Dabaghchian and Kai Zeng},
  doi          = {10.1109/TNNLS.2021.3110194},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2908-2922},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-unaware adversarial multi-armed bandits with switching costs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Entropy minimization versus diversity maximization for
domain adaptation. <em>TNNLS</em>, <em>34</em>(6), 2896–2907. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entropy minimization has been widely used in unsupervised domain adaptation (UDA). However, existing works reveal that the use of entropy-minimization-only may lead to collapsed trivial solutions for UDA. In this article, we try to seek possible close-to-ideal UDA solutions by focusing on some intuitive properties of the ideal domain adaptation solution. In particular, we propose to introduce diversity maximization for further regulating entropy minimization. In order to achieve the possible minimum target risk for UDA, we show that diversity maximization should be elaborately balanced with entropy minimization, the degree of which can be finely controlled with the use of deep embedded validation in an unsupervised manner. The proposed minimal-entropy diversity maximization (MEDM) can be directly implemented by stochastic gradient descent without the use of adversarial learning. Empirical evidence demonstrates that MEDM outperforms the state-of-the-art methods on four popular domain adaptation datasets.},
  archive      = {J_TNNLS},
  author       = {Xiaofu Wu and Suofei Zhang and Quan Zhou and Zhen Yang and Chunming Zhao and Longin Jan Latecki},
  doi          = {10.1109/TNNLS.2021.3110109},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2896-2907},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Entropy minimization versus diversity maximization for domain adaptation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safety-critical containment maneuvering of underactuated
autonomous surface vehicles based on neurodynamic optimization with
control barrier functions. <em>TNNLS</em>, <em>34</em>(6), 2882–2895.
(<a href="https://doi.org/10.1109/TNNLS.2021.3110014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the safety-critical containment maneuvering of multiple underactuated autonomous surface vehicles (ASVs) in the presence of multiple stationary/moving obstacles. In a complex marine environment, every ASV suffers from model uncertainties, external disturbances, and input constraints. A safety-critical control method is proposed for achieving a collision-free containment formation. Specifically, a fixed-time extended state observer is employed for estimating the model uncertainties and external disturbances. By estimating lumped disturbances in fixed time, nominal containment maneuvering control laws are designed in an Earth-fixed reference frame. Input-to-state safe control barrier functions (ISSf-CBFs) are constructed for mapping safety constraints on states to constraints on control inputs. A distributed quadratic optimization problem with the norm of control inputs as the objective function and ISSf-CBFs as constraints is formulated. A recurrent neural network-based neurodynamic optimization approach is adopted to solve the quadratic optimization problem for computing the forces and moments within the safety and input constraints in real time. It is proven that the error signals in the closed-loop control system are uniformly ultimately bounded and the multi-ASVs system is guaranteed for input-to-state safety. Simulation results are elaborated to substantiate the effectiveness of the proposed safety-critical control method for ASVs based on neurodynamic optimization with control barrier functions.},
  archive      = {J_TNNLS},
  author       = {Nan Gu and Dan Wang and Zhouhua Peng and Jun Wang},
  doi          = {10.1109/TNNLS.2021.3110014},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2882-2895},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Safety-critical containment maneuvering of underactuated autonomous surface vehicles based on neurodynamic optimization with control barrier functions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the accuracy of spiking neural networks for radar
gesture recognition through preprocessing. <em>TNNLS</em>,
<em>34</em>(6), 2869–2881. (<a
href="https://doi.org/10.1109/TNNLS.2021.3109958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event-based neural networks are currently being explored as efficient solutions for performing AI tasks at the extreme edge. To fully exploit their potential, event-based neural networks coupled to adequate preprocessing must be investigated. Within this context, we demonstrate a 4-b-weight spiking neural network (SNN) for radar gesture recognition, achieving a state-of-the-art 93\% accuracy within only four processing time steps while using only one convolutional layer and two fully connected layers. This solution consumes very little energy and area if implemented in event-based hardware, which makes it suited for embedded extreme-edge applications. In addition, we demonstrate the importance of signal preprocessing for achieving this high recognition accuracy in SNNs compared to deep neural networks (DNNs) with the same network topology and training strategy. We show that efficient preprocessing prior to the neural network is drastically more important for SNNs compared to DNNs. We also demonstrate, for the first time, that the preprocessing parameters can affect SNNs and DNNs in antagonistic ways, prohibiting the generalization of conclusions drawn from DNN design to SNNs. We demonstrate our findings by comparing the gesture recognition accuracy achieved with our SNN to a DNN with the same architecture and similar training. Unlike previously proposed neural networks for radar processing, this work enables ultralow-power radar-based gesture recognition for extreme-edge devices.},
  archive      = {J_TNNLS},
  author       = {Ali Safa and Federico Corradi and Lars Keuninckx and Ilja Ocket and André Bourdoux and Francky Catthoor and Georges G. E. Gielen},
  doi          = {10.1109/TNNLS.2021.3109958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2869-2881},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving the accuracy of spiking neural networks for radar gesture recognition through preprocessing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Neural network model-based control for manipulator: An
autoencoder perspective. <em>TNNLS</em>, <em>34</em>(6), 2854–2868. (<a
href="https://doi.org/10.1109/TNNLS.2021.3109953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, neural network model-based control has received wide interests in kinematics control of manipulators. To enhance learning ability of neural network models, the autoencoder method is used as a powerful tool to achieve deep learning and has gained success in recent years. However, the performance of existing autoencoder approaches for manipulator control may be still largely dependent on the quality of data, and for extreme cases with noisy data it may even fail. How to incorporate the model knowledge into the autoencoder controller design with an aim to increase the robustness and reliability remains a challenging problem. In this work, a sparse autoencoder controller for kinematic control of manipulators with weights obtained directly from the robot model rather than training data is proposed for the first time. By encoding and decoding the control target though a new dynamic recurrent neural network architecture, the control input can be solved through a new sparse optimization formulation. In this work, input saturation, which holds for almost all practical systems but usually is ignored for analysis simplicity, is also considered in the controller construction. Theoretical analysis and extensive simulations demonstrate that the proposed sparse autoencoder controller with input saturation can make the end-effector of the manipulator system track the desired path efficiently. Further performance comparison and evaluation against the additive noise and parameter uncertainty substantiate robustness of the proposed sparse autoencoder manipulator controller.},
  archive      = {J_TNNLS},
  author       = {Zhan Li and Shuai Li},
  doi          = {10.1109/TNNLS.2021.3109953},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2854-2868},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network model-based control for manipulator: An autoencoder perspective},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An optimal transport analysis on generalization in deep
learning. <em>TNNLS</em>, <em>34</em>(6), 2842–2853. (<a
href="https://doi.org/10.1109/TNNLS.2021.3109942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved state-of-the-art performance in various learning tasks, such as computer vision, natural language processing, and speech recognition. However, the fundamental theory of generalization still remains obscure in deep learning—why DNN models can generalize well, despite that they are heavily overparametrized in both depth and width? Recently, some work shows that traditional theory of analyzing the generalization error of learning models fails to explain the generalization of DNNs. The failure is mainly because of one simple fact that the worse case analysis of generalization error for learning models would be too loose for models with large parameter space, such as DNNs. In this work, we propose a new analysis of generalization in DNNs from an optimal transport perspective. Unlike traditional worse-case uniform convergence analysis in learning theory, our analysis of generalization error is dependent on both the learning algorithm and the data distribution and is the average-case analysis. Thus, our theory can be more practical and accurate to describe the generalization behavior of DNNs. More specifically, in this article, we try to answer a fundamental yet unsolved question in deep learning—why deeper models can generalize well than shallow models? The main contribution of this article can be summarized in four aspects. First, under a general learning framework, we derive upper bounds on the generalization error of learning algorithms by their algorithmic transport cost: the expected Wasserstein distance between the output hypothesis and the output hypothesis conditioned on an input example. We further provide several upper bounds on the algorithmic transport cost in terms of total variation distance, relative entropy, and Vapnik–Chervonenkis (VC) dimension. Moreover, we also study different conditions for loss functions under which the generalization error of a learning algorithm can be upper bounded by different probability metrics between distributions relating to the output hypothesis and/or the input data. Finally, under our established framework, we obtain our main results, showing that the generalization error in DNNs decreases exponentially to zero as the number of layers increases.},
  archive      = {J_TNNLS},
  author       = {Jingwei Zhang and Tongliang Liu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2021.3109942},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2842-2853},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An optimal transport analysis on generalization in deep learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anomaly detection with representative neighbors.
<em>TNNLS</em>, <em>34</em>(6), 2831–2841. (<a
href="https://doi.org/10.1109/TNNLS.2021.3109898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying anomalies from data has attracted increasing attention in recent years due to its broad range of potential applications. Although many efforts have been made for anomaly detection, how to effectively handle high-dimensional data and how to exactly explore neighborhood information, a fundamental issue in anomaly detection, have not yet received sufficient concerns. To circumvent these challenges, in this article, we propose an effective anomaly detection method with representative neighbors for high-dimensional data. Specifically, it projects the high-dimensional data into a low-dimensional space via a sparse operation and explores representative neighbors with a self-representation learning technique. The neighborhood information is then transformed into similarity relations, making the data converge or disperse. Eventually, anomalies are discriminated by a tailored graph clustering technique, which can effectively reveal structural information of the data. Extensive experiments were conducted on ten public real-world datasets with 11 popular anomaly detection algorithms. The results show that the proposed method has encouraging and promising performance compared to the state-of-the-art anomaly detection algorithms.},
  archive      = {J_TNNLS},
  author       = {Huawen Liu and Xiaodan Xu and Enhui Li and Shichao Zhang and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3109898},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2831-2841},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Anomaly detection with representative neighbors},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topological structure and semantic information transfer
network for cross-scene hyperspectral image classification.
<em>TNNLS</em>, <em>34</em>(6), 2817–2830. (<a
href="https://doi.org/10.1109/TNNLS.2021.3109872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation techniques have been widely applied to the problem of cross-scene hyperspectral image (HSI) classification. Most existing methods use convolutional neural networks (CNNs) to extract statistical features from data and often neglect the potential topological structure information between different land cover classes. CNN-based approaches generally only model the local spatial relationships of the samples, which largely limits their ability to capture the nonlocal topological relationship that would better represent the underlying data structure of HSI. In order to make up for the above shortcomings, a Topological structure and Semantic information Transfer network (TSTnet) is developed. The method employs the graph structure to characterize topological relationships and the graph convolutional network (GCN) that is good at processing for cross-scene HSI classification. In the proposed TSTnet, graph optimal transmission (GOT) is used to align topological relationships to assist distribution alignment between the source domain and the target domain based on the maximum mean difference (MMD). Furthermore, subgraphs from the source domain and the target domain are dynamically constructed based on CNN features to take advantage of the discriminative capacity of CNN models that, in turn, improve the robustness of classification. In addition, to better characterize the correlation between distribution alignment and topological relationship alignment, a consistency constraint is enforced to integrate the output of CNN and GCN. Experimental results on three cross-scene HSI datasets demonstrate that the proposed TSTnet performs significantly better than some state-of-the-art domain-adaptive approaches. The codes will be available from the website: https://github.com/YuxiangZhang-BIT/IEEE_TNNLS_TSTnet .},
  archive      = {J_TNNLS},
  author       = {Yuxiang Zhang and Wei Li and Mengmeng Zhang and Ying Qu and Ran Tao and Hairong Qi},
  doi          = {10.1109/TNNLS.2021.3109872},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2817-2830},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Topological structure and semantic information transfer network for cross-scene hyperspectral image classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural network algorithm with reinforcement learning for
parameters extraction of photovoltaic models. <em>TNNLS</em>,
<em>34</em>(6), 2806–2816. (<a
href="https://doi.org/10.1109/TNNLS.2021.3109565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research focuses on the application of artificial neural networks (ANNs) on parameters extraction of photovoltaic (PV) models. Extracting parameters of the PV models accurately is crucial to control and optimize PV systems. Although many algorithms have been proposed to address this issue, how to extract the parameters of the PV models accurately and reliably is still a great challenge. Neural network algorithm (NNA) is a recently reported metaheuristic algorithm. NNA is inspired by ANNs. Benefiting from the unique structure of ANNs, NNA shows excellent global search ability. However, NNA faces the challenge of slow convergence rate and local optima stagnation in solving complex optimization problems. This article presents an improved NNA, named neural network algorithm with reinforcement learning (RLNNA), for extracting parameters of the PV models. In RLNNA, three strategies, namely modification factor with reinforcement learning (RL), transfer operator with historical population, and feedback operator, are designed to overcome the challenge of NNA. To verify the performance of RLNNA, it is employed to extract the parameters of the three PV models. Experimental results show that RLNNA can extract the parameters of the considered PV models with higher accuracy and stronger stability compared with NNA and the other 12 powerful algorithms, which fully indicates the effectiveness of the improved strategies.},
  archive      = {J_TNNLS},
  author       = {Yiying Zhang},
  doi          = {10.1109/TNNLS.2021.3109565},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2806-2816},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network algorithm with reinforcement learning for parameters extraction of photovoltaic models},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comprehensive SNN compression using ADMM optimization and
activity regularization. <em>TNNLS</em>, <em>34</em>(6), 2791–2805. (<a
href="https://doi.org/10.1109/TNNLS.2021.3109064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As well known, the huge memory and compute costs of both artificial neural networks (ANNs) and spiking neural networks (SNNs) greatly hinder their deployment on edge devices with high efficiency. Model compression has been proposed as a promising technique to improve the running efficiency via parameter and operation reduction, whereas this technique is mainly practiced in ANNs rather than SNNs. It is interesting to answer how much an SNN model can be compressed without compromising its functionality, where two challenges should be addressed: 1) the accuracy of SNNs is usually sensitive to model compression, which requires an accurate compression methodology and 2) the computation of SNNs is event-driven rather than static, which produces an extra compression dimension on dynamic spikes. To this end, we realize a comprehensive SNN compression through three steps. First, we formulate the connection pruning and weight quantization as a constrained optimization problem. Second, we combine spatiotemporal backpropagation (STBP) and alternating direction method of multipliers (ADMMs) to solve the problem with minimum accuracy loss. Third, we further propose activity regularization to reduce the spike events for fewer active operations. These methods can be applied in either a single way for moderate compression or a joint way for aggressive compression. We define several quantitative metrics to evaluate the compression performance for SNNs. Our methodology is validated in pattern recognition tasks over MNIST, N-MNIST, CIFAR10, and CIFAR100 datasets, where extensive comparisons, analyses, and insights are provided. To the best of our knowledge, this is the first work that studies SNN compression in a comprehensive manner by exploiting all compressible components and achieves better results.},
  archive      = {J_TNNLS},
  author       = {Lei Deng and Yujie Wu and Yifan Hu and Ling Liang and Guoqi Li and Xing Hu and Yufei Ding and Peng Li and Yuan Xie},
  doi          = {10.1109/TNNLS.2021.3109064},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2791-2805},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Comprehensive SNN compression using ADMM optimization and activity regularization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel discrete-time recurrent neural network for robot
manipulator: A direct discretization technical route. <em>TNNLS</em>,
<em>34</em>(6), 2781–2790. (<a
href="https://doi.org/10.1109/TNNLS.2021.3108050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controlling and processing of time-variant problem is universal in the fields of engineering and science, and the discrete-time recurrent neural network (RNN) model has been proven as an effective method for handling a variety of discrete time-variant problems. However, such model usually originates from the discretization research of continuous time-variant problem, and there is little research on the direct discretization method. To address the aforementioned problem, this article introduces a novel discrete-time RNN model for solving the discrete time-variant problem in a pioneering manner. Specifically, a discrete time-variant nonlinear system, which originates from the mathematical modeling of serial robot manipulator, is presented as a target problem. For solving the problem, first, the technique of second-order Taylor expansion is used to deal with the discrete time-variant nonlinear system, and the novel discrete-time RNN model is proposed subsequently. Second, the theoretical analyses are investigated and developed, which shows the convergence and precision of the proposed discrete-time RNN model. Furthermore, three distinct numerical experiments verify the excellent performance of the proposed discrete-time RNN model. In addition, a robot manipulator example further verifies the effectiveness and practicability of the proposed novel discrete-time RNN model.},
  archive      = {J_TNNLS},
  author       = {Yang Shi and Wenhan Zhao and Shuai Li and Bin Li and Xiaobing Sun},
  doi          = {10.1109/TNNLS.2021.3108050},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2781-2790},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Novel discrete-time recurrent neural network for robot manipulator: A direct discretization technical route},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). C-DeepTrust: A context-aware deep trust prediction model in
online social networks. <em>TNNLS</em>, <em>34</em>(6), 2767–2780. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust prediction provides valuable support for decision making, information dissemination, and product promotion in online social networks. As a complex concept in the social network community, trust relationships among people can be established virtually based on: 1) their interaction behaviors, e.g., the ratings and comments that they provided; 2) the contextual information associated with their interactions, e.g., location and culture; and 3) the relative temporal features of interactions and the time periods when the trust relationships hold. Most of the existing works only focus on some aspects of trust, and there is not a comprehensive study of user trust development that considers and incorporates 1)–3) in trust prediction. In this article, we propose a context-aware deep trust prediction model C-DeepTrust to fill this gap. First, we conduct user feature modeling to obtain the user’s static and dynamic preference features in each context. Static user preference features are obtained from all the ratings and reviews that a user provided, while dynamic user preference features are obtained from the items rated/reviewed by the user in time series. The obtained context-aware user features are then combined and fed into the multilayer projection structure to further mine the context-aware latent features. Finally, the context-aware trust relationships between users are calculated by their context-aware feature vector cosine similarities according to the social homophily theory, which shows a pervasive property of social networks that trust relationships are more likely to be developed among similar people. Extensive experiments conducted on two real-world datasets show the superior performance of our approach compared with the representative baseline methods.},
  archive      = {J_TNNLS},
  author       = {Qi Wang and Weiliang Zhao and Jian Yang and Jia Wu and Shan Xue and Qianli Xing and Philip S. Yu},
  doi          = {10.1109/TNNLS.2021.3107948},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2767-2780},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {C-DeepTrust: A context-aware deep trust prediction model in online social networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint dynamic manifold and discriminant information learning
for feature extraction. <em>TNNLS</em>, <em>34</em>(6), 2753–2766. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighborhood reconstruction is a good recipe to learn the local manifold structure. Representation-based discriminant analysis methods normally learn the reconstruction relationship between each sample and all the other samples. However, reconstruction graphs constructed in these methods have three limitations: 1) they cannot guarantee the local sparsity of reconstruction coefficients; 2) heterogeneous samples may own nonzero coefficients; and 3) they learn the manifold information prior to the process of dimensionality reduction. Due to the existence of noise and redundant features in the original space, the prelearned manifold structure may be inaccurate. Accordingly, the performance of dimensionality reduction would be affected. In this article, we propose a joint model to simultaneously learn the affinity relationship, reconstruction relationship, and projection matrix. In this model, we actively assign neighbors for each sample and learn the inter-reconstruction coefficients between each sample and their neighbors with the same label information in the process of dimensionality reduction. Specifically, a sparse constraint is employed to ensure the sparsity of neighbors and reconstruction coefficients. The whitening constraint is imposed on the projection matrix to remove the relevance between features. An iterative algorithm is proposed to solve this method. Extensive experiments on toy data and public datasets show the superiority of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Xiaowei Zhao and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3107912},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2753-2766},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint dynamic manifold and discriminant information learning for feature extraction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Adaptive NN-based event-triggered containment control for
unknown nonlinear networked systems. <em>TNNLS</em>, <em>34</em>(6),
2742–2752. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article systematically addresses the distributed event-triggered containment control issues for multiagent systems subjected to unknown nonlinearities and external disturbances over a directed communication topology. Novel composite distributed adaptive neural network (NN) event-triggering conditions and event-triggered controller are raised meanwhile. Furthermore, the designed event-triggered controller is updated in an aperiodic way at the moment of event sampling, which saves the computation, resources, and transmission load. On the basis of the NN-based adaptive control techniques and event-triggered control strategies, the uniform ultimate bounded containment control can be achieved. In addition, the Zeno behavior is proven to be excluded. Simulation is presented to testify the effectiveness and advantages of the presented distributed containment control scheme.},
  archive      = {J_TNNLS},
  author       = {Yukan Zheng and Yuan-Xin Li and Wei-Wei Che and Zhongsheng Hou},
  doi          = {10.1109/TNNLS.2021.3107623},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2742-2752},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive NN-based event-triggered containment control for unknown nonlinear networked systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive neural network control for a class of nonlinear
systems with function constraints on states. <em>TNNLS</em>,
<em>34</em>(6), 2732–2741. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the problem of tracking control for a class of nonlinear time-varying full state constrained systems is investigated. By constructing the time-varying asymmetric barrier Lyapunov function (BLF) and combining it with the backstepping algorithm, the intelligent controller and adaptive law are developed. Neural networks (NNs) are utilized to approximate the uncertain function. It is well known that in the past research of nonlinear systems with state constraints, the state constraint boundary is either a constant or a time-varying function. In this article, the constraint boundaries both related to state and time are investigated, which makes the design of control algorithm more complex and difficult. Furthermore, by employing the Lyapunov stability analysis, it is proven that all signals in the closed-loop system are bounded and the time-varying full state constraints are not violated. In the end, the effectiveness of the control algorithm is verified by numerical simulation.},
  archive      = {J_TNNLS},
  author       = {Yan-Jun Liu and Wei Zhao and Lei Liu and Dapeng Li and Shaocheng Tong and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3107600},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2732-2741},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network control for a class of nonlinear systems with function constraints on states},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximate optimal control for nonlinear systems with
periodic event-triggered mechanism. <em>TNNLS</em>, <em>34</em>(6),
2722–2731. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the approximate optimal control problem for nonlinear affine systems under the periodic event triggered control (PETC) strategy. In terms of optimal control, a theoretical comparison of continuous control, traditional event-based control (ETC), and PETC from the perspective of stability convergence, concluding that PETC does not significantly affect the convergence rate than ETC. It is the first time to present PETC for optimal control target of nonlinear systems. A critic network is introduced to approximate the optimal value function based on the idea of reinforcement learning (RL). It is proven that the discrete updating time series from PETC can also be utilized to determine the updating time of the learning network. In this way, the gradient-based weight estimation for continuous systems is developed in discrete form. Then, the uniformly ultimately bounded (UUB) condition of controlled systems is analyzed to ensure the stability of the designed method. Finally, two illustrative examples are given to show the effectiveness of the method.},
  archive      = {J_TNNLS},
  author       = {Shengbo Wang and Shiping Wen and Kaibo Shi and Xiaojun Zhou and Tingwen Huang},
  doi          = {10.1109/TNNLS.2021.3107550},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2722-2731},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Approximate optimal control for nonlinear systems with periodic event-triggered mechanism},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DPSNet: Multitask learning using geometry reasoning for
scene depth and semantics. <em>TNNLS</em>, <em>34</em>(6), 2710–2721.
(<a href="https://doi.org/10.1109/TNNLS.2021.3107362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask joint learning technology continues gaining more attention as a paradigm shift and has shown promising performance in many applications. Depth estimation and semantic understanding from monocular images emerge as a challenging problem in computer vision. While the other joint learning frameworks establish the relationship between the semantics and depth from stereo pairs, the lack of learning camera motion renders the frameworks that fail to model the geometric structure of the image scene. We make a further step in this article by proposing a multitask learning method, namely DPSNet, which can jointly perform depth and camera pose estimation and semantic scene segmentation. Our core idea for depth and camera pose prediction is that we present the rigid semantic consistency loss to overcome the limitation of moving pixels from image reconstruction technology and further infer the segmentation of moving instances based on them. In addition, our proposed model performs semantic segmentation by reasoning the geometric correspondences between the pixel semantic outputs and the semantic labels at multiscale resolutions. Experiments on open-source datasets and a video dataset captured on a micro-smart car show the effectiveness of each component of DPSNet, and DPSNet achieves state-of-the-art results in all three tasks compared with the best popular methods. All our models and code are available at https://github.com/jn-z/DPSNet : Multitask Learning Using Geometry Reasoning for Scene Depth and semantics.},
  archive      = {J_TNNLS},
  author       = {Junning Zhang and Qunxing Su and Bo Tang and Cheng Wang and Yining Li},
  doi          = {10.1109/TNNLS.2021.3107362},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2710-2721},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DPSNet: Multitask learning using geometry reasoning for scene depth and semantics},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flying through a narrow gap using end-to-end deep
reinforcement learning augmented with curriculum learning and Sim2Real.
<em>TNNLS</em>, <em>34</em>(5), 2701–2708. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traversing through a tilted narrow gap is previously an intractable task for reinforcement learning mainly due to two challenges. First, searching feasible trajectories is not trivial because the goal behind the gap is difficult to reach. Second, the error tolerance after Sim2Real is low due to the relatively high speed in comparison to the gap’s narrow dimensions. This problem is aggravated by the intractability of collecting real-world data due to the risk of collision damage. In this brief, we propose an end-to-end reinforcement learning framework that solves this task successfully by addressing both problems. To search for dynamically feasible flight trajectories, we use a curriculum learning to guide the agent toward the sparse reward behind the obstacle. To tackle the Sim2Real problem, we propose a Sim2Real framework that can transfer control commands to a real quadrotor without using real flight data. To the best of our knowledge, our brief is the first work that accomplishes successful gap traversing task purely using deep reinforcement learning.},
  archive      = {J_TNNLS},
  author       = {Chenxi Xiao and Peng Lu and Qizhi He},
  doi          = {10.1109/TNNLS.2021.3107742},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2701-2708},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Flying through a narrow gap using end-to-end deep reinforcement learning augmented with curriculum learning and Sim2Real},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weak stabilization of boolean networks under state-flipped
control. <em>TNNLS</em>, <em>34</em>(5), 2693–2700. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, stabilization of Boolean networks (BNs) by flipping a subset of nodes is considered, here we call such action state-flipped control. The state-flipped control implies that the logical variables of certain nodes are flipped from 1 to 0 or 0 to 1 as time flows. Under state-flipped control on certain nodes, a state-flipped-transition matrix is defined to describe the impact on the state transition space. Weak stabilization is first defined and then some criteria are presented to judge the same. An algorithm is proposed to find a stabilizing kernel such that BNs can achieve weak stabilization to the desired state with in-degree more than 0. By defining a reachable set, another approach is proposed to verify weak stabilization, and an algorithm is given to obtain a flip sequence steering an initial state to a given target state. Subsequently, the issue of finding flip sequences to steer BNs from weak stabilization to global stabilization is addressed. In addition, a model-free reinforcement algorithm, namely the $Q$ -learning ( $Q\text{L}$ ) algorithm, is developed to find flip sequences to achieve global stabilization. Finally, several numerical examples are given to illustrate the obtained theoretical results.},
  archive      = {J_TNNLS},
  author       = {Zejiao Liu and Jie Zhong and Yang Liu and Weihua Gui},
  doi          = {10.1109/TNNLS.2021.3106918},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2693-2700},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weak stabilization of boolean networks under state-flipped control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-fragile h∞ synchronization for markov jump singularly
perturbed coupled neural networks subject to double-layer switching
regulation. <em>TNNLS</em>, <em>34</em>(5), 2682–2692. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work explores the $H_{\infty }$ synchronization issue for singularly perturbed coupled neural networks (SPCNNs) affected by both nonlinear constraints and gain uncertainties, in which a novel double-layer switching regulation containing Markov chain and persistent dwell-time switching regulation (PDTSR) is used. The first layer of switching regulation is the Markov chain to characterize the switching stochastic properties of the systems suffering from random component failures and sudden environmental disturbances. Meanwhile, PDTSR, as the second-layer switching regulation, is used to depict the variations in the transition probability of the aforementioned Markov chain. For systems under double-layer switching regulation, the purpose of the addressed issue is to design a mode-dependent synchronization controller for the network with the desired controller gains calculated by solving convex optimization problems. As such, new sufficient conditions are established to ensure that the synchronization error systems are mean-square exponentially stable with a specified level of the $H_{\infty }$ performance. Eventually, the solvability and validity of the proposed control scheme are illustrated through a numerical simulation.},
  archive      = {J_TNNLS},
  author       = {Hao Shen and Xiaohui Hu and Jing Wang and Jinde Cao and Wenhua Qian},
  doi          = {10.1109/TNNLS.2021.3107607},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2682-2692},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Non-fragile h∞ synchronization for markov jump singularly perturbed coupled neural networks subject to double-layer switching regulation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From ensemble clustering to subspace clustering: Cluster
structure encoding. <em>TNNLS</em>, <em>34</em>(5), 2670–2681. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel algorithm to encode the cluster structure by incorporating ensemble clustering (EC) into subspace clustering (SC). First, the low-rank representation (LRR) is learned from a higher order data relationship induced by ensemble K-means coding, which exploits the cluster structure in a co-association matrix of basic partitions (i.e., clustering results). Second, to provide a fast predictive coding mechanism, an encoding function parameterized by neural networks is introduced to predict the LRR derived from partitions. These two steps are jointly proceeded to seamlessly integrate partition information and original features and thus deliver better representations than the ones obtained from each single source. Moreover, an alternating optimization framework is developed to learn the LRR, train the encoding function, and fine-tune the higher order relationship. Extensive experiments on eight benchmark datasets validate the effectiveness of the proposed algorithm on several clustering tasks compared with state-of-the-art EC and SC methods.},
  archive      = {J_TNNLS},
  author       = {Zhiqiang Tao and Jun Li and Huazhu Fu and Yu Kong and Yun Fu},
  doi          = {10.1109/TNNLS.2021.3107354},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2670-2681},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {From ensemble clustering to subspace clustering: Cluster structure encoding},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamical bifurcation for a class of large-scale fractional
delayed neural networks with complex ring-hub structure and hybrid
coupling. <em>TNNLS</em>, <em>34</em>(5), 2659–2669. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real neural networks are characterized by large-scale and complex topology. However, the current dynamical analysis is limited to low-dimensional models with simplified topology. Therefore, there is still a huge gap between neural network theory and its application. This article proposes a class of large-scale neural networks with a ring-hub structure, where a hub node is connected to $n$ peripheral nodes and these peripheral nodes are linked by a ring. In particular, there exists a hybrid coupling mode in the network topology. The mathematical model of such systems is described by fractional-order delayed differential equations. The aim of this article is to investigate the local stability and Hopf bifurcation of this high-dimensional neural network. First, the Coates flow graph is employed to obtain the characteristic equation of the linearized high-dimensional neural network model, which is a transcendental equation including multiple exponential items. Then, the sufficient conditions ensuring the stability of equilibrium and the existence of Hopf bifurcation are achieved by taking time delay as a bifurcation parameter. Finally, some numerical examples are given to support the theoretical results. It is revealed that the increasing time delay can effectively induce the occurrence of periodic oscillation. Moreover, the fractional order, the self-feedback coefficient, and the number of neurons also have effects on the onset of Hopf bifurcation.},
  archive      = {J_TNNLS},
  author       = {Jing Chen and Min Xiao and Youhong Wan and Chengdai Huang and Fengyu Xu},
  doi          = {10.1109/TNNLS.2021.3107330},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2659-2669},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamical bifurcation for a class of large-scale fractional delayed neural networks with complex ring-hub structure and hybrid coupling},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elastic knowledge distillation by learning from
recollection. <em>TNNLS</em>, <em>34</em>(5), 2647–2658. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model performance can be further improved with the extra guidance apart from the one-hot ground truth. To achieve it, recently proposed recollection-based methods utilize the valuable information contained in the past training history and derive a “recollection” from it to provide data-driven prior to guide the training. In this article, we focus on two fundamental aspects of this method, i.e., recollection construction and recollection utilization. Specifically, to meet the various demands of models with different capacities and at different training periods, we propose to construct a set of recollections with diverse distributions from the same training history. After that, all the recollections collaborate together to provide guidance, which is adaptive to different model capacities, as well as different training periods, according to our similarity-based elastic knowledge distillation (KD) algorithm. Without any external prior to guide the training, our method achieves a significant performance gain and outperforms the methods of the same category, even as well as KD with well-trained teacher. Extensive experiments and further analysis are conducted to demonstrate the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Yongjian Fu and Songyuan Li and Hanbin Zhao and Wenfu Wang and Weihao Fang and Yueting Zhuang and Zhijie Pan and Xi Li},
  doi          = {10.1109/TNNLS.2021.3107317},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2647-2658},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Elastic knowledge distillation by learning from recollection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust scene parsing by mining supportive knowledge from
dataset. <em>TNNLS</em>, <em>34</em>(5), 2633–2646. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene parsing, or semantic segmentation, aims at labeling all pixels in an image with the predefined categories of things and stuff. Learning a robust representation for each pixel is crucial for this task. Existing state-of-the-art (SOTA) algorithms employ deep neural networks to learn (discover) the representations needed for parsing from raw data. Nevertheless, these networks discover desired features or representations only from the given image (content), ignoring more generic knowledge contained in the dataset. To overcome this deficiency, we make the first attempt to explore the meaningful supportive knowledge, including general visual concepts (i.e., the generic representations for objects and stuff) and their relations from the whole dataset to enhance the underlying representations of a specific scene for better scene parsing. Specifically, we propose a novel supportive knowledge mining module (SKMM) and a knowledge augmentation operator (KAO), which can be easily plugged into modern scene parsing networks. By taking image-specific content and dataset-level supportive knowledge into full consideration, the resulting model, called knowledge augmented neural network (KANN), can better understand the given scene and provide greater representational power. Experiments are conducted on three challenging scene parsing and semantic segmentation datasets: Cityscapes, Pascal-Context, and ADE20K. The results show that our KANN is effective and achieves better results than all existing SOTA methods.},
  archive      = {J_TNNLS},
  author       = {Ao Luo and Fan Yang and Xin Li and Yuezun Li and Zhicheng Jiao and Hong Cheng and Siwei Lyu},
  doi          = {10.1109/TNNLS.2021.3107194},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2633-2646},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust scene parsing by mining supportive knowledge from dataset},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regularization effect of random node fault/noise on gradient
descent learning algorithm. <em>TNNLS</em>, <em>34</em>(5), 2619–2632.
(<a href="https://doi.org/10.1109/TNNLS.2021.3107051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For decades, adding fault/noise during training by gradient descent has been a technique for getting a neural network (NN) tolerant to persistent fault/noise or getting an NN with better generalization. In recent years, this technique has been readvocated in deep learning to avoid overfitting. Yet, the objective function of such fault/noise injection learning has been misinterpreted as the desired measure (i.e., the expected mean squared error (mse) of the training samples) of the NN with the same fault/noise. The aims of this article are: 1) to clarify the above misconception and 2) investigate the actual regularization effect of adding node fault/noise when training by gradient descent. Based on the previous works on adding fault/noise during training, we speculate the reason why the misconception appears. In the sequel, it is shown that the learning objective of adding random node fault during gradient descent learning (GDL) for a multilayer perceptron (MLP) is identical to the desired measure of the MLP with the same fault. If additive (resp. multiplicative) node noise is added during GDL for an MLP, the learning objective is not identical to the desired measure of the MLP with such noise. For radial basis function (RBF) networks, it is shown that the learning objective is identical to the corresponding desired measure for all three fault/noise conditions. Empirical evidence is presented to support the theoretical results and, hence, clarify the misconception that the objective function of a fault/noise injection learning might not be interpreted as the desired measure of the NN with the same fault/noise. Afterward, the regularization effect of adding node fault/noise during training is revealed for the case of RBF networks. Notably, it is shown that the regularization effect of adding additive or multiplicative node noise (MNN) during training an RBF is reducing network complexity. Applying dropout regularization in RBF networks, its effect is the same as adding MNN during training.},
  archive      = {J_TNNLS},
  author       = {John Sum and Chi-Sing Leung},
  doi          = {10.1109/TNNLS.2021.3107051},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2619-2632},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Regularization effect of random node Fault/Noise on gradient descent learning algorithm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient iterative approach to explainable feature
learning. <em>TNNLS</em>, <em>34</em>(5), 2606–2618. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a new iterative approach to explainable feature learning. During each iteration, new features are generated, first by applying arithmetic operations on the input set of features. These are then evaluated in terms of probability distribution agreements between values of samples belonging to different classes. Finally, a graph-based approach for feature selection is proposed, which allows for selecting high-quality and uncorrelated features to be used in feature generation during the next iteration. As shown by the results, the proposed method improved the accuracy of all tested classifiers, where the best accuracies were achieved using random forest. In addition, the method turned out to be insensitive to both of the input parameters, while superior performances in comparison to the state of the art were demonstrated on nine out of 15 test sets and achieving comparable results in the others. Finally, we demonstrate the explainability of the learned feature representation for knowledge discovery.},
  archive      = {J_TNNLS},
  author       = {Dino Vlahek and Domen Mongus},
  doi          = {10.1109/TNNLS.2021.3107049},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2606-2618},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient iterative approach to explainable feature learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MGRC: An end-to-end multigranularity reading comprehension
model for question answering. <em>TNNLS</em>, <em>34</em>(5), 2594–2605.
(<a href="https://doi.org/10.1109/TNNLS.2021.3107029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network-based models have achieved great success in extractive question answering. Recently, many works have been proposed to model multistage matching for this task, which usually first retrieve relevant paragraphs or sentences and then extract an answer span from the retrieved results. However, such a pipeline-based approach suffers from the error propagation problem, especially for sentence-level retrieval that is usually difficult to achieve high accuracy due to the severe data imbalance problem. Furthermore, since the paragraph/sentence selector and the answer extractor are closely related, modeling them independently does not fully exploit the power of multistage matching. To solve these problems, we propose a novel end-to-end multigranularity reading comprehension model, which is a unified framework to explicitly model three matching granularities, including paragraph identification, sentence selection, and answer extraction. Our approach has two main advantages. First, the end-to-end approach alleviates the error propagation problem in both the training and inference phases. Second, the shared features in a unified model improve the learning of representations of different matching granularities. We conduct a comprehensive comparison on four large-scale datasets (SQuAD-open, NewsQA, SQuAD 2.0, and SQuAD Adversarial) and verify that the proposed approach outperforms both the vanilla BERT model and existing multistage matching approaches. We also conduct an ablation study and verify the effectiveness of the proposed components in our model structure.},
  archive      = {J_TNNLS},
  author       = {Qian Liu and Xiubo Geng and Heyan Huang and Tao Qin and Jie Lu and Daxin Jiang},
  doi          = {10.1109/TNNLS.2021.3107029},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2594-2605},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MGRC: An end-to-end multigranularity reading comprehension model for question answering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combination of manifold learning and deep learning
algorithms for mid-term electrical load forecasting. <em>TNNLS</em>,
<em>34</em>(5), 2584–2593. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mid-term load forecasting (MTLF) is of great significance for power system planning, operation, and power trading. However, the mid-term electrical load is affected by the coupling of multiple factors and demonstrates complex characteristics, which leads to low prediction accuracy in MTLF. Furthermore, MTLF is faced with the “curse of dimensionality” problem due to a large number of variables. This article proposes an MTLF method based on manifold learning, which can extract the underlying factors of load variations to help improve the accuracy of MTLF and significantly reduce the calculation. Unlike linear dimensionality reduction methods, manifold learning has better nonlinear feature extraction ability and is more suitable for load data with nonlinear characteristics. Furthermore, long short-term memory (LSTM) neural networks are used to establish forecasting models in the low-dimensional space obtained by manifold learning. The proposed MTLF method is tested on independent system operator (ISO) New England datasets, and load forecasting in 24, 168, and 720 h ahead is carried out. The numerical results validate that the proposed method has higher prediction accuracy than many mature methods in the mid-term time scale.},
  archive      = {J_TNNLS},
  author       = {Jinghua Li and Shanyang Wei and Wei Dai},
  doi          = {10.1109/TNNLS.2021.3106968},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2584-2593},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Combination of manifold learning and deep learning algorithms for mid-term electrical load forecasting},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring adversarial attack in spiking neural networks with
spike-compatible gradient. <em>TNNLS</em>, <em>34</em>(5), 2569–2583.
(<a href="https://doi.org/10.1109/TNNLS.2021.3106961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural network (SNN) is broadly deployed in neuromorphic devices to emulate brain function. In this context, SNN security becomes important while lacking in-depth investigation. To this end, we target the adversarial attack against SNNs and identify several challenges distinct from the artificial neural network (ANN) attack: 1) current adversarial attack is mainly based on gradient information that presents in a spatiotemporal pattern in SNNs, hard to obtain with conventional backpropagation algorithms; 2) the continuous gradient of the input is incompatible with the binary spiking input during gradient accumulation, hindering the generation of spike-based adversarial examples; and 3) the input gradient can be all-zeros (i.e., vanishing) sometimes due to the zero-dominant derivative of the firing function. Recently, backpropagation through time (BPTT)-inspired learning algorithms are widely introduced into SNNs to improve the performance, which brings the possibility to attack the models accurately given spatiotemporal gradient maps. We propose two approaches to address the above challenges of gradient-input incompatibility and gradient vanishing. Specifically, we design a gradient-to-spike (G2S) converter to convert continuous gradients to ternary ones compatible with spike inputs. Then, we design a restricted spike flipper (RSF) to construct ternary gradients that can randomly flip the spike inputs with a controllable turnover rate, when meeting all-zero gradients. Putting these methods together, we build an adversarial attack methodology for SNNs. Moreover, we analyze the influence of the training loss function and the firing threshold of the penultimate layer on the attack effectiveness. Extensive experiments are conducted to validate our solution. Besides the quantitative analysis of the influence factors, we also compare SNNs and ANNs against adversarial attacks under different attack methods. This work can help reveal what happens in SNN attacks and might stimulate more research on the security of SNN models and neuromorphic devices.},
  archive      = {J_TNNLS},
  author       = {Ling Liang and Xing Hu and Lei Deng and Yujie Wu and Guoqi Li and Yufei Ding and Peng Li and Yuan Xie},
  doi          = {10.1109/TNNLS.2021.3106961},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2569-2583},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploring adversarial attack in spiking neural networks with spike-compatible gradient},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Cluster synchronization control for discrete-time complex
dynamical networks: When data transmission meets constrained bit rate.
<em>TNNLS</em>, <em>34</em>(5), 2554–2568. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the cluster synchronization control problem is studied for discrete-time complex dynamical networks when the data transmission is subject to constrained bit rate. A bit-rate model is presented to quantify the limited network bandwidth, and the effects from the constrained bit rate onto the control performance of the cluster synchronization are evaluated. A sufficient condition is first proposed to guarantee the ultimate boundedness of the error dynamics of the cluster synchronization, and then, a bit-rate condition is established to reveal the fundamental relationship between the bit rate and the certain performance index of the cluster synchronization. Subsequently, two optimization problems are formulated to design the desired synchronization controllers with aim to achieve two distinct synchronization performance indices. The codesign issue for the bit-rate allocation protocol and the controller gains is further discussed to reduce the conservatism by locally minimizing a certain asymptotic upper bound of the synchronization error dynamics. Finally, three illustrative simulation examples are utilized to validate the feasibility and effectiveness of the developed synchronization control scheme.},
  archive      = {J_TNNLS},
  author       = {Jun-Yi Li and Zidong Wang and Renquan Lu and Yong Xu},
  doi          = {10.1109/TNNLS.2021.3106947},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2554-2568},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cluster synchronization control for discrete-time complex dynamical networks: When data transmission meets constrained bit rate},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing LGMD’s looming selectivity for UAV with
spatial–temporal distributed presynaptic connections. <em>TNNLS</em>,
<em>34</em>(5), 2539–2553. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collision detection is one of the most challenging tasks for unmanned aerial vehicles (UAVs). This is especially true for small or micro-UAVs due to their limited computational power. In nature, flying insects with compact and simple visual systems demonstrate their remarkable ability to navigate and avoid collision in complex environments. A good example of this is provided by locusts. They can avoid collisions in a dense swarm through the activity of a motion-based visual neuron called the Lobula giant movement detector (LGMD). The defining feature of the LGMD neuron is its preference for looming. As a flying insect’s visual neuron, LGMD is considered to be an ideal basis for building UAV’s collision detecting system. However, existing LGMD models cannot distinguish looming clearly from other visual cues, such as complex background movements caused by UAV agile flights. To address this issue, we proposed a new model implementing distributed spatial–temporal synaptic interactions, which is inspired by recent findings in locusts’ synaptic morphology. We first introduced the locally distributed excitation to enhance the excitation caused by visual motion with preferred velocities. Then, radially extending temporal latency for inhibition is incorporated to compete with the distributed excitation and selectively suppress the nonpreferred visual motions. This spatial–temporal competition between excitation and inhibition in our model is, therefore, tuned to preferred image angular velocity representing looming rather than background movements with these distributed synaptic interactions. Systematic experiments have been conducted to verify the performance of the proposed model for UAV agile flights. The results have demonstrated that this new model enhances the looming selectivity in complex flying scenes considerably and has the potential to be implemented on embedded collision detection systems for small or micro-UAVs.},
  archive      = {J_TNNLS},
  author       = {Jiannan Zhao and Hongxin Wang and Nicola Bellotto and Cheng Hu and Jigen Peng and Shigang Yue},
  doi          = {10.1109/TNNLS.2021.3106946},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2539-2553},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhancing LGMD’s looming selectivity for UAV with Spatial–Temporal distributed presynaptic connections},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep low-shot learning for biological image classification
and visualization from limited training samples. <em>TNNLS</em>,
<em>34</em>(5), 2528–2538. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive modeling is useful but very challenging in biological image analysis due to the high cost of obtaining and labeling training data. For example, in the study of gene interaction and regulation in Drosophila embryogenesis, the analysis is most biologically meaningful when in situ hybridization (ISH) gene expression pattern images from the same developmental stage are compared. However, labeling training data with precise stages is very time-consuming even for developmental biologists. Thus, a critical challenge is how to build accurate computational models for precise developmental stage classification from limited training samples. In addition, identification and visualization of developmental landmarks are required to enable biologists to interpret prediction results and calibrate models. To address these challenges, we propose a deep two-step low-shot learning framework to accurately classify ISH images using limited training images. Specifically, to enable accurate model training on limited training samples, we formulate the task as a deep low-shot learning problem and develop a novel two-step learning approach, including data-level learning and feature-level learning. We use a deep residual network as our base model and achieve improved performance in the precise stage prediction task of ISH images. Furthermore, the deep model can be interpreted by computing saliency maps, which consists of pixel-wise contributions of an image to its prediction result. In our task, saliency maps are used to assist the identification and visualization of developmental landmarks. Our experimental results show that the proposed model can not only make accurate predictions but also yield biologically meaningful interpretations. We anticipate our methods to be easily generalizable to other biological image classification tasks with small training datasets. Our open-source code is available at https://github.com/divelab/lsl-fly .},
  archive      = {J_TNNLS},
  author       = {Lei Cai and Zhengyang Wang and Rob Kulathinal and Sudhir Kumar and Shuiwang Ji},
  doi          = {10.1109/TNNLS.2021.3106831},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2528-2538},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep low-shot learning for biological image classification and visualization from limited training samples},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning a low-dimensional representation of a safe region
for safe reinforcement learning on dynamical systems. <em>TNNLS</em>,
<em>34</em>(5), 2513–2527. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the safe application of reinforcement learning algorithms to high-dimensional nonlinear dynamical systems, a simplified system model is used to formulate a safe reinforcement learning (SRL) framework. Based on the simplified system model, a low-dimensional representation of the safe region is identified and used to provide safety estimates for learning algorithms. However, finding a satisfying simplified system model for complex dynamical systems usually requires a considerable amount of effort. To overcome this limitation, we propose a general data-driven approach that is able to efficiently learn a low-dimensional representation of the safe region. By employing an online adaptation method, the low-dimensional representation is updated using the feedback data to obtain more accurate safety estimates. The performance of the proposed approach for identifying the low-dimensional representation of the safe region is illustrated using the example of a quadcopter. The results demonstrate a more reliable and representative low-dimensional representation of the safe region compared with previous works, which extends the applicability of the SRL framework.},
  archive      = {J_TNNLS},
  author       = {Zhehua Zhou and Ozgur S. Oguz and Marion Leibold and Martin Buss},
  doi          = {10.1109/TNNLS.2021.3106818},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2513-2527},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning a low-dimensional representation of a safe region for safe reinforcement learning on dynamical systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive learning control of switched strict-feedback
nonlinear systems with dead zone using NN and DOB. <em>TNNLS</em>,
<em>34</em>(5), 2503–2512. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive learning control for a class of switched strict-feedback nonlinear systems with external disturbances and input dead zone. To handle unknown nonlinearity and compound disturbances, a collaborative estimation learning strategy based on neural approximation and disturbance observation is proposed, and the adaptive neural switched control scheme is studied in a dynamic surface control framework. In the adaptive learning control design, to obtain the evaluation information of uncertain learning, the prediction error is constructed based on the composite learning scheme. Then, the prediction error and the compensated tracking error are applied to construct the adaptive laws of switched neural weights and switched disturbance observers. The system stability analysis is carried out through the Lyapunov approach, where the switching signal with average dwell time is considered. Through the simulation test, the effectiveness of the proposed adaptive learning controller is verified.},
  archive      = {J_TNNLS},
  author       = {Yixin Cheng and Bin Xu and Zhi Lian and Zhongke Shi and Peng Shi},
  doi          = {10.1109/TNNLS.2021.3106781},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2503-2512},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive learning control of switched strict-feedback nonlinear systems with dead zone using NN and DOB},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modal regression-based graph representation for noise robust
face hallucination. <em>TNNLS</em>, <em>34</em>(5), 2490–2502. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manifold learning-based face hallucination technologies have been widely developed during the past decades. However, the conventional learning methods always become ineffective in noise environment due to the least-square regression, which usually generates distorted representations for noisy inputs they employed for error modeling. To solve this problem, in this article, we propose a modal regression-based graph representation (MRGR) model for noisy face hallucination. In MRGR, the modal regression-based function is incorporated into graph learning framework to improve the resolution of noisy face images. Specifically, the modal regression-induced metric is used instead of the least-square metric to regularize the encoding errors, which admits the MRGR to robust against noise with uncertain distribution. Moreover, a graph representation is learned from feature space to exploit the inherent typological structure of patch manifold for data representation, resulting in more accurate reconstruction coefficients. Besides, for noisy color face hallucination, the MRGR is extended into quaternion (MRGR-Q) space, where the abundant correlations among different color channels can be well preserved. Experimental results on both the grayscale and color face images demonstrate the superiority of MRGR and MRGR-Q compared with several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Licheng Liu and C. L. Philip Chen and Yaonan Wang},
  doi          = {10.1109/TNNLS.2021.3106773},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2490-2502},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modal regression-based graph representation for noise robust face hallucination},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TRCLA: A transfer learning approach to reduce negative
transfer for cellular learning automata. <em>TNNLS</em>, <em>34</em>(5),
2480–2489. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most traditional machine learning algorithms, the training and testing datasets have identical distributions and feature spaces. However, these assumptions have not held in many real applications. Although transfer learning methods have been invented to fill this gap, they introduce new challenges as negative transfers (NTs). Most previous research considered NT a significant problem, but they pay less attention to solving it. This study will propose a transductive learning algorithm based on cellular learning automata (CLA) to alleviate the NT issue. Two famous learning automata (LA) entitled estimators are applied as estimator CLA in the proposed algorithms. A couple of new decision criteria called merit and and attitude parameters are introduced to CLA to limit NT. The proposed algorithms are applied to standard LA environments. The experiments show that the proposed algorithm leads to higher accuracy and less NT results.},
  archive      = {J_TNNLS},
  author       = {Seyyed Amir Hadi Minoofam and Azam Bastanfard and Mohammad Reza Keyvanpour},
  doi          = {10.1109/TNNLS.2021.3106705},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2480-2489},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TRCLA: A transfer learning approach to reduce negative transfer for cellular learning automata},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Robust discriminant subspace clustering with adaptive local
structure embedding. <em>TNNLS</em>, <em>34</em>(5), 2466–2479. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised dimension reduction and clustering are frequently used as two separate steps to conduct clustering tasks in subspace. However, the two-step clustering methods may not necessarily reflect the cluster structure in the subspace. In addition, the existing subspace clustering methods do not consider the relationship between the low-dimensional representation and local structure in the input space. To address the above issues, we propose a robust discriminant subspace (RDS) clustering model with adaptive local structure embedding. Specifically, unlike the existing methods which incorporate dimension reduction and clustering via regularizer, thereby introducing extra parameters, RDS first integrates them into a unified matrix factorization (MF) model through theoretical proof. Furthermore, a similarity graph is constructed to learn the local structure. A constraint is imposed on the graph to guarantee that it has the same connected components with low-dimensional representation. In this spirit, the similarity graph serves as a tradeoff that adaptively balances the learning process between the low-dimensional space and the original space. Finally, RDS adopts the $\ell _{2,1}$ -norm to measure the residual error, which enhances the robustness to noise. Using the property of the $\ell _{2,1}$ -norm, RDS can be optimized efficiently without introducing more penalty terms. Experimental results on real-world benchmark datasets show that RDS can provide more interpretable clustering results and also outperform other state-of-the-art alternatives.},
  archive      = {J_TNNLS},
  author       = {Junyan Liu and Dapeng Li and Haitao Zhao and Lin Gao},
  doi          = {10.1109/TNNLS.2021.3106702},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2466-2479},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust discriminant subspace clustering with adaptive local structure embedding},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust to rank selection: Low-rank sparse tensor-ring
completion. <em>TNNLS</em>, <em>34</em>(5), 2451–2465. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor-ring (TR) decomposition was recently studied and applied for low-rank tensor completion due to its powerful representation ability of high-order tensors. However, most of the existing TR-based methods tend to suffer from deterioration when the selected rank is larger than the true one. To address this issue, this article proposes a new low-rank sparse TR completion method by imposing the Frobenius norm regularization on its latent space. Specifically, we theoretically establish that the proposed method is capable of exploiting the low rankness and Kronecker-basis-representation (KBR)-based sparsity of the target tensor using the Frobenius norm of latent TR-cores. We optimize the proposed TR completion by block coordinate descent (BCD) algorithm and design a modified TR decomposition for the initialization of this algorithm. Extensive experimental results on synthetic data and visual data have demonstrated that the proposed method is able to achieve better results compared to the conventional TR-based completion methods and other state-of-the-art methods and, meanwhile, is quite robust even if the selected TR-rank increases.},
  archive      = {J_TNNLS},
  author       = {Jinshi Yu and Guoxu Zhou and Weijun Sun and Shengli Xie},
  doi          = {10.1109/TNNLS.2021.3106654},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2451-2465},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust to rank selection: Low-rank sparse tensor-ring completion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and control of robotic manipulators based on
symbolic regression. <em>TNNLS</em>, <em>34</em>(5), 2440–2450. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based design is an important method of addressing problems associated with designing complex control systems. For complex dynamic systems in the presence of uncertainties, the modeling process from the first principles becomes extremely tedious and simplification in mechanism and parameter measurement may result in model inaccuracy. On the contrary, machine learning has the characteristic of fitting complicated equations, which makes it widely used in the research of model identification. However, it only brings a black-box model where the design schemes based on an analytical model cannot be applied. In this article, a simple and novel scheme for modeling and control of robotic manipulators is proposed; without prior knowledge, a dynamic model in an analytical form is obtained from artificially excited training data using the symbolic regression technique, and then, a controller is designed based on the dynamic model. Due to the ingenious experimental design, on one hand, the amount of training data is far less than the system identification method by machine learning. On the other hand, a decoupling feature is used in the model that greatly simplifies controller design. The experimental results on two-degree of freedom (DOF) and 6-DOF robotic manipulator simulators verify that the scheme is feasible and effective.},
  archive      = {J_TNNLS},
  author       = {Zhixin Zhang and Zhiyong Chen},
  doi          = {10.1109/TNNLS.2021.3106648},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2440-2450},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modeling and control of robotic manipulators based on symbolic regression},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical regression and classification for accurate
object detection. <em>TNNLS</em>, <em>34</em>(5), 2425–2439. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate object detection requires correct classification and high-quality localization. Currently, most of the single shot detectors (SSDs) conduct simultaneous classification and regression using a fully convolutional network. Despite high efficiency, this structure has some inappropriate designs for accurate object detection. The first one is the mismatch of bounding box classification, where the classification results of the default bounding boxes are improperly treated as the results of the regressed bounding boxes during the inference. The second one is that only one-time regression is not good enough for high-quality object localization. To solve the problem of classification mismatch, we propose a novel reg-offset-cls (ROC) module including three hierarchical steps: the regression of the default bounding box, the prediction of new feature sampling locations, and the classification of the regressed bounding box with more accurate features. For high-quality localization, we stack two ROC modules together. The input of the second ROC module is the output of the first ROC module. In addition, we inject a feature enhanced (FE) module between two stacked ROC modules to extract more contextual information. The experiments on three different datasets (i.e., MS COCO, PASCAL VOC, and UAVDT) are performed to demonstrate the effectiveness and superiority of our method. Without any bells or whistles, our proposed method outperforms state-of-the-art one-stage methods at a real-time speed. The source code is available at https://github.com/JialeCao001/HSD .},
  archive      = {J_TNNLS},
  author       = {Jiale Cao and Yanwei Pang and Jungong Han and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3106641},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2425-2439},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical regression and classification for accurate object detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A segmented variable-parameter ZNN for dynamic quadratic
minimization with improved convergence and robustness. <em>TNNLS</em>,
<em>34</em>(5), 2413–2424. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a category of the recurrent neural network (RNN), zeroing neural network (ZNN) can effectively handle time-variant optimization issues. Compared with the fixed-parameter ZNN that needs to be adjusted frequently to achieve good performance, the conventional variable-parameter ZNN (VPZNN) does not require frequent adjustment, but its variable parameter will tend to infinity as time grows. Besides, the existing noise-tolerant ZNN model is not good enough to deal with time-varying noise. Therefore, a new-type segmented VPZNN (SVPZNN) for handling the dynamic quadratic minimization issue (DQMI) is presented in this work. Unlike the previous ZNNs, the SVPZNN includes an integral term and a nonlinear activation function, in addition to two specially constructed time-varying piecewise parameters. This structure keeps the time-varying parameters stable and makes the model have strong noise tolerance capability. Besides, theoretical analysis on SVPZNN is proposed to determine the upper bound of convergence time in the absence or presence of noise interference. Numerical simulations verify that SVPZNN has shorter convergence time and better robustness than existing ZNN models when handling DQMI.},
  archive      = {J_TNNLS},
  author       = {Lin Xiao and Yongjun He and Yaonan Wang and Jianhua Dai and Ran Wang and Wensheng Tang},
  doi          = {10.1109/TNNLS.2021.3106640},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2413-2424},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A segmented variable-parameter ZNN for dynamic quadratic minimization with improved convergence and robustness},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Long-term influenza outbreak forecast using time-precedence
correlation of web data. <em>TNNLS</em>, <em>34</em>(5), 2400–2412. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influenza leads to many deaths every year and is a threat to human health. For effective prevention, traditional national-scale statistical surveillance systems have been developed, and numerous studies have been conducted to predict influenza outbreaks using web data. Most studies have captured the short-term signs of influenza outbreaks, such as one-week prediction using the characteristics of web data uploaded in real time; however, long-term predictions of more than 2–10 weeks are required to effectively cope with influenza outbreaks. In this study, we determined that web data uploaded in real time have a time-precedence relationship with influenza outbreaks. For example, a few weeks before an influenza pandemic, the word “colds” appears frequently in web data. The web data after the appearance of the word “colds” can be used as information for forecasting future influenza outbreaks, which can improve long-term influenza prediction accuracy. In this study, we propose a novel long-term influenza outbreak forecast model utilizing the time precedence between the emergence of web data and an influenza outbreak. Based on the proposed model, we conducted experiments on: 1) selecting suitable web data for long-term influenza prediction; 2) determining whether the proposed model is regionally dependent; and 3) evaluating the accuracy according to the prediction timeframe. The proposed model showed a correlation of 0.87 in the long-term prediction of ten weeks while significantly outperforming other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Beakcheol Jang and Inhwan Kim and Jong Wook Kim},
  doi          = {10.1109/TNNLS.2021.3106637},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2400-2412},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Long-term influenza outbreak forecast using time-precedence correlation of web data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inverse reinforcement q-learning through expert imitation
for discrete-time systems. <em>TNNLS</em>, <em>34</em>(5), 2386–2399.
(<a href="https://doi.org/10.1109/TNNLS.2021.3106635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In inverse reinforcement learning (RL), there are two agents. An expert target agent has a performance cost function and exhibits control and state behaviors to a learner. The learner agent does not know the expert’s performance cost function but seeks to reconstruct it by observing the expert’s behaviors and tries to imitate these behaviors optimally by its own response. In this article, we formulate an imitation problem where the optimal performance intent of a discrete-time (DT) expert target agent is unknown to a DT Learner agent. Using only the observed expert’s behavior trajectory, the learner seeks to determine a cost function that yields the same optimal feedback gain as the expert’s, and thus, imitates the optimal response of the expert. We develop an inverse RL approach with a new scheme to solve the behavior imitation problem. The approach consists of a cost function update based on an extension of RL policy iteration and inverse optimal control, and a control policy update based on optimal control. Then, under this scheme, we develop an inverse reinforcement Q-learning algorithm, which is an extension of RL Q-learning. This algorithm does not require any knowledge of agent dynamics. Proofs of stability, convergence, and optimality are given. A key property about the nonunique solution is also shown. Finally, simulation experiments are presented to show the effectiveness of the new approach.},
  archive      = {J_TNNLS},
  author       = {Wenqian Xue and Bosen Lian and Jialu Fan and Patrik Kolaric and Tianyou Chai and Frank L. Lewis},
  doi          = {10.1109/TNNLS.2021.3106635},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2386-2399},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Inverse reinforcement Q-learning through expert imitation for discrete-time systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rational continuous neural network identifier for singular
perturbed systems with uncertain dynamical models. <em>TNNLS</em>,
<em>34</em>(5), 2374–2385. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims at designing a robust nonparametric identifier for a class of singular perturbed systems (SPSs) with uncertain mathematical models. The identifier structure uses a novel identifier based on a differential neural network (DNN) with rational form, which can take into account the multirate nature of SPS. The identifier uses a mixed learning law including a rational formulation of neural networks which is useful to solve the identification of the fast dynamics in the SPS dynamics. The rational form of the design is proposed in such a way that no-singularities (denominator part of the rational form never touches the origin) are allowed in the identifier dynamics. A proposed control Lyapunov function and a nonlinear parameter identification methodology yield to design the learning laws for the class of novel rational DNN which appears as the main contribution of this study. A complementary matrix inequality-based optimization method allows to get the smallest attainable convergence invariant region. A detailed implementation methodology is also given in the study with the aim of clarifying how the proposed identifier can be used in diverse SPSs. A numerical example considering the dynamics of the enzymatic-substrate-inhibitor system with uncertain dynamics is showing how to apply the DNN identifier using the multirate nature of the proposed DNN identifier for SPSs. The proposed identifier is compared to a classical identifier which is not taking into account the multirate nature of SPS. The benefits of using the rational form for the identifier are highlighted in the numerical performance comparison based on the mean square error (MSE). This example justifies the ability of the suggested identifier to reconstruct both the fast and slow dynamics of the SPS.},
  archive      = {J_TNNLS},
  author       = {O. Andrianova and A. Poznyak and R. Q. Fuentes-Aguilar and Isaac Chairez},
  doi          = {10.1109/TNNLS.2021.3106574},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2374-2385},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rational continuous neural network identifier for singular perturbed systems with uncertain dynamical models},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuromorphic system using memcapacitors and autonomous local
learning. <em>TNNLS</em>, <em>34</em>(5), 2366–2373. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence is used for various applications and is promising as an indispensable infrastructure in future societies. Neural networks are representative technologies that imitate human brains and exhibit various advantages. However, the size is bulky, the power is huge, and some advantages are not demonstrated because they are executed on Neumann-type computers. Neuromorphic systems are biomimetic systems from the hardware level to implement neuron and synapse elements, and the size is compact, the power is low, and the operation is robust. However, because the conventional ones are not composed of fully optimized hardware, the power is not yet minimal, and extra control circuits must be used. In this article, we developed a neuromorphic system using memcapacitors and autonomous local learning. By using memcapacitors, the power can be minimal, and by using autonomous local learning, the control circuits to handle the synapse elements can be deleted. First, the memcapacitors are completed in a cross-bar array, where the ferroelectric layers are sandwiched between the horizontal and perpendicular electrodes. The polarization and capacitance exhibit hysteresis due to the dielectric polarization. Next, autonomous local learning is introduced as follows. During the training phase, associative patterns to be memorized are directly sent, relatively high voltages are applied, and dielectric polarizations are induced. During the operation phase, relatively low voltages are applied, and input signals are weighted with the capacitances of the memcapacitors, summed, and transferred as the output signals. Finally, the experimental system is set up, and the experimental results are acquired. The memorized patterns during the training phase, distorted patterns as the input signals during the operation phase, and retrieved patterns as the output signals in the operation phase are shown. Researchers found that the retrieved patterns are completely the same as the memorized patterns. This means that the neuromorphic system works as an associative memory.},
  archive      = {J_TNNLS},
  author       = {Mutsumi Kimura and Yuma Ishisaki and Yuta Miyabe and Homare Yoshida and Isato Ogawa and Tomoharu Yokoyama and Ken-Ichi Haga and Eisuke Tokumitsu and Yasuhiko Nakashima},
  doi          = {10.1109/TNNLS.2021.3106566},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2366-2373},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuromorphic system using memcapacitors and autonomous local learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A minimax probability machine for nondecomposable
performance measures. <em>TNNLS</em>, <em>34</em>(5), 2353–2365. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced classification tasks are widespread in many real-world applications. For such classification tasks, in comparison with the accuracy rate (AR), it is usually much more appropriate to use nondecomposable performance measures such as the area under the receiver operating characteristic curve (AUC) and the $F_\beta $ measure as the classification criterion since the label class is imbalanced. On the other hand, the minimax probability machine is a popular method for binary classification problems and aims at learning a linear classifier by maximizing the AR, which makes it unsuitable to deal with imbalanced classification tasks. The purpose of this article is to develop a new minimax probability machine for the $F_\beta $ measure, called minimax probability machine for the $F_\beta $ -measures (MPMF), which can be used to deal with imbalanced classification tasks. A brief discussion is also given on how to extend the MPMF model for several other nondecomposable performance measures listed in the article. To solve the MPMF model effectively, we derive its equivalent form which can then be solved by an alternating descent method to learn a linear classifier. Further, the kernel trick is employed to derive a nonlinear MPMF model to learn a nonlinear classifier. Several experiments on real-world benchmark datasets demonstrate the effectiveness of our new model.},
  archive      = {J_TNNLS},
  author       = {Junru Luo and Hong Qiao and Bo Zhang},
  doi          = {10.1109/TNNLS.2021.3106484},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2353-2365},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A minimax probability machine for nondecomposable performance measures},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surrogate-assisted hybrid-model estimation of distribution
algorithm for mixed-variable hyperparameters optimization in
convolutional neural networks. <em>TNNLS</em>, <em>34</em>(5),
2338–2352. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of a convolutional neural network (CNN) heavily depends on its hyperparameters. However, finding a suitable hyperparameters configuration is difficult, challenging, and computationally expensive due to three issues, which are 1) the mixed-variable problem of different types of hyperparameters; 2) the large-scale search space of finding optimal hyperparameters; and 3) the expensive computational cost for evaluating candidate hyperparameters configuration. Therefore, this article focuses on these three issues and proposes a novel estimation of distribution algorithm (EDA) for efficient hyperparameters optimization, with three major contributions in the algorithm design. First, a hybrid-model EDA is proposed to efficiently deal with the mixed-variable difficulty. The proposed algorithm uses a mixed-variable encoding scheme to encode the mixed-variable hyperparameters and adopts an adaptive hybrid-model learning (AHL) strategy to efficiently optimize the mixed-variables. Second, an orthogonal initialization (OI) strategy is proposed to efficiently deal with the challenge of large-scale search space. Third, a surrogate-assisted multi-level evaluation (SME) method is proposed to reduce the expensive computational cost. Based on the above, the proposed algorithm is named $s$ urrogate-assisted hybrid-model EDA (SHEDA). For experimental studies, the proposed SHEDA is verified on widely used classification benchmark problems, and is compared with various state-of-the-art methods. Moreover, a case study on aortic dissection (AD) diagnosis is carried out to evaluate its performance. Experimental results show that the proposed SHEDA is very effective and efficient for hyperparameters optimization, which can find a satisfactory hyperparameters configuration for the CIFAR10, CIFAR100, and AD diagnosis with only 0.58, 0.97, and 1.18 GPU days, respectively.},
  archive      = {J_TNNLS},
  author       = {Jian-Yu Li and Zhi-Hui Zhan and Jin Xu and Sam Kwong and Jun Zhang},
  doi          = {10.1109/TNNLS.2021.3106399},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2338-2352},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Surrogate-assisted hybrid-model estimation of distribution algorithm for mixed-variable hyperparameters optimization in convolutional neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph polish: A novel graph generation paradigm for
molecular optimization. <em>TNNLS</em>, <em>34</em>(5), 2323–2337. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular optimization, which transforms a given input molecule $X$ into another $Y$ with desired properties, is essential in molecular drug discovery. The traditional approaches either suffer from sample-inefficient learning or ignore information that can be captured with the supervised learning of optimized molecule pairs. In this study, we present a novel molecular optimization paradigm, Graph Polish. In this paradigm, with the guidance of the source and target molecule pairs of the desired properties, a heuristic optimization solution can be derived: given an input molecule, we first predict which atom can be viewed as the optimization center, and then the nearby regions are optimized around this center. We then propose an effective and efficient learning framework, Teacher and Student polish, to capture the dependencies in the optimization steps. A teacher component automatically identifies and annotates the optimization centers and the preservation, removal, and addition of some parts of the molecules; a student component learns these knowledges and applies them to a new molecule. The proposed paradigm can offer an intuitive interpretation for the molecular optimization result. Experiments with multiple optimization tasks are conducted on several benchmark datasets. The proposed approach achieves a significant advantage over the six state-of-the-art baseline methods. Also, extensive studies are conducted to validate the effectiveness, explainability, and time savings of the novel optimization paradigm.},
  archive      = {J_TNNLS},
  author       = {Chaojie Ji and Yijia Zheng and Ruxin Wang and Yunpeng Cai and Hongyan Wu},
  doi          = {10.1109/TNNLS.2021.3106392},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2323-2337},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph polish: A novel graph generation paradigm for molecular optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MGML: Multigranularity multilevel feature ensemble network
for remote sensing scene classification. <em>TNNLS</em>, <em>34</em>(5),
2308–2322. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing (RS) scene classification is a challenging task to predict scene categories of RS images. RS images have two main issues: large intraclass variance caused by large resolution variance and confusing information from large geographic covering area. To ease the negative influence from the above two issues. We propose a multigranularity multilevel feature ensemble network (MGML-FENet) to efficiently tackle the RS scene classification task in this article. Specifically, we propose multigranularity multilevel feature fusion branch (MGML-FFB) to extract multigranularity features in different levels of network by channel-separate feature generator (CS-FG). To avoid the interference from confusing information, we propose a multigranularity multilevel feature ensemble module (MGML-FEM), which can provide diverse predictions by full-channel feature generator (FC-FG). Compared to previous methods, our proposed networks have the ability to use structure information and abundant fine-grained features. Furthermore, through the ensemble learning method, our proposed MGML-FENets can obtain more convincing final predictions. Extensive classification experiments on multiple RS datasets (AID, NWPU-RESISC45, UC-Merced, and VGoogle) demonstrate that our proposed networks achieve better performance than previous state-of-the-art (SOTA) networks. The visualization analysis also shows the good interpretability of MGML-FENet.},
  archive      = {J_TNNLS},
  author       = {Qi Zhao and Shuchang Lyu and Yuewen Li and Yujing Ma and Lijiang Chen},
  doi          = {10.1109/TNNLS.2021.3106391},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2308-2322},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MGML: Multigranularity multilevel feature ensemble network for remote sensing scene classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Impulsive-based almost surely synchronization for neural
network systems subject to deception attacks. <em>TNNLS</em>,
<em>34</em>(5), 2298–2307. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is dedicated to investigating the impulsive-based almost surely synchronization issue of neural network systems (NSSs) with quality-of-service constraints. First, the communication network considered suffers from random double deception attacks, which are modeled as a nonlinear function and a desynchronizing impulse sequence, respectively. Meanwhile, the impulsive instants and impulsive gains are randomly and only their expectations are available. Second, by taking two different types of random deception attacks into consideration, a novel mathematical model for vulnerable NSSs is constructed. Then, almost surely synchronization criteria are established by using Borel–Cantelli lemma. Furthermore, based on the derived strong and weak sufficient conditions, the almost surely synchronization of NSSs is achieved. Finally, the section of numerical example is shown to illustrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Shiyu Dong and Hong Zhu and Shouming Zhong and Kaibo Shi and Jianquan Lu},
  doi          = {10.1109/TNNLS.2021.3106383},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2298-2307},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Impulsive-based almost surely synchronization for neural network systems subject to deception attacks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Adaptive subspace optimization ensemble method for
high-dimensional imbalanced data classification. <em>TNNLS</em>,
<em>34</em>(5), 2284–2297. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is hard to construct an optimal classifier for high-dimensional imbalanced data, on which the performance of classifiers is seriously affected and becomes poor. Although many approaches, such as resampling, cost-sensitive, and ensemble learning methods, have been proposed to deal with the skewed data, they are constrained by high-dimensional data with noise and redundancy. In this study, we propose an adaptive subspace optimization ensemble method (ASOEM) for high-dimensional imbalanced data classification to overcome the above limitations. To construct accurate and diverse base classifiers, a novel adaptive subspace optimization (ASO) method based on adaptive subspace generation (ASG) process and rotated subspace optimization (RSO) process is designed to generate multiple robust and discriminative subspaces. Then a resampling scheme is applied on the optimized subspace to build a class-balanced data for each base classifier. To verify the effectiveness, our ASOEM is implemented based on different resampling strategies on 24 real-world high-dimensional imbalanced datasets. Experimental results demonstrate that our proposed methods outperform other mainstream imbalance learning approaches and classifier ensemble methods.},
  archive      = {J_TNNLS},
  author       = {Yuhong Xu and Zhiwen Yu and C. L. Philip Chen and Zhulin Liu},
  doi          = {10.1109/TNNLS.2021.3106306},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2284-2297},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive subspace optimization ensemble method for high-dimensional imbalanced data classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Full-scale information diffusion prediction with reinforced
recurrent networks. <em>TNNLS</em>, <em>34</em>(5), 2271–2283. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information diffusion prediction is an important task, which studies how information items spread among users. With the success of deep learning techniques, recurrent neural networks (RNNs) have shown their powerful capability in modeling information diffusion as sequential data. However, previous works focused on either microscopic diffusion prediction, which aims at guessing who will be the next influenced user at what time, or macroscopic diffusion prediction, which estimates the total numbers of influenced users during the diffusion process. To the best of our knowledge, few attempts have been made to suggest a unified model for both microscopic and macroscopic scales. In this article, we propose a novel full-scale diffusion prediction model based on reinforcement learning (RL). RL incorporates the macroscopic diffusion size information into the RNN-based microscopic diffusion model by addressing the nondifferentiable problem. We also employ an effective structural context extraction strategy to utilize the underlying social graph information. Experimental results show that our proposed model outperforms state-of-the-art baseline models on both microscopic and macroscopic diffusion predictions on three real-world datasets.},
  archive      = {J_TNNLS},
  author       = {Cheng Yang and Hao Wang and Jian Tang and Chuan Shi and Maosong Sun and Ganqu Cui and Zhiyuan Liu},
  doi          = {10.1109/TNNLS.2021.3106156},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2271-2283},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Full-scale information diffusion prediction with reinforced recurrent networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Orthogonal inductive matrix completion. <em>TNNLS</em>,
<em>34</em>(5), 2259–2270. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose orthogonal inductive matrix completion (OMIC), an interpretable approach to matrix completion based on a sum of multiple orthonormal side information terms, together with nuclear-norm regularization. The approach allows us to inject prior knowledge about the singular vectors of the ground-truth matrix. We optimize the approach by a provably converging algorithm, which optimizes all components of the model simultaneously. We study the generalization capabilities of our method in both the distribution-free setting and in the case where the sampling distribution admits uniform marginals, yielding learning guarantees that improve with the quality of the injected knowledge in both cases. As particular cases of our framework, we present models that can incorporate user and item biases or community information in a joint and additive fashion. We analyze the performance of OMIC on several synthetic and real datasets. On synthetic datasets with a sliding scale of user bias relevance, we show that OMIC better adapts to different regimes than other methods. On real-life datasets containing user/items recommendations and relevant side information, we find that OMIC surpasses the state of the art, with the added benefit of greater interpretability.},
  archive      = {J_TNNLS},
  author       = {Antoine Ledent and Rodrigo Alves and Marius Kloft},
  doi          = {10.1109/TNNLS.2021.3106155},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2259-2270},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Orthogonal inductive matrix completion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bidirectional relationship inferring network for referring
image localization and segmentation. <em>TNNLS</em>, <em>34</em>(5),
2246–2258. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, referring image localization and segmentation has aroused widespread interest. However, the existing methods lack a clear description of the interdependence between language and vision. To this end, we present a bidirectional relationship inferring network (BRINet) to effectively address the challenging tasks. Specifically, we first employ a vision-guided linguistic attention module to perceive the keywords corresponding to each image region. Then, language-guided visual attention adopts the learned adaptive language to guide the update of the visual features. Together, they form a bidirectional cross-modal attention module (BCAM) to achieve the mutual guidance between language and vision. They can help the network align the cross-modal features better. Based on the vanilla language-guided visual attention, we further design an asymmetric language-guided visual attention, which significantly reduces the computational cost by modeling the relationship between each pixel and each pooled subregion. In addition, a segmentation-guided bottom-up augmentation module (SBAM) is utilized to selectively combine multilevel information flow for object localization. Experiments show that our method outperforms other state-of-the-art methods on three referring image localization datasets and four referring image segmentation datasets.},
  archive      = {J_TNNLS},
  author       = {Guang Feng and Zhiwei Hu and Lihe Zhang and Jiayu Sun and Huchuan Lu},
  doi          = {10.1109/TNNLS.2021.3106153},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2246-2258},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bidirectional relationship inferring network for referring image localization and segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonlinear causal discovery for high-dimensional
deterministic data. <em>TNNLS</em>, <em>34</em>(5), 2234–2245. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear causal discovery with high-dimensional data where each variable is multidimensional plays a significant role in many scientific disciplines, such as social network analysis. Previous work majorly focuses on exploiting asymmetry in the causal and anticausal directions between two high-dimensional variables (a cause–effect pair). Although there exist some works that concentrate on the causal order identification between multiple variables, i.e., more than two high-dimensional variables, they do not validate the consistency of methods through theoretical analysis on multiple-variable data. In particular, based on the asymmetry for the cause–effect pair, if model assumptions for any pair of the data are violated, the asymmetry condition will not hold, resulting in the deduction of incorrect order identification. Thus, in this article, we propose a causal functional model, namely high-dimensional deterministic model (HDDM), to identify the causal orderings among multiple high-dimensional variables. We derive two candidates’ selection rules to alleviate the inconvenient effects resulted from the violated-assumption pairs. The corresponding theoretical justification is provided as well. With these theoretical results, we develop a method to infer causal orderings for nonlinear multiple-variable data. Simulations on synthetic data and real-world data are conducted to verify the efficacy of our proposed method. Since we focus on deterministic relations in our method, we also verify the robustness of the noises in simulations.},
  archive      = {J_TNNLS},
  author       = {Yan Zeng and Zhifeng Hao and Ruichu Cai and Feng Xie and Libo Huang and Shohei Shimizu},
  doi          = {10.1109/TNNLS.2021.3106111},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2234-2245},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonlinear causal discovery for high-dimensional deterministic data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leaderless and leader-following bipartite consensus of
multiagent systems with sampled and delayed information. <em>TNNLS</em>,
<em>34</em>(5), 2220–2233. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a hybrid systems approach to address the sampled-data leaderless and leader-following bipartite consensus problems of multiagent systems (MAS) with communication delays. First, distributed asynchronous sampled-data bipartite consensus protocols are proposed based on estimators. Then, by introducing appropriate intermediate variables and internal auxiliary variables, a unified hybrid model, consisting of flow dynamics and jump dynamics, is constructed to describe the closed-loop dynamics of both leaderless and leader-following MAS. Based on this model, the leaderless and leader-following bipartite consensus is equivalent to stability of a hybrid system, and Lyapunov-based stability results are then developed under hybrid systems framework. With the proposed method, explicit upper bounds of sampling periods and communication delays can be calculated. Finally, simulation examples are given to show the effectiveness.},
  archive      = {J_TNNLS},
  author       = {Guanglei Zhao and Changchun Hua},
  doi          = {10.1109/TNNLS.2021.3106015},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2220-2233},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Leaderless and leader-following bipartite consensus of multiagent systems with sampled and delayed information},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Kronecker CP decomposition with fast multiplication for
compressing RNNs. <em>TNNLS</em>, <em>34</em>(5), 2205–2219. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) are powerful in the tasks oriented to sequential data, such as natural language processing and video recognition. However, because the modern RNNs have complex topologies and expensive space/computation complexity, compressing them becomes a hot and promising topic in recent years. Among plenty of compression methods, tensor decomposition, e.g., tensor train (TT), block term (BT), tensor ring (TR), and hierarchical Tucker (HT), appears to be the most amazing approach because a very high compression ratio might be obtained. Nevertheless, none of these tensor decomposition formats can provide both space and computation efficiency. In this article, we consider to compress RNNs based on a novel Kronecker CANDECOMP/PARAFAC (KCP) decomposition, which is derived from Kronecker tensor (KT) decomposition, by proposing two fast algorithms of multiplication between the input and the tensor-decomposed weight. According to our experiments based on UCF11, Youtube Celebrities Face, UCF50, TIMIT, TED-LIUM, and Spiking Heidelberg digits datasets, it can be verified that the proposed KCP-RNNs have a comparable performance of accuracy with those in other tensor-decomposed formats, and even 278 $219\times $ compression ratio could be obtained by the low-rank KCP. More importantly, KCP-RNNs are efficient in both space and computation complexity compared with other tensor-decomposed ones. Besides, we find KCP has the best potential of parallel computing to accelerate the calculations in neural networks.},
  archive      = {J_TNNLS},
  author       = {Dingheng Wang and Bijiao Wu and Guangshe Zhao and Man Yao and Hengnu Chen and Lei Deng and Tianyi Yan and Guoqi Li},
  doi          = {10.1109/TNNLS.2021.3105961},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2205-2219},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Kronecker CP decomposition with fast multiplication for compressing RNNs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable binding for sparse distributed representations:
Theory and applications. <em>TNNLS</em>, <em>34</em>(5), 2191–2204. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable binding is a cornerstone of symbolic reasoning and cognition. But how binding can be implemented in connectionist models has puzzled neuroscientists, cognitive psychologists, and neural network researchers for many decades. One type of connectionist model that naturally includes a binding operation is vector symbolic architectures (VSAs). In contrast to other proposals for variable binding, the binding operation in VSAs is dimensionality-preserving, which enables representing complex hierarchical data structures, such as trees, while avoiding a combinatoric expansion of dimensionality. Classical VSAs encode symbols by dense randomized vectors, in which information is distributed throughout the entire neuron population. By contrast, in the brain, features are encoded more locally, by the activity of single neurons or small groups of neurons, often forming sparse vectors of neural activation. Following Laiho et al. (2015), we explore symbolic reasoning with a special case of sparse distributed representations. Using techniques from compressed sensing, we first show that variable binding in classical VSAs is mathematically equivalent to tensor product binding between sparse feature vectors, another well-known binding operation which increases dimensionality. This theoretical result motivates us to study two dimensionality-preserving binding methods that include a reduction of the tensor matrix into a single sparse vector. One binding method for general sparse vectors uses random projections, the other, block-local circular convolution, is defined for sparse vectors with block structure, sparse block-codes. Our experiments reveal that block-local circular convolution binding has ideal properties, whereas random projection based binding also works, but is lossy. We demonstrate in example applications that a VSA with block-local circular convolution and sparse block-codes reaches similar performance as classical VSAs. Finally, we discuss our results in the context of neuroscience and neural networks.},
  archive      = {J_TNNLS},
  author       = {Edward Paxon Frady and Denis Kleyko and Friedrich T. Sommer},
  doi          = {10.1109/TNNLS.2021.3105949},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2191-2204},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Variable binding for sparse distributed representations: Theory and applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synergistic integration between machine learning and
agent-based modeling: A multidisciplinary review. <em>TNNLS</em>,
<em>34</em>(5), 2170–2190. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent-based modeling (ABM) involves developing models in which agents make adaptive decisions in a changing environment. Machine-learning (ML) based inference models can improve sequential decision-making by learning agents’ behavioral patterns. With the aid of ML, this emerging area can extend traditional agent-based schemes that hardcode agents’ behavioral rules into an adaptive model. Even though there are plenty of studies that apply ML in ABMs, the generalized applicable scenarios, frameworks, and procedures for implementations are not well addressed. In this article, we provide a comprehensive review of applying ML in ABM based on four major scenarios, i.e., microagent-level situational awareness learning, microagent-level behavior intervention, macro-ABM-level emulator, and sequential decision-making. For these four scenarios, the related algorithms, frameworks, procedures of implementations, and multidisciplinary applications are thoroughly investigated. We also discuss how ML can improve prediction in ABMs by trading off the variance and bias and how ML can improve the sequential decision-making of microagent and macrolevel policymakers via a mechanism of reinforced behavioral intervention. At the end of this article, future perspectives of applying ML in ABMs are discussed with respect to data acquisition and quality issues, the possible solution of solving the convergence problem of reinforcement learning, interpretable ML applications, and bounded rationality of ABM.},
  archive      = {J_TNNLS},
  author       = {Wei Zhang and Andrea Valencia and Ni-Bin Chang},
  doi          = {10.1109/TNNLS.2021.3106777},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2170-2190},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synergistic integration between machine learning and agent-based modeling: A multidisciplinary review},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Activated gradients for deep neural networks.
<em>TNNLS</em>, <em>34</em>(4), 2156–2168. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks often suffer from poor performance or even training failure due to the ill-conditioned problem, the vanishing/exploding gradient problem, and the saddle point problem. In this article, a novel method by acting the gradient activation function (GAF) on the gradient is proposed to handle these challenges. Intuitively, the GAF enlarges the tiny gradients and restricts the large gradient. Theoretically, this article gives conditions that the GAF needs to meet and, on this basis, proves that the GAF alleviates the problems mentioned above. In addition, this article proves that the convergence rate of SGD with the GAF is faster than that without the GAF under some assumptions. Furthermore, experiments on CIFAR, ImageNet, and PASCAL visual object classes confirm the GAF’s effectiveness. The experimental results also demonstrate that the proposed method is able to be adopted in various deep neural networks to improve their performance. The source code is publicly available at https://github.com/LongJin-lab/Activated-Gradients-for-Deep-Neural-Networks .},
  archive      = {J_TNNLS},
  author       = {Mei Liu and Liangming Chen and Xiaohao Du and Long Jin and Mingsheng Shang},
  doi          = {10.1109/TNNLS.2021.3106044},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2156-2168},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Activated gradients for deep neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Granular ball sampling for noisy label classification or
imbalanced classification. <em>TNNLS</em>, <em>34</em>(4), 2144–2155.
(<a href="https://doi.org/10.1109/TNNLS.2021.3105984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a general sampling method, called granular-ball sampling (GBS), for classification problems by introducing the idea of granular computing. The GBS method uses some adaptively generated hyperballs to cover the data space, and the points on the hyperballs constitute the sampled data. GBS is the first sampling method that not only reduces the data size but also improves the data quality in noisy label classification. In addition, because the GBS method can be used to exactly describe the boundary, it can obtain almost the same classification accuracy as the results on the original datasets, and it can obtain an obviously higher classification accuracy than random sampling. Therefore, for the data reduction classification task, GBS is a general method that is not especially restricted by any specific classifier or dataset. Moreover, the GBS can be effectively used as an undersampling method for imbalanced classification. It has a time complexity that is close to O( $N$ ), so it can accelerate most classifiers. These advantages make GBS powerful for improving the performance of classifiers. All codes have been released in the open source GBS library at http://www.cquptshuyinxia.com/GBS.html .},
  archive      = {J_TNNLS},
  author       = {Shuyin Xia and Shaoyuan Zheng and Guoyin Wang and Xinbo Gao and Binggui Wang},
  doi          = {10.1109/TNNLS.2021.3105984},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2144-2155},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Granular ball sampling for noisy label classification or imbalanced classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective neural evolutionary algorithm for
combinatorial optimization problems. <em>TNNLS</em>, <em>34</em>(4),
2133–2143. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a recent surge of success in optimizing deep reinforcement learning (DRL) models with neural evolutionary algorithms. This type of method is inspired by biological evolution and uses different genetic operations to evolve neural networks. Previous neural evolutionary algorithms mainly focused on single-objective optimization problems (SOPs). In this article, we present an end-to-end multi-objective neural evolutionary algorithm based on decomposition and dominance (MONEADD) for combinatorial optimization problems. The proposed MONEADD is an end-to-end algorithm that utilizes genetic operations and rewards signals to evolve neural networks for different combinatorial optimization problems without further engineering. To accelerate convergence, a set of nondominated neural networks is maintained based on the notion of dominance and decomposition in each generation. In inference time, the trained model can be directly utilized to solve similar problems efficiently, while the conventional heuristic methods need to learn from scratch for every given test problem. To further enhance the model performance in inference time, three multi-objective search strategies are introduced in this work. Our experimental results clearly show that the proposed MONEADD has a competitive and robust performance on a bi-objective of the classic travel salesman problem (TSP), as well as Knapsack problem up to 200 instances. We also empirically show that the designed MONEADD has good scalability when distributed on multiple graphics processing units (GPUs).},
  archive      = {J_TNNLS},
  author       = {Yinan Shao and Jerry Chun-Wei Lin and Gautam Srivastava and Dongdong Guo and Hongchun Zhang and Hu Yi and Alireza Jolfaei},
  doi          = {10.1109/TNNLS.2021.3105937},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2133-2143},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-objective neural evolutionary algorithm for combinatorial optimization problems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Solving dynamic traveling salesman problems with deep
reinforcement learning. <em>TNNLS</em>, <em>34</em>(4), 2119–2132. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A traveling salesman problem (TSP) is a well-known NP-complete problem. Traditional TSP presumes that the locations of customers and the traveling time among customers are fixed and constant. In real-life cases, however, the traffic conditions and customer requests may change over time. To find the most economic route, the decisions can be made constantly upon the time-point when the salesman completes his service of each customer. This brings in a dynamic version of the traveling salesman problem (DTSP), which takes into account the information of real-time traffic and customer requests. DTSP can be extended to a dynamic pickup and delivery problem (DPDP). In this article, we ameliorate the attention model to make it possible to perceive environmental changes. A deep reinforcement learning algorithm is proposed to solve DTSP and DPDP instances with a size of up to 40 customers in 100 locations. Experiments show that our method can capture the dynamic changes and produce a highly satisfactory solution within a very short time. Compared with other baseline approaches, more than 5\% improvements can be observed in many cases.},
  archive      = {J_TNNLS},
  author       = {Zizhen Zhang and Hong Liu and MengChu Zhou and Jiahai Wang},
  doi          = {10.1109/TNNLS.2021.3105905},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2119-2132},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Solving dynamic traveling salesman problems with deep reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully complex-valued dendritic neuron model. <em>TNNLS</em>,
<em>34</em>(4), 2105–2118. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single dendritic neuron model (DNM) that owns the nonlinear information processing ability of dendrites has been widely used for classification and prediction. Complex-valued neural networks that consist of a number of multiple/deep-layer McCulloch-Pitts neurons have achieved great successes so far since neural computing was utilized for signal processing. Yet no complex value representations appear in single neuron architectures. In this article, we first extend DNM from a real-value domain to a complex-valued one. Performance of complex-valued DNM (CDNM) is evaluated through a complex XOR problem, a non-minimum phase equalization problem, and a real-world wind prediction task. Also, a comparative analysis on a set of elementary transcendental functions as an activation function is implemented and preparatory experiments are carried out for determining hyperparameters. The experimental results indicate that the proposed CDNM significantly outperforms real-valued DNM, complex-valued multi-layer perceptron, and other complex-valued neuron models.},
  archive      = {J_TNNLS},
  author       = {Shangce Gao and MengChu Zhou and Ziqian Wang and Daiki Sugiyama and Jiujun Cheng and Jiahai Wang and Yuki Todo},
  doi          = {10.1109/TNNLS.2021.3105901},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2105-2118},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully complex-valued dendritic neuron model},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UNMAS: Multiagent reinforcement learning for unshaped
cooperative scenarios. <em>TNNLS</em>, <em>34</em>(4), 2093–2104. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent reinforcement learning methods, such as VDN, QMIX, and QTRAN, that adopt centralized training with decentralized execution (CTDE) framework have shown promising results in cooperation and competition. However, in some multiagent scenarios, the number of agents and the size of the action set actually vary over time. We call these unshaped scenarios, and the methods mentioned above fail in performing satisfyingly. In this article, we propose a new method, called Unshaped Networks for Multiagent Systems (UNMAS), which adapts to the number and size changes in multiagent systems. We propose the self-weighting mixing network to factorize the joint action-value. Its adaption to the change in agent number is attributed to the nonlinear mapping from each-agent Q value to the joint action-value with individual weights. Besides, in order to address the change in an action set, each agent constructs an individual action-value network that is composed of two streams to evaluate the constant environment-oriented subset and the varying unit-oriented subset. We evaluate UNMAS on various StarCraft II micromanagement scenarios and compare the results with several state-of-the-art MARL algorithms. The superiority of UNMAS is demonstrated by its highest winning rates especially on the most difficult scenario 3s5z_vs_3s6z. The agents learn to perform effectively cooperative behaviors, while other MARL algorithms fail. Animated demonstrations and source code are provided in https://sites.google.com/view/unmas .},
  archive      = {J_TNNLS},
  author       = {Jiajun Chai and Weifan Li and Yuanheng Zhu and Dongbin Zhao and Zhe Ma and Kewu Sun and Jishiyu Ding},
  doi          = {10.1109/TNNLS.2021.3105869},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2093-2104},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {UNMAS: Multiagent reinforcement learning for unshaped cooperative scenarios},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multisource heterogeneous domain adaptation with conditional
weighting adversarial network. <em>TNNLS</em>, <em>34</em>(4),
2079–2092. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous domain adaptation (HDA) tackles the learning of cross-domain samples with both different probability distributions and feature representations. Most of the existing HDA studies focus on the single-source scenario. In reality, however, it is not uncommon to obtain samples from multiple heterogeneous domains. In this article, we study the multisource HDA problem and propose a conditional weighting adversarial network (CWAN) to address it. The proposed CWAN adversarially learns a feature transformer, a label classifier, and a domain discriminator. To quantify the importance of different source domains, CWAN introduces a sophisticated conditional weighting scheme to calculate the weights of the source domains according to the conditional distribution divergence between the source and target domains. Different from existing weighting schemes, the proposed conditional weighting scheme not only weights the source domains but also implicitly aligns the conditional distributions during the optimization process. Experimental results clearly demonstrate that the proposed CWAN performs much better than several state-of-the-art methods on four real-world datasets.},
  archive      = {J_TNNLS},
  author       = {Yuan Yao and Xutao Li and Yu Zhang and Yunming Ye},
  doi          = {10.1109/TNNLS.2021.3105868},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2079-2092},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multisource heterogeneous domain adaptation with conditional weighting adversarial network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spectral clustering with adaptive neighbors for deep
learning. <em>TNNLS</em>, <em>34</em>(4), 2068–2078. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is a well-known clustering algorithm for unsupervised learning, and its improved algorithms have been successfully adapted for many real-world applications. However, traditional spectral clustering algorithms are still facing many challenges to the task of unsupervised learning for large-scale datasets because of the complexity and cost of affinity matrix construction and the eigen-decomposition of the Laplacian matrix. From this perspective, we are looking forward to finding a more efficient and effective way by adaptive neighbor assignments for affinity matrix construction to address the above limitation of spectral clustering. It tries to learn an affinity matrix from the view of global data distribution. Meanwhile, we propose a deep learning framework with fully connected layers to learn a mapping function for the purpose of replacing the traditional eigen-decomposition of the Laplacian matrix. Extensive experimental results have illustrated the competitiveness of the proposed algorithm. It is significantly superior to the existing clustering algorithms in the experiments of both toy datasets and real-world datasets.},
  archive      = {J_TNNLS},
  author       = {Yang Zhao and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3105822},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2068-2078},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spectral clustering with adaptive neighbors for deep learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DRRNets: Dynamic recurrent routing via low-rank
regularization in recurrent neural networks. <em>TNNLS</em>,
<em>34</em>(4), 2057–2067. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) continue to show outstanding performance in sequence learning tasks such as language modeling, but it remains difficult to train RNNs for long sequences. The main challenges lie in the complex dependencies, gradient vanishing or exploding, and low resource requirement in model deployment. In order to address these challenges, we propose dynamic recurrent routing neural networks (DRRNets), which can: 1) shorten the recurrent lengths by allocating recurrent routes dynamically for different dependencies and 2) reduce the number of parameters significantly by imposing low-rank constraints on the fully connected layers. A novel optimization algorithm via low-rank constraint and sparsity projection is developed to train the network. We verify the effectiveness of the proposed method by comparing it with multiple competitive approaches in several popular sequential learning tasks, such as language modeling and speaker recognition. The results in terms of different criteria demonstrate the superiority of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Dongjing Shan and Yong Luo and Xiongwei Zhang and Chao Zhang},
  doi          = {10.1109/TNNLS.2021.3105818},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2057-2067},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DRRNets: Dynamic recurrent routing via low-rank regularization in recurrent neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised subspace learning with flexible neighboring.
<em>TNNLS</em>, <em>34</em>(4), 2043–2056. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based subspace learning has been widely used in various applications as the rapid growth of data dimension, while the graph is constructed by affinity matrix of input data. However, it is difficult for these subspace learning methods to preserve the intrinsic local structure of data with the high-dimensional noise. To address this problem, we proposed a novel unsupervised dimensionality reduction approach named unsupervised subspace learning with flexible neighboring (USFN). We learn a similarity graph by adaptive probabilistic neighborhood learning process to preserve the manifold structure of high-dimensional data. In addition, we utilize the flexible neighboring to learn projection and latent representation of manifold structure of high-dimensional data to remove the impact of noise. The adaptive similarity graph and latent representation are jointly learned by integrating adaptive probabilistic neighborhood learning and manifold residue term into a unified objection function. The experimental results on synthetic and real-world datasets demonstrate the performance of the proposed unsupervised subspace learning USFN method.},
  archive      = {J_TNNLS},
  author       = {Weizhong Yu and Jintang Bian and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3105813},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2043-2056},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised subspace learning with flexible neighboring},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward on-device federated learning: A direct acyclic
graph-based blockchain approach. <em>TNNLS</em>, <em>34</em>(4),
2028–2042. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the distributed characteristics of federated learning (FL), the vulnerability of the global model and the coordination of devices are the main obstacle. As a promising solution of decentralization, scalability, and security, leveraging the blockchain in FL has attracted much attention in recent years. However, the traditional consensus mechanisms designed for blockchain-like proof of work (PoW) would cause extreme resource consumption, which reduces the efficiency of FL greatly, especially when the participating devices are wireless and resource-limited. In order to address device asynchrony and anomaly detection in FL while avoiding the extra resource consumption caused by blockchain, this article introduces a framework for empowering FL using direct acyclic graph (DAG)-based blockchain systematically (DAG-FL). Accordingly, DAG-FL is first introduced from a three-layer architecture in detail, and then, two algorithms DAG-FL Controlling and DAG-FL Updating are designed running on different nodes to elaborate the operation of the DAG-FL consensus mechanism. After that, a Poisson process model is formulated to discuss that how to set deployment parameters to maintain DAG-FL stably in different FL tasks. The extensive simulations and experiments show that DAG-FL can achieve better performance in terms of training efficiency and model accuracy compared with the typical existing on-device FL systems as the benchmarks.},
  archive      = {J_TNNLS},
  author       = {Mingrui Cao and Long Zhang and Bin Cao},
  doi          = {10.1109/TNNLS.2021.3105810},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2028-2042},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward on-device federated learning: A direct acyclic graph-based blockchain approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proposal of a control algorithm for multiagent cooperation
using spiking neural networks. <em>TNNLS</em>, <em>34</em>(4),
2016–2027. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study deals with the issue of using spiking neural networks (SNNs) in multiagent systems. The research objective is a proposal of a control algorithm for the cooperation of a group of agents using SNNs, application of the Izhikevich model, and plasticity depending on the timing of action potentials. The proposed method has been verified and experimentally tested, proving numerous advantages over second-generation networks. The advantages and the application in real systems are described in the research conclusions.},
  archive      = {J_TNNLS},
  author       = {Adam Barton and Eva Volna and Martin Kotyrba and Robert Jarusek},
  doi          = {10.1109/TNNLS.2021.3105800},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2016-2027},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Proposal of a control algorithm for multiagent cooperation using spiking neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A projection neural network to nonsmooth constrained
pseudoconvex optimization. <em>TNNLS</em>, <em>34</em>(4), 2001–2015.
(<a href="https://doi.org/10.1109/TNNLS.2021.3105732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a single-layer projection neural network based on penalty function and differential inclusion is proposed to solve nonsmooth pseudoconvex optimization problems with linear equality and convex inequality constraints, and the bound constraints, such as box and sphere types, in inequality constraints are processed by projection operator. By introducing the Tikhonov-like regularization method, the proposed neural network no longer needs to calculate the exact penalty parameters. Under mild assumptions, by nonsmooth analysis, it is proved that the state solution of the proposed neural network is always bounded and globally exists, and enters the constrained feasible region in a finite time, and never escapes from this region again. Finally, the state solution converges to an optimal solution for the considered optimization problem. Compared with some other existing neural networks based on subgradients, this algorithm eliminates the dependence on the selection of the initial point, which is a neural network model with a simple structure and low calculation load. Three numerical experiments and two application examples are used to illustrate the global convergence and effectiveness of the proposed neural network.},
  archive      = {J_TNNLS},
  author       = {Jingxin Liu and Xiaofeng Liao},
  doi          = {10.1109/TNNLS.2021.3105732},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {2001-2015},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A projection neural network to nonsmooth constrained pseudoconvex optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discontinuous event-triggered control for local
stabilization of memristive neural networks with actuator saturation:
Discrete- and continuous-time lyapunov methods. <em>TNNLS</em>,
<em>34</em>(4), 1988–2000. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the local stabilization problem is investigated for a class of memristive neural networks (MNNs) with communication bandwidth constraints and actuator saturation. To overcome these challenges, a discontinuous event-trigger (DET) scheme, consisting of the rest interval and work interval, is proposed to cut down the triggering times and save the limited communication resources. Then, a novel relaxed piecewise functional is constructed for closed-loop MNNs. The main advantage of the designed functional consists in that it is positive definite only in the work intervals and the sampling instants but not necessarily inside the rest intervals. With the aid of extended reciprocally convex combination lemma, generalized sector condition, and some inequality techniques, two local stabilization criteria are established on the basis of both the discrete- and continuous-time Lyapunov methods. The proposed analysis technique fully takes advantage of the looped-functional and the event-trigger mechanism. Moreover, two optimization schemes are, respectively, established to design the control gain and enlarge the estimates of the admissible initial conditions (AICs) and the upper bound of rest intervals. Finally, some comparison results are given to validate the superiority of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Yingjie Fan and Xia Huang and Zhen Wang and Jianwei Xia and Hao Shen},
  doi          = {10.1109/TNNLS.2021.3105731},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1988-2000},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discontinuous event-triggered control for local stabilization of memristive neural networks with actuator saturation: Discrete- and continuous-time lyapunov methods},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AttentionGAN: Unpaired image-to-image translation using
attention-guided generative adversarial networks. <em>TNNLS</em>,
<em>34</em>(4), 1972–1987. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art methods in the image-to-image translation are capable of learning a mapping from a source domain to a target domain with unpaired image data. Though the existing methods have achieved promising results, they still produce visual artifacts, being able to translate low-level information but not high-level semantics of input images. One possible reason is that generators do not have the ability to perceive the most discriminative parts between the source and target domains, thus making the generated images low quality. In this article, we propose a new Attention-Guided Generative Adversarial Networks (AttentionGAN) for the unpaired image-to-image translation task. AttentionGAN can identify the most discriminative foreground objects and minimize the change of the background. The attention-guided generators in AttentionGAN are able to produce attention masks, and then fuse the generation output with the attention masks to obtain high-quality target images. Accordingly, we also design a novel attention-guided discriminator which only considers attended regions. Extensive experiments are conducted on several generative tasks with eight public datasets, demonstrating that the proposed method is effective to generate sharper and more realistic images compared with existing competitive models. The code is available at https://github.com/Ha0Tang/AttentionGAN .},
  archive      = {J_TNNLS},
  author       = {Hao Tang and Hong Liu and Dan Xu and Philip H. S. Torr and Nicu Sebe},
  doi          = {10.1109/TNNLS.2021.3105725},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1972-1987},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AttentionGAN: Unpaired image-to-image translation using attention-guided generative adversarial networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SFANet: A spectrum-aware feature augmentation network for
visible-infrared person reidentification. <em>TNNLS</em>,
<em>34</em>(4), 1958–1971. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared person reidentification (VI-ReID) is a challenging matching problem due to large modality variations between visible and infrared images. Existing approaches usually bridge the modality gap with only feature-level constraints, ignoring pixel-level variations. Some methods employ a generative adversarial network (GAN) to generate style-consistent images, but it destroys the structure information and incurs a considerable level of noise. In this article, we explicitly consider these challenges and formulate a novel spectrum-aware feature augmentation network named SFANet for cross-modality matching problem. Specifically, we put forward to employ grayscale-spectrum images to fully replace RGB images for feature learning. Learning with the grayscale-spectrum images, our model can apparently reduce modality discrepancy and detect inner structure relations across the different modalities, making it robust to color variations. At feature level, we improve the conventional two-stream network by balancing the number of specific and sharable convolutional blocks, which preserve the spatial structure information of features. Additionally, a bidirectional tri-constrained top-push ranking loss (BTTR) is embedded in the proposed network to improve the discriminability, which efficiently further boosts the matching accuracy. Meanwhile, we further introduce an effective dual-linear with batch normalization identification (ID) embedding method to model the identity-specific information and assist BTTR loss in magnitude stabilizing. On SYSU-MM01 and RegDB datasets, we conducted extensively experiments to demonstrate that our proposed framework contributes indispensably and achieves a very competitive VI-ReID performance.},
  archive      = {J_TNNLS},
  author       = {Haojie Liu and Shun Ma and Daoxun Xia and Shaozi Li},
  doi          = {10.1109/TNNLS.2021.3105702},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1958-1971},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SFANet: A spectrum-aware feature augmentation network for visible-infrared person reidentification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IDARTS: Improving DARTS by node normalization and
decorrelation discretization. <em>TNNLS</em>, <em>34</em>(4), 1945–1957.
(<a href="https://doi.org/10.1109/TNNLS.2021.3105698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable ARchiTecture Search (DARTS) uses a continuous relaxation of network representation and dramatically accelerates Neural Architecture Search (NAS) by almost thousands of times in GPU-day. However, the searching process of DARTS is unstable, which suffers severe degradation when training epochs become large, thus limiting its application. In this article, we claim that this degradation issue is caused by the imbalanced norms between different nodes and the highly correlated outputs from various operations. We then propose an improved version of DARTS, namely iDARTS, to deal with the two problems. In the training phase, it introduces node normalization to maintain the norm balance. In the discretization phase, the continuous architecture is approximated based on the similarity between the outputs of the node and the decorrelated operations rather than the values of the architecture parameters. Extensive evaluation is conducted on CIFAR-10 and ImageNet, and the error rates of 2.25\% and 24.7\% are reported within 0.2 and 1.9 GPU-day for architecture search, respectively, which shows its effectiveness. Additional analysis also reveals that iDARTS has the advantage in robustness and generalization over other DARTS-based counterparts.},
  archive      = {J_TNNLS},
  author       = {Huiqun Wang and Ruijie Yang and Di Huang and Yunhong Wang},
  doi          = {10.1109/TNNLS.2021.3105698},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1945-1957},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IDARTS: Improving DARTS by node normalization and decorrelation discretization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered adaptive neural network control for
stochastic nonlinear systems with state constraints and time-varying
delays. <em>TNNLS</em>, <em>34</em>(4), 1932–1944. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we pay attention to develop an event-triggered adaptive neural network (ANN) control strategy for stochastic nonlinear systems with state constraints and time-varying delays. The state constraints are disposed by relying on the barrier Lyapunov function. The neural networks are exploited to identify the unknown dynamics. In addition, the Lyapunov–Krasovskii functional is employed to counteract the adverse effect originating from time-varying delays. The backstepping technique is employed to design controller by combining event-triggered mechanism (ETM), which can alleviate data transmission and save communication resource. The constructed ANN control scheme can guarantee the stability of the considered systems, and the predefined constraints are not violated. Simulation results and comparison are given to validate the feasibility of the presented scheme.},
  archive      = {J_TNNLS},
  author       = {Yongchao Liu and Qidan Zhu},
  doi          = {10.1109/TNNLS.2021.3105681},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1932-1944},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive neural network control for stochastic nonlinear systems with state constraints and time-varying delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resilient optimal defensive strategy of TSK
fuzzy-model-based microgrids’ system via a novel reinforcement learning
approach. <em>TNNLS</em>, <em>34</em>(4), 1921–1931. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With consideration of false data injection (FDI) on the demand side, it brings a great challenge for the optimal defensive strategy with the security issue, voltage stability, power flow, and economic cost indexes. This article proposes a Takagi–Sugeuo–Kang (TSK) fuzzy system-based reinforcement learning approach for the resilient optimal defensive strategy of interconnected microgrids. Due to FDI uncertainty of the system load, TSK-based deep deterministic policy gradient (DDPG) is proposed to learn the actor network and the critic network, where multiple indexes’ assessment occurs in the critic network, and the security switching control strategy is made in the actor network. Alternating direction method of multipliers (ADMM) method is improved for policy gradient with online coordination between the actor network and the critic network learning, and its convergence and optimality are proved properly. On the basis of security switching control strategy, the penalty-based boundary intersection (PBI)-based multiobjective optimization method is utilized to solve economic cost and emission issues simultaneously with considering voltage stability and rate-of-change of frequency (RoCoF) limits. According to simulation results, it reveals that the proposed resilient optimal defensive strategy can be a viable and promising alternative for tackling uncertain attack problems on interconnected microgrids.},
  archive      = {J_TNNLS},
  author       = {Huifeng Zhang and Dong Yue and Chunxia Dou and Xiangpeng Xie and Kang Li and Gerhardus P. Hancke},
  doi          = {10.1109/TNNLS.2021.3105668},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1921-1931},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Resilient optimal defensive strategy of TSK fuzzy-model-based microgrids’ system via a novel reinforcement learning approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fixed-time adaptive neural network control for nonlinear
systems with input saturation. <em>TNNLS</em>, <em>34</em>(4),
1911–1920. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study concentrates on the tracking control problem for nonlinear systems subject to actuator saturation. To improve the performance of the controller, we propose a fixed-time tracking control scheme, in which the upper bound of the convergence time is independent of the initial conditions. In the control scheme, first, a smooth nonlinear function is employed to approximate the saturation function so that the controller can be designed under the framework of backstepping. Then, the effect of input saturation is compensated by introducing an auxiliary system. Furthermore, a fixed-time adaptive neural network control method is given with the help of fixed-time control theory, in which the dynamic order of controllers is reduced to a certain extent since there is only one updating law in the entire control design. Through rigorous theoretical analysis, it is concluded that the proposed control scheme can guarantee that: 1) the output tracking error can converge to a small neighborhood near the origin in a fixed time and 2) all signals in the closed-loop system are bounded. Finally, a numerical example and a practical example based on the single-link manipulator are provided to verify the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Wei Sun and Shuzhen Diao and Shun-Feng Su and Zong-Yao Sun},
  doi          = {10.1109/TNNLS.2021.3105664},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1911-1920},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fixed-time adaptive neural network control for nonlinear systems with input saturation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Further results on optimal tracking control for nonlinear
systems with nonzero equilibrium via adaptive dynamic programming.
<em>TNNLS</em>, <em>34</em>(4), 1900–1910. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a novel cost function (performance index function) to overcome the obstacles in solving the optimal tracking control problem for a class of nonlinear systems with known system dynamics via adaptive dynamic programming (ADP) technique. For the traditional optimal control problems, the assumption that the controlled system has zero equilibrium is generally required to guarantee the finiteness of an infinite horizon cost function and a unique solution. In order to solve the optimal tracking control problem of nonlinear systems with nonzero equilibrium, a specific cost function related to tracking errors and their derivatives is designed in this article, in which the aforementioned assumption and related obstacles are removed and the controller design process is simplified. Finally, comparative simulations are conducted on an inverted pendulum system to illustrate the effectiveness and advantages of the proposed optimal tracking control strategy.},
  archive      = {J_TNNLS},
  author       = {Tong Wang and Yujia Wang and Xuebo Yang and Jiae Yang},
  doi          = {10.1109/TNNLS.2021.3105646},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1900-1910},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Further results on optimal tracking control for nonlinear systems with nonzero equilibrium via adaptive dynamic programming},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on brain effective connectivity network learning.
<em>TNNLS</em>, <em>34</em>(4), 1879–1899. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human brain effective connectivity characterizes the causal effects of neural activities among different brain regions. Studies of brain effective connectivity networks (ECNs) for different populations contribute significantly to the understanding of the pathological mechanism associated with neuropsychiatric diseases and facilitate finding new brain network imaging markers for the early diagnosis and evaluation for the treatment of cerebral diseases. A deeper understanding of brain ECNs also greatly promotes brain-inspired artificial intelligence (AI) research in the context of brain-like neural networks and machine learning. Thus, how to picture and grasp deeper features of brain ECNs from functional magnetic resonance imaging (fMRI) data is currently an important and active research area of the human brain connectome. In this survey, we first show some typical applications and analyze existing challenging problems in learning brain ECNs from fMRI data. Second, we give a taxonomy of ECN learning methods from the perspective of computational science and describe some representative methods in each category. Third, we summarize commonly used evaluation metrics and conduct a performance comparison of several typical algorithms both on simulated and real datasets. Finally, we present the prospects and references for researchers engaged in learning ECNs.},
  archive      = {J_TNNLS},
  author       = {Junzhong Ji and Aixiao Zou and Jinduo Liu and Cuicui Yang and Xiaodan Zhang and Yongduan Song},
  doi          = {10.1109/TNNLS.2021.3106299},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1879-1899},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey on brain effective connectivity network learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient method for modeling nonoccurring behaviors by
negative sequential patterns with loose constraints. <em>TNNLS</em>,
<em>34</em>(4), 1864–1878. (<a
href="https://doi.org/10.1109/TNNLS.2021.3063162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sequence analysis handles sequential discrete events and behaviors, which can be represented by temporal point processes (TPPs). However, TPP models only occurring events and behaviors. This article explores an efficient method for the negative sequential pattern (NSP) mining to leverage TPP in modeling both frequently occurring and nonoccurring events and behaviors. NSP mining is good at the challenging modeling of nonoccurrences of events and behaviors and their combinations with occurring events, with existing methods built on incorporating various constraints into NSP representations, e.g., simplifying NSP formulations and reducing computational costs. Such constraints restrict the flexibility of NSPs, and nonoccurring behaviors (NOBs) cannot be comprehensively exposed. This article addresses this issue by loosening some inflexible constraints in NSP mining and solves a series of consequent challenges. First, we provide a new definition of negative containment with the set theory according to the loose constraints. Second, an efficient method quickly calculates the supports of negative sequences. Our method only uses the information about the corresponding positive sequential patterns (PSPs) and avoids additional database scans. Finally, a novel and efficient algorithm, NegI-NSP, is proposed to efficiently identify highly valuable NSPs. Theoretical analyses, comparisons, and experiments on four synthetic and two real-life data sets clearly show that NegI-NSP can efficiently discover more useful NOBs.},
  archive      = {J_TNNLS},
  author       = {Ping Qiu and Yongshun Gong and Yuhai Zhao and Longbing Cao and Chengqi Zhang and Xiangjun Dong},
  doi          = {10.1109/TNNLS.2021.3063162},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1864-1878},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient method for modeling nonoccurring behaviors by negative sequential patterns with loose constraints},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdapNet: Adaptability decomposing encoder–decoder network
for weakly supervised action recognition and localization.
<em>TNNLS</em>, <em>34</em>(4), 1852–1863. (<a
href="https://doi.org/10.1109/TNNLS.2019.2962815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The point process is a solid framework to model sequential data, such as videos, by exploring the underlying relevance. As a challenging problem for high-level video understanding, weakly supervised action recognition and localization in untrimmed videos have attracted intensive research attention. Knowledge transfer by leveraging the publicly available trimmed videos as external guidance is a promising attempt to make up for the coarse-grained video-level annotation and improve the generalization performance. However, unconstrained knowledge transfer may bring about irrelevant noise and jeopardize the learning model. This article proposes a novel adaptability decomposing encoder–decoder network to transfer reliable knowledge between the trimmed and untrimmed videos for action recognition and localization by bidirectional point process modeling, given only video-level annotations. By decomposing the original features into the domain-adaptable and domain-specific ones based on their adaptability, trimmed–untrimmed knowledge transfer can be safely confined within a more coherent subspace. An encoder–decoder-based structure is carefully designed and jointly optimized to facilitate effective action classification and temporal localization. Extensive experiments are conducted on two benchmark data sets (i.e., THUMOS14 and ActivityNet1.3), and the experimental results clearly corroborate the efficacy of our method.},
  archive      = {J_TNNLS},
  author       = {Xiao-Yu Zhang and Changsheng Li and Haichao Shi and Xiaobin Zhu and Peng Li and Jing Dong},
  doi          = {10.1109/TNNLS.2019.2962815},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1852-1863},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AdapNet: Adaptability decomposing Encoder–Decoder network for weakly supervised action recognition and localization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep semantic multimodal hashing network for scalable
image-text and video-text retrievals. <em>TNNLS</em>, <em>34</em>(4),
1838–1851. (<a
href="https://doi.org/10.1109/TNNLS.2020.2997020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing has been widely applied to multimodal retrieval on large-scale multimedia data due to its efficiency in computation and storage. In this article, we propose a novel deep semantic multimodal hashing network (DSMHN) for scalable image-text and video-text retrieval. The proposed deep hashing framework leverages 2-D convolutional neural networks (CNN) as the backbone network to capture the spatial information for image-text retrieval, while the 3-D CNN as the backbone network to capture the spatial and temporal information for video-text retrieval. In the DSMHN, two sets of modality-specific hash functions are jointly learned by explicitly preserving both intermodality similarities and intramodality semantic labels. Specifically, with the assumption that the learned hash codes should be optimal for the classification task, two stream networks are jointly trained to learn the hash functions by embedding the semantic labels on the resultant hash codes. Moreover, a unified deep multimodal hashing framework is proposed to learn compact and high-quality hash codes by exploiting the feature representation learning, intermodality similarity-preserving learning, semantic label-preserving learning, and hash function learning with different types of loss functions simultaneously. The proposed DSMHN method is a generic and scalable deep hashing framework for both image-text and video-text retrievals, which can be flexibly integrated with different types of loss functions. We conduct extensive experiments for both single-modal- and cross-modal-retrieval tasks on four widely used multimodal-retrieval data sets. Experimental results on both image-text- and video-text-retrieval tasks demonstrate that the DSMHN significantly outperforms the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Lu Jin and Zechao Li and Jinhui Tang},
  doi          = {10.1109/TNNLS.2020.2997020},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1838-1851},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep semantic multimodal hashing network for scalable image-text and video-text retrievals},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the comparisons of decorrelation approaches for
non-gaussian neutral vector variables. <em>TNNLS</em>, <em>34</em>(4),
1823–1837. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a typical non-Gaussian vector variable, a neutral vector variable contains nonnegative elements only, and its $l_{1}$ -norm equals one. In addition, its neutral properties make it significantly different from the commonly studied vector variables (e.g., the Gaussian vector variables). Due to the aforementioned properties, the conventionally applied linear transformation approaches [e.g., principal component analysis (PCA) and independent component analysis (ICA)] are not suitable for neutral vector variables, as PCA cannot transform a neutral vector variable, which is highly negatively correlated, into a set of mutually independent scalar variables and ICA cannot preserve the bounded property after transformation. In recent work, we proposed an efficient nonlinear transformation approach, i.e., the parallel nonlinear transformation (PNT), for decorrelating neutral vector variables. In this article, we extensively compare PNT with PCA and ICA through both theoretical analysis and experimental evaluations. The results of our investigations demonstrate the superiority of PNT for decorrelating the neutral vector variables.},
  archive      = {J_TNNLS},
  author       = {Zhanyu Ma and Xiaoou Lu and Jiyang Xie and Zhen Yang and Jing-Hao Xue and Zheng-Hua Tan and Bo Xiao and Jun Guo},
  doi          = {10.1109/TNNLS.2020.2978858},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1823-1837},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the comparisons of decorrelation approaches for non-gaussian neutral vector variables},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Entropic dynamic time warping kernels for co-evolving
financial time series analysis. <em>TNNLS</em>, <em>34</em>(4),
1808–1822. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network representations are powerful tools to modeling the dynamic time-varying financial complex systems consisting of multiple co-evolving financial time series, e.g., stock prices. In this work, we develop a novel framework to compute the kernel-based similarity measure between dynamic time-varying financial networks. Specifically, we explore whether the proposed kernel can be employed to understand the structural evolution of the financial networks with time associated with standard kernel machines. For a set of time-varying financial networks with each vertex representing the individual time series of a different stock and each edge between a pair of time series representing the absolute value of their Pearson correlation, our start point is to compute the commute time (CT) matrix associated with the weighted adjacency matrix of the network structures, where each element of the matrix can be seen as the enhanced correlation value between pairwise stocks. For each network, we show how the CT matrix allows us to identify a reliable set of dominant correlated time series as well as an associated dominant probability distribution of the stock belonging to this set. Furthermore, we represent each original network as a discrete dominant Shannon entropy time series computed from the dominant probability distribution. With the dominant entropy time series for each pair of financial networks to hand, we develop an entropic dynamic time warping kernels through the classical dynamic time warping framework, for analyzing the financial time-varying networks. We show that the proposed kernel bridges the gap between graph kernels and the classical dynamic time warping framework for multiple financial time series analysis. Experiments on time-varying networks extracted through New York Stock Exchange (NYSE) database demonstrate that the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Lu Bai and Lixin Cui and Zhihong Zhang and Lixiang Xu and Yue Wang and Edwin R. Hancock},
  doi          = {10.1109/TNNLS.2020.3006738},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1808-1822},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Entropic dynamic time warping kernels for co-evolving financial time series analysis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling mood polarity and declaration occurrence by neural
temporal point processes. <em>TNNLS</em>, <em>34</em>(4), 1800–1807. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural point processes provide the flexibility needed to deal with time series of heterogeneous nature within the robust framework of point processes. This aspect is of particular relevance when dealing with real-world data, mixing generative processes characterized by radically different distributions and sampling. This brief discusses a neural point process approach for health and behavioral data, comprising both sparse events coming from user subjective declarations as well as fast-flowing time series from wearable sensors. We propose and empirically validate different neural architectures and we assess the effect of including input sources of different nature. The empirical analysis is built on the top of a challenging original dataset, never published before, and collected as part of a real-world experiment in an uncontrolled setting. Results show the potential of neural point processes both in terms of predicting the next event type as well as in predicting the time to next user interaction.},
  archive      = {J_TNNLS},
  author       = {Davide Bacciu and Davide Morelli and Vlad Pandelea},
  doi          = {10.1109/TNNLS.2022.3172871},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1800-1807},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modeling mood polarity and declaration occurrence by neural temporal point processes},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning social relations and spatiotemporal trajectories
for next check-in inference. <em>TNNLS</em>, <em>34</em>(4), 1789–1799.
(<a href="https://doi.org/10.1109/TNNLS.2020.3016737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of location-aware social networks (LSNs) has facilitated the research of user mobility modeling and check-in prediction, thereby benefiting various downstream applications such as precision marketing and urban management. Most of the existing studies only focus on predicting the spatial aspect of check-ins, whereas the joint inference of the spatial and temporal aspects more fits the real application scenarios. Moreover, although social relations have been extensively studied in a recommender system, only a few efforts have been observed in the next check-in location prediction, leaving room for further improvement. In this article, we study the next check-in inference problem, which demands the joint inference of the next check-in location (Where) and time (When) for a target user (Who). We devise a model named ARNPP-GAT, which combines an attention-based recurrent neural point process with a graph attention networks. The core technical insight of ARNPP-GAT is to integrate user long-term representation learning, short-term behavior modeling, and temporal point process into a unified architecture. Specifically, ARNPP-GAT first leverages graph attention networks to learn the long-term representation of users by encoding their social relations. More importantly, the ARNPP endows the model with the capability of characterizing the effects of past check-in events and performing multitask learning to yield the next check-in time and location prediction. Empirical results on two real-world data sets demonstrate that ARNPP-GAT is superior compared with several competitors, validating the contributions of multitask learning and social relation modeling.},
  archive      = {J_TNNLS},
  author       = {Wenwei Liang and Wei Zhang},
  doi          = {10.1109/TNNLS.2020.3016737},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1789-1799},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning social relations and spatiotemporal trajectories for next check-in inference},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatio-temporal point process for multiple object tracking.
<em>TNNLS</em>, <em>34</em>(4), 1777–1788. (<a
href="https://doi.org/10.1109/TNNLS.2020.2997006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple object tracking (MOT) focuses on modeling the relationship of detected objects among consecutive frames and merge them into different trajectories. MOT remains a challenging task as noisy and confusing detection results often hinder the final performance. Furthermore, most existing research are focusing on improving detection algorithms and association strategies. As such, we propose a novel framework that can effectively predict and mask-out the noisy and confusing detection results before associating the objects into trajectories. In particular, we formulate such “bad” detection results as a sequence of events and adopt the spatio-temporal point process to model such events. Traditionally, the occurrence rate in a point process is characterized by an explicitly defined intensity function, which depends on the prior knowledge of some specific tasks. Thus, designing a proper model is expensive and time-consuming, with also limited ability to generalize well. To tackle this problem, we adopt the convolutional recurrent neural network (conv-RNN) to instantiate the point process, where its intensity function is automatically modeled by the training data. Furthermore, we show that our method captures both temporal and spatial evolution, which is essential in modeling events for MOT. Experimental results demonstrate notable improvements in addressing noisy and confusing detection results in MOT data sets. An improved state-of-the-art performance is achieved by incorporating our baseline MOT algorithm with the spatio-temporal point process model.},
  archive      = {J_TNNLS},
  author       = {Tao Wang and Kean Chen and Weiyao Lin and John See and Zenghui Zhang and Qian Xu and Xia Jia},
  doi          = {10.1109/TNNLS.2020.2997006},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1777-1788},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatio-temporal point process for multiple object tracking},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial human trajectory learning for trip
recommendation. <em>TNNLS</em>, <em>34</em>(4), 1764–1776. (<a
href="https://doi.org/10.1109/TNNLS.2021.3058102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of trip recommendation has been extensively studied in recent years, by both researchers and practitioners. However, one of its key aspects—understanding human mobility—remains under-explored. Many of the proposed methods for trip modeling rely on empirical analysis of attributes associated with historical points-of-interest (POIs) and routes generated by tourists while attempting to also intertwine personal preferences—such as contextual topics, geospatial, and temporal aspects. However, the implicit transitional preferences and semantic sequential relationships among various POIs, along with the constraints implied by the starting point and destination of a particular trip, have not been fully exploited. Inspired by the recent advances in generative neural networks, in this work we propose DeepTrip—an end-to-end method for better understanding of the underlying human mobility and improved modeling of the POIs’ transitional distribution in human moving patterns. DeepTrip consists of: a trip encoder (TE) to embed the contextual route into a latent variable with a recurrent neural network (RNN); and a trip decoder to reconstruct this route conditioned on an optimized latent space. Simultaneously, we define an Adversarial Net composed of a generator and critic, which generates a representation for a given query and uses a critic to distinguish the trip representation generated from TE and query representation obtained from Adversarial Net. DeepTrip enables regularizing the latent space and generalizing users’ complex check-in preferences. We demonstrate, both theoretically and empirically, the effectiveness and efficiency of the proposed model, and the experimental evaluations show that DeepTrip outperforms the state-of-the-art baselines on various evaluation metrics},
  archive      = {J_TNNLS},
  author       = {Qiang Gao and Fan Zhou and Kunpeng Zhang and Fengli Zhang and Goce Trajcevski},
  doi          = {10.1109/TNNLS.2021.3058102},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1764-1776},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial human trajectory learning for trip recommendation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning adversarial transformer for symbolic music
generation. <em>TNNLS</em>, <em>34</em>(4), 1754–1763. (<a
href="https://doi.org/10.1109/TNNLS.2020.2990746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic music generation is still an unsettled problem facing several challenges. The complete music score is a quite long note sequence, which consists of multiple tracks with recurring elements and their variants at various levels. The transformer model, benefiting from its self-attention has shown advantages in modeling long sequences. There have been some attempts at applying the transformer-based model to music generation. However, previous works train the model using the same strategy as the text generation task, despite the obvious differences between the pattern of texts and musics. These models cannot consistently produce music samples of high quality. In this article, we propose a novel adversarial transformer to generate transformer to generate music pieces with high musicality. The generative adversarial learning and the self-attention networks are combined creatively. The generation of long sequence is guided by the adversarial objectives, which provides a strong regularization to enforce the transformer to focus on learning of the global and local structures. Instead of adopting the time-consuming Monte Carlo (MC) search method that is commonly used in the existing sequence generative models, we propose an effective and convenient method to compute the reward for each generated step (REGS) for the long sequence. The discriminator is trained to optimize the elaborately designed global and local loss objective functions simultaneously, which enables the discriminator to give reliable REGS for the generator. The adversarial objective combined with the teacher forcing objective is used to guide the training of the generator. The proposed model can be used to generate single-track or multitrack music pieces. Experiments show that our model can generate long music pieces with the improved quality compared with the original music transformers.},
  archive      = {J_TNNLS},
  author       = {Ning Zhang},
  doi          = {10.1109/TNNLS.2020.2990746},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1754-1763},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning adversarial transformer for symbolic music generation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asynchronous spatiotemporal spike metric for event cameras.
<em>TNNLS</em>, <em>34</em>(4), 1742–1753. (<a
href="https://doi.org/10.1109/TNNLS.2021.3061122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras as bioinspired vision sensors have shown great advantages in high dynamic range and high temporal resolution in vision tasks. Asynchronous spikes from event cameras can be depicted using the marked spatiotemporal point processes (MSTPPs). However, how to measure the distance between asynchronous spikes in the MSTPPs still remains an open issue. To address this problem, we propose a general asynchronous spatiotemporal spike metric considering both spatiotemporal structural properties and polarity attributes for event cameras. Technically, the conditional probability density function is first introduced to describe the spatiotemporal distribution and polarity prior in the MSTPPs. Besides, a spatiotemporal Gaussian kernel is defined to capture the spatiotemporal structure, which transforms discrete spikes into the continuous function in a reproducing kernel Hilbert space (RKHS). Finally, the distance between asynchronous spikes can be quantified by the inner product in the RKHS. The experimental results demonstrate that the proposed approach outperforms the state-of-the-art methods and achieves significant improvement in computational efficiency. Especially, it is able to better depict the changes involving spatiotemporal structural properties and polarity attributes.},
  archive      = {J_TNNLS},
  author       = {Jianing Li and Yihua Fu and Siwei Dong and Zhaofei Yu and Tiejun Huang and Yonghong Tian},
  doi          = {10.1109/TNNLS.2021.3061122},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1742-1753},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asynchronous spatiotemporal spike metric for event cameras},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic regularized extreme learning for robust
modeling of traffic flow forecasting. <em>TNNLS</em>, <em>34</em>(4),
1732–1741. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive neurofuzzy inference system (ANFIS) is a structured multioutput learning machine that has been successfully adopted in learning problems without noise or outliers. However, it does not work well for learning problems with noise or outliers. High-accuracy real-time forecasting of traffic flow is extremely difficult due to the effect of noise or outliers from complex traffic conditions. In this study, a novel probabilistic learning system, probabilistic regularized extreme learning machine combined with ANFIS (probabilistic R-ELANFIS), is proposed to capture the correlations among traffic flow data and, thereby, improve the accuracy of traffic flow forecasting. The new learning system adopts a fantastic objective function that minimizes both the mean and the variance of the model bias. The results from an experiment based on real-world traffic flow data showed that, compared with some kernel-based approaches, neural network approaches, and conventional ANFIS learning systems, the proposed probabilistic R-ELANFIS achieves competitive performance in terms of forecasting ability and generalizability.},
  archive      = {J_TNNLS},
  author       = {Jungang Lou and Yunliang Jiang and Qing Shen and Ruiqin Wang and Zechao Li},
  doi          = {10.1109/TNNLS.2020.3027822},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1732-1741},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic regularized extreme learning for robust modeling of traffic flow forecasting},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social link inference via multiview matching network from
spatiotemporal trajectories. <em>TNNLS</em>, <em>34</em>(4), 1720–1731.
(<a href="https://doi.org/10.1109/TNNLS.2020.2986472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the problem of social link inference in a target location-aware social network (LSN), which aims at predicting the unobserved links between users within the network. This problem is critical for downstream applications, including network completion and friend recommendation. In addition to the network structures commonly used in general link prediction, the studies tailored for social link inference in an LSN leverage user trajectories from the spatial aspect. However, the temporal factor lying in user trajectories is largely overlooked by most of the prior studies, limiting the capabilities of capturing the temporal relevance between users. Moreover, effective user matching by fusing different views, i.e., social, spatial, and temporal factors, remains unresolved, which hinders the potential improvement of link inference. To this end, this article devises a novel multiview matching network (MVMN) by regarding each of the three factors as one view of any target user pair. MVMN enjoys the flexibility and completeness of modeling each factor by developing its suitable matching module: 1) location matching module; 2) time-series matching module; and 3) relation matching module. Each module learns a view-specific representation for matching, and MVMN fuses them for final link inference. Extensive experiments on two real-world data sets demonstrate the superiority of our approach against several competitive baselines for link prediction and sequence matching, validating the contribution of its key components.},
  archive      = {J_TNNLS},
  author       = {Wei Zhang and Xin Lai and Jianyong Wang},
  doi          = {10.1109/TNNLS.2020.2986472},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1720-1731},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Social link inference via multiview matching network from spatiotemporal trajectories},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spatiotemporal deep learning approach for unsupervised
anomaly detection in cloud systems. <em>TNNLS</em>, <em>34</em>(4),
1705–1719. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a critical task for maintaining the performance of a cloud system. Using data-driven methods to address this issue is the mainstream in recent years. However, due to the lack of labeled data for training in practice, it is necessary to enable an anomaly detection model trained on contaminated data in an unsupervised way. Besides, with the increasing complexity of cloud systems, effectively organizing data collected from a wide range of components of a system and modeling spatiotemporal dependence among them become a challenge. In this article, we propose TopoMAD, a stochastic seq2seq model which can robustly model spatial and temporal dependence among contaminated data. We include system topological information to organize metrics from different components and apply sliding windows over metrics collected continuously to capture the temporal dependence. We extract spatial features with the help of graph neural networks and temporal features with long short-term memory networks. Moreover, we develop our model based on variational auto-encoder, enabling it to work well robustly even when trained on contaminated data. Our approach is validated on the run-time performance data collected from two representative cloud systems, namely, a big data batch processing system and a microservice-based transaction processing system. The experimental results show that TopoMAD outperforms some state-of-the-art methods on these two data sets.},
  archive      = {J_TNNLS},
  author       = {Zilong He and Pengfei Chen and Xiaoyun Li and Yongfeng Wang and Guangba Yu and Cailin Chen and Xinrui Li and Zibin Zheng},
  doi          = {10.1109/TNNLS.2020.3027736},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1705-1719},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A spatiotemporal deep learning approach for unsupervised anomaly detection in cloud systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Delinquent events prediction in temporal networked-guarantee
loans. <em>TNNLS</em>, <em>34</em>(4), 1692–1704. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under debt obligation promises, small- and medium-sized enterprises (SMEs) can guarantee each other to enhance their financial security to get loans from commercial banks. When the economy rises, the banks may reduce the threshold to some extent, which may introduce default risk during the economy down period, especially when many SMEs bind together and form complex networks. The risk may diffuse across the guarantee network and may result in a financial crisis. Macroprudential oversight of the guarantee network to eliminate any potential systematics financial risk is the central task of the regulatory commission and the commercial banks. Based on our observation, the delinquent probability of an SME depends not only on self-financial status but also highly related to its temporal behaviors and structural position in networks. The classic approach for loan assessment criteria face challenges in extracting temporal and structural patterns from dynamic networks. To address these issues, we propose a temporal delinquent event prediction (TDEP) framework that preserves temporal network structures and credit behavior sequences in an end-to-end model. In particular, we first employ a graph attention layer to learn the representation of nodes in temporal guarantee networks. We then design a recursive and self-attention mechanism to integrate both credit behavior and network structure information. The learned attentional weights are leveraged to uncover high-risk guarantee patterns that effectively accelerate the risk assessment process. Afterward, we conduct extensive experiments in a real-world guaranteed-loan data set to evaluate its performance. The results show the effectiveness of our proposed approach compared with the state-of-the-art baselines. Finally, we integrate the proposed model in a real-world loan risk management system. We present the implementation details of each subcomponent of the system and report out the performance after online deployment.},
  archive      = {J_TNNLS},
  author       = {Dawei Cheng and Zhibin Niu and Liqing Zhang},
  doi          = {10.1109/TNNLS.2020.3027346},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1692-1704},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Delinquent events prediction in temporal networked-guarantee loans},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling event propagation via graph biased temporal point
process. <em>TNNLS</em>, <em>34</em>(4), 1681–1691. (<a
href="https://doi.org/10.1109/TNNLS.2020.3004626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal point process is widely used for sequential data modeling. In this article, we focus on the problem of modeling sequential event propagation in graph, such as retweeting by social network users and news transmitting between websites. Given a collection of event propagation sequences, the conventional point process model considers only the event history, i.e., embed event history into a vector, not the latent graph structure. We propose a graph biased temporal point process (GBTPP) leveraging the structural information from graph representation learning, where the direct influence between nodes and indirect influence from event history is modeled. Moreover, the learned node embedding vector is also integrated into the embedded event history as side information. Experiments on a synthetic data set and two real-world data sets show the efficacy of our model compared with conventional methods and state-of-the-art ones.},
  archive      = {J_TNNLS},
  author       = {Weichang Wu and Huanxi Liu and Xiaohu Zhang and Yu Liu and Hongyuan Zha},
  doi          = {10.1109/TNNLS.2020.3004626},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1681-1691},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modeling event propagation via graph biased temporal point process},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Calibration and uncertainty in neural time-to-event
modeling. <em>TNNLS</em>, <em>34</em>(4), 1666–1680. (<a
href="https://doi.org/10.1109/TNNLS.2020.3029631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models for predicting the time of a future event are crucial for risk assessment, across a diverse range of applications. Existing time-to-event (survival) models have focused primarily on preserving pairwise ordering of estimated event times (i.e., relative risk). We propose neural time-to-event models that account for calibration and uncertainty while predicting accurate absolute event times. Specifically, an adversarial nonparametric model is introduced for estimating matched time-to-event distributions for probabilistically concentrated and accurate predictions. We also consider replacing the discriminator of the adversarial nonparametric model with a survival-function matching estimator that accounts for model calibration. The proposed estimator can be used as a means of estimating and comparing conditional survival distributions while accounting for the predictive uncertainty of probabilistic models. Extensive experiments show that the distribution matching methods outperform existing approaches in terms of both calibration and concentration of time-to-event distributions.},
  archive      = {J_TNNLS},
  author       = {Paidamoyo Chapfuwa and Chenyang Tao and Chunyuan Li and Irfan Khan and Karen J. Chandross and Michael J. Pencina and Lawrence Carin and Ricardo Henao},
  doi          = {10.1109/TNNLS.2020.3029631},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1666-1680},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Calibration and uncertainty in neural time-to-event modeling},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph motif entropy for understanding time-evolving
networks. <em>TNNLS</em>, <em>34</em>(4), 1651–1665. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structure of networks can be efficiently represented using motifs, which are those subgraphs that recur most frequently. One route to understanding the motif structure of a network is to study the distribution of subgraphs using statistical mechanics. In this article, we address the use of motifs as network primitives using the cluster expansion from statistical physics. By mapping the network motifs to clusters in the gas model, we derive the partition function for a network, and this allows us to calculate global thermodynamic quantities, such as energy and entropy. We present analytical expressions for the number of certain types of motifs, and compute their associated entropy. We conduct numerical experiments for synthetic and real-world data sets and evaluate the qualitative and quantitative characterizations of the motif entropy derived from the partition function. We find that the motif entropy for real-world networks, such as financial stock market networks, is sensitive to the variance in network structure. This is in line with recent evidence that network motifs can be regarded as basic elements with well-defined information-processing functions.},
  archive      = {J_TNNLS},
  author       = {Zhihong Zhang and Dongdong Chen and Lu Bai and Jianjia Wang and Edwin R. Hancock},
  doi          = {10.1109/TNNLS.2020.3027426},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1651-1665},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph motif entropy for understanding time-evolving networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial recurrent time series imputation.
<em>TNNLS</em>, <em>34</em>(4), 1639–1650. (<a
href="https://doi.org/10.1109/TNNLS.2020.3010524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the real-world time series analysis, data missing is a ubiquitously existing problem due to anomalies during data collecting and storage. If not treated properly, this problem will seriously hinder the classification, regression, or related tasks. Existing methods for time series imputation either impose too strong assumptions on the distribution of missing data or cannot fully exploit, even simply ignore, the informative temporal dependencies and feature correlations across different time steps. In this article, inspired by the idea of conditional generative adversarial networks, we propose a generative adversarial learning framework for time series imputation under the condition of observed data (as well as the labels, if possible). In our model, we employ a modified bidirectional RNN structure as the generator G, which is aimed at generating the missing values by taking advantage of the temporal and nontemporal information extracted from the observed time series. The discriminator D is designed to distinguish whether each value in a time series is generated or not so that it can help the generator to make an adjustment toward a more authentic imputation result. For an empirical verification of our model, we conduct imputation and classification experiments on several real-world time series data sets. The experimental results show an eminent improvement compared with state-of-the-art baseline models.},
  archive      = {J_TNNLS},
  author       = {Shuo Yang and Minjing Dong and Yunhe Wang and Chang Xu},
  doi          = {10.1109/TNNLS.2020.3010524},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1639-1650},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial recurrent time series imputation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial robust learning of spatio-temporal point
processes: Modeling, algorithm, and applications. <em>TNNLS</em>,
<em>34</em>(4), 1634–1638. (<a
href="https://doi.org/10.1109/TNNLS.2023.3258199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal data are ubiquitous in real-world applications, and they can be generally divided into two categories: 1) synchronous temporal data which are basically equivalent to time series data; and 2) the asynchronous data which are often in the form of event data with a time stamp in continuous time-space. In fact, the event data are often converted to the time series by aggregating the event count in equal time intervals in many previous approaches. While it is often of one’s greater interest to directly establish models based on the raw event data whose time stamps carry useful information, especially for those time-sensitive tasks, ranging from earthquake prediction, crime analysis, to infectious disease diffusion forecasting, etc. Developing the spatio-temporal point process and the related applications is the theme of this Special Issue, which treats an event as a point in the spatio-temporal space, with possibly extra attributes. The model captures the instantaneous happening rate of the events and their potential dependency. The derived use cases often refer to future events prediction, and causality estimation.},
  archive      = {J_TNNLS},
  author       = {Junchi Yan and Hongteng Xu and Liangda Li and Mehrdad Farajtab and Xiaokang Yang},
  doi          = {10.1109/TNNLS.2023.3258199},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1634-1638},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial robust learning of spatio-temporal point processes: Modeling, algorithm, and applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Self-weighted unsupervised LDA. <em>TNNLS</em>,
<em>34</em>(3), 1627–1632. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a hot topic in unsupervised learning, clustering methods have been greatly developed. However, the model becomes more and more complex, and the number of parameters becomes more and more with the continuous development of clustering methods. And parameter-tuning in most methods is a laborious work due to its complexity and unpredictability. How to propose a concise and beautiful model in which the parameters can be learned adaptively becomes a very meaningful problem. Aim at tackling this problem, we develop a novel self-weighted unsupervised linear discriminative analysis method, namely SWULDA. The proposed method not only avoids adjusting parameters but also explains the link between $k$ -means and linear discriminant analysis (LDA). To obtain superior structural performance, the idea of minimizing the within-class scatter matrix and maximizing the between-class scatter matrix is embedded in the unsupervised model. Moreover, equipped with the proposed quadratic weighted optimization framework, the parameter can be adaptively learned. The extensive experiments on several datasets are conducted to validate the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Xuelong Li and Yunxing Zhang and Rui Zhang},
  doi          = {10.1109/TNNLS.2021.3105196},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1627-1632},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-weighted unsupervised LDA},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intrinsic graph learning with discrete constrained
diffusion–fusion. <em>TNNLS</em>, <em>34</em>(3), 1613–1626. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are essential to improve the performance of graph-based machine learning methods, such as spectral clustering. Various well-designed methods have been proposed to learn graphs that depict specific properties of real-world data. Joint learning of knowledge in different graphs is an effective means to uncover the intrinsic structure of samples. However, the existing methods fail to simultaneously mine the global and local information related to sample structure and distribution when multiple graphs are available, and further research is needed. Hence, we propose a novel intrinsic graph learning (IGL) with discrete constrained diffusion–fusion to solve the above problem in this article. In detail, given a set of the predefined graphs, IGL first obtains the graph encoding the global high-order manifold structure via the diffusion–fusion mechanism based on the tensor product graph. Then, two discrete operators are integrated to fine-prune the obtained graph. One of them limits the maximum number of neighbors connected to each sample, thereby removing redundant and erroneous edges. The other one forces the rank of the Laplacian matrix of the obtained graph to be equal to the number of sample clusters, which guarantees that samples from the same subgraph belong to the same cluster and vice versa. Moreover, a new strategy of weight learning is designed to accurately quantify the contribution of pairwise predefined graphs in the optimization process. Extensive experiments on six single-view and two multiview datasets have demonstrated that our proposed method outperforms the previous state-of-the-art methods on the clustering task.},
  archive      = {J_TNNLS},
  author       = {Xiaohui Wei and Ting Lu and Shutao Li},
  doi          = {10.1109/TNNLS.2021.3105678},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1613-1626},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intrinsic graph learning with discrete constrained Diffusion–Fusion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Open set domain adaptation with soft unknown-class
rejection. <em>TNNLS</em>, <em>34</em>(3), 1601–1612. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of domain adaptation (DA) is to train a good model for a target domain, with a large amount of labeled data in a source domain but only limited labeled data in the target domain. Conventional closed set domain adaptation (CSDA) assumes source and target label spaces are the same. However, this is not quite practical in real-world applications. In this work, we study the problem of open set domain adaptation (OSDA), which only requires the target label space to partially overlap with the source label space. Consequently, the solution to OSDA requires unknown classes detection and separation, which is normally achieved by introducing a threshold for the prediction of target unknown classes; however, the performance can be quite sensitive to that threshold. In this article, we tackle the above issues by proposing a novel OSDA method to perform soft rejection of unknown target classes and simultaneously match the source and target domains. Extensive experiments on three standard datasets validate the effectiveness of the proposed method over the state-of-the-art competitors.},
  archive      = {J_TNNLS},
  author       = {Yiming Xu and Lin Chen and Lixin Duan and Ivor W. Tsang and Jiebo Luo},
  doi          = {10.1109/TNNLS.2021.3105614},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1601-1612},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Open set domain adaptation with soft unknown-class rejection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph fusion network-based multimodal learning for freezing
of gait detection. <em>TNNLS</em>, <em>34</em>(3), 1588–1600. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freezing of gait (FoG) is identified as a sudden and brief episode of movement cessation despite the intention to continue walking. It is one of the most disabling symptoms of Parkinson’s disease (PD) and often leads to falls and injuries. Many computer-aided FoG detection methods have been proposed to use data collected from unimodal sources, such as motion sensors, pressure sensors, and video cameras. However, there are limited efforts of multimodal-based methods to maximize the value of all the information collected from different modalities in clinical assessments and improve the FoG detection performance. Therefore, in this study, a novel end-to-end deep architecture, namely graph fusion neural network (GFN), is proposed for multimodal learning-based FoG detection by combining footstep pressure maps and video recordings. GFN constructs multimodal graphs by treating the encoded features of each modality as vertex-level inputs and measures their adjacency patterns to construct complementary FoG representations, thus reducing the representation redundancy among different modalities. In addition, since GFN is devised to process multimodal graphs of arbitrary structures, it is expected to achieve superior performance with inputs containing missing modalities, compared to the alternative unimodal methods. A multimodal FoG dataset was collected, which included clinical assessment videos and footstep pressure sequences of 340 trials from 20 PD patients. Our proposed GFN demonstrates a great promise of multimodal FoG detection with an area under the curve (AUC) of 0.882. To the best of our knowledge, this is one of the first studies to utilize multimodal learning for automated FoG detection, which offers significant opportunities for better patient assessments and clinical trials in the future.},
  archive      = {J_TNNLS},
  author       = {Kun Hu and Zhiyong Wang and Kaylena A. Ehgoetz Martens and Markus Hagenbuchner and Mohammed Bennamoun and Ah Chung Tsoi and Simon J. G. Lewis},
  doi          = {10.1109/TNNLS.2021.3105602},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1588-1600},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph fusion network-based multimodal learning for freezing of gait detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Delay-variation-dependent criteria on extended dissipativity
for discrete-time neural networks with time-varying delay.
<em>TNNLS</em>, <em>34</em>(3), 1578–1587. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the extended dissipativity of discrete-time neural networks (NNs) with time-varying delay. First, the necessary and sufficient condition on matrix-valued polynomial inequalities reported recently is extended to a general case, where the variable of the polynomial does not need to start from zero. Second, a novel Lyapunov functional with a delay-dependent Lyapunov matrix is constructed by taking into consideration more information on nonlinear activation functions. By employing the Lyapunov functional method, a novel delay and its variation-dependent criterion are obtained to investigate the effects of the time-varying delay and its variation rate on several performances, such as $H_\infty $ performance, passivity, and $l_{2}-l_\infty $ performance, of a delayed discrete-time NN in a unified framework. Finally, a numerical example is given to show that the proposed criterion outperforms some existing ones.},
  archive      = {J_TNNLS},
  author       = {Xian-Ming Zhang and Qing-Long Han and Xiaohua Ge and Bao-Lin Zhang},
  doi          = {10.1109/TNNLS.2021.3105591},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1578-1587},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Delay-variation-dependent criteria on extended dissipativity for discrete-time neural networks with time-varying delay},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online causal feature selection for streaming features.
<em>TNNLS</em>, <em>34</em>(3), 1563–1577. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, causal feature selection (CFS) has attracted considerable attention due to its outstanding interpretability and predictability performance. Such a method primarily includes the Markov blanket (MB) discovery and feature selection based on Granger causality. Representatively, the max–min MB (MMMB) can mine an optimal feature subset, i.e., MB; however, it is unsuitable for streaming features. Online streaming feature selection (OSFS) via online process streaming features can determine parents and children (PC), a subset of MB; however, it cannot mine the MB of the target attribute ( $T$ ), i.e., a given feature, thus resulting in insufficient prediction accuracy. The Granger selection method (GSM) establishes a causal matrix of all features by performing excessively time; however, it cannot achieve a high prediction accuracy and only forecasts fixed multivariate time series data. To address these issues, we proposed an online CFS for streaming features (OCFSSFs) that mine MB containing PC and spouse and adopt the interleaving PC and spouse learning method. Furthermore, it distinguishes between PC and spouse in real time and can identify children with parents online when identifying spouses. We experimentally evaluated the proposed algorithm on synthetic datasets using precision, recall, and distance. In addition, the algorithm was tested on real-world and time series datasets using classification precision, the number of selected features, and running time. The results validated the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Dianlong You and Ruiqi Li and Shunpan Liang and Miaomiao Sun and Xinju Ou and Fuyong Yuan and Limin Shen and Xindong Wu},
  doi          = {10.1109/TNNLS.2021.3105585},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1563-1577},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online causal feature selection for streaming features},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel two-stage generation framework for promoting the
persona-consistency and diversity of responses in neural dialog systems.
<em>TNNLS</em>, <em>34</em>(3), 1552–1562. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although quite natural for human beings to communicate based on their own personality in daily life, it is rather challenging for neural dialog systems to do the same. This is because the general dialog systems are difficult to generate diverse responses while at the same time maintaining consistent persona information. Existing methods basically focus on merely one of them, ignoring either of them will reduce the quality of dialog. In this work, we propose a two-stage generation framework to promote the persona-consistency and diversity of responses. In the first stage, we propose a persona-guided conditional variational autoencoder (persona-guided CVAE) to generate diverse responses, and the main difference when compared with general CVAE-based model is that we use additional dialog attribute to assist the latent variables to encode the effective information in the response and further use it as a guiding vector for response generation. In the second stage, we employ persona-consistency checking module and the response rewriting module to mask the inconsistent word in the generated response prototype and rewrite it to more consistent. Automatic evaluation results demonstrate that the proposed model is able to generate diverse and persona-consistent responses.},
  archive      = {J_TNNLS},
  author       = {Tianyuan Shi and Yongduan Song},
  doi          = {10.1109/TNNLS.2021.3105584},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1552-1562},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel two-stage generation framework for promoting the persona-consistency and diversity of responses in neural dialog systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering hidden markov models with variational bayesian
hierarchical EM. <em>TNNLS</em>, <em>34</em>(3), 1537–1551. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hidden Markov model (HMM) is a broadly applied generative model for representing time-series data, and clustering HMMs attract increased interest from machine learning researchers. However, the number of clusters ( $K$ ) and the number of hidden states ( $S$ ) for cluster centers are still difficult to determine. In this article, we propose a novel HMM-based clustering algorithm, the variational Bayesian hierarchical EM algorithm, which clusters HMMs through their densities and priors and simultaneously learns posteriors for the novel HMM cluster centers that compactly represent the structure of each cluster. The numbers $K$ and $S$ are automatically determined in two ways. First, we place a prior on the pair $(K,S)$ and approximate their posterior probabilities, from which the values with the maximum posterior are selected. Second, some clusters and states are pruned out implicitly when no data samples are assigned to them, thereby leading to automatic selection of the model complexity. Experiments on synthetic and real data demonstrate that our algorithm performs better than using model selection techniques with maximum likelihood estimation.},
  archive      = {J_TNNLS},
  author       = {Hui Lan and Ziquan Liu and Janet H. Hsiao and Dan Yu and Antoni B. Chan},
  doi          = {10.1109/TNNLS.2021.3105570},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1537-1551},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Clustering hidden markov models with variational bayesian hierarchical EM},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimized backstepping consensus control using reinforcement
learning for a class of nonlinear strict-feedback-dynamic multi-agent
systems. <em>TNNLS</em>, <em>34</em>(3), 1524–1536. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an optimized leader-following consensus control scheme is proposed for the nonlinear strict-feedback-dynamic multi-agent system by learning from the controlling idea of optimized backstepping technique, which designs the virtual and actual controls of backstepping to be the optimized solution of corresponding subsystems so that the entire backstepping control is optimized. Since this control needs to not only ensure the optimizing system performance but also synchronize the multiple system state variables, it is an interesting and challenging topic. In order to achieve this optimized control, the neural network approximation-based reinforcement learning (RL) is performed under critic-actor architecture. In most of the existing RL-based optimal controls, since both the critic and actor RL updating laws are derived from the negative gradient of square of the Hamilton–Jacobi–Bellman (HJB) equation’s approximation, which contains multiple nonlinear terms, their algorithm are inevitably intricate. However, the proposed optimized control derives the RL updating laws from the negative gradient of a simple positive function, which is correlated with the HJB equation; hence, it can be significantly simple in the algorithm. Meanwhile, it can also release two general conditions, known dynamic and persistence excitation, which are required in most of the RL-based optimal controls. Therefore, the proposed optimized scheme can be a natural selection for the high-order nonlinear multi-agent control. Finally, the effectiveness is demonstrated by both theory and simulation.},
  archive      = {J_TNNLS},
  author       = {Guoxing Wen and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3105548},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1524-1536},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimized backstepping consensus control using reinforcement learning for a class of nonlinear strict-feedback-dynamic multi-agent systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced deep blind hyperspectral image fusion.
<em>TNNLS</em>, <em>34</em>(3), 1513–1523. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of hyperspectral image fusion (HIF) is to reconstruct high spatial resolution hyperspectral images (HR-HSI) via fusing low spatial resolution hyperspectral images (LR-HSI) and high spatial resolution multispectral images (HR-MSI) without loss of spatial and spectral information. Most existing HIF methods are designed based on the assumption that the observation models are known, which is unrealistic in many scenarios. To address this blind HIF problem, we propose a deep learning-based method that optimizes the observation model and fusion processes iteratively and alternatively during the reconstruction to enforce bidirectional data consistency, which leads to better spatial and spectral accuracy. However, general deep neural network inherently suffers from information loss, preventing us to achieve this bidirectional data consistency. To settle this problem, we enhance the blind HIF algorithm by making part of the deep neural network invertible via applying a slightly modified spectral normalization to the weights of the network. Furthermore, in order to reduce spatial distortion and feature redundancy, we introduce a Content-Aware ReAssembly of FEatures module and an SE-ResBlock model to our network. The former module helps to boost the fusion performance, while the latter make our model more compact. Experiments demonstrate that our model performs favorably against compared methods in terms of both nonblind HIF fusion and semiblind HIF fusion.},
  archive      = {J_TNNLS},
  author       = {Wu Wang and Xueyang Fu and Weihong Zeng and Liyan Sun and Ronghui Zhan and Yue Huang and Xinghao Ding},
  doi          = {10.1109/TNNLS.2021.3105543},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1513-1523},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhanced deep blind hyperspectral image fusion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive neural event-triggered control of networked markov
jump systems under hybrid cyberattacks. <em>TNNLS</em>, <em>34</em>(3),
1502–1512. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the neural network (NN)-based event-triggered control problem for discrete-time networked Markov jump systems with hybrid cyberattacks and unmeasured states. The event-triggered mechanism (ETM) is used to reduce the communication load, and a Luenberger observer is introduced to estimate the unmeasured states. Two kinds of cyberattacks, denial-of-service (DoS) attacks and deception attacks, are investigated due to the vulnerability of cyberlayer. For the sake of mitigating the impact of these two types of cyberattacks on system performance, the ETM under DoS jamming attacks is discussed first, and a new estimation of such mechanism is given. Then, the NN technique is applied to approximate the injected false information. Some sufficient conditions are derived to guarantee the boundedness of the closed-loop system, and the observer and controller gains are presented by solving a set of matrix inequalities. The effectiveness of the presented control method is demonstrated by a numerical example.},
  archive      = {J_TNNLS},
  author       = {Xiaobin Gao and Feiqi Deng and Pengyu Zeng and Hongyang Zhang},
  doi          = {10.1109/TNNLS.2021.3105532},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1502-1512},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural event-triggered control of networked markov jump systems under hybrid cyberattacks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). State estimation for discrete time-delayed impulsive neural
networks under communication constraints: A delay-range-dependent
approach. <em>TNNLS</em>, <em>34</em>(3), 1489–1501. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a delay-range-dependent approach is put forward to tackle the state estimation problem for delayed impulsive neural networks. A new type of nonlinear function, which is more general than the normal sigmoid function and functions constrained by the Lipschitz condition, is adopted as the neuron activation function. To effectively alleviate data collisions and save energy, the round-robin protocol is utilized to mitigate the occurrence of unnecessary network congestion in communication channels from sensors to the estimator. With the aid of the Lyapunov stability theory, a state observer is constructed such that the estimation error dynamics are asymptotically stable. The observer existence is ensured by resorting to a set of delay-range-dependent criteria which is dependent on both the impulsive time instant and a coefficient matrix. In addition, the synthesis of the observer is discussed by using linear matrix inequalities. Simulations are provided to illustrate the reasonability of our delay-range-dependent estimation approach.},
  archive      = {J_TNNLS},
  author       = {Yuqiang Luo and Zidong Wang and Weiguo Sheng and Dong Yue},
  doi          = {10.1109/TNNLS.2021.3105449},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1489-1501},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {State estimation for discrete time-delayed impulsive neural networks under communication constraints: A delay-range-dependent approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Activation functions for convolutional neural networks:
Proposals and experimental study. <em>TNNLS</em>, <em>34</em>(3),
1478–1488. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Activation functions lie at the core of every neural network model from shallow to deep convolutional neural networks. Their properties and characteristics shape the output range of each layer and, thus, their capabilities. Modern approaches rely mostly on a single function choice for the whole network, usually ReLU or other similar alternatives. In this work, we propose two new activation functions and analyze their properties and compare them with 17 different function proposals from recent literature on six distinct problems with different characteristics. The objective is to shed some light on their comparative performance. The results show that the proposed functions achieved better performance than the most commonly used ones.},
  archive      = {J_TNNLS},
  author       = {Víctor Manuel Vargas and Pedro Antonio Gutiérrez and Javier Barbero-Gómez and César Hervás-Martínez},
  doi          = {10.1109/TNNLS.2021.3105444},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1478-1488},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Activation functions for convolutional neural networks: Proposals and experimental study},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered recursive state estimation for stochastic
complex dynamical networks under hybrid attacks. <em>TNNLS</em>,
<em>34</em>(3), 1465–1477. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the event-based recursive state estimation problem is investigated for a class of stochastic complex dynamical networks under cyberattacks. A hybrid cyberattack model is introduced to take into account both the randomly occurring deception attack and the randomly occurring denial-of-service attack. For the sake of reducing the transmission rate and mitigating the network burden, the event-triggered mechanism is employed under which the measurement output is transmitted to the estimator only when a preset condition is satisfied. An upper bound on the estimation error covariance on each node is first derived through solving two coupled Riccati-like difference equations. Then, the desired estimator gain matrix is recursively acquired that minimizes such an upper bound. Using the stochastic analysis theory, the estimation error is proven to be stochastically bounded with probability 1. Finally, an illustrative example is provided to verify the effectiveness of the developed estimator design method.},
  archive      = {J_TNNLS},
  author       = {Yun Chen and Xueyang Meng and Zidong Wang and Hongli Dong},
  doi          = {10.1109/TNNLS.2021.3105409},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1465-1477},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered recursive state estimation for stochastic complex dynamical networks under hybrid attacks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-reinforcement learning with dynamic adaptiveness
distillation. <em>TNNLS</em>, <em>34</em>(3), 1454–1464. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning is confronted with problems of sampling inefficiency and poor task migration capability. Meta-reinforcement learning (meta-RL) enables meta-learners to utilize the task-solving skills trained on similar tasks and quickly adapt to new tasks. However, meta-RL methods lack enough queries toward the relationship between task-agnostic exploitation of data and task-related knowledge introduced by latent context, limiting their effectiveness and generalization ability. In this article, we develop an algorithm for off-policy meta-RL that can provide the meta-learners with self-oriented cognition toward how they adapt to the family of tasks. In our approach, we perform dynamic task-adaptiveness distillation to describe how the meta-learners adjust the exploration strategy in the meta-training process. Our approach also enables the meta-learners to balance the influence of task-agnostic self-oriented adaption and task-related information through latent context reorganization. In our experiments, our method achieves 10\%–20\% higher asymptotic reward than probabilistic embeddings for actor–critic RL (PEARL).},
  archive      = {J_TNNLS},
  author       = {Hangkai Hu and Gao Huang and Xiang Li and Shiji Song},
  doi          = {10.1109/TNNLS.2021.3105407},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1454-1464},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Meta-reinforcement learning with dynamic adaptiveness distillation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CLRNet: Component-level refinement network for deep face
parsing. <em>TNNLS</em>, <em>34</em>(3), 1439–1453. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face parsing aims to assign pixel-wise semantic labels to different facial components (e.g., hair, brows, and lips) in given face images. However, directly predicting pixel-level labels for each facial component over the whole face image would obtain limited accuracy, especially for tiny facial components. To address this problem, some recent works propose to first crop tiny patches from the whole face image and then predict masks for each facial component. However, such cropping-and-segmenting strategy consists of two independent stages, which cannot be jointly optimized. Besides, as one valuable piece of information for parsing the highly structured facial components, context cues are not elaborately explored by the existing works. To address these issues, we propose a component-level refinement network (CLRNet) for precisely segmenting out each facial component. Specifically, we introduce an attention mechanism to bridge the two independent stages together and form an end-to-end trainable pipeline for face parsing. Furthermore, we incorporate the global context information into the refining process for each cropped facial component patch, providing informative cues for accurate parsing. Extensive experiments are carried out on two benchmark datasets, LFW-PL and HELEN. The results demonstrate the superiority of the proposed CLRNet over other state-of-the-art methods, especially for tiny facial components.},
  archive      = {J_TNNLS},
  author       = {Peiliang Huang and Junwei Han and Dingwen Zhang and Mingliang Xu},
  doi          = {10.1109/TNNLS.2021.3105386},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1439-1453},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CLRNet: Component-level refinement network for deep face parsing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive exact penalty design for optimal resource
allocation. <em>TNNLS</em>, <em>34</em>(3), 1430–1438. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a distributed adaptive continuous-time optimization algorithm based on the Laplacian-gradient method and adaptive control is designed for resource allocation problem with the resource constraint and the local convex set constraints. In order to deal with local convex sets, a distance-based exact penalty function method is adopted to reformulate the resource allocation problem instead of the widely used projection operator method. By using the nonsmooth analysis and set-valued LaSalle invariance principle, it is proven that the proposed algorithm is capable of solving the nonsmooth resource allocation problem. Finally, two simulation examples are presented to substantiate the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Mengke Lian and Zhenyuan Guo and Xiaoxuan Wang and Shiping Wen and Tingwen Huang},
  doi          = {10.1109/TNNLS.2021.3105385},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1430-1438},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive exact penalty design for optimal resource allocation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A novel convolutional neural network model based on beetle
antennae search optimization algorithm for computerized tomography
diagnosis. <em>TNNLS</em>, <em>34</em>(3), 1418–1429. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are widely used in the field of medical imaging diagnosis but have the disadvantages of slow training speed and low diagnostic accuracy due to the initialization of parameters before training. In this article, a CNN optimization method based on the beetle antennae search (BAS) optimization algorithm is proposed. The method optimizes the initial parameters of the CNN through the BAS optimization algorithm. Based on this optimization approach, a novel CNN model with a pretrained BAS optimization algorithm was developed and applied to the analysis and diagnosis of medical imaging data for intracranial hemorrhage. Experimental results on 330 test images show that the proposed method has a better diagnostic performance than the traditional CNN. The proposed method achieves a diagnostic accuracy of 93.9394\% and 100\% recall, and the diagnosis of 66 human head computerized tomography image data only takes 0.1596 s. Moreover, the proposed method has more advantages than the three other optimization algorithms.},
  archive      = {J_TNNLS},
  author       = {Dechao Chen and Xiang Li and Shuai Li},
  doi          = {10.1109/TNNLS.2021.3105384},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1418-1429},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel convolutional neural network model based on beetle antennae search optimization algorithm for computerized tomography diagnosis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive prototypical networks with label words and joint
representation learning for few-shot relation classification.
<em>TNNLS</em>, <em>34</em>(3), 1406–1417. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation classification (RC) task is one of fundamental tasks of information extraction, aiming to detect the relation information between entity pairs in unstructured natural language text and generate structured data in the form of entity–relation triple. Although distant supervision methods can effectively alleviate the problem of lack of training data in supervised learning, they also introduce noise into the data and still cannot fundamentally solve the long-tail distribution problem of the training instances. In order to enable the neural network to learn new knowledge through few instances such as humans, this work focuses on few-shot relation classification (FSRC), where a classifier should generalize to new classes that have not been seen in the training set, given only a number of samples for each class. To make full use of the existing information and get a better feature representation for each instance, we propose to encode each class prototype in an adaptive way from two aspects. First, based on the prototypical networks, we propose an adaptive mixture mechanism to add label words to the representation of the class prototype, which, to the best of our knowledge, is the first attempt to integrate the label information into features of the support samples of each class so as to get more interactive class prototypes. Second, to more reasonably measure the distances between samples of each category, we introduce a loss function for joint representation learning (JRL) to encode each support instance in an adaptive manner. Extensive experiments have been conducted on FewRel under different few-shot (FS) settings, and the results show that the proposed adaptive prototypical networks with label words and JRL has not only achieved significant improvements in accuracy but also increased the generalization ability of FSRC.},
  archive      = {J_TNNLS},
  author       = {Yan Xiao and Yaochu Jin and Kuangrong Hao},
  doi          = {10.1109/TNNLS.2021.3105377},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1406-1417},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive prototypical networks with label words and joint representation learning for few-shot relation classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Massive ultrasonic data compression using wavelet packet
transformation optimized by convolutional autoencoders. <em>TNNLS</em>,
<em>34</em>(3), 1395–1405. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasonic signal acquisition platforms generate considerable amounts of data to be stored and processed, especially when multichannel scanning or beamforming is employed. Reducing the mass storage and allowing high-speed data transmissions necessitate the compression of ultrasonic data into a representation with fewer bits. High compression accuracy is crucial in many applications, such as ultrasonic medical imaging and nondestructive testing (NDT). In this study, we present learning models for massive ultrasonic data compression on the order of megabytes. A common and highly efficient compression method for ultrasonic data is signal decomposition and subband elimination using wavelet packet transformation (WPT). We designed an algorithm for finding the wavelet kernel that provides maximum energy compaction and the optimal subband decomposition tree structure for a given ultrasonic signal. Furthermore, the WPT convolutional autoencoder (WPTCAE) compression algorithm is proposed based on the WPT compression tree structure and the use of machine learning for estimating the optimal kernel. To further improve the compression accuracy, an autoencoder (AE) is incorporated into the WPTCAE model to build a hybrid model. The performance of the WPTCAE compression model is examined and benchmarked against other compression algorithms using ultrasonic radio frequency (RF) datasets acquired in NDT and medical imaging applications. The experimental results clearly show that the WPTCAE compression model provides improved compression ratios while maintaining high signal fidelity. The proposed learning models can achieve a compression accuracy of 98\% by using only 6\% of the original data.},
  archive      = {J_TNNLS},
  author       = {Boyang Wang and Jafar Saniie},
  doi          = {10.1109/TNNLS.2021.3105367},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1395-1405},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Massive ultrasonic data compression using wavelet packet transformation optimized by convolutional autoencoders},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multitask learning for visual question answering.
<em>TNNLS</em>, <em>34</em>(3), 1380–1394. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering (VQA) is a task that machines should provide an accurate natural language answer given an image and a question about the image. Many studies have found that the current VQA methods are heavily driven by the surface correlation or statistical bias in the training data, and lack sufficient image grounding. To address this issue, we devise a novel end-to-end architecture that uses multitask learning to promote more sufficient image grounding and learn effective multimodality representations. The tasks consist of VQA and our proposed image cloze (IC) task requires machines to fill in the blanks accurately given an image and a textual description of the image. To ensure our model performs sufficient image grounding as much as possible, we propose a novel word-masking algorithm to develop the multimodal IC task based on the part-of-speech of words. Our model predicts the VQA answer and fills in the blanks after the multimodality representation learning that is shared by the two tasks. Experimental results show that our model achieves almost the equivalent, state-of-the-art, second-best performance on the VQA v2.0, VQA-changing priors (CP) v2, and grounded question answering (GQA) datasets, respectively, with fewer parameters and without additional data compared with baselines.},
  archive      = {J_TNNLS},
  author       = {Jie Ma and Jun Liu and Qika Lin and Bei Wu and Yaxian Wang and Yang You},
  doi          = {10.1109/TNNLS.2021.3105284},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1380-1394},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitask learning for visual question answering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Question-guided erasing-based spatiotemporal attention
learning for video question answering. <em>TNNLS</em>, <em>34</em>(3),
1367–1379. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal attention learning for video question answering (VideoQA) has always been a challenging task, where existing approaches treat the attention parts and the nonattention parts in isolation. In this work, we propose to enforce the correlation between the attention parts and the nonattention parts as a distance constraint for discriminative spatiotemporal attention learning. Specifically, we first introduce a novel attention-guided erasing mechanism in the traditional spatiotemporal attention to obtain multiple aggregated attention features and nonattention features and then learn to separate the attention and the nonattention features with an appropriate distance. The distance constraint is enforced by a metric learning loss, without increasing the inference complexity. In this way, the model can learn to produce more discriminative spatiotemporal attention distribution on videos, thus enabling more accurate question answering. In order to incorporate the multiscale spatiotemporal information that is beneficial for video understanding, we additionally develop a pyramid variant on basis of the proposed approach. Comprehensive ablation experiments are conducted to validate the effectiveness of our approach, and state-of-the-art performance is achieved on several widely used datasets for VideoQA.},
  archive      = {J_TNNLS},
  author       = {Fei Liu and Jing Liu and Richang Hong and Hanqing Lu},
  doi          = {10.1109/TNNLS.2021.3105280},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1367-1379},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Question-guided erasing-based spatiotemporal attention learning for video question answering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised learning for salient object detection via
minimization of bilinear factor matrix norm. <em>TNNLS</em>,
<em>34</em>(3), 1354–1366. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency detection is an important but challenging task in the study of computer vision. In this article, we develop a new unsupervised learning approach for the saliency detection by an intrinsic regularization model, in which the Schatten-2/3 norm is integrated with the nonconvex sparse ${l_{2/3}}$ norm. The ${l_{2/3}}$ -norm is shown to be capable of detecting consistent values among sparse foreground by using image geometrical structure and feature similarity, while the Schatten-2/3 norm can capture the lower rank of background by matrix factorization. To improve effective performance of separation for Schatten-2/3-norm and ${l_{2/3}}$ -norm, a Laplacian regularization is adopted to the foreground for the smoothness. The proposed model essentially converts the required nonconvex optimization problem into the convex one, conducted by splitting the objective function based on singular value decomposition on one much smaller factor matrix and then optimized by using the alternating direction method of the multiplier. The convergence of the proposed algorithm is discussed in detail. Extensive experiments on three benchmark datasets demonstrate that our unsupervised learning approach is very competitive and appears to be more consistent across various salient objects than the current existing approaches.},
  archive      = {J_TNNLS},
  author       = {Min Li and Yao Zhang and Mingqing Xiao and Weiqiang Zhang and Xiaoli Sun},
  doi          = {10.1109/TNNLS.2021.3105276},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1354-1366},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised learning for salient object detection via minimization of bilinear factor matrix norm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A facial landmark detection method based on deep knowledge
transfer. <em>TNNLS</em>, <em>34</em>(3), 1342–1353. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial landmark detection is a crucial preprocessing step in many applications that process facial images. Deep-learning-based methods have become mainstream and achieved outstanding performance in facial landmark detection. However, accurate models typically have a large number of parameters, which results in high computational complexity and execution time. A simple but effective facial landmark detection model that achieves a balance between accuracy and speed is crucial. To achieve this, a lightweight, efficient, and effective model is proposed called the efficient face alignment network (EfficientFAN) in this article. EfficientFAN adopts the encoder-decoder structure, with a simple backbone EfficientNet-B0 as the encoder and three upsampling layers and convolutional layers as the decoder. Moreover, deep dark knowledge is extracted through feature-aligned distillation and patch similarity distillation on the teacher network, which contains pixel distribution information in the feature space and multiscale structural information in the affinity space of feature maps. The accuracy of EfficientFAN is further improved after it absorbs dark knowledge. Extensive experimental results on public datasets, including 300 Faces in the Wild (300W), Wider Facial Landmarks in the Wild (WFLW), and Caltech Occluded Faces in the Wild (COFW), demonstrate the superiority of EfficientFAN over state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Pengcheng Gao and Ke Lu and Jian Xue and Jiayi Lyu and Ling Shao},
  doi          = {10.1109/TNNLS.2021.3105247},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1342-1353},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A facial landmark detection method based on deep knowledge transfer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting adversarial examples by input transformations,
defense perturbations, and voting. <em>TNNLS</em>, <em>34</em>(3),
1329–1341. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, convolutional neural networks (CNNs) have proved to reach superhuman performance in visual recognition tasks. However, CNNs can easily be fooled by adversarial examples (AEs), i.e., maliciously crafted images that force the networks to predict an incorrect output while being extremely similar to those for which a correct output is predicted. Regular AEs are not robust to input image transformations, which can then be used to detect whether an AE is presented to the network. Nevertheless, it is still possible to generate AEs that are robust to such transformations. This article extensively explores the detection of AEs via image transformations and proposes a novel methodology, called defense perturbation, to detect robust AEs with the same input transformations the AEs are robust to. Such a defense perturbation is shown to be an effective counter-measure to robust AEs. Furthermore, multinetwork AEs are introduced. This kind of AEs can be used to simultaneously fool multiple networks, which is critical in systems that use network redundancy, such as those based on architectures with majority voting over multiple CNNs. An extensive set of experiments based on state-of-the-art CNNs trained on the Imagenet dataset is finally reported.},
  archive      = {J_TNNLS},
  author       = {Federico Nesti and Alessandro Biondi and Giorgio Buttazzo},
  doi          = {10.1109/TNNLS.2021.3105238},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1329-1341},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Detecting adversarial examples by input transformations, defense perturbations, and voting},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Training generative adversarial networks via stochastic nash
games. <em>TNNLS</em>, <em>34</em>(3), 1319–1328. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) are a class of generative models with two antagonistic neural networks: a generator and a discriminator. These two neural networks compete against each other through an adversarial process that can be modeled as a stochastic Nash equilibrium problem. Since the associated training process is challenging, it is fundamental to design reliable algorithms to compute an equilibrium. In this article, we propose a stochastic relaxed forward-backward (SRFB) algorithm for GANs, and we show convergence to an exact solution when an increasing number of data is available. We also show convergence of an averaged variant of the SRFB algorithm to a neighborhood of the solution when only a few samples are available. In both cases, convergence is guaranteed when the pseudogradient mapping of the game is monotone. This assumption is among the weakest known in the literature. Moreover, we apply our algorithm to the image generation problem.},
  archive      = {J_TNNLS},
  author       = {Barbara Franci and Sergio Grammatico},
  doi          = {10.1109/TNNLS.2021.3105227},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1319-1328},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Training generative adversarial networks via stochastic nash games},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative and multilevel feature selection network for
action recognition. <em>TNNLS</em>, <em>34</em>(3), 1304–1318. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The feature pyramid has been widely used in many visual tasks, such as fine-grained image classification, instance segmentation, and object detection, and had been achieving promising performance. Although many algorithms exploit different-level features to construct the feature pyramid, they usually treat them equally and do not make an in-depth investigation on the inherent complementary advantages of different-level features. In this article, to learn a pyramid feature with the robust representational ability for action recognition, we propose a novel collaborative and multilevel feature selection network (FSNet) that applies feature selection and aggregation on multilevel features according to action context. Unlike previous works that learn the pattern of frame appearance by enhancing spatial encoding, the proposed network consists of the position selection module and channel selection module that can adaptively aggregate multilevel features into a new informative feature from both position and channel dimensions. The position selection module integrates the vectors at the same spatial location across multilevel features with positionwise attention. Similarly, the channel selection module selectively aggregates the channel maps at the same channel location across multilevel features with channelwise attention. Positionwise features with different receptive fields and channelwise features with different pattern-specific responses are emphasized respectively depending on their correlations to actions, which are fused as a new informative feature for action recognition. The proposed FSNet can be inserted into different backbone networks flexibly, and extensive experiments are conducted on three benchmark action datasets, Kinetics, UCF101, and HMDB51. Experimental results show that FSNet is practical and can be collaboratively trained to boost the representational ability of existing networks. FSNet achieves superior performance against most top-tier models on Kinetics and all models on UCF101 and HMDB51.},
  archive      = {J_TNNLS},
  author       = {Zhenxing Zheng and Gaoyun An and Shan Cao and Dapeng Wu and Qiuqi Ruan},
  doi          = {10.1109/TNNLS.2021.3105184},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1304-1318},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Collaborative and multilevel feature selection network for action recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimized backstepping tracking control using reinforcement
learning for a class of stochastic nonlinear strict-feedback systems.
<em>TNNLS</em>, <em>34</em>(3), 1291–1303. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an optimized backstepping (OB) control scheme is proposed for a class of stochastic nonlinear strict-feedback systems with unknown dynamics by using reinforcement learning (RL) strategy of identifier-critic-actor architecture, where the identifier aims to compensate the unknown dynamic, the critic aims to evaluate the control performance and to give the feedback to the actor, and the actor aims to perform the control action. The basic control idea is that all virtual controls and the actual control of backstepping are designed as the optimized solution of corresponding subsystems so that the entire backstepping control is optimized. Different from the deterministic system, stochastic system control needs to consider not only the stochastic disturbance depicted by the Wiener process but also the Hessian term in stability analysis. If the backstepping control is developed on the basis of the published RL optimization methods, it will be difficult to be achieved because, on the one hand, RL of these methods are very complex in the algorithm thanks to their critic and actor updating laws deriving from the negative gradient of the square of approximation of Hamilton–Jacobi–Bellman (HJB) equation; on the other hand, these methods require persistence excitation and known dynamic, where persistence excitation is for training adaptive parameters sufficiently. In this research, both critic and actor updating laws are derived from the negative gradient of a simple positive function, which is yielded on the basis of a partial derivative of the HJB equation. As a result, the RL algorithm can be significantly simplified, meanwhile, two requirements of persistence excitation and known dynamic can be released. Therefore, it can be a natural selection for stochastic optimization control. Finally, from two aspects of theory and simulation, it is demonstrated that the proposed control can arrive at the desired system performance.},
  archive      = {J_TNNLS},
  author       = {Guoxing Wen and Liguang Xu and Bin Li},
  doi          = {10.1109/TNNLS.2021.3105176},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1291-1303},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimized backstepping tracking control using reinforcement learning for a class of stochastic nonlinear strict-feedback systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure-aware feature disentanglement with knowledge
transfer for appearance-changing place recognition. <em>TNNLS</em>,
<em>34</em>(3), 1278–1290. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term visual place recognition (VPR) is challenging as the environment is subject to drastic appearance changes across different temporal resolutions, such as time of the day, month, and season. A wide variety of existing methods address the problem by means of feature disentangling or image style transfer but ignore the structural information that often remains stable even under environmental condition changes. To overcome this limitation, this article presents a novel structure-aware feature disentanglement network (SFDNet) based on knowledge transfer and adversarial learning. Explicitly, probabilistic knowledge transfer (PKT) is employed to transfer knowledge obtained from the Canny edge detector to the structure encoder. An appearance teacher module is then designed to ensure that the learning of appearance encoder does not only rely on metric learning. The generated content features with structural information are used to measure the similarity of images. We finally evaluate the proposed approach and compare it to state-of-the-art place recognition methods using six datasets with extreme environmental changes. Experimental results demonstrate the effectiveness and improvements achieved using the proposed framework. Source code and some trained models will be available at http://www.tianshu.org.cn .},
  archive      = {J_TNNLS},
  author       = {Cao Qin and Yunzhou Zhang and Yingda Liu and Delong Zhu and Sonya A. Coleman and Dermot Kerr},
  doi          = {10.1109/TNNLS.2021.3105175},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1278-1290},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Structure-aware feature disentanglement with knowledge transfer for appearance-changing place recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed online learning with multiple kernels.
<em>TNNLS</em>, <em>34</em>(3), 1263–1277. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of learning a nonlinear function over a network of learners in a fully decentralized fashion. Online learning is additionally assumed, where every learner receives continuous streaming data locally. This learning model is called a fully distributed online learning (or a fully decentralized online federated learning). For this model, we propose a novel learning framework with multiple kernels, which is named DOMKL. The proposed DOMKL is devised by harnessing the principles of an online alternating direction method of multipliers and a distributed Hedge algorithm. We theoretically prove that DOMKL over $T$ time slots can achieve an optimal sublinear regret ${\mathcal{ O}}(\sqrt {T})$ , implying that every learner in the network can learn a common function having a diminishing gap from the best function in hindsight. Our analysis also reveals that DOMKL yields the same asymptotic performance as the state-of-the-art centralized approach while keeping local data at edge learners. Via numerical tests with real datasets, we demonstrate the effectiveness of the proposed DOMKL on various online regression and time-series prediction tasks.},
  archive      = {J_TNNLS},
  author       = {Songnam Hong and Jeongmin Chae},
  doi          = {10.1109/TNNLS.2021.3105146},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1263-1277},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed online learning with multiple kernels},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilabel feature selection with constrained latent
structure shared term. <em>TNNLS</em>, <em>34</em>(3), 1253–1262. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional multilabel data have increasingly emerged in many application areas, suffering from two noteworthy issues: instances with high-dimensional features and large-scale labels. Multilabel feature selection methods are widely studied to address the issues. Previous multilabel feature selection methods focus on exploring label correlations to guide the feature selection process, ignoring the impact of latent feature structure on label correlations. In addition, one encouraging property regarding correlations between features and labels is that similar features intend to share similar labels. To this end, a latent structure shared (LSS) term is designed, which shares and preserves both latent feature structure and latent label structure. Furthermore, we employ the graph regularization technique to guarantee the consistency between original feature space and latent feature structure space. Finally, we derive the shared latent feature and label structure feature selection (SSFS) method based on the constrained LSS term, and then, an effective optimization scheme with provable convergence is proposed to solve the SSFS method. Better experimental results on benchmark datasets are achieved in terms of multiple evaluation criteria.},
  archive      = {J_TNNLS},
  author       = {Wanfu Gao and Yonghao Li and Liang Hu},
  doi          = {10.1109/TNNLS.2021.3105142},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1253-1262},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multilabel feature selection with constrained latent structure shared term},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VASE: Variational assorted surprise exploration for
reinforcement learning. <em>TNNLS</em>, <em>34</em>(3), 1243–1252. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploration in environments with continuous control and sparse rewards remains a key challenge in reinforcement learning (RL). One of the approaches to encourage more systematic and efficient exploration relies on surprise as an intrinsic reward for the agent. We introduce a new definition of surprise and its RL implementation named variational assorted surprise exploration (VASE). VASE uses a Bayesian neural network as a model of the environment dynamics and is trained using variational inference, alternately updating the accuracy of the agent’s model and policy. Our experiments show that in continuous control sparse reward environments, VASE outperforms other surprise-based exploration techniques.},
  archive      = {J_TNNLS},
  author       = {Haitao Xu and Lech Szymanski and Brendan McCane},
  doi          = {10.1109/TNNLS.2021.3105140},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1243-1252},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {VASE: Variational assorted surprise exploration for reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Refinements of approximation results of conditional
restricted boltzmann machines. <em>TNNLS</em>, <em>34</em>(3),
1228–1242. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional restricted Boltzmann machines (CRBMs) are the conditional variant of restricted Boltzmann machines (RBMs), which are used to simulate conditional probability distributions. While promising for practical applications, there is a lack of theoretical studies on the approximation ability of CRBMs. In this article, by contributing analysis tools, especially designed for the conditional models, we improve the results of the representational power of CRBMs based on existing work on RBMs. We first study the properties of CRBMs to approximate conditional Markov random fields. On this basis, a universal approximation result is obtained by deriving upper bounds on the minimal number of hidden units, which improves the previous result. Furthermore, the result about maximal approximation errors of CRBMs is also improved by reducing the number of hidden units that can guarantee approximations within a given error tolerance. Furthermore, the representational efficiency of CRBMs in computing deterministic conditional distributions is investigated. Specifically, we show that there are exponentially many deterministic conditional distributions that cannot be computed by CRBMs whose size does not exponentially grow with the number of input units. Some specific examples of these hard-to-present deterministic conditional distributions are provided. Finally, some numerical experiments are carried out to verify the theoretical results of the properties of CRBMs to approximate conditional Markov random fields and the universal approximation of CRBMs.},
  archive      = {J_TNNLS},
  author       = {Linyan Gu and Lihua Yang and Feng Zhou},
  doi          = {10.1109/TNNLS.2021.3105129},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1228-1242},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Refinements of approximation results of conditional restricted boltzmann machines},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A weighting method for feature dimension by semisupervised
learning with entropy. <em>TNNLS</em>, <em>34</em>(3), 1218–1227. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a semisupervised weighting method for feature dimension based on entropy is proposed for classification, dimension reduction, and correlation analysis. For real-world data, different feature dimensions usually show different importance. Generally, data in the same class are supposed to be similar, so their entropy should be small; and those in different classes are supposed to be dissimilar, so their entropy should be large. According to this, we propose a way to construct the weights of feature dimensions with the whole entropy and the innerclass entropies. The weights indicate the contribution of their corresponding feature dimensions in classification. They can be used to improve the performance of classification by giving a weighted distance metric and can be applied to dimension reduction and correlation analysis as well. Some numerical experiments are given to test the proposed method by comparing it with some other representative methods. They demonstrate that the proposed method is feasible and efficient in classification, dimension reduction, and correlation analysis.},
  archive      = {J_TNNLS},
  author       = {Dequan Jin and Murong Yang and Ziyan Qin and Jigen Peng and Shihui Ying},
  doi          = {10.1109/TNNLS.2021.3105127},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1218-1227},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A weighting method for feature dimension by semisupervised learning with entropy},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sampling-based event-triggered exponential synchronization
for reaction-diffusion neural networks. <em>TNNLS</em>, <em>34</em>(3),
1209–1217. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the exponential synchronization control issue of reaction–diffusion neural networks (RDNNs) is mainly resolved by the sampling-based event-triggered scheme under Dirichlet boundary conditions. Based on the sampled state information, the event-triggered control protocol is updated only when the triggering condition is met, which effectively reduces the communication burden and saves energy. In addition, the proposed control algorithm is combined with sampled-data control, which can effectively avoid the Zeno phenomenon. By thinking of the proper Lyapunov–Krasovskii functional and using some momentous inequalities, a sufficient condition is obtained for RDNNs to achieve exponential synchronization. Finally, some simulation results are shown to demonstrate the validity of the algorithm.},
  archive      = {J_TNNLS},
  author       = {Qian Qiu and Housheng Su},
  doi          = {10.1109/TNNLS.2021.3105126},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1209-1217},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sampling-based event-triggered exponential synchronization for reaction-diffusion neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A probabilistic formulation for meta-weight-net.
<em>TNNLS</em>, <em>34</em>(3), 1194–1208. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, deep neural networks (DNNs) have become dominant tools for various of supervised learning tasks, especially classification. However, it is demonstrated that they can easily overfit to training set biases, such as label noise and class imbalance. Example reweighting algorithms are simple and effective solutions against this issue, but most of them require manually specifying the weighting functions as well as additional hyperparameters. Recently, a meta-learning-based method Meta-Weight-Net (MW-Net) has been proposed to automatically learn the weighting function parameterized by an MLP via additional unbiased metadata, which significantly improves the robustness of prior arts. The method, however, is proposed in a deterministic manner, and short of intrinsic statistical support. In this work, we propose a probabilistic formulation for MW-Net, probabilistic MW-Net (PMW-Net) in short, which treats the weighting function in a probabilistic way, and can include the original MW-Net as a special case. By this probabilistic formulation, additional randomness is introduced while the flexibility of the weighting function can be further controlled during learning. Our experimental results on both synthetic and real datasets show that the proposed method improves the performance of the original MW-Net. Besides, the proposed PMW-Net can also be further extended to fully Bayesian models, to improve their robustness.},
  archive      = {J_TNNLS},
  author       = {Qian Zhao and Jun Shu and Xiang Yuan and Ziming Liu and Deyu Meng},
  doi          = {10.1109/TNNLS.2021.3105104},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1194-1208},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A probabilistic formulation for meta-weight-net},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust semisupervised deep generative model under compound
noise. <em>TNNLS</em>, <em>34</em>(3), 1179–1193. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semisupervised learning has been widely applied to deep generative model such as variational autoencoder. However, there are still limited work in noise-robust semisupervised deep generative model where the noise exists in both of the data and the labels simultaneously, which are referred to as outliers and noisy labels or compound noise. In this article, we propose a novel noise-robust semisupervised deep generative model by jointly tackling the noisy labels and outliers in a unified robust semisupervised variational autoencoder randomized generative adversarial network (URSVAE-GAN). Typically, we consider the uncertainty of the information of the input data in order to enhance the robustness of the variational encoder toward the noisy data in our unified robust semisupervised variational autoencoder (URSVAE). Subsequently, in order to alleviate the detrimental effects of noisy labels, a denoising layer is integrated naturally into the semisupervised variational autoencoder so that the variational inference is conditioned on the corrected labels. Moreover, to enhance the robustness of the variational inference in the presence of outliers, the robust $\beta $ -divergence measure is employed to derive the novel variational lower bound, which already achieves competitive performance. This further motivates the development of URSVAE-GAN that collapses the decoder of URSVAE and the generator of a robust semisupervised generative adversarial network into one unit. By applying the end-to-end denoising scheme in the joint optimization, the experimental results demonstrate the superiority of the proposed framework by the evaluating on image classification and face recognition tasks and comparing with the state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Xu Chen},
  doi          = {10.1109/TNNLS.2021.3105080},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1179-1193},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust semisupervised deep generative model under compound noise},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resilient output synchronization of heterogeneous multiagent
systems with DoS attacks under distributed event-/self-triggered
control. <em>TNNLS</em>, <em>34</em>(3), 1169–1178. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the resilient output synchronization problem of a class of linear heterogeneous multiagent systems subjected to denial-of-service (DoS) attacks. Two types of control mechanisms, namely, event- and self-triggered control mechanisms, are presented so as to cut down unnecessary information transmission. Both of these two mechanisms are distributed, and thus, only local information of each agent and its neighboring agents is adopted for the event condition design. The DoS attacks are considered to be aperiodic, and the quantitative relationship between the attributes of the DoS attacks and the synchronization is also revealed. It is shown that the output synchronization can be achieved exponentially in the presence of DoS attacks under the proposed control mechanisms. The validness of the provided mechanisms is certified by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Shengli Du and Wenying Xu and Junfei Qiao and Daniel W. C. Ho},
  doi          = {10.1109/TNNLS.2021.3105006},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1169-1178},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Resilient output synchronization of heterogeneous multiagent systems with DoS attacks under distributed event-/Self-triggered control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Formation tracking of multiagent systems with time-varying
actuator faults and its application to task-space cooperative tracking
of manipulators. <em>TNNLS</em>, <em>34</em>(3), 1156–1168. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with a fault-tolerant formation tracking problem of nonlinear systems under unknown faults, where the leader’s states are only accessible to a small set of followers via a directed graph. Under these faults, not only the amplitudes but also the signs of control coefficients become time-varying and unknown. The current setting will enhance the investigated problem’s practical relevance and at the same time, it poses nontrivial design challenges of distributed control algorithms and convergence analysis. To solve this problem, a novel distributed control algorithm is developed by incorporating an estimation-based control framework together with a Nussbaum gain approach to guarantee an asymptotic cooperative formation tracking of nonlinear networked systems under unknown and dynamic actuator faults. Moreover, the proposed control framework is extended to ensure an asymptotic task-space coordination of multiple manipulators under unknown actuator faults, kinematics, and dynamics. Lastly, numerical simulation results are provided to validate the effectiveness of the proposed distributed designs.},
  archive      = {J_TNNLS},
  author       = {Zhi Feng and Guoqiang Hu},
  doi          = {10.1109/TNNLS.2021.3104987},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1156-1168},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Formation tracking of multiagent systems with time-varying actuator faults and its application to task-space cooperative tracking of manipulators},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed model-free adaptive control for learning
nonlinear MASs under DoS attacks. <em>TNNLS</em>, <em>34</em>(3),
1146–1155. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the distributed model-free adaptive control (DMFAC) problem for learning nonlinear multiagent systems (MASs) subjected to denial-of-service (DoS) attacks. An improved dynamic linearization method is proposed to obtain an equivalent linear data model for learning systems. To alleviate the influence of DoS attacks, an attack compensation mechanism is developed. Based on the equivalent linear data model and the attack compensation mechanism, a novel learning-based DMFAC algorithm is developed to resist DoS attacks, which provides a unified framework to solve the leaderless consensus control, the leader-following consensus control, and the containment control problems. Finally, simulation examples are shown to illustrate the effectiveness of the developed DMFAC algorithm.},
  archive      = {J_TNNLS},
  author       = {Yong-Sheng Ma and Wei-Wei Che and Chao Deng and Zheng-Guang Wu},
  doi          = {10.1109/TNNLS.2021.3104978},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1146-1155},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed model-free adaptive control for learning nonlinear MASs under DoS attacks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning context-based nonlocal entropy modeling for image
compression. <em>TNNLS</em>, <em>34</em>(3), 1132–1145. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The entropy of the codes usually serves as the rate loss in the recent learned lossy image compression methods. Precise estimation of the probabilistic distribution of the codes plays a vital role in reducing the entropy and boosting the joint rate-distortion performance. However, existing deep learning based entropy models generally assume the latent codes are statistically independent or depend on some side information or local context, which fails to take the global similarity within the context into account and thus hinders the accurate entropy estimation. To address this issue, we propose a special nonlocal operation for context modeling by employing the global similarity within the context. Specifically, due to the constraint of context, nonlocal operation is incalculable in context modeling. We exploit the relationship between the code maps produced by deep neural networks and introduce the proxy similarity functions as a workaround. Then, we combine the local and the global context via a nonlocal attention block and employ it in masked convolutional networks for entropy modeling. Taking the consideration that the width of the transforms is essential in training low distortion models, we finally produce a U-net block in the transforms to increase the width with manageable memory consumption and time complexity. Experiments on Kodak and Tecnick datasets demonstrate the priority of the proposed context-based nonlocal attention block in entropy modeling and the U-net block in low distortion situations. On the whole, our model performs favorably against the existing image compression standards and recent deep image compression models.},
  archive      = {J_TNNLS},
  author       = {Mu Li and Kai Zhang and Jinxing Li and Wangmeng Zuo and Radu Timofte and David Zhang},
  doi          = {10.1109/TNNLS.2021.3104974},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1132-1145},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning context-based nonlocal entropy modeling for image compression},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic modeling cross-modal interactions in two-phase
prediction for entity-relation extraction. <em>TNNLS</em>,
<em>34</em>(3), 1122–1131. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint extraction of entities and their relations benefits from the close interaction between named entities and their relation information. Therefore, how to effectively model such cross-modal interactions is critical for the final performance. Previous works have used simple methods, such as label-feature concatenation, to perform coarse-grained semantic fusion among cross-modal instances but fail to capture fine-grained correlations over token and label spaces, resulting in insufficient interactions. In this article, we propose a dynamic cross-modal attention network (CMAN) for joint entity and relation extraction. The network is carefully constructed by stacking multiple attention units in depth to dynamic model dense interactions over token-label spaces, in which two basic attention units and a novel two-phase prediction are proposed to explicitly capture fine-grained correlations across different modalities (e.g., token-to-token and label-to-token). Experiment results on the CoNLL04 dataset show that our model obtains state-of-the-art results by achieving 91.72\% F1 on entity recognition and 73.46\% F1 on relation classification. In the ADE and DREC datasets, our model surpasses existing approaches by more than 2.1\% and 2.54\% F1 on relation classification. Extensive analyses further confirm the effectiveness of our approach.},
  archive      = {J_TNNLS},
  author       = {Shan Zhao and Minghao Hu and Zhiping Cai and Fang Liu},
  doi          = {10.1109/TNNLS.2021.3104971},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1122-1131},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic modeling cross-modal interactions in two-phase prediction for entity-relation extraction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural architecture search for portrait parsing.
<em>TNNLS</em>, <em>34</em>(3), 1112–1121. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a neural architecture search (NAS) method for portrait parsing, which is a novel up-level task based on portrait segmentation and face labeling. Recently, NAS has become an effective method in terms of automatic machine learning. However, remarkable achievements have been made only in image classification and natural language processing (NLP) areas. Meanwhile, state-of-the-art portrait segmentation and face labeling approaches are all manually designed, but few models reach a tradeoff between efficiency and performance. Thus, we are extremely interested in improving existing NAS methods for dense-per-pixel prediction tasks on portrait datasets. To achieve that, we resort to a cell-based encoder–decoder architecture with an elaborate design of connectivity structure and searching space. As a result, we achieve state-of-the-art performance on three portrait tasks, including 96.8\% MIOU on EG1800 (portrait segmentation), 91.2\% overall $F1$ -score on HELEN (face labeling), and 95.1\% overall $F1$ -score on CelebAMask-HQ (portrait parsing) with only 2.29M model parameters. That is, our approach compares favorably with all previous works on portrait datasets. More crucially, we empirically prove that even a fundamental encoder–decoder architecture may reach an outstanding result on the aforementioned tasks with the help of the innovative approach of NAS. To the best of our knowledge, our work is also the first to report the success of applying NAS on these portrait tasks.},
  archive      = {J_TNNLS},
  author       = {Bo Lyu and Yin Yang and Shiping Wen and Tingwen Huang and Ke Li},
  doi          = {10.1109/TNNLS.2021.3104872},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1112-1121},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural architecture search for portrait parsing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An overview of the stability analysis of recurrent neural
networks with multiple equilibria. <em>TNNLS</em>, <em>34</em>(3),
1098–1111. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stability analysis of recurrent neural networks (RNNs) with multiple equilibria has received extensive interest since it is a prerequisite for successful applications of RNNs. With the increasing theoretical results on this topic, it is desirable to review the results for a systematical understanding of the state of the art. This article provides an overview of the stability results of RNNs with multiple equilibria including complete stability and multistability. First, preliminaries on the complete stability and multistability analysis of RNNs are introduced. Second, the complete stability results of RNNs are summarized. Third, the multistability results of various RNNs are reviewed in detail. Finally, future directions in these interesting topics are suggested.},
  archive      = {J_TNNLS},
  author       = {Peng Liu and Jun Wang and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2021.3105519},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1098-1111},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An overview of the stability analysis of recurrent neural networks with multiple equilibria},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning graph representations with maximal cliques.
<em>TNNLS</em>, <em>34</em>(2), 1089–1096. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Euclidean property of graph structures has faced interesting challenges when deep learning methods are applied. Graph convolutional networks (GCNs) can be regarded as one of the successful approaches to classification tasks on graph data, although the structure of this approach limits its performance. In this work, a novel representation learning approach is introduced based on spectral convolutions on graph-structured data in a semisupervised learning setting. Our proposed method, COnvOlving cLiques (COOL), is constructed as a neighborhood aggregation approach for learning node representations using established GCN architectures. This approach relies on aggregating local information by finding maximal cliques. Unlike the existing graph neural networks which follow a traditional neighborhood averaging scheme, COOL allows for aggregation of densely connected neighboring nodes of potentially differing locality. This leads to substantial improvements on multiple transductive node classification tasks.},
  archive      = {J_TNNLS},
  author       = {Soheila Molaei and Nima Ghanbari Bousejin and Hadi Zare and Mahdi Jalili and Shirui Pan},
  doi          = {10.1109/TNNLS.2021.3104901},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {1089-1096},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning graph representations with maximal cliques},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constrained center loss for convolutional neural networks.
<em>TNNLS</em>, <em>34</em>(2), 1080–1088. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the feature representation’s point of view, the feature learning module of a convolutional neural network (CNN) is to transform an input pattern into a feature vector. This feature vector is then multiplied with a number of output weight vectors to produce softmax scores. The common training objective in CNNs is based on the softmax loss, which ignores the intra-class compactness. This brief proposes a constrained center loss (CCL)-based algorithm to extract robust features. The training objective of a CNN consists of two terms, softmax loss and CCL. The aim of the softmax loss is to push the feature vectors from different classes apart. Meanwhile, the CCL aims at clustering the feature vectors such that the feature vectors from the same classes are close together. Instead of using stochastic gradient descent (SGD) algorithms to learn all the connection weights and the cluster centers at the same time. Our CCL-based algorithm is based on the alternative learning strategy. We first fix the connection weights of the CNN and update the cluster centers based on an analytical formula, which can be implemented based on the minibatch concept. We then fix the cluster centers and update the connection weights for a number of SGD minibatch iterations. We also propose a simplified CCL (SCCL) algorithm. Experiments are performed on six commonly used benchmark datasets. The results demonstrate that the two proposed algorithms outperform several state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Zhanglei Shi and Hao Wang and Chi-Sing Leung},
  doi          = {10.1109/TNNLS.2021.3104392},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {1080-1088},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Constrained center loss for convolutional neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proportional–integral state estimator for quaternion-valued
neural networks with time-varying delays. <em>TNNLS</em>,
<em>34</em>(2), 1074–1079. (<a
href="https://doi.org/10.1109/TNNLS.2021.3103979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief investigates the problem of state estimation of quaternion-valued neural networks (QVNNs) with time-varying delays. First, by extending the Jensen inequality to quaternion domain, an extended Jensen inequality with quaternion term is derived. Second, a class of proportional–integral state estimator (PISE) with exponential decay term is proposed. Then, by constructing a suitable Lyapunov–Krasovskii functional (LKF), some sufficient conditions are obtained to ensure the existence of the designed PISE and the gain matrices of the designed PISE can be directly computed. Simulations are given to illustrate the advantage of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Guoqiang Tan and Zhanshan Wang and Zhan Shi},
  doi          = {10.1109/TNNLS.2021.3103979},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {1074-1079},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Proportional–Integral state estimator for quaternion-valued neural networks with time-varying delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning from hierarchical critics.
<em>TNNLS</em>, <em>34</em>(2), 1066–1073. (<a
href="https://doi.org/10.1109/TNNLS.2021.3103642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we investigate the use of global information to speed up the learning process and increase the cumulative rewards of reinforcement learning (RL) in competition tasks. Within the framework of actor–critic RL, we introduce multiple cooperative critics from two levels of a hierarchy and propose an RL from the hierarchical critics (RLHC) algorithm. In our approach, each agent receives value information from local and global critics regarding a competition task and accesses multiple cooperative critics in a top-down hierarchy. Thus, each agent not only receives low-level details, but also considers coordination from higher levels, thereby obtaining global information to improve the training performance. Then, we test the proposed RLHC algorithm against a benchmark algorithm, that is, proximal policy optimization (PPO), under four experimental scenarios consisting of tennis, soccer, banana collection, and crawler competitions within the Unity environment. The results show that RLHC outperforms the benchmark on these four competitive tasks.},
  archive      = {J_TNNLS},
  author       = {Zehong Cao and Chin-Teng Lin},
  doi          = {10.1109/TNNLS.2021.3103642},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {1066-1073},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning from hierarchical critics},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample-based neural approximation approach for probabilistic
constrained programs. <em>TNNLS</em>, <em>34</em>(2), 1058–1065. (<a
href="https://doi.org/10.1109/TNNLS.2021.3102323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a neural approximation-based method for solving continuous optimization problems with probabilistic constraints. After reformulating the probabilistic constraints as the quantile function, a sample-based neural network model is used to approximate the quantile function. The statistical guarantees of the neural approximation are discussed by showing the convergence and feasibility analysis. Then, by introducing the neural approximation, a simulated annealing-based algorithm is revised to solve the probabilistic constrained programs. An interval predictor model (IPM) of wind power is investigated to validate the proposed method.},
  archive      = {J_TNNLS},
  author       = {Xun Shen and Tinghui Ouyang and Nan Yang and Jiancang Zhuang},
  doi          = {10.1109/TNNLS.2021.3102323},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {1058-1065},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sample-based neural approximation approach for probabilistic constrained programs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leader-following consensus control for uncertain feedforward
stochastic nonlinear multiagent systems. <em>TNNLS</em>, <em>34</em>(2),
1049–1057. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the leader-following consensus problem of feedforward stochastic nonlinear multiagent systems with switching topologies. Output information for all agents, except for state information, can be acquired based on sensor measurement. Moreover, the stochastic disturbances from external unpredictable environments are considered on all agent systems with a feedforward structure. In these conditions, we propose a novel consensus scheme with a simple design procedure. First, for each follower, we construct a dynamic gain-based switched compensator using its output and its neighbor agents’ outputs to provide feedback control signals. Then, for each follower, we develop a compensator-based distributed controller that is not directly associated with the topology switching signal such that it has a first derivative and antishake. Thereafter, by means of the Lyapunov stability theory, we verify that the leader-following consensus can be acquired asymptotically in probability under the controllers’ action if the topology switching signal fulfills an average dwell time condition. Finally, the feasibility of the control algorithm is checked via numerical simulation.},
  archive      = {J_TNNLS},
  author       = {Kuo Li and Changchun Hua and Xiu You and Choon Ki Ahn},
  doi          = {10.1109/TNNLS.2021.3105109},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {1049-1057},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Leader-following consensus control for uncertain feedforward stochastic nonlinear multiagent systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based transfer reinforcement learning based on
graphical model representations. <em>TNNLS</em>, <em>34</em>(2),
1035–1048. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) plays an essential role in the field of artificial intelligence but suffers from data inefficiency and model-shift issues. One possible solution to deal with such issues is to exploit transfer learning. However, interpretability problems and negative transfer may occur without explainable models. In this article, we define Relation Transfer as explainable and transferable learning based on graphical model representations, inferring the skeleton and relations among variables in a causal view and generalizing to the target domain. The proposed algorithm consists of the following three steps. First, we leverage a suitable casual discovery method to identify the causal graph based on the augmented source domain data. After that, we make inferences on the target model based on the prior causal knowledge. Finally, offline RL training on the target model is utilized as prior knowledge to improve the policy training in the target domain. The proposed method can answer the question of what to transfer and realize zero-shot transfer across related domains in a principled way. To demonstrate the robustness of the proposed framework, we conduct experiments on four classical control problems as well as one simulation to the real-world application. Experimental results on both continuous and discrete cases demonstrate the efficacy of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Yuewen Sun and Kun Zhang and Changyin Sun},
  doi          = {10.1109/TNNLS.2021.3107375},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {1035-1048},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-based transfer reinforcement learning based on graphical model representations},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bilinear graph networks for visual question answering.
<em>TNNLS</em>, <em>34</em>(2), 1023–1034. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article revisits the bilinear attention networks (BANs) in the visual question answering task from a graph perspective. The classical BANs build a bilinear attention map to extract the joint representation of words in the question and objects in the image but lack fully exploring the relationship between words for complex reasoning. In contrast, we develop bilinear graph networks to model the context of the joint embeddings of words and objects. Two kinds of graphs are investigated, namely, image-graph and question-graph. The image-graph transfers features of the detected objects to their related query words, enabling the output nodes to have both semantic and factual information. The question-graph exchanges information between these output nodes from image-graph to amplify the implicit yet important relationship between objects. These two kinds of graphs cooperate with each other, and thus, our resulting model can build the relationship and dependency between objects, which leads to the realization of multistep reasoning. Experimental results on the VQA v2.0 validation dataset demonstrate the ability of our method to handle complex questions. On the test-std set, our best single model achieves state-of-the-art performance, boosting the overall accuracy to 72.56\%, and we are one of the top-two entries in the VQA Challenge 2020.},
  archive      = {J_TNNLS},
  author       = {Dalu Guo and Chang Xu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2021.3104937},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {1023-1034},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bilinear graph networks for visual question answering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random hyperboxes. <em>TNNLS</em>, <em>34</em>(2),
1008–1022. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a simple yet powerful ensemble classifier, called Random Hyperboxes, constructed from individual hyperbox-based classifiers trained on the random subsets of sample and feature spaces of the training set. We also show a generalization error bound of the proposed classifier based on the strength of the individual hyperbox-based classifiers as well as the correlation among them. The effectiveness of the proposed classifier is analyzed using a carefully selected illustrative example and compared empirically with other popular single and ensemble classifiers via 20 datasets using statistical testing methods. The experimental results confirmed that our proposed method outperformed other fuzzy min–max neural networks (FMNNs), popular learning algorithms, and is competitive with other ensemble methods. Finally, we identify the existing issues related to the generalization error bounds of the real datasets and inform the potential research directions.},
  archive      = {J_TNNLS},
  author       = {Thanh Tung Khuat and Bogdan Gabrys},
  doi          = {10.1109/TNNLS.2021.3104896},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {1008-1022},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Random hyperboxes},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Adaptive neural control of nonlinear nonstrict feedback
systems with full-state constraints: A novel nonlinear mapping method.
<em>TNNLS</em>, <em>34</em>(2), 999–1007. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a neural-networks (NNs)-based adaptive asymptotic tracking control scheme is presented for a class of uncertain nonstrict feedback nonlinear systems with time-varying full-state constraints. First, we construct a novel exponentially decaying nonlinear mapping to map the constrained system states to new system states without constraints. Instead of the traditional barrier Lyapunov function methods, the feasible conditions which require the virtual control signals satisfying the constraint requirements are removed. By employing the Nussbaum design method to eliminate the effect of unknown control gains, the general assumption about the signs of the unknown control gains is relaxed. Then, the nonstrict feedback form of the system can be pulled back to the strict feedback form through the basic properties of radial basis function NNs. Simultaneously, the intermediate control signals and the desired controller are constructed by the backstepping process and the Nussbaum design method. The designed controller can ensure that all signals in the whole closed-loop system are bounded without the violation of the constraints and hold the asymptotic tracking performance. In the end, a practical example about a brush dc motor driving a one-link robot manipulator is given to illustrate the effectiveness of the proposed design scheme.},
  archive      = {J_TNNLS},
  author       = {Jiaming Zhang and Ben Niu and Ding Wang and Huanqing Wang and Peiyong Duan and Guangdeng Zong},
  doi          = {10.1109/TNNLS.2021.3104877},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {999-1007},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural control of nonlinear nonstrict feedback systems with full-state constraints: A novel nonlinear mapping method},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compact and stable memristive visual geometry group neural
network. <em>TNNLS</em>, <em>34</em>(2), 987–998. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As edge computing platforms need low power consumption and small volume circuit with artificial intelligence (AI), we design a compact and stable memristive visual geometry group (MVGG) neural network for image classification. According to characteristics of matrix–vector multiplication (MVM) using memristor crossbars, we design three pruning methods named row pruning, column pruning, and parameter distribution pruning. With a loss of only 0.41\% of the classification accuracy, a pruning rate of 36.87\% is obtained. In the MVGG circuit, both the batch normalization (BN) layers and dropout layers are combined into the memristive convolutional computing layer for decreasing the computing amount of the memristive neural network. In order to further reduce the influence of multistate conductance of memristors on classification accuracy of MVGG circuit, the layer optimization circuit and the channel optimization circuit are designed in this article. The theoretical analysis shows that the introduction of the optimized methods can greatly reduce the impact of the multistate conductance of memristors on the classification accuracy of MVGG circuits. Circuit simulation experiments show that, for the layer-optimized MVGG circuit, when the number of multistate conductance of memristors is $2^{5}= 32$ , the optimized circuit can basically achieve an accuracy of the full-precision MVGG. For the channel-optimized MVGG circuit, when the number of multistate conductance of memristors is $2^{2}= 4$ , the optimized circuit can basically achieve an accuracy of the full-precision MVGG.},
  archive      = {J_TNNLS},
  author       = {Huanhuan Ran and Shiping Wen and Qian Li and Yuting Cao and Kaibo Shi and Tingwen Huang},
  doi          = {10.1109/TNNLS.2021.3104860},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {987-998},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Compact and stable memristive visual geometry group neural network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiview clustering via proximity learning in latent
representation space. <em>TNNLS</em>, <em>34</em>(2), 973–986. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing multiview clustering methods are based on the original feature space. However, the feature redundancy and noise in the original feature space limit their clustering performance. Aiming at addressing this problem, some multiview clustering methods learn the latent data representation linearly, while performance may decline if the relation between the latent data representation and the original data is nonlinear. The other methods which nonlinearly learn the latent data representation usually conduct the latent representation learning and clustering separately, resulting in that the latent data representation might be not well adapted to clustering. Furthermore, none of them model the intercluster relation and intracluster correlation of data points, which limits the quality of the learned latent data representation and therefore influences the clustering performance. To solve these problems, this article proposes a novel multiview clustering method via proximity learning in latent representation space, named multiview latent proximity learning (MLPL). For one thing, MLPL learns the latent data representation in a nonlinear manner which takes the intercluster relation and intracluster correlation into consideration simultaneously. For another, through conducting the latent representation learning and consensus proximity learning simultaneously, MLPL learns a consensus proximity matrix with $k$ connected components to output the clustering result directly. Extensive experiments are conducted on seven real-world datasets to demonstrate the effectiveness and superiority of the MLPL method compared with the state-of-the-art multiview clustering methods.},
  archive      = {J_TNNLS},
  author       = {Bao-Yu Liu and Ling Huang and Chang-Dong Wang and Jian-Huang Lai and Philip S. Yu},
  doi          = {10.1109/TNNLS.2021.3104846},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {973-986},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview clustering via proximity learning in latent representation space},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An accelerated maximally split ADMM for a class of
generalized ridge regression. <em>TNNLS</em>, <em>34</em>(2), 958–972.
(<a href="https://doi.org/10.1109/TNNLS.2021.3104840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridge regression (RR) has been commonly used in machine learning, but is facing computational challenges in big data applications. To meet the challenges, this article develops a highly parallel new algorithm, i.e., an accelerated maximally split alternating direction method of multipliers (A-MS-ADMM), for a class of generalized RR (GRR) that allows different regularization factors for different regression coefficients. Linear convergence of the new algorithm along with its convergence ratio is established. Optimal parameters of the algorithm for the GRR with a particular set of regularization factors are derived, and a selection scheme of the algorithm parameters for the GRR with general regularization factors is also discussed. The new algorithm is then applied in the training of single-layer feedforward neural networks. Experimental results on performance validation on real-world benchmark datasets for regression and classification and comparisons with existing methods demonstrate the fast convergence, low computational complexity, and high parallelism of the new algorithm.},
  archive      = {J_TNNLS},
  author       = {Xiaoping Lai and Jiuwen Cao and Zhiping Lin},
  doi          = {10.1109/TNNLS.2021.3104840},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {958-972},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An accelerated maximally split ADMM for a class of generalized ridge regression},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Adaptive NN optimal consensus fault-tolerant control for
stochastic nonlinear multiagent systems. <em>TNNLS</em>, <em>34</em>(2),
947–957. (<a href="https://doi.org/10.1109/TNNLS.2021.3104839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of adaptive neural network (NN) optimal consensus tracking control for nonlinear multiagent systems (MASs) with stochastic disturbances and actuator bias faults. In control design, NN is adopted to approximate the unknown nonlinear dynamic, and a state identifier is constructed. The fault estimator is designed to solve the problem raised by time-varying actuator bias fault. By utilizing adaptive dynamic programming (ADP) in identifier–critic–actor construction, an adaptive NN optimal consensus fault-tolerant control algorithm is presented. It is proven that all signals of the controlled system are uniformly ultimately bounded (UUB) in probability, and all states of the follower agents can remain consensus with the leader’s state. Finally, simulation results are given to illustrate the effectiveness of the developed optimal consensus control scheme and theorem.},
  archive      = {J_TNNLS},
  author       = {Kewen Li and Yongming Li},
  doi          = {10.1109/TNNLS.2021.3104839},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {947-957},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive NN optimal consensus fault-tolerant control for stochastic nonlinear multiagent systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dictionary learning with low-rank coding coefficients for
tensor completion. <em>TNNLS</em>, <em>34</em>(2), 932–946. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel tensor learning and coding model for third-order data completion. The aim of our model is to learn a data-adaptive dictionary from given observations and determine the coding coefficients of third-order tensor tubes. In the completion process, we minimize the low-rankness of each tensor slice containing the coding coefficients. By comparison with the traditional predefined transform basis, the advantages of the proposed model are that: 1) the dictionary can be learned based on the given data observations so that the basis can be more adaptively and accurately constructed and 2) the low-rankness of the coding coefficients can allow the linear combination of dictionary features more effectively. Also we develop a multiblock proximal alternating minimization algorithm for solving such tensor learning and coding model and show that the sequence generated by the algorithm can globally converge to a critical point. Extensive experimental results for real datasets such as videos, hyperspectral images, and traffic data are reported to demonstrate these advantages and show that the performance of the proposed tensor learning and coding method is significantly better than the other tensor completion methods in terms of several evaluation metrics.},
  archive      = {J_TNNLS},
  author       = {Tai-Xiang Jiang and Xi-Le Zhao and Hao Zhang and Michael K. Ng},
  doi          = {10.1109/TNNLS.2021.3104837},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {932-946},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dictionary learning with low-rank coding coefficients for tensor completion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the compressive power of boolean threshold autoencoders.
<em>TNNLS</em>, <em>34</em>(2), 921–931. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An autoencoder is a layered neural network whose structure can be viewed as consisting of an encoder, which compresses an input vector to a lower dimensional vector, and a decoder, which transforms the low-dimensional vector back to the original input vector (or one that is very similar). In this article, we explore the compressive power of autoencoders that are Boolean threshold networks by studying the numbers of nodes and layers that are required to ensure that each vector in a given set of distinct input binary vectors is transformed back to its original. We show that for any set of $n$ distinct vectors there exists a seven-layer autoencoder with the optimal compression ratio, (i.e., the size of the middle layer is logarithmic in $n$ ), but that there is a set of $n$ vectors for which there is no three-layer autoencoder with a middle layer of logarithmic size. In addition, we present a kind of tradeoff: if the compression ratio is allowed to be considerably larger than the optimal, then there is a five-layer autoencoder. We also study the numbers of nodes and layers required only for encoding, and the results suggest that the decoding part is the bottleneck of autoencoding. For example, there always is a three-layer Boolean threshold encoder that compresses $n$ vectors into a dimension that is twice the logarithm of $n$ .},
  archive      = {J_TNNLS},
  author       = {Avraham A. Melkman and Sini Guo and Wai-Ki Ching and Pengyu Liu and Tatsuya Akutsu},
  doi          = {10.1109/TNNLS.2021.3104646},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {921-931},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the compressive power of boolean threshold autoencoders},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic sampled-data exponential synchronization of
markovian jump neural networks with time-varying delays. <em>TNNLS</em>,
<em>34</em>(2), 909–920. (<a
href="https://doi.org/10.1109/TNNLS.2021.3103958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the exponential synchronization of Markovian jump neural networks (MJNNs) with time-varying delays is investigated via stochastic sampling and looped-functional (LF) approach. For simplicity, it is assumed that there exist two sampling periods, which satisfies the Bernoulli distribution. To model the synchronization error system, two random variables that, respectively, describe the location of the input delays and the sampling periods are introduced. In order to reduce the conservativeness, a time-dependent looped-functional (TDLF) is designed, which takes full advantage of the available information of the sampling pattern. The Gronwall–Bellman inequalities and the discrete-time Lyapunov stability theory are utilized jointly to analyze the mean-square exponential stability of the error system. A less conservative exponential synchronization criterion is derived, based on which a mode-independent stochastic sampled-data controller (SSDC) is designed. Finally, the effectiveness of the proposed control strategy is demonstrated by a numerical example.},
  archive      = {J_TNNLS},
  author       = {Lan Yao and Zhen Wang and Xia Huang and Yuxia Li and Qian Ma and Hao Shen},
  doi          = {10.1109/TNNLS.2021.3103958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {909-920},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stochastic sampled-data exponential synchronization of markovian jump neural networks with time-varying delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Passivity and finite-time passivity for multi-weighted
fractional-order complex networks with fixed and adaptive couplings.
<em>TNNLS</em>, <em>34</em>(2), 894–908. (<a
href="https://doi.org/10.1109/TNNLS.2021.3103809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents several new $\alpha $ -passivity and $\alpha $ -finite-time passivity ( $\alpha $ -FTP) concepts for the fractional-order systems with different input and output dimensions, which are distinct from the concepts for integer-order systems and extend the existing passivity and FTP definitions to some extent. On one hand, we not only develop some sufficient conditions for ensuring the $\alpha $ -passivity of the multi-weighted fractional-order complex dynamical networks (MWFOCDNs) with fixed and adaptive couplings, but also discuss the synchronization for the MWFOCDNs based on the $\alpha $ -output-strict passivity ( $\alpha $ -OSP). On the other hand, the $\alpha $ -FTP for the MWFOCDNs with fixed and adaptive couplings are also studied on the basis of the designed state feedback controller, and the relationship between finite-time synchronization (FTS) and $\alpha $ -FTP for the MWFOCDNs is also illustrated. Finally, two numerical examples with simulation results are used to demonstrate the validity of the obtained criteria.},
  archive      = {J_TNNLS},
  author       = {Jin-Liang Wang and Xiao-Xiao Zhang and Guoguang Wen and Yiwen Chen and Huai-Ning Wu},
  doi          = {10.1109/TNNLS.2021.3103809},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {894-908},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Passivity and finite-time passivity for multi-weighted fractional-order complex networks with fixed and adaptive couplings},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An annealing mechanism for adversarial training
acceleration. <em>TNNLS</em>, <em>34</em>(2), 882–893. (<a
href="https://doi.org/10.1109/TNNLS.2021.3103528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the empirical success in various domains, it has been revealed that deep neural networks are vulnerable to maliciously perturbed input data that can dramatically degrade their performance. These are known as adversarial attacks. To counter adversarial attacks, adversarial training formulated as a form of robust optimization has been demonstrated to be effective. However, conducting adversarial training brings much computational overhead compared with standard training. In order to reduce the computational cost, we propose an annealing mechanism, annealing mechanism for adversarial training acceleration (Amata), to reduce the overhead associated with adversarial training. The proposed Amata is provably convergent, well-motivated from the lens of optimal control theory, and can be combined with existing acceleration methods to further enhance performance. It is demonstrated that, on standard datasets, Amata can achieve similar or better robustness with around 1/3–1/2 the computational time compared with traditional methods. In addition, Amata can be incorporated into other adversarial training acceleration algorithms (e.g., YOPO, Free, Fast, and ATTA), which leads to a further reduction in computational time on large-scale problems.},
  archive      = {J_TNNLS},
  author       = {Nanyang Ye and Qianxiao Li and Xiao-Yun Zhou and Zhanxing Zhu},
  doi          = {10.1109/TNNLS.2021.3103528},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {882-893},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An annealing mechanism for adversarial training acceleration},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DisP+v: A unified framework for disentangling prototype and
variation from single sample per person. <em>TNNLS</em>, <em>34</em>(2),
867–881. (<a href="https://doi.org/10.1109/TNNLS.2021.3103194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single sample per person face recognition (SSPP FR) is one of the most challenging problems in FR due to the extreme lack of enrolment data. To date, the most popular SSPP FR methods are the generic learning methods, which recognize query face images based on the so-called prototype plus variation (i.e., P+V) model. However, the classic P+V model suffers from two major limitations: 1) it linearly combines the prototype and variation images in the observational pixel-spatial space and cannot generalize to multiple nonlinear variations, e.g., poses, which are common in face images and 2) it would be severely impaired once the enrolment face images are contaminated by nuisance variations. To address the two limitations, it is desirable to disentangle the prototype and variation in a latent feature space and to manipulate the images in a semantic manner. To this end, we propose a novel disentangled prototype plus variation model, dubbed DisP+V, which consists of an encoder–decoder generator and two discriminators. The generator and discriminators play two adversarial games such that the generator nonlinearly encodes the images into a latent semantic space, where the more discriminative prototype feature and the less discriminative variation feature are disentangled. Meanwhile, the prototype and variation features can guide the generator to generate an identity-preserved prototype and the corresponding variation, respectively. Experiments on various real-world face datasets demonstrate the superiority of our DisP+V model over the classic P+V model for SSPP FR. Furthermore, DisP+V demonstrates its unique characteristics in both prototype recovery and face editing/interpolation.},
  archive      = {J_TNNLS},
  author       = {Meng Pang and Binghui Wang and Mang Ye and Yiu-ming Cheung and Yiran Chen and Bihan Wen},
  doi          = {10.1109/TNNLS.2021.3103194},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {867-881},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DisP+V: A unified framework for disentangling prototype and variation from single sample per person},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LadRa-net: Locally aware dynamic reread attention net for
sentence semantic matching. <em>TNNLS</em>, <em>34</em>(2), 853–866. (<a
href="https://doi.org/10.1109/TNNLS.2021.3103185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentence semantic matching requires an agent to determine the semantic relation between two sentences, which is widely used in various natural language tasks, such as natural language inference (NLI) and paraphrase identification (PI). Much recent progress has been made in this area, especially attention-based methods and pretrained language model-based methods. However, most of these methods focus on all the important parts in sentences in a static way and only emphasize how important the words are to the query, inhibiting the ability of the attention mechanism. In order to overcome this problem and boost the performance of the attention mechanism, we propose a novel dynamic reread (DRr) attention, which can pay close attention to one small region of sentences at each step and reread the important parts for better sentence representations. Based on this attention variation, we develop a novel DRr network (DRr-Net) for sentence semantic matching. Moreover, selecting one small region in DRr attention seems insufficient for sentence semantics, and employing pretrained language models as input encoders will introduce incomplete and fragile representation problems. To this end, we extend DRr-Net to locally aware dynamic reread attention net (LadRa-Net), in which local structure of sentences is employed to alleviate the shortcoming of byte-pair encoding (BPE) in pretrained language models and boost the performance of DRr attention. Extensive experiments on two popular sentence semantic matching tasks demonstrate that DRr-Net can significantly improve the performance of sentence semantic matching. Meanwhile, LadRa-Net is able to achieve better performance by considering the local structures of sentences. In addition, it is exceedingly interesting that some discoveries in our experiments are consistent with some findings of psychological research.},
  archive      = {J_TNNLS},
  author       = {Kun Zhang and Guangyi Lv and Le Wu and Enhong Chen and Qi Liu and Meng Wang},
  doi          = {10.1109/TNNLS.2021.3103185},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {853-866},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LadRa-net: Locally aware dynamic reread attention net for sentence semantic matching},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label distribution learning by exploiting label distribution
manifold. <em>TNNLS</em>, <em>34</em>(2), 839–852. (<a
href="https://doi.org/10.1109/TNNLS.2021.3103178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label correlation is helpful to alleviate the overwhelming output space of label distribution learning (LDL). However, existing studies either only consider one of global and local label correlations or exploit label correlation by some prior knowledge (e.g., low-rank assumption, which may not hold sometimes). To efficiently exploit both global and local label correlations in a data-driven way, we propose in this article a new LDL method called label distribution learning by exploiting label distribution manifold (LDL-LDM). Our basic idea is that the underlying manifold structure of label distribution may encode the correlations among labels. LDL-LDM works as follows. First, to exploit global label correlation, we learn the label distribution manifold and encourage the outputs of our model to lie in the same manifold. Second, we learn the label distribution manifold of different clusters of samples to consider local label correlations. Third, to handle incomplete label distribution learning (incomplete LDL), we jointly learn label distribution and label distribution manifold. Theoretical analysis demonstrates the generalization of our method. Finally, experimental results validate the effectiveness of LDL-LDM in both full and incomplete LDL cases.},
  archive      = {J_TNNLS},
  author       = {Jing Wang and Xin Geng},
  doi          = {10.1109/TNNLS.2021.3103178},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {839-852},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Label distribution learning by exploiting label distribution manifold},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SpaRCe: Improved learning of reservoir computing systems
through sparse representations. <em>TNNLS</em>, <em>34</em>(2), 824–838.
(<a href="https://doi.org/10.1109/TNNLS.2021.3102378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“Sparse” neural networks, in which relatively few neurons or connections are active, are common in both machine learning and neuroscience. While, in machine learning, “sparsity” is related to a penalty term that leads to some connecting weights becoming small or zero, in biological brains, sparsity is often created when high spiking thresholds prevent neuronal activity. Here, we introduce sparsity into a reservoir computing network via neuron-specific learnable thresholds of activity, allowing neurons with low thresholds to contribute to decision-making but suppressing information from neurons with high thresholds. This approach, which we term “SpaRCe,” optimizes the sparsity level of the reservoir without affecting the reservoir dynamics. The read-out weights and the thresholds are learned by an online gradient rule that minimizes an error function on the outputs of the network. Threshold learning occurs by the balance of two opposing forces: reducing interneuronal correlations in the reservoir by deactivating redundant neurons, while increasing the activity of neurons participating in correct decisions. We test SpaRCe on classification problems and find that threshold learning improves performance compared to standard reservoir computing. SpaRCe alleviates the problem of catastrophic forgetting, a problem most evident in standard echo state networks (ESNs) and recurrent neural networks in general, due to increasing the number of task-specialized neurons that are included in the network decisions.},
  archive      = {J_TNNLS},
  author       = {Luca Manneschi and Andrew C. Lin and Eleni Vasilaki},
  doi          = {10.1109/TNNLS.2021.3102378},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {824-838},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SpaRCe: Improved learning of reservoir computing systems through sparse representations},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Globally adaptive neural network tracking for uncertain
output-feedback systems. <em>TNNLS</em>, <em>34</em>(2), 814–823. (<a
href="https://doi.org/10.1109/TNNLS.2021.3102274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of global neural network (NN) tracking control for uncertain nonlinear systems in output feedback form under disturbances with unknown bounds. Compared with the existing NN control method, the differences of the proposed scheme are as follows. The designed actual controller consists of an NN controller working in the approximate domain and a robust controller working outside the approximate domain, in addition, a new smooth switching function is designed to achieve the smooth switching between the two controllers, in order to ensure the globally uniformly ultimately bounded of all closed-loop signals. The Lyapunov analysis method is used to strictly prove the global stability under the combined action of unmeasured states and system uncertainties, and the output tracking error is guaranteed to converge to an arbitrarily small neighborhood through a reasonable selection of design parameters. A numerical example and a practical example were put forward to verify the effectiveness of the control strategy.},
  archive      = {J_TNNLS},
  author       = {Qiufeng Wang and Zhengqiang Zhang and Xue-Jun Xie},
  doi          = {10.1109/TNNLS.2021.3102274},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {814-823},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Globally adaptive neural network tracking for uncertain output-feedback systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilevel graph matching networks for deep graph similarity
learning. <em>TNNLS</em>, <em>34</em>(2), 799–813. (<a
href="https://doi.org/10.1109/TNNLS.2021.3102234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the celebrated graph neural networks (GNNs) yield effective representations for individual nodes of a graph, there has been relatively less success in extending to the task of graph similarity learning. Recent work on graph similarity learning has considered either global-level graph–graph interactions or low-level node–node interactions, however, ignoring the rich cross-level interactions (e.g., between each node of one graph and the other whole graph). In this article, we propose a multilevel graph matching network (MGMN) framework for computing the graph similarity between any pair of graph-structured objects in an end-to-end fashion. In particular, the proposed MGMN consists of a node–graph matching network (NGMN) for effectively learning cross-level interactions between each node of one graph and the other whole graph, and a siamese GNN to learn global-level interactions between two input graphs. Furthermore, to compensate for the lack of standard benchmark datasets, we have created and collected a set of datasets for both the graph–graph classification and graph–graph regression tasks with different sizes in order to evaluate the effectiveness and robustness of our models. Comprehensive experiments demonstrate that MGMN consistently outperforms state-of-the-art baseline models on both the graph–graph classification and graph–graph regression tasks. Compared with previous work, multilevel graph matching network (MGMN) also exhibits stronger robustness as the sizes of the two input graphs increase.},
  archive      = {J_TNNLS},
  author       = {Xiang Ling and Lingfei Wu and Saizhuo Wang and Tengfei Ma and Fangli Xu and Alex X. Liu and Chunming Wu and Shouling Ji},
  doi          = {10.1109/TNNLS.2021.3102234},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {799-813},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multilevel graph matching networks for deep graph similarity learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural-network-based set-membership fault estimation for 2-d
systems under encoding–decoding mechanism. <em>TNNLS</em>,
<em>34</em>(2), 786–798. (<a
href="https://doi.org/10.1109/TNNLS.2021.3102127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the simultaneous state and fault estimation problem is investigated for a class of nonlinear 2-D shift-varying systems, where the sensors and the estimator are connected via a communication network of limited bandwidth. With the purpose of relieving the communication burden and enhancing the transmission security, a new encoding–decoding mechanism is put forward so as to encode the transmitted data with a finite number of bits. The aim of the addressed problem is to develop a neural-network (NN)-based set-membership estimator for jointly estimating the system states and the faults, where the estimation errors are guaranteed to reside within an optimized ellipsoidal set. With the aid of the mathematical induction technique and certain convex optimization approaches, sufficient conditions are derived for the existence of the desired set-membership estimator, and the estimator gains and the NN tuning scalars are then presented in terms of the solutions to a set of optimization problems subject to ellipsoidal constraints. Finally, an illustrative example is given to demonstrate the effectiveness of the proposed estimator design method.},
  archive      = {J_TNNLS},
  author       = {Kaiqun Zhu and Zidong Wang and Yun Chen and Guoliang Wei},
  doi          = {10.1109/TNNLS.2021.3102127},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {786-798},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based set-membership fault estimation for 2-D systems under Encoding–Decoding mechanism},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fixed-time stabilization of discontinuous neutral neural
networks with proportional delays via new fixed-time stability lemmas.
<em>TNNLS</em>, <em>34</em>(2), 775–785. (<a
href="https://doi.org/10.1109/TNNLS.2021.3101252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When studying the stability of time-delayed discontinuous systems, Lyapunov-Krasovskii functional (LKF) is an essential tool. More relaxed conditions imposed on the LKF are preferred and can take more advantages in real applications. In this article, novel conditions imposed on the LKF are first given which are different from the previous ones. New fixed-time (FXT) stability lemmas are established using some inequality techniques which can greatly extend the pioneers. The new estimations of the settling times (STs) are also obtained. For the purpose of examining the applicability of the new FXT stability lemmas, a class of discontinuous neutral-type neural networks (NTNNs) with proportional delays is formulated which is more generalized than the existing ones. Using differential inclusions theory, set-valued map, and the newly obtained FXT stability lemma, some algebraic FXT stabilization criteria are derived. Finally, examples are given to show the correctness of the established results.},
  archive      = {J_TNNLS},
  author       = {Fanchao Kong and Quanxin Zhu},
  doi          = {10.1109/TNNLS.2021.3101252},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {775-785},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fixed-time stabilization of discontinuous neutral neural networks with proportional delays via new fixed-time stability lemmas},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Coarse-to-fine: Progressive knowledge transfer-based
multitask convolutional neural network for intelligent large-scale fault
diagnosis. <em>TNNLS</em>, <em>34</em>(2), 761–774. (<a
href="https://doi.org/10.1109/TNNLS.2021.3100928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern industry, large-scale fault diagnosis of complex systems is emerging and becoming increasingly important. Most deep learning-based methods perform well on small number of fault diagnosis, but cannot converge to satisfactory results when handling large-scale fault diagnosis because the huge number of fault types will lead to the problems of intra/inter-class distance unbalance and poor local minima in neural networks. To address the above problems, a progressive knowledge transfer-based multitask convolutional neural network (PKT-MCNN) is proposed. First, to construct the coarse-to-fine knowledge structure intelligently, a structure learning algorithm is proposed via clustering fault types in different coarse-grained nodes. Thus, the intra/inter-class distance unbalance problem can be mitigated by spreading similar tasks into different nodes. Then, an MCNN architecture is designed to learn the coarse and fine-grained task simultaneously and extract more general fault information, thereby pushing the algorithm away from poor local minima. Last but not least, a PKT algorithm is proposed, which can not only transfer the coarse-grained knowledge to the fine-grained task and further alleviate the intra/inter-class distance unbalance in feature space, but also regulate different learning stages by adjusting the attention weight to each task progressively. To verify the effectiveness of the proposed method, a dataset of a nuclear power system with 66 fault types was collected and analyzed. The results demonstrate that the proposed method can be a promising tool for large-scale fault diagnosis.},
  archive      = {J_TNNLS},
  author       = {Yu Wang and Ruonan Liu and Di Lin and Dongyue Chen and Ping Li and Qinghua Hu and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3100928},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {761-774},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coarse-to-fine: Progressive knowledge transfer-based multitask convolutional neural network for intelligent large-scale fault diagnosis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep convolutional neural network model for improving WRF
simulations. <em>TNNLS</em>, <em>34</em>(2), 750–760. (<a
href="https://doi.org/10.1109/TNNLS.2021.3100902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in numerical weather prediction (NWP) models have accelerated, fostering a more comprehensive understanding of physical phenomena pertaining to the dynamics of weather and related computing resources. Despite these advancements, these models contain inherent biases due to parameterization of the physical processes and discretization of the differential equations that reduce simulation accuracy. In this work, we investigate the use of a computationally efficient deep learning (DL) method, the convolutional neural network (CNN), as a postprocessing technique that improves mesoscale Weather Research and Forecasting (WRF) one-day simulation (with a 1-h temporal resolution) outputs. Using the CNN architecture, we bias-correct several meteorological parameters calculated by the WRF model for all of 2018. We train the CNN model with a four-year history (2014–2017) to investigate the patterns in WRF biases and then reduce these biases in simulations for surface wind speed and direction, precipitation, relative humidity, surface pressure, dewpoint temperature, and surface temperature. The WRF data, with a spatial resolution of 27 km, cover South Korea. We obtain ground observations from the Korean Meteorological Administration station network for 93 weather station locations. The results indicate a noticeable improvement in WRF simulations in all station locations. The average of annual index of agreement for surface wind, precipitation, surface pressure, temperature, dewpoint temperature, and relative humidity of all stations is 0.85 (WRF:0.67), 0.62 (WRF:0.56), 0.91 (WRF:0.69), 0.99 (WRF:0.98), 0.98 (WRF:0.98), and 0.92 (WRF:0.87), respectively. While this study focuses on South Korea, the proposed approach can be applied for any measured weather parameters at any location.},
  archive      = {J_TNNLS},
  author       = {Alqamah Sayeed and Yunsoo Choi and Jia Jung and Yannic Lops and Ebrahim Eslami and Ahmed Khan Salman},
  doi          = {10.1109/TNNLS.2021.3100902},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {750-760},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A deep convolutional neural network model for improving WRF simulations},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mutual information-driven subject-invariant and
class-relevant deep representation learning in BCI. <em>TNNLS</em>,
<em>34</em>(2), 739–749. (<a
href="https://doi.org/10.1109/TNNLS.2021.3100583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning-based feature representation methods have shown a promising impact on electroencephalography (EEG)-based brain–computer interface (BCI). Nonetheless, owing to high intra- and inter-subject variabilities, many studies on decoding EEG were designed in a subject-specific manner by using calibration samples, with no concern of its practical use, hampered by time-consuming steps and a large data requirement. To this end, recent studies adopted a transfer learning strategy, especially domain adaptation techniques. Among those, we have witnessed the potential of adversarial learning-based transfer learning in BCIs. In the meantime, it is known that adversarial learning-based domain adaptation methods are prone to negative transfer that disrupts learning generalized feature representations, applicable to diverse domains, for example, subjects or sessions in BCIs. In this article, we propose a novel framework that learns class-relevant and subject-invariant feature representations in an information-theoretic manner, without using adversarial learning. To be specific, we devise two operational components in a deep network that explicitly estimate mutual information between feature representations: 1) to decompose features in an intermediate layer into class-relevant and class-irrelevant ones and 2) to enrich class-discriminative feature representation. On two large EEG datasets, we validated the effectiveness of our proposed framework by comparing with several comparative methods in performance. Furthermore, we conducted rigorous analyses by performing an ablation study in regard to the components in our network, explaining our model’s decision on input EEG signals via layer-wise relevance propagation, and visualizing the distribution of learned features via t-SNE.},
  archive      = {J_TNNLS},
  author       = {Eunjin Jeon and Wonjun Ko and Jee Seok Yoon and Heung-Il Suk},
  doi          = {10.1109/TNNLS.2021.3100583},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {739-749},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mutual information-driven subject-invariant and class-relevant deep representation learning in BCI},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive event-triggered neural network control for
switching nonlinear systems with time delays. <em>TNNLS</em>,
<em>34</em>(2), 729–738. (<a
href="https://doi.org/10.1109/TNNLS.2021.3100533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive event-triggered-based neural network control is explored for switching nonlinear systems with nonstrict-feedback structure and time-varying delays in this article. First, the switching observer is designed to estimate the unmeasurable states. Due to the existence of time-varying input delay, a compensation system is introduced. The average dwell-time (ADT) scheme and the event-triggered controller are established. Furthermore, the semiglobal uniform ultimate boundedness (SGUUB) of all the variables in the closed-loop system is achieved and the Zeno behavior is avoided. Finally, the numerical simulation shows that our proposed control approach is effective.},
  archive      = {J_TNNLS},
  author       = {Yingkang Xie and Qian Ma},
  doi          = {10.1109/TNNLS.2021.3100533},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {729-738},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive event-triggered neural network control for switching nonlinear systems with time delays},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Markovian RNN: An adaptive time series prediction network
with HMM-based switching for nonstationary environments. <em>TNNLS</em>,
<em>34</em>(2), 715–728. (<a
href="https://doi.org/10.1109/TNNLS.2021.3100528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate nonlinear regression for nonstationary sequential data. In most real-life applications such as business domains including finance, retail, energy, and economy, time series data exhibit nonstationarity due to the temporally varying dynamics of the underlying system. We introduce a novel recurrent neural network (RNN) architecture, which adaptively switches between internal regimes in a Markovian way to model the nonstationary nature of the given data. Our model, Markovian RNN employs a hidden Markov model (HMM) for regime transitions, where each regime controls hidden state transitions of the recurrent cell independently. We jointly optimize the whole network in an end-to-end fashion. We demonstrate the significant performance gains compared to conventional methods such as Markov Switching ARIMA, RNN variants and recent statistical and deep learning-based methods through an extensive set of experiments with synthetic and real-life datasets. We also interpret the inferred parameters and regime belief values to analyze the underlying dynamics of the given sequences.},
  archive      = {J_TNNLS},
  author       = {Fatih Ilhan and Oguzhan Karaahmetoglu and Ismail Balaban and Suleyman Serdar Kozat},
  doi          = {10.1109/TNNLS.2021.3100528},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {715-728},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Markovian RNN: An adaptive time series prediction network with HMM-based switching for nonstationary environments},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Full information control for switched neural networks
subject to fault and disturbance. <em>TNNLS</em>, <em>34</em>(2),
703–714. (<a href="https://doi.org/10.1109/TNNLS.2021.3100143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article investigates full information control problem for switched neural networks subject to fault and disturbance. First, the main objective is realizing interval stability and zero tracking error under condition that neither of the neuron states’ vectors including the plant and reference models is available. Second, the desired full information controller and neural networks’ observer are designed to ensure observer-based dynamic error system mean-square exponentially stable with sufficient condition of strict weight $\mathcal {H}_{\infty } /\mathcal {H}_{-}$ performance levels. Finally, we concentrate on stability analyses and fault tolerance for switched neural networks with fault accompanied by disturbance through linear matrix inequalities (LMIs), Lyapunov function, and average dwell time, discussing it according to different values of fault. Finally, simulation examples are listed to account for the availability and effectiveness of the research methodology.},
  archive      = {J_TNNLS},
  author       = {Jiayue Sun and Huaguang Zhang and Shun Xu and Yang Liu},
  doi          = {10.1109/TNNLS.2021.3100143},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {703-714},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Full information control for switched neural networks subject to fault and disturbance},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multistability of dynamic memristor delayed cellular neural
networks with application to associative memories. <em>TNNLS</em>,
<em>34</em>(2), 690–702. (<a
href="https://doi.org/10.1109/TNNLS.2021.3099814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, dynamic memristor (DM)-cellular neural networks (CNNs) have received widespread attention due to their advantage of low power consumption. The previous works showed that DM-CNNs have at most 318 equilibrium points (EPs) with $n=16$ cells. Since time delay is unavoidable during the process of information transmission, the goal of this article is to research the multistability of DM-CNNs with time delay, and, meanwhile, to increase the storage capacity of DM-delay (D)CNNs. Depending on the different constitutive relations of memristors, two cases of the multistability for DM-DCNNs are discussed. After determining the constitutive relations, the number of EPs of DM-DCNNs is increased to $3^{n}$ with $n$ cells by means of the appropriate state-space decomposition and the Brouwer’s fixed point theorem. Furthermore, the enlarged attraction domains of EPs can be obtained, and $2^{n}$ of these EPs are locally exponentially stable in two cases. Compared with standard CNNs, the dynamic behavior of DM-DCNNs shows an outstanding merit. That is, the value of voltage and current approach to zero when the system becomes stable, and the memristor provides a nonvolatile memory to store the computation results. Finally, two numerical simulations are presented to illustrate the effectiveness of the theoretical results, and the applications of associative memories are shown at the end of this article.},
  archive      = {J_TNNLS},
  author       = {Kun Deng and Song Zhu and Gang Bao and Jun Fu and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2021.3099814},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {690-702},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multistability of dynamic memristor delayed cellular neural networks with application to associative memories},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the guaranteed almost equivalence between imitation
learning from observation and demonstration. <em>TNNLS</em>,
<em>34</em>(2), 677–689. (<a
href="https://doi.org/10.1109/TNNLS.2021.3099621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imitation learning from observation (LfO) is more preferable than imitation learning from demonstration (LfD) because of the nonnecessity of expert actions when reconstructing the expert policy from the expert data. However, previous studies imply that the performance of LfO is inferior to LfD by a tremendous gap, which makes it challenging to employ LfO in practice. By contrast, this article proves that LfO is almost equivalent to LfD in the deterministic robot environment, and more generally even in the robot environment with bounded randomness. In the deterministic robot environment, from the perspective of the control theory, we show that the inverse dynamics disagreement between LfO and LfD approaches zero, meaning that LfO is almost equivalent to LfD. To further relax the deterministic constraint and better adapt to the practical environment, we consider bounded randomness in the robot environment and prove that the optimizing targets for both LfD and LfO remain almost the same in the more generalized setting. Extensive experiments for multiple robot tasks are conducted to demonstrate that LfO achieves comparable performance to LfD empirically. In fact, the most common robot systems in reality are the robot environment with bounded randomness (i.e., the environment this article considered). Hence, our findings greatly extend the potential of LfO and suggest that we can safely apply LfO in practice without sacrificing the performance compared to LfD.},
  archive      = {J_TNNLS},
  author       = {Zhihao Cheng and Liu Liu and Aishan Liu and Hao Sun and Meng Fang and Dacheng Tao},
  doi          = {10.1109/TNNLS.2021.3099621},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {677-689},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the guaranteed almost equivalence between imitation learning from observation and demonstration},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conservative novelty synthesizing network for malware
recognition in an open-set scenario. <em>TNNLS</em>, <em>34</em>(2),
662–676. (<a href="https://doi.org/10.1109/TNNLS.2021.3099122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the challenging task of malware recognition on both known and novel unknown malware families, called malware open-set recognition (MOSR). Previous works usually assume the malware families are known to the classifier in a close-set scenario, i.e., testing families are the subset or at most identical to training families. However, novel unknown malware families frequently emerge in real-world applications, and as such, require recognizing malware instances in an open-set scenario, i.e., some unknown families are also included in the test set, which has been rarely and nonthoroughly investigated in the cyber-security domain. One practical solution for MOSR may consider jointly classifying known and detecting unknown malware families by a single classifier (e.g., neural network) from the variance of the predicted probability distribution on known families. However, conventional well-trained classifiers usually tend to obtain overly high recognition probabilities in the outputs, especially when the instance feature distributions are similar to each other, e.g., unknown versus known malware families, and thus, dramatically degrade the recognition on novel unknown malware families. To address the problem and construct an applicable MOSR system, we propose a novel model that can conservatively synthesize malware instances to mimic unknown malware families and support a more robust training of the classifier. More specifically, we build upon the generative adversarial networks to explore and obtain marginal malware instances that are close to known families while falling into mimical unknown ones to guide the classifier to lower and flatten the recognition probabilities of unknown families and relatively raise that of known ones to rectify the performance of classification and detection. A cooperative training scheme involving the classification, synthesizing and rectification are further constructed to facilitate the training and jointly improve the model performance. Moreover, we also build a new large-scale malware dataset, named MAL-100, to fill the gap of lacking a large open-set malware benchmark dataset. Experimental results on two widely used malware datasets and our MAL-100 demonstrate the effectiveness of our model compared with other representative methods.},
  archive      = {J_TNNLS},
  author       = {Jingcai Guo and Song Guo and Shiheng Ma and Yuxia Sun and Yuanyuan Xu},
  doi          = {10.1109/TNNLS.2021.3099122},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {662-676},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Conservative novelty synthesizing network for malware recognition in an open-set scenario},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Solving two-person zero-sum stochastic games with incomplete
information using learning automata with artificial barriers.
<em>TNNLS</em>, <em>34</em>(2), 650–661. (<a
href="https://doi.org/10.1109/TNNLS.2021.3099095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning automata (LA) with artificially absorbing barriers was a completely new horizon of research in the 1980s (Oommen, 1986). These new machines yielded properties that were previously unknown. More recently, absorbing barriers have been introduced in continuous estimator algorithms so that the proofs could follow a martingale property, as opposed to monotonicity (Zhang et al., 2014), (Zhang et al., 2015). However, the applications of LA with artificial barriers are almost nonexistent. In that regard, this article is pioneering in that it provides effective and accurate solutions to an extremely complex application domain, namely that of solving two-person zero-sum stochastic games that are provided with incomplete information. LA have been previously used (Sastry et al., 1994) to design algorithms capable of converging to the game’s Nash equilibrium under limited information. Those algorithms have focused on the case where the saddle point of the game exists in a pure strategy. However, the majority of the LA algorithms used for games are absorbing in the probability simplex space, and thus, they converge to an exclusive choice of a single action. These LA are thus unable to converge to other mixed Nash equilibria when the game possesses no saddle point for a pure strategy. The pioneering contribution of this article is that we propose an LA solution that is able to converge to an optimal mixed Nash equilibrium even though there may be no saddle point when a pure strategy is invoked. The scheme, being of the linear reward-inaction ( $L_{R-I}$ ) paradigm, is in and of itself, absorbing. However, by incorporating artificial barriers, we prevent it from being “stuck” or getting absorbed in pure strategies. Unlike the linear reward- $\epsilon $ penalty ( $L_{R-\epsilon P}$ ) scheme proposed by Lakshmivarahan and Narendra almost four decades ago, our new scheme achieves the same goal with much less parameter tuning and in a more elegant manner. This article includes the nontrial proofs of the theoretical results characterizing our scheme and also contains experimental verification that confirms our theoretical findings.},
  archive      = {J_TNNLS},
  author       = {Anis Yazidi and Daniel Silvestre and B. John Oommen},
  doi          = {10.1109/TNNLS.2021.3099095},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {650-661},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Solving two-person zero-sum stochastic games with incomplete information using learning automata with artificial barriers},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-free λ-policy iteration for discrete-time linear
quadratic regulation. <em>TNNLS</em>, <em>34</em>(2), 635–649. (<a
href="https://doi.org/10.1109/TNNLS.2021.3098985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a model-free $\lambda $ -policy iteration ( $\lambda $ -PI) for the discrete-time linear quadratic regulation (LQR) problem. To solve the algebraic Riccati equation arising from solving the LQR in an iterative manner, we define two novel matrix operators, named the weighted Bellman operator and the composite Bellman operator. Then, the $\lambda $ -PI algorithm is first designed as a recursion with the weighted Bellman operator, and its equivalent formulation as a fixed-point iteration with the composite Bellman operator is shown. The contraction and monotonic properties of the composite Bellman operator guarantee the convergence of the $\lambda $ -PI algorithm. In contrast to the PI algorithm, the $\lambda $ -PI does not require an admissible initial policy, and the convergence rate outperforms the value iteration (VI) algorithm. Model-free extension of the $\lambda $ -PI algorithm is developed using the off-policy reinforcement learning technique. It is also shown that the off-policy variants of the $\lambda $ -PI algorithm are robust against the probing noise. Finally, simulation examples are conducted to validate the efficacy of the $\lambda $ -PI algorithm.},
  archive      = {J_TNNLS},
  author       = {Yongliang Yang and Bahare Kiumarsi and Hamidreza Modares and Chengzhong Xu},
  doi          = {10.1109/TNNLS.2021.3098985},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {635-649},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-free λ-policy iteration for discrete-time linear quadratic regulation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Deep hybrid 2-d–3-d CNN based on dual second-order
attention with camera spectral sensitivity prior for spectral
super-resolution. <em>TNNLS</em>, <em>34</em>(2), 623–634. (<a
href="https://doi.org/10.1109/TNNLS.2021.3098767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A largely ignored fact in spectral super-resolution (SSR) is that the subsistent mapping methods neglect the auxiliary prior of camera spectral sensitivity (CSS) and only pay attention to wider or deeper network framework design while ignoring to excavate the spatial and spectral dependencies among intermediate layers, hence constraining representational capability of convolutional neural networks (CNNs). To conquer these drawbacks, we propose a novel deep hybrid 2-D–3-D CNN based on dual second-order attention with CSS prior (HSACS), which can excavate sufficient spatial–spectral context information. Specifically, dual second-order attention embedded in the residual block for more powerful spatial–spectral feature representation and relation learning is composed of a brand new trainable 2-D second-order channel attention (SCA) or 3-D second-order band attention (SBA) and a structure tensor attention (STA). Concretely, the band and channel attention modules are developed to adaptively recalibrate the band-wise and interchannel features via employing second-order band or channel feature statistics for more discriminative representations. Besides, the STA is promoted to rebuild the significant high-frequency spatial details for enough spatial feature extraction. Moreover, the CSS is first employed as a superior prior to avoid its effect of SSR quality, on the strength of which the resolved RGB can be calculated naturally through the super-reconstructed hyperspectral image (HSI); then, the final loss consists of the discrepancies of RGB and the HSI as a finer constraint. Experimental results demonstrate the superiority and progressiveness of the presented approach in terms of quantitative metrics and visual effect over SOTA SSR methods.},
  archive      = {J_TNNLS},
  author       = {Jiaojiao Li and Chaoxiong Wu and Rui Song and Yunsong Li and Weiying Xie and Lihuo He and Xinbo Gao},
  doi          = {10.1109/TNNLS.2021.3098767},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {623-634},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep hybrid 2-d–3-D CNN based on dual second-order attention with camera spectral sensitivity prior for spectral super-resolution},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Stochastically controlled compositional gradient for
composition problems. <em>TNNLS</em>, <em>34</em>(2), 611–622. (<a
href="https://doi.org/10.1109/TNNLS.2021.3098222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider composition problems of the form $(1/n)\sum _{i= 1}^{n} F_{i} (1/m)\sum _{j = 1}^{m} G_{j}(x)$ , which are important for machine learning. Although gradient descent and stochastic gradient descent are straightforward solutions, the essential computation of $G (x)= (1/m)\sum _{j = 1}^{m}{G_{j}(x)}$ in each single iteration is expensive, let alone for large $m$ . In this article, we devise a stochastically controlled compositional gradient algorithm. Specifically, we introduce two variants of stochastically controlled technique to estimate the inner function $G(x)$ and the gradient of the objective function, respectively. The computational cost is largely reduced. However, the natural needs of two stochastic subsets ${\mathcal D}_{1}$ and ${\mathcal D}_{2}$ form direct barriers to guarantee the convergence of the algorithm, especially the theoretical proof of the convergence. To this end, we present a general convergence analysis by proving $|{\mathcal{ D}}_{1}|=\min {1/\epsilon,m}$ and $|{\mathcal{ D}}_{2}|=\min {1/\epsilon,n }$ , through which the proposed method significantly improve composition algorithms under low target accuracy (i.e., $1/\epsilon \ll m$ or $n$ ) in both strongly convex and nonconvex settings. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.},
  archive      = {J_TNNLS},
  author       = {Liu Liu and Ji Liu and Cho-Jui Hsieh and Dacheng Tao},
  doi          = {10.1109/TNNLS.2021.3098222},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {611-622},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stochastically controlled compositional gradient for composition problems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive synchronization for delayed chaotic memristor-based
neural networks. <em>TNNLS</em>, <em>34</em>(2), 601–610. (<a
href="https://doi.org/10.1109/TNNLS.2021.3096963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the adaptive synchronization problem of delayed chaotic memristor-based neural networks (MNNs). Note that MNNs are modeled as continuous systems in the flux-voltage-time $(\phi,x,t)$ domain where memristors are viewed as continuous systems based on HP memristors. New adaptive controllers of MNNs are proposed, where controllers are both on memristors in the flux-time $(\phi,t)$ domain and neurons in the voltage-time $(x,t)$ domain. Based on the Lyapunov method, Barbalat’s lemma, differential mean value Theorem, and other inequality techniques, completed synchronization criteria for delayed chaotic MNNs are derived. In the end, two examples are given to demonstrate the validity of the derived results.},
  archive      = {J_TNNLS},
  author       = {Youming Xin and Zunshui Cheng},
  doi          = {10.1109/TNNLS.2021.3096963},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {601-610},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive synchronization for delayed chaotic memristor-based neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nearest neighbor-based strategy to optimize multi-view
triplet network for classification of small-sample medical imaging data.
<em>TNNLS</em>, <em>34</em>(2), 586–600. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view classification with limited sample size and data augmentation is a very common machine learning (ML) problem in medicine. With limited data, a triplet network approach for two-stage representation learning has been proposed. However, effective training and verifying the features from the representation network for their suitability in subsequent classifiers are still unsolved problems. Although typical distance-based metrics for the training capture the overall class separability of the features, the performance according to these metrics does not always lead to an optimal classification. Consequently, an exhaustive tuning with all feature–classifier combinations is required to search for the best end result. To overcome this challenge, we developed a novel nearest-neighbor (NN) validation strategy based on the triplet metric. This strategy is supported by a theoretical foundation to provide the best selection of the features with a lower bound of the highest end performance. The proposed strategy is a transparent approach to identify whether to improve the features or the classifier. This avoids the need for repeated tuning. Our evaluations on real-world medical imaging tasks (i.e., radiation therapy delivery error prediction and sarcoma survival prediction) show that our strategy is superior to other common deep representation learning baselines [i.e., autoencoder (AE) and softmax]. The strategy addresses the issue of feature’s interpretability which enables more holistic feature creation such that the medical experts can focus on specifying relevant data as opposed to tedious feature engineering.},
  archive      = {J_TNNLS},
  author       = {Phawis Thammasorn and Wanpracha A. Chaovalitwongse and Daniel S. Hippe and Landon S. Wootton and Eric C. Ford and Matthew B. Spraker and Stephanie E. Combs and Jan C. Peeken and Matthew J. Nyflot},
  doi          = {10.1109/TNNLS.2021.3059635},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {586-600},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nearest neighbor-based strategy to optimize multi-view triplet network for classification of small-sample medical imaging data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward better structure and constraint to mine negative
sequential patterns. <em>TNNLS</em>, <em>34</em>(2), 571–585. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonoccurring behavior (NOB) studies have attracted the growing attention of scholars as a crucial part of behavioral science. As an effective method to discover both NOB and occurring behaviors (OB), negative sequential pattern (NSP) mining is successfully used in analyzing medical treatment and abnormal behavior patterns. At this time, NSP mining is still an active and challenging research domain. Most of the algorithms are inefficient in practice. Briefly, the key weaknesses of NSP mining are: 1) an inefficient positive sequential pattern (PSP) mining process, 2) a strict constraint of negative containment, and 3) the lack of an effective Negative Sequential Candidate (NSC) generation method. To address these weaknesses, we propose a highly efficient algorithm with improved techniques, named sc-NSP, to mine NSP efficiently. We first propose an improved PrefixSpan algorithm in the PSP mining process, which connects to a bitmap storage structure instead of the original structure. Second, sc-NSP loosens the frequency constraint and exploits the NSC generation method of positive and negative sequential patterns mining (PNSP) (a classic NSP mining method). Furthermore, a novel pruning strategy is designed to reduce the computational complexity of sc-NSP. Finally, sc-NSP obtains the support of NSC by using the most efficient bitwise-based calculation operation. Theoretical analyses show that sc-NSP performs particularly well on data sets with a large number of elements and items in sequence. Comparison and extensive experiments along with case studies on health data show that sc-NSP is 10 times more efficient than other state-of-the-art methods, and the number of NSPs obtained is 5 times greater than other methods.},
  archive      = {J_TNNLS},
  author       = {Xinming Gao and Yongshun Gong and TianTian Xu and Jinhu Lü and Yuhai Zhao and Xiangjun Dong},
  doi          = {10.1109/TNNLS.2020.3041732},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {571-585},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward better structure and constraint to mine negative sequential patterns},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). A survey on evolutionary neural architecture search.
<em>TNNLS</em>, <em>34</em>(2), 550–570. (<a
href="https://doi.org/10.1109/TNNLS.2021.3100554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved great success in many applications. The architectures of DNNs play a crucial role in their performance, which is usually manually designed with rich expertise. However, such a design process is labor-intensive because of the trial-and-error process and also not easy to realize due to the rare expertise in practice. Neural architecture search (NAS) is a type of technology that can design the architectures automatically. Among different methods to realize NAS, the evolutionary computation (EC) methods have recently gained much attention and success. Unfortunately, there has not yet been a comprehensive summary of the EC-based NAS algorithms. This article reviews over 200 articles of most recent EC-based NAS methods in light of the core components, to systematically discuss their design principles and justifications on the design. Furthermore, current challenges and issues are also discussed to identify future research in this emerging field.},
  archive      = {J_TNNLS},
  author       = {Yuqiao Liu and Yanan Sun and Bing Xue and Mengjie Zhang and Gary G. Yen and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2021.3100554},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {550-570},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey on evolutionary neural architecture search},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast algorithms for deep octonion networks. <em>TNNLS</em>,
<em>34</em>(1), 543–548. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief presents the results of a study of the possibilities of reducing the arithmetic complexity of computing basic operations in octonionic neural networks and also proposes new algorithmic solutions for efficiently performing these operations. Here, we primarily mean the operation of multiplying octonions, the operation of computing the dot product of two octonion-valued vectors, and the operation of multiple multiplications of an octonion by several other octonions. In order to reduce the computational complexity of these operations, it is proposed to use the fast Walsh–Hadamard transform, which is well known in digital signal processing. Using this transform reduces the number of multiplications and additions of real numbers required to perform computations. Thus, the use of the proposed algorithms will speed up computations in octonion-valued neural networks.},
  archive      = {J_TNNLS},
  author       = {Aleksandr Cariow and Galina Cariowa},
  doi          = {10.1109/TNNLS.2021.3124131},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {543-548},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast algorithms for deep octonion networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An RNN-based algorithm for decentralized-partial-consensus
constrained optimization. <em>TNNLS</em>, <em>34</em>(1), 534–542. (<a
href="https://doi.org/10.1109/TNNLS.2021.3098668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This technical note proposes a decentralized-partial-consensus optimization (DPCO) problem with inequality constraints. The partial-consensus matrix originating from the Laplacian matrix is constructed to tackle the partial-consensus constraints. A continuous-time algorithm based on multiple interconnected recurrent neural networks (RNNs) is derived to solve the optimization problem. In addition, based on nonsmooth analysis and Lyapunov theory, the convergence of continuous-time algorithm is further proved. Finally, several examples demonstrate the effectiveness of main results.},
  archive      = {J_TNNLS},
  author       = {Zicong Xia and Yang Liu and Jianlong Qiu and Qihua Ruan and Jinde Cao},
  doi          = {10.1109/TNNLS.2021.3098668},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {534-542},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An RNN-based algorithm for decentralized-partial-consensus constrained optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural networks as geometric chaotic maps. <em>TNNLS</em>,
<em>34</em>(1), 527–533. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of artificial neural networks (NNs) as models of chaotic dynamics has been rapidly expanding. Still, a theoretical understanding of how NNs learn chaos is lacking. Here, we employ a geometric perspective to show that NNs can efficiently model chaotic dynamics by becoming structurally chaotic themselves. We first confirm NN’s efficiency in emulating chaos by showing that a parsimonious NN trained only on few data points can reconstruct strange attractors, extrapolate outside training data boundaries, and accurately predict local divergence rates. We then posit that the trained network’s map comprises sequential geometric stretching, rotation, and compression operations. These geometric operations indicate topological mixing and chaos, explaining why NNs are naturally suitable to emulate chaotic dynamics.},
  archive      = {J_TNNLS},
  author       = {Ziwei Li and Sai Ravela},
  doi          = {10.1109/TNNLS.2021.3087497},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {527-533},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural networks as geometric chaotic maps},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep multiview collaborative clustering. <em>TNNLS</em>,
<em>34</em>(1), 516–526. (<a
href="https://doi.org/10.1109/TNNLS.2021.3097748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The clustering methods have absorbed even-increasing attention in machine learning and computer vision communities in recent years. In this article, we focus on the real-world applications where a sample can be represented by multiple views. Traditional methods learn a common latent space for multiview samples without considering the diversity of multiview representations and use $K$ -means to obtain the final results, which are time and space consuming. On the contrary, we propose a novel end-to-end deep multiview clustering model with collaborative learning to predict the clustering results directly. Specifically, multiple autoencoder networks are utilized to embed multi-view data into various latent spaces and a heterogeneous graph learning module is employed to fuse the latent representations adaptively, which can learn specific weights for different views of each sample. In addition, intraview collaborative learning is framed to optimize each single-view clustering task and provide more discriminative latent representations. Simultaneously, interview collaborative learning is employed to obtain complementary information and promote consistent cluster structure for a better clustering solution. Experimental results on several datasets show that our method significantly outperforms several state-of-the-art clustering approaches.},
  archive      = {J_TNNLS},
  author       = {Xu Yang and Cheng Deng and Zhiyuan Dang and Dacheng Tao},
  doi          = {10.1109/TNNLS.2021.3097748},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {516-526},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep multiview collaborative clustering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust visual tracking via multitask sparse correlation
filters learning. <em>TNNLS</em>, <em>34</em>(1), 502–515. (<a
href="https://doi.org/10.1109/TNNLS.2021.3097498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel multitask sparse correlation filters (MTSCF) model, which introduces multitask sparse learning into the CFs framework, is proposed for visual tracking. Specifically, the proposed MTSCF method exploits multitask learning to take the interdependencies among different visual features (e.g., histogram of oriented gradient (HOG), color names, and CNN features) into account to simultaneously learn the CFs and make the learned filters enhance and complement each other to boost the tracking performance. Moreover, it also performs feature selection to dynamically select discriminative spatial features from the target region to distinguish the target object from the background. A $l_{2,1}$ regularization term is considered to realize multitask sparse learning. In order to solve the objective model, alternating direction method of multipliers is utilized for learning the CFs. By considering multitask sparse learning, the proposed MTSCF model can fully utilize the strength of different visual features and select effective spatial features to better model the appearance of the target object. Extensive experiment results on multiple tracking benchmarks demonstrate that our MTSCF tracker achieves competitive tracking performance in comparison with several state-of-the-art trackers.},
  archive      = {J_TNNLS},
  author       = {Ke Nai and Zhiyong Li and Yihui Gan and Qi Wang},
  doi          = {10.1109/TNNLS.2021.3097498},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {502-515},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust visual tracking via multitask sparse correlation filters learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernel path for ν-support vector classification.
<em>TNNLS</em>, <em>34</em>(1), 490–501. (<a
href="https://doi.org/10.1109/TNNLS.2021.3097248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that the performance of a kernel method is highly dependent on the choice of kernel parameter. However, existing kernel path algorithms are limited to plain support vector machines (SVMs), which has one equality constraint. It is still an open question to provide a kernel path algorithm to $\nu $ -support vector classification ( $\nu $ -SVC) with more than one equality constraint. Compared with plain SVM, $\nu $ -SVC has the advantage of using a regularization parameter $\nu $ for controlling the number of support vectors and margin errors. To address this problem, in this article, we propose a kernel path algorithm (KP $\nu $ SVC) to trace the solutions of $\nu $ -SVC exactly with respect to the kernel parameter. Specifically, we first provide an equivalent formulation of $\nu $ -SVC with two equality constraints, which can avoid possible conflicts during tracing the solutions of $\nu $ -SVC. Based on this equivalent formulation of $\nu $ -SVC, we propose the KP $\nu $ SVC algorithm to trace the solutions with respect to the kernel parameter. However, KP $\nu $ SVC traces nonlinear solutions of kernel method rather than the errors of loss function, and it is still a challenge to provide the algorithm that guarantees to find the global optimal model. To address this challenging problem, we extend the classical error path algorithm to the nonlinear kernel solution paths and propose a new kernel error path (KEP) algorithm that ensures to find the global optimal kernel parameter by minimizing the cross validation error. We also provide the finite convergence analysis and computational complexity analysis to KP $\nu $ SVC and KEP. Extensive experimental results on a variety of benchmark datasets not only verify the effectiveness of KP $\nu $ SVC but also show the advantage of applying KEP to select the optimal kernel parameter.},
  archive      = {J_TNNLS},
  author       = {Bin Gu and Ziran Xiong and Xiang Li and Zhou Zhai and Guansheng Zheng},
  doi          = {10.1109/TNNLS.2021.3097248},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {490-501},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Kernel path for ν-support vector classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RelativeNAS: Relative neural architecture search via
slow-fast learning. <em>TNNLS</em>, <em>34</em>(1), 475–489. (<a
href="https://doi.org/10.1109/TNNLS.2021.3096658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable successes of convolutional neural networks (CNNs) in computer vision, it is time-consuming and error-prone to manually design a CNN. Among various neural architecture search (NAS) methods that are motivated to automate designs of high-performance CNNs, the differentiable NAS and population-based NAS are attracting increasing interests due to their unique characters. To benefit from the merits while overcoming the deficiencies of both, this work proposes a novel NAS method, RelativeNAS. As the key to efficient search, RelativeNAS performs joint learning between fast learners (i.e., decoded networks with relatively lower loss value) and slow learners in a pairwise manner. Moreover, since RelativeNAS only requires low-fidelity performance estimation to distinguish each pair of fast learner and slow learner, it saves certain computation costs for training the candidate architectures. The proposed RelativeNAS brings several unique advantages: 1) it achieves state-of-the-art performances on ImageNet with top-1 error rate of 24.88\%, that is, outperforming DARTS and AmoebaNet-B by 1.82\% and 1.12\%, respectively; 2) it spends only 9 h with a single 1080Ti GPU to obtain the discovered cells, that is, ${3.75\times }$ and ${7875\times }$ faster than DARTS and AmoebaNet, respectively; and 3) it provides that the discovered cells obtained on CIFAR-10 can be directly transferred to object detection, semantic segmentation, and keypoint detection, yielding competitive results of 73.1\% mAP on PASCAL VOC, 78.7\% mIoU on Cityscapes, and 68.5\% AP on MSCOCO, respectively. The implementation of RelativeNAS is available at https://github.com/EMI-Group/RelativeNAS .},
  archive      = {J_TNNLS},
  author       = {Hao Tan and Ran Cheng and Shihua Huang and Cheng He and Changxiao Qiu and Fan Yang and Ping Luo},
  doi          = {10.1109/TNNLS.2021.3096658},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {475-489},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RelativeNAS: Relative neural architecture search via slow-fast learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lifelong mixture of variational autoencoders.
<em>TNNLS</em>, <em>34</em>(1), 461–474. (<a
href="https://doi.org/10.1109/TNNLS.2021.3096457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose an end-to-end lifelong learning mixture of experts. Each expert is implemented by a variational autoencoder (VAE). The experts in the mixture system are jointly trained by maximizing a mixture of individual component evidence lower bounds (MELBO) on the log-likelihood of the given training samples. The mixing coefficients in the mixture model control the contributions of each expert in the global representation. These are sampled from a Dirichlet distribution whose parameters are determined through nonparametric estimation during lifelong learning. The model can learn new tasks fast when these are similar to those previously learned. The proposed lifelong mixture of VAE (L-MVAE) expands its architecture with new components when learning a completely new task. After the training, our model can automatically determine the relevant expert to be used when fed with new data samples. This mechanism benefits both the memory efficiency and the required computational cost as only one expert is used during the inference. The L-MVAE inference model is able to perform interpolations in the joint latent space across the data domains associated with different tasks and is shown to be efficient for disentangled learning representation.},
  archive      = {J_TNNLS},
  author       = {Fei Ye and Adrian G. Bors},
  doi          = {10.1109/TNNLS.2021.3096457},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {461-474},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Lifelong mixture of variational autoencoders},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tandem learning rule for effective training and rapid
inference of deep spiking neural networks. <em>TNNLS</em>,
<em>34</em>(1), 446–460. (<a
href="https://doi.org/10.1109/TNNLS.2021.3095724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) represent the most prominent biologically inspired computing model for neuromorphic computing (NC) architectures. However, due to the nondifferentiable nature of spiking neuronal functions, the standard error backpropagation algorithm is not directly applicable to SNNs. In this work, we propose a tandem learning framework that consists of an SNN and an artificial neural network (ANN) coupled through weight sharing. The ANN is an auxiliary structure that facilitates the error backpropagation for the training of the SNN at the spike-train level. To this end, we consider the spike count as the discrete neural representation in the SNN and design an ANN neuronal activation function that can effectively approximate the spike count of the coupled SNN. The proposed tandem learning rule demonstrates competitive pattern recognition and regression capabilities on both the conventional frame- and event-based vision datasets, with at least an order of magnitude reduced inference time and total synaptic operations over other state-of-the-art SNN implementations. Therefore, the proposed tandem learning rule offers a novel solution to training efficient, low latency, and high-accuracy deep SNNs with low computing resources.},
  archive      = {J_TNNLS},
  author       = {Jibin Wu and Yansong Chua and Malu Zhang and Guoqi Li and Haizhou Li and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2021.3095724},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {446-460},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A tandem learning rule for effective training and rapid inference of deep spiking neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ECBC: Efficient convolution via blocked columnizing.
<em>TNNLS</em>, <em>34</em>(1), 433–445. (<a
href="https://doi.org/10.1109/TNNLS.2021.3095276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct convolution methods are now drawing increasing attention as they eliminate the additional storage demand required by indirect convolution algorithms (i.e., the transformed matrix generated by the im2col convolution algorithm). Nevertheless, the direct methods require special input–output tensor formatting, leading to extra time and memory consumption to get the desired data layout. In this article, we show that indirect convolution, if implemented properly, is able to achieve high computation performance with the help of highly optimized subroutines in matrix multiplication while avoid incurring substantial memory overhead. The proposed algorithm is called efficient convolution via blocked columnizing (ECBC). Inspired by the im2col convolution algorithm and the block algorithm of general matrix-to-matrix multiplication, we propose to conduct the convolution computation blockwisely. As a result, the tensor-to-matrix transformation process (e.g., the im2col operation) can also be done in a blockwise manner so that it only requires a small block of memory as small as the data block. Extensive experiments on various platforms and networks validate the effectiveness of ECBC, as well as the superiority of our proposed method against a set of widely used industrial-level convolution algorithms.},
  archive      = {J_TNNLS},
  author       = {Tianli Zhao and Qinghao Hu and Xiangyu He and Weixiang Xu and Jiaxing Wang and Cong Leng and Jian Cheng},
  doi          = {10.1109/TNNLS.2021.3095276},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {433-445},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ECBC: Efficient convolution via blocked columnizing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical passivity criterion for delayed neural networks
via a general delay-product-type lyapunov–krasovskii functional.
<em>TNNLS</em>, <em>34</em>(1), 421–432. (<a
href="https://doi.org/10.1109/TNNLS.2021.3095183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with passivity analysis of neural networks with a time-varying delay. Several techniques in the domain are improved to establish the new passivity criterion with less conservatism. First, a Lyapunov–Krasovskii functional (LKF) is constructed with two general delay-product-type terms which contain any chosen degree of polynomials in time-varying delay. Second, a general convexity lemma without conservatism is developed to address the positive-definiteness of the LKF and the negative-definiteness of its time-derivative. Then, with these improved results, a hierarchical passivity criterion of less conservatism is obtained for neural networks with a time-varying delay, whose size and conservatism vary with the maximal degree of the time-varying delay polynomial in the LKF. It is shown that the conservatism of the passivity criterion does not always reduce as the degree of the time-varying delay polynomial increases. Finally, a numerical example is given to illustrate the proposed criterion and benchmark against the existing results.},
  archive      = {J_TNNLS},
  author       = {Fei Long and Chuan-Ke Zhang and Yong He and Qing-Guo Wang and Zhen-Man Gao and Min Wu},
  doi          = {10.1109/TNNLS.2021.3095183},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {421-432},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical passivity criterion for delayed neural networks via a general delay-product-type Lyapunov–Krasovskii functional},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel neural approach to infinity-norm joint-velocity
minimization of kinematically redundant robots under joint limits.
<em>TNNLS</em>, <em>34</em>(1), 409–420. (<a
href="https://doi.org/10.1109/TNNLS.2021.3095122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, the infinity-norm joint-velocity minimization (INVM) of physically constrained kinematically redundant robots can be formulated as time-variant linear programming (TVLP) with equality and inequality constraints. Zeroing neural network (ZNN) is an effective neural method for solving equality-constrained TVLP. For inequality-constrained TVLP, however, existing ZNNs become incompetent due to the lack of relevant derivative information and the inability to handle inequality constraints. Currently, there is no capable ZNN in the literature that has achieved the INVM of redundant robots under joint limits. To fill this gap, a classical INVM scheme is first introduced in this article. Then, a new joint-limit handling technique is proposed and employed to convert the INVM scheme into a unified TVLP with full derivative information. By using a perturbed Fisher–Burmeister function, the TVLP is further converted into a nonlinear equation. These conversion techniques lay a foundation for the success of designing a capable ZNN. To solve the nonlinear equation and the TVLP, a novel continuous-time ZNN (CTZNN) is designed and its corresponding discrete-time ZNN (DTZNN) is established using an extrapolated backward differentiation formula. Theoretical analysis is rigorously conducted to prove the convergence of the neural approach. Numerical studies are performed by comparing the DTZNN solver and the state-of-the-art (SOTA) linear programming (LP) solvers. Comparative results show that the DTZNN consumes the least computing time and can be a powerful alternative to the SOTA solvers. The DTZNN and the INVM scheme are finally applied to control two kinematically redundant robots. Both simulative and experimental results show that the robots successfully accomplish user-specified path-tracking tasks, verifying the effectiveness and practicability of the proposed neural approach and the INVM scheme equipped with the new joint-limit handling technique.},
  archive      = {J_TNNLS},
  author       = {Weibing Li and Philip Wai Yan Chiu and Zheng Li},
  doi          = {10.1109/TNNLS.2021.3095122},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {409-420},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel neural approach to infinity-norm joint-velocity minimization of kinematically redundant robots under joint limits},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A supervised learning algorithm for multilayer spiking
neural networks based on temporal coding toward energy-efficient VLSI
processor design. <em>TNNLS</em>, <em>34</em>(1), 394–408. (<a
href="https://doi.org/10.1109/TNNLS.2021.3095068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are brain-inspired mathematical models with the ability to process information in the form of spikes. SNNs are expected to provide not only new machine-learning algorithms but also energy-efficient computational models when implemented in very-large-scale integration (VLSI) circuits. In this article, we propose a novel supervised learning algorithm for SNNs based on temporal coding. A spiking neuron in this algorithm is designed to facilitate analog VLSI implementations with analog resistive memory, by which ultrahigh energy efficiency can be achieved. We also propose several techniques to improve the performance on recognition tasks and show that the classification accuracy of the proposed algorithm is as high as that of the state-of-the-art temporal coding SNN algorithms on the MNIST and Fashion-MNIST datasets. Finally, we discuss the robustness of the proposed SNNs against variations that arise from the device manufacturing process and are unavoidable in analog VLSI implementation. We also propose a technique to suppress the effects of variations in the manufacturing process on the recognition performance.},
  archive      = {J_TNNLS},
  author       = {Yusuke Sakemi and Kai Morino and Takashi Morie and Kazuyuki Aihara},
  doi          = {10.1109/TNNLS.2021.3095068},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {394-408},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A supervised learning algorithm for multilayer spiking neural networks based on temporal coding toward energy-efficient VLSI processor design},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TopicBERT: A topic-enhanced neural language model fine-tuned
for sentiment classification. <em>TNNLS</em>, <em>34</em>(1), 380–393.
(<a href="https://doi.org/10.1109/TNNLS.2021.3094987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment classification is a form of data analytics where people’s feelings and attitudes toward a topic are mined from data. This tantalizing power to “predict the zeitgeist” means that sentiment classification has long attracted interest, but with mixed results. However, the recent development of the BERT framework and its pretrained neural language models is seeing new-found success for sentiment classification. BERT models are trained to capture word-level information via mask language modeling and sentence-level contexts via next sentence prediction tasks. Out of the box, they are adequate models for some natural language processing tasks. However, most models are further fine-tuned with domain-specific information to increase accuracy and usefulness. Motivated by the idea that a further fine-tuning step would improve the performance for downstream sentiment classification tasks, we developed TopicBERT—a BERT model fine-tuned to recognize topics at the corpus level in addition to the word and sentence levels. TopicBERT comprises two variants: TopicBERT-ATP (aspect topic prediction), which captures topic information via an auxiliary training task, and TopicBERT-TA, where topic representation is directly injected into a topic augmentation layer for sentiment classification. With TopicBERT-ATP, the topics are predetermined by an LDA mechanism and collapsed Gibbs sampling. With TopicBERT-TA, the topics can change dynamically during the training. Experimental results show that both approaches deliver the state-of-the-art performance in two different domains with SemEval 2014 Task 4. However, in a test of methods, direct augmentation outperforms further training. Comprehensive analyses in the form of ablation, parameter, and complexity studies accompany the results.},
  archive      = {J_TNNLS},
  author       = {Yuxiang Zhou and Lejian Liao and Yang Gao and Rui Wang and Heyan Huang},
  doi          = {10.1109/TNNLS.2021.3094987},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {380-393},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TopicBERT: A topic-enhanced neural language model fine-tuned for sentiment classification},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered multigradient recursive reinforcement
learning tracking control for multiagent systems. <em>TNNLS</em>,
<em>34</em>(1), 366–379. (<a
href="https://doi.org/10.1109/TNNLS.2021.3094901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the tracking control problem of event-triggered multigradient recursive reinforcement learning is investigated for nonlinear multiagent systems (MASs). Attention is focused on the distributed reinforcement learning approach for MASs. The critic neural network (NN) is applied to estimate the long-term strategic utility function, and the actor NN is designed to approximate the uncertain dynamics in MASs. The multigradient recursive (MGR) strategy is tailored to learn the weight vector in NN, which eliminates the local optimal problem inherent in gradient descent method and decreases the dependence of initial value. Furthermore, reinforcement learning and event-triggered mechanism can improve the energy conservation of MASs by decreasing the amplitude of the controller signal and the controller update frequency, respectively. It is proved that all signals in MASs are semiglobal uniformly ultimately bounded (SGUUB) according to the Lyapunov theory. Simulation results are given to demonstrate the effectiveness of the proposed strategy.},
  archive      = {J_TNNLS},
  author       = {Weiwei Bai and Tieshan Li and Yue Long and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3094901},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {366-379},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered multigradient recursive reinforcement learning tracking control for multiagent systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Correlated SVD and its application in bearing fault
diagnosis. <em>TNNLS</em>, <em>34</em>(1), 355–365. (<a
href="https://doi.org/10.1109/TNNLS.2021.3094799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The singular value decomposition (SVD) based on the Hankel matrix is commonly used in signal processing and fault diagnosis. The noise reduction performance of SVD based on the Hankel matrix is affected by three factors: the reconstruction component(s), the structure of the Hankel matrix, and the point number of the analysis data. In this article, the three influencing factors are systematically studied, and a method based on correlated SVD (C-SVD) is proposed and successfully applied to bearing fault diagnosis. First, perform SVD analysis on the collected original signal. Then, the reconstructed component(s) determination method of SVD based on the combination of singular value ratio (SVR) and correlation coefficient is proposed. Then, based on the SVR, using the envelope kurtosis as the indicator, the optimal structure of the Hankel matrix (number of rows and columns) is studied. Then, the number of data points of the analysis signal is discussed, and the constraint range is given. Finally, the envelope power spectrum analysis is performed on the reconstructed signal to extract the fault features. The proposed C-SVD method is compared with the existing typical methods and applied to the simulated signal and the actual bearing fault signal, and its superiority is verified.},
  archive      = {J_TNNLS},
  author       = {Hua Li and Tao Liu and Xing Wu and Shaobo Li},
  doi          = {10.1109/TNNLS.2021.3094799},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {355-365},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Correlated SVD and its application in bearing fault diagnosis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Finite-time estimation for markovian BAM neural networks
with asymmetrical mode-dependent delays and inconstant measurements.
<em>TNNLS</em>, <em>34</em>(1), 344–354. (<a
href="https://doi.org/10.1109/TNNLS.2021.3094551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of finite-time state estimation is studied for discrete-time Markovian bidirectional associative memory neural networks. The asymmetrical system mode-dependent (SMD) time-varying delays (TVDs) are considered, which means that the interval of TVDs is SMD. Because the sensors are inevitably influenced by the measurement environments and indirectly influenced by the system mode, a Markov chain, whose transition probability matrix is SMD, is used to describe the inconstant measurement. A nonfragile estimator is designed to improve the robustness of the estimator. The stochastically finite-time bounded stability is guaranteed under certain conditions. Finally, an example is used to clarify the effectiveness of the state estimation.},
  archive      = {J_TNNLS},
  author       = {Chang Liu and Zhuo Wang and Renquan Lu and Tingwen Huang and Yong Xu},
  doi          = {10.1109/TNNLS.2021.3094551},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {344-354},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time estimation for markovian BAM neural networks with asymmetrical mode-dependent delays and inconstant measurements},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-label sentiment analysis on 100 languages with dynamic
weighting for label imbalance. <em>TNNLS</em>, <em>34</em>(1), 331–343.
(<a href="https://doi.org/10.1109/TNNLS.2021.3094304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate cross-lingual sentiment analysis, which has attracted significant attention due to its applications in various areas including market research, politics, and social sciences. In particular, we introduce a sentiment analysis framework in multi-label setting as it obeys Plutchik’s wheel of emotions. We introduce a novel dynamic weighting method that balances the contribution from each class during training, unlike previous static weighting methods that assign non-changing weights based on their class frequency. Moreover, we adapt the focal loss that favors harder instances from single-label object recognition literature to our multi-label setting. Furthermore, we derive a method to choose optimal class-specific thresholds that maximize the macro-f1 score in linear time complexity. Through an extensive set of experiments, we show that our method obtains the state-of-the-art performance in seven of nine metrics in three different languages using a single model compared with the common baselines and the best performing methods in the SemEval competition. We publicly share our code for our model, which can perform sentiment analysis in 100 languages, to facilitate further research.},
  archive      = {J_TNNLS},
  author       = {Selim F. Yilmaz and E. Batuhan Kaynak and Aykut Koç and Hamdi Dibeklioğlu and Suleyman Serdar Kozat},
  doi          = {10.1109/TNNLS.2021.3094304},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {331-343},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-label sentiment analysis on 100 languages with dynamic weighting for label imbalance},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A time-delay feedback neural network for discriminating
small, fast-moving targets in complex dynamic environments.
<em>TNNLS</em>, <em>34</em>(1), 316–330. (<a
href="https://doi.org/10.1109/TNNLS.2021.3094205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminating small moving objects within complex visual environments is a significant challenge for autonomous micro-robots that are generally limited in computational power. By exploiting their highly evolved visual systems, flying insects can effectively detect mates and track prey during rapid pursuits, even though the small targets equate to only a few pixels in their visual field. The high degree of sensitivity to small target movement is supported by a class of specialized neurons called small target motion detectors (STMDs). Existing STMD-based computational models normally comprise four sequentially arranged neural layers interconnected via feedforward loops to extract information on small target motion from raw visual inputs. However, feedback, another important regulatory circuit for motion perception, has not been investigated in the STMD pathway and its functional roles for small target motion detection are not clear. In this article, we propose an STMD-based neural network with feedback connection (feedback STMD), where the network output is temporally delayed, then fed back to the lower layers to mediate neural responses. We compare the properties of the model with and without the time-delay feedback loop and find that it shows a preference for high-velocity objects. Extensive experiments suggest that the feedback STMD achieves superior detection performance for fast-moving small targets, while significantly suppressing background false positive movements which display lower velocities. The proposed feedback model provides an effective solution in robotic visual systems for detecting fast-moving small targets that are always salient and potentially threatening.},
  archive      = {J_TNNLS},
  author       = {Hongxin Wang and Huatian Wang and Jiannan Zhao and Cheng Hu and Jigen Peng and Shigang Yue},
  doi          = {10.1109/TNNLS.2021.3094205},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {316-330},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A time-delay feedback neural network for discriminating small, fast-moving targets in complex dynamic environments},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Gradient descent-barzilai borwein-based neural network
tracking control for nonlinear systems with unknown dynamics.
<em>TNNLS</em>, <em>34</em>(1), 305–315. (<a
href="https://doi.org/10.1109/TNNLS.2021.3093877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a combined gradient descent-Barzilai Borwein (GD-BB) algorithm and radial basis function neural network (RBFNN) output tracking control strategy was proposed for a family of nonlinear systems with unknown drift function and control input gain function. In such a method, a neural network (NN) is used to approximate the controller directly. The main merits of the proposed strategy are given as follows: first, not only the NN parameters, such as weights, centers, and widths but also the learning rates of NN parameter updating laws are updated online via the proposed learning algorithm based on Barzilai-Borwein technique; and second, the controller design process can be further simplified, the controller parameters that should be tuned can be greatly reduced. Theoretical analysis about the stability of the closed-loop system is manifested. In addition, simulations were conducted on a numerical discrete time system and an inverted pendulum system to validate the presented control strategy.},
  archive      = {J_TNNLS},
  author       = {Yujia Wang and Tong Wang and Xuebo Yang and Jiae Yang},
  doi          = {10.1109/TNNLS.2021.3093877},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {305-315},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gradient descent-barzilai borwein-based neural network tracking control for nonlinear systems with unknown dynamics},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convolutional sparse support estimator network (CSEN): From
energy-efficient support estimation to learning-aided compressive
sensing. <em>TNNLS</em>, <em>34</em>(1), 290–304. (<a
href="https://doi.org/10.1109/TNNLS.2021.3093818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support estimation (SE) of a sparse signal refers to finding the location indices of the nonzero elements in a sparse representation. Most of the traditional approaches dealing with SE problems are iterative algorithms based on greedy methods or optimization techniques. Indeed, a vast majority of them use sparse signal recovery (SR) techniques to obtain support sets instead of directly mapping the nonzero locations from denser measurements (e.g., compressively sensed measurements). This study proposes a novel approach for learning such a mapping from a training set. To accomplish this objective, the convolutional sparse support estimator networks (CSENs), each with a compact configuration, are designed. The proposed CSEN can be a crucial tool for the following scenarios: 1) real-time and low-cost SE can be applied in any mobile and low-power edge device for anomaly localization, simultaneous face recognition, and so on and 2) CSEN’s output can directly be used as “prior information,” which improves the performance of sparse SR algorithms. The results over the benchmark datasets show that state-of-the-art performance levels can be achieved by the proposed approach with a significantly reduced computational complexity.},
  archive      = {J_TNNLS},
  author       = {Mehmet Yamaç and Mete Ahishali and Serkan Kiranyaz and Moncef Gabbouj},
  doi          = {10.1109/TNNLS.2021.3093818},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {290-304},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convolutional sparse support estimator network (CSEN): From energy-efficient support estimation to learning-aided compressive sensing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). H∞bipartite synchronization of double-layer markov switched
cooperation-competition neural networks: A distributed dynamic
event-triggered mechanism. <em>TNNLS</em>, <em>34</em>(1), 278–289. (<a
href="https://doi.org/10.1109/TNNLS.2021.3093700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the $\mathcal {H}_{\infty }$ bipartite synchronization issue is studied for a class of discrete-time coupled switched neural networks with antagonistic interactions via a distributed dynamic event-triggered control scheme. Essentially different from most current literature, the topology switching of the investigated signed graph is governed by a double-layer switching signal, which integrates a flexible deterministic switching regularity, the persistent dwell-time switching, into a Markov chain to represent the variation of transition probability. Considering the coexistence of cooperative and antagonistic interactions among nodes, the bipartite synchronization of which the dynamics of nodes converge to values with the same modulus but the opposite signs is explored. A distributed control strategy based on the dynamic event-triggered mechanism is utilized to achieve this goal. Under this circumstance, the information update of the controller presents an aperiodic manner, and the frequency of data transmission can be reduced extensively. Thereafter, by constructing a novel Lyapunov function depending on both the switching signal and the internal dynamic nonnegative variable of the triggering mechanism, the exponential stability of bipartite synchronization error systems in the mean-square sense is analyzed. Finally, two simulation examples are provided to illustrate the effectiveness of the derived results.},
  archive      = {J_TNNLS},
  author       = {Jing Wang and Mengping Xing and Jinde Cao and Ju H. Park and Hao Shen},
  doi          = {10.1109/TNNLS.2021.3093700},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {278-289},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {H∞Bipartite synchronization of double-layer markov switched cooperation-competition neural networks: A distributed dynamic event-triggered mechanism},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Rethinking maximum mean discrepancy for visual domain
adaptation. <em>TNNLS</em>, <em>34</em>(1), 264–277. (<a
href="https://doi.org/10.1109/TNNLS.2021.3093468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing domain adaptation approaches often try to reduce distribution difference between source and target domains and respect domain-specific discriminative structures by some distribution [e.g., maximum mean discrepancy (MMD)] and discriminative distances (e.g., intra-class and inter-class distances). However, they usually consider these losses together and trade off their relative importance by estimating parameters empirically. It is still under insufficient exploration so far to deeply study their relationships to each other so that we cannot manipulate them correctly and the model’s performance degrades. To this end, this article theoretically proves two essential facts: 1) minimizing MMD equals to jointly minimizing their data variance with some implicit weights but, respectively, maximizing the source and target intra-class distances so that feature discriminability degrades and 2) the relationship between intra-class and inter-class distances is as one falls and another rises. Based on this, we propose a novel discriminative MMD with two parallel strategies to correctly restrain the degradation of feature discriminability or the expansion of intra-class distance; specifically: 1) we directly impose a tradeoff parameter on the intra-class distance that is implicit in the MMD according to 1) and 2) we reformulate the inter-class distance with special weights that are analogical to those implicit ones in the MMD and maximizing it can also lead to the intra-class distance falling according to 2). Notably, we do not consider the two strategies in one model due to 2). The experiments on several benchmark datasets not only prove the validity of our revealed theoretical results but also demonstrate that the proposed approach could perform better than some compared state-of-art methods substantially. Our preliminary MATLAB code will be available at https://github.com/WWLoveTransfer/ .},
  archive      = {J_TNNLS},
  author       = {Wei Wang and Haojie Li and Zhengming Ding and Feiping Nie and Junyang Chen and Xiao Dong and Zhihui Wang},
  doi          = {10.1109/TNNLS.2021.3093468},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {264-277},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rethinking maximum mean discrepancy for visual domain adaptation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple kernel clustering with compressed subspace
alignment. <em>TNNLS</em>, <em>34</em>(1), 252–263. (<a
href="https://doi.org/10.1109/TNNLS.2021.3093426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel clustering (MKC) has recently achieved remarkable progress in fusing multisource information to boost the clustering performance. However, the $\mathcal {O}({n}^{2})$ memory consumption and $\mathcal {O}({n}^{3})$ computational complexity prohibit these methods from being applied into median- or large-scale applications, where $n$ denotes the number of samples. To address these issues, we carefully redesign the formulation of subspace segmentation-based MKC, which reduces the memory and computational complexity to $\mathcal {O}({n})$ and $\mathcal {O}({n}^{2})$ , respectively. The proposed algorithm adopts a novel sampling strategy to enhance the performance and accelerate the speed of MKC. Specifically, we first mathematically model the sampling process and then learn it simultaneously during the procedure of information fusion. By this way, the generated anchor point set can better serve data reconstruction across different views, leading to improved discriminative capability of the reconstruction matrix and boosted clustering performance. Although the integrated sampling process makes the proposed algorithm less efficient than the linear complexity algorithms, the elaborate formulation makes our algorithm straightforward for parallelization. Through the acceleration of GPU and multicore techniques, our algorithm achieves superior performance against the compared state-of-the-art methods on six datasets with comparable time cost to the linear complexity algorithms.},
  archive      = {J_TNNLS},
  author       = {Sihang Zhou and Qiyuan Ou and Xinwang Liu and Siqi Wang and Luyan Liu and Siwei Wang and En Zhu and Jianping Yin and Xin Xu},
  doi          = {10.1109/TNNLS.2021.3093426},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {252-263},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiple kernel clustering with compressed subspace alignment},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite-time stability of nonlinear impulsive systems with
applications to neural networks. <em>TNNLS</em>, <em>34</em>(1),
243–251. (<a href="https://doi.org/10.1109/TNNLS.2021.3093418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the problem of finite-time stability (FTS) and finite-time contractive stability (FTCS) for nonlinear impulsive systems, where the possibility of time delay in impulses is fully considered. Some sufficient conditions for FTS/FTCS are constructed in the framework of Lyapunov function methods. A relationship between impulsive frequency and the time delay existing in impulses is established to reveal FTS/FTCS performance. As an application, we apply the theoretical results to finite-time state estimation of neural networks, including time-varying neural networks and switched neural networks. Finally, two illustrated examples are given to show the effectiveness and distinctiveness of the proposed delay-dependent impulsive schemes.},
  archive      = {J_TNNLS},
  author       = {Xueyan Yang and Xiaodi Li},
  doi          = {10.1109/TNNLS.2021.3093418},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {243-251},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time stability of nonlinear impulsive systems with applications to neural networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mimicking the brain’s cognition of sarcasm from
multidisciplines for twitter sarcasm detection. <em>TNNLS</em>,
<em>34</em>(1), 228–242. (<a
href="https://doi.org/10.1109/TNNLS.2021.3093416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm is a sophisticated construct to express contempt or ridicule. It is well-studied in multiple disciplines (e.g., neuroanatomy and neuropsychology) but is still in its infancy in computational science (e.g., Twitter sarcasm detection). In contrast to previous methods that are usually geared toward a single discipline, we focus on the multidisciplinary cross-innovation, i.e., improving embryonic sarcasm detection in computational science by leveraging the advanced knowledge of sarcasm cognition in neuroanatomy and neuropsychology. In this work, we are oriented toward sarcasm detection in social media and correspondingly propose a multimodal, multi-interactive, and multihierarchical neural network ( $M_{3}N_{2} $ ). We select Twitter, image, text in image, and image caption as the input of $M_{3}N_{2} $ since the brain’s perception of sarcasm requires multiple modalities. To reasonably address the multimodalities, we introduce singlewise, pairwise, triplewise, and tetradwise modality interactions incorporating gate mechanism and guide attention (GA) to simulate the interactions and collaborations of involved regions in the brain while perceiving multiple modes. Specifically, we exploit a multihop process for each modality interaction to extract modal information multiple times using GA for obtaining multiperspective information. Also, we adopt a two-hierarchical structure leveraging self-attention accompanied by attention pooling to integrate multimodal semantic information from different levels mimicking the brain’s first- and second-order comprehensions of sarcasm. Experimental results show that $M_{3}N_{2} $ achieves competitive performance in sarcasm detection and displays powerful generalization ability in multimodal sentiment analysis and emotion recognition.},
  archive      = {J_TNNLS},
  author       = {Fanglong Yao and Xian Sun and Hongfeng Yu and Wenkai Zhang and Wei Liang and Kun Fu},
  doi          = {10.1109/TNNLS.2021.3093416},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {228-242},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mimicking the brain’s cognition of sarcasm from multidisciplines for twitter sarcasm detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GopGAN: Gradients orthogonal projection generative
adversarial network with continual learning. <em>TNNLS</em>,
<em>34</em>(1), 215–227. (<a
href="https://doi.org/10.1109/TNNLS.2021.3093319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generative adversarial networks (GANs) in continual learning suffer from catastrophic forgetting. In continual learning, GANs tend to forget about previous generation tasks and only remember the tasks they just learned. In this article, we present a novel conditional GAN, called the gradients orthogonal projection GAN (GopGAN), which updates the weights in the orthogonal subspace of the space spanned by the representations of training examples, and we also mathematically demonstrate its ability to retain the old knowledge about learned tasks in learning a new task. Furthermore, the orthogonal projection matrix for modulating gradients is mathematically derived and its iterative calculation algorithm for continual learning is given so that training examples for learned tasks do not need to be stored when learning a new task. In addition, a task-dependent latent vector construction is presented and the constructed conditional latent vectors are used as the inputs of generator in GopGAN to avoid the disappearance of orthogonal subspace of learned tasks. Extensive experiments on MNIST, EMNIST, SVHN, CIFAR10, and ImageNet-200 generation tasks show that the proposed GopGAN can effectively cope with the issue of catastrophic forgetting and stably retain learned knowledge.},
  archive      = {J_TNNLS},
  author       = {Xiaobin Li and Weiqiang Wang},
  doi          = {10.1109/TNNLS.2021.3093319},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {215-227},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GopGAN: Gradients orthogonal projection generative adversarial network with continual learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view clustering via nonnegative and orthogonal graph
reconstruction. <em>TNNLS</em>, <em>34</em>(1), 201–214. (<a
href="https://doi.org/10.1109/TNNLS.2021.3093297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of multi-view clustering is to partition samples into different subsets according to their diverse features. Previous multi-view clustering methods mainly exist two forms: multi-view spectral clustering and multi-view matrix factorization. Although they have shown excellent performance in many occasions, there are still many disadvantages. For example, multi-view spectral clustering usually needs to perform postprocessing. Multi-view matrix factorization directly decomposes the original data features. When the size of features is large, it encounters the expensive time consumption to decompose these data features thoroughly. Therefore, we proposed a novel multi-view clustering approach. The main advantages include the following three aspects: 1) it searches for a common joint graph across multiple views, which fully explores the hidden structure information by utilizing the compatibility among views; 2) the introduced nonnegative constraint manipulates that the final clustering results can be directly obtained; and 3) straightforwardly decomposing the similarity matrix can transform the eigenvalue factorization in spectral clustering with computational complexity $O(n^{3})$ into the singular value decomposition (SVD) with $O({n}{c}^{2})$ time cost, where ${n}$ and $c$ , respectively, denote the numbers of samples and classes. Thus, the computational efficiency can be improved. Moreover, in order to learn a better clustering model, we set that the constructed similarity graph approximates each view affinity graph as close as possible by adding the constraint as the initial affinity matrices own. Furthermore, substantial experiments are conducted, which verifies the superiority of the proposed two clustering methods comparing with single-view clustering approaches and state-of-the-art multi-view clustering methods.},
  archive      = {J_TNNLS},
  author       = {Shaojun Shi and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3093297},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {201-214},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-view clustering via nonnegative and orthogonal graph reconstruction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online active learning for drifting data streams.
<em>TNNLS</em>, <em>34</em>(1), 186–200. (<a
href="https://doi.org/10.1109/TNNLS.2021.3091681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification methods for streaming data are not new, but very few current frameworks address all three of the most common problems with these tasks: concept drift, noise, and the exorbitant costs associated with labeling the unlabeled instances in data streams. Motivated by this gap in the field, we developed an active learning framework based on a dual-query strategy and Ebbinghaus’s law of human memory cognition. Called CogDQS, the query strategy samples only the most representative instances for manual annotation based on local density and uncertainty, thus significantly reducing the cost of labeling. The policy for discerning drift from noise and replacing outdated instances with new concepts is based on the three criteria of the Ebbinghaus forgetting curve: recall, the fading period, and the memory strength. Simulations comparing CogDQS with baselines on six different data streams containing gradual drift or abrupt drift with and without noise show that our approach produces accurate, stable models with good generalization ability at minimal labeling, storage, and computation costs.},
  archive      = {J_TNNLS},
  author       = {Sanmin Liu and Shan Xue and Jia Wu and Chuan Zhou and Jian Yang and Zhao Li and Jie Cao},
  doi          = {10.1109/TNNLS.2021.3091681},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {186-200},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online active learning for drifting data streams},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rank plus sparse decomposition of covariance matrices
using neural network parametrization. <em>TNNLS</em>, <em>34</em>(1),
171–185. (<a href="https://doi.org/10.1109/TNNLS.2021.3091598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article revisits the problem of decomposing a positive semidefinite matrix as a sum of a matrix with a given rank plus a sparse matrix. An immediate application can be found in portfolio optimization, when the matrix to be decomposed is the covariance between the different assets in the portfolio. Our approach consists in representing the low-rank part of the solution as the product $MM^{T}$ , where $M$ is a rectangular matrix of appropriate size, parametrized by the coefficients of a deep neural network. We then use a gradient descent algorithm to minimize an appropriate loss function over the parameters of the network. We deduce its convergence rate to a local optimum from the Lipschitz smoothness of our loss function. We show that the rate of convergence grows polynomially in the dimensions of the input–output, and the size of each of the hidden layers.},
  archive      = {J_TNNLS},
  author       = {Michel Baes and Calypso Herrera and Ariel Neufeld and Pierre Ruyssen},
  doi          = {10.1109/TNNLS.2021.3091598},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {171-185},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low-rank plus sparse decomposition of covariance matrices using neural network parametrization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metro passenger-flow representation via dynamic mode
decomposition and its application. <em>TNNLS</em>, <em>34</em>(1),
157–170. (<a href="https://doi.org/10.1109/TNNLS.2021.3090695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passenger-flow anomaly detection and prediction are essential tasks for intelligent operation of the metro system. Accurate passenger-flow representation is the foundation of them. However, spatiotemporal dependencies, complex dynamic changes, and anomalies of passenger-flow data bring great challenges to data representation. Taking advantage of the time-varying characteristics of data, we propose a novel passenger-flow representation model based on low-rank dynamic mode decomposition (DMD), which also integrates the global low-rank nature and sparsity to explore the spatiotemporal consistency of data and depict abrupt data, respectively. The model can detect anomalies and predict short-term passenger flow conveniently and flexibly. For anomaly detection, we further introduce a strong temporal Toeplitz regularization to characterize the temporal periodic change of data, so as to more accurately detect anomalies. We conduct experiments with smart card transaction data from the Beijing metro system to assess the performance of the model in two use cases. In terms of anomaly detection, the experimental results demonstrate that our method can detect anomalies efficiently, especially for time sequence anomalies. As for short-term prediction, our model is superior to other methods in most cases.},
  archive      = {J_TNNLS},
  author       = {Xiulan Wei and Yong Zhang and Yun Wei and Yongli Hu and Shuzhen Tong and Wei Huang and Jinde Cao},
  doi          = {10.1109/TNNLS.2021.3090695},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {157-170},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Metro passenger-flow representation via dynamic mode decomposition and its application},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Adaptive multigradient recursive reinforcement learning
event-triggered tracking control for multiagent systems. <em>TNNLS</em>,
<em>34</em>(1), 144–156. (<a
href="https://doi.org/10.1109/TNNLS.2021.3090570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a fault-tolerant adaptive multigradient recursive reinforcement learning (RL) event-triggered tracking control scheme for strict-feedback discrete-time multiagent systems. The multigradient recursive RL algorithm is used to avoid the local optimal problem that may exist in the gradient descent scheme. Different from the existing event-triggered control results, a new lemma about the relative threshold event-triggered control strategy is proposed to handle the compensation error, which can improve the utilization of communication resources and weaken the negative impact on tracking accuracy and closed-loop system stability. To overcome the difficulty caused by sensor fault, a distributed control method is introduced by adopting the adaptive compensation technique, which can effectively decrease the number of online estimation parameters. Furthermore, by using the multigradient recursive RL algorithm with less learning parameters, the online estimation time can be effectively reduced. The stability of closed-loop multiagent systems is proved by using the Lyapunov stability theorem, and it is verified that all signals are semiglobally uniformly ultimately bounded. Finally, two simulation examples are given to show the availability of the presented control scheme.},
  archive      = {J_TNNLS},
  author       = {Hongyi Li and Ying Wu and Mou Chen and Renquan Lu},
  doi          = {10.1109/TNNLS.2021.3090570},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {144-156},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive multigradient recursive reinforcement learning event-triggered tracking control for multiagent systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time global inference network for one-stage referring
expression comprehension. <em>TNNLS</em>, <em>34</em>(1), 134–143. (<a
href="https://doi.org/10.1109/TNNLS.2021.3090426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring expression comprehension (REC) is an emerging research topic in computer vision, which refers to the detection of a target region in an image given a test description. Most existing REC methods follow a multistage pipeline, which is computationally expensive and greatly limits the applications of REC. In this article, we propose a one-stage model toward real-time REC, termed real-time global inference network (RealGIN). RealGIN addresses the issues of expression diversity and complexity of REC with two innovative designs: adaptive feature selection (AFS) and Global Attentive ReAsoNing (GARAN). Expression diversity concerns varying expression content, which includes information such as colors, attributes, locations, and fine-grained categories. To address this issue, AFS adaptively fuses features of different semantic levels to tackle the changes in expression content. In contrast, expression complexity concerns the complex relational conditions in expressions that are used to identify the referent. To this end, GARAN uses the textual feature as a pivot to collect expression-aware visual information from all regions and then diffuses this information back to each region, which provides sufficient context for modeling the relational conditions in expressions. On five benchmark datasets, i.e., RefCOCO, RefCOCO+, RefCOCOg, ReferIT, and Flickr30k, the proposed RealGIN outperforms most existing methods and achieves very competitive performances against the most advanced one, i.e., MAttNet. More importantly, under the same hardware, RealGIN can boost the processing speed by 10–20 times over the existing methods.},
  archive      = {J_TNNLS},
  author       = {Yiyi Zhou and Rongrong Ji and Gen Luo and Xiaoshuai Sun and Jinsong Su and Xinghao Ding and Chia-Wen Lin and Qi Tian},
  doi          = {10.1109/TNNLS.2021.3090426},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {134-143},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A real-time global inference network for one-stage referring expression comprehension},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A theoretical insight into the effect of loss function for
deep semantic-preserving learning. <em>TNNLS</em>, <em>34</em>(1),
119–133. (<a href="https://doi.org/10.1109/TNNLS.2021.3090358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Good generalization performance is the fundamental goal of any machine learning algorithm. Using the uniform stability concept, this article theoretically proves that the choice of loss function impacts the generalization performance of a trained deep neural network (DNN). The adopted stability-based framework provides an effective tool for comparing the generalization error bound with respect to the utilized loss function. The main result of our analysis is that using an effective loss function makes stochastic gradient descent more stable which consequently leads to the tighter generalization error bound, and so better generalization performance. To validate our analysis, we study learning problems in which the classes are semantically correlated. To capture this semantic similarity of neighboring classes, we adopt the well-known semantics-preserving learning framework, namely label distribution learning (LDL). We propose two novel loss functions for the LDL framework and theoretically show that they provide stronger stability than the other widely used loss functions adopted for training DNNs. The experimental results on three applications with semantically correlated classes, including facial age estimation, head pose estimation, and image esthetic assessment, validate the theoretical insights gained by our analysis and demonstrate the usefulness of the proposed loss functions in practical applications.},
  archive      = {J_TNNLS},
  author       = {Ali Akbari and Muhammad Awais and Manijeh Bashar and Josef Kittler},
  doi          = {10.1109/TNNLS.2021.3090358},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {119-133},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A theoretical insight into the effect of loss function for deep semantic-preserving learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered approximate optimal path-following control
for unmanned surface vehicles with state constraints. <em>TNNLS</em>,
<em>34</em>(1), 104–118. (<a
href="https://doi.org/10.1109/TNNLS.2021.3090054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of path following for the underactuated unmanned surface vehicles (USVs) subject to state constraints. A useful control algorithm is proposed by combining the backstepping technique, adaptive dynamic programming (ADP), and the event-triggered mechanism. The presented approach consists of three modules: guidance law, dynamic controller, and event triggering. First, to deal with the “singularity” problem, the guidance-based path-following (GBPF) principle is introduced in the guidance law loop. In contrast to the traditional barrier Lyapunov function (BLF) method, this article converts the USV’s constraint model to a class of nonlinear systems without state constraints by introducing a nonlinear mapping. The control signal generated by the dynamic controller module consists of a backstepping-based feedforward control signal and an ADP-based approximate optimal feedback control signal. Therefore, the presented scheme can guarantee the approximate optimal performance. To approximate the cost function and its partial derivative, a critic neural network (NN) is constructed. By considering the event-triggered condition, the dynamic controller is further improved. Compared with traditional time-triggered control methods, the proposed approach can greatly reduce communication and computational burdens. This article proves that the closed-loop system is stable, and the simulation results and experimental validation are given to illustrate the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Weixiang Zhou and Jun Fu and Huaicheng Yan and Xin Du and Yueying Wang and Hua Zhou},
  doi          = {10.1109/TNNLS.2021.3090054},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {104-118},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered approximate optimal path-following control for unmanned surface vehicles with state constraints},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical and stable multiagent reinforcement learning
for cooperative navigation control. <em>TNNLS</em>, <em>34</em>(1),
90–103. (<a href="https://doi.org/10.1109/TNNLS.2021.3089834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We solve an important and challenging cooperative navigation control problem, Multiagent Navigation to Unassigned Multiple targets (MNUM) in unknown environments with minimal time and without collision. Conventional methods are based on multiagent path planning that requires building an environment map and expensive real-time path planning computations. In this article, we formulate MNUM as a stochastic game and devise a novel multiagent deep reinforcement learning (MADRL) algorithm to learn an end-to-end solution, which directly maps raw sensor data to control signals. Once learned, the policy can be deployed onto each agent, and thereby, the expensive online planning computations can be offloaded. However, to solve MNUM, traditional MADRL suffers from large policy solution space and nonstationary environment when agents make decisions independently and concurrently. Accordingly, we propose a hierarchical and stable MADRL algorithm. The hierarchical learning part introduces a two-layer policy model to reduce the solution space and uses an interlaced learning paradigm to learn two coupled policies. In the stable learning part, we propose to learn an extended action-value function that implicitly incorporates estimations of other agents’ actions, based on which the environment’s nonstationarity caused by other agents’ changing policies can be alleviated. Extensive experiments demonstrate that our method can converge in a fast way and generate more efficient cooperative navigation policies than comparable methods.},
  archive      = {J_TNNLS},
  author       = {Yue Jin and Shuangqing Wei and Jian Yuan and Xudong Zhang},
  doi          = {10.1109/TNNLS.2021.3089834},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {90-103},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical and stable multiagent reinforcement learning for cooperative navigation control},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive bipartite output tracking consensus in switching
networks of heterogeneous linear multiagent systems based on edge
events. <em>TNNLS</em>, <em>34</em>(1), 79–89. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the problem of adaptive bipartite output tracking for a class of heterogeneous linear multiagent systems (MASs) by asynchronous edge-event-triggered communications under jointly connected signed topologies. By designing the observers to estimate the states of followers and the dynamic compensators to estimate the states of zero input and nonzero input leader, respectively, the fully distributed edge-event-triggered control protocol is presented. Moreover, it is proven that the bipartite output tracking problem is implemented, and the systems do not exhibit Zeno behavior under a fully distributed control strategy with edge-event-triggered mechanisms. Compared with the existing works, one of the highlights of this article is the design of triggering mechanisms, under which the leader avoids continuous information transmission and any pair of followers that make up the edge asynchronously transmit information through the edge. The methods greatly avoid unnecessary information transmission in the systems. Finally, several simulation examples are introduced to demonstrate the theoretical results obtained in this article.},
  archive      = {J_TNNLS},
  author       = {Juan Zhang and Huaguang Zhang and Yuling Liang and Weizhao Song},
  doi          = {10.1109/TNNLS.2021.3089596},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {79-89},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive bipartite output tracking consensus in switching networks of heterogeneous linear multiagent systems based on edge events},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative fisher embedding dictionary transfer learning
for object recognition. <em>TNNLS</em>, <em>34</em>(1), 64–78. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In transfer learning model, the source domain samples and target domain samples usually share the same class labels but have different distributions. In general, the existing transfer learning algorithms ignore the interclass differences and intraclass similarities across domains. To address these problems, this article proposes a transfer learning algorithm based on discriminative Fisher embedding and adaptive maximum mean discrepancy (AMMD) constraints, called discriminative Fisher embedding dictionary transfer learning (DFEDTL). First, combining the label information of source domain and part of target domain, we construct the discriminative Fisher embedding model to preserve the interclass differences and intraclass similarities of training samples in transfer learning. Second, an AMMD model is constructed using atoms and profiles, which can adaptively minimize the distribution differences between source domain and target domain. The proposed method has three advantages: 1) using the Fisher criterion, we construct the discriminative Fisher embedding model between source domain samples and target domain samples, which encourages the samples from the same class to have similar coding coefficients; 2) instead of using the training samples to design the maximum mean discrepancy (MMD), we construct the AMMD model based on the relationship between the dictionary atoms and profiles; thus, the source domain samples can be adaptive to the target domain samples; and 3) the dictionary learning is based on the combination of source and target samples which can avoid the classification error caused by the difference among samples and reduce the tedious and expensive data annotation. A large number of experiments on five public image classification datasets show that the proposed method obtains better classification performance than some state-of-the-art dictionary and transfer learning methods. The code has been available at https://github.com/shilinrui/DFEDTL .},
  archive      = {J_TNNLS},
  author       = {Zizhu Fan and Linrui Shi and Qiang Liu and Zhengming Li and Zheng Zhang},
  doi          = {10.1109/TNNLS.2021.3089566},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {64-78},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative fisher embedding dictionary transfer learning for object recognition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMIX(λ): Enhancing centralized value functions for
cooperative multiagent reinforcement learning. <em>TNNLS</em>,
<em>34</em>(1), 52–63. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a stable and generalizable centralized value function (CVF) is a crucial but challenging task in multiagent reinforcement learning (MARL), as it has to deal with the issue that the joint action space increases exponentially with the number of agents in such scenarios. This article proposes an approach, named SMIX( ${\lambda }$ ), that uses an OFF-policy training to achieve this by avoiding the greedy assumption commonly made in CVF learning. As importance sampling for such OFF-policy training is both computationally costly and numerically unstable, we proposed to use the ${\lambda }$ -return as a proxy to compute the temporal difference (TD) error. With this new loss function objective, we adopt a modified QMIX network structure as the base to train our model. By further connecting it with the ${Q(\lambda)}$ approach from a unified expectation correction viewpoint, we show that the proposed SMIX( ${\lambda }$ ) is equivalent to ${Q(\lambda)}$ and hence shares its convergence properties, while without being suffered from the aforementioned curse of dimensionality problem inherent in MARL. Experiments on the StarCraft Multiagent Challenge (SMAC) benchmark demonstrate that our approach not only outperforms several state-of-the-art MARL methods by a large margin but also can be used as a general tool to improve the overall performance of other centralized training with decentralized execution (CTDE)-type algorithms by enhancing their CVFs.},
  archive      = {J_TNNLS},
  author       = {Xinghu Yao and Chao Wen and Yuhui Wang and Xiaoyang Tan},
  doi          = {10.1109/TNNLS.2021.3089493},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {52-63},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SMIX(λ): Enhancing centralized value functions for cooperative multiagent reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The wisdom of the crowd: Reliable deep reinforcement
learning through ensembles of q-functions. <em>TNNLS</em>,
<em>34</em>(1), 43–51. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) agents learn by exploring the environment and then exploiting what they have learned. This frees the human trainers from having to know the preferred action or intrinsic value of each encountered state. The cost of this freedom is that RL is slower and more unstable than supervised learning. We explore the possibility that ensemble methods can remedy these shortcomings by investigating a novel technique which harnesses the wisdom of crowds by combining Q-function approximator estimates utilizing a simple combination scheme similar to the supervised learning approach known as bagging. Bagging approaches have not yet found widespread adoption in the RL literature nor has a comprehensive look at its performance been performed. Our results show that the proposed approach improves all three tasks and RL approaches attempted. The primary contribution of this work is a demonstration that the improvement is a direct result of the increased stability of the action portion of the state-action-value function. Subsequent experimentation demonstrates that the stability in learning allows an actor-critic method to find more efficient solutions. Finally we show that this approach can be used to decrease the amount of time necessary to solve problems which require a deep Q-learning (DQN) approach.},
  archive      = {J_TNNLS},
  author       = {Daniel L. Elliott and Charles Anderson},
  doi          = {10.1109/TNNLS.2021.3089425},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {43-51},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The wisdom of the crowd: Reliable deep reinforcement learning through ensembles of Q-functions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ESCNet: An end-to-end superpixel-enhanced change detection
network for very-high-resolution remote sensing images. <em>TNNLS</em>,
<em>34</em>(1), 28–42. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD), as one of the central problems in Earth observation, has attracted a lot of research interest over recent decades. Due to the rapid development of satellite sensors in recent years, we have witnessed an enrichment of the CD source data with the availability of very-high-resolution (VHR) multispectral imagery, which provides abundant change clues. However, precisely locating real changed areas still remains a challenge. In this article, we propose an end-to-end superpixel-enhanced CD network (ESCNet) for VHR images, which combines differentiable superpixel segmentation and a deep convolutional neural network (DCNN). Two weight-sharing superpixel sampling networks (SSNs) are tailored for the feature extraction and superpixel segmentation of bitemporal image pairs. A UNet-based Siamese neural network is then employed to mine the different information. The superpixels are then leveraged to reduce the latent noise in the pixel-level feature maps while preserving the edges, where a novel superpixelation module is used to serve this purpose. Furthermore, to compensate for the dependence on the number of superpixels, we propose an innovative adaptive superpixel merging (ASM) module, which has a concise form and is fully differentiable. A pixel-level refinement module making use of the multilevel decoded features is also appended to the end of the framework. Experiments on two public datasets confirmed the superiority of ESCNet compared to the traditional and state-of-the-art (SOTA) deep learning-based CD (DLCD) methods.},
  archive      = {J_TNNLS},
  author       = {Hongyan Zhang and Manhui Lin and Guangyi Yang and Liangpei Zhang},
  doi          = {10.1109/TNNLS.2021.3089332},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {28-42},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ESCNet: An end-to-end superpixel-enhanced change detection network for very-high-resolution remote sensing images},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relation-aware fine-grained reasoning network for textbook
question answering. <em>TNNLS</em>, <em>34</em>(1), 15–27. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textbook question answering (TQA) is a task that one should answer non-diagram and diagram questions accurately, given a large context which consists of abundant diagrams and essays. Although lots of studies have made significant progress in the natural image question answering (QA), they are not applicable to comprehending diagrams and reasoning over the long multimodal context. To address the above issues, we propose a relation-aware fine-grained reasoning (RAFR) network that performs fine-grained reasoning over the nodes of relation-based diagram graphs. Our method uses semantic dependencies and relative positions between nodes in the diagram to construct relation graphs and applies graph attention networks to learn diagram representations. To extract and reason over the multimodal knowledge, we first extract the text that is the most relevant to questions, options, and the instructional diagram which is the most relevant to question diagrams at the word-sentence level and the node-diagram level, respectively. Then, we apply instructional-diagram-guided attention and question-guided attention to reason over the node of question diagrams, respectively. The experimental results show that our proposed method achieves the best performance on the TQA dataset compared with baselines. We also conduct extensive ablation studies to comprehensively analyze the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jie Ma and Jun Liu and Yaxian Wang and Junjun Li and Tongliang Liu},
  doi          = {10.1109/TNNLS.2021.3089140},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {15-27},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Relation-aware fine-grained reasoning network for textbook question answering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model compression hardens deep neural networks: A new
perspective to prevent adversarial attacks. <em>TNNLS</em>,
<em>34</em>(1), 3–14. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been demonstrating phenomenal success in many real-world applications. However, recent works show that DNN’s decision can be easily misguided by adversarial examples–the input with imperceptible perturbations crafted by an ill-disposed adversary, causing the ever-increasing security concerns for DNN-based systems. Unfortunately, current defense techniques face the following issues: 1) they are usually unable to mitigate all types of attacks, given that diversified attacks, which may occur in practical scenarios, have different natures and 2) most of them are subject to considerable implementation cost such as complete retraining. This prompts an urgent need of developing a comprehensive defense framework with low deployment costs. In this work, we reveal that “defensive decision boundary” and “small gradient” are two critical conditions to ease the effectiveness of adversarial examples with different properties. We propose to wisely use “hash compression” to reconstruct a low-cost “defensive hash classifier” to form the first line of our defense. We then propose a set of retraining-free “gradient inhibition” (GI) methods to extremely suppress and randomize the gradient used to craft adversarial examples. Finally, we develop a comprehensive defense framework by orchestrating “defensive hash classifier” and “GI.” We evaluate our defense across traditional white-box, strong adaptive white-box, and black-box settings. Extensive studies show that our solution can enormously decrease the attack success rate of various adversarial attacks on the diverse dataset.},
  archive      = {J_TNNLS},
  author       = {Qi Liu and Wujie Wen},
  doi          = {10.1109/TNNLS.2021.3089128},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {3-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model compression hardens deep neural networks: A new perspective to prevent adversarial attacks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial happy new year! <em>TNNLS</em>, <em>34</em>(1), 2.
(<a href="https://doi.org/10.1109/TNNLS.2022.3231677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I would like to take this opportunity to sincerely wish you and your loved ones all the best for the holidays and a very happy, healthy, and prosperous New Year of 2023! It has been my greatest honor and privilege to have this opportunity in serving this role. Thank you so much for your trust, support, and encouragement.},
  archive      = {J_TNNLS},
  author       = {Yongduan Song},
  doi          = {10.1109/TNNLS.2022.3231677},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {2},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Editorial happy new year!},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
