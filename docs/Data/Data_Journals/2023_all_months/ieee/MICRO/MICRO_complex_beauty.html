<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MICRO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="micro---83">MICRO - 83</h2>
<ul>
<li><details>
<summary>
(2023d). The AI gold rush. <em>MICRO</em>, <em>43</em>(6), 126–128.
(<a href="https://doi.org/10.1109/MM.2023.3322049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent frenzy of commercial interest in large language models could be compared to a gold rush. We consider the metaphor and where it illuminates the mechanisms shaping business decisions. If the rush indicates the presence of a large long-term opportunity, then expect the supply chain of equipment and software to develop to support it.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2023.3322049},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {126-128},
  shortjournal = {IEEE Micro},
  title        = {The AI gold rush},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fifty years of the international symposium on computer
architecture: A data-driven retrospective. <em>MICRO</em>,
<em>43</em>(6), 109–124. (<a
href="https://doi.org/10.1109/MM.2023.3324465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {2023 marked the fiftieth year of the International Symposium on Computer Architecture (ISCA). As one of the oldest and preeminent computer architecture conferences, ISCA represents a microcosm of the broader community; correspondingly, a 50-year-retrospective offers us a great way to track the impact and evolution of the field. Analyzing the content and impact of all the papers published at ISCA so far, we show how computer architecture research has been at the forefront of advances that have driven the broader computing ecosystem. Decadal trends show a dynamic and rapidly-evolving field, with diverse contributions. Examining how the most highly-cited papers achieve their popularity reveals interesting trends on technology adoption curves and the path to impact. Our data also highlights a growing and thriving community, with interesting insights on diversity and scale. We conclude with a summary of the celebratory panel held at ISCA, with observations on the exciting future ahead.},
  archive      = {J_MICRO},
  author       = {Matthew D. Sinclair and Parthasarathy Ranganathan and Gaurang Upasani and Adrian Sampson and David Patterson and Rutwik Jain and Nidhi Parthasarathy and Shaan Shah},
  doi          = {10.1109/MM.2023.3324465},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {109-124},
  shortjournal = {IEEE Micro},
  title        = {Fifty years of the international symposium on computer architecture: A data-driven retrospective},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part VII:
Relationship between prosecution time and claims. <em>MICRO</em>,
<em>43</em>(6), 103–108. (<a
href="https://doi.org/10.1109/MM.2023.3320318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A previous article in this series showed that the correlation between the prosecution time and the number of claims was relatively low. This article further analyzes that correlation by examining the effect that patent class has.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2023.3320318},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {103-108},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part VII: Relationship between prosecution time and claims},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hardware–software co-design for real-time latency–accuracy
navigation in tiny machine learning applications. <em>MICRO</em>,
<em>43</em>(6), 93–101. (<a
href="https://doi.org/10.1109/MM.2023.3317243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tiny machine learning (TinyML) applications increasingly operate in dynamically changing deployment scenarios, requiring optimization for both accuracy and latency. Existing methods mainly target a single point in the accuracy/latency tradeoff space, which is insufficient as no single static point can be optimal under variable conditions. We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that activates different SubNets within a SuperNet. This creates an opportunity to exploit the inherent temporal locality of different queries that use the same SuperNet. We propose a hardware–software co-design called SUSHI that introduces a novel SubGraph Stationary optimization. SUSHI consists of a novel field-programmable gate array implementation and a software scheduler that controls which SubNets to serve and which SubGraph to cache in real time. SUSHI yields up to a 32\% improvement in latency, 0.98\% increase in served accuracy, and achieves up to 78.7\% off-chip energy saved across several neural network architectures.},
  archive      = {J_MICRO},
  author       = {Payman Behnam and Jianming Tong and Alind Khare and Yangyu Chen and Yue Pan and Pranav Gadikar and Abhimanyu Bambhaniya and Tushar Krishna and Alexey Tumanov},
  doi          = {10.1109/MM.2023.3317243},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {93-101},
  shortjournal = {IEEE Micro},
  title        = {Hardware–Software co-design for real-time Latency–Accuracy navigation in tiny machine learning applications},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NetDistiller: Empowering tiny deep learning via in situ
distillation. <em>MICRO</em>, <em>43</em>(6), 84–92. (<a
href="https://doi.org/10.1109/MM.2023.3324261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boosting the task accuracy of tiny neural networks (TNNs) has become a fundamental challenge for enabling the deployment of TNNs on edge devices, which are constrained by strict limitations in terms of memory, computation, bandwidth, and power supply. To this end, we propose a framework called NetDistiller to boost the achievable accuracy of TNNs by treating them as subnetworks of a weight-sharing teacher constructed by expanding the number of channels of the TNN. Specifically, the target TNN model is jointly trained with the weight-sharing teacher model via 1) gradient surgery to tackle the gradient conflicts between them and 2) uncertainty-aware distillation to mitigate the overfitting of the teacher model. Extensive experiments across diverse tasks validate NetDistiller’s effectiveness in boosting TNNs’ achievable accuracy over state-of-the-art methods. Our code is available at https://github.com/GATECH-EIC/NetDistiller.},
  archive      = {J_MICRO},
  author       = {Shunyao Zhang and Yonggan Fu and Shang Wu and Jyotikrishna Dass and Haoran You and Yingyan Lin},
  doi          = {10.1109/MM.2023.3324261},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {84-92},
  shortjournal = {IEEE Micro},
  title        = {NetDistiller: Empowering tiny deep learning via in situ distillation},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reg-TuneV2: A hardware-aware and multiobjective
regression-based fine-tuning approach for deep neural networks on
embedded platforms. <em>MICRO</em>, <em>43</em>(6), 74–83. (<a
href="https://doi.org/10.1109/MM.2023.3316433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-tuning deep neural networks (DNNs) for deployment has traditionally relied on computationally intensive methods such as grid searches and neural architecture searches, which may not consider hardware-aware metrics. Moreover, it is essential to consider multiple objectives to develop a range of solutions for tiny machine learning hardware deployment with real-time latency and low power constraints. To address these problems, we propose Reg-TuneV2, a systematic approach to fine-tune DNNs for hardware deployment by considering multiple objectives, including accuracy, power, and latency contours. In addition, this approach uses metric learning to achieve smaller and better-suited configurations for deployment, achieving 90.5\% accuracy with only 340 KB of memory for keyword spotting (KWS) on a field-programmable gate array. When compared to baselines for KWS and image classification on the Nvidia Jetson Nano 4-GB Software Development Kit, the proposed method achieves a 14.5$ \times $× and 101.8$ \times $× reduction in model size coupled with 2.5$ \times $× and 5.9$ \times $× better inference efficiency, respectively.},
  archive      = {J_MICRO},
  author       = {Arnab Neelim Mazumder and Tinoosh Mohsenin},
  doi          = {10.1109/MM.2023.3316433},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {74-83},
  shortjournal = {IEEE Micro},
  title        = {Reg-TuneV2: A hardware-aware and multiobjective regression-based fine-tuning approach for deep neural networks on embedded platforms},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Addressing the gap between training data and deployed
environment by on-device learning. <em>MICRO</em>, <em>43</em>(6),
66–73. (<a href="https://doi.org/10.1109/MM.2023.3314711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of tiny machine learning applications is often affected by various environmental factors, such as noises, location/calibration of sensors, and time-related changes. This article introduces a neural network based on-device learning (ODL) approach to address this issue by retraining in deployed environments. Our approach relies on semisupervised sequential training of multiple neural networks tailored for low-end edge devices. This article introduces its algorithm and implementation on wireless sensor nodes consisting of a Raspberry Pi Pico and low-power wireless modules. Experiments using vibration patterns of rotating machines demonstrate that retraining by ODL improves anomaly detection accuracy compared with a prediction-only deep neural network in a noisy environment. The results also show that the ODL approach can save communication cost and energy consumption for battery-powered Internet of Things devices.},
  archive      = {J_MICRO},
  author       = {Kazuki Sunaga and Masaaki Kondo and Hiroki Matsutani},
  doi          = {10.1109/MM.2023.3314711},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {66-73},
  shortjournal = {IEEE Micro},
  title        = {Addressing the gap between training data and deployed environment by on-device learning},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On-device tiny machine learning for anomaly detection based
on the extreme values theory. <em>MICRO</em>, <em>43</em>(6), 58–65. (<a
href="https://doi.org/10.1109/MM.2023.3316918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significance of anomaly detection is particularly pronounced in Industry 4.0 applications. For instance, in manufacturing, the timely detection of equipment malfunctions can prevent costly downtime and maintain production efficiency. In energy systems, spotting anomalies in power consumption patterns can enhance resource allocation and optimize energy usage. Equally noteworthy is the ascendancy of tiny machine learning (TinyML), emerging as a potent tool for real-time anomaly detection, exemplifying its versatile utility. This work presents an unsupervised on-device learning TinyML algorithm, drawing inspiration from the extreme value theory. The algorithm leverages the two-parameter Weibull distribution function to efficiently identify anomalies within discrete time series data. Optimal hyperparameters are ascertained via grid search methodology. Notably, employing synthetic data with randomized anomalies elucidates the algorithm’s proficiency in binary classification within time series, highlighting an accuracy of 99.80\%, recall of 93.10\%, and F1 score of 96.43\%. The amalgamation of theoretical foundations from the extreme value theory and practical capabilities of TinyML accentuates its pertinence across a broad spectrum of domains.},
  archive      = {J_MICRO},
  author       = {Eduardo S. Pereira and Leonardo S. Marcondes and Josemar M. Silva},
  doi          = {10.1109/MM.2023.3316918},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {58-65},
  shortjournal = {IEEE Micro},
  title        = {On-device tiny machine learning for anomaly detection based on the extreme values theory},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On-device customization of tiny deep learning models for
keyword spotting with few examples. <em>MICRO</em>, <em>43</em>(6),
50–57. (<a href="https://doi.org/10.1109/MM.2023.3311826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing a customized keyword spotting (KWS) deep neural network (DNN) for tiny sensors is a time-consuming process, demanding training a new model on a remote server with a dataset of collected keywords. This article investigates the effectiveness of a DNN-based KWS classifier that can be initialized on-device simply by recording a few examples of the target commands. At runtime, the classifier computes the distance between the DNN output and the prototypes of the recorded keywords. By experimenting with multiple tiny machine learning models on the Google Speech Command dataset, we report an accuracy of up to 80\% using only 10 examples of utterances not seen during training. When deployed on a multicore microcontroller with a power envelope of 25 mW, the most accurate ResNet15 model takes 9.7 ms to process a 1-s speech frame, demonstrating the feasibility of on-device KWS customization for tiny devices without requiring any backpropagation-based transfer learning.},
  archive      = {J_MICRO},
  author       = {Manuele Rusci and Tinne Tuytelaars},
  doi          = {10.1109/MM.2023.3311826},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {50-57},
  shortjournal = {IEEE Micro},
  title        = {On-device customization of tiny deep learning models for keyword spotting with few examples},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring memory-oriented design optimization of edge AI
hardware for extended reality applications. <em>MICRO</em>,
<em>43</em>(6), 40–49. (<a
href="https://doi.org/10.1109/MM.2023.3321249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-power edge AI capabilities are essential for on-device extended reality (XR) applications to support the vision of the metaverse. In this work, we investigate two representative XR workloads, 1) hand detection and 2) eye segmentation, for hardware design space exploration. For both applications, we train deep neural networks and analyze the impact of quantization and hardware-specific bottlenecks. Through simulations, we evaluate a CPU and two systolic inference accelerator implementations. Next, we compare these hardware solutions with advanced technology nodes. The impact of integrating state-of-the-art emerging nonvolatile memory (NVM) technology (STT-/SOT-/VGSOT-MRAM) into the XR–AI inference pipeline is evaluated. We found that significant energy benefits ($ \geq $≥24\%) can be achieved for hand detection [inferences per second (IPS) = 10] and eye segmentation (IPS = 0.1) by introducing NVM into the memory hierarchy for designs at the 7-nm node while meeting minimum IPS values. Moreover, we can realize a substantial reduction in area ($ \geq $≥30\%) owing to the small form factor of MRAM.},
  archive      = {J_MICRO},
  author       = {Vivek Parmar and Syed Shakib Sarwar and Ziyun Li and Hsien-Hsin S. Lee and Barbara De Salvo and Manan Suri},
  doi          = {10.1109/MM.2023.3321249},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {40-49},
  shortjournal = {IEEE Micro},
  title        = {Exploring memory-oriented design optimization of edge AI hardware for extended reality applications},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MetaE2RL: Toward meta-reasoning for energy-efficient
multigoal reinforcement learning with squeezed-edge you only look once.
<em>MICRO</em>, <em>43</em>(6), 29–39. (<a
href="https://doi.org/10.1109/MM.2023.3318200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-reasoning shows promise in efficiently using the computational resources of tiny edge devices while performing highly computationally intensive reinforcement learning (RL) algorithms. We propose meta-reasoning for energy efficiency of multigoal RL, a hardware-aware framework that incorporates low-power preprocessing solutions and meta-reasoning to enable deployment of multigoal RL on tiny autonomous devices. For this aim, a meta-level is proposed to allocate resources efficiently in real time by switching between models with different complexities. Moreover, squeezed-edge you only look once (YOLO) is proposed for energy-efficient object detection in the preprocessing phase. For the experimental results, the proposed squeezed-edge YOLO was deployed on board a tiny drone named Crazyflie with a GAP8 processor that includes eight parallel RISC-V cluster cores. We compared latency and power consumption of squeezed-edge YOLO and a lighter convolutional neural network (CNN)-based model while deploying them separately on board on GAP8. The experimental results show squeezed-edge YOLO is 8× smaller than previous work and consumes 541 mW on GAP8 with inference latency of 130 ms.},
  archive      = {J_MICRO},
  author       = {Mozhgan Navardi and Edward Humes and Tejaswini Manjunath and Tinoosh Mohsenin},
  doi          = {10.1109/MM.2023.3318200},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {29-39},
  shortjournal = {IEEE Micro},
  title        = {MetaE2RL: Toward meta-reasoning for energy-efficient multigoal reinforcement learning with squeezed-edge you only look once},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A 10.7-µJ/frame 88% accuracy CIFAR-10 single-chip
neuromorphic field-programmable gate array processor featuring various
nonlinear functions of dendrites in the human cerebrum. <em>MICRO</em>,
<em>43</em>(6), 19–27. (<a
href="https://doi.org/10.1109/MM.2023.3315676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A neuromorphic architecture is suitable for low-power tiny-machine learning processors. However, the large number of synapses utilized in recent deep neural networks require multichip implementation, resulting in large power consumption due to chip-to-chip interfaces. Here, we present a 10.7-µJ/frame single-chip neuromorphic field-programmable gate array (FPGA) processor. To reduce the required hardware (HW) resources, we have developed two techniques. The first is a dendrite-inspired nonlinear neural network that mimics various nonlinear functions of dendrite spines in the human cerebrum. The second is a line-scan-based architecture that reduces the total amount of HW resources. The 14-layer convolutional neural network (CNN), which achieves an 88\% accuracy with the CIFAR-10 dataset, was implemented on a single FPGA board. Compared to a state-of-the-art spiking CNN-based neuromorphic FPGA processor, the energy efficiency of the proposed architecture is improved by a factor of 94.4 while achieving a 6\% better classification accuracy.},
  archive      = {J_MICRO},
  author       = {Atsutake Kosuge and Yao-Chung Hsu and Rei Sumikawa and Mototsugu Hamada and Tadahiro Kuroda and Tomoe Ishikawa},
  doi          = {10.1109/MM.2023.3315676},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {19-27},
  shortjournal = {IEEE Micro},
  title        = {A 10.7-µJ/Frame 88\% accuracy CIFAR-10 single-chip neuromorphic field-programmable gate array processor featuring various nonlinear functions of dendrites in the human cerebrum},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Making machine learning more energy efficient by bringing it
closer to the sensor. <em>MICRO</em>, <em>43</em>(6), 11–18. (<a
href="https://doi.org/10.1109/MM.2023.3316348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing data close to the sensor on a low-cost, low-power embedded device has the potential to unlock new areas for machine learning (ML). Whether it is possible to deploy such ML applications or not depends on the energy efficiency of the solution. One way to realize lower energy consumption is to bring the application as close as possible to the sensor. We demonstrate the concept of transforming an ML application running near the sensor into a hybrid near-sensor in-sensor application. This approach aims to reduce overall energy consumption and we showcase it using a motion classification example, which can be considered a simpler subproblem of activity recognition. The reduction of energy consumption is achieved by combining a convolutional neural network with a decision tree. Both applications are compared in terms of accuracy and energy consumption, illustrating the benefits of the hybrid approach.},
  archive      = {J_MICRO},
  author       = {Marius Brehler and Lucas Camphausen and Benjamin Heidebroek and Dennis Krön and Henri Gründer and Simon Camphausen},
  doi          = {10.1109/MM.2023.3316348},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {11-18},
  shortjournal = {IEEE Micro},
  title        = {Making machine learning more energy efficient by bringing it closer to the sensor},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on TinyML. <em>MICRO</em>, <em>43</em>(6),
7–10. (<a href="https://doi.org/10.1109/MM.2023.3322048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This IEEE Micro special issue on tiny machine learning (TinyML) explores cutting-edge research on optimizing machine learning models for highly resource-constrained devices like microcontrollers and embedded systems. The articles cover techniques across the full TinyML stack, including efficient neural network design, on-device learning, model compression, hardware–software co-design, and specialized applications. These selected works showcase techniques to enable increasingly sophisticated intelligence on low-power, memory-constrained edge devices. They provide valuable insights to overcome challenges in deploying performant yet compact TinyML solutions that can perceive, reason, and interact intelligently, even at the very edge.},
  archive      = {J_MICRO},
  author       = {Vijay Janapa Reddi and Boris Murmann},
  doi          = {10.1109/MM.2023.3322048},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {7-10},
  shortjournal = {IEEE Micro},
  title        = {Special issue on TinyML},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). TinyML but by no means a tiny feat! <em>MICRO</em>,
<em>43</em>(6), 4–6. (<a
href="https://doi.org/10.1109/MM.2023.3322910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the EIC introduces the Special Issue on TinyML and bids farewell to the readers, editors, and staff. A collage with the cover pages of IEEE Micro for the last five years is included.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2023.3322910},
  journal      = {IEEE Micro},
  month        = {11},
  number       = {6},
  pages        = {4-6},
  shortjournal = {IEEE Micro},
  title        = {TinyML but by no means a tiny feat!},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Interview with ronnie chatterji, coordinator for the
creating helpful incentives to produce semiconductors and science act.
<em>MICRO</em>, <em>43</em>(5), 98–100. (<a
href="https://doi.org/10.1109/MM.2023.3297868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This short article prints an interview with Ronnie Chatterji, the CHIPS and Science Act coordinator. It reviews the Act&#39;s main provisions, which will shape the work experience of many IEEE members. The interview concentrates on its anticipated impact on the semiconductor industry and the principal economic policy challenges.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2023.3297868},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {98-100},
  shortjournal = {IEEE Micro},
  title        = {Interview with ronnie chatterji, coordinator for the creating helpful incentives to produce semiconductors and science act},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). E-booster: A field-programmable gate array-based accelerator
for secure tree boosting using additively homomorphic encryption.
<em>MICRO</em>, <em>43</em>(5), 88–96. (<a
href="https://doi.org/10.1109/MM.2023.3293845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree boosting is a widely used machine learning model in many financial fields. Additively homomorphic encryption is an important cryptographic tool used for secure tree boosting in the setting of federated learning. However, homomorphic encryption includes computationally expensive operations. Current frameworks for secure tree boosting are extremely slow. In this article, we propose E-Booster, a novel accelerator for the training of secure tree boosting. E-Booster can fully exploit algorithmic superiority and architectural optimization to achieve unprecedented performance, and to address the obstacle in deploying additively homomorphic encryption in industrial applications. E-Booster has been implemented on an Intel Agilex field-programmable gate array and evaluated on four public datasets. It achieves a 5.1–7.8-times speedup over a CPU with 32 threads for secure tree boosting. To the best of our knowledge, E-Booster is the first additively homomorphic encryption accelerator that can be applied to industrial secure tree boosting.},
  archive      = {J_MICRO},
  author       = {Guiming Wu and Qianwen He and Jiali Jiang and Zhenxiang Zhang and Yunfeng Shi and Xin Long and Linquan Jiang and Shuangchen Li and Yuan Xie and Changzheng Wei and Yuan Zhao and Ying Yan and Hui Zhang and Yinchao Zou},
  doi          = {10.1109/MM.2023.3293845},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {88-96},
  shortjournal = {IEEE Micro},
  title        = {E-booster: A field-programmable gate array-based accelerator for secure tree boosting using additively homomorphic encryption},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The intel programmable and integrated unified memory
architecture graph analytics processor. <em>MICRO</em>, <em>43</em>(5),
78–87. (<a href="https://doi.org/10.1109/MM.2023.3295848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance large-scale graph analytics are essential to timely analyze relationships in big datasets. Conventional processor architectures suffer from inefficient resource usage and bad scaling on those workloads. To enable efficient and scalable graph analysis, Intel developed the Programmable Integrated Unified Memory Architecture (PIUMA) as a part of the DARPA Hierarchical Identify Verify Exploit (HIVE) program. PIUMA consists of many multithreaded cores, fine-grained memory and network accesses, a globally shared address space, powerful offload engines, and a tightly integrated optical interconnection network. This article presents the PIUMA architecture and documents our experience in designing and building a prototype chip and its bring-up process. PIUMA silicon has successfully powered on demonstrating key aspects of the architecture, some of which will be incorporated into future Intel products.},
  archive      = {J_MICRO},
  author       = {Sriram Aananthakrishnan and Shamsul Abedin and Vincent Cavé and Fabio Checconi and Kristof Du Bois and Stijn Eyerman and Joshua B. Fryman and Wim Heirman and Jason Howard and Ibrahim Hur and Samkit Jain and Marek M. Landowski and Kevin Ma and Jarrod A. Nelson and Robert Pawlowski and Fabrizio Petrini and Sebastian Szkoda and Sanjaya Tayal and Jesmin Jahan Tithi and Yves Vandriessche},
  doi          = {10.1109/MM.2023.3295848},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {78-87},
  shortjournal = {IEEE Micro},
  title        = {The intel programmable and integrated unified memory architecture graph analytics processor},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A golden-free approach to detect trojans in COTS multi-PCB
systems. <em>MICRO</em>, <em>43</em>(5), 64–76. (<a
href="https://doi.org/10.1109/MM.2023.3300713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Untrusted third parties in commercial-off-the-shelf (COTS) printed circuit board (PCB) supply chains may poison PCBs with hardware, firmware, and software implants. Hence, we focus on detection of malicious implants in PCBs. State-of-the-art hardware Trojan detection methods require a golden PCB system/model to detect malicious implants and do not scale to large-scale COTS PCB systems. We map a COTS PCB system to a graph and propose a golden-free methodology comprising a graph-based mathematical construction on “node” and “edge” equivalences, and clustering of identical nodes and paths and validation of hypothesized statistical properties on measured sidechannel data. We evaluate the methodology on a multi-PCB testbed with hierarchically networked PCB devices and several types of Trojans.},
  archive      = {J_MICRO},
  author       = {Animesh Basak Chowdhury and Anushree Mahapatra and Yang Liu and Prashanth Krishnamurthy and Farshad Khorrami and Ramesh Karri},
  doi          = {10.1109/MM.2023.3300713},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {64-76},
  shortjournal = {IEEE Micro},
  title        = {A golden-free approach to detect trojans in COTS multi-PCB systems},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating finite field arithmetic for homomorphic
encryption on GPUs. <em>MICRO</em>, <em>43</em>(5), 55–63. (<a
href="https://doi.org/10.1109/MM.2023.3253052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully homomorphic encryption (FHE) is a rapidly developing technology that enables computation directly on encrypted data, making it a compelling solution for security in cloud-based systems. In addition, modern FHE schemes are believed to be resistant to quantum attacks. Although FHE offers unprecedented potential for security, current implementations suffer from prohibitively high latency. Finite field arithmetic operations, particularly the multiplication of high-degree polynomials, are key computational bottlenecks. The parallel processing capabilities provided by modern GPUs make them compelling candidates to target these highly parallelizable workloads. In this article, we discuss methods to accelerate polynomial multiplication with GPUs, with the goal of making FHE practical.},
  archive      = {J_MICRO},
  author       = {Neal Livesay and Gilbert Jonatan and Evelio Mora and Kaustubh Shivdikar and Rashmi Agrawal and Ajay Joshi and José L. Abellán and John Kim and David Kaeli},
  doi          = {10.1109/MM.2023.3253052},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {55-63},
  shortjournal = {IEEE Micro},
  title        = {Accelerating finite field arithmetic for homomorphic encryption on GPUs},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). XCRYPT: Accelerating lattice-based cryptography with
memristor crossbar arrays. <em>MICRO</em>, <em>43</em>(5), 45–54. (<a
href="https://doi.org/10.1109/MM.2023.3248080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article makes a case for accelerating lattice-based postquantum cryptography with memristor-based crossbars. We map the polynomial multiplications in a representative algorithm, SABER, and show that analog dot products can yield 1.7–32.5× performance and energy efficiency improvement compared to recent hardware proposals. We introduce several additional techniques to address the bottlenecks in this initial design. First, we show that software techniques used in SABER that are effective on central processing unit platforms are unhelpful in crossbars. Relying on simpler algorithms further improves our efficiency by 1.3–3.6×. Second, modular arithmetic in SABER offers an opportunity to drop most significant bits, enabling techniques that exploit a few variable-precision analog-to-digital converters (ADCs) and yielding up to 1.8× higher efficiency. Third, to further reduce ADC pressure, we propose a simple analog shift-and-add technique, demonstrating a 1.3–6.3× improvement. Overall, the Xbar-based accelerator for postquantum cryptography (called XCRYPT) achieves 3–15× higher efficiency over the initial design and highlights the importance of algorithm–accelerator co-design.},
  archive      = {J_MICRO},
  author       = {Sarabjeet Singh and Xiong Fan and Ananth Krishna Prasad and Lin Jia and Anirban Nag and Rajeev Balasubramonian and Mahdi Nazm Bojnordi and Elaine Shi},
  doi          = {10.1109/MM.2023.3248080},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {45-54},
  shortjournal = {IEEE Micro},
  title        = {XCRYPT: Accelerating lattice-based cryptography with memristor crossbar arrays},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial attacks against machine learning-based resource
provisioning systems. <em>MICRO</em>, <em>43</em>(5), 35–44. (<a
href="https://doi.org/10.1109/MM.2023.3267481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microarchitectural attacks, such as side-channel, exploit shared resources to leak sensitive information. Performing microarchitectural attacks on the cloud is possible once the attacker’s virtual machine (VM) is co-located with the victim’s VM. Hence, the co-location requirement with the victim limits the practicality of microarchitectural attacks on the cloud. In this work, we demonstrate that resource provisioning systems (RPSs) can be exploited to solve the co-location challenge of microarchitectural attacks in the cloud by deploying adversarial evasion attacks on RPSs to co-locate attackers’ VMs with victims’ VMs. Moreover, we discuss the adaptability of defense techniques proposed against adversarial attacks in the image classification domain on the RPSs.},
  archive      = {J_MICRO},
  author       = {Najmeh Nazari and Hosein Mohammadi Makrani and Chongzhou Fang and Behnam Omidi and Setareh Rafatirad and Hossein Sayadi and Khaled N. Khasawneh and Houman Homayoun},
  doi          = {10.1109/MM.2023.3267481},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {35-44},
  shortjournal = {IEEE Micro},
  title        = {Adversarial attacks against machine learning-based resource provisioning systems},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploration of bitflip’s effect on deep neural network
accuracy in plaintext and ciphertext. <em>MICRO</em>, <em>43</em>(5),
24–34. (<a href="https://doi.org/10.1109/MM.2023.3273115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) are increasingly deployed to solve complex classification problems and produce accurate results on reliable systems. However, their accuracy quickly degrades in the presence of bit flips from memory errors or targeted attacks on dynamic random-access main memory. Prior work has shown that a few bit errors significantly reduce NN accuracies, but it is unclear which bits have an outsized impact on network accuracy and why. This article first investigates the relationship of the number representation for NN parameters with the impacts of bit flips on NN accuracy. We then explore the bit flip detection framework— four software-based error detectors that detect bit flips independent of NN topology. We discuss exciting findings and evaluate the various detectors’ efficacy, characteristics, and tradeoffs.},
  archive      = {J_MICRO},
  author       = {Kyle Thomas and Muhammad Santriaji and David Mohaisen and Yan Solihin},
  doi          = {10.1109/MM.2023.3273115},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {24-34},
  shortjournal = {IEEE Micro},
  title        = {Exploration of bitflip’s effect on deep neural network accuracy in plaintext and ciphertext},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Persistent memory security threats to interprocess
isolation. <em>MICRO</em>, <em>43</em>(5), 16–23. (<a
href="https://doi.org/10.1109/MM.2023.3264938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent memory object (PMO) is a general system abstraction for holding persistent data in persistent main memory, managed by an operating system. A PMO programming model breaks interprocess isolation as it results in the sharing of persistent data between two processes as they alternatively access the same PMO. In this article, we discuss security implications of a PMO model. We demonstrate that the model enables one process to affect execution of another process, even without sharing a PMO over time. This allows an adversary to launch inter-PMO security attacks if two processes are linked via other unshared PMOs. We present formalization of inter-PMO attacks, their examples, and potential strategies to defend against them.},
  archive      = {J_MICRO},
  author       = {Naveed Ul Mustafa and Yan Solihin},
  doi          = {10.1109/MM.2023.3264938},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {16-23},
  shortjournal = {IEEE Micro},
  title        = {Persistent memory security threats to interprocess isolation},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding and characterizing side channels exploiting
phase-change memories. <em>MICRO</em>, <em>43</em>(5), 8–15. (<a
href="https://doi.org/10.1109/MM.2023.3238894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in nonvolatile memory (NVM), together with their performance-optimized architectural schemes, position NVMs as promising building blocks for future main memory. However, the security of such techniques has not been explored. This article performs the first study on information leakage threats in phase-change memories (PCM). We propose an attack framework, read-saw (R-SAW), that systematically investigates side channel vulnerabilities in representative read techniques under interline and intraline interleaving for multilevel cells. Our evaluation shows that the new side channels can accurately leak program secrets (e.g., crypto keys) and are extremely robust to noise. Our work highlights the need to understand microarchitecture security for emerging memory devices.},
  archive      = {J_MICRO},
  author       = {Md Hafizul Islam Chowdhuryy and Rickard Ewetz and Amro Awad and Fan Yao},
  doi          = {10.1109/MM.2023.3238894},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {8-15},
  shortjournal = {IEEE Micro},
  title        = {Understanding and characterizing side channels exploiting phase-change memories},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on security and privacy-preserving execution
environments. <em>MICRO</em>, <em>43</em>(5), 6–7. (<a
href="https://doi.org/10.1109/MM.2023.3302730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This Special Issue on Security and Privacy-Preserving Execution Environments highlights emerging security and privacy challenges and explores the novel computer architectures that effectively address these issues. The articles cover a wide-ranging set of topics, including security of neural networks, nonvolatile and persistent memory technologies, acceleration of postquantum cryptography (PQC) and homomorphic encryption (HE), and detection of trojan circuits without golden models, to name a few. The articles present a vivid illustration of the newer threat vectors as well as outline opportunities to fortify computer architectures against attackers targeting system security and privacy.},
  archive      = {J_MICRO},
  author       = {Guru Venkataramani},
  doi          = {10.1109/MM.2023.3302730},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on security and privacy-preserving execution environments},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Hardware security and privacy: Threats and opportunities.
<em>MICRO</em>, <em>43</em>(5), 4–5. (<a
href="https://doi.org/10.1109/MM.2023.3304091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the 2023 September/October issue of IEEE Micro! This month, we bring the Special Issue on Security and Privacy-Preserving Execution Environments and the Special Issue on Commercial Products 2023 to IEEE Micro’s readers.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2023.3304091},
  journal      = {IEEE Micro},
  month        = {9},
  number       = {5},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Hardware security and privacy: Threats and opportunities},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part VI: Relationship
between prosecution time and claims. <em>MICRO</em>, <em>43</em>(4),
119–123. (<a href="https://doi.org/10.1109/MM.2023.3280396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A previous article in this series showed that the average patent prosecution time varies for different companies and depending on the year the patent application was filed. This article analyzes the relationship between the prosecution time and the number of claims.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2023.3280396},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {119-123},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part VI: Relationship between prosecution time and claims},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Architectural CO2 footprint tool: Designing sustainable
computer systems with an architectural carbon modeling tool.
<em>MICRO</em>, <em>43</em>(4), 107–117. (<a
href="https://doi.org/10.1109/MM.2023.3275139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the performance and efficiency optimizations realized by the computer systems and architecture community over the last decades, the dominating source of computing’s carbon footprint is shifting from operational emissions to embodied emissions. These embodied emissions are attributable to hardware manufacturing and infrastructure-related activities. Despite the rising embodied emissions, there is a distinct lack of architectural modeling tools to quantify and optimize the end-to-end carbon footprint of computing. This work proposes the Architectural CO 2 Footprint Tool (ACT), &lt; an architectural carbon footprint modeling framework, to enable carbon characterization and sustainability-driven early design space exploration. Using ACT, we demonstrate that optimizing hardware for carbon yields distinct solutions compared to optimizing for performance and efficiency. We construct use cases based on the three tenets of sustainable design—reduce, reuse, and recycle—to highlight future methods that enable strong performance and efficiency scaling in an environmentally sustainable manner.},
  archive      = {J_MICRO},
  author       = {Udit Gupta and Mariam Elgamal and Gage Hills and Gu-Yeon Wei and Hsien-Hsin S. Lee and David Brooks and Carole-Jean Wu},
  doi          = {10.1109/MM.2023.3275139},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {107-117},
  shortjournal = {IEEE Micro},
  title        = {Architectural CO2 footprint tool: Designing sustainable computer systems with an architectural carbon modeling tool},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward developing high-performance RISC-v processors using
agile methodology. <em>MICRO</em>, <em>43</em>(4), 98–106. (<a
href="https://doi.org/10.1109/MM.2023.3273562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agile chip design methodology has shown promise for sustaining the scaling of computing performance more efficiently. However, the practical application of this methodology has been limited by major obstacles. This article presents MinJie, an open source platform supporting agile hardware development flow. We demonstrate the usage and effectiveness of MinJie by building two generations of XiangShan, an open source RISC-V processor with industry-competitive performance. This article highlights the potential impact of MinJie and XiangShan in advancing the field of agile processor design and development as well as the potential for open source collaboration to democratize the field and drive innovation forward.},
  archive      = {J_MICRO},
  author       = {Yinan Xu and Zihao Yu and Dan Tang and Ye Cai and Dandan Huan and Wei He and Ninghui Sun and Yungang Bao},
  doi          = {10.1109/MM.2023.3273562},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {98-106},
  shortjournal = {IEEE Micro},
  title        = {Toward developing high-performance RISC-V processors using agile methodology},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EyeCoD: Eye tracking system acceleration via FlatCam-based
algorithm and hardware co-design. <em>MICRO</em>, <em>43</em>(4), 88–97.
(<a href="https://doi.org/10.1109/MM.2023.3274736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye tracking has become an essential human–machine interaction modality in virtual reality (VR) and augmented reality (AR) applications requiring high throughput (e.g., more than 240 frames per second), small form factor, and enhanced visual privacy. Existing eye tracking systems have adopted bulky, lens-based cameras, and thus suffer from both a large form factor and high communication cost between the camera and back-end processor. This work presents a camera, algorithm, and accelerator co-designed lensless eye tracking system dubbed EyeCoD, which, to the best of our knowledge, is the first to provide a general, front-end eye tracking solution for AR/VR while satisfying the requirements for both high throughput and smaller form factor. Specifically, EyeCoD integrates system-, algorithm-, and accelerator-level techniques to boost system efficiency without sacrificing eye tracking accuracy. We believe that our EyeCoD system will pave the way for next-generation eye tracking solutions in VR/AR and shed light on future innovations for intelligent imaging systems.},
  archive      = {J_MICRO},
  author       = {Haoran You and Yang Zhao and Cheng Wan and Zhongzhi Yu and Yonggan Fu and Jiayi Yuan and Shang Wu and Shunyao Zhang and Yongan Zhang and Chaojian Li and Vivek Boominathan and Ashok Veeraraghavan and Ziyun Li and Yingyan Celine Lin},
  doi          = {10.1109/MM.2023.3274736},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {88-97},
  shortjournal = {IEEE Micro},
  title        = {EyeCoD: Eye tracking system acceleration via FlatCam-based algorithm and hardware co-design},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IOCost: Block input–output control for containers in
datacenters. <em>MICRO</em>, <em>43</em>(4), 80–87. (<a
href="https://doi.org/10.1109/MM.2023.3277783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource isolation is a requirement in datacenter environments. However, our production experience in Meta’s large-scale datacenters shows that existing input–output (IO) control mechanisms for block storage are inadequate in containerized environments. This article presents IOCost, an IO control solution designed for containerized environments that provides scalable, work-conserving, and low-overhead IO control for heterogeneous storage devices and diverse workloads in datacenters. IOCost performs offline profiling to build a device model and uses it to estimate device occupancy of each IO request. To minimize runtime overhead, it separates IO control into a fast per-IO issue path and a slower periodic planning path. A novel work-conserving budget donation algorithm enables containers to dynamically share unused budget. We have deployed IOCost across Meta’s datacenters comprising millions of machines, upstreamed IOCost to the Linux kernel, and open sourced our device-profiling tools.},
  archive      = {J_MICRO},
  author       = {Tejun Heo and Dan Schatzberg and Andrew Newell and Song Liu and Saravanan Dhakshinamurthy and Iyswarya Narayanan and Josef Bacik and Chris Mason and Chunqiang Tang and Dimitrios Skarlatos},
  doi          = {10.1109/MM.2023.3277783},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {80-87},
  shortjournal = {IEEE Micro},
  title        = {IOCost: Block Input–Output control for containers in datacenters},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online code layout optimizations via OCOLOS. <em>MICRO</em>,
<em>43</em>(4), 71–79. (<a
href="https://doi.org/10.1109/MM.2023.3274758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The processor front end has become an increasingly important bottleneck in recent years due to growing application code footprints, particularly in data centers. Profile-guided optimizations performed by compilers represent a promising approach, as they rearrange code to maximize instruction cache locality and branch prediction efficiency along a relatively small number of hot code paths. However, these optimizations require continuous profiling and rebuilding of applications to ensure that the code layout matches the collected profiles. In this article, we propose Online COde Layout OptimizationS (OCOLOS), the first online code layout optimization system for unmodified applications written in unmanaged languages. OCOLOS allows profile-guided optimization to be performed on a running process instead of being performed offline and requiring the application to be relaunched. Our experiments show that OCOLOS can accelerate MySQL by up to 41\%.},
  archive      = {J_MICRO},
  author       = {Yuxuan Zhang and Tanvir Ahmed Khan and Gilles Pokam and Baris Kasikci and Heiner Litz and Joseph Devietti},
  doi          = {10.1109/MM.2023.3274758},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {71-79},
  shortjournal = {IEEE Micro},
  title        = {Online code layout optimizations via OCOLOS},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HeteroGen: Automatic synthesis of heterogeneous cache
coherence protocols. <em>MICRO</em>, <em>43</em>(4), 62–70. (<a
href="https://doi.org/10.1109/MM.2023.3274993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the two challenges architects face when designing heterogeneous processors with cache-coherent shared memory. First, we introduce HeteroGen, an automated tool for composing clusters of cores, each with its own coherence protocol. Second, we show that the output of HeteroGen conforms to a precisely defined memory consistency model that we call a compound consistency model. We also demonstrate that HeteroGen can correctly fuse a wide range of coherence protocols. Our experiments indicate that protocols generated by HeteroGen perform comparably to a publicly available manually generated heterogeneous protocol.},
  archive      = {J_MICRO},
  author       = {Nicolai Oswald and Vijay Nagarajan and Daniel J. Sorin and Vasilis Gavrielatos and Theo X. Olausson and Reece Carr},
  doi          = {10.1109/MM.2023.3274993},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {62-70},
  shortjournal = {IEEE Micro},
  title        = {HeteroGen: Automatic synthesis of heterogeneous cache coherence protocols},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting residue codes for modern memories.
<em>MICRO</em>, <em>43</em>(4), 53–61. (<a
href="https://doi.org/10.1109/MM.2023.3273489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article shows how residue codes, traditionally used for compute rather than storage error correction, can be applied to memories with surprising results. We show that adapting residue codes to modern memory systems offers a level of error correction comparable to those of traditional schemes, such as Reed–Solomon, but with fewer bits of storage. For instance, our adaptation of residue codes—multiuse error-correcting code (MUSE ECC)—can offer ChipKill protection using approximately 30\% fewer bits. We use the storage gains to hold the metadata needed for emerging security functionality, such as memory tagging, or to provide better detection capabilities against Rowhammer attacks. In a system with memory tagging and MUSE, we achieve a 12\% reduction in memory bandwidth utilization with the same error correction level as a traditional ECC baseline and without a noticeable performance loss. Thus, our work demonstrates a new, flexible primitive for co-designing reliability with security and performance.},
  archive      = {J_MICRO},
  author       = {Evgeny Manzhosov and Adam Hastings and Meghna Pancholi and Ryan Piersma and Mohamed Tarek Ibn Ziad and Simha Sethumadhavan},
  doi          = {10.1109/MM.2023.3273489},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {53-61},
  shortjournal = {IEEE Micro},
  title        = {Revisiting residue codes for modern memories},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective mimicry of bélády’s MIN policy. <em>MICRO</em>,
<em>43</em>(4), 45–52. (<a
href="https://doi.org/10.1109/MM.2023.3275079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the Mockingjay cache replacement policy, which advances the state of the art by producing single-core performance that approaches that of Bélády’s MIN policy, and which does so with a surprisingly simple solution. The basic idea is to predict a line’s reuse distance so that when a line is inserted into the last-level cache, its estimated time of arrival (ETA) can be computed. The lines are then evicted based on their ETA ordering. This article describes the Mockingjay policy and explains the key ideas that make it successful.},
  archive      = {J_MICRO},
  author       = {Ishan Shah and Akanksha Jain and Calvin Lin},
  doi          = {10.1109/MM.2023.3275079},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {45-52},
  shortjournal = {IEEE Micro},
  title        = {Effective mimicry of bélády’s MIN policy},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revizor: Testing black-box CPUs against speculation
contracts. <em>MICRO</em>, <em>43</em>(4), 37–44. (<a
href="https://doi.org/10.1109/MM.2023.3273009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speculative execution attacks such as Spectre and Meltdown exploit microarchitectural optimizations to leak information across security domains. These vulnerabilities often stay undetected for years because we lack the tools for systematic analysis of CPUs to find them. In this article, we introduce such a tool, called Revizor, which automatically detects microarchitectural leakage in black-box CPUs. The key idea is to employ speculation contracts to model the expected information leaks, and then to use randomized testing to compare the CPU’s leakage against the model and thus detect unexpected leaks. We showcase the effectiveness of this approach on Intel CPUs, where we demonstrate that Revizor is capable of detecting both known and previously unknown speculative leaks.},
  archive      = {J_MICRO},
  author       = {Oleksii Oleksenko and Christof Fetzer and Boris Köpf and Mark Silberstein},
  doi          = {10.1109/MM.2023.3273009},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {37-44},
  shortjournal = {IEEE Micro},
  title        = {Revizor: Testing black-box CPUs against speculation contracts},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). There’s always a bigger fish: A clarifying analysis of a
machine-learning-assisted side-channel attack. <em>MICRO</em>,
<em>43</em>(4), 28–36. (<a
href="https://doi.org/10.1109/MM.2023.3273457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has made it possible to mount powerful attacks through side channels that are otherwise challenging to exploit. However, due to the black-box nature of machine learning models, these attacks can be difficult to interpret correctly. Models that simply find correlations cannot be used to analyze the various sources of information leakage behind an attack. This article highlights the limitations of relying on machine learning for side-channel attacks without completing a comprehensive security analysis. We show that a state-of-the-art website-fingerprinting attack powered by machine learning was only partially analyzed. Its authors were misled into believing their attack exploited a cache-based side channel when it actually exploited an interrupt-based side channel. We demonstrate this through a comprehensive analysis, in which we run controlled experiments to rule out alternative hypotheses about the attack’s primary source of leakage, and ultimately instrument the attack’s code to prove our hypothesis.},
  archive      = {J_MICRO},
  author       = {Jack Cook and Jules Drean and Jonathan Behrens and Mengjia Yan},
  doi          = {10.1109/MM.2023.3273457},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {28-36},
  shortjournal = {IEEE Micro},
  title        = {There’s always a bigger fish: A clarifying analysis of a machine-learning-assisted side-channel attack},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hertzbleed: Turning power side-channel attacks into remote
timing attacks on x86. <em>MICRO</em>, <em>43</em>(4), 19–27. (<a
href="https://doi.org/10.1109/MM.2023.3274619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power side-channel attacks exploit data-dependent variations in a CPU’s power consumption to leak secrets. In this article, we show that on modern CPUs, power side-channel attacks can be turned into timing attacks that can be mounted without access to any power measurement interface. This discovery exploits how, under certain circumstances, the dynamic frequency scaling of modern x86 CPU depends on the current power consumption (and hence, data). We demonstrate that this “frequency side channel” is a real threat to the security of cryptographic software. First, we reverse engineer the dependency between data, power, and frequency on a modern x86 CPU—finding, among other things, that differences as small as a set bit’s position in a word can be distinguished through frequency changes. Second, we describe a novel chosen-ciphertext attack against (constant-time implementations of) supersingular isogeny key encapsulation that allows full key extraction via remote timing.},
  archive      = {J_MICRO},
  author       = {Yingchen Wang and Riccardo Paccagnella and Elizabeth Tang He and Hovav Shacham and Christopher W. Fletcher and David Kohlbrenner},
  doi          = {10.1109/MM.2023.3274619},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {19-27},
  shortjournal = {IEEE Micro},
  title        = {Hertzbleed: Turning power side-channel attacks into remote timing attacks on x86},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PACMAN: Attacking ARM pointer authentication with
speculative execution. <em>MICRO</em>, <em>43</em>(4), 11–18. (<a
href="https://doi.org/10.1109/MM.2023.3273189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory corruption vulnerabilities have resulted in numerous exploits and significant damage to computer systems. ARM Pointer Authentication is a memory corruption mitigation that attempts to mitigate these issues by cryptographically signing pointers at runtime. We present PACMAN, a novel attack methodology that can forge correct pointer signatures and bypass the protection of pointer authentication without causing any crashes using microarchitectural side channels. PACMAN removes the primary barrier to conducting control-flow hijacking attacks on a platform protected with pointer authentication. We built two proof-of-concept attacks showing that PACMAN works across privilege levels on the Apple M1 CPU. We have also released a suite of open-source tools to enable the community to perform future research on Apple Silicon devices.},
  archive      = {J_MICRO},
  author       = {Joseph Ravichandran and Weon Taek Na and Jay Lang and Mengjia Yan},
  doi          = {10.1109/MM.2023.3273189},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {11-18},
  shortjournal = {IEEE Micro},
  title        = {PACMAN: Attacking ARM pointer authentication with speculative execution},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on top picks from the 2022 computer
architecture conferences. <em>MICRO</em>, <em>43</em>(4), 6–10. (<a
href="https://doi.org/10.1109/MM.2023.3278069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every year, IEEE Micro publishes a special issue that recognizes the most significant research outcomes in computer architecture in terms of novelty and potential for long-term impact. Continuing this tradition, the 2022 selection committee identified 12 articles as top picks from the 2022 computer architecture conferences and another 12 articles as honorable mentions. The articles in this special issue span diverse areas in computer architecture including hardware security, memory systems, data center and cloud computing, real prototype systems, and sustainability.},
  archive      = {J_MICRO},
  author       = {Christopher Batten and Jae W. Lee},
  doi          = {10.1109/MM.2023.3278069},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {6-10},
  shortjournal = {IEEE Micro},
  title        = {Special issue on top picks from the 2022 computer architecture conferences},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023f). Top picks from computer architecture conferences!
<em>MICRO</em>, <em>43</em>(4), 4–5. (<a
href="https://doi.org/10.1109/MM.2023.3278128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue, IEEE Micro is presenting to you the Top Picks issue with 12 selected papers from all of the computer architecture conference papers of 2022. The Top Picks articles belong to five themes: 1) security, 2) memory systems, 3) data center and cloud computing, 4) building real systems, and 5) sustainability.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2023.3278128},
  journal      = {IEEE Micro},
  month        = {7},
  number       = {4},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Top picks from computer architecture conferences!},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Bank runs without the wisdom of the crowds. <em>MICRO</em>,
<em>43</em>(3), 86–88. (<a
href="https://doi.org/10.1109/MM.2023.3263340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual runs have appeared recently. Virtual runs resemble historical bank runs in their human elements. Panicked crowds play a role, as do strong incentives to act early. This article explains why these runs frighten experts and outline some key open issues.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2023.3263340},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {86-88},
  shortjournal = {IEEE Micro},
  title        = {Bank runs without the wisdom of the crowds},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A mobile 3-d object recognition processor with
deep-learning-based monocular depth estimation. <em>MICRO</em>,
<em>43</em>(3), 74–82. (<a
href="https://doi.org/10.1109/MM.2023.3255502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 3-D object recognition system is a heavy task that consumes high sensor power and requires complex 3-D data processing. In this article, the proposed processor produces 3-D red, green, blue, and depth (RGB-D) data from an RGB image through a deep learning-based monocular depth estimation, and then its RGB-D data are sporadically calibrated with low-resolution depth data from a low-power depth sensor, lowering the sensor power by 27.3 times. Then, the proposed processor accelerates various convolution operations in the system by integrating the in-out skipping-based bit-slice-level computing processing elements and flexibly allocating workloads considering data properties. Moreover, the point feature (PF) aggregator is designed close to the global memory to support the PF reuse algorithm’s data aggregation. Additionally, the window-based search algorithm and its memory management are presented for efficient point processing in the point processing unit. Consequently, the 210-mW and 34-frames-per-second end-to-end 3-D object recognition processor is successfully demonstrated.},
  archive      = {J_MICRO},
  author       = {Dongseok Im and Gwangtae Park and Zhiyong Li and Junha Ryu and Sanghoon Kang and Donghyeon Han and Jinsu Lee and Wonhoon Park and Hankyul Kwon and Hoi-Jun Yoo},
  doi          = {10.1109/MM.2023.3255502},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {74-82},
  shortjournal = {IEEE Micro},
  title        = {A mobile 3-D object recognition processor with deep-learning-based monocular depth estimation},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HALO: A hardware–software co-designed processor for
brain–computer interfaces. <em>MICRO</em>, <em>43</em>(3), 64–72. (<a
href="https://doi.org/10.1109/MM.2023.3258907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–computer interfaces (BCIs) enable direct communication with the brain, providing valuable information about brain function and enabling novel treatment of brain disorders. Our group has been building Hardware Architecture for Low-power BCIs (HALO), a flexible and ultralow-power processing architecture for BCIs. HALO can process up to 46 Mbps of neural data, a significant increase over the interfacing bandwidth achievable by prior BCIs. HALO can also be programmed to support several applications, unlike most prior BCIs. Key to HALO’s effectiveness is a hardware accelerator cluster, where each accelerator operates within its own clock domain. A configurable interconnect connects the accelerators to create data flow pipelines that realize neural signal processing algorithms. We have taped-out our design in a 12-nm CMOS process. The resulting chip runs at 0.88 V, per-accelerator frequencies of 3–180 MHz, and consumes, at most, 5 mW for each signal processing pipeline. Evaluations using electrophysiological data collected from a nonhuman primate confirm HALO’s flexibility and superior performance per watt.},
  archive      = {J_MICRO},
  author       = {Karthik Sriram and Ioannis Karageorgos and Xiayuan Wen and Ján Veselý and Nick Lindsay and Michael Wu and Lenny Khazan and Raghavendra Pradyumna Pothukuchi and Rajit Manohar and Abhishek Bhattacharjee},
  doi          = {10.1109/MM.2023.3258907},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {64-72},
  shortjournal = {IEEE Micro},
  title        = {HALO: A Hardware–Software co-designed processor for Brain–Computer interfaces},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SpeedAI240: A 2-petaflop, 30-teraflops/w at-memory inference
acceleration device with 1456 RISC-v cores. <em>MICRO</em>,
<em>43</em>(3), 58–63. (<a
href="https://doi.org/10.1109/MM.2023.3255864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The speedAI family of devices is Untether AI’s second-generation at-memory compute architecture specifically designed for the acceleration of neural networks. Untether AI achieves state-of-the-art efficiency by minimizing the cost of data movement with its at-memory compute architecture, combined with innovative data types and purpose-built on-chip networking. The result is an industry-best 2000 tera-floating point operations per second (TFLOPS) per chip with an energy efficiency of 30 TFLOPS/W.},
  archive      = {J_MICRO},
  author       = {Martin Snelgrove and Robert Beachler},
  doi          = {10.1109/MM.2023.3255864},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {58-63},
  shortjournal = {IEEE Micro},
  title        = {SpeedAI240: A 2-petaflop, 30-Teraflops/W at-memory inference acceleration device with 1456 RISC-V cores},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The arm morello evaluation platform—validating CHERI-based
security in a high-performance system. <em>MICRO</em>, <em>43</em>(3),
50–57. (<a href="https://doi.org/10.1109/MM.2023.3264676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory safety issues are a persistent source of security vulnerabilities, with conventional architectures and the C/C++ codebase chronically prone to exploitable errors. The Capability Hardware Enhanced RISC Instructions (CHERI) research project has explored a novel architectural approach to ameliorate such issues using unforgeable hardware capabilities to implement pointers. Morello is an Arm experimental platform for evaluation of CHERI in the Arm architecture context to explore its potential for mass-market adoption. This article describes the Morello Evaluation Platform, covering the motivation and functionality of the Morello architectural hardware extensions; their potential for fine-grained memory safety and software compartmentalization; formally proven security properties; impact on the microarchitecture of the high-performance, out-of-order multiprocessor Arm Morello processor; and the software-enablement program by Arm, the University of Cambridge, and Linaro. Together, this allows a wide range of researchers in both industry and academia to explore and assess the Morello platform.},
  archive      = {J_MICRO},
  author       = {Richard Grisenthwaite and Graeme Barnes and Robert N. M. Watson and Simon W. Moore and Peter Sewell and Jonathan Woodruff},
  doi          = {10.1109/MM.2023.3264676},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {50-57},
  shortjournal = {IEEE Micro},
  title        = {The arm morello evaluation Platform—Validating CHERI-based security in a high-performance system},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The AMD 400-g adaptive SmartNIC system on chip: A technology
preview. <em>MICRO</em>, <em>43</em>(3), 40–49. (<a
href="https://doi.org/10.1109/MM.2023.3260186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the AMD 400-G Adaptive Smart Network Interface Card (SmartNIC) system on chip (SoC), a domain-specific architecture for offload acceleration of multitenant cloud network and storage applications. The article introduces the motivating factors behind SmartNIC and data processing unit accelerators in the datacenter. The article then describes how and why the adaptive SoC aims to strike a balance among software processing on embedded processors, fast-path application-specific integrated circuit-like processing on hardened logic, and adaptive and composable processing on an integrated field-programmable gate array. In architecting various subsystems of an SoC, the article describes how the device adapts to varying workloads over time through a dynamic function exchange of programmable logic (PL) and adapts to varying workloads over space by having different hardware execution pipelines executing alongside different PL binaries. The article also describes how security attestation and authentication, firewalls, and cryptographic acceleration permeate the device for a robust, multitenant confidential compute environment.},
  archive      = {J_MICRO},
  author       = {Jaideep Dastidar and David Riddoch and Jason Moore and Steven Pope and Jim Wesselkamper},
  doi          = {10.1109/MM.2023.3260186},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {40-49},
  shortjournal = {IEEE Micro},
  title        = {The AMD 400-G adaptive SmartNIC system on chip: A technology preview},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The microarchitecture of DOJO, tesla’s exa-scale computer.
<em>MICRO</em>, <em>43</em>(3), 31–39. (<a
href="https://doi.org/10.1109/MM.2023.3258906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Tesla-built DOJO system is a scalable solution targeted towards machine learning training applications. It is based on the D1 custom compute chip which packs together 354 independent processors, resulting in 362 TFLOPS of compute and 440 MB of internal static random-access memory storage. While maintaining full programmability, DOJO emphasizes distribution of resources and an extremely high bandwidth interconnect, allowing it to scale from small systems all the way to exaFLOP supercomputers.},
  archive      = {J_MICRO},
  author       = {Emil Talpes and Debjit Das Sarma and Doug Williams and Sahil Arora and Thomas Kunjan and Benjamin Floering and Ankit Jalote and Christopher Hsiong and Chandrasekhar Poorna and Vaidehi Samant and John Sicilia and Anantha Kumar Nivarti and Raghuvir Ramachandran and Tim Fischer and Ben Herzberg and Bill McGee and Ganesh Venkataramanan and Pete Banon},
  doi          = {10.1109/MM.2023.3258906},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {31-39},
  shortjournal = {IEEE Micro},
  title        = {The microarchitecture of DOJO, tesla’s exa-scale computer},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cerebras architecture deep dive: First look inside the
hardware/software co-design for deep learning. <em>MICRO</em>,
<em>43</em>(3), 18–30. (<a
href="https://doi.org/10.1109/MM.2023.3256384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The compute and memory demands for deep learning and machine learning (ML) have increased by several orders of magnitude in just the last couple of years, and there is no end in sight. Traditional improvements in processor performance alone struggle to keep up with the exponential demand. A new chip architecture co-designed with the ML algorithms can be better equipped to satisfy this unprecedented demand and enable the ML workloads of the future. This article describes the Cerebras architecture and how it is designed specifically with this purpose, from the ground up, as a wafer-sized chip to enable emerging extreme-scale ML models. It uses fine-grained data flow compute cores to accelerate unstructured sparsity, distributed static random-access memory for full memory bandwidth to the data paths, and a specially designed on-chip and off-chip interconnect for ML training. With these techniques, the Cerebras architecture provides unique capabilities beyond traditional designs.},
  archive      = {J_MICRO},
  author       = {Sean Lie},
  doi          = {10.1109/MM.2023.3256384},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {18-30},
  shortjournal = {IEEE Micro},
  title        = {Cerebras architecture deep dive: First look inside the Hardware/Software co-design for deep learning},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NVIDIA hopper h100 GPU: Scaling performance. <em>MICRO</em>,
<em>43</em>(3), 9–17. (<a
href="https://doi.org/10.1109/MM.2023.3256796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The H100 Tensor Core GPU is NVIDIA’s latest flagship GPU. It has been designed to provide industry leading performance for high-performance computing, artificial intelligence, and data analytics datacenter workloads. Notable new features include a fourth-generation Tensor Core, new Tensor Memory Accelerator unit, a new CUDA cluster capability, and HBM3 dynamic random-access memory.},
  archive      = {J_MICRO},
  author       = {Jack Choquette},
  doi          = {10.1109/MM.2023.3256796},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {9-17},
  shortjournal = {IEEE Micro},
  title        = {NVIDIA hopper h100 GPU: Scaling performance},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on hot chips 34. <em>MICRO</em>,
<em>43</em>(3), 7–8. (<a
href="https://doi.org/10.1109/MM.2023.3264401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hot Chips conference has served as a leading venue for presenting architectural details of new chips and chip-related technologies, from established industry leaders, startups, and academia. This article presents an introduction to the seven articles that are chosen to be in the IEEE Micro HotChips Special Issue of 2023.},
  archive      = {J_MICRO},
  author       = {Ron Diamant and Krste Asanovic},
  doi          = {10.1109/MM.2023.3264401},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {7-8},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot chips 34},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Hot chips 34 and more! <em>MICRO</em>, <em>43</em>(3), 4–6.
(<a href="https://doi.org/10.1109/MM.2023.3267179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an overview of the special issue that contains papers from the 2022 Hot Chips Symposium. The IEEE Micro Editor-in-Chief also congratulates the 2022 Turing Award winner Dr. Robert Metcalfe and reprints a few quotes from his 1976 paper on Ethernet.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2023.3267179},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {4-6},
  shortjournal = {IEEE Micro},
  title        = {Hot chips 34 and more!},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Butterfly effects and legacies. <em>MICRO</em>,
<em>43</em>(2), 142–144. (<a
href="https://doi.org/10.1109/MM.2023.3242036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reports on the invention of the transistor and examines how it exemplifies the &quot;butterfly effect.&quot; The history of this invention overflows with minutiae and monumental consequences. Or, to put it more prosaically, this invention set off a series of butterfly effects. A butterfly effect arises when an invention creates unintended consequences far removed from the original invention. These effects are named after an old metaphor that describes how a butterfly’s wing flapping in one location causes a ripple in the wind that leads to a tornado somewhere else. Butterfly effects are worth recalling on any seventy-fifth anniversary. In this case, small economic decisions accumulated after inventions, evolving into something different than what motivated the invention in the first place. A hefty dose of managerial foresight and a fair amount of human folly then accumulated. In other words, the creation of the transistor would have improved life so much less if management at Bell Labs had not acted sagely or so much more had contemporaries not screwed up in such quirky ways.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2023.3242036},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {142-144},
  shortjournal = {IEEE Micro},
  title        = {Butterfly effects and legacies},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network-assisted noncontiguous transfers for GPU-aware MPI
libraries. <em>MICRO</em>, <em>43</em>(2), 131–139. (<a
href="https://doi.org/10.1109/MM.2023.3241133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of graphics processing units (GPUs) in accelerating HPC applications is evident by the fact that a large number of supercomputing clusters are GPU enabled. Many of these HPC applications use message passing interface (MPI) as their programming model. These MPI applications frequently exchange data that is noncontiguous in GPU memory. MPI provides derived datatypes (DDTs) to represent such data. Past research on DDTs mainly focused on optimizing the pack–unpack kernels. Modern HCAs are capable of gathering/scattering data from/to noncontiguous GPU memory regions. We propose a low-overhead HCA-assisted scheme to improve the performance of GPU-based noncontiguous exchanges without the GPU-based pack–unpack kernels. We show that the proposed scheme provides up to 2× benefits compared to the existing pack-based scheme at the benchmark level. Furthermore, we show up to 17\% improvement with the SW4Lite application compared to other MPI libraries, such as MVAPICH2-GDR and OpenMPI+UCX.},
  archive      = {J_MICRO},
  author       = {Kaushik Kandadi Suresh and Kawthar Shafie Khorassani and Chen Chun Chen and Bharath Ramesh and Mustafa Abduljabbar and Aamir Shafi and Hari Subramoni and Dhabaleswar K. Panda},
  doi          = {10.1109/MM.2023.3241133},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {131-139},
  shortjournal = {IEEE Micro},
  title        = {Network-assisted noncontiguous transfers for GPU-aware MPI libraries},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monitoring InfiniBand networks to react efficiently to
congestion. <em>MICRO</em>, <em>43</em>(2), 120–130. (<a
href="https://doi.org/10.1109/MM.2023.3241840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current high-performance interconnection networks for high-performance computing and data-center systems incorporate mechanisms to prevent congestion from degrading network performance. Specifically, the popular InfiniBand specification defines a mechanism to reduce the injection rate of the traffic flows contributing to congestion. However, the efficiency of this mechanism depends on the values configured for certain parameters, that may be suitable for some congestion situations but not for others. Therefore, we think that these parameters should be reconfigured dynamically, based on accurate and updated information about the actual status of congestion. For that purpose, we have combined a light-weight platform monitoring tool (LIMITLESS) with the InfiniBand control software (OpenSM), so that the former provides the latter with enhanced knowledge about congestion to appropriately reconfigure the parameters driving the behavior of the congestion-control mechanism. Experiments performed in a real InfiniBand-based cluster confirm that this approach significantly reduces the number of wrong reactions to the congestion-control mechanism.},
  archive      = {J_MICRO},
  author       = {Alberto Cascajo and Gabriel Gomez-Lopez and Jesus Escudero-Sahuquillo and Pedro Javier Garcia and David E. Singh and Francisco Alfaro-Cortés and Francisco J. Quiles and Jesus Carretero},
  doi          = {10.1109/MM.2023.3241840},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {120-130},
  shortjournal = {IEEE Micro},
  title        = {Monitoring InfiniBand networks to react efficiently to congestion},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel and updatable architecture for FPGA-based packet
classification with large-scale rule sets. <em>MICRO</em>,
<em>43</em>(2), 110–119. (<a
href="https://doi.org/10.1109/MM.2023.3238012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a programmable hardware, field-programmable gate array (FPGA) provides more opportunities for algorithmic network packet classification. Despite more than 10 years of research, the most actively investigated pipeline architectures still struggle to support fast rule search and efficient rule update for large-scale rule sets. In this article, we design and implement a novel architecture for multitree-based packet classification on FPGA, where the search and update processes are decoupled. A strategy of multi-processing elements (PEs), parallel search, and serial update is adopted. The parsing of multiple tree search results adopts a modular and hierarchical design, supporting architecture with various tree numbers. In addition, incremental rule updates can be achieved simply by traversing all PEs in one pass, with little and bounded impact on rule searching. Compared with TcbTree, the state-of-the-art updatable classifier, the experimental results on FPGA show that the classification performance of our design improves 3.4× on average for various 100k-scale rule sets.},
  archive      = {J_MICRO},
  author       = {Yao Xin and Wenjun Li and Gaogang Xie and Yang Xu and Yi Wang},
  doi          = {10.1109/MM.2023.3238012},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {110-119},
  shortjournal = {IEEE Micro},
  title        = {A parallel and updatable architecture for FPGA-based packet classification with large-scale rule sets},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Compute express link (CXL): Enabling heterogeneous
data-centric computing with heterogeneous memory hierarchy.
<em>MICRO</em>, <em>43</em>(2), 99–109. (<a
href="https://doi.org/10.1109/MM.2022.3228561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute Express Link is an open industry standard interconnect offering caching and memory semantics on top of peripheral component interconnect-Express, with resource pooling and fabric capabilities. In addition to providing high-bandwidth and low-latency connectivity between host processor and accelerators, smart network interface cards, and memory expansion devices, it also enables resource pooling across multiple systems for scalable, power-efficient, and cost-effective computing. This article delves into the microarchitectural design to deliver power-efficient performance based on our experience designing a Xeon central processing unit and field-programmable gate array with this technology with demonstrated silicon interoperability.},
  archive      = {J_MICRO},
  author       = {Debendra Das Sharma},
  doi          = {10.1109/MM.2022.3228561},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {99-109},
  shortjournal = {IEEE Micro},
  title        = {Compute express link (CXL): Enabling heterogeneous data-centric computing with heterogeneous memory hierarchy},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on hot interconnects 29. <em>MICRO</em>,
<em>43</em>(2), 97–98. (<a
href="https://doi.org/10.1109/MM.2023.3241798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the IEEE Micro Special Issue dedicated to the 29th IEEE Hot Interconnects Symposium (Hot Interconnects 29). Each year Hot Interconnects presents cutting-edge research from industry and academia on the design and implementation of high-performance interconnects. The program of this year’s symposium included invited speakers, panels, tutorials, and peer-reviewer article.},
  archive      = {J_MICRO},
  author       = {Scott Levy},
  doi          = {10.1109/MM.2023.3241798},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {97-98},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot interconnects 29},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Photonic network-on-wafer for multichiplet GPUs.
<em>MICRO</em>, <em>43</em>(2), 86–95. (<a
href="https://doi.org/10.1109/MM.2023.3237927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the photonic network-on-wafer graphics processing unit (GPU) architecture to overcome fundamental limitations in electrical interconnect scaling by implementing the inter-GPU network in a wafer-scale optical interposer. We argue that the photonic-NoW GPU is a scalable architecture, delivering significant performance benefits in a power-efficient manner.},
  archive      = {J_MICRO},
  author       = {Shiqing Zhang and Ziyue Zhang and Mahmood Naderan-Tahan and Hossein SeyyedAghaei and Xin Wang and He Li and Senbiao Qin and Didier Colle and Guy Torfs and Mario Pickavet and Johan Bauwelinck and Günther Roelkens and Lieven Eeckhout},
  doi          = {10.1109/MM.2023.3237927},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {86-95},
  shortjournal = {IEEE Micro},
  title        = {Photonic network-on-wafer for multichiplet GPUs},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). System on a package innovations with universal chiplet
interconnect express (UCIe) interconnect. <em>MICRO</em>,
<em>43</em>(2), 76–85. (<a
href="https://doi.org/10.1109/MM.2023.3235770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universal Chiplet Interconnect Express (UCIe) is an open industry standard interconnect for developing an open chiplet ecosystem, where chiplets from any supplier can be packaged anywhere in an interoperable manner. This article delves into the architectural and protocol aspects that we developed and have been adopted in the UCIe 1.0 specification. We present our results on these aspects based on our implementation studies.},
  archive      = {J_MICRO},
  author       = {Debendra Das Sharma},
  doi          = {10.1109/MM.2023.3235770},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {76-85},
  shortjournal = {IEEE Micro},
  title        = {System on a package innovations with universal chiplet interconnect express (UCIe) interconnect},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Failure tolerant training with persistent memory
disaggregation over CXL. <em>MICRO</em>, <em>43</em>(2), 66–75. (<a
href="https://doi.org/10.1109/MM.2023.3237548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes TrainingCXL that can efficiently process large-scale recommendation datasets in the pool of disaggregated memory while making training fault tolerant with low overhead. To this end, we integrate persistent memory (PMEM) and graphics processing unit (GPU) into a cache-coherent domain as type 2. Enabling Compute Express Link (CXL) allows PMEM to be directly placed in GPU’s memory hierarchy, such that GPU can access PMEM without software intervention. TrainingCXL introduces computing and checkpointing logic near the CXL controller, thereby training data and managing persistency in an active manner. Considering PMEM’s vulnerability, we utilize the unique characteristics of recommendation models and take the checkpointing overhead off the critical path of their training. Finally, TrainingCXL employs an advanced checkpointing technique that relaxes the updating sequence of model parameters and embeddings across training batches. The evaluation shows that TrainingCXL achieves 5.2× training performance improvement and 76\% energy savings, compared to the modern PMEM-based recommendation systems.},
  archive      = {J_MICRO},
  author       = {Miryeong Kwon and Junhyeok Jang and Hanjin Choi and Sangwon Lee and Myoungsoo Jung},
  doi          = {10.1109/MM.2023.3237548},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {66-75},
  shortjournal = {IEEE Micro},
  title        = {Failure tolerant training with persistent memory disaggregation over CXL},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CXL-enabled enhanced memory functions. <em>MICRO</em>,
<em>43</em>(2), 58–65. (<a
href="https://doi.org/10.1109/MM.2022.3229627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The arrival of the Compute Express Link (CXL) protocol is a significant milestone for the systems community. CXL provides a standardized, cache-coherent memory protocol that can be used to attach devices and memory to a system, while maintaining memory coherency with the host processor. CXL enables accelerators (e.g., graphics processing units and data processing units) to both have direct load/store access to the host memory and the ability to make their own on-device memory likewise accessible to the host central processing unit. Because CXL allows technology interposition on the memory data plane, it opens up the possibility of “pushing down” functions into the memory subsystem. In this article, we introduce the concept of enhanced memory functions (EMFs). We then describe two use cases, one prototyped using a field-programmable gate array-based intelligent memory controller platform. Finally, we show initial experimental results indicating that EMFs could present valuable solutions to problems that are difficult to solve within existing computer architectures.},
  archive      = {J_MICRO},
  author       = {David Boles and Daniel Waddington and David A. Roberts},
  doi          = {10.1109/MM.2022.3229627},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {58-65},
  shortjournal = {IEEE Micro},
  title        = {CXL-enabled enhanced memory functions},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memory pooling with CXL. <em>MICRO</em>, <em>43</em>(2),
48–57. (<a href="https://doi.org/10.1109/MM.2023.3237491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute Express Link (CXL) has recently attracted great attention thanks to its excellent hardware heterogeneity management and resource disaggregation capabilities. Even though there is yet no commercially available product or platform integrating CXL into memory pooling, it is expected to make memory resources practically and efficiently disaggregated much better than ever before. In this article, we propose directly accessible memory disaggregation, DirectCXL that straight connects a host processor complex and remote memory resources over CXL’s memory protocol (CXL.mem). Our empirical evaluation shows that DirectCXL exhibits around 7× better performance than remote direct memory access (RDMA)-based memory pooling for diverse real-world workloads.},
  archive      = {J_MICRO},
  author       = {Donghyun Gouk and Miryeong Kwon and Hanyeoreum Bae and Sangwon Lee and Myoungsoo Jung},
  doi          = {10.1109/MM.2023.3237491},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {48-57},
  shortjournal = {IEEE Micro},
  title        = {Memory pooling with CXL},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic capacity service for improving CXL pooled memory
efficiency. <em>MICRO</em>, <em>43</em>(2), 39–47. (<a
href="https://doi.org/10.1109/MM.2023.3237756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute Express Link (CXL) pooled memory is gaining attention from the industry as a viable memory disaggregation solution offering memory expansion and alleviating memory overprovisioning. One essential feature for the efficient use of the pooled memory is to dynamically allocate or release memory from the pool based on hosts’ demands. We refer to this feature dynamic capacity service (DCS). This article introduces one of the industry’s first DCS implementation for CXL pooled memory. We demonstrate fully functional DCS by implementing a field-programmable gate array-based CXL pooled memory prototype and full software stacks. Our experiment shows that DCS can substantially improve system memory utilization by dynamically allocating and releasing memory resources on demand. We also present the lessons learned from the DCS implementation.},
  archive      = {J_MICRO},
  author       = {Minho Ha and Junhee Ryu and Jungmin Choi and Kwangjin Ko and Sunwoong Kim and Sungwoo Hyun and Donguk Moon and Byungil Koh and Hokyoon Lee and Myoungseo Kim and Hoshik Kim and Kyoung Park},
  doi          = {10.1109/MM.2023.3237756},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {39-47},
  shortjournal = {IEEE Micro},
  title        = {Dynamic capacity service for improving CXL pooled memory efficiency},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design tradeoffs in CXL-based memory pools for public cloud
platforms. <em>MICRO</em>, <em>43</em>(2), 30–38. (<a
href="https://doi.org/10.1109/MM.2023.3241586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic random-access memory (DRAM) is a key driver of performance and cost in public cloud servers. At the same time, a significant amount of DRAM is underutilized due to fragmented use across servers. Emerging interconnects such as Compute Express Link (CXL) offers a path toward improving utilization through memory pooling. However, the design space of CXL-based memory systems is large, with key questions around the size, reach, and topology of the memory pool. At the same time, using pools require navigating complex design constraints around performance, virtualization, and management. This article discusses why cloud providers should deploy CXL memory pools, key design constraints, and observations in designing toward practical deployment. We identify configuration examples with significant positive return of investment.},
  archive      = {J_MICRO},
  author       = {Daniel S. Berger and Daniel Ernst and Huaicheng Li and Pantea Zardoshti and Monish Shah and Samir Rajadnya and Scott Lee and Lisa Hsu and Ishwar Agarwal and Mark D. Hill and Ricardo Bianchini},
  doi          = {10.1109/MM.2023.3241586},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {30-38},
  shortjournal = {IEEE Micro},
  title        = {Design tradeoffs in CXL-based memory pools for public cloud platforms},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMT: Software-defined memory tiering for heterogeneous
computing systems with CXL memory expander. <em>MICRO</em>,
<em>43</em>(2), 20–29. (<a
href="https://doi.org/10.1109/MM.2023.3240774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of data-intensive technologies has driven an increasing demand for new architectural solutions with scalable, composable, and coherent computing environments. Compute Express Link (CXL), an open-standard interconnect protocol, overcomes architectural limitations by efficiently expanding memory capacity and bandwidth. In this work, we propose a power-efficient and cost-effective solution consisting of CXL-attached memory hardware and a software suite. The memory module hardware integrated with double-data-rate (DDR) dynamic random access memory and a CXL controller expands bandwidth by dozens of gigabytes per second and increases memory capacity by a few terabytes. Our software suite, scalable memory development kit, inherits and expands the traditional memory management architecture of existing Linux systems. We proved the functionality of the proposed solution by integrating renowned data centers and computing-intensive applications. The proposed CXL solution improved throughput for in-memory database and artificial intelligence applications by 1.5-fold and 1.99-fold, respectively, compared with the conventional DDR-only memory system.},
  archive      = {J_MICRO},
  author       = {Kyungsan Kim and Hyunseok Kim and Jinin So and Wonjae Lee and Junhyuk Im and Sungjoo Park and Jeonghyeon Cho and Hoyoung Song},
  doi          = {10.1109/MM.2023.3240774},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {20-29},
  shortjournal = {IEEE Micro},
  title        = {SMT: Software-defined memory tiering for heterogeneous computing systems with CXL memory expander},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Novel composable and scaleout architectures using compute
express link. <em>MICRO</em>, <em>43</em>(2), 9–19. (<a
href="https://doi.org/10.1109/MM.2023.3235972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute Express Link (CXL) is an open industry standard interconnect offering caching and memory semantics on top of PCI-Express. In addition to providing high-bandwidth and low-latency connectivity between host processor and accelerators, smart network interface card, and memory expansion devices, it also enables resource pooling across multiple systems for scalable, power-efficient, and cost-effective computing. This article delves into a novel composable and scaleout architecture to enable large-scale systems at rack level and beyond with pooled and shared heterogeneous memory and heterogenous compute resources using CXL interconnect.},
  archive      = {J_MICRO},
  author       = {Debendra Das Sharma},
  doi          = {10.1109/MM.2023.3235972},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {9-19},
  shortjournal = {IEEE Micro},
  title        = {Novel composable and scaleout architectures using compute express link},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on emerging system interconnects.
<em>MICRO</em>, <em>43</em>(2), 6–8. (<a
href="https://doi.org/10.1109/MM.2023.3240453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue on emerging system interconnects addresses recent advances in system interconnect technologies, with special emphasis on compute express link (cxl) as it becomes more widely adopted in the industry.},
  archive      = {J_MICRO},
  author       = {John Kim and Nam Sung Kim},
  doi          = {10.1109/MM.2023.3240453},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {6-8},
  shortjournal = {IEEE Micro},
  title        = {Special issue on emerging system interconnects},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Emerging system interconnects enabling more opportunities
than ever! <em>MICRO</em>, <em>43</em>(2), 4–5. (<a
href="https://doi.org/10.1109/MM.2023.3244677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2023.3244677},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Emerging system interconnects enabling more opportunities than ever!},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). The modern digital operating model. <em>MICRO</em>,
<em>43</em>(1), 90–92. (<a
href="https://doi.org/10.1109/MM.2022.3228422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reports on the concept of the digital operating model (DOM). A DOM describes the organization of people and processes around digital technologies for the purposes of achieving strategic objectives. What should go into a checklist for a modern DOM? What organizational attributes—at a high level—would you investigate to understand whether the organization possessed a DOM at the frontier? There are many reasons to want a good checklist. Principally, it defines an ideal benchmark without losing too much detail. That stresses aspirations and abstracts away from failures at implementation.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2022.3228422},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {90-92},
  shortjournal = {IEEE Micro},
  title        = {The modern digital operating model},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Does academic research drive industrial innovation in
computer architecture?—analyzing citations to academic papers in
patents. <em>MICRO</em>, <em>43</em>(1), 83–88. (<a
href="https://doi.org/10.1109/MM.2022.3226101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Examines the number of citations to papers published in International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), International Symposium on High- Performance Computer Architecture (HPCA), International Symposium on Computer Architecture (ISCA), and International Symposium on Microarchitecture (MICRO). Given that these four conferences are the most prestigious in computer architecture, they are an excellent proxy for academic research as they likely contain the most innovative ideas. As such, papers from these conferences could provide important building blocks for a company’s patented inventions; if so, these papers should be cited in the company’s patents.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2022.3226101},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {83-88},
  shortjournal = {IEEE Micro},
  title        = {Does academic research drive industrial innovation in computer Architecture?—Analyzing citations to academic papers in patents},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RDMA congestion control: It is only for the compliant.
<em>MICRO</em>, <em>43</em>(1), 76–82. (<a
href="https://doi.org/10.1109/MM.2022.3208746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote direct memory access (RDMA) networks enable low latency and low central processing unit utilization, and their widespread adoption in datacenters enables improved application performance. However, there are performance isolation concerns for RDMA deployed in a shared cloud environment. In particular, congestion control enforcement and congestion control algorithms in RDMA make the network susceptible to performance hacking attacks, which give the attacker extra bandwidth and cause severe congestion in the network. These attacks can increase short flow completion times by several orders of magnitude. We surface a fundamental tradeoff in congestion control between short flow completion time and performance isolation. We discuss this tradeoff and how existing approaches do not provide a robust solution. We also advocate that researchers incorporate performance isolation concerns into the design and evaluation of congestion control.},
  archive      = {J_MICRO},
  author       = {John Snyder and Alvin R. Lebeck and Danyang Zhuo},
  doi          = {10.1109/MM.2022.3208746},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {76-82},
  shortjournal = {IEEE Micro},
  title        = {RDMA congestion control: It is only for the compliant},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data movement accelerator engines on a prototype power10
processor. <em>MICRO</em>, <em>43</em>(1), 67–75. (<a
href="https://doi.org/10.1109/MM.2022.3193949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the design and implementation of active messaging engines (AMEs) on an IBM Power10 prototype chip. AMEs are tiny, simple, but fully programmable 64-bit processors, for offloading operations related to data movement. AMEs can offload the execution flow of the message passing interface and other messaging stacks from the host central processing unit, enabling truly asynchronous progress to overlap computation and communication. The AMEs are implemented as onboard OpenCAPI-compliant accelerators, leveraging existing OpenCAPI infrastructure. As realized in a 7-nm technology, each AME takes 0.034 mm2 of silicon area and 4.1 mW of power. AME performance is evaluated across several contiguous and noncontiguous memory copy scenarios. AMEs can perform up to the bandwidth limit of their access path to the main memory (32 GB/s) and incur a per-request overhead of about 600 ns. These results indicate that AMEs will confer advantages to general messaging libraries for processing, sending, and receiving on-node and off-node messages.},
  archive      = {J_MICRO},
  author       = {Yutaka Sugawara and Dong Chen and Ruud A. Haring and Abdullah Kayi and Eugene Ratzlaff and Robert M. Senger and Krishnan Sugavanam and Ralph Bellofatto and Ben J. Nathanson and Craig Stunkel},
  doi          = {10.1109/MM.2022.3193949},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {67-75},
  shortjournal = {IEEE Micro},
  title        = {Data movement accelerator engines on a prototype power10 processor},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enterprise-class multilevel cache design: Low latency, huge
capacity, and high reliability. <em>MICRO</em>, <em>43</em>(1), 58–66.
(<a href="https://doi.org/10.1109/MM.2022.3193642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IBM Z computing platform is optimized for processing vast amounts of data and transactions with low latency in a highly virtualized and secured environment with sustained processor utilization of over 90\%. The platform and its microprocessor chip are designed to deliver consistent system performance, throughput, and response times under all conditions. The innovative cache architecture of the IBM Telum Processor provides low latency, large capacity, and reliable L2 caches. Based on a novel horizontal cache persistence algorithm, these L2 caches also serves as system wide L3 and L4 caches delivering optimal enterprise application performance. When built into an IBM z16 system, these architectural features deliver 11\% per-core performance improvement over the prior z15 hardware, running real-world enterprise applications.},
  archive      = {J_MICRO},
  author       = {Deanna Berger and Christian Jacobi and Craig R. Walters and Robert J. Sonnelitter and Mike Cadigan and Matthias Klein},
  doi          = {10.1109/MM.2022.3193642},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {58-66},
  shortjournal = {IEEE Micro},
  title        = {Enterprise-class multilevel cache design: Low latency, huge capacity, and high reliability},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A scalable body bias optimization method toward low-power
CGRAs. <em>MICRO</em>, <em>43</em>(1), 49–57. (<a
href="https://doi.org/10.1109/MM.2022.3226739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Body biasing is one of the critical techniques to realize more energy-efficient computing with reconfigurable devices, such as coarse-grained reconfigurable architectures. Its benefit depends on the control granularity, whereas fine-grained control makes it challenging to find the best body bias voltage for each domain due to the complexity of the optimization problem. This work reformulates the optimization problem and introduces continuous relaxation to solve it faster than previous work based on an integer linear program. Experimental result shows the proposed method can solve the problem within 0.5 s for all benchmarks in any conditions. For a middle-class problem, up to 5.65× speedup and a geometric mean of 2.06× speedup are demonstrated compared to the previous method with negligible loss of accuracy. Besides, we explore finer body bias control considering the power- and area-overhead of an on-chip body bias generator and suggest the most reasonable design saves 66\% of energy consumption.},
  archive      = {J_MICRO},
  author       = {Takuya Kojima and Hayate Okuhara and Masaaki Kondo and Hideharu Amano},
  doi          = {10.1109/MM.2022.3226739},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {49-57},
  shortjournal = {IEEE Micro},
  title        = {A scalable body bias optimization method toward low-power CGRAs},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TCN-CUTIE: A 1,036-TOp/s/w, 2.72-µJ/inference, 12.2-mW
all-digital ternary accelerator in 22-nm FDX technology. <em>MICRO</em>,
<em>43</em>(1), 42–48. (<a
href="https://doi.org/10.1109/MM.2022.3226630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tiny machine learning (TinyML) applications impose µJ/inference constraints, with a maximum power consumption of tens of megawatt. It is extremely challenging to meet these requirements at a reasonable accuracy level. This work addresses the challenge with a flexible, fully digital ternary neural network (TNN) accelerator in a reduced instruction set computer-five (RISC-V)-based System-on-Chip (SoC). Besides supporting ternary convolutional neural networks, we introduce extensions to the accelerator design that enable the processing of time-dilated temporal convolutional neural networks (TCNs). The design achieves 5.5-µJ/inference, 12.2 mW, 8,000 inferences/s at 0.5 V for a dynamic vision sensor (DVS)-based TCN and an accuracy of 94.5\%, and 2.72-µJ/inference, 12.2 mW, 3,200 inferences/s at 0.5 V for a nontrivial 9-layer, 96 channels-per-layer convolutional network with CIFAR-10 accuracy of 86\%. The peak energy efficiency is 1,036 TOp/s/W, outperforming the state-of-the-art silicon-proven TinyML quantized accelerators by 1.67× while achieving competitive accuracy.},
  archive      = {J_MICRO},
  author       = {Moritz Scherer and Alfio Di Mauro and Tim Fischer and Georg Rutishauser and Luca Benini},
  doi          = {10.1109/MM.2022.3226630},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {42-48},
  shortjournal = {IEEE Micro},
  title        = {TCN-CUTIE: A 1,036-TOp/s/W, 2.72-µJ/Inference, 12.2-mW all-digital ternary accelerator in 22-nm FDX technology},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on cool chips. <em>MICRO</em>, <em>43</em>(1),
40–41. (<a href="https://doi.org/10.1109/MM.2022.3229244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue of IEEE Micro captures two contributions from among the 12 regular presentations of Cool Chips 25. The two articles are focused on a system-on- chip (SoC) with ternary neural network (TNN)/temporal convolutional neural network (TCN) accelerator, and body bias optimization for coarse-grained reconfigurable architectures (CGRAs), respectively. All of them are the emerging topics at Cool Chips 25.},
  archive      = {J_MICRO},
  author       = {Ryusuke Egawa and Yasutaka Wada},
  doi          = {10.1109/MM.2022.3229244},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {40-41},
  shortjournal = {IEEE Micro},
  title        = {Special issue on cool chips},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data centers on wheels: Emissions from computing onboard
autonomous vehicles. <em>MICRO</em>, <em>43</em>(1), 29–39. (<a
href="https://doi.org/10.1109/MM.2022.3219803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While much attention has been paid to data centers’ greenhouse gas emissions, less attention has been paid to autonomous vehicles’ (AVs) potential emissions. In this work, we introduce a framework to probabilistically model the emissions from computing onboard a global fleet of AVs and show that the emissions have the potential to make a nonnegligible impact on global emissions, comparable to that of all data centers today. Based on current trends, a widespread AV adoption scenario where approximately 95\% of all vehicles are autonomous requires computer power to be less than 1.2 kW for emissions from computing on AVs to be less than emissions from all data centers in 2018 in 90\% of modeled scenarios. Anticipating a future scenario with high adoption of AVs, business-as-usual decarbonization, and workloads doubling every three years, hardware efficiency must double every 1.1 years for emissions in 2050 to equal 2018 data center emissions. The rate of increase in hardware efficiency needed in many scenarios to contain emissions is faster than the current rate. We discuss several avenues of future research unique to AVs to further analyze and potentially reduce the carbon footprint of AVs.},
  archive      = {J_MICRO},
  author       = {Soumya Sudhakar and Vivienne Sze and Sertac Karaman},
  doi          = {10.1109/MM.2022.3219803},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {29-39},
  shortjournal = {IEEE Micro},
  title        = {Data centers on wheels: Emissions from computing onboard autonomous vehicles},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sustainable AI processing at the edge. <em>MICRO</em>,
<em>43</em>(1), 19–28. (<a
href="https://doi.org/10.1109/MM.2022.3220399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is a popular paradigm for accelerating light- to medium-weight machine learning algorithms initiated from mobile devices without requiring the long communication latencies to send them to remote datacenters in the cloud. Edge servers primarily consider traditional concerns, such as size, weight, and power constraints for their installations. However, such metrics are not entirely sufficient to consider environmental impacts from computing given the significant contributions from embodied energy and carbon. In this article we explore the tradeoffs of hardware strategies for convolutional neural network acceleration engines considering inference and online training. In particular, we explore the use of mobile graphics processing unit (GPU) accelerators, recently released edge-class field-programmable gate arrays, and novel processing in memory (PIM) using dynamic random-access memory (DRAM) and emerging Racetrack memory. Given edge servers already employ DRAM and sometimes GPU accelerators, we consider the sustainability implications using breakeven analysis of replacing or augmenting DDR3 with Racetrack memory. We also consider the implications for provisioning edge servers with different accelerators using indifference analysis. While mobile GPUs are typically much more energy efficient, their significant embodied energy can make them less sustainable than PIM solutions in certain scenarios that consider activity time and compute effort.},
  archive      = {J_MICRO},
  author       = {Sébastien Ollivier and Sheng Li and Yue Tang and Stephen Cahoon and Ryan Caginalp and Chayanika Chaudhuri and Peipei Zhou and Xulong Tang and Jingtong Hu and Alex K. Jones},
  doi          = {10.1109/MM.2022.3220399},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {19-28},
  shortjournal = {IEEE Micro},
  title        = {Sustainable AI processing at the edge},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kaya for computer architects: Toward sustainable computer
systems. <em>MICRO</em>, <em>43</em>(1), 9–18. (<a
href="https://doi.org/10.1109/MM.2022.3218034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reformulates the well-known Kaya identity to understand computer systems’ impact on sustainability and its total carbon footprint. By making a distinction between embodied and operational carbon emissions, we are able to understand 1) how the global carbon footprint of computing is likely to scale in the future, and 2) what we, as computer architects, can do to reduce the environmental impact of computing. We conclude that computer architects should first and foremost design smaller chips; reducing lifetime energy consumption is of secondary importance, yet still significant.},
  archive      = {J_MICRO},
  author       = {Lieven Eeckhout},
  doi          = {10.1109/MM.2022.3218034},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {9-18},
  shortjournal = {IEEE Micro},
  title        = {Kaya for computer architects: Toward sustainable computer systems},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on environmentally sustainable computing.
<em>MICRO</em>, <em>43</em>(1), 7–8. (<a
href="https://doi.org/10.1109/MM.2022.3226362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The articles in this special section focus on environmentally sustainable computing.},
  archive      = {J_MICRO},
  author       = {Carole-Jean Wu},
  doi          = {10.1109/MM.2022.3226362},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {7-8},
  shortjournal = {IEEE Micro},
  title        = {Special issue on environmentally sustainable computing},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Environmentally sustainable computing. <em>MICRO</em>,
<em>43</em>(1), 4–6. (<a
href="https://doi.org/10.1109/MM.2022.3229504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_MICRO},
  author       = {Lizy Kurian John},
  doi          = {10.1109/MM.2022.3229504},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {4-6},
  shortjournal = {IEEE Micro},
  title        = {Environmentally sustainable computing},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
