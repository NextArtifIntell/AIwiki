<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpds---226">TPDS - 226</h2>
<ul>
<li><details>
<summary>
(2023). Efficient edge data management framework for IIoT via
prediction-based data reduction. <em>TPDS</em>, <em>34</em>(12),
3309–3322. (<a href="https://doi.org/10.1109/TPDS.2023.3327750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large amounts of time series data are required to support data analysis at the edge in the end-edge-cloud Industrial Internet of Things (IIoT) architecture. Reducing the storage cost is one of the main challenges in edge data management due to the limited storage resource of edge nodes. The state-of-the-art data reduction method has a high time overhead and poor reduction efficiency for unstable data sets. To solve this problem, this study proposes a time-series data management framework that combines data partition and data compression techniques. For the data partition technique, we propose an adaptive selection strategy to integrate the access pattern of the application and the characteristics of the time series data, thereby improving the partition accuracy. For the data compression technique, we propose a compression scheme based on time series data segmentation by using the idea of divide and conquer; we further introduce a change point detection technique to improve the compression efficiency for unstable data sets. Experimental results obtained with three types of real industrial data sets show that our framework is significantly better than the state-of-the-art method in terms of compression ratio and time overhead.},
  archive      = {J_TPDS},
  author       = {Lei Yang and Yuwei Liao and Xin Cheng and Mengyuan Xia and Guoqi Xie},
  doi          = {10.1109/TPDS.2023.3327750},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3309-3322},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient edge data management framework for IIoT via prediction-based data reduction},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communication optimization algorithms for distributed deep
learning systems: A survey. <em>TPDS</em>, <em>34</em>(12), 3294–3308.
(<a href="https://doi.org/10.1109/TPDS.2023.3323282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning&#39;s widespread adoption in various fields has made distributed training across multiple computing nodes essential. However, frequent communication between nodes can significantly slow down training speed, creating a bottleneck in distributed training. To address this issue, researchers are focusing on communication optimization algorithms for distributed deep learning systems. In this paper, we propose a standard that systematically classifies all communication optimization algorithms based on mathematical modeling, which is not achieved by existing surveys in the field. We categorize existing works into four categories based on the optimization strategies of communication: communication masking, communication compression, communication frequency reduction, and hybrid optimization. Finally, we discuss potential future challenges and research directions in the field of communication optimization algorithms for distributed deep learning systems.},
  archive      = {J_TPDS},
  author       = {Enda Yu and Dezun Dong and Xiangke Liao},
  doi          = {10.1109/TPDS.2023.3323282},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3294-3308},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Communication optimization algorithms for distributed deep learning systems: A survey},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CERT-DF: A computing-efficient and robust distributed deep
forest framework with low communication overhead. <em>TPDS</em>,
<em>34</em>(12), 3280–3293. (<a
href="https://doi.org/10.1109/TPDS.2023.3324911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an alternative to the deep learning model, deep forest outperforms deep neural networks in many aspects with fewer hyperparameters and better robustness. To improve the computing performance of deep forest, ForestLayer proposes an efficient task-parallel algorithm S-FTA at a fine sub-forest granularity, but the granularity of the sub-forest cannot be adaptively adjusted. BLB-gcForest further proposes an adaptive sub-forest splitting algorithm to dynamically adjust the sub-forest granularity. However, with distributed storage, its BLB method needs to scan the whole dataset when sampling, which generates considerable communication overhead. Moreover, BLB-gcForest&#39;s tree-based vector aggregation produces extensive redundant transfers and significantly degrades the system&#39;s performance in vector aggregation stage. To deal with these existing issues and further improve the computing efficiency and scalability of the distributed deep forest, in this paper, we propose a novel Computing-Efficient and RobusT distributed Deep Forest framework, named CERT-DF. CERT-DF integrates three customized schemes, namely, block-level pre-sampling, two-stage pre-aggregation, and system-level backup. Specifically, CERT-DF adopts the block-level pre-sampling method to implement data blocks&#39; local sampling eliminating frequent data remote access and maximizing parallel efficiency, applies the two-stage pre-aggregation method to adjust the class vector aggregation granularity to greatly decrease the communication overhead, and leverages the system-level backup method to enhance the system&#39;s disaster tolerance and immensely accelerate task recovery with minimal system resource overhead. Comprehensive experimental evaluations on multiple datasets show that our CERT-DF significantly outperforms the state-of-the-art approaches with higher computing efficiency, lower system resource overhead, and better system robustness while ensuring good accuracy.},
  archive      = {J_TPDS},
  author       = {Li&#39;an Xie and Ting Wang and Shuyi Du and Haibin Cai},
  doi          = {10.1109/TPDS.2023.3324911},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3280-3293},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CERT-DF: A computing-efficient and robust distributed deep forest framework with low communication overhead},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RLPTO: A reinforcement learning-based performance-time
optimized task and resource scheduling mechanism for distributed machine
learning. <em>TPDS</em>, <em>34</em>(12), 3266–3279. (<a
href="https://doi.org/10.1109/TPDS.2023.3317388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide application of deep learning, the amount of data required to train deep learning models is becoming increasingly larger, resulting in an increased training time and higher requirements for computing resources. To improve the throughput of a distributed learning system, task scheduling and resource scheduling are required. This article proposes to combine ARIMA and GRU models to predict the future task volume. In terms of task scheduling, multi-priority task queues are used to divide tasks into different queues according to their priorities to ensure that high-priority tasks can be completed in advance. In terms of resource scheduling, the reinforcement learning method is adopted to manage limited computing resources. The reward function of reinforcement learning is constructed based on the resources occupied by the task, the training time, the accuracy of the model. When a distributed learning model tends to converge, the computing resources of the task are gradually reduced so that they can be allocated to other learning tasks. The results of experiments demonstrate that RLPTO tends to use more compu-ting nodes when facing tasks with large data scale and has good scalability. The distributed learning system reward experiment shows that RLPTO can make the computing cluster get the largest reward.},
  archive      = {J_TPDS},
  author       = {Xiaofeng Lu and Chao Liu and Senhao Zhu and Yilu Mao and Pietro Lio and Pan Hui},
  doi          = {10.1109/TPDS.2023.3317388},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3266-3279},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RLPTO: A reinforcement learning-based performance-time optimized task and resource scheduling mechanism for distributed machine learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating convolutional neural networks by exploiting the
sparsity of output activation. <em>TPDS</em>, <em>34</em>(12),
3253–3265. (<a href="https://doi.org/10.1109/TPDS.2023.3324934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Convolutional Neural Networks (CNNs) are the most widely used family of machine learning methods that have had a transformative effect on a wide range of applications. Previous studies have made great breakthroughs in accelerating CNNs, but they only target on the input sparsity of activation and weight, thus do not eliminate the unnecessary computations due to the fact that more zeros in the output results are not directly caused by the zero-valued positions of the input data. In this paper, we take advantage of the output activation sparsity to reduce the execution time and energy consumption of CNNs. First, we propose an effective prediction method that leverages the output activation sparsity. Our method first predicts the output activation polarity of convolutional layers based on the singular value decomposition (SVD) approach. Then, it uses the predicted negative value to skip invalid computations. Second, an effective accelerator is designed to take advantage of sparsity to achieve CNN inference acceleration. Each PE is equipped with a prediction unit and a non-zero value detection unit to remove invalid computation blocks. And an instruction bypass technique is proposed which further exploits the sparsity of the weights. The efficient dataflow graph mapping approach and pipeline execution ensure high computational resource utilization. Experiments show that our approach achieves up to 1.63× speedup and 55.30\% energy reduction compared with dense networks with a slight loss of accuracy. Compared with Eyeriss, our accelerator achieves on average 1.31 × performance improvement and 54\% energy reduction. Our accelerator also achieves a similar performance to SnaPEA, but with a better energy efficiency.},
  archive      = {J_TPDS},
  author       = {Zhihua Fan and Wenming Li and Zhen Wang and Tianyu Liu and Haibin Wu and Yanhuan Liu and Meng Wu and Xinxin Wu and Xiaochun Ye and Dongrui Fan and Ninghui Sun and Xuejun An},
  doi          = {10.1109/TPDS.2023.3324934},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3253-3265},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating convolutional neural networks by exploiting the sparsity of output activation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoRS: Environment-dependent real-time scheduling for
end-to-end autonomous driving. <em>TPDS</em>, <em>34</em>(12),
3238–3252. (<a href="https://doi.org/10.1109/TPDS.2023.3323975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of autonomous driving poses new research challenges for on-vehicle computing system. The execution time of autonomous driving tasks heavily depends on the driving environment. As the scene becomes complex, task execution time increases significantly, leading to end-to-end deadline misses and potential accidents. Hence, a framework that can effectively schedule tasks according to the driving environment in order to guarantee end-to-end deadlines is critical for autonomous driving. In this article, we propose AutoRS, an environment-dependent real-time scheduling framework for end-to-end autonomous driving. AutoRS consists of two nested control loops. The inner control loop schedules tasks based on the driving environment to help them meet end-to-end deadlines while prioritizing the responsiveness and throughput of control commands. The outer control loop tunes task rates based on schedulability to efficiently utilize system resources with an RL-based design. We conduct extensive experiments on both simulation and hardware testbeds using representative autonomous driving applications. The results demonstrate that AutoRS effectively improves the driving performance by $7.95\%-56.9\%$ in different driving environments. AutoRS can significantly enhance the safety and reliability of autonomous driving systems by providing timely control commands in complex and dynamic driving environments while guaranteeing task deadlines.},
  archive      = {J_TPDS},
  author       = {Jialiang Ma and Li Li and Chengzhong Xu},
  doi          = {10.1109/TPDS.2023.3323975},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3238-3252},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AutoRS: Environment-dependent real-time scheduling for end-to-end autonomous driving},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A point cloud video recognition acceleration framework based
on tempo-spatial information. <em>TPDS</em>, <em>34</em>(12), 3224–3237.
(<a href="https://doi.org/10.1109/TPDS.2023.3323263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In point cloud video recognition (PVR) tasks, deep neural networks (DNNs) have been widely adopted to enhance accuracy. However, real-time processing is hindered due to the increasing volume of points and frames that require processes. Point clouds represent 3D-shaped discrete objects using a multitude of points. Consequently, these points often exhibit an uneven distribution in the view space, resulting in strong spatial similarity within each point cloud frame. Taking advantage of this observation, this article introduces PRADA, a P oint Cloud R ecognition A cceleration algorithm via D ynamic A pproximation. PRADA approximates and eliminates the similar local pairs’ computations and recovers their results by copying dissimilar local pairs’ features for speedup with negligible accuracy loss. Furthermore, considering the slow changes in point cloud frames that lead to the high temporal similarity among points across multiple frames, we design PointV, a Point Cloud V ideo Recognition Acceleration algorithm, to minimize unnecessary computations of similar points in the temporal domain. Moreover, we propose the PRADA and PointV architectures to accelerate the PRADA and PointV algorithms. These two architectures can be integrated to gain higher performance improvement. Our experiments on a wide variety of datasets show that PRADA averagely achieves about $7\times$ speedup over 1080TI GPU. In addition, the experimental results show that the PointV architecture and the integrated architecture can respectively achieve $11.7\times$ and $13.9\times$ performance improvement with acceptable accuracy compared to the 1080TI GPU.},
  archive      = {J_TPDS},
  author       = {Zhuoran Song and Wanzhen Liu and Tao Yang and Fangxin Liu and Naifeng Jing and Xiaoyao Liang},
  doi          = {10.1109/TPDS.2023.3323263},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3224-3237},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A point cloud video recognition acceleration framework based on tempo-spatial information},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FT-BLAS: A fault tolerant high performance BLAS
implementation on x86 CPUs. <em>TPDS</em>, <em>34</em>(12), 3207–3223.
(<a href="https://doi.org/10.1109/TPDS.2023.3316011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basic Linear Algebra Subprograms (BLAS) serve as a foundational library for scientific computing and machine learning. In this article, we present a new BLAS implementation, FT-BLAS, that provides performance comparable to or faster than state-of-the-art BLAS libraries, while being capable of tolerating soft errors on-the-fly. At the algorithmic level, we propose a hybrid strategy to incorporate fault-tolerant functionality. For memory-bound Level-1 and Level-2 BLAS routines, we duplicate computing instructions and re-use data at the register level to avoid memory overhead when validating the runtime correctness. Here we novelly propose to utilize mask registers on AVX512-enabled processors and SIMD registers on AVX2-enabled processors to store intermediate comparison results. For compute-bound Level-3 BLAS routines, we fuse memory-intensive operations such as checksum encoding and verification into the GEMM assembly kernels to optimize the memory footprint. We also design cache-friendly parallel algorithms for our fault-tolerant library. Through a series of architectural-aware optimizations, we manage to maintain the fault-tolerant overhead at a negligible order ( $&amp;lt; $ 3\%). Experimental results obtained on widely-used processors such as Intel Skylake, Intel Cascade Lake, and AMD Zen2 demonstrate that FT-BLAS offers high reliability and high performance – faster than Intel MKL, OpenBLAS, and BLIS by up to 3.50\%, 22.14\%, and 21.70\%, respectively, for both serial and parallel routines spanning all three levels of BLAS we benchmarked, even under hundreds of errors injected per minute.},
  archive      = {J_TPDS},
  author       = {Yujia Zhai and Elisabeth Giem and Kai Zhao and Jinyang Liu and Jiajun Huang and Bryan M. Wong and Christian R. Shelton and Zizhong Chen},
  doi          = {10.1109/TPDS.2023.3316011},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3207-3223},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FT-BLAS: A fault tolerant high performance BLAS implementation on x86 CPUs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HashCache: Accelerating serverless computing by skipping
duplicated function execution. <em>TPDS</em>, <em>34</em>(12),
3192–3206. (<a href="https://doi.org/10.1109/TPDS.2023.3323330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing is a leading force behind deploying and managing software in cloud computing. One inherent challenge in serverless computing is the increased overall latency due to duplicate computations. Our initial investigation into the function invocations of serverless applications reveals an abundance of duplicate invocations. Inspired by this critical observation, we introduce HashCache , a system designed to cache duplicate function invocations, thereby mitigating duplicate computations. In HashCache, serverless functions are classified into three categories, namely, computational functions, stateful functions, and environment-related functions. On the grounds of such a function classification, HashCache associates the stateful functions and their states to build an adaptive synchronization mechanism. With this support, HashCache exploits the cached results of computational and stateful functions to serve upcoming invocation requests to the same functions, thereby reducing duplicate computations. Moreover, HashCache stores remote files probed by stateful functions into a local cache layer, which further curtails invocation latency. We implement HashCache within the Apache OpenWhisk to forge a cache-enabled serverless computing platform. We conduct extensive experiments to quantitatively evaluate the performance of HashCache in terms of invocation latency and resource utilization. We compare HashCache against two state-of-the-art approaches - FaaSCache and OpenWhisk . The experimental results unveil that our HashCache remarkably reduces invocation latency and resource overhead. More specifically, HashCache curbs the 99-tail latency of FaaSCache and OpenWhisk by up to 91.37\% and 95.96\% in real-world serverless applications. HashCache also slashes the resource utilization of FaaSCache and OpenWhisk by up to 31.62\% and 35.51\%, respectively.},
  archive      = {J_TPDS},
  author       = {Zhaorui Wu and Yuhui Deng and Yi Zhou and Lin Cui and Xiao Qin},
  doi          = {10.1109/TPDS.2023.3323330},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3192-3206},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HashCache: Accelerating serverless computing by skipping duplicated function execution},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CHEESE: Distributed clustering-based hybrid federated split
learning over edge networks. <em>TPDS</em>, <em>34</em>(12), 3174–3191.
(<a href="https://doi.org/10.1109/TPDS.2023.3322755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementing either Federated learning (FL) or split learning (SL) over clients with limited computation/communication resources faces challenges on achieving delay-efficient model training. To overcome such challenges, we investigate a novel distributed C lustering-based H ybrid f E d E rated S plit l E arning ( CHEESE ) framework, consolidating distributed resources among clients by device-to-device (D2D) communications, working in an intra-serial inter-parallel manner. In CHEESE , each learning client can form a cluster with its neighboring helping clients via D2D communications to train an FL model collaboratively. Inside each cluster, the model is split into multiple segments via a model splitting and allocation (MSA) strategy, while each cluster member trains one segment. After completing intra-cluster training, a transmission client (TC) is determined from each cluster to upload a complete model to the base station for global model aggregation under allocated bandwidth. Accordingly, an overall training delay cost minimization problem is formulated, involving the following subproblems: client clustering, MSA, TC selection, and bandwidth allocation. Due to its NP-Hardness, the problem is decoupled and solved iteratively. The client clustering problem is first transformed into a distributed clustering game based on potential game theory, where each cluster further investigates the remaining three subproblems to evaluate the utility of each clustering strategy. Specifically, a heuristic algorithm is proposed to solve the MSA problem under a given clustering strategy, while a greedy-based convex optimization approach is introduced to solve the joint TC selection and bandwidth allocation problem. Extensive experiments on practical models and datasets demonstrate that CHEESE can significantly reduce training delay costs.},
  archive      = {J_TPDS},
  author       = {Zhipeng Cheng and Xiaoyu Xia and Minghui Liwang and Xuwei Fan and Yanglong Sun and Xianbin Wang and Lianfen Huang},
  doi          = {10.1109/TPDS.2023.3322755},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3174-3191},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CHEESE: Distributed clustering-based hybrid federated split learning over edge networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient blockchain-based data integrity auditing for
multi-copy in decentralized storage. <em>TPDS</em>, <em>34</em>(12),
3162–3173. (<a href="https://doi.org/10.1109/TPDS.2023.3323155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the disruptor of cloud storage, decentralized storage could lead to a major shift in how organizations store data in the future. To ensure data availability, users generally encrypt the data and distribute it to multiple storage service providers. It is necessary to study data integrity verification in decentralized storage. Although some recent studies have proposed the using blockchain technology to assist auditing work in decentralized storage networks, the on-chain overhead still increases linearly with an increase in audit requests. Blockchain networks will inevitably be overloaded. In this study, we propose an efficient data integrity auditing scheme for multiple copies in decentralized storage. Particularly, using different polynomial commitment schemes, we first propose a basic scheme for verifying multiple copies of a single file, and then we propose an efficient batch auditing scheme for multiple copies of multiple files. Our scheme can significantly reduce the computation overhead of storage service providers while keeping the on-chain storage overhead constant. Security analysis and performance analysis show that our scheme is efficient and practical.},
  archive      = {J_TPDS},
  author       = {Qingyang Zhang and Zhiming Zhang and Jie Cui and Hong Zhong and Yang Li and Chengjie Gu and Debiao He},
  doi          = {10.1109/TPDS.2023.3323155},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3162-3173},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient blockchain-based data integrity auditing for multi-copy in decentralized storage},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse stream semantic registers: A lightweight ISA
extension accelerating general sparse linear algebra. <em>TPDS</em>,
<em>34</em>(12), 3147–3161. (<a
href="https://doi.org/10.1109/TPDS.2023.3322029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse linear algebra is crucial in many application domains, but challenging to handle efficiently in both software and hardware, with one- and two-sided operand sparsity handled with distinct approaches. In this work, we enhance an existing memory-streaming RISC-V ISA extension to accelerate both one- and two-sided operand sparsity on widespread sparse tensor formats like compressed sparse row (CSR) and compressed sparse fiber (CSF) by accelerating the underlying operations of streaming indirection, intersection, and union. Our extensions enable single-core speedups over an optimized RISC-V baseline of up to 7.0x, 7.7x, and 9.8x on sparse-dense multiply, sparse-sparse multiply, and sparse-sparse addition, respectively, and peak FPU utilizations of up to 80\% on sparse-dense problems. On an eight-core cluster, sparse-dense and sparse-sparse matrix-vector multiply using real-world matrices are up to 4.9x and 5.9x faster and up to 2.9x and 3.0x more energy efficient. We explore further applications for our extensions, such as stencil codes and graph pattern matching. Compared to recent CPU, GPU, and accelerator approaches, our extensions enable higher flexibility on data representation, degree of sparsity, and dataflow at a minimal hardware footprint, adding only 1.8\% in area to a compute cluster. A cluster with our extensions running CSR matrix-vector multiplication achieves 9.9x and 1.7x higher peak floating-point utilizations than recent highly optimized sparse data structures and libraries for CPU and GPU, respectively, even when accounting for off-chip main memory (HBM) and on-chip interconnect latency and bandwidth effects.},
  archive      = {J_TPDS},
  author       = {Paul Scheffler and Florian Zaruba and Fabian Schuiki and Torsten Hoefler and Luca Benini},
  doi          = {10.1109/TPDS.2023.3322029},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3147-3161},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Sparse stream semantic registers: A lightweight ISA extension accelerating general sparse linear algebra},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frequency-domain inference acceleration for convolutional
neural networks using ReRAMs. <em>TPDS</em>, <em>34</em>(12), 3133–3146.
(<a href="https://doi.org/10.1109/TPDS.2023.3322907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) (including 2D and 3D convolutions) are popular in video analysis tasks such as action recognition and activity understanding. Fast algorithms such as fast Fourier transforms (FFTs) are promising in significantly reducing computation complexity by transforming convolution into frequency domain. In frequency space, conventional spatial convolutions are replaced with simpler element-wise complex multiplications. Conventional application-specific-integrated-circuit (ASIC) based frequency-domain accelerators can achieve effective performance boost but come at the cost of significant energy consumption, owing to the hierarchical memory organization. We propose a frequency-domain resistive random access memory (ReRAM) based inference accelerator called FDA that can process element-wise complex multiplication in memory for both 2D and 3D CNNs. Each ReRAM-based frequency-domain process element (PE) with two ReRAM cells can perform an element-wise complex multiplication in two continuous execution cycles. We then provide a flexible dataflow to alleviate the redundant data movements by frequency-domain data reuse and inherent symmetrical characteristic for both 2D and 3D convolutions. Evaluation results based on representative both 2D and 3D CNN benchmarks demonstrate that FDA outperforms state-of-the-art baselines with better performance and energy efficiency.},
  archive      = {J_TPDS},
  author       = {Bosheng Liu and Zhuoshen Jiang and Yalan Wu and Jigang Wu and Xiaoming Chen and Peng Liu and Qingguo Zhou and Yinhe Han},
  doi          = {10.1109/TPDS.2023.3322907},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3133-3146},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Frequency-domain inference acceleration for convolutional neural networks using ReRAMs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Redesign and accelerate the AIREBO bond-order potential on
the new sunway supercomputer. <em>TPDS</em>, <em>34</em>(12), 3117–3132.
(<a href="https://doi.org/10.1109/TPDS.2023.3321927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular dynamics (MD) is one of the most crucial computer simulation methods for understanding real-world processes at the atomic level. Reactive potentials based on the bond order concept have the ability to model dynamic bond breaking and formation with close to quantum mechanical (QM) precision without actually requiring expensive QM calculations. In this article, we focus on the adaptive intermolecular reactive empirical bond-order (AIREBO) potential in LAMMPS for the simulation of carbon and hydrocarbon systems on the new Sunway supercomputer. To achieve scalable performance, we propose a parallel two-level building scheme and periodic buffering strategy for the tailored data design to explore data locality and data reuse. Furthermore, we design two optimized nearest-neighbor access algorithms: the redistribution of accumulated coefficients algorithm and the double-end search connectivity algorithm. Finally, we implement parallel force computation with an AoS data layout and hardware/software co-cache. In addition, we have designed a low-overhead atomic operation-based load balancing method and vectorization. The overall performance of AIREBO achieves a speedup of nearly $20\times$ on a single core group (CG), and more than $5\times$ and $4\times$ over an Intel Xeon E5 2680 v3 core and an Intel Xeon Gold 6138 core, respectively. Compared with the Intel accelerator package in LAMMPS, our performance further achieves $3.0\times$ of an Intel Xeon E5 2680 v3 core and is better than that of an Intel Xeon Gold 6138 core. We complete the validation of the results in no more than 20.5 hours on a single node with 2,000,000 running steps (i.e., 1 ns). Our experiments show that the simulation of 2,139,095,040 atoms on 798,720 ((1MPE+64CPEs) × 12,288 processes) cores exhibits a parallel efficiency of 88\% under weak scaling.},
  archive      = {J_TPDS},
  author       = {Ping Gao and Xiaohui Duan and Bertil Schmidt and Wubing Wan and Jiaxu Guo and Wusheng Zhang and Lin Gan and Haohuan Fu and Wei Xue and Weiguo Liu and Guangwen Yang},
  doi          = {10.1109/TPDS.2023.3321927},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3117-3132},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Redesign and accelerate the AIREBO bond-order potential on the new sunway supercomputer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and implementation of deep learning 2D convolutions
on modern CPUs. <em>TPDS</em>, <em>34</em>(12), 3104–3116. (<a
href="https://doi.org/10.1109/TPDS.2023.3322037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new method is provided for accelerating the execution of convolution layers in Deep Neural Networks. This research work provides the theoretical background to efficiently design and implement the convolution layers on x86/x64 CPUs, based on the target layer parameters, quantization level and hardware architecture. The proposed work is general and can be applied to other processor families too, e.g., Arm. The proposed work achieves high speedup values over the state of the art, which is Intel oneDNN library, by applying compiler optimizations, such as vectorization, register blocking and loop tiling, in a more efficient way. This is achieved by developing an analytical modelling approach for finding the optimization parameters. A thorough experimental evaluation has been applied on two Intel CPU platforms, for DenseNet-121, ResNet-50 and SqueezeNet (including 112 different convolution layers), and for both FP32 and int8 input/output tensors (quantization). The experimental results show that the convolution layers of the aforementioned models are executed from $x1.1$ up to $x7.2$ times faster.},
  archive      = {J_TPDS},
  author       = {Vasilios Kelefouras and Georgios Keramidas},
  doi          = {10.1109/TPDS.2023.3322037},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3104-3116},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Design and implementation of deep learning 2D convolutions on modern CPUs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OfpCNN: On-demand fine-grained partitioning for CNN
inference acceleration in heterogeneous devices. <em>TPDS</em>,
<em>34</em>(12), 3090–3103. (<a
href="https://doi.org/10.1109/TPDS.2023.3321755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative inference is a promising method for balancing the limited computational power of Internet of Things (IoT) devices with the huge computational demands of convolutional neural networks (CNNs). In this approach, a CNN is divided into multiple partitions and placed on multiple devices to run simultaneously. However, two major challenges are raised. (1) Computational latencies vary when the central processing unit (CPU) loads of devices are different. However, no suitable methods are available for accurately determining computation latencies on the basis of CPU utilization. (2) Existing methods partition a CNN model either vertically or horizontally. The granularity of these methods is extremely coarse and their accuracy is low. To address the aforementioned issues, this study proposes a distributed collaborative inference framework that supports a fine-grained partitioning scheme for CNN in heterogeneous devices (hereafter referred to as OfpCNN). First, the framework uses the layer latency prediction model based on floating-point operations and CPU load (FCPM) to accurately predict the computation latency of each layer of CNN in different devices. Subsequently, OfpCNN uses horizontal and vertical partitioning methods (HVPM) to partition the input feature maps and the structure of CNN respectively in accordance with network conditions and computing capacity, then assigns them to multiple devices for execution. The HVPM solution overall considers the execution position of the layer, parallelism, and location of devices responsible for data aggregation and distribution, which can consequently obtain more fine-grained partition schemes. Experimental results show that FCPM can achieve a minimum accuracy of 88\% and HVPM can improve the inference speed by 1–2.54 times compared with other state-of-the-art methods.},
  archive      = {J_TPDS},
  author       = {Lei Yang and Can Zheng and Xiaoyuan Shen and Guoqi Xie},
  doi          = {10.1109/TPDS.2023.3321755},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3090-3103},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {OfpCNN: On-demand fine-grained partitioning for CNN inference acceleration in heterogeneous devices},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task placement and resource allocation for edge machine
learning: A GNN-based multi-agent reinforcement learning paradigm.
<em>TPDS</em>, <em>34</em>(12), 3073–3089. (<a
href="https://doi.org/10.1109/TPDS.2023.3313779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) tasks are one of the major workloads in today&#39;s edge computing networks. Existing edge-cloud schedulers allocate the requested amounts of resources to each task, falling short of best utilizing the limited edge resources for ML tasks. This paper proposes TapFinger , a distributed scheduler for edge clusters that minimizes the total completion time of ML tasks through co-optimizing task placement and fine-grained multi-resource allocation. To learn the tasks’ uncertain resource sensitivity and enable distributed scheduling, we adopt multi-agent reinforcement learning (MARL) and propose several techniques to make it efficient, including a heterogeneous graph attention network as the MARL backbone, a tailored task selection phase in the actor network, and the integration of Bayes’ theorem and masking schemes. We first implement a single-task scheduling version, which schedules at most one task each time. Then we generalize to the multi-task scheduling case, in which a sequence of tasks is scheduled simultaneously. Our design can mitigate the expanded decision space and yield fast convergence to optimal scheduling solutions. Extensive experiments using synthetic and test-bed ML task traces show that TapFinger can achieve up to 54.9\% reduction in the average task completion time and improve resource efficiency as compared to state-of-the-art schedulers.},
  archive      = {J_TPDS},
  author       = {Yihong Li and Xiaoxi Zhang and Tianyu Zeng and Jingpu Duan and Chuan Wu and Di Wu and Xu Chen},
  doi          = {10.1109/TPDS.2023.3313779},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3073-3089},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Task placement and resource allocation for edge machine learning: A GNN-based multi-agent reinforcement learning paradigm},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GEM: Ultra-efficient near-memory reconfigurable acceleration
for read mapping by dividing and predictive scattering. <em>TPDS</em>,
<em>34</em>(12), 3059–3072. (<a
href="https://doi.org/10.1109/TPDS.2023.3309462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Read mapping, which maps billions of reads to a reference DNA, poses a significant performance bottleneck in genomic analysis. Current accelerators for read mapping are primarily bounded by the intensive and random memory access to huge datasets. Near-data processing (NDP) infrastructures are promising to provide extremely high bandwidth. However, existing frameworks failed to reach this potential due to poor locality and high redundancy. Our idea is to introduce prediction under the insight that candidate mapping positions become predictable when the reference is organized in coarse-grain slices. We present GEM ( Ge nomic M emory), an ultra-efficient near-memory accelerator for read mapping. GEM adopts a novel data-centric framework, named dividing-and-predictive-scattering (DPS), which synthesizes information of seed existence to predict the target mapping locations to reduce memory access redundancy. During preparation, DPS divides the reference into coarse-grained slices and creates predictive filters to assess the likelihood of reads belonging to each slice. During mapping, DPS predicts and scatters reads to considerably fewer slices compared than without prediction. By employing small on-chip SRAM-based predictors with high accuracy, DPS minimizes unnecessary DRAM access and data movement from remote memory. In essence, DPS trades pre-seeding predictors for localized access patterns and low redundancy, hence achieving high throughput for data-intensive applications. We implement GEM by integrating coarse-grain reconfigurable architectures (CGRAs) in the logic layer of a 3D-stacked DRAM infrastructure, utilizing the massive banks as slices. GEM leverages CGRAs for their flexibility in supporting various algorithms tailored to different datasets. Bloom filters are leveraged for slice prediction, providing an error rate below 1\%. Evaluation results demonstrate that GEM reduces memory requests by 95\% and alignments by 87\%, achieving a throughput improvement of 15.3× and 11.0× compared to compute-centric and broadcast-based baselines on the same NDP platform. Overall, GEM achieves a $3.5\times$ throughput improvement and $2.1\times$ energy efficiency compared to state-of-the-art ASIC accelerators.},
  archive      = {J_TPDS},
  author       = {Longlong Chen and Jianfeng Zhu and Guiqiang Peng and Mingxu Liu and Shaojun Wei and Leibo Liu},
  doi          = {10.1109/TPDS.2023.3309462},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3059-3072},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GEM: Ultra-efficient near-memory reconfigurable acceleration for read mapping by dividing and predictive scattering},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Accelerating data delivery of latency-sensitive
applications in container overlay network. <em>TPDS</em>,
<em>34</em>(12), 3046–3058. (<a
href="https://doi.org/10.1109/TPDS.2023.3300745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container overlay network, though being widely adopted to enable communication between containers on different hosts, is a key downside for latency-sensitive applications. The state-of-the-art solution seeks to shorten the data path in packet processing by replacing overlay connection file descriptors with host namespace ones. While promising, it must block each overlay connection until the relevant host connection is set up, thus heavily influencing the request latency. In this paper, we present ShuntFlow, a systematic data delivery framework that seamlessly integrates the host and overlay networks to reduce the application&#39;s request-response latency. ShuntFlow first lets all connections flow in the overlay network directly. Then, it adopts a simple-yet-effective syscall-threshold-based mechanism to pick appropriate connections and switches their data delivery to the host network in a blocking-free way using a multi-threading technique. As such, unnecessary connection switches are prevented; yet, the pre-setup phase dilemma is eliminated. We have implemented a ShuntFlow prototype based on Linux and Docker and evaluated it extensively on a 40 Gbps testbed. The results show that ShuntFlow achieves 13\%/72\% and 19\%/69\% reductions, in average/tail request-response latency of a web server and an in-memory key-value store, respectively, while incurring less CPU overhead, compared to Slim.},
  archive      = {J_TPDS},
  author       = {Hao Liu and Wenxin Li and Yiren Pang and Renjie Pei and Yitao Hu and Yuan Liu and Lide Suo and Keqiu Li},
  doi          = {10.1109/TPDS.2023.3300745},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3046-3058},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating data delivery of latency-sensitive applications in container overlay network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On-line network traffic anomaly detection based on tensor
sketch. <em>TPDS</em>, <em>34</em>(12), 3028–3045. (<a
href="https://doi.org/10.1109/TPDS.2023.3316717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network traffic anomaly detection is critical for advanced network applications. However, network traffic monitoring data arrive in a streaming fashion and could be infinite, which makes the offline algorithms that attempt to store the entire stream monitoring data for analysis not scalable. To well utilize the strong ability of tensor model, we use a tensor to represent the prior non-anomalous traffic matrices and propose a novel unsupervised anomaly detection framework that can be used to detect anomalies in a streaming fashion by making only one pass over the data while utilizing limited storage. In the framework, we propose a succinct tensor sketch to maintain, in a streaming model, the subspace that can well represent all prior non-anomalous data detected. Using the subspace, anomalies in each new incoming traffic monitoring data can be quickly detected based on a simple outlier score calculation. Further, we prove that the tensor sketch is mergeable. Exploiting this property, we propose a distributed anomaly detection framework in which the distributed node only needs to upload its succinct tensor sketch instead of the raw monitoring data to the central node to calculate the global subspace of the whole network, which greatly saves the transmission cost. We theoretically prove that our tensor sketch based anomaly detection algorithm compares favorably with the offline approach which calculates the subspace based on expensive global Singular Value Decomposition (SVD). The experimental results demonstrate the effectiveness and efficiency of our approach over other popular online anomaly detection algorithms.},
  archive      = {J_TPDS},
  author       = {Shuyu Pei and Jigang Wen and Kun Xie and Gaogang Xie and Kenli Li},
  doi          = {10.1109/TPDS.2023.3316717},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3028-3045},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On-line network traffic anomaly detection based on tensor sketch},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consistent low latency scheduler for distributed key-value
stores. <em>TPDS</em>, <em>34</em>(12), 3012–3027. (<a
href="https://doi.org/10.1109/TPDS.2023.3315777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the distributed key-value stores have become the basic building block for large-scale cloud applications. In large-scale distributed key-value stores, many key-value access operations, which will be processed in parallel on different servers, are usually generated for a single end-user request. Accordingly, the completion time of an end-user request is determined by the last completed key-value access operation. Scheduling the order of serving key-value access operations can effectively reduce the completion times of end requests, thereby improving the user experience. However, existing scheduling algorithms hardly achieve consistent low latency due to the following challenges: the large overhead of cooperating clients and servers, the time-varying load and performance of servers, the traffic distribution can be either heavy-tailed or light-tailed and both the mean and the tail completion time are expected to be low. In this paper, we formalize the problem of scheduling key-value access operations and show it is NP-hard. Furthermore, we heuristically design the distributed adaptive scheduler (DAS), which distributively combines the largest remaining processing time last and the shortest remaining process time first algorithms. Theoretical analysis shows that DAS is adaptive to the time-varying traffic and server performance and can achieve consistent low mean and tail latency regardless of traffic distributions. Extensive simulations show that DAS reduces the mean request completion time by $17 \! \sim \! 50\%$ with heavy-tailed traffic and $2 \! \sim 26 \!\%$ with light-tailed traffic, while keeping the smallest tail completion time, compared to the default first come first served algorithm. Moreover, DAS outperforms the existing Rein-SBF algorithm under various scenarios.},
  archive      = {J_TPDS},
  author       = {Wanchun Jiang and Haoyang Li and Yulong Yan and Fa Ji and Jiawei Huang and Jianxin Wang and Tong Zhang},
  doi          = {10.1109/TPDS.2023.3315777},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3012-3027},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Consistent low latency scheduler for distributed key-value stores},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Back to homogeneous computing: A tightly-coupled
neuromorphic processor with neuromorphic ISA. <em>TPDS</em>,
<em>34</em>(11), 2910–2927. (<a
href="https://doi.org/10.1109/TPDS.2023.3307408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, neuromorphic processors are widely used in many scenarios, showing extreme energy efficiency over traditional architectures. However, almost all existing neuromorphic hardware are following the heterogeneous computing methodology without Instruction Set Architecture (ISA), leading to inflexibility in programming. In this paper, we first propose a RISC-V Neuromorphic Extension (RVNE) to enable fine-grained and flexible homogeneous programming for neuromorphic algorithms while utilizing SNN sparsity from different levels of granularity and computing flows. Based on RVNE, we next implement a neuromorphic micro-architecture that is tightly coupled to the CPU pipeline to accelerate neuromorphic computing. To demonstrate the proposed homogeneous neuromorphic architecture, we implement a prototype processor called NeuroRVcore based on RISC-V ISA and an open-source RISC-V core. The evaluation results show that RVNE achieves a 2.8 × −4.3 × reduction in code density compared with the general-purpose ISAs. Compared with the state-of-the-art neuromorphic processor, the proposed homogeneous computing reduces energy consumption by 3.4\%−22.5\% while enabling fine-grained and flexible homogeneous programming.},
  archive      = {J_TPDS},
  author       = {Zhijie Yang and Lei Wang and Wei Shi and Yao Wang and Junbo Tie and Feng Wang and Xiang Yu and Linghui Peng and Chao Xiao and Xun Xiao and Yao Yao and Gan Zhou and Xuhu Yu and Rui Gong and Xia Zhao and Yuhua Tang and Weixia Xu},
  doi          = {10.1109/TPDS.2023.3307408},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2910-2927},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Back to homogeneous computing: A tightly-coupled neuromorphic processor with neuromorphic ISA},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TDTA: Topology-based real-time DAG task allocation on
identical multiprocessor platforms. <em>TPDS</em>, <em>34</em>(11),
2895–2909. (<a href="https://doi.org/10.1109/TPDS.2023.3310294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern real-time systems contain complex workloads, which are usually modeled as directed acyclic graph (DAG) tasks and deployed on multiprocessor platforms. The complex execution logic of DAG tasks results in excessive schedulability analysis overhead, and the current DAG task allocation strategy cannot efficiently utilize processor resources (inner parallelization of DAG tasks). In this article, an invalid-edge deletion (IED) method is proposed to reduce the execution complexity of the DAG tasks while guaranteeing the correctness of the execution logic. Besides, we bound the number of complete paths for DAG tasks, which re-limits the searching space of the schedulability analysis. Then, a topology-based DAG tasks allocation (TDTA) strategy is developed, which reduces the interference caused by higher-priority DAG tasks to enable the full utilization of the processor resources. The experimental results show that the IED method effectively reduces the overhead of DAG task analysis, and the performance of the TDTA strategy is better than the performance of other state-of-the-art strategies.},
  archive      = {J_TPDS},
  author       = {Yulong Wu and Weizhe Zhang and Nan Guan and Yehan Ma},
  doi          = {10.1109/TPDS.2023.3310294},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2895-2909},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TDTA: Topology-based real-time DAG task allocation on identical multiprocessor platforms},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SketchINT: Empowering INT with TowerSketch for per-flow
per-switch measurement. <em>TPDS</em>, <em>34</em>(11), 2876–2894. (<a
href="https://doi.org/10.1109/TPDS.2023.3303924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network measurement is indispensable to network operations. INT solutions that can provide fine-grained per-switch per-packet information serve as promising solutions for per-flow per-switch measurement. The main shortcoming of INT is its high network overhead incurred by collecting INT information, making INT impractical for production deployment. Sketches that can compactly record per-flow information with small memory footprint, are a promising choice for compressing INT information to reduce INT overhead. An ideal sketch for efficiently compressing INT information in practice should achieve both simplicity and accuracy, but no existing sketch achieves both. Motivated by this, we first design SketchINT to combine INT and sketches, aiming to obtain all per-flow per-switch information with low network overhead. Second, we design a new sketch for SketchINT, namely TowerSketch, which achieves both simplicity and accuracy. The key idea of TowerSketch is to use different-sized counters for different arrays under the property that the number of bits used for different arrays stays the same. TowerSketch can automatically record larger flows in larger counters and smaller flows in smaller counters. To further ease the configuration and give network operators more confidence on performance of TowerSketch, we propose a method for precise error bound estimation. We have fully implemented our SketchINT prototype on a testbed consisting of 10 switches. We also implement our TowerSketch on P4, single-core CPU, multi-core CPU, and FPGA platforms to verify its deployment flexibility. Extensive experimental results verify that 1) TowerSketch achieves better accuracy than prior art on various tasks, outperforming the state-of-the-art ElasticSketch up to 27.7 times in terms of error; 2) Compared to INT, SketchINT reduces the number of packets belonging to the control plane overhead by $3 \sim 4$ orders of magnitude with an error smaller than 5\%; 3) The estimated error bound of TowerSketch can almost match the actual error bound.},
  archive      = {J_TPDS},
  author       = {Kaicheng Yang and Sheng Long and Qilong Shi and Yuanpeng Li and Zirui Liu and Yuhan Wu and Tong Yang and Zhengyi Jia},
  doi          = {10.1109/TPDS.2023.3303924},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2876-2894},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SketchINT: Empowering INT with TowerSketch for per-flow per-switch measurement},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-SP network slicing parallel relieving edge network
conflict. <em>TPDS</em>, <em>34</em>(11), 2860–2875. (<a
href="https://doi.org/10.1109/TPDS.2023.3310013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing is rapidly prevailing in the edge network, which provides computing, network, and storage resources for various services. When the multiple service providers (SPs) respond to their tenants in parallel, individual decisions on the dynamic and shared edge network may lead to resource conflicts, which affects the delivery of network slicing services. Existing works ignore resource interaction and coordination in the multi-SP scenario, which is not in line with the actual situation. Indeed, the complexity of resource interaction caused by the coexistence of multiple SP policies increases the difficulty to solve the formulated optimization model. In this article, we focus on the multi-SP network slicing deployment in parallel. The coordination of network resources between SPs is designed as an effective multi-agent communication mechanism that is merged into multi-agent deep reinforcement learning (MADRL). To deal with dynamic edge networks, we design the neurons hotplugging learning which realizes scalability without a high cost of model retraining. Experiments on real and random networks demonstrate that the proposed multi-SP network slicing mechanism can successfully learn coordination policies and easily adapt to various network scales. It improves the accepted requests by 7.4\%, reduces resource conflicts by 14.5\%, and shortens the model convergence time by 83.3\%.},
  archive      = {J_TPDS},
  author       = {Rongxin Han and Dezhi Chen and Song Guo and Jingyu Wang and Qi Qi and Lu Lu and Jianxin Liao},
  doi          = {10.1109/TPDS.2023.3310013},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2860-2875},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-SP network slicing parallel relieving edge network conflict},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive data placement in multi-cloud storage: A
non-stationary combinatorial bandit approach. <em>TPDS</em>,
<em>34</em>(11), 2843–2859. (<a
href="https://doi.org/10.1109/TPDS.2023.3306150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-cloud storage is recently a viable approach to solve the vendor lock-in, reliability, and security issues in cloud storage systems. As a key concern, data placement influences the cost and performance of storage services. Yet, in practice it remains challenging to address the huge solution space. Previous studies typically focus on constructing efficient data placement schemes based on the predicted pattern of workloads or assuming fully a-priori known network conditions. They cannot be easily applied in multi-cloud storage scenarios, which typically involve dynamic network conditions and time-varying workloads. To this end, we formulate the data placement optimization in a combinatorial multi-arm bandit (CMAB) perspective and solve it by learning placement strategy online. In contrast to a stationary setting where reward distributions are unknown but identical over time, we consider a realistic multi-cloud environment with non-stationary conditions, i.e., reward distributions change over time. To swiftly accommodate this, we propose an adaptive window combinatorial upper confidence bound based data placement (AW-CUCB-DP) scheme to reduce latency and cost. In AW-CUCB-DP, a simple and efficient change detector, i.e., Page-Hinkley test with forgetting mechanism (FM-PHT), is employed to enable variable-size sliding windows to handle both gradual and abrupt variations in network conditions or workloads. We establish that AW-CUCB-DP is asymptotically optimal in the non-stationary multi-cloud environment. Trace-driven experiments further verify that our scheme outperforms alternatives, especially in highly dynamic environments.},
  archive      = {J_TPDS},
  author       = {Li Li and Jiajie Shen and Bochun Wu and Yangfan Zhou and Xin Wang and Keqin Li},
  doi          = {10.1109/TPDS.2023.3306150},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2843-2859},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive data placement in multi-cloud storage: A non-stationary combinatorial bandit approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint model pruning and topology construction for
accelerating decentralized machine learning. <em>TPDS</em>,
<em>34</em>(10), 2827–2842. (<a
href="https://doi.org/10.1109/TPDS.2023.3303967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, mobile and embedded devices worldwide generate a massive amount of data at the network edge. To efficiently exploit the data from distributed devices, we concentrate on decentralized machine learning (DML), where the workers collaboratively train models under the peer-to-peer (P2P) setting. DML avoids the bottleneck of the parameter server (PS) by enabling the workers to exchange local models with their neighbors rather than the PS. However, DML still faces some key challenges, i.e., resource limitation, system heterogeneity, network dynamics and non-IID data. In this article, we design and implement MOTOR, an efficient DML mechanism that simultaneously addresses these challenges by applying model pruning and topology construction, thus accelerating DML. Specifically, MOTOR assigns different pruning ratios to heterogeneous workers. After model pruning, each worker will train and transmit a sub-model that fits its capabilities, reducing both computation and communication overhead. Besides, MOTOR dynamically constructs the network topology considering the time-varying network conditions and non-IID data distributions. We theoretically analyze the impact of pruning ratio and network topology on model training performance. Guided by the theoretical analysis, we develop a joint optimization algorithm for pruning ratio decision and topology construction to achieve the trade-off between resource overhead and training performance. We implement MOTOR on commercial devices and evaluate the performance with different DML tasks. Extensive experiments show that MOTOR achieves up to 4.2× speedup compared to the existing DML approaches.},
  archive      = {J_TPDS},
  author       = {Zhida Jiang and Yang Xu and Hongli Xu and Lun Wang and Chunming Qiao and Liusheng Huang},
  doi          = {10.1109/TPDS.2023.3303967},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2827-2842},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Joint model pruning and topology construction for accelerating decentralized machine learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The doctrine of MEAN: Realizing deduplication storage at
unreliable edge. <em>TPDS</em>, <em>34</em>(10), 2811–2826. (<a
href="https://doi.org/10.1109/TPDS.2023.3305460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Placing popular data at the network edge helps reduce the retrieval latency, but it also brings challenges to the limited edge storage space. Currently, using available yet not necessarily reliable edge resources is common sense for edge space expansion, while deploying deduplication storage strategies is a general method for better space utilization. However, a contradiction arises when jointly implementing data deduplication with unreliable edge resources. On the one hand, the deduplication policy stipulates that any data chunk can be stored exactly once; on the other hand, the use of unreliable resources imposes that data should be backed up for the seek of file availability. To resolve such contradiction, we propose MEAN, a deduplication-enabled storage system using unreliable resources at the network edge. The core idea of MEAN is to place similar files together for better deduplication and maintain replicas of popular files for higher reliability. We first formulate this problem and prove its NP-hardness, then provide efficient heuristics based on similarity-aware hierarchical clustering. Three different reliability scenarios are comprehensively considered to develop our algorithms. We also implement a prototype system and evaluate the performance of MEAN with a real-world dataset. The results show that MEAN can fortify the file hit ratio under unreliable environments by 77\% while reducing the file retrieval delay up to 71\%, compared with the state-of-the-art approach.},
  archive      = {J_TPDS},
  author       = {Junxu Xia and Geyao Cheng and Lailong Luo and Deke Guo and Pin Lv and Bowen Sun},
  doi          = {10.1109/TPDS.2023.3305460},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2811-2826},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The doctrine of MEAN: Realizing deduplication storage at unreliable edge},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LB-chain: Load-balanced and low-latency blockchain sharding
via account migration. <em>TPDS</em>, <em>34</em>(10), 2797–2810. (<a
href="https://doi.org/10.1109/TPDS.2023.3238343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain sharding has been increasingly used to improve blockchain systems’ performance, in which a blockchain is split into multiple smaller, disjoint shards. In practice, however, sharding can only achieve limited throughput and latency improvement, especially for the user-perceived transaction confirmation delay. The performance degradation is believed to be caused by the cross-shard transactions. However, we show, through comprehensive system deployment and measurement studies, that the main culprit is the imbalanced transaction load on different blockchain shards. To address this problem, we propose a novel sharding system, called LB-Chain, which dynamically balances the transaction load on different shards by periodically migrating active accounts from heavily-loaded shards to less-loaded ones. We have implemented a prototype of LB-Chain, and evaluated its performance through large-scale blockchain deployment using real-world transaction traces. Extensive experiments confirm that LB-Chain significantly boosts sharding performance, reducing the transaction confirmation delays by up to 90\% while increasing the transaction throughput by more than 10\%. The delay difference between different accounts is also reduced dramatically, leading to improved fairness in the system.},
  archive      = {J_TPDS},
  author       = {Mingzhe Li and Wei Wang and Jin Zhang},
  doi          = {10.1109/TPDS.2023.3238343},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2797-2810},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LB-chain: Load-balanced and low-latency blockchain sharding via account migration},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IO-sets: Simple and efficient approaches for i/o bandwidth
management. <em>TPDS</em>, <em>34</em>(10), 2783–2796. (<a
href="https://doi.org/10.1109/TPDS.2023.3305028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main performance issues faced by high-performance computing platforms is the congestion caused by concurrent I/O from applications. When this happens, the platform&#39;s overall performance and utilization are harmed. From the extensive work in this field, I/O scheduling is the essential solution to this problem. The main drawback of current techniques is the amount of information needed about applications, which compromises their applicability. In this paper, we propose a novel method for I/O management, IO-Sets . We present its potential through a scheduling heuristic called Set-10 , which is simple and requires only minimal information. Our extensive experimental campaign shows the importance of IO-Sets and the robustness of Set-10 under various workloads. In particular in most of the simulated scenarios we improve the I/O slowdown over fairshare by 50\%, which corresponds in our scenarios to a platform utilization gain of 2.5\%. In the practical scenarios that we did, the utilization gain varies between 10 and 30\%. We also provide insights on using our proposal in practice.},
  archive      = {J_TPDS},
  author       = {Francieli Boito and Guillaume Pallez and Luan Teylo and Nicolas Vidal},
  doi          = {10.1109/TPDS.2023.3305028},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2783-2796},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IO-sets: Simple and efficient approaches for I/O bandwidth management},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simeuro: A hybrid CPU-GPU parallel simulator for
neuromorphic computing chips. <em>TPDS</em>, <em>34</em>(10), 2767–2782.
(<a href="https://doi.org/10.1109/TPDS.2023.3291795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the success of deep learning, there have been numerous efforts to build hardware for it. One approach that is gaining momentum is neuromorphic computing with spiking neural networks (SNNs), which are multiplication-free and open the possibility of using analog computing via novel technologies. However, to design effective and efficient hardware for such architectures, a fast and accurate software simulator is key. This article presents Simeuro, a fast and scalable system-level simulator for SNN models used in neuromorphic accelerators. The simulator uses spike-level details and configurable architectural constraints that are independent of the underlying hardware implementation. Simeuro supports a wide range of features including analog computing, novel memory (currently, RRAM is supported), and a full network-on-chip. The simulator can provide detailed simulation results such as routing statistics, energy consumption, delay, and accuracy of arbitrarily defined SNN architectures. Our simulator leverages a CPU-GPU hybrid environment to expedite the simulation by scaling out to multi-nodes equipped with multi-GPUs. We are able to conduct core simulations for a system-scale SNN chip of 20,000 neuromorphic cores on up to 512 A100 GPUs in a few minutes.},
  archive      = {J_TPDS},
  author       = {Huaipeng Zhang and Nhut-Minh Ho and Dogukan Yigit Polat and Peng Chen and Mohamed Wahib and Truong Thao Nguyen and Jintao Meng and Rick Siow Mong Goh and Satoshi Matsuoka and Tao Luo and Weng-Fai Wong},
  doi          = {10.1109/TPDS.2023.3291795},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2767-2782},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Simeuro: A hybrid CPU-GPU parallel simulator for neuromorphic computing chips},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When deduplication meets migration: An efficient and
adaptive strategy in distributed storage systems. <em>TPDS</em>,
<em>34</em>(10), 2749–2766. (<a
href="https://doi.org/10.1109/TPDS.2023.3299309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional migration methods are confronted with formidable challenges when data deduplication technologies are incorporated. First, the deduplication creates data-sharing dependencies in the stored files; breaking such dependencies in migration may attach extra space overhead. Second, the redundancy elimination makes the storage system reserves only one copy for each storage file, and heightens the risk of data unavailability. The existing methods fail to tackle them in one shot. To this end, we propose Jingwei, an efficient and adaptive data migration strategy for deduplicated storage systems. To be specific, Jingwei tries to minimize the extra space cost in migration for space efficiency. Meanwhile, Jingwei realizes the service adaptability by encouraging replicas of hot files to spread out their data access requirements. We first model such a problem as an integer linear programming (ILP) and solve it with a commercial solver when only one empty migration target server is allowed. We then extend this problem to a scenario wherein multiple non-empty target servers are available for migration. We solve it by effective heuristic algorithms based on the Bloom Filter-based data sketches. The Jingwei strategy can suffer from performance degradation when the heat degree varies significantly. Therefore, we further present incremental adjustment strategies for the two scenarios, which adjust the number of block replicas and their locations in an incremental manner. The mathematical analyses and trace-driven experiments show the effectiveness of our Jingwei strategy. To be specific, Jingwei fortifies the file replicas by 25\% with only 5.7\% of the extra storage space, compared with the latest “Goseed” method. With the small extra space cost, the file retrieval throughput of Jingwei can reach up to 333.5 Mbps, which is 12.3\% higher than that of the Random method.},
  archive      = {J_TPDS},
  author       = {Geyao Cheng and Lailong Luo and Junxu Xia and Deke Guo and Yuchen Sun},
  doi          = {10.1109/TPDS.2023.3299309},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2749-2766},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {When deduplication meets migration: An efficient and adaptive strategy in distributed storage systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SFC orchestration method for edge cloud and central cloud
collaboration: QoS and energy consumption joint optimization combined
with reputation assessment. <em>TPDS</em>, <em>34</em>(10), 2735–2748.
(<a href="https://doi.org/10.1109/TPDS.2023.3301670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization (NFV) is an emerging technology that uses virtualization technology to provide various services in enterprise networks and reduce costs. However, in cloud edge networks, effective virtual network function (VNF) configuration is particularly difficult, and the system design needs to consider the reliability and energy-saving while meeting the requirements of Quality of Service (QoS). This paper uses the binary integer programming (BIP) model to study the service function chain (SFC) orchestration problem, and designs a federated deep reinforcement learning SFC orchestration algorithm (FDOA). With this method, energy consumption can be reduced and the QoS of users can be improved. In addition, considering the limitations of local deep reinforcement learning (DRL) model training, this paper proposes a federated DRL algorithm to help obtain a more robust model, and simultaneously improve the convergence speed of the model. Among them, we introduce reputation theory during model training to evaluate the reliability of the nodes carrying the DRL model, avoiding the influence of unreliable models on the training effect. Finally, the simulation results show that FDOA has better performance in training time and end-to-end delay compared with other existing algorithms.},
  archive      = {J_TPDS},
  author       = {Lanlan Rui and Shiyou Chen and Shuyun Wang and Zhipeng Gao and Xuesong Qiu and Wenjing Li and Shaoyong Guo},
  doi          = {10.1109/TPDS.2023.3301670},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2735-2748},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SFC orchestration method for edge cloud and central cloud collaboration: QoS and energy consumption joint optimization combined with reputation assessment},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating k-shape time series clustering algorithm using
GPU. <em>TPDS</em>, <em>34</em>(10), 2718–2734. (<a
href="https://doi.org/10.1109/TPDS.2023.3298148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the data space, time-series analysis has emerged in many fields, including biology, healthcare, and numerous large-scale scientific facilities like astronomy, climate science, particle physics, and genomics. Clustering is one of the most critical methods in time-series analysis. So far, the state-of-art time series clustering algorithm k-Shape has been widely used not only because of its high accuracy, but also because of its relatively low computation cost. However, due to the high heterogeneity of time series data, it can not be simply regarded as a high-dimensional vector. Two time series often need some alignment method in similarity comparison. The alignment between sequences is often a time-consuming process. For example, when using dynamic time warping as a sequence alignment algorithm and if the length of time series is greater than 1,000, a single iteration in the clustering process may take hundreds to tens of thousands of seconds, while the entire clustering cycle often requires dozens of iterations. In this article, we propose a set of novel parallel strategies suitable for GPU&#39;s computation model, called Times-C, which is an abbreviation for Time Series Clustering. We define three stages in the analysis process: aggregation, centroid, and class assignment. Times-C includes efficient parallel algorithms and corresponding implementations for these three stages. Overall, the experimental results show that the Times-C algorithm exhibits a performance improvement of one to two orders of magnitude compared to the multi-core CPU version of k-Shape. Furthermore, compared to the GPU version of the k-Shape algorithm, the Times-C algorithm achieves a maximum acceleration of up to 345 times.},
  archive      = {J_TPDS},
  author       = {Xun Wang and Ruibao Song and Junmin Xiao and Tong Li and Xueqi Li},
  doi          = {10.1109/TPDS.2023.3298148},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2718-2734},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating k-shape time series clustering algorithm using GPU},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling efficient random access to hierarchically
compressed text data on diverse GPU platforms. <em>TPDS</em>,
<em>34</em>(10), 2699–2717. (<a
href="https://doi.org/10.1109/TPDS.2023.3294341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tremendous computing capacity of GPU offers significant potential in processing hierarchically compressed text data without decompression. However, current GPU techniques offer only traversal-based text data analytics; random access is exceedingly inefficient, limiting their utility significantly. To address this issue, we develop a novel and widely applicable solution that prompts random access to hierarchically compressed text data without decompression in GPU memory. We address three main challenges for enabling efficient random access to compressed text data on GPUs. The first challenge is designing GPU data structures that facilitate random access. The second challenge is efficiently generating data structures on GPU. The CPU is inefficient when generating data structures for random access, and this inefficiency increases considerably when PCIe transmission is incorporated. The third challenge is query processing on compressed text data in GPU memory. Random accesses, such as data updates, cause massive conflicts among countless threads. In order to address the first challenge, we develop several compressed GPU data structures, including indexing within the intricate GPU memory hierarchy. To handle the second challenge, we propose a two-phase process for producing these data structures on GPU. For the third challenge, a double-parsing design is proposed as a solution to avoid conflicts. We evaluate our solution on three platforms, two server-grade GPU platforms and one edge-grade GPU platform, using five real-world datasets. Experimental results show that random access operations on GPU achieve an average speedup of 52.98× compared to the state-of-the-art solution.},
  archive      = {J_TPDS},
  author       = {Yihua Hu and Feng Zhang and Yifei Xia and Zhiming Yao and Letian Zeng and Haipeng Ding and Zhewei Wei and Xiao Zhang and Jidong Zhai and Xiaoyong Du and Siqi Ma},
  doi          = {10.1109/TPDS.2023.3294341},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2699-2717},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Enabling efficient random access to hierarchically compressed text data on diverse GPU platforms},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel integrated simulation framework for cyber-physical
systems modelling. <em>TPDS</em>, <em>34</em>(10), 2684–2698. (<a
href="https://doi.org/10.1109/TPDS.2023.3300081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing use of Cyber-Physical Systems (CPS) in a plethora of domains (e.g. healthcare, industry, smart homes, transportation, etc.) triggers an urgent demand for simulation frameworks that can simulate in an integrated manner all the components (i.e. CPUs, Memories, Networks, Physical Environment) of a system-under-design(SuD). By utilizing such a simulator, software design can proceed in parallel with physical development which results in the reduction of the so important time-to-market. The main problem, however, is that currently there is a shortage of such simulation frameworks; most simulators used for modelling the digital aspects of CPS applications (i.e. full-system CPU/Mem/Peripheral simulators) lack any support of the CPS physical aspects and vice versa. The presented fully-distributed simulation framework (APOLLON) is the first known open-source, high-performance simulator that can handle holistically complex CPSs including processors, peripherals, networks and physical aspects of them. APOLLON is an extension of the COSSIM simulation framework and it integrates, in a novel and efficient way, a combined processing and network simulator with the widely-used Ptolemy physical simulator, in a transparent way. Our highly integrated approach is further augmented with Machine Learning capabilities by implementing Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) recurrent neural networks in both the Cyber and Physical domains, enabling users to develop their complex recurrent neural networks significantly fast and accurately. APOLLON has been evaluated when executing a number of benchmarks and real-world use cases; the end results demonstrate that the presented approach has up to 99\% accuracy in the reported SuD aspects.},
  archive      = {J_TPDS},
  author       = {Nikolaos Tampouratzis and Panagiotis Mousouliotis and Ioannis Papaefstathiou},
  doi          = {10.1109/TPDS.2023.3300081},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2684-2698},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A novel integrated simulation framework for cyber-physical systems modelling},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A low-memory community detection algorithm with hybrid
sparse structure and structural information for large-scale networks.
<em>TPDS</em>, <em>34</em>(10), 2671–2683. (<a
href="https://doi.org/10.1109/TPDS.2023.3277885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection plays an essential role in the domains of social, bioinformatics, and e-commerce. The innovative structural information theory (SInfo, introduced by Li et al.) has achieved excellent performance for network analysis. Nevertheless, similar to traditional network algorithms, the SInfo algorithm will exhaust all system memory resources when processing large-scale networks based on adjacency data structures. Moreover, due to the irregular and sequential characteristics, the SInfo algorithm is challenging to parallelize. In this article, we propose a hybrid sparse data structure and design a low-memory community detection algorithm based on structural information theory (HSSInfo), which shrinks memory requirements and achieves high parallelism. Specifically, we first propose a general quantization method to quantify the information change in community transformation, with which community inner and outer connection complexity can be semantically interpreted. Second, we design a general hybrid sparse structure to store network data among CPU and GPU, which can reduce memory resource consumption for community detection on large-scale networks. Finally, we develop an HSSInfo algorithm based on the quantization method and hybrid sparse structure, in which parallelism intersection and community fusion algorithms are employed to improve parallel scalability. We execute the comparison experiments for HSSInfo and eleven baseline algorithms on nine real-world datasets. Empirically, HSSInfo can achieve orders of magnitude speedups and extend structural information theory to large-scale datasets of billions of edges. Meanwhile, compared with various community detection algorithms, HSSInfo can achieve higher or similar accuracy with up to 19x memory shrinkage ratio.},
  archive      = {J_TPDS},
  author       = {Weiguo Zhu and Yongqi Sun and Rongqiang Fang and Baomin Xu},
  doi          = {10.1109/TPDS.2023.3277885},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2671-2683},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A low-memory community detection algorithm with hybrid sparse structure and structural information for large-scale networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of memory-centric energy efficient computer
architecture. <em>TPDS</em>, <em>34</em>(10), 2657–2670. (<a
href="https://doi.org/10.1109/TPDS.2023.3297595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy efficient architecture is essential to improve both the performance and power consumption of a computer system. However, modern computers suffer from the severe “memory wall” problem due to the significant performance gap between the processor technology and the memory technology. Thus, the computer architecture community is evolving from compute-centric to memory-centric designs to reduce the data movement overhead. This paper presents a comprehensive survey of the main challenges and recent advances in memory-centric energy efficient computer architecture. We summarize two research directions: improving the memory technology and processing closer to memory. The former focuses on optimizing the conventional memory technology and exploiting emerging non-volatile memory (NVM) technology. The latter talks about currently popular processing in memory (PIM) technology, including near-memory processing (NMP) and in-memory processing (IMP). Moreover, some other topics like hardware for machine learning (ML), ML for hardware, security, privacy, and reliability are gaining increasing attention and should be considered seriously in the design phase of a computer system. The community is facing various challenges and opportunities simultaneously, requiring researchers to have a more comprehensive understanding of this field which is also the goal of this paper.},
  archive      = {J_TPDS},
  author       = {Changwu Zhang and Hao Sun and Shuman Li and Yaohua Wang and Haiyan Chen and Hengzhu Liu},
  doi          = {10.1109/TPDS.2023.3297595},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2657-2670},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A survey of memory-centric energy efficient computer architecture},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FCDedup: A two-level deduplication system for encrypted data
in fog computing. <em>TPDS</em>, <em>34</em>(10), 2642–2656. (<a
href="https://doi.org/10.1109/TPDS.2023.3298684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed fog computing has received increasing attention recently and fog-assisted cloud storage can provide a real-time service to collect and manage large-scale data for the applications of Internet of Things. Encrypted data deduplication over cloud storage can significantly save storage space of the cloud server while protecting the confidentiality of the outsourced data. Previous encrypted data deduplication schemes are mostly designed for traditional cloud storage with a two-layer architecture and cannot be applied to the emerging fog-assisted cloud storage that has a more complex three-layer architecture (i.e., cloud server, fog node and endpoint device). In this paper, we design, analyze and implement FCDedup, a new encrypted data deduplication scheme for fog-assisted cloud storage. FCDedup is a two-level deduplication system that enables each fog node to detect duplicated encrypted data uploaded by different endpoint devices, as well as enables cloud server to detect duplicated encrypted data from different fog nodes. By doing so, FCDedup can achieve both intra-deduplication within a single data owner and inter-deduplication across different data owners. FCDedup is also designed to prevent cloud server and fog nodes launching the brute-force attacks, and to guarantee the reliability of files downloaded from the cloud. Formal analysis is provided to justify its deduplication correctness and security. Besides, we implement a prototype of FCDedup using Alibaba Cloud as backend storage. Our evaluations demonstrate that FCDedup is completely compatible with existing cloud storage systems and achieves modest performance overhead.},
  archive      = {J_TPDS},
  author       = {Mingyang Song and Zhongyun Hua and Yifeng Zheng and Tao Xiang and Xiaohua Jia},
  doi          = {10.1109/TPDS.2023.3298684},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2642-2656},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FCDedup: A two-level deduplication system for encrypted data in fog computing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical federated learning with momentum acceleration
in multi-tier networks. <em>TPDS</em>, <em>34</em>(10), 2629–2641. (<a
href="https://doi.org/10.1109/TPDS.2023.3294688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose Hierarchical Federated Learning with Momentum Acceleration (HierMo), a three-tier worker-edge-cloud federated learning algorithm that applies momentum for training acceleration. Momentum is calculated and aggregated in the three tiers. We provide convergence analysis for HierMo, showing a convergence rate of $\mathcal {O}(\frac{1}{T})$ . In the analysis, we develop a new approach to characterize model aggregation, momentum aggregation, and their interactions. Based on this result, we prove that HierMo achieves a tighter convergence upper bound compared with HierFAVG without momentum. We also propose HierOPT, which optimizes the aggregation periods (worker-edge and edge-cloud aggregation periods) to minimize the loss given a limited training time. By conducting the experiment, we verify that HierMo outperforms existing mainstream benchmarks under a wide range of settings. In addition, HierOPT can achieve a near-optimal performance when we test HierMo under different aggregation periods.},
  archive      = {J_TPDS},
  author       = {Zhengjie Yang and Sen Fu and Wei Bao and Dong Yuan and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2023.3294688},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2629-2641},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Hierarchical federated learning with momentum acceleration in multi-tier networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting VM-agnostic KVM vCPU scheduler for mitigating
excessive vCPU spinning. <em>TPDS</em>, <em>34</em>(10), 2615–2628. (<a
href="https://doi.org/10.1109/TPDS.2023.3297688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtualized environments, virtual CPUs (vCPUs) are commonly oversubscribed on physical CPUs (pCPUs) to utilize CPU resources efficiently. However, excessive vCPU spinning, which occurs when a vCPU is waiting in a spin loop for an event from a descheduled vCPU, greatly degrades application performance in virtualized environments. VM-agnostic hypervisors aim to prevent excessive vCPU spinning by rescheduling vCPUs when an excessive spin is detected by hardware support for virtualization. We investigate the effectiveness of the KVM vCPU scheduler and show that it fails to avoid excessive vCPU spinning under various situations. We identify three problems: 1) scheduler mismatch, 2) aggressive limitation of candidate vCPUs, and 3) IPI context misuse. The first problem stems from the mismatch between the KVM vCPU scheduler and the Linux scheduler. The second and third problems come from failures in choosing candidate vCPUs to be scheduled next. Our in-depth analysis reveals simple modification to KVM (89 LoC) can mitigate excessive vCPU spinning. Our simple modification reduces excessive vCPU spinning by up to 96\% and improves benchmark performance by up to 2.6×. Part of the proposed mitigation has been integrated with KVM from Linux KVM v5.13 onward.},
  archive      = {J_TPDS},
  author       = {Kenta Ishiguro and Naoki Yasuno and Pierre-Louis Aublin and Kenji Kono},
  doi          = {10.1109/TPDS.2023.3297688},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2615-2628},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Revisiting VM-agnostic KVM vCPU scheduler for mitigating excessive vCPU spinning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating distributed GNN training by codes.
<em>TPDS</em>, <em>34</em>(9), 2598–2614. (<a
href="https://doi.org/10.1109/TPDS.2023.3295184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging graph neural network (GNN) has recently attracted much attention and has been used extensively in many real-world applications thanks to its powerful expression ability of unstructured data. The real-world graph datasets are very large-scale, which can contain up to billions of nodes and tens of billions of edges. It usually requires distributed system to train GNN on such huge datasets. As a result, the data communication overheads between machines become the bottleneck of GNN computation. Our profiling results show that getting attributes from remote machines during sampling phase in GNN occupies $&amp;gt; $ 75\% of the time of the training process. To address this issue, in this article, we propose Coded Neighbor Sampling (CNS) framework, which introduces codes technique to reduce the communication overheads of GNN. In the proposed CNS framework, the codes technique is coupled with GNN sampling method to exploit the data excess among different machines caused by unstructured nature of graph data. An analytical performance model is built for the proposed CNS framework, whose results are corroborated by the simulation and validate the benefit of the proposed CNS framework over both conventional GNN training method and conventional codes technique. Performance metrics, such as communication overheads, runtime, and throughput, of the proposed CNS framework are evaluated on a distributed GNN training simulation system implemented on MPI4py platform. The results show that, on average, the proposed CNS framework can save communication overhead by 40.6\%, 35.5\%, and 16.5\%, reduce the runtime by 12.1\%, 17.0\%, and 10.0\%, and improve the throughput by 16.2\%, 24.4\%, and 11.2\%, respectively, when training GNN models with Cora, PubMed, and Large Taobao.},
  archive      = {J_TPDS},
  author       = {Yanhong Wang and Tianchan Guan and Dimin Niu and Qiaosha Zou and Hongzhong Zheng and C.-J. Richard Shi and Yuan Xie},
  doi          = {10.1109/TPDS.2023.3295184},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2598-2614},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating distributed GNN training by codes},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GraphAGILE: An FPGA-based overlay accelerator for
low-latency GNN inference. <em>TPDS</em>, <em>34</em>(9), 2580–2597. (<a
href="https://doi.org/10.1109/TPDS.2023.3287883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents GraphAGILE, a domain-specific FPGA-based overlay accelerator for graph neural network (GNN) inference. GraphAGILE consists of (1) a novel unified architecture design with an instruction set , and (2) a compiler built upon the instruction set that can quickly generate optimized code. Due to the proposed instruction set architecture (ISA) and the compiler, GraphAGILE does not require any FPGA reconfiguration when performing inference on various GNN models and input graphs. For the architecture design, we propose a novel hardware module named Adaptive Computation Kernel (ACK), that can execute various computation kernels of GNNs, including general matrix multiplication (GEMM), sparse-dense matrix multiplication (SpDMM), and sampled dense-dense matrix multiplication (SDDMM). The compiler takes the specifications of a GNN model and the graph meta data (e.g., the number of vertices and edges) as input, and generates a sequence of instructions for inference execution. We develop the following compiler optimizations to reduce inference latency: 1) computation order optimization that automatically reorders the computation graph to reduce the total computation complexity, 2) layer fusion that merges adjacent layers to reduce data communication volume, 3) data partitioning with a partition-centric execution scheme that partitions the input graph to fit the available on-chip memory of FPGA, 4) kernel mapping that automatically selects execution mode for ACK, and performs task scheduling to overlap computation with data communication and achieves dynamic load balance. We implement GraphAGILE on a state-of-the-art FPGA platform, Xilinx Alveo U250. GraphAGILE can execute widely used GNN models, including GCN, GAT, GIN, GraphSAGE, SGC and other GNN models supported by GraphGym. Experimental results show that GraphAGILE achieves up to $47.1\times$ ( $3.9\times$ ) reduction in end-to-end latency, including the latency of compilation and hardware execution, compared with the state-of-the-art implementations on CPU (GPU), and achieves up to $2.9\times$ reduction in hardware execution latency compared with the state-of-the-art FPGA accelerators.},
  archive      = {J_TPDS},
  author       = {Bingyi Zhang and Hanqing Zeng and Viktor K. Prasanna},
  doi          = {10.1109/TPDS.2023.3287883},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2580-2597},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GraphAGILE: An FPGA-based overlay accelerator for low-latency GNN inference},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating content-defined chunking for data deduplication
based on speculative jump. <em>TPDS</em>, <em>34</em>(9), 2568–2579. (<a
href="https://doi.org/10.1109/TPDS.2023.3290770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data deduplication systems, chunking has a significant impact on the deduplication ratio and throughput. Existing Content-Defined Chunking (CDC) approaches exploit a sliding window to calculate rolling hashes of the input data stream byte-by-byte, and then determine chunk cut-points if the rolling hash satisfies a given cut-condition. Since previous CDC approaches are extremely costly, it often significantly degrades the throughput of data deduplication systems. In this paper, we argue that calculating and checking the rolling hashes byte-by-byte is unnecessary. To reduce the CPU overhead of CDC, we propose a jump-based chunking (JC) approach. The key idea is to introduce a jump-condition, and the sliding window can jump over a specific length of the input data stream if the rolling hashes satisfy the jump-condition. Moreover, we also explore the impact of the cut-condition and the jump-condition on the chunk size. Our theoretic studies demonstrate the effectiveness and efficiency of JC, without compromising the deduplication ratio. Experimental results show that JC improves the throughput of chunking by about 2× on average compared with the state-of-the-art CDC approaches while still guaranteeing high deduplication ratio.},
  archive      = {J_TPDS},
  author       = {Xiaozhong Jin and Haikun Liu and Chencheng Ye and Xiaofei Liao and Hai Jin and Yu Zhang},
  doi          = {10.1109/TPDS.2023.3290770},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2568-2579},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating content-defined chunking for data deduplication based on speculative jump},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepBoot: Dynamic scheduling system for training and
inference deep learning tasks in GPU cluster. <em>TPDS</em>,
<em>34</em>(9), 2553–2567. (<a
href="https://doi.org/10.1109/TPDS.2023.3293835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning tasks (DLT) include training and inference tasks, where training DLTs have requirements on minimizing average job completion time (JCT) and inference tasks need sufficient GPUs to meet real-time performance. Unfortunately, existing work separately deploys multi-tenant training and inference GPU cluster, leading to the high JCT of training DLTs with limited GPUs when the inference cluster is under insufficient GPU utilization due to the periodic inference workload. DeepBoot solves the challenges by utilizing idle GPUs in the inference cluster for the training DLTs. Specifically, 1) DeepBoot designs adaptive task scaling (ATS) algorithm to allocate GPUs in the training and inference clusters for training DLTs and minimize the performance loss when reclaiming inference GPUs. 2) DeepBoot implements auto-fast elastic (AFE) training based on Pollux to reduce the restart overhead by inference GPU reclaiming. Our implementation on the testbed and large-scale simulation in Microsoft deep learning workload shows that DeepBoot can achieve 32\% and 38\% average JCT reduction respectively compared with the scheduler without utilizing idle GPUs in the inference cluster.},
  archive      = {J_TPDS},
  author       = {Zhenqian Chen and Xinkui Zhao and Chen Zhi and Jianwei Yin},
  doi          = {10.1109/TPDS.2023.3293835},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2553-2567},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DeepBoot: Dynamic scheduling system for training and inference deep learning tasks in GPU cluster},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). An efficient hierarchical-reduction architecture for
aggregation in route travel time estimation. <em>TPDS</em>,
<em>34</em>(9), 2541–2552. (<a
href="https://doi.org/10.1109/TPDS.2023.3292841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Route travel time estimation (RTTE) is crucial in intelligent transportation systems. Performing aggregation is a fundamental operation in RTTE and is widely used in the traffic prediction and route calculation stages. Observations have revealed that aggregation operations in RTTE are influenced by the road network structure and aggregation requests, resulting in irregular data access, redundant processing, and workload imbalance. Existing architectures have not addressed these issues effectively. In this study, we begin by characterizing the execution pattern of performing aggregation operations on an Intel Core CPU. Guided by these characterizations, we propose an aggregation accelerator that utilizes a hierarchical reduction architecture (HRA) to perform aggregations in RTTE efficiently. Specifically, we construct an inverted table based on the road network and aggregation requests. Building upon the concept of hierarchical reduction, we design an HRA to accelerate aggregation operations which reduces irregular data access and eliminates redundant processing. Additionally, we introduce a reconfiguration mode for HRA to mitigate workload imbalance issues. Compared to a benchmark method executed on an Intel Core CPU, our design achieves an average $10\times$ speedup.},
  archive      = {J_TPDS},
  author       = {Zhao Liu and MengQuan Li and MinCan Li and Lei Liao and Kenli Li},
  doi          = {10.1109/TPDS.2023.3292841},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2541-2552},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An efficient hierarchical-reduction architecture for aggregation in route travel time estimation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ENLARGE: An efficient SNN simulation framework on GPU
clusters. <em>TPDS</em>, <em>34</em>(9), 2529–2540. (<a
href="https://doi.org/10.1109/TPDS.2023.3291825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs) are currently the most widely used computing model for neuroscience communities. There is also an increasing research interest in exploring the potential of SNN in brain-inspired computing, artificial intelligence, and other areas. As SNNs possess distinguished characteristics that originate from biological authenticity, they require dedicated simulation frameworks to achieve usability and efficiency. However, there is no widely-used, easily accessible, high performance SNN simulation framework for GPU clusters. In this paper, we propose ENLARGE, an efficient SNN simulation framework on GPU clusters. ENLARGE provides a multi-level architecture that deals with computation, communication, and synchronization hierarchically. We also propose an efficient communication method with an all-to-all communication pattern. To deal with the delay of spike delivery, which is the most distinguished SNN characteristic, several delay-aware optimization methods are also proposed. We further propose a multilevel workload management method. Various experiments are carried out to demonstrate the performance and scalability of the framework, as well as the effects of the optimization methods. Test results show that ENLARGE can achieve $3.17\times \sim 28.12\times$ speedup compared with the most widely used NEST simulator and $3.26\times \sim 13.57\times$ speedup compared with the widely used NEST GPU simulator for GPU clusters.},
  archive      = {J_TPDS},
  author       = {Peng Qu and Hui Lin and Meng Pang and Xiaofei Liu and Weimin Zheng and Youhui Zhang},
  doi          = {10.1109/TPDS.2023.3291825},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2529-2540},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ENLARGE: An efficient SNN simulation framework on GPU clusters},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative intrusion detection system for SDVN: A
fairness federated deep learning approach. <em>TPDS</em>,
<em>34</em>(9), 2512–2528. (<a
href="https://doi.org/10.1109/TPDS.2023.3290650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous innovations and development in communication technology and intelligent transportation systems, a new generation of vehicular ad hoc networks (VANETs) has become increasingly popular, making VANET communication security increasingly important. An intrusion detection system (IDS) is an important tool for detecting network attacks and is an effective means of improving network security. However, existing IDSs encounter several problems involving inaccurate detections, low detection efficiencies, and incomplete detections owing to extensive changes in vehicle locations in VANETs. This study explores federated learning in software-defined VANETs and designs an efficient and accurate collaborative intrusion detection system (CIDS) model. The model utilizes the collaboration among local software-defined networks (SDNs) to jointly train the CIDS model without directly exchanging local network data flows to improve the expansibility and globality of IDSs. To reduce the model difference between different SDN clients and improve the detection accuracy, this study regards the prediction loss for each SDN client as an objective from the perspective of constrained multi-objective optimization. By optimizing a surrogate maximum function containing all the objectives, the method adopts two-stage gradient optimization to achieve Pareto optimality for SDN clients with the worst fairness constraint maximization performance. In addition, this study evaluates the training model using two open-source datasets and compares it with the latest methods. Experimental results reveal that the proposed model ensures local data privacy and demonstrates high accuracy and efficiency in detecting attacks and is thus superior to the current schemes.},
  archive      = {J_TPDS},
  author       = {Jie Cui and Hu Sun and Hong Zhong and Jing Zhang and Lu Wei and Irina Bolodurina and Debiao He},
  doi          = {10.1109/TPDS.2023.3290650},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2512-2528},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Collaborative intrusion detection system for SDVN: A fairness federated deep learning approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). APQ: Automated DNN pruning and quantization for ReRAM-based
accelerators. <em>TPDS</em>, <em>34</em>(9), 2498–2511. (<a
href="https://doi.org/10.1109/TPDS.2023.3290010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging ReRAM-based accelerators support in-memory computation to accelerate deep neural network (DNN) inference. Weight matrix pruning is a widely used technique to reduce the size of DNN models, thereby reducing the resource and energy consumption of ReRAM-based accelerators. However, existing pruning works for ReRAM-based accelerators have three major issues. First, they use heuristics or rules from domain experts to prune the weights, leading to sub-optimal pruning policies. Second, they use row or column-level coarse-granularity methods to prune weights, resulting in poor compression rates with model accuracy constraints. Third, they only apply the weight pruning technique individually, losing the compression opportunity of both pruning and quantization. In this article, we propose an Automated DNN Pruning and Quantization framework, named APQ , for ReRAM-based accelerators. First, APQ adopts reinforcement learning (RL) to automatically determine the pruning policy for DNN layers for a global optimum. Second, it prunes and maps weight matrices to a ReRAM-based accelerator in a finer granularity of column-vector, which improves the compression rates with the accuracy constraints. To address the dislocation problem, it uses a new data path in ReRAM-based accelerators to correctly index and feed input to matrix-vector computation. Third, to further reduce resource consumption, APQ also leverages reinforcement learning to automatically determine the quantization bitwidth of each layer of the pruned DNN model. Experimental results show that, APQ achieves up to 4.52X compression rate, 4.11X area efficiency, and 4.51X energy efficiency with similar or even higher model accuracy, compared to the state-of-the-art work.},
  archive      = {J_TPDS},
  author       = {Siling Yang and Shuibing He and Hexiao Duan and Weijian Chen and Xuechen Zhang and Tong Wu and Yanlong Yin},
  doi          = {10.1109/TPDS.2023.3290010},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2498-2511},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {APQ: Automated DNN pruning and quantization for ReRAM-based accelerators},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On jointly optimizing partial offloading and SFC mapping: A
cooperative dual-agent deep reinforcement learning approach.
<em>TPDS</em>, <em>34</em>(8), 2479–2497. (<a
href="https://doi.org/10.1109/TPDS.2023.3287633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) and network function virtualization (NFV) are promising technologies to support emerging IoT applications, especially those computation-intensive. In NFV-enabled MEC environment, service function chain (SFC), i.e., a set of ordered virtual network functions (VNFs), can be mapped on MEC servers. Mobile devices (MDs) can offload computation-intensive applications, which can be represented by SFCs, fully or partially to MEC servers for remote execution. This article studies the partial offloading and SFC mapping joint optimization (POSMJO) problem in an NFV-enabled MEC system, where the data from an incoming task is partitioned into two parts, with one part executed locally and the other offloaded to the edge infrastructure for execution. These two parts are independent of each other, but both need to be processed by the same SFC. The objective is to minimize the average cost in the long term which is a combination of execution delay, MD&#39;s energy consumption, and usage charge for edge computing. This problem consists of two closely related decision-making steps, namely task partition and VNF placement, which is highly complex and quite challenging. To address this, we propose a cooperative dual-agent deep reinforcement learning (CDADRL) algorithm, where two agents interact with each other. Simulation results show that the proposed algorithm outperforms three combinations of deep reinforcement learning algorithms with respect to cumulative reward and it overweighs a number of baseline algorithms in terms of execution delay, energy consumption, and usage charge.},
  archive      = {J_TPDS},
  author       = {Xinhan Wang and Huanlai Xing and Fuhong Song and Shouxi Luo and Penglin Dai and Bowen Zhao},
  doi          = {10.1109/TPDS.2023.3287633},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2479-2497},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On jointly optimizing partial offloading and SFC mapping: A cooperative dual-agent deep reinforcement learning approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive fragment-based parallel state recovery for stream
processing systems. <em>TPDS</em>, <em>34</em>(8), 2464–2478. (<a
href="https://doi.org/10.1109/TPDS.2023.3251997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, large-scale cloud organizations are deploying datacenters and “edge” clusters globally to provide low-latency access to services. Running stream applications across geo-distributed sites are emerging as a daily requirement. However, existing efforts have dominantly centered around stateless stream processing , leaving another urgent trend- stateful stream processing -much less explored. A driving need is to store and update states during processing, and most importantly, successfully recover large distributed states when faults and failures happen. Existing studies exhibit major limitations including: (1) they mostly inherit MapReduce&#39;s “single master/many workers” architecture, where the central master can easily become ascalability bottleneck; (2) they offer state recovery mainly through three approaches: replication recovery, checkpointing recovery, and DStream-based lineage recovery, which are either slow, resource-expensive or failing to handle multiple failures; and (3) they are not adaptive to heterogeneous hardware settings. We present A-FP4S, a novel adaptive fragments-based parallel state recovery mechanism for stream processing systems. A-FP4S organizes stream operators into a distributed hash table based peer-to-peer overlay and divides each node&#39;s local state into many fragments. These fragments are periodically stored in node&#39;s multiple neighbors, ensuring different sets of available fragments can reconstruct failed states in parallel. This mechanism is extremely scalable to the lost state, significantly reduces failure recovery time, and can tolerate multiple node failures. A-FP4S is adaptive to heterogeneous hardware settings by automatic parameter tuning over phases. Compared to Apache Storm, A-FP4S achieves 31.8\% to 50.5\% reduction in recovery latency. Large-scale experiments using real-world datasets demonstrate A-FP4S&#39;s attractive scalability and adaptivity properties.},
  archive      = {J_TPDS},
  author       = {Hailu Xu and Pinchao Liu and Sarker Tanzir Ahmed and Dilma Da Silva and Liting Hu},
  doi          = {10.1109/TPDS.2023.3251997},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2464-2478},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive fragment-based parallel state recovery for stream processing systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Serpens: A high performance FaaS platform for network
functions. <em>TPDS</em>, <em>34</em>(8), 2448–2463. (<a
href="https://doi.org/10.1109/TPDS.2023.3263272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More and more enterprises deploy applications on Function-as-a-Service (FaaS) platforms to improve resource efficiency and save monetary costs. Network Functions (NFs) suffer from staggered peaks of traffic patterns and could benefit from fine-grained resource multiplexing in FaaS platform. However, naively exploring existing FaaS platforms to support NFs can introduce significant performance overheads in three aspects, including slow instance startup, remote state access for NFs, and costly packet delivery between NFs. To address these problems, we propose ${\sf Serpens}$ , a high performance FaaS platform for NFs. First, ${\sf Serpens}$ proposes a reusable NF runtime design to slash instance startup overhead. Second, ${\sf Serpens}$ designs a novel state management mechanism to support local state access. Third, ${\sf Serpens}$ introduces an advanced service chaining approach to avoid extra packet delivery. Besides, ${\sf Serpens}$ designs an NF scaling mechanism to minimize performance fluctuation. We have implemented a prototype of ${\sf Serpens}$ and conducted comprehensive experiments. Compared with the NFs and Service Function Chains (SFCs) that run on existing FaaS platforms, ${\sf Serpens}$ can improve the throughput by more than 10× and reduce the latency by more than 90\%.},
  archive      = {J_TPDS},
  author       = {Heng Yu and Han Zhang and Junxian Shen and Yantao Geng and Jilong Wang and Congcong Miao and Mingwei Xu},
  doi          = {10.1109/TPDS.2023.3263272},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2448-2463},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Serpens: A high performance FaaS platform for network functions},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EHotSnap: An efficient and hot distributed snapshots system
for virtual machine cluster. <em>TPDS</em>, <em>34</em>(8), 2433–2447.
(<a href="https://doi.org/10.1109/TPDS.2023.3272014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of IaaS clouds, many distributed and networked applications are running in virtual machine cluster (VMC). The distributed snapshots of VMC are a practical approach to guarantee system reliability. It rewinds the system to an intermediate state from failures so that the applications can continue execution from a point near the failure. However, the applications running in the VMC suffer from long disruption and significant performance degradation due to the heavy cost distributed snapshots, especially when designed to guarantee global consistency of VMC snapshots. This article presents eHotSnap, which takes distributed snapshots of a VMC efficiently. eHotSnap divides the native snapshot into light cost transient snapshot and heavy cost memory snapshot and then coordinates the VM snapshots immediately after transient snapshots. In this way, it decouples coordination from heavy cost snapshots so that the distributed snapshots are taken (completed in logic) within a second. Then, it performs memory snapshot and optimizes it with a two-layer optimization, which first employs de-duplication to reduce the amount of snapshot data and then leverages priority queue to serve guest write operations preferentially. In addition to presenting eHotSnap, we have implemented a prototype on QEMU/KVM. The experimental results demonstrate the effectiveness and efficiency of the proposed approach.},
  archive      = {J_TPDS},
  author       = {Bo Li and Lei Cui and Zhiyu Hao and Lun Li and Yongji Liu and Yongnan Li},
  doi          = {10.1109/TPDS.2023.3272014},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2433-2447},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EHotSnap: An efficient and hot distributed snapshots system for virtual machine cluster},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel parallel algorithm for sparse tensor matrix chain
multiplication via TCU-acceleration. <em>TPDS</em>, <em>34</em>(8),
2419–2432. (<a href="https://doi.org/10.1109/TPDS.2023.3288520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of multi-dimensional data, especially tensor decomposition, which extracts latent information, is becoming considerably popular. Although multi-dimensional sparse data is typically processed on multi-core processors, developing highly optimized GPU-based Sp arse T ensor M atrix C hain M ultiplication (SpTMCM) is challenging. The purpose of this paper is to investigate a novel approach named SpTMCM and to explore the discovery of SpTMCM coupled with the emerging computing core, Tensor Core Unit (TCU). In contrast to prior work, the proposed novel approach enables a uniform storage format and optimization approach for SpTMCM. We design a hybrid tensor format based on multi-dimensional tiling that divides the tensor depending on the tile threshold to address the inefficient memory accesses caused by the irregular nonzero distribution of the sparse tensor. Further, we develop a TCU-based tensor parallel algorithm with our novel approach to increase the memory bandwidth. Compared to state-of-the-art works, our method achieves $1.16\sim 24.12\times$ speedup for SpMTTKRP and $5.07\sim 7.15\times$ speedup for SpTTMChain across NVIDIA A100 GPU on a range of real-world sparse tensors.},
  archive      = {J_TPDS},
  author       = {Haotian Wang and Wangdong Yang and Rong Hu and Renqiu Ouyang and Kenli Li and Keqin Li},
  doi          = {10.1109/TPDS.2023.3288520},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2419-2432},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A novel parallel algorithm for sparse tensor matrix chain multiplication via TCU-acceleration},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STR: Hybrid tensor re-generation to break memory wall for
DNN training. <em>TPDS</em>, <em>34</em>(8), 2403–2418. (<a
href="https://doi.org/10.1109/TPDS.2023.3266110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of the depth of neural networks and the scale of data, the difficulty of network training also increases. When the GPU memory is insufficient, it is challenging to train deeper models. Recent research uses tensor swapping and recomputation techniques in a combined manner to optimize memory usage. However, complex dependencies and enormous scales of the DNN graph limit the improvement of single GPU memory optimization. Improper swap and recomputation decisions even bring negative effects on training performance. In this article, we propose a novel hybrid tensor re-generation strategy, called STR, which combines swap and recomputation techniques to find the optimal execution plan for the DNN training when the memory is limited. We formalize our memory optimization problem with constraints that describe the dependency of the operator calculation and the bandwidth usage of the swap. A host checkpoint mechanism is designed to make full use of the swapped tensors, which reduces the cost of the recomputation. We also present a recursive source tracing algorithm to improve the optimization efficiency by constraint relaxation with a performance bound. To optimize large models, we further introduce an approximation method based on a weighted graph coarsening. We implement a prototype of STR as a plugin on TensorFlow and evaluated based on 5 popular DNN models. The experimental result shows that the approximate solution of STR improves the training throughput of ResNet series of models by up to 28.1\% compared to the state-of-the-art hybrid optimization strategy.},
  archive      = {J_TPDS},
  author       = {Zan Zong and Li Lin and Leilei Lin and Lijie Wen and Yu Sun},
  doi          = {10.1109/TPDS.2023.3266110},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2403-2418},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {STR: Hybrid tensor re-generation to break memory wall for DNN training},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-minimized scheduling of intermittent real-time tasks
in a CPU-GPU cloud computing platform. <em>TPDS</em>, <em>34</em>(8),
2391–2402. (<a href="https://doi.org/10.1109/TPDS.2023.3288702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the flexibility, availability, and scalability of cloud computing services, more and more users seek solutions via cloud computing techniques. A cloud computing platform often consists of a large number of infrastructures, and its energy consumption is a big problem. In this article, we study how to minimize the energy consumption of a cloud computing platform when handling some intermittent real-time tasks. Unlike previous works that abstract users’ submitted tasks as single computation jobs and process them using CPU, this work proposes using CPU and GPU to process intermittent real-time tasks that occur at irregular intervals and their released computation jobs must be completed within required time limits. The energy consumption minimization problem is formulated as an integer nonlinear programming problem that needs to decide on a task assignment plan and a specific resource allocation plan. To effectively solve this problem, we define a state that represents the optimal solution for a given set of tasks with a given amount of resources, as well as a value function that represents the value of a state. In this way, we derive a state-transition equation and develop a dynamic programming method to solve the problem. This method is also extended to handle tasks whose arrival time is dynamic and unpredictable. Experiments show that the proposed algorithm can effectively reduce energy consumption, while its computation time is quite low compared to some other greedy methods.},
  archive      = {J_TPDS},
  author       = {Biao Hu and Xincheng Yang and Mingguo Zhao},
  doi          = {10.1109/TPDS.2023.3288702},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2391-2402},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy-minimized scheduling of intermittent real-time tasks in a CPU-GPU cloud computing platform},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on auto-parallelism of large-scale deep learning
training. <em>TPDS</em>, <em>34</em>(8), 2377–2390. (<a
href="https://doi.org/10.1109/TPDS.2023.3281931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has gained great success in recent years, leading to state-of-the-art performance in research community and industrial fields like computer vision and natural language processing. One of the reasons for this success is the huge amount parameters adopted in DL models. However, it is impractical to train a moderately large model with a large number of parameters on a typical single device. Thus, It is necessary to train DL models in clusters with distributed training algorithms. However, traditional distributed training algorithms are usually sub-optimal and highly customized, which owns the drawbacks to train large-scale DL models in varying computing clusters. To handle the above problem, researchers propose auto-parallelism, which is promising to train large-scale DL models efficiently and practically in various computing clusters. In this survey, we perform a broad and thorough investigation on challenges, basis, and strategy searching methods of auto-parallelism in DL training. First, we abstract basic parallelism schemes with their communication cost and memory consumption in DL training. Further, we analyze and compare a series of current auto-parallelism works and investigate strategies and searching methods which are commonly used in practice. At last, we discuss several trends in auto-parallelism which are promising in further research.},
  archive      = {J_TPDS},
  author       = {Peng Liang and Yu Tang and Xiaoda Zhang and Youhui Bai and Teng Su and Zhiquan Lai and Linbo Qiao and Dongsheng Li},
  doi          = {10.1109/TPDS.2023.3281931},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2377-2390},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A survey on auto-parallelism of large-scale deep learning training},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-GPU aggregation-based AMG preconditioner for
iterative linear solvers. <em>TPDS</em>, <em>34</em>(8), 2365–2376. (<a
href="https://doi.org/10.1109/TPDS.2023.3287238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present and release in open source format a sparse linear solver which efficiently exploits heterogeneous parallel computers. The solver can be easily integrated into scientific applications that need to solve large and sparse linear systems on modern parallel computers made of hybrid nodes hosting Nvidia Graphics Processing Unit (GPU) accelerators. The work extends previous efforts of some of the authors in the exploitation of a single GPU accelerator and proposes an implementation, based on the hybrid MPI-CUDA software environment, of a Krylov-type linear solver relying on an efficient Algebraic MultiGrid (AMG) preconditioner already available in the BootCMatchG library. Our design for the hybrid implementation has been driven by the best practices for minimizing data communication overhead when multiple GPUs are employed, yet preserving the efficiency of the GPU kernels. Strong and weak scalability results of the new version of the library on well-known benchmark test cases are discussed. Comparisons with the Nvidia AmgX solution show a speedup, in the solve phase, up to 2.0x.},
  archive      = {J_TPDS},
  author       = {Massimo Bernaschi and Alessandro Celestini and Flavio Vella and Pasqua D&#39;Ambra},
  doi          = {10.1109/TPDS.2023.3287238},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2365-2376},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A multi-GPU aggregation-based AMG preconditioner for iterative linear solvers},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preventive priority setting against multiple controller
failures in software defined networks. <em>TPDS</em>, <em>34</em>(8),
2352–2364. (<a href="https://doi.org/10.1109/TPDS.2023.3285898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a preventive priority setting model to minimize the worst-case maximum utilization ratio against multiple controller failures in software defined networks. For a set of controllers able to manage a switch, we introduce a priority for each controller to become the main controller that controls the switch. Once the existing main controller fails, the survived controller which has the highest priority works as the main controller. This reassignment is automatically obtained according to the priority setting decided at the network operation start time. In this way, the proposed model provides a prompt recovery with reducing the network instability due to unnecessary controller reassignment. We formulate the proposed model in two different forms, which are an integer linear programming formulation and a min-max formulation. We prove that the considered problem is NP-hard. A basic heuristic is introduced based on the min-max formulation. Its two extensions are further developed considering the accuracy and the computation time in practical computation. Numerical results reveal that, compared to two baselines that sacrifice certain network stability to achieve a more flexible reassignment, the proposed model reduces the network instability by 27\% and 52\% in average, respectively, with obtaining the comparable maximum utilization ratio.},
  archive      = {J_TPDS},
  author       = {Fujun He and Eiji Oki},
  doi          = {10.1109/TPDS.2023.3285898},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2352-2364},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Preventive priority setting against multiple controller failures in software defined networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable deep reinforcement learning-based online routing
for multi-type service requirements. <em>TPDS</em>, <em>34</em>(8),
2337–2351. (<a href="https://doi.org/10.1109/TPDS.2023.3284651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging applications raise critical QoS requirements for the Internet. The improvements in flow classification technologies, software-defined networks (SDN), and programmable network devices make it possible to fast identify users’ requirements and control the routing for fine-grained traffic flows. Meanwhile, the problem of optimizing the forwarding paths for traffic flows with multiple QoS requirements in an online fashion is not addressed sufficiently. To address the problem, we propose DRL-OR-S, a highly scalable online routing algorithm using multi-agent deep reinforcement learning. DRL-OR-S adopts a comprehensive reward function, an efficient learning algorithm, and a novel deep neural network structure to learn appropriate routing strategies for different types of flow requirements. In order to enhance the generalization and scalability, we propose a novel graph-based actor-critic network architecture and a carefully designed input state for DRL-OR-S. To accelerate the training process and guarantee reliability, we further introduce an NN-simulator for efficient offline training and a safe learning mechanism to avoid unsafe routes during the online routing process. We implement DRL-OR-S under SDN architecture and conduct Mininet-based experiments using real network topologies and traffic traces. The results validate that DRL-OR-S can well satisfy the requirements of latency-sensitive, throughput-sensitive, latency-throughput-sensitive, and latency-loss-sensitive flows at the same time, while exhibiting great adaptiveness and reliability under the scenarios of link failure, traffic change, unseen large topology and partial deployment.},
  archive      = {J_TPDS},
  author       = {Chenyi Liu and Pingfei Wu and Mingwei Xu and Yuan Yang and Nan Geng},
  doi          = {10.1109/TPDS.2023.3284651},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2337-2351},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scalable deep reinforcement learning-based online routing for multi-type service requirements},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Taskgraph: A low contention OpenMP tasking framework.
<em>TPDS</em>, <em>34</em>(8), 2325–2336. (<a
href="https://doi.org/10.1109/TPDS.2023.3284219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OpenMP is the de-facto standard for shared memory systems in High-Performance Computing (HPC). It includes a tasking model that offers a high-level of abstraction to effectively exploit structured (loop-based) and highly dynamic unstructured (task-based) parallelism in an easy and flexible way. Unfortunately, the run-time overheads introduced to manage tasks are (very) high in most common OpenMP frameworks (e.g., GCC, LLVM), which defeats the potential benefits of the tasking model, and makes it suitable for coarse-grained tasks only. This paper presents taskgraph , a framework that uses a task dependency graph (TDG) to represent a region of code implemented with OpenMP tasks in order to reduce the run-time overheads associated with the management of tasks, i.e., contention and parallel orchestration, including task creation and synchronization. The TDG avoids the overheads related to the resolution of task dependencies and greatly reduces those deriving from accesses to shared resources. Moreover, the taskgraph framework introduces in OpenMP the record-and-replay execution model that accelerates the taskgraph region from its second execution. Overall, the multiple optimizations presented in this paper allow exploiting fine-grained OpenMP tasks to cope with the trend in current applications pointing to leverage massive on-node parallelism, fine-grained and dynamic scheduling paradigms. The framework is implemented on LLVM 15.0. Results show that the taskgraph implementation outperforms the vanilla OpenMP system in terms of performance and scalability, for all structured and unstructured parallelism, and considering coarse and fine grained tasks. Furthermore, the proposed framework makes the tasking model a competitive alternative to the OpenMP thread model in most cases.},
  archive      = {J_TPDS},
  author       = {Chenle Yu and Sara Royuela and Eduardo Quiñones},
  doi          = {10.1109/TPDS.2023.3284219},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2325-2336},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Taskgraph: A low contention OpenMP tasking framework},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). P4SGD: Programmable switch enhanced model-parallel training
on generalized linear models on distributed FPGAs. <em>TPDS</em>,
<em>34</em>(8), 2311–2324. (<a
href="https://doi.org/10.1109/TPDS.2023.3279255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized linear models (GLMs) are a widely utilized family of machine learning models in real-world applications. As data size increases, it is essential to perform efficient distributed training for these models. However, existing systems for distributed training have a high cost for communication and often use large batch sizes to balance computation and communication, which negatively affects convergence. Therefore, we argue for an efficient distributed GLM training system that strives to achieve linear scalability, while keeping batch size reasonably low. As a start, we propose P4SGD, a distributed heterogeneous training system that efficiently trains GLMs through model parallelism between distributed FPGAs and through forward-communication-backward pipeline parallelism within an FPGA. Moreover, we propose a light-weight, latency-centric in-switch aggregation protocol to minimize the latency of the AllReduce operation between distributed FPGAs, powered by a programmable switch. As such, to our knowledge, P4SGD is the first solution that achieves almost linear scalability between distributed accelerators through model parallelism. We implement P4SGD on eight Xilinx U280 FPGAs and a Tofino P4 switch. Our experiments show P4SGD converges up to 6.5X faster than the state-of-the-art GPU counterpart.},
  archive      = {J_TPDS},
  author       = {Hongjing Huang and Yingtao Li and Jie Sun and Xueying Zhu and Jie Zhang and Liang Luo and Jialin Li and Zeke Wang},
  doi          = {10.1109/TPDS.2023.3279255},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2311-2324},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {P4SGD: Programmable switch enhanced model-parallel training on generalized linear models on distributed FPGAs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ESMO: Joint frame scheduling and model caching for edge
video analytics. <em>TPDS</em>, <em>34</em>(8), 2295–2310. (<a
href="https://doi.org/10.1109/TPDS.2023.3281598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancements in Machine Learning (ML) and edge computing, increasing efforts have been devoted to edge video analytics . However, most of the existing works fail to consider the cooperation of edge nodes for ML model caching and video frame scheduling, thus less efficient in practical scenarios with diverse requirements. In this article, we propose a novel approach named ESMO (joint fram E S cheduling and MO del caching) to jointly optimize Frame Scheduling and Model Caching (FSMC), aiming at enhancing the performance of edge video analytics. In detail, we decompose the FSMC as three sub-problems, where the first two sub-problems (i.e., user&#39;s transmit power and edge computing resources allocation problems) are proven to be quasi-convex and strictly convex, respectively; while the third main sub-problem (i.e., trade-off among the video analytics (VA) accuracy, service delay and energy consumption) is NP-hard. Therefore, an efficient Two-layers Genetic Algorithm based algorithm (i.e., TGA-FSMC) is designed to find the close-to-optimal frame scheduling and the model caching decisions in an iterative manner. Finally, we deploy a target recognition prototype to comprehensively evaluate the practical performance in diverse edge nodes and CNN models. Extensive experiments demonstrate the empirical superiority of the ESMO over alternatives on real-world edge video analytics platforms, and it achieves 37.5\% $\sim$ 87.2\% performance improvement.},
  archive      = {J_TPDS},
  author       = {Ting Li and Jiyan Sun and Yinlong Liu and Xu Zhang and Dali Zhu and Zhaorui Guo and Liru Geng},
  doi          = {10.1109/TPDS.2023.3281598},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2295-2310},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ESMO: Joint frame scheduling and model caching for edge video analytics},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeuroSpector: Systematic optimization of dataflow scheduling
in DNN accelerators. <em>TPDS</em>, <em>34</em>(8), 2279–2294. (<a
href="https://doi.org/10.1109/TPDS.2023.3283491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an optimization framework named NeuroSpector that systematically analyzes the dataflow of deep neural network (DNN) accelerators and rapidly identifies optimal execution methods. The proposed methodology is demonstrated to work effectively with a variety of accelerator architectures and DNN workloads. It has been a baffling challenge to devise scheduling schemes for neural accelerators to maximize energy efficiency and performance. The challenge lies in that hardware specifications associated with multi-dimensional DNN data create an enormous number of possible scheduling options that can be exerted on accelerators. Related work suggested various techniques to solve the challenge encompassing brute-force search of massive solution spaces pruned by user constraints, solving the objective functions of system models, learning-based optimization, etc. However, each suggested technique was devised only for a specific accelerator model. Therefore, we find that they are not adaptively applicable to different accelerators and DNN workloads in that they produce hit-or-miss results with 100.1\% greater energy and cycles on average compared to optimal scheduling schemes obtained from fully comprehensive brute-force searches. In contrast, NeuroSpector identifies efficient execution methods for various accelerators and workloads with only 1.5\% differences on average to the optimal scheduling solutions. The optimization strategy of NeuroSpector is based on an observation that optimal executions are strongly correlated with minimizing data movements to the lower-level memory hierarchy of accelerators rather than maximizing the utilization of processing elements. Thus, NeuroSpector prioritizes optimizing lower-level components in the accelerator hierarchy, which is proven highly effective for various accelerators and DNN workloads.},
  archive      = {J_TPDS},
  author       = {Chanho Park and Bogil Kim and Sungmin Ryu and William J. Song},
  doi          = {10.1109/TPDS.2023.3283491},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2279-2294},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {NeuroSpector: Systematic optimization of dataflow scheduling in DNN accelerators},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Falcon: Fair and efficient online file transfer
optimization. <em>TPDS</em>, <em>34</em>(8), 2265–2278. (<a
href="https://doi.org/10.1109/TPDS.2023.3282872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research networks provide high-speed wide-area network connectivity between research and education institutions to facilitate large-scale data transfers. However, scalability issues of legacy transfer applications such as scp and FTP hinder the effective utilization of these networks. Although researchers extended the legacy transfer applications to increase their performance by exploiting I/O and network parallelism, these solutions necessitate users to fine-tune parallelism level, a task that is challenging even for experienced users due to the dynamic nature of networks. In this article, we propose an online optimization algorithm, Falcon , to tune the degree of parallelism for file transfers to maximize transfer throughput while keeping system overhead at a minimum. As research networks are shared infrastructures, we introduce a game theory-inspired novel utility function to evaluate the performance of various parallelism levels such that competing transfers are guaranteed to converge to a fair and stable solution. We assessed the performance of Falcon in isolated and production high-speed networks and found that it can discover optimal transfer parallelism in as little as 20 seconds and outperform the state-of-the-art solutions by more than $2\times$ . Moreover, Falcon is guaranteed to converge to Nash Equilibrium when multiple transfers compete for the same resources with the help of its game theory-inspired utility function. Finally, we demonstrate that Falcon can also be used as a central transfer scheduler to speed up convergence time, increase stability, and enforce system/user-level resource limitations in shared networks.},
  archive      = {J_TPDS},
  author       = {Md Arifuzzaman and Brian Bockelman and James Basney and Engin Arslan},
  doi          = {10.1109/TPDS.2023.3282872},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2265-2278},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Falcon: Fair and efficient online file transfer optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting erasure-coded multi-stripe repair in rack
architecture and heterogeneous clusters: Design and analysis.
<em>TPDS</em>, <em>34</em>(8), 2251–2264. (<a
href="https://doi.org/10.1109/TPDS.2023.3282180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale storage systems have introduced erasure codes to guarantee high data reliability, yet inevitably at the expense of high repair costs. In practice, storage nodes are usually divided into different racks, and data blocks in nodes are organized into multiple stripes independently manipulated by erasure code. Due to the scarcity and heterogeneity of the cross-rack bandwidth, the cross-rack transmission dominates the entire repair costs. When erasure code is deployed in rack architectures, existing repair techniques are limited in different aspects: neglecting the heterogeneous cross-rack bandwidth, less consideration for multi-stripe failure, and no special treatment on repair-link scheduling. In this paper, we present CMRepair, a Cross-rack Multi-stripe Repair technique that aims to reduce the repair time for multi-stripes failure repair in heterogeneous erasure-coded clusters. CMRepair first carefully chooses the nodes for reading/repairing blocks and searches for the multi-stripe repair solution. It adopts different algorithms to adjust the solution, including the Computation Time Priority (CTP) algorithm based on the greedy idea and the Repair Time Priority (RTP) algorithm based on the meta-heuristics idea. Furthermore, CMRepair selectively schedules the execution orders of cross-rack links, with the primary objective of saturating the unused upload/download bandwidth resources and avoiding network congestion. The experiments show that CMRepair with the CTP algorithm can reduce 27.59\%-58.12\% of the repair time while only introducing negligible computation overhead, and CMRepair with the RTP algorithm can reduce 33.52\%-97.75\% of the repair time in an acceptable computation time, over existing repair techniques.},
  archive      = {J_TPDS},
  author       = {Hai Zhou and Dan Feng},
  doi          = {10.1109/TPDS.2023.3282180},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2251-2264},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Boosting erasure-coded multi-stripe repair in rack architecture and heterogeneous clusters: Design and analysis},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint caching and routing in cache networks with arbitrary
topology. <em>TPDS</em>, <em>34</em>(8), 2237–2250. (<a
href="https://doi.org/10.1109/TPDS.2023.3276724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-network caching and flexible routing are two of the most celebrated advantages of next generation network infrastructures. Yet few solutions are available for jointly optimizing caching and routing that provide performance guarantees for networks with arbitrary topology. We take a holistic approach towards this fundamental problem by analyzing its complexity in all the cases and developing polynomial-time algorithms with approximation guarantees in important special cases. We also reveal the fundamental challenge in achieving guaranteed approximation in the general case and propose an alternating optimization algorithm with good empirical performance and fast convergence. Our algorithms have demonstrated superior performance in both routing cost and congestion compared to the state-of-the-art solutions in evaluations based on real topology and request traces.},
  archive      = {J_TPDS},
  author       = {Tian Xie and Sanchal Thakkar and Ting He and Patrick McDaniel and Quinn Burke},
  doi          = {10.1109/TPDS.2023.3276724},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2237-2250},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Joint caching and routing in cache networks with arbitrary topology},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Full-stack optimizing transformer inference on ARM many-core
CPU. <em>TPDS</em>, <em>34</em>(7), 2221–2235. (<a
href="https://doi.org/10.1109/TPDS.2023.3280805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past several years have witnessed tremendous success of transformer models in natural language processing (NLP), and their current landscape is increasingly diverse. Although GPU gradually becomes the dominating workhorse and de facto standard for deep learning, there are still many scenarios where using CPU remains a prevalent choice.Recently, ARM many-core processor starts emigrating to cloud computing and high-performance computing, which is promising to deploy transformer inference. In this paper, we identify several performance bottlenecks of existing inference runtime on many-core CPU including low-core usage, isolated thread configuration, inappropriate implementation of general matrix multiply (GEMM), and redundant computations for variable-length inputs. To tackle these problems, full-stack optimizations are conducted for these challenges from service level to operator level. We explore multi-instance parallelization at the service level to improve CPU core usage. To improve parallel efficiency of the inference runtime, we design NUMA-aware thread scheduling and a look-up table for optimal parallel configurations. The GEMM implementation is tailored for some critical modules to exploit the characteristics of transformer workload. To eliminate redundant computations, a novel storage format is designed and implemented to pack sparse data and a load balancing strategy is proposed for tasks with different sparsity. Experiments show that our implementation can outperform existing solutions by 1.1x to 6x with fixed-length inputs. For variable-length inputs, it achieves 1.9x to 8x speedups on different ARM many-core processors.},
  archive      = {J_TPDS},
  author       = {Jiazhi Jiang and Jiangsu Du and Dan Huang and Zhiguang Chen and Yutong Lu and Xiangke Liao},
  doi          = {10.1109/TPDS.2023.3280805},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2221-2235},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Full-stack optimizing transformer inference on ARM many-core CPU},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DOPpler: Parallel measurement infrastructure for auto-tuning
deep learning tensor programs. <em>TPDS</em>, <em>34</em>(7), 2208–2220.
(<a href="https://doi.org/10.1109/TPDS.2023.3279233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneity of Deep Learning models, libraries, and hardware poses an important challenge for improving model inference performance. Auto-tuners address this challenge via automatic tensor program optimization towards a target-device. However, auto-tuners incur a substantial time cost to complete given their design necessitates performing tensor program candidate measurements serially within an isolated target-device to minimize latency measurement inaccuracy. In this article we propose DOPpler, a parallel auto-tuning measurement infrastructure. DOPpler allows for considerable auto-tuning speedup over conventional approaches whilst maintaining high-quality tensor program optimization. DOPpler accelerates the auto-tuning process by proposing a parallel execution engine to efficiently execute candidate tensor programs in parallel across the CPU-host and GPU target-device, and overcomes measurement inaccuracy by introducing a high-precision on-device measurement technique when measuring tensor program kernel latency. DOPpler is designed to automatically calculate the optimal degree of parallelism to provision fast and accurate auto-tuning for different tensor programs, auto-tuners and target-devices. Experiment results show that DOPpler reduces total auto-tuning time by 50.5\% on average whilst achieving optimization gains equivalent to conventional auto-tuning infrastructure.},
  archive      = {J_TPDS},
  author       = {Damian Borowiec and Gingfung Yeung and Adrian Friday and Richard Harper and Peter Garraghan},
  doi          = {10.1109/TPDS.2023.3279233},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2208-2220},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DOPpler: Parallel measurement infrastructure for auto-tuning deep learning tensor programs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Faster federated learning with decaying number of local SGD
steps. <em>TPDS</em>, <em>34</em>(7), 2198–2207. (<a
href="https://doi.org/10.1109/TPDS.2023.3277367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Federated Learning (FL) client devices connected over the internet collaboratively train a machine learning model without sharing their private data with a central server or with other clients. The seminal Federated Averaging (FedAvg) algorithm trains a single global model by performing rounds of local training on clients followed by model averaging. FedAvg can improve the communication-efficiency of training by performing more steps of Stochastic Gradient Descent (SGD) on clients in each round. However, client data in real-world FL is highly heterogeneous, which has been extensively shown to slow model convergence and harm final performance when $K &amp;gt; 1$ steps of SGD are performed on clients per round. In this article we propose decaying $K$ as training progresses, which can jointly improve the final performance of the FL model whilst reducing the wall-clock time and the total computational cost of training compared to using a fixed $K$ . We analyse the convergence of FedAvg with decaying $K$ for strongly-convex objectives, providing novel insights into the convergence properties, and derive three theoretically-motivated decay schedules for $K$ . We then perform thorough experiments on four benchmark FL datasets (FEMNIST, CIFAR100, Sentiment140, Shakespeare) to show the real-world benefit of our approaches in terms of real-world convergence time, computational cost, and generalisation performance.},
  archive      = {J_TPDS},
  author       = {Jed Mills and Jia Hu and Geyong Min},
  doi          = {10.1109/TPDS.2023.3277367},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2198-2207},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Faster federated learning with decaying number of local SGD steps},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ObjDedup: High-throughput object storage layer for backup
systems with block-level deduplication. <em>TPDS</em>, <em>34</em>(7),
2180–2197. (<a href="https://doi.org/10.1109/TPDS.2023.3250501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immense popularity of object storage is also affecting the market of backup. Not only have novel backup solutions emerged that utilize cloud-based object storage as backends, but also support for object storage interfaces is increasingly expected from traditional dedicated backup appliances. This latter trend especially concerns systems with data deduplication, as they can offer compelling gains in storage capacity and throughput. However, such systems have been designed for interfaces and workloads that are markedly different from those encountered in object storage. Notably, they expect data to be written in portions that are orders of magnitude longer than those in the novel object-storage-oriented backup applications. In this light, we contribute twofold. First, contrasting the properties of object storage interfaces with usage patterns from 686 commercial deployments of backup appliances, we identify specific issues an implementation of such an interface has to address to offer adequate performance in a backup system with block-level deduplication. In particular, we show that a major challenge is efficient metadata management. Second, we present distributed data structures and algorithms to handle object metadata in backup systems with block-level deduplication. Subsequently, we implement them as an object storage layer for our HYDRAstor backup system. In comparison to object storage without in-line deduplication, our solution achieves 1.8–3.93x higher write throughput. Compared to object storage on top of a state-of-the-art file-based backup system, it processes 5.26–11.34x more object put operations per time unit.},
  archive      = {J_TPDS},
  author       = {Andrzej Jackowski and Łukasz Ślusarczyk and Krzysztof Lichota and Michał Wełnicki and Rafał Wijata and Mateusz Kielar and Tadeusz Kopeć and Cezary Dubnicki and Konrad Iwanicki},
  doi          = {10.1109/TPDS.2023.3250501},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2180-2197},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ObjDedup: High-throughput object storage layer for backup systems with block-level deduplication},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COFFEE: Cross-layer optimization for fast and efficient
executions of sinkhorn-knopp algorithm on HPC systems. <em>TPDS</em>,
<em>34</em>(7), 2167–2179. (<a
href="https://doi.org/10.1109/TPDS.2023.3277915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present COFFEE, cross-layer optimization for fast and efficient executions of the Sinkhorn-Knopp (SK) algorithm on HPC systems with clusters of compute nodes by exploring some architectural features of the system. By analyzing the performance of a typical implementation of the SK algorithm on such a system, a huge performance gap is observed between the row rescaling and column rescaling of the algorithm, where the latter requires much more time than the former. We also found that the costly MPI communication of the column rescaling seriously hinders the exploitation of parallelism. By observing and leveraging unique architectural characteristics across different system optimizations, such as column rescaling redesign, data blocking, micro-kernel design, enhanced intra-node and inter-node communication in MPI, etc., COFFEE is able to explore cross-layer optimization opportunities that enable fast and efficient execution of the SK algorithm. Our experimental results show that COFFEE provides up to 7.5X with an average of 2.0X performance improvement over the typical implementation on a single node, and up to 2.9X with an average of 1.6X performance improvement over the state-of-the-art MPI Allreduce algorithms on Tianhe-1 supercomputer.},
  archive      = {J_TPDS},
  author       = {Chengyu Sun and Huizhang Luo and Hong Jiang and Jeff Zhang and Kenli Li},
  doi          = {10.1109/TPDS.2023.3277915},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2167-2179},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {COFFEE: Cross-layer optimization for fast and efficient executions of sinkhorn-knopp algorithm on HPC systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EESaver: Saving energy dynamically for green multi-access
edge computing. <em>TPDS</em>, <em>34</em>(7), 2155–2166. (<a
href="https://doi.org/10.1109/TPDS.2023.3277619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rollout of the 5G network around the globe, a massive number of edge servers have been deployed to host online applications demanding low service latency for users. These edge servers constitute multi-access edge computing (MEC) systems. Running 24/7, edge servers consume tremendous energy and take up a great part of global carbon emissions. The edge energy-saving (EES) problem is needed to facilitate energy-efficient edge resource provisions. Unfortunately, existing energy-saving approaches designed for data centers are becoming impractical. First, edge servers are used to provide services to a specific geographical area. Its energy utilization is impacted by the temporal distribution of users within its coverage. Second, a user could be accommodated by any of its neighbor edge servers. Third, it is possible to activate and deactivate individual physical machines that facilitate an edge server as needed. Thus, EES is designed to save the system energy of physical machines in a long term by serving the users over time. EES problem has been formulated systematically and its problem hardness has been analyzed theoretically, then we propose EESaver (Edge Energy Saver) for formulating EES strategies dynamically over time to facilitate green MEC. EESaver&#39;s superior performance is tested comprehensively.},
  archive      = {J_TPDS},
  author       = {Guangming Cui and Qiang He and Xiaoyu Xia and Feifei Chen and Yun Yang},
  doi          = {10.1109/TPDS.2023.3277619},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2155-2166},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EESaver: Saving energy dynamically for green multi-access edge computing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-aware, device-to-device assisted federated learning
in edge computing. <em>TPDS</em>, <em>34</em>(7), 2138–2154. (<a
href="https://doi.org/10.1109/TPDS.2023.3277423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surging of deep learning brings new vigor and vitality to shape the prospect of intelligent Internet of Things (IoT), and the rise of edge intelligence enables provisioning real-time deep neural network (DNN) inference services for mobile users. To perform efficient and effective DNN model training in edge computing environments while preserving training data security and privacy of IoT devices, federated learning has been envisioned as an ideal learning paradigm for this purpose. In this article, we study energy-aware DNN model training in edge computing. We first formulate a novel energy-aware, Device-to-Device (D2D) assisted federated learning problem with the aim to minimize the global loss of a training DNN model, subject to bandwidth capacity on an edge server and energy capacity on each IoT device. We then devise a near-optimal learning algorithm for the problem when the training data follows the i.i.d. data distribution. The crux of the proposed algorithm is to explore using the energy of neighboring devices of each device for its local model uploading, by reducing the problem to a series of weighted maximum matching problems in corresponding auxiliary graphs. We also consider the problem without the assumption of the i.i.d. data distribution, for which we propose an efficient heuristic algorithm. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results show that the proposed algorithms are promising.},
  archive      = {J_TPDS},
  author       = {Yuchen Li and Weifa Liang and Jing Li and Xiuzhen Cheng and Dongxiao Yu and Albert Y. Zomaya and Song Guo},
  doi          = {10.1109/TPDS.2023.3277423},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2138-2154},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy-aware, device-to-device assisted federated learning in edge computing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed encoding and updating for SAZD coded distributed
training. <em>TPDS</em>, <em>34</em>(7), 2124–2137. (<a
href="https://doi.org/10.1109/TPDS.2023.3276888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear combination (LC) based coded distributed computing (CDC) suffers from the problem of poor numerical stability. Therefore, LC-CDC based model parallel (MP) training for a deep nueral network (DNN) may have poor accuracy. To enhance accuracy, we propose to replace LC by shift-and-addition (SA) and replace matrix inversion by zigzag decoding (ZD) in the encoding and decoding process of each layer, respectively, and call the scheme Naive SAZD-CDC based MP training (N-SAZD-CDC-MP-T). However, N-SAZD-CDC-MP-T encounters the problem of bottleneck at the master node, which is caused by frequent encoding/decoding at the master node and frequent huge volume of data delivery between master and worker node. This bottleneck problem may pull down the training speed significantly. To alleviate this bottleneck problem, we further design an enhanced version, by offloading certain processing from master node to distributed encoding and updating (DEU) at the worker nodes and call it DEU-SAZD-CDC-MP-T. A proof that DEU-SAZD-CDC-MP-T automatically maitains the code structure during each iteration is provided. Extensive numerical studies show that the prediction accuracy of SAZD-CDC-MP-T improves significantly over that of Poly (which is representative of LC) based scheme. In addition, the training speed of DEU-SAZD-CDC-MP-T over N-SAZD-CDC-MP-T is improved significantly.},
  archive      = {J_TPDS},
  author       = {Mingjun Dai and Jialong Yuan and Qingwen Huang and Xiaohui Lin and Hui Wang},
  doi          = {10.1109/TPDS.2023.3276888},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2124-2137},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed encoding and updating for SAZD coded distributed training},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-tier GPU virtualization for deep learning in
cloud-edge systems. <em>TPDS</em>, <em>34</em>(7), 2107–2123. (<a
href="https://doi.org/10.1109/TPDS.2023.3274957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerator virtualization offers several advantages in the context of cloud-edge computing. Relatively weak user devices can enhance performance when running workloads by accessing virtualized accelerators available on other resources in the cloud-edge continuum. However, cloud-edge systems are heterogeneous, often leading to compatibility issues arising from various hardware and software stacks present in the system. One mechanism to alleviate this issue is using containers for deploying workloads. Containers isolate applications and their dependencies and store them as images that can run on any device. In addition, user devices may move during the course of application execution, and thus mechanisms such as container migration are required to move running workloads from one resource to another in the network. Furthermore, an optimal destination will need to be determined when migrating between virtual accelerators. Scheduling and placement strategies are incorporated to choose the best possible location depending on the workload requirements. This paper presents AVEC , a framework for accelerator virtualization in cloud-edge computing. The AVEC framework enables the offloading of deep learning workloads for inference from weak user devices to computationally more powerful devices in a cloud-edge network. AVEC incorporates a mechanism that efficiently manages and schedules the virtualization of accelerators. It also supports migration between accelerators to enable stateless container migration. The experimental analysis highlights that AVEC can achieve up to 7x speedup by offloading applications to remote resources. Furthermore, AVEC features a low migration downtime that is less than 5 seconds.},
  archive      = {J_TPDS},
  author       = {Jason Kennedy and Vishal Sharma and Blesson Varghese and Carlos Reaño},
  doi          = {10.1109/TPDS.2023.3274957},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2107-2123},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-tier GPU virtualization for deep learning in cloud-edge systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CD-MSA: Cooperative and deadline-aware scheduling for
efficient multi-tenancy on DNN accelerators. <em>TPDS</em>,
<em>34</em>(7), 2091–2106. (<a
href="https://doi.org/10.1109/TPDS.2023.3276759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With DNN turning into the backbone of AI cloud services and propelling the emergence of INFerence-as-a-Service (INFaaS), DNN-specific accelerators have become the indispensable components of cloud inference systems. Due to the conservative “one-task-at-a-time” working mode and deadline blindness of those accelerators, implementing multi-tenancy that aims to improve the cost-effectiveness and meet SLA requirements is intractable. Recent studies including the temporal and spatial approaches, employ manifold scheduling mechanisms and sophisticated architecture innovations to address the challenge. However, these researches either still neglect the deadline awareness or render inevitable and expensive hardware overheads such as switches and storage. In this paper, we present Cooperative and Deadline-aware Multi-Systolic-Array scheduling (CD-MSA), a low-cost solution for the cloud inference that utilizes the real time mechanism and task-level parallelism to enable efficient multi-tenancy. Based on our preemptive multi-systolic-array accelerator architecture supporting the simultaneous task co-location, we first construct a fine-grained DNN execution model to lay the groundwork for the lightweight preemption. Second, we design a cooperative, deadline- and laxity-aware scheduler in conjunction with an efficient schedulability test method for better QoS guarantee without introducing additional hardware cost. Finally, to further promote the overall throughput, we propose dynamic task fusion , a software approach that fuses different tasks into the logically “multi-threading” tasks at runtime. We compare CD-MSA with several state-of-the-art researches across three multi-DNN workloads. The evaluation results show CD-MSA improves the latency-bounded throughput, SLA satisfaction rate and weighted system throughput by up to 62\%, 63\% and 27\%, respectively.},
  archive      = {J_TPDS},
  author       = {Chunyang Wang and Yuebin Bai and Desen Sun},
  doi          = {10.1109/TPDS.2023.3276759},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2091-2106},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CD-MSA: Cooperative and deadline-aware scheduling for efficient multi-tenancy on DNN accelerators},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A proactive on-demand content placement strategy in edge
intelligent gateways. <em>TPDS</em>, <em>34</em>(7), 2072–2090. (<a
href="https://doi.org/10.1109/TPDS.2023.3249797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bandwidth-intensive applications transmit large-scale video data in the network. It causes backhaul bottlenecks and affects user experience. Deploying edge cache on an access point (AP) is a popular method to bring content files closer to end-users, but it faces significant challenges, especially in efficiently predicting and satisfying different users’ future content requests with limited cache capacity. In this article, we propose an intelligent gateway assisted edge cache deployment strategy (GACD), which jointly considers traffic usage patterns in multiple APs and the impact of new content on the cache performance. In GACD, The cache content placement problem is formulated as a many-to-one bidirectional matching problem with a dynamic quota allocation, aiming to improve cache resource utilization and minimize the average delivery latency. To address this problem, we design a heterogeneous information networks based prediction algorithm to predict end-users’ potential preference of new content files. Then, we adapt the seasonal autoregressive integrated moving average model for traffic usage prediction, and propose a many-to-one matching algorithm to achieve dynamic matching quota adjustment and efficient cache content placement. We conduct extensive real-world trace-based experiments to validate the performance of GACD. Compared with six alternative cache strategies, GACD improves the hit rate by 23.9\% on average, reduces the average content delivery delay by 19.02\%, and increases the accuracy by 31.02\% on average.},
  archive      = {J_TPDS},
  author       = {Hui Sun and Yiru Chen and Kewei Sha and Shaoyuan Huang and Xiaofei Wang and Weisong Shi},
  doi          = {10.1109/TPDS.2023.3249797},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2072-2090},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A proactive on-demand content placement strategy in edge intelligent gateways},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical cloud-edge scheduling for large-scale crowdsourced
live streaming. <em>TPDS</em>, <em>34</em>(7), 2055–2071. (<a
href="https://doi.org/10.1109/TPDS.2023.3267731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though conventional wisdom claims that in order to improve viewer engagement, the cloud-edge providers should serve the viewers with the nearest edge nodes, however, we show that doing this for crowdsourced live streaming (CLS) services can introduce significant costs inefficiency. In this paper, we first carry out large-scale measurement analysis by using the real-world service data from Huawei Cloud, a representative cloud-edge provider in China. We observe that the massive number of channels has proposed great burdens to the operating expenditure of the cloud-edge providers, and most importantly, unbalanced viewer distribution makes the edge nodes suffer significant costs inefficiency. To tackle the above concerns, we propose AggCast , a novel CLS scheduling framework to optimize the edge node utilization for the cloud-edge provider. The core idea of AggCast is to aggregate some viewers that are initially scattered on different regions, and assign them to fewer pre-selected nodes, thereby reducing bandwidth costs. In particular, by integrating the useful insights obtained from our large-scale measurement, AggCast can not only ensure that quality of experience (QoS) does not suffer degradation, but also satisfy the systematic requirements of CLS services. AggCast has been A/B tested and fully deployed. The online and trace-driven experiments show that, compared to the most prevalent method, AggCast saves over 16.3\% back-to-source (BTS) bandwidth costs while significantly improving QoS (startup latency, stall frequency and stall time are reduced over 12.3\%, 4.57\% and 3.91\%, respectively).},
  archive      = {J_TPDS},
  author       = {Ruixiao Zhang and Changpeng Yang and Xiaochan Wang and Tianchi Huang and Chenglei Wu and Jiangchuan Liu and Lifeng Sun},
  doi          = {10.1109/TPDS.2023.3267731},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2055-2071},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Practical cloud-edge scheduling for large-scale crowdsourced live streaming},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Securing distributed SGD against gradient leakage threats.
<em>TPDS</em>, <em>34</em>(7), 2040–2054. (<a
href="https://doi.org/10.1109/TPDS.2023.3273490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a holistic approach to gradient leakage resilient distributed Stochastic Gradient Descent (SGD). First , we analyze two types of strategies for privacy-enhanced federated learning: (i) gradient pruning with random selection or low-rank filtering and (ii) gradient perturbation with additive random noise or differential privacy noise. We analyze the inherent limitations of these approaches and their underlying impact on privacy guarantee, model accuracy, and attack resilience. Next , we present a gradient leakage resilient approach to securing distributed SGD in federated learning, with differential privacy controlled noise as the tool. Unlike conventional methods with the per-client federated noise injection and fixed noise parameter strategy, our approach keeps track of the trend of per-example gradient updates. It makes adaptive noise injection closely aligned throughout the federated model training. Finally , we provide an empirical privacy analysis on the privacy guarantee, model utility, and attack resilience of the proposed approach. Extensive evaluation using five benchmark datasets demonstrates that our gradient leakage resilient approach can outperform the state-of-the-art methods with competitive accuracy performance, strong differential privacy guarantee, and high resilience against gradient leakage attacks.},
  archive      = {J_TPDS},
  author       = {Wenqi Wei and Ling Liu and Jingya Zhou and Ka-Ho Chow and Yanzhao Wu},
  doi          = {10.1109/TPDS.2023.3273490},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2040-2054},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Securing distributed SGD against gradient leakage threats},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Offloading algorithms for maximizing inference accuracy on
edge device in an edge intelligence system. <em>TPDS</em>,
<em>34</em>(7), 2025–2039. (<a
href="https://doi.org/10.1109/TPDS.2023.3267458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of edge computing, the problem of offloading jobs between an Edge Device (ED) and an Edge Server (ES) received significant attention in the past. Motivated by the fact that an increasing number of applications are using Machine Learning (ML) inference from the data samples collected at the EDs, we study the problem of offloading inference jobs by considering the following novel aspects: 1) in contrast to a typical computational job, the processing time of an inference job depends on the size of the ML model, and 2) recently proposed Deep Neural Networks (DNNs) for resource-constrained devices provide the choice of scaling down the model size by trading off the inference accuracy. Considering that multiple ML models are available at the ED, and a powerful ML model is available at the ES, we formulate an Integer Linear Programming (ILP) problem with the objective of maximizing the total inference accuracy of $n$ data samples at the ED subject to a time constraint $T$ on the makespan. Noting that the problem is NP-hard, we propose an approximation algorithm Accuracy Maximization using LP-Relaxation and Rounding (AMR $^{2}$ ) and prove that it results in a makespan at most $\text{2}T$ and achieves a total accuracy that is lower by a small constant from the optimal total accuracy implying that AMR $^{2}$ is asymptotically optimal. Further, if the data samples are identical we propose Accuracy Maximization using Dynamic Programming (AMDP), an optimal pseudo-polynomial time algorithm. Furthermore, we extend AMR $^{2}$ for the case of multiple ESs, where each ES is equipped with a powerful ML model. As proof of concept, we implemented AMR $^{2}$ on a Raspberry Pi, equipped with MobileNets, that is connected to a server equipped with ResNet, and studied the total accuracy and makespan performance of AMR $^{2}$ for image classification.},
  archive      = {J_TPDS},
  author       = {Andrea Fresa and Jaya Prakash Champati},
  doi          = {10.1109/TPDS.2023.3267458},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2025-2039},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Offloading algorithms for maximizing inference accuracy on edge device in an edge intelligence system},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FLUPS - a flexible and performant massively parallel fourier
transform library. <em>TPDS</em>, <em>34</em>(7), 2011–2024. (<a
href="https://doi.org/10.1109/TPDS.2023.3254302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massively parallel Fourier transforms are widely used in computational sciences, and specifically in computational fluid dynamics which involves unbounded Poisson problems. In practice the latter is usually the most time-consuming operation due to its inescapable all-to-all communication pattern. The original flups library tackles that issue with an implementation of the distributed Fourier transform tailor-made for successive resolutions of unbounded Poisson problems. However the proposed implementation lacks of flexibility as it only supports cell-centered data layout and features a plain communication strategy. This work extends the library along two directions. First, flups ’ implementation is generalized to support a node-centered data layout. Second, three distinct approaches are provided to handle the communications: one all-to-all, and two non-blocking implementations relying on manual packing and MPI_Datatype to communicate over the network. The proposed software is validated against analytical solutions for unbounded, semi-unbounded, and periodic domains. The performance of the approaches is then compared against accFFT , another distributed FFT implementation, using a periodic case. Finally the performance metrics of each implementation are analyzed and detailed on various top-tier European facilities up to 49,152 cores. This work brings flups up to a fully production-ready and performant distributed FFT library, featuring all the possible types of FFTs and with flexibility in the data-layout.},
  archive      = {J_TPDS},
  author       = {Pierre Balty and Philippe Chatelain and Thomas Gillis},
  doi          = {10.1109/TPDS.2023.3254302},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {2011-2024},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FLUPS - a flexible and performant massively parallel fourier transform library},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Redesigning OpenKMC for multi-component trillion-atom
simulations on the new sunway supercomputer. <em>TPDS</em>,
<em>34</em>(7), 1997–2010. (<a
href="https://doi.org/10.1109/TPDS.2023.3269625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The atomic kinetic Monte Carlo method plays an important role in material simulations by connecting the microscale mechanism with macroscale evolution. However, the long-time simulation of multi-component materials is highly challenging because it demands significant computing resources. With the advent of exascale computing, ultra-high computing power can enable kinetic Monte Carlo (KMC) simulations. In this paper, we deeply optimize OpenKMC for the new-generation Sunway supercomputer. This includes optimizing the memory access for the SW39000 architecture, eliminating various redundant computations at growing scales, and proposing a communication strategy for heterogeneous platforms. In addition, we expanded OpenKMC&#39;s simulation for multi-component alloys. Finally, the acceleration framework can produces a $37\times$ performance enhancement on the Sunway platform. Furthermore, when powered by 10 million cores, our program can perform trillion-atom simulations of complex multi-component alloys with 85\% parallel efficiency.},
  archive      = {J_TPDS},
  author       = {Lei Xu and Honghui Shang and Xin Chen and Yunquan Zhang and Lifang Wang and Xingyu Gao and Haifeng Song},
  doi          = {10.1109/TPDS.2023.3269625},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1997-2010},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Redesigning OpenKMC for multi-component trillion-atom simulations on the new sunway supercomputer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NIOT: A novel inference optimization of transformers on
modern CPUs. <em>TPDS</em>, <em>34</em>(6), 1982–1995. (<a
href="https://doi.org/10.1109/TPDS.2023.3269530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the machine learning era, model inference efficiency is one of the most important issues for machine learning systems. It is a major challenge to find the optimal configuration in a huge search space as the combinations of kernel fusion, memory tiling, and thread allocation strategies result in highly variable and unpredictable inference performance. The problem is particularly pronounced in models with large parameter matrices such as Transformers. In this article, we aim to develop a general and powerful framework for inference optimization, called NIOT, to achieve desirable efficiency for the prevailing Transformer-like models on CPUs. To take full advantage of modern CPU features such as SIMD and cache hierarchy, NIOT employs various methods to provide promising strategies tailored to the target Transformer model. Our C++ implementation of NIOT shows significant performance improvements over popular well-optimized model-serving runtimes such as PyTorch and ONNXRuntime.},
  archive      = {J_TPDS},
  author       = {Zining Zhang and Yao Chen and Bingsheng He and Zhenjie Zhang},
  doi          = {10.1109/TPDS.2023.3269530},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1982-1995},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {NIOT: A novel inference optimization of transformers on modern CPUs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TurboMGNN: Improving concurrent GNN training tasks on GPU
with fine-grained kernel fusion. <em>TPDS</em>, <em>34</em>(6),
1968–1981. (<a href="https://doi.org/10.1109/TPDS.2023.3267943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNN) have evolved as powerful models for graph representation learning. Many works have been proposed to support GNN training efficiently on GPU. However, these works only focus on a single GNN training task such as operator optimization, task scheduling, and programming model. Concurrent GNN training, which is needed in the applications such as neural network structure search, has not been explored yet. This work aims to improve the training efficiency of the concurrent GNN training tasks on GPU by developing fine-grained methods to fuse the kernels from different tasks. Specifically, we propose a fine-grained Sparse Matrix Multiplication (SpMM) based kernel fusion method to eliminate redundant accesses to graph data. In order to increase the fusion opportunity and reduce the synchronization cost, we further propose a novel technique to enable the fusion of the kernels in forward and backward propagation. Finally, in order to reduce the resource contention caused by the increased number of concurrent, heterogeneous GNN training tasks, we propose an adaptive strategy to group the tasks and match their operators according to resource contention. We have conducted extensive experiments, including kernel- and model-level benchmarks. The results show that the proposed methods can achieve up to 2.6X performance speedup.},
  archive      = {J_TPDS},
  author       = {Wenchao Wu and Xuanhua Shi and Ligang He and Hai Jin},
  doi          = {10.1109/TPDS.2023.3267943},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1968-1981},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TurboMGNN: Improving concurrent GNN training tasks on GPU with fine-grained kernel fusion},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Liberator: A data reuse framework for out-of-memory graph
computing on GPUs. <em>TPDS</em>, <em>34</em>(6), 1954–1967. (<a
href="https://doi.org/10.1109/TPDS.2023.3268662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph analytics are widely used including recommender systems, scientific computing, and data mining. Meanwhile, GPU has become the major accelerator for such applications. However, the graph size increases rapidly and often exceeds the GPU memory, incurring severe performance degradation due to frequent data transfers between the main memory and GPUs. To relieve this problem, we focus on the utilization of data in GPUs by taking advantage of the data reuse across iterations. In our studies, we deeply analyze the memory access patterns of graph applications at different granularities. We have found that the memory footprint is accessed with a roughly sequential scan without a hotspot, which infers an extremely long reuse distance. Based on our observation, we propose a novel framework, called Liberator , to exploit the data reuse within GPU memory. In Liberator , GPU memory is reserved for the data potentially accessed across iterations to avoid excessive data transfer between the main memory and GPUs. For the data not existing in GPU memory, a Merged and Aligned memory access manner is employed to improve the transmission efficiency. We also further optimize the framework by parallel processing of data in GPU memory and data in the main memory. We have implemented a prototype of the Liberator framework and conducted a series of experiments on performance evaluation. The experimental results show that Liberator can significantly reduce the data transfer overhead, which achieves an average of 2.7x speedup over a state-of-the-art approach.},
  archive      = {J_TPDS},
  author       = {Shiyang Li and Ruiqi Tang and Jingyu Zhu and Ziyi Zhao and Xiaoli Gong and Wenwen Wang and Jin Zhang and Pen-Chung Yew},
  doi          = {10.1109/TPDS.2023.3268662},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1954-1967},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Liberator: A data reuse framework for out-of-memory graph computing on GPUs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedProf: Selective federated learning based on
distributional representation profiling. <em>TPDS</em>, <em>34</em>(6),
1942–1953. (<a href="https://doi.org/10.1109/TPDS.2023.3265588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has shown great potential as a privacy-preserving solution to learning from decentralized data that are only accessible to end devices (i.e., clients). The data locality constraint offers strong privacy protection but also makes FL sensitive to the condition of local data. Apart from statistical heterogeneity, a large proportion of the clients, in many scenarios, are probably in possession of low-quality data that are biased, noisy or even irrelevant. As a result, they could significantly slow down the convergence of the global model we aim to build and also compromise its quality. In light of this, we first present a new view of local data by looking into the representation space and observing that they converge in distribution to Normal distributions before activation. We provide theoretical analysis to support our finding. Further, we propose FedProf , a novel algorithm for optimizing FL over non-IID data of mixed quality. The key of our approach is a distributional representation profiling and matching scheme that uses the global model to dynamically profile data representations and allows for low-cost, lightweight representation matching. Using the scheme we sample clients adaptively in FL to mitigate the impact of low-quality data on the training process. We evaluated our solution with extensive experiments on different tasks and data conditions under various FL settings. The results demonstrate that the selective behavior of our algorithm leads to a significant reduction in the number of communication rounds and the amount of time (up to 2.4× speedup) for the global model to converge and also provides accuracy gain.},
  archive      = {J_TPDS},
  author       = {Wentai Wu and Ligang He and Weiwei Lin and Carsten Maple},
  doi          = {10.1109/TPDS.2023.3265588},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1942-1953},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedProf: Selective federated learning based on distributional representation profiling},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dap-FL: Federated learning flourishes by adaptive tuning and
secure aggregation. <em>TPDS</em>, <em>34</em>(6), 1923–1941. (<a
href="https://doi.org/10.1109/TPDS.2023.3267897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL), an attractive and promising distributed machine learning paradigm, has sparked extensive interest in exploiting tremendous data stored on ubiquitous mobile devices. However, conventional FL suffers severely from resource heterogeneity, as clients with weak computational and communication capabilities may be unable to complete local training using the same local training hyper-parameters. In this article, we propose Dap-FL, a deep deterministic policy gradient (DDPG)-assisted adaptive FL system, in which local learning rates and local training epochs are adaptively adjusted by all resource-heterogeneous clients through locally deployed DDPG-assisted adaptive hyper-parameter selection schemes. Particularly, the rationality of the proposed hyper-parameter selection scheme is confirmed through rigorous mathematical proof. Besides, due to the thoughtlessness of security consideration of adaptive FL systems in previous studies, we introduce the Paillier cryptosystem to aggregate local models in a secure and privacy-preserving manner. Rigorous analyses show that the proposed Dap-FL system could protect clients’ private local models against chosen-plaintext attacks and chosen-message attacks in a widely used honest-but-curious participants and active adversaries security model. More importantly, through ingenious and extensive experiments, the proposed Dap-FL achieves higher model prediction accuracy than two state-of-the-art RL-assisted FL methods, i.e., 6.03\% higher than DDPG-based FL and 7.85\% higher than DQN-based FL. In addition, experimental results also show that the proposed Dap-FL achieves higher global model prediction accuracy and faster convergence rates than conventional FL, and the comprehensiveness of the adjusted local training hyper-parameters is validated.},
  archive      = {J_TPDS},
  author       = {Qian Chen and Zilong Wang and Jiawei Chen and Haonan Yan and Xiaodong Lin},
  doi          = {10.1109/TPDS.2023.3267897},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1923-1941},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Dap-FL: Federated learning flourishes by adaptive tuning and secure aggregation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdaptChain: Adaptive scaling blockchain with transaction
deduplication. <em>TPDS</em>, <em>34</em>(6), 1909–1922. (<a
href="https://doi.org/10.1109/TPDS.2023.3267071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although existing schemes improve blockchain throughput by allowing concurrent blocks to be appended to the blockchain, little attention has been devoted to adjusting blockchain throughput dynamically and deduplicating transactions between concurrent blocks. In this article, we propose AdaptChain, an adaptive scaling blockchain with transaction deduplication. When the transaction demand of users in the network is high, the blockchain expands to meet the demand; when the transaction demand is low, the blockchain shrinks to save communication and storage costs. Our transaction deduplication mechanism ensures that no duplicate transactions are added to the blockchain, thereby improving bandwidth utilization and achieving higher effective throughput. Besides, we randomly split the mining power of the system to achieve mining power load balancing and resist attacks. We formally analyze the blockchain security and implement the proposed prototype on Amazon EC2. Experimental results show that AdaptChain achieves dynamic and higher effective blockchain throughput.},
  archive      = {J_TPDS},
  author       = {Jie Xu and Qingyuan Xie and Sen Peng and Cong Wang and Xiaohua Jia},
  doi          = {10.1109/TPDS.2023.3267071},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1909-1922},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AdaptChain: Adaptive scaling blockchain with transaction deduplication},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Static algorithm allocation with duplication in robotic
network cloud systems. <em>TPDS</em>, <em>34</em>(6), 1897–1908. (<a
href="https://doi.org/10.1109/TPDS.2023.3267293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic networks can be used to accomplish tasks that exceed the capacity of a single robot. In a robotic network, robots can work together to accomplish a common task. Cloud robotics allows robots to benefit from the massive storage and computing power of the cloud. Previous studies mainly focus on minimizing the cost of resource retrieval by robots by knowing the resource allocation in advance. Duplicating algorithms on multiple nodes can reduce the total time required to execute a task. We address the question of which algorithms should be duplicated and where the duplicates should be placed to improve overall performance. We have developed a procedure to answer wherein a robotic network cloud system should algorithms be executed and whether they should be duplicated to achieve optimal performance in terms of overall task execution time for all robots. Our proposed duplication procedure is optimal in the sense that the number of duplicated algorithms is minimal, while the result provides minimal overall completion time for all robots.},
  archive      = {J_TPDS},
  author       = {Saeid Alirezazadeh and Luís A. Alexandre},
  doi          = {10.1109/TPDS.2023.3267293},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1897-1908},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Static algorithm allocation with duplication in robotic network cloud systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental multilayer resource partitioning for application
placement in dynamic fog. <em>TPDS</em>, <em>34</em>(6), 1877–1896. (<a
href="https://doi.org/10.1109/TPDS.2023.3262695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing platforms became essential for deploying low-latency applications at the network&#39;s edge. However, placing and managing time-critical applications over a Fog infrastructure with many heterogeneous and resource-constrained devices over a dynamic network is challenging. This paper proposes an incremental multilayer resource-aware partitioning (M-RAP) method that minimizes resource wastage and maximizes service placement and deadline satisfaction in a dynamic Fog with many application requests. M-RAP represents the heterogeneous Fog resources as a multilayer graph, partitions it based on the network structure and resource types, and constantly updates it upon dynamic changes in the underlying Fog infrastructure. Finally, it identifies the device partitions for placing the application services according to their resource requirements, which must overlap in the same low-latency network partition. We evaluated M-RAP through extensive simulation and two applications executed on a real testbed. The results show that M-RAP can place 1.6 times as many services, satisfy deadlines for 43\% more applications, lower their response time by up to 58\%, and reduce resource wastage by up to 54\% compared to three state-of-the-art methods.},
  archive      = {J_TPDS},
  author       = {Zahra Najafabadi Samani and Narges Mehran and Dragi Kimovski and Shajulin Benedict and Nishant Saurabh and Radu Prodan},
  doi          = {10.1109/TPDS.2023.3262695},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1877-1896},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Incremental multilayer resource partitioning for application placement in dynamic fog},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practice of streaming processing of dynamic graphs:
Concepts, models, and systems. <em>TPDS</em>, <em>34</em>(6), 1860–1876.
(<a href="https://doi.org/10.1109/TPDS.2021.3131677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph processing has become an important part of various areas of computing, including machine learning, medical applications, social network analysis, computational sciences, and others. A growing amount of the associated graph processing workloads are dynamic , with millions of edges added or removed per second. Graph streaming frameworks are specifically crafted to enable the processing of such highly dynamic workloads. Recent years have seen the development of many such frameworks. However, they differ in their general architectures (with key details such as the support for the concurrent execution of graph updates and queries, or the incorporated graph data organization), the types of updates and workloads allowed, and many others. To facilitate the understanding of this growing field, we provide the first analysis and taxonomy of dynamic and streaming graph processing. We focus on identifying the fundamental system designs and on understanding their support for concurrency, and for different graph updates as well as analytics workloads. We also crystallize the meaning of different concepts associated with streaming graph processing, such as dynamic, temporal, online, and time-evolving graphs, edge-centric processing, models for the maintenance of updates, and graph databases. Moreover, we provide a bridge with the very rich landscape of graph streaming theory by giving a broad overview of recent theoretical related advances, and by discussing which graph streaming models and settings could be helpful in developing more powerful streaming frameworks and designs. We also outline graph streaming workloads and research challenges. Author: Please confirm or add details for any funding or financial support for the research of this article. ?&gt;},
  archive      = {J_TPDS},
  author       = {Maciej Besta and Marc Fischer and Vasiliki Kalavri and Michael Kapralov and Torsten Hoefler},
  doi          = {10.1109/TPDS.2021.3131677},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1860-1876},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Practice of streaming processing of dynamic graphs: Concepts, models, and systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated ensemble model-based reinforcement learning in
edge computing. <em>TPDS</em>, <em>34</em>(6), 1848–1859. (<a
href="https://doi.org/10.1109/TPDS.2023.3264480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a privacy-preserving distributed machine learning paradigm that enables collaborative training among geographically distributed and heterogeneous devices without gathering their data. Extending FL beyond the supervised learning models, federated reinforcement learning (FRL) was proposed to handle sequential decision-making problems in edge computing systems. However, the existing FRL algorithms directly combine model-free RL with FL, thus often leading to high sample complexity and lacking theoretical guarantees. To address the challenges, we propose a novel FRL algorithm that effectively incorporates model-based RL and ensemble knowledge distillation into FL for the first time. Specifically, we utilise FL and knowledge distillation to create an ensemble of dynamics models for clients, and then train the policy by solely using the ensemble model without interacting with the environment. Furthermore, we theoretically prove that the monotonic improvement of the proposed algorithm is guaranteed. The extensive experimental results demonstrate that our algorithm obtains much higher sample efficiency compared to classic model-free FRL algorithms in the challenging continuous control benchmark environments under edge computing settings. The results also highlight the significant impact of heterogeneous client data and local model update steps on the performance of FRL, validating the insights obtained from our theoretical analysis.},
  archive      = {J_TPDS},
  author       = {Jin Wang and Jia Hu and Jed Mills and Geyong Min and Ming Xia and Nektarios Georgalas},
  doi          = {10.1109/TPDS.2023.3264480},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1848-1859},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Federated ensemble model-based reinforcement learning in edge computing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RHDOFS: A distributed online algorithm towards scalable
streaming feature selection. <em>TPDS</em>, <em>34</em>(6), 1830–1847.
(<a href="https://doi.org/10.1109/TPDS.2023.3265974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important topic in data mining and machine learning, which aims to select an optimal feature subset for building effective and explainable prediction models. This article introduces Rough Hypercuboid based Distributed Online Feature Selection (RHDOFS) method to tackle two critical challenges of Volume and Velocity associated with Big Data. By exploring the class separability in the boundary region of rough hypercuboid approach, a novel integrated feature evaluation criterion is proposed by examining not only the explicit patterns contained in the positive region but also the useful implicit patterns derived from the boundary region. An efficient online feature selection method for streaming feature scenario is developed to identify relevant and nonredundant features in an incremental iterative fashion. Furthermore, a parallel optimization mechanism by combining both data and computational independence is further employed to accelerate the original sequential implementation. An efficient distributed online feature selection algorithm is presented and implemented on the Apache Spark platform to scale for massive amount of data by exploiting the computational capabilities of multicore clusters. Encouraging results of extensive experiments indicate the superiority and notable advantages of the proposed algorithm over the relevant and representative online feature selection algorithms. Empirical tests on scalability and extensibility also demonstrate our distributed implementation significantly reduces the computational times requirements while maintaining the prediction accuracy, and is capable of scaling well in volume of data and number of computing nodes.},
  archive      = {J_TPDS},
  author       = {Chuan Luo and Sizhao Wang and Tianrui Li and Hongmei Chen and Jiancheng Lv and Zhang Yi},
  doi          = {10.1109/TPDS.2023.3265974},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1830-1847},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RHDOFS: A distributed online algorithm towards scalable streaming feature selection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for mapping DRL algorithms with prioritized
replay buffer onto heterogeneous platforms. <em>TPDS</em>,
<em>34</em>(6), 1816–1829. (<a
href="https://doi.org/10.1109/TPDS.2023.3264823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent success of Deep Reinforcement Learning (DRL) in self-driving cars, robotics and surveillance, training DRL agents takes tremendous amount of time and computation resources. In this article, we aim to accelerate DRL with Prioritized Replay Buffer due to its state-of-the-art performance on various benchmarks. The computation primitives of DRL with Prioritized Replay Buffer include environment emulation, neural network inference, sampling from Prioritized Replay Buffer, updating Prioritized Replay Buffer and neural network training. The speed of running these primitives varies for various DRL algorithms such as Deep Q Network and Deep Deterministic Policy Gradient. This makes a fixed mapping of DRL algorithms inefficient. In this work, we propose a framework for mapping DRL algorithms onto heterogeneous platforms consisting of a multi-core CPU, a GPU and a FPGA. First, we develop specific accelerators for each primitive on CPU, FPGA and GPU. Second, we relax the data dependency between priority update and sampling performed in the Prioritized Replay Buffer. By doing so, the latency caused by data transfer between GPU, FPGA and CPU can be completely hidden without sacrificing the rewards achieved by agents learned using the target DRL algorithms. Finally, given a DRL algorithm specification, our design space exploration automatically chooses the optimal mapping of various primitives based on an analytical performance model. On widely used benchmark environments, our experimental results demonstrate up to 997.3× improvement in training throughput compared with baseline mappings on the same heterogeneous platform. Compared with the state-of-the-art distributed Reinforcement Learning framework RLlib, we achieve 1.06 $\times \sim$ 1005× improvement in training throughput.},
  archive      = {J_TPDS},
  author       = {Chi Zhang and Yuan Meng and Viktor Prasanna},
  doi          = {10.1109/TPDS.2023.3264823},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1816-1829},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A framework for mapping DRL algorithms with prioritized replay buffer onto heterogeneous platforms},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient algorithm for hamiltonian path embedding of
<span class="math inline"><em>k</em></span>k-ary <span
class="math inline"><em>n</em></span>n-cubes under the partitioned edge
fault model. <em>TPDS</em>, <em>34</em>(6), 1802–1815. (<a
href="https://doi.org/10.1109/TPDS.2023.3264698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The $k$ -ary $n$ -cube $Q_{n}^{k}$ is one of the most important interconnection networks for building network-on-chips, data center networks, and parallel computing systems owing to its desirable properties. Since edge faults grow rapidly and the path structure plays a vital role in large-scale networks for parallel computing, fault-tolerant path embedding and its related problems have attracted extensive attention in the literature. However, the existing path embedding approaches usually only focus on the theoretical proofs and produce an $n$ -related linear fault tolerance since they are based on the traditional fault model, which allows all faults to be adjacent to the same node. In this paper, we design an efficient fault-tolerant Hamiltonian path embedding algorithm for enhancing the fault-tolerant capacity of $k$ -ary $n$ -cubes. To facilitate the algorithm, we first introduce a new conditional fault model, named Partitioned Edge Fault model (PEF model). Based on this model, for the $k$ -ary $n$ -cube $Q_{n}^{k}$ with $n\geq 2$ and odd $k\geq 3$ , we explore the existence of a Hamiltonian path in $Q_{n}^{k}$ with large-scale edge faults. Then we give an $O(N)$ algorithm, named HP-PEF, to embed the Hamiltonian path into $Q_{n}^{k}$ under the PEF model, where $N$ is the number of nodes in $Q_{n}^{k}$ . The performance analysis of HP-PEF shows the average path length of adjacent node pairs in the Hamiltonian path constructed by HP-PEF. We also make comparisons to show that our result of edge fault tolerance has exponentially improved other known results. We further experimentally show that HP-PEF can support the dynamic degradation of average success rate of constructing Hamiltonian paths when increasing faulty edges exceed the fault tolerance.},
  archive      = {J_TPDS},
  author       = {Hongbin Zhuang and Xiao-Yan Li and Jou-Ming Chang and Dajin Wang},
  doi          = {10.1109/TPDS.2023.3264698},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1802-1815},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An efficient algorithm for hamiltonian path embedding of $k$k-ary $n$n-cubes under the partitioned edge fault model},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DRFL: Federated learning in diabetic retinopathy grading
using fundus images. <em>TPDS</em>, <em>34</em>(6), 1789–1801. (<a
href="https://doi.org/10.1109/TPDS.2023.3264473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is a complication of diabetic Mellitus, developing retinal lesions that impair vision. The DR detection in the early stages avoids permanent vision loss. The treatments provide relief, and the vision loss due to DR is irreversible. The manual grading of DR is time-consuming and prone to human errors. The other real-time problem is exchanging patient fundus image information with hospitals worldwide while upholding the organisations’ privacy concerns. When training a deep learning (DL) network, two critical factors to keep in mind are creating a collaborative platform and protecting patient data privacy. Therefore, an automated DR detection technique is required while protecting patient data and privacy. In this work, we propose a novel DR severity grading technique based on Federated Learning (FL), a recent advancement in DL called DRFL. FL is a new research paradigm that allows DL models to be trained collectively without disclosing clinical information. In DRFL, we combined the Federated averaging (FedAvg) technique and the median of the categorical cross-entropy loss. Since in comparison to FedAvg, the median cross-entropy is better suited for either under-fitted or over-fitted clients. Also, we propose a novel central server that extracts multi-scale features from the fundus images to identify small lesions present in the fundus image. In this work, we consider five clients holding different preprocessed fundus images collected from publicly available databases such as MESSIDOR-2, IDRiD, Kaggle, and a local database collected from Silchar Medical College and Hospital. The proposed model obtained an accuracy of 98.6\%, specificity of 99.3\%, precision of 97.25\%, and an F1 score of 97.5\%, which are better results than other techniques.},
  archive      = {J_TPDS},
  author       = {N Jagan Mohan and R Murugan and Tripti Goel and Parthapratim Roy},
  doi          = {10.1109/TPDS.2023.3264473},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1789-1801},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DRFL: Federated learning in diabetic retinopathy grading using fundus images},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TSC-VEE: A TrustZone-based smart contract virtual execution
environment. <em>TPDS</em>, <em>34</em>(6), 1773–1788. (<a
href="https://doi.org/10.1109/TPDS.2023.3263882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TrustZone as a trusted execution environment (TEE) has been proven to preserve the confidentiality of blockchain transactions supported by smart contracts. Despite some academic effort, TrustZone can only support limited languages for now. The lack of the corresponding execution environment for smart contracts seriously hinders blockchain applications from directly running on TrustZone. In this paper, we design the first virtual execution environment named TSC-VEE for performing Solidity smart contracts on TrustZone, to the best of our knowledge. TSC-VEE can be decomposed into fourfold: (1) an instruction set adapted to the isolation and world switching mechanism of TrustZone. (2) a runtime memory management mechanism that provides a pair of instructions with the corresponding processing mechanism to allocate and release the work memory. (3) a hybrid granularity resource analysis algorithm which computes and records the value of maximum stack height and static gas cost through bytecode pre-execution, avoiding runtime overflow and invalid computations. (4) a cross-isolation-environment prefetching approach that supports loading and storing the storage data from the normal world into the secure world on TrustZone before execution, thus avoiding switching the world state frequently at runtime. Extensive experimental results show that TSC-VEE can perform smart contracts correctly and efficiently on TrustZone. Compared with the most commonly used Ethereum client— Geth , TSC-VEE achieves execution performance improvements by $9.29\times$ . We also implement the Ethereum virtual machine— evmone on TrustZone. TSC-VEE can reduce the latency by 12.63\% with our optimization techniques, and decrease the work memory footprint by 22.95\% on average when executing various scale contracts.},
  archive      = {J_TPDS},
  author       = {Zhaolong Jian and Ye Lu and Youyang Qiao and Yaozheng Fang and Xueshuo Xie and Dayi Yang and Zhiyuan Zhou and Tao Li},
  doi          = {10.1109/TPDS.2023.3263882},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1773-1788},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TSC-VEE: A TrustZone-based smart contract virtual execution environment},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring fine-grained in-memory database performance for
modern CPUs. <em>TPDS</em>, <em>34</em>(6), 1757–1772. (<a
href="https://doi.org/10.1109/TPDS.2023.3262782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern CPUs keep integrating more cores and large size cache, which is beneficial for in-memory databases to improve parallel processing power and cache locality. While state-of-the-art CPUs have diverse architectures and roadmaps such as large core count and large cache size (AMD x86), moderate core count and cache size (intel x86), large core count and moderate cache size (ARM), exploring in-memory databases performance characteristics for different CPU architectures is important for in-memory database designs and optimizations. In this article, we develop a fine-grained in-memory database benchmark to evaluate the performance of each operator on different CPUs to explore how CPU hardware architectures influence performance. Different from well known conclusions that more cores and larger cache size can achieve higher performance, we find out that the micro cache architectures play an important role opposite to core count and cache size, the shared monolithic L3 cache with moderate size beats large disaggregated L3 cache. The experiments also show that predicting operator performance on different CPUs is difficult according to diverse CPU architectures and micro cache architectures, and different implementations of each operator are not always high or low with interleaved strong and weak performance regions influenced by CPU hardware architectures. Intel x86 CPUs represent cache-centric processor design, while AMD x86 and ARM CPUs represent computing-centric processor design, the OLAP benchmark experiments of SSB discover that OmniSciDB and OLAP Accelerator with vector-wise processing model performs well on intel x86 CPUs compared to AMD x86 CPUs and the JIT compliant based Hyper prefers to AMD x86 CPUs rather than intel x86 CPUs. The CPU roadmaps of increasing cores or improving cache locality should be considered for in-memory database algorithm design and platform selection.},
  archive      = {J_TPDS},
  author       = {Zhuan Liu and Ruichen Han and Yansong Zhang and Yu Zhang and Xi Tang and Gang Deng and Tao Zhong and Roman Dementiev and Yunfei Lu and Mingjian Que},
  doi          = {10.1109/TPDS.2023.3262782},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1757-1772},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploring fine-grained in-memory database performance for modern CPUs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse symmetric format for tucker decomposition.
<em>TPDS</em>, <em>34</em>(6), 1743–1756. (<a
href="https://doi.org/10.1109/TPDS.2023.3263124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor-based methods are receiving renewed attention in recent years due to their prevalence in diverse real-world applications. There is considerable literature on tensor representations and algorithms for tensor decompositions, both for dense and sparse tensors. Many applications in hypergraph analytics, machine learning, psychometry, and signal processing result in tensors that are both sparse and symmetric, making them an important class for further study. Similar to the critical Tensor Times Matrix chain operation (TTM c ) in general sparse tensors, the S parse S ymmetric T ensor T imes S ame M atrix c hain (S $^{3}$ TTM c ) operation is compute and memory intensive due to high tensor order and the associated factorial explosion in the number of non-zeros. We present the novel Compressed Sparse Symmetric (CSS) format for sparse symmetric tensors, along with an efficient parallel algorithm for the S $^{3}$ TTM c operation. We theoretically establish that S $^{3}$ TTM c on CSS achieves a better memory versus run-time trade-off compared to state-of-the-art implementations, and visualize the variation of the performance gap over the parameter space. We demonstrate experimental findings that confirm these results and achieve up to $2.72 \times$ speedup on synthetic and real datasets. The scaling of the algorithm on different test architectures is also showcased to highlight the effect of machine characteristics on algorithm performance.},
  archive      = {J_TPDS},
  author       = {Shruti Shivakumar and Jiajia Li and Ramakrishnan Kannan and Srinivas Aluru},
  doi          = {10.1109/TPDS.2023.3263124},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1743-1756},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Sparse symmetric format for tucker decomposition},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AIDTN: Towards a real-time AI optimized DTN system with
NVMeoF. <em>TPDS</em>, <em>34</em>(6), 1731–1742. (<a
href="https://doi.org/10.1109/TPDS.2023.3260806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale data transport for data-intensive sciences is a complex multidimensional challenge. The challenge includes optimizing the end-to-end Big Data movement performance in real-time, supporting direct remote data access using NVMe over Fabrics (NVMeoF) and deploying to existing research platforms. AIDTN is the first effort to provide a unique AI system designed to incorporate NVMe over Fabrics (NVMeoF) and optimize coordination among multiple components supporting large-scale, multi-domain Wide Area Network (WAN) data-intensive science. AIDTN&#39;s research objective is to integrate next-generation storage architecture using NVMeoF, specialized network design using high-performance network appliances, Data Transfer Nodes (DTNs), catalysts in driving data transport, and a unique AI system explicitly designed for high-performance data movement challenges. AIDTN is the first system that uses network and system features to predict the end-to-end performance of high-performance data movement and further extends the model with NVMe-specific features for NVMeoF remote data access. As a result, AIDTN improves data movement performance by up to 284\% while minimizing packet loss compared to other heuristics approaches. It also has a prediction error rate as low as 0.16 compared to AI models with the only network (error rate = 0.29) or network and system features (error rate = 0.19).},
  archive      = {J_TPDS},
  author       = {Se-Young Yu and Qingyang Zeng and Jim Chen and Yan Chen and Joe Mambretti},
  doi          = {10.1109/TPDS.2023.3260806},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1731-1742},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AIDTN: Towards a real-time AI optimized DTN system with NVMeoF},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Critique of: “A parallel framework for constraint-based
bayesian network learning via markov blanket discovery” by SCC team from
UC san diego. <em>TPDS</em>, <em>34</em>(6), 1727–1730. (<a
href="https://doi.org/10.1109/TPDS.2022.3217284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks (BNs) have become popular in recent years to describe natural phenomena in situations where causal linkages are important to understand. In order to get around the inherent non-tractability of learning BNs, Srivastava et al. propose a markov blanket discovery-based approach to learning in their paper titled “A Parallel Framework for Constraint-based Bayesian Network Learning via Markov Blanket Discovery.” We are able to reproduce both the strong and weak scaling experiments from the paper up to 128 cores, and verify communication cost scaling for all three algorithms in the paper. We also introduce methodological improvements to weak scaling that show the paper&#39;s findings are unique to the methodology and not the datasets used. Slight variations in performance were observed due to differences in datasets, core count, and job scheduling.},
  archive      = {J_TPDS},
  author       = {Arunav Gupta and John Ge and John Li and Zihao Kong and Kaiwen He and Matthew Mikhailov and Bryan Chin and Xiaochen Li and Max Apodaca and Paul Rodriguez and Mahidar Tatineni and Mary Thomas and Santosh Bhatt},
  doi          = {10.1109/TPDS.2022.3217284},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1727-1730},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of: “A parallel framework for constraint-based bayesian network learning via markov blanket discovery” by SCC team from UC san diego},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Critique of “a parallel framework for constraint-based
bayesian network learning via markov blanket discovery” by SCC team from
tsinghua university. <em>TPDS</em>, <em>34</em>(6), 1723–1726. (<a
href="https://doi.org/10.1109/TPDS.2022.3209723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Srivastava et al. propose a parallel framework to optimize Bayesian network learning in the SC20 article entitled “A Parallel Framework for Constraint-Based Bayesian Network Learning via Markov Blanket Discovery”. They parallelize all the phases in network constructing algorithms to achieve high performance and scalability. In this article, we reproduce the strong scaling and weak scaling experiments in that SC article. We conduct experiments on a 4-node cluster with Intel CPUs provided by the SCC committee. We further analyze the results of communication overhead. Our results show that the proposed method in that SC article scales well on the provided cluster, in accordance with the SC article. Author: Please confirm or add details for any funding or financial support for the research of this article. ?&gt;},
  archive      = {J_TPDS},
  author       = {Juncheng Cao and Kaiyuan Rong and Mingshu Zhai and Zeyu Song and Yanyu Ren and Yuxi Zhu and Wentao Han and Jidong Zhai},
  doi          = {10.1109/TPDS.2022.3209723},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1723-1726},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “A parallel framework for constraint-based bayesian network learning via markov blanket discovery” by SCC team from tsinghua university},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Critique of “a parallel framework for constraint-based
bayesian network learning via markov blanket discovery” by SCC team from
peking university. <em>TPDS</em>, <em>34</em>(6), 1720–1722. (<a
href="https://doi.org/10.1109/TPDS.2022.3206099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ankit Srivastava et al. (Srivastava et al. 2020) proposed a parallel framework for Constraint-Based Bayesian Network (BN) Learning via Markov Blanket Discovery (referred to as ramBLe) and implemented it over three existing BN learning algorithms, namely, GS, IAMB and Inter-IAMB. As part of the Student Cluster Competition at SC21, we reproduce the computational efficiency of ramBLe on our assigned Oracle cluster. The cluster has 4x36 cores in total with 100 Gbps RoCE v2 support and is equipped with CentOS-compatible Oracle Linux. Our experiments, covering the same three algorithms of the original ramBLe article (Srivastava et al. 2020), evaluate the strong and weak scalability of the algorithms using real COVID-19 data sets. We verify part of the conclusions from the original article and propose our explanation of the differences obtained in our results. Author: Please confirm or add details for any funding or financial support for the research of this article. ?&gt;},
  archive      = {J_TPDS},
  author       = {Jiaqi Si and Junyi Guo and Zhewen Hao and Wenyang He and Ruihan Li and Yueyang Pan and Zhenxin Fu and Chun Fan},
  doi          = {10.1109/TPDS.2022.3206099},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1720-1722},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “A parallel framework for constraint-based bayesian network learning via markov blanket discovery” by SCC team from peking university},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Critique of “a parallel framework for constraint-based
bayesian network learning via markov blanket discovery” by SCC team from
ShanghaiTech university. <em>TPDS</em>, <em>34</em>(6), 1716–1719. (<a
href="https://doi.org/10.1109/TPDS.2022.3205479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In SC20, (Srivastava et al. 2020) proposed a Parallel F ram ework for B ayesian Le arning, or ramBLe, for short, which is a highly parallel and efficient framework for learning the structure of Bayesian Networks (BNs) from samples, There was a discrepancy in Bibliography in the PDF and the source file. We have followed the source file. ?&gt; particularly large genome-scale networks. As part of our participation in the SC21 Student Cluster Competition, our task was to verify conclusions from the original work (Srivastava et al. 2020). Here we present the outcome of our experiments, which were performed on a four-node cluster from the Oracle Cloud HPC platform. We reproduce the numerical results from (Srivastava et al. 2020), namely the algorithm&#39;s performance and scaling behavior using MPI and different Python and Boost libraries on the Oracle cloud.},
  archive      = {J_TPDS},
  author       = {Guancheng Li and Songhui Cao and Chuyi Zhao and Siyuan Zhang and Yuchen Ji and Haotian Jing and Zecheng Li and Jiajun Cheng and Yiwei Yang and Shu Yin},
  doi          = {10.1109/TPDS.2022.3205479},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1716-1719},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “A parallel framework for constraint-based bayesian network learning via markov blanket discovery” by SCC team from ShanghaiTech university},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel framework for constraint-based bayesian network
learning via markov blanket discovery. <em>TPDS</em>, <em>34</em>(6),
1699–1715. (<a href="https://doi.org/10.1109/TPDS.2023.3244135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks (BNs) are a widely used graphical model in machine learning. As learning the structure of BNs is NP-hard, high-performance computing methods are necessary for constructing large-scale networks. In this article, we present a parallel framework to scale BN structure learning algorithms to tens of thousands of variables. Our framework is applicable to learning algorithms that rely on the discovery of Markov blankets (MBs) as an intermediate step. We demonstrate the applicability of our framework by parallelizing three different algorithms: Grow-Shrink ( GS ), Incremental Association MB ( IAMB ), and Interleaved IAMB ( Inter-IAMB ). Our implementations are available as part of an open-source software called ramBLe , and are able to construct BNs from real data sets with tens of thousands of variables and thousands of observations in less than a minute on 1024 cores, with a speedup of up to 845X and 82.5\% efficiency. Furthermore, we demonstrate using simulated data sets that our proposed parallel framework can scale to BNs of even higher dimensionality. Our implementations were selected for the reproducibility challenge component of the 2021 student cluster competition (SCC’21), which tasked undergraduate teams from around the world with reproducing the results that we obtained using the implementations. We discuss details of the challenge and the results of the experiments conducted by the top teams in the competition. The results of these experiments indicate that our key results are reproducible, despite the use of completely different data sets and experiment infrastructure, and validate the scalability of our implementations.},
  archive      = {J_TPDS},
  author       = {Ankit Srivastava and Sriram P. Chockalingam and Srinivas Aluru},
  doi          = {10.1109/TPDS.2023.3244135},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1699-1715},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A parallel framework for constraint-based bayesian network learning via markov blanket discovery},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial reproducibility initiative at the SC
conference series: A preface to the special section. <em>TPDS</em>,
<em>34</em>(6), 1697–1698. (<a
href="https://doi.org/10.1109/TPDS.2023.3262899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to our special section on reproducibility in large-scale computational science. Reproducibility is one of the foundations of science, ensuring not only the transparency and rigor of the methods utilized for discovery but also that the research involved can be built upon. In order to enable research and to support open science, it is crucial that the computational work performed on high performance computing (HPC) systems is also reproducible.},
  archive      = {J_TPDS},
  author       = {Le Mai Weakley and Tim Robinson},
  doi          = {10.1109/TPDS.2023.3262899},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1697-1698},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Guest editorial reproducibility initiative at the SC conference series: A preface to the special section},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Congestion control for datacenter networks: A
control-theoretic approach. <em>TPDS</em>, <em>34</em>(5), 1682–1696.
(<a href="https://doi.org/10.1109/TPDS.2023.3259799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present RoCC , a robust congestion control approach for datacenter networks based on RDMA. RoCC leverages switch queue size as an input to a PI controller, which computes the fair data rate of flows in the queue. The PI parameters are self-tuning to guarantee stability, rapid convergence, and fair and near-optimal throughput in a wide range of congestion scenarios. Our simulation and DPDK implementation results show that RoCC can achieve up to $7\times$ reduction in PFC frames generated under high load levels, compared to DCQCN. At the same time, RoCC can achieve $1.7-4.5\times$ and $1.4-3.9\times$ lower tail latency for long flows and $2.1-7\times$ and $3.5-8.2\times$ lower tail latency for short flows, compared to DCQCN and HPCC, respectively. We also find that RoCC does not require PFC. The functional components of RoCC can be efficiently implemented in P4 and FPGA-based switch hardware.},
  archive      = {J_TPDS},
  author       = {Danushka Menikkumbura and Parvin Taheri and Erico Vanini and Sonia Fahmy and Patrick Eugster and Tom Edsall},
  doi          = {10.1109/TPDS.2023.3259799},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1682-1696},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Congestion control for datacenter networks: A control-theoretic approach},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shuffle differential private data aggregation for random
population. <em>TPDS</em>, <em>34</em>(5), 1667–1681. (<a
href="https://doi.org/10.1109/TPDS.2023.3247541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bridging the advantages of differential privacy in both centralized model (i.e., high accuracy) and local model (i.e., minimum trust), the shuffle privacy model has potential applications in many privacy-sensitive scenarios, such as mobile user data aggregation and federated learning. Since messages from users are anonymized by semi-trusted shufflers (e.g., anonymous channels, edge servers), every user could hide message among other users’ messages and inject only part of noises (a.k.a. privacy amplification). However, existing works assume that the participating user population is known in advance, which is unrealistic for dynamic environments (e.g., mobile computing, vehicular networks). In this work, we study the shuffle privacy model with a random participating population, and give privacy amplification bounds for population size with commonly encountered binomial, Poisson, sub-Gaussian distribution and etc. For further improving accuracy, we formulate and derive optimal dummy sizes for both non-adaptive and adaptive dummies. Finally, to break the error barrier due to the constraint of sending one single message per user, we design a multi-message shuffle private protocol supporting random population. Experiment results show that our approaches reduce more than 60\% error when compared to the local model and naive approaches. We hope this work provides tailored solutions of shuffle privacy for dynamic mobile/distributed computing.},
  archive      = {J_TPDS},
  author       = {Shaowei Wang and Xuandi Luo and Yuqiu Qian and Youwen Zhu and Kongyang Chen and Qi Chen and Bangzhou Xin and Wei Yang},
  doi          = {10.1109/TPDS.2023.3247541},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1667-1681},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Shuffle differential private data aggregation for random population},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating distributed DNN training via transport layer
scheduling. <em>TPDS</em>, <em>34</em>(5), 1650–1666. (<a
href="https://doi.org/10.1109/TPDS.2023.3250462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication scheduling is crucial to accelerate the training of large deep learning models, in which the transmission order of layer-wise deep neural network (DNN) tensors is determined for a better computation-communication overlap. Prior approaches adopt user-level tensor partitioning to enhance the priority scheduling with finer granularity. However, a startup time slot inserted before every tensor partition will neutralize this scheduling gain. Tuning hyper-parameters for tensor partitioning is difficult, especially when the network bandwidth is shared or time-varying in multi-tenant clusters. In this article, we propose Mercury, a simple transport layer scheduler that moves the priority scheduling to the transport layer at the packet granularity. The packets with the highest priority in the Mercury buffer will be transmitted first. Mercury achieves the near-optimal overlapping between communication and computation. It also leverages the immediate aggregation at the transport layer to enable the full overlapping of gradient push and pull. We implement Mercury in MXNet and conduct comprehensive experiments on five popular DNN models in various environments. Mercury can well adapt to dynamic communication and computation resources. Experiments show that Mercury accelerates the training by up to 130\% compared to the classical PS architecture, and 104\% compared to state-of-the-art tensor partitioning methods.},
  archive      = {J_TPDS},
  author       = {Qingyang Duan and Chao Peng and Zeqin Wang and Yuedong Xu and Shaoteng Liu and Jun Wu and John C. S. Lui},
  doi          = {10.1109/TPDS.2023.3250462},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1650-1666},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating distributed DNN training via transport layer scheduling},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topology-aware scheduling framework for microservice
applications in cloud. <em>TPDS</em>, <em>34</em>(5), 1635–1649. (<a
href="https://doi.org/10.1109/TPDS.2023.3238751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loosely coupled and highly cohesived microservices running in containers are becoming the new paradigm for application development. Compared with monolithic applications, applications built on microservices architecture can be deployed and scaled independently, which promises to simplify software development and operation. However, the dramatic increase in the scale of microservices and east-west network traffic in the data center have made the cluster management more complex. Not only does the scale of microservices cause a great deal of pressure on cluster management, but also cascading QoS violations present a substantial risk for SLOs (Service Level Objectives). In this paper, we propose a Microservice-Oriented Topology-Aware Scheduling Framework (MOTAS), which effectively utilizes the topologies of microservices and clusters to optimize the network overhead of microservice applications through a heuristic graph mapping algorithm. The proposed framework can also guarantee the cluster resource utilization. To deal with the dynamic environment of microservice, we propose a mechanism based on distributed trace analysis to detect and handle QoS violations in microservice applications. Through real-world experiments, the framework has been proved to be effective in ensuring cluster resource utilization, reducing application end-to-end latency, improving throughput, and handling QoS violations.},
  archive      = {J_TPDS},
  author       = {Xin Li and Junsong Zhou and Xin Wei and Dawei Li and Zhuzhong Qian and Jie Wu and Xiaolin Qin and Sanglu Lu},
  doi          = {10.1109/TPDS.2023.3238751},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1635-1649},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Topology-aware scheduling framework for microservice applications in cloud},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental survey of FPGA-based monolithic switches and a
novel queue balancer. <em>TPDS</em>, <em>34</em>(5), 1621–1634. (<a
href="https://doi.org/10.1109/TPDS.2023.3244589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies small to medium-sized monolithic switches for FPGA implementation and presents a novel switch design that achieves high algorithmic performance and FPGA implementation efficiency. Crossbar switches based on virtual output queues (VOQs) and variations have been rather popular for implementing switches on FPGAs, with applications in network switches, memory interconnects, network-on-chip (NoC) routers etc. The implementation efficiency of crossbar-based switches is well-documented on ASICs, though we show that their disadvantages can outweigh their advantages on FPGAs. One of the most important challenges in such input-queued switches is the requirement for iterative scheduling algorithms. In contrast to ASICs, this is more harmful on FPGAs, as the reduced operating frequency and narrower packets cannot “hide” multiple iterations of scheduling that are required to achieve a modest scheduling performance. Our proposed design uses an output-queued switch internally for simplifying scheduling, and a queue balancing technique to avoid queue fragmentation and reduce the need for memory-sharing VOQs. Its implementation approaches the scheduling performance of a state-of-the-art FPGA-based switch, while requiring considerably fewer resources.},
  archive      = {J_TPDS},
  author       = {Philippos Papaphilippou and Kentaro Sano and Boma Anantasatya Adhi and Wayne Luk},
  doi          = {10.1109/TPDS.2023.3244589},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1621-1634},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Experimental survey of FPGA-based monolithic switches and a novel queue balancer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel algorithm to accelerate DEVS simulations in
shared memory architectures. <em>TPDS</em>, <em>34</em>(5), 1609–1620.
(<a href="https://doi.org/10.1109/TPDS.2023.3256083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new algorithm for the execution of Discrete Event System Specification (DEVS) simulations on parallel shared memory architectures. Our approach executes parallel discrete-event simulations by executing all tasks in the PDEVS simulation protocol in parallel. The algorithm works by distributing the computations among different cores on shared memory architectures. To show the benefits of our algorithm, we present the results of a set of experiments using a synthetic benchmark and a real-world scenario using two independent computer architectures. The results obtained show how our algorithm accelerates simulations up to eight times, improving previous approaches. In addition, we show that our approach scales when we increase the number of CPU-cores used.},
  archive      = {J_TPDS},
  author       = {Guillermo German Trabes and Gabriel A. Wainer and Veronica Gil-Costa},
  doi          = {10.1109/TPDS.2023.3256083},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1609-1620},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A parallel algorithm to accelerate DEVS simulations in shared memory architectures},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Precise event sampling on AMD versus intel: Quantitative and
qualitative comparison. <em>TPDS</em>, <em>34</em>(5), 1594–1608. (<a
href="https://doi.org/10.1109/TPDS.2023.3257105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise event sampling is a profiling feature in commodity processors that can sample hardware events and accurately locate the instructions that trigger the events. This feature has been used in a large number of tools to detect application performance issues. Although precise event sampling is readily supported in modern multicore architectures, vendor supports exhibit great differences that affect their accuracy, stability, overhead, and functionality. This work presents the most comprehensive study to date on benchmarking the event sampling features of Intel PEBS and AMD IBS and performs in-depth analysis on key differences through series of microbenchmarks. Our qualitative and quantitative analysis shows that PEBS allows finer-grained and more accurate sampling of hardware events, while IBS offers richer set of information at each sample though it suffers from lower accuracy and stability. Moreover, OS signal delivery, which is a common method used by the profiling software, introduces significant time overhead to the original overhead incurred by the hardware mechanisms in both PEBS and IBS. We also found that both PEBS and IBS have bias in sampling events across multiple different locations in a code. Lastly, we demonstrate how our findings on microbenchmarks under different thread counts hold for a full-fledged profiling tool that runs on the state-of-the-art Intel and AMD machines. Overall our detailed comparisons serve as a great reference and provide invaluable information for hardware designers and profiling tool developers.},
  archive      = {J_TPDS},
  author       = {Muhammad Aditya Sasongko and Milind Chabbi and Paul H J Kelly and Didem Unat},
  doi          = {10.1109/TPDS.2023.3257105},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1594-1608},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Precise event sampling on AMD versus intel: Quantitative and qualitative comparison},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detailed modeling of heterogeneous and
contention-constrained point-to-point MPI communication. <em>TPDS</em>,
<em>34</em>(5), 1580–1593. (<a
href="https://doi.org/10.1109/TPDS.2023.3253881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network topology of modern parallel computing systems is inherently heterogeneous, with a variety of latency and bandwidth values. Moreover, contention for the bandwidth can exist on different levels when many processes communicate with each other. Many-pair, point-to-point MPI communication is thus characterized by heterogeneity and contention, even on a cluster of homogeneous multicore CPU nodes. To get a detailed understanding of the individual communication cost per MPI process, we propose a new modeling methodology that incorporates both heterogeneity and contention. First, we improve the standard max-rate model to better quantify the actually achievable bandwidth depending on the number of MPI processes in competition. Then, we make a further extension that more detailedly models the bandwidth contention when the competing MPI processes have different numbers of neighbors, with also non-uniform message sizes. Thereafter, we include more flexibility by considering interactions between intra-socket and inter-socket messaging. Through a series of experiments done on different processor architectures, we show that the new heterogeneous and contention-constrained performance models can adequately explain the individual communication cost associated with each MPI process. The largest test of realistic point-to-point MPI communication involves 8,192 processes and in total 2,744,632 simultaneous messages over 64 dual-socket AMD Epyc Rome compute nodes connected by InfiniBand, for which the overall prediction accuracy achieved is 84\%.},
  archive      = {J_TPDS},
  author       = {Andreas Thune and Sven-Arne Reinemo and Tor Skeie and Xing Cai},
  doi          = {10.1109/TPDS.2023.3253881},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1580-1593},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Detailed modeling of heterogeneous and contention-constrained point-to-point MPI communication},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HiFlash: Communication-efficient hierarchical federated
learning with adaptive staleness control and heterogeneity-aware
client-edge association. <em>TPDS</em>, <em>34</em>(5), 1560–1579. (<a
href="https://doi.org/10.1109/TPDS.2023.3238049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a promising paradigm that enables collaboratively learning a shared model across massive clients while keeping the training data locally. However, for many existing FL systems, clients need to frequently exchange model parameters of large data size with the remote cloud server directly via wide-area networks (WAN), leading to significant communication overhead and long transmission time. To mitigate the communication bottleneck, we resort to the hierarchical federated learning paradigm of HiFL, which reaps the benefits of mobile edge computing and combines synchronous client-edge model aggregation and asynchronous edge-cloud model aggregation together to greatly reduce the traffic volumes of WAN transmissions. Specifically, we first analyze the convergence bound of HiFL theoretically and identify the key controllable factors for model performance improvement. We then advocate an enhanced design of HiFlash by innovatively integrating deep reinforcement learning based adaptive staleness control and heterogeneity-aware client-edge association strategy to boost the system efficiency and mitigate the staleness effect without compromising model accuracy. Extensive experiments corroborate the superior performance of HiFlash in model accuracy, communication reduction, and system efficiency.},
  archive      = {J_TPDS},
  author       = {Qiong Wu and Xu Chen and Tao Ouyang and Zhi Zhou and Xiaoxi Zhang and Shusen Yang and Junshan Zhang},
  doi          = {10.1109/TPDS.2023.3238049},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1560-1579},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HiFlash: Communication-efficient hierarchical federated learning with adaptive staleness control and heterogeneity-aware client-edge association},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From deterioration to acceleration: A calibration approach
to rehabilitating step asynchronism in federated optimization.
<em>TPDS</em>, <em>34</em>(5), 1548–1559. (<a
href="https://doi.org/10.1109/TPDS.2023.3250513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the setting of federated optimization, where a global model is aggregated periodically, step asynchronism occurs when participants conduct model training by efficiently utilizing their computational resources. It is well acknowledged that step asynchronism leads to objective inconsistency under non-i.i.d. data, which degrades the model’s accuracy. To address this issue, we propose a new algorithm FedaGrac , which calibrates the local direction to a predictive global orientation. Taking advantage of the estimated orientation, we guarantee that the aggregated model does not excessively deviate from the global optimum while fully utilizing the local updates of faster nodes. We theoretically prove that FedaGrac holds an improved order of convergence rate than the state-of-the-art approaches and eliminates the negative effect of step asynchronism. Empirical results show that our algorithm accelerates the training and enhances the final accuracy.},
  archive      = {J_TPDS},
  author       = {Feijie Wu and Song Guo and Haozhao Wang and Haobo Zhang and Zhihao Qu and Jie Zhang and Ziming Liu},
  doi          = {10.1109/TPDS.2023.3250513},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1548-1559},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {From deterioration to acceleration: A calibration approach to rehabilitating step asynchronism in federated optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incentive mechanism design for joint resource allocation in
blockchain-based federated learning. <em>TPDS</em>, <em>34</em>(5),
1536–1547. (<a href="https://doi.org/10.1109/TPDS.2023.3253604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-based federated learning (BCFL) has recently gained tremendous attention because of its advantages, such as decentralization and privacy protection of raw data. However, there has been few studies focusing on the allocation of resources for the participated devices (i.e., clients) in the BCFL system. Especially, in the BCFL framework where the FL clients are also the blockchain miners, clients have to train the local models, broadcast the trained model updates to the blockchain network, and then perform mining to generate new blocks. Since each client has a limited amount of computing resources, the problem of allocating computing resources to training and mining needs to be carefully addressed. In this paper, we design an incentive mechanism to help the model owner (MO) (i.e., the BCFL task publisher) assign each client appropriate rewards for training and mining, and then the client will determine the amount of computing power to allocate for each subtask based on these rewards using the two-stage Stackelberg game. After analyzing the utilities of the MO and clients, we transform the game model into two optimization problems, which are sequentially solved to derive the optimal strategies for both the MO and clients. Further, considering the fact that local training related information of each client may not be known by others, we extend the game model with analytical solutions to the incomplete information scenario. Extensive experimental results demonstrate the validity of our proposed schemes.},
  archive      = {J_TPDS},
  author       = {Zhilin Wang and Qin Hu and Ruinian Li and Minghui Xu and Zehui Xiong},
  doi          = {10.1109/TPDS.2023.3253604},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1536-1547},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Incentive mechanism design for joint resource allocation in blockchain-based federated learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance portable batched sparse linear solvers.
<em>TPDS</em>, <em>34</em>(5), 1524–1535. (<a
href="https://doi.org/10.1109/TPDS.2023.3249110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving large number of small linear systems is increasingly becoming a bottleneck in computational science applications. While dense linear solvers for such systems have been studied before, batched sparse linear solvers are just starting to emerge. In this paper, we discuss algorithms for solving batched sparse linear systems and their implementation in the Kokkos Kernels library. The new algorithms are performance portable and map well to the hierarchical parallelism available in modern accelerator architectures. The sparse matrix vector product (SPMV) kernel is the main performance bottleneck of the Krylov solvers we implement in this work. The implementation of the batched SPMV and its performance are therefore discussed thoroughly in this paper. The implemented kernels are tested on different Central Processing Unit (CPU) and Graphic Processing Unit (GPU) architectures. We also develop batched Conjugate Gradient (CG) and batched Generalized Minimum Residual (GMRES) solvers using the batched SPMV. Our proposed solver was able to solve 20,000 sparse linear systems on V100 GPUs with a mean speedup of 76x and 924x compared to using a parallel sparse solver with a block diagonal system with all the small linear systems, and compared to solving the small systems one at a time, respectively. We see mean speedup of 0.51 compared to dense batched solver of cuSOLVER on V100, while using lot less memory. Thorough performance evaluation on three different architectures and analysis of the performance are presented.},
  archive      = {J_TPDS},
  author       = {Kim Liegeois and Sivasankaran Rajamanickam and Luc Berger-Vergiat},
  doi          = {10.1109/TPDS.2023.3249110},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1524-1535},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Performance portable batched sparse linear solvers},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Alleviating the impact of abnormal events through
multi-constrained VM placement. <em>TPDS</em>, <em>34</em>(5),
1508–1523. (<a href="https://doi.org/10.1109/TPDS.2023.3248681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a simple and low-cost way to obtain enough computing resources, more and more tenants migrate their tasks to the cloud. However, the frequent occurrence of abnormal events (e.g., malicious tenants and node failures) in the cloud will seriously affect the tenants’ QoS. Conventionally, the cloud vendors reduce the frequency of abnormal events by deploying auxiliary systems, which requires additional costs and increases network complexity. Considering that it is an unrealistic expectation to eliminate the occurrence of abnormal events in clouds, this paper proposes a complementary scheme to alleviate the negative impact scope when an abnormal event occurs through multi-constrained VM placement without consuming additional resources. Specifically, when deploying VMs, we limit the number of pods (or service nodes) each tenant can access and the number of tenants hosted by each pod (or service node). However, the multi-dimensional interaction among numerous system parameters and performance/resource considerations makes the problem of multi-constrained VM placement for alleviating the impact of abnormal events very challenging. To solve this problem, we formulate an integer linear programming and propose a rounding-based algorithm with a logarithmic approximation ratio. We implement our proposed algorithm on a physical testbed. The experimental and simulation results show the high efficiency of the proposed algorithm. For example, our algorithm reduces the impact scope of service node failure by 60\%, the impact scope of malicious tenants by 40\%, and the tenant task makespan by 25\% compared with other alternatives.},
  archive      = {J_TPDS},
  author       = {Gongming Zhao and Jiawei Liu and Yutong Zhai and Hongli Xu and Huang He},
  doi          = {10.1109/TPDS.2023.3248681},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1508-1523},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Alleviating the impact of abnormal events through multi-constrained VM placement},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cloud configuration optimization for recurring
batch-processing applications. <em>TPDS</em>, <em>34</em>(5), 1495–1507.
(<a href="https://doi.org/10.1109/TPDS.2023.3246086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing the diversity of Big Data analytic jobs, cloud providers offer a wide range of VM instance types or even clusters to cater for different use cases. The choice of cloud configurations can have a significant impact on the response time and running cost of batch-processing applications, which may need to be re-run regularly with cloud-scale resources. However, identifying the best cloud configuration with a low search cost is quite challenging due to i) the large and high-dimensional configuration space, ii) the time-varying cloud service cost (e.g., AWS Spot instances), and iii) job response time variation even given the same configuration. To tackle these challenges, we design and implement Accordia , a system that enables Adaptive Cloud Configuration Optimization for Recurring Data-Intensive Applications. By leveraging recent algorithmic advances in Gaussian Process UCB techniques, Accordia can unearth the cost-optimal configuration with a deadline constraint (i.e., maximum tolerated running time) under the time-varying cloud service cost. More importantly, Accordia manages to achieve a theoretical performance guarantee, sub-linearly increasing dynamic regret of the job completion cost. Using extensive trace-driven simulations and empirical measurements of our Kubernetes-based implementation, we demonstrate that Accordia can identify a near-cost-optimal configuration (i.e., within 10\% of the optimum) after fewer than 20 runs from over 7000 candidate choices, which translates to a 2X-speedup and up to 17.9\% cost-savings, when comparing to the state-of-the-art approach, CherryPick .},
  archive      = {J_TPDS},
  author       = {Yang Liu and Huanle Xu and Wing Cheong Lau},
  doi          = {10.1109/TPDS.2023.3246086},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1495-1507},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cloud configuration optimization for recurring batch-processing applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LAS: Locality-aware scheduling for GEMM-accelerated
convolutions in GPUs. <em>TPDS</em>, <em>34</em>(5), 1479–1494. (<a
href="https://doi.org/10.1109/TPDS.2023.3247808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a graphics processing unit (GPU) scheduling scheme that maximizes the exploitation of data locality in deep neural networks (DNNs). Convolution is one of the fundamental operations used in DNNs and accounts for more than 90\% of the total execution time. To leverage massive thread-level parallelism (TLP) in a GPU, deeply nested convolution loops are lowered (or unrolled) into large matrix multiplication, which trades memory capacity and bandwidth for TLP augmentation. A large workspace matrix is split into tiles of general matrix multiplication (GEMM) and concurrently executed by many thread blocks. Notably, the workspace is filled with a number of duplicate data that originate from the same sources in the input feature map during the lowering process. However, conventional GPU scheduling is oblivious to data duplication patterns in the workspace, and thread blocks are assigned to streaming multiprocessors (SMs) irrespective of data similarity between GEMM tiles. Such scheduling misses a significant opportunity to exploit data locality manifested in the DNN convolution. This article proposes a GPU scheduling technique called Locality-Aware Scheduling (LAS) that i) identifies which thread blocks share the largest amount of identical data based on the lowered patterns of a DNN convolution and ii) allocates such thread blocks showing the greatest data similarity to the same SM. In this way, small caches in SMs can efficiently utilize the data locality of the DNN convolution. Experimental results show that LAS with tensor cores achieves 20.1\% performance improvements on average with 14.8\% increases in L1 cache hit rates.},
  archive      = {J_TPDS},
  author       = {Hyeonjin Kim and William J. Song},
  doi          = {10.1109/TPDS.2023.3247808},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1479-1494},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LAS: Locality-aware scheduling for GEMM-accelerated convolutions in GPUs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Merak: An efficient distributed DNN training framework with
automated 3D parallelism for giant foundation models. <em>TPDS</em>,
<em>34</em>(5), 1466–1478. (<a
href="https://doi.org/10.1109/TPDS.2023.3247001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models are in the process of becoming the dominant deep learning technology. Pretraining a foundation model is always time-consuming due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the pretraining process is extremely memory- and communication-intensive. These challenges make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism, and tensor model parallelism, to achieve high training efficiency. However, current 3D parallelism frameworks still encounter two issues: i) they are not transparent to model developers, requiring manual model modification to parallelize training, and ii) their utilization of computation resources, GPU memory, and network bandwidth is insufficient. We propose Merak , an automated 3D parallelism deep learning training framework with high resource utilization. Merak automatically deploys 3D parallelism with an automatic model partitioner, which includes a graph-sharding algorithm and proxy node-based model graph. Merak also offers a non-intrusive API to scale out foundation model training with minimal code modification. In addition, we design a high-performance 3D parallel runtime engine that employs several techniques to exploit available training resources, including a shifted critical path pipeline schedule that increases computation utilization, stage-aware recomputation that makes use of idle worker memory, and sub-pipelined tensor model parallelism that overlaps communication and computation. Experiments on 64 GPUs demonstrate Merak&#39;s capability to speed up training performance over state-of-the-art 3D parallelism frameworks of models with 1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42, 1.39, 1.43, and 1.61×, respectively.},
  archive      = {J_TPDS},
  author       = {Zhiquan Lai and Shengwei Li and Xudong Tang and Keshi Ge and Weijie Liu and Yabo Duan and Linbo Qiao and Dongsheng Li},
  doi          = {10.1109/TPDS.2023.3247001},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1466-1478},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Merak: An efficient distributed DNN training framework with automated 3D parallelism for giant foundation models},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RTGPU: Real-time GPU scheduling of hard deadline parallel
tasks with fine-grain utilization. <em>TPDS</em>, <em>34</em>(5),
1450–1465. (<a href="https://doi.org/10.1109/TPDS.2023.3235439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many emerging cyber-physical systems, such as autonomous vehicles and robots, rely heavily on artificial intelligence and machine learning algorithms to perform important system operations. Since these highly parallel applications are computationally intensive, they need to be accelerated by graphics processing units (GPUs) to meet stringent timing constraints. However, despite the wide adoption of GPUs, efficiently scheduling multiple GPU applications while providing rigorous real-time guarantees remains challenging. Each GPU application has multiple CPU execution and memory copy segments, with GPU kernels running on different hardware resources. Because of the complicated interactions between heterogeneous segments of parallel tasks, high schedulability is hard to achieve with conventional approaches. This paper proposes RTGPU, which combines fine-grain GPU partitioning on the system-side with a novel scheduling algorithm on the theory-side. We start by building a model for CPU and memory copy segments. Leveraging persistent threads, we then implement fine-grained GPU partitioning with improved performance through interleaved execution. To reap the benefits of fine-grained GPU partitioning and schedule multiple parallel GPU applications, we propose a novel real-time scheduling algorithm based on federated scheduling and grid search with uniprocessor fixed-priority scheduling. Our approach provides real-time guarantees to meet hard deadlines and achieves over 11\% improvement in system throughput and up to 57\% schedulability improvement compared with previous work. We validate and evaluate RTGPU on NVIDIA GPU systems. Our system-side techniques can be applied on mainstream GPUs, and the proposed scheduling theory can be used in general heterogeneous computing platforms which have a similar task execution pattern.},
  archive      = {J_TPDS},
  author       = {An Zou and Jing Li and Christopher D. Gill and Xuan Zhang},
  doi          = {10.1109/TPDS.2023.3235439},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1450-1465},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RTGPU: Real-time GPU scheduling of hard deadline parallel tasks with fine-grain utilization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fold3D: Rethinking and parallelizing computational and
communicational tasks in the training of large DNN models.
<em>TPDS</em>, <em>34</em>(5), 1432–1449. (<a
href="https://doi.org/10.1109/TPDS.2023.3247883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training a large DNN (e.g., GPT3) efficiently on commodity clouds is challenging even with the latest 3D parallel training systems (e.g., Megatron v3.0). In particular, along the pipeline parallelism dimension, computational tasks that produce a whole DNN&#39;s gradients with multiple input batches should be concurrently activated; along the data parallelism dimension, a set of heavy-weight communications (for aggregating the accumulated outputs of computational tasks) is inevitably serialized after the pipelined tasks, undermining the training performance (e.g., in Megatron, data parallelism caused all GPUs idle for over 44\% of the training time) over commodity cloud networks. To deserialize these communicational and computational tasks, we propose the AIAO scheduling (for 3D parallelism) which slices a DNN into multiple segments, so that the computational tasks processing the same DNN segment can be scheduled together, and the communicational tasks that synchronize this segment can be launched and overlapped (deserialized) with other segments’ computational tasks. We realized this idea in our Fold3D training system. Extensive evaluation shows Fold3D eliminated most of the all-GPU 44\% idle time in Megatron (caused by data parallelism), leading to 25.2\%–42.1\% training throughput improvement compared to four notable baselines over various settings; Fold3D &#39;s high performance scaled to many GPUs.},
  archive      = {J_TPDS},
  author       = {Fanxin Li and Shixiong Zhao and Yuhao Qing and Xusheng Chen and Xiuxian Guan and Sen Wang and Gong Zhang and Heming Cui},
  doi          = {10.1109/TPDS.2023.3247883},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1432-1449},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fold3D: Rethinking and parallelizing computational and communicational tasks in the training of large DNN models},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling balanced data deduplication in mobile edge
computing. <em>TPDS</em>, <em>34</em>(5), 1420–1431. (<a
href="https://doi.org/10.1109/TPDS.2023.3247061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the mobile edge computing (MEC) environment, edge servers with storage and computing resources are deployed at base stations within users’ geographic proximity to extend the capabilities of cloud computing to the network edge. Edge storage system (ESS), is comprised by connected edge servers in a specific area, which ensures low-latency services for users. However, high data storage overheads incurred by edge servers’ limited storage capacities is a key challenge in ensuring the performance of applications deployed on an ESS. Data deduplication, as a classic data reduction technology, has been widely applied in cloud storage systems. It also offers a promising solution to reducing data redundancy in ESSs. However, the unique characteristics of MEC, such as edge servers’ geographic distribution and coverage, render cloud data deduplication mechanisms obsolete. In addition, data distribution must be balanced over edge storage systems to accommodate future data demands, which cannot be undermined by data deduplication. Thus, balanced edge data deduplication (BEDD) must consider deduplication ratio, data storage benefits, and resource balance systematically under the latency constraint. In this article, we model the novel BEDD problem formally and prove its $\mathcal {NP}$ -hardness. Then, we propose an optimal approach for solving the BEDD problem exactly in small-scale scenarios and a sub-optimal approach to solve large-scale BEDD problems with a theoretical performance guarantee. Extensive and comprehensive experiments conducted on a real-world dataset demonstrate the significant performance improvements of our approaches against four representative approaches.},
  archive      = {J_TPDS},
  author       = {Ruikun Luo and Hai Jin and Qiang He and Song Wu and Xiaoyu Xia},
  doi          = {10.1109/TPDS.2023.3247061},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1420-1431},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Enabling balanced data deduplication in mobile edge computing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved NSGA-III algorithm based on deep q-networks for
cloud storage optimization of blockchain. <em>TPDS</em>, <em>34</em>(5),
1406–1419. (<a href="https://doi.org/10.1109/TPDS.2023.3243634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the underlying technology of cryptocurrencies, blockchain has gained a lot of attention in recent years. However, the storage problem needs to be solved with the increasing number of blocks in the blockchain network. Cloud storage optimization is an effective way to solve the storage issue, which selects and stores parts of blocks to the cloud. Precisely, block selection can be described as a multiobjective optimization problem (MOP) and solved by evolutionary algorithms (EAs). To obtain well results of block selection, an improved NSGA-III algorithm based on deep Q-networks (DQN), termed NSGA-DQN, is proposed in this paper, which aims to maintain well convergence and diversity of the population. This way, a set of suitable solutions is obtained to determine the number of blocks stored to the cloud, and the storage problem can be solved effectively. To be specific, DQN creates a decision-making agent to maximize the expected reward by learning a policy that evaluates $Q$ values of each action in each state. In the proposed selection mechanism, the reward values are set according to the convergence and diversity of the population, and the actions correspond to the individuals. This way, our method can determine a set of individuals that maximizes the convergence and diversity of the population. In addition, an adaptive maximum reward enhancement module (AMREM) is developed to further enhance the maximum expected reward by updating the new better reward and modifying the replay memory. We conduct the experimental study on block selection, and the results demonstrate that the proposed algorithm is superior to five state-of-the-art algorithms.},
  archive      = {J_TPDS},
  author       = {Yu Zhou and Yanli Ren and Mengtian Xu and Guorui Feng},
  doi          = {10.1109/TPDS.2023.3243634},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1406-1419},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An improved NSGA-III algorithm based on deep Q-networks for cloud storage optimization of blockchain},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large-scale measurements and prediction of DC-WAN traffic.
<em>TPDS</em>, <em>34</em>(5), 1390–1405. (<a
href="https://doi.org/10.1109/TPDS.2023.3245092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large cloud service providers have built an increasing number of geo-distributed data centers (DCs) connected by Wide Area Networks (WANs). These DC-WANs carry both high-priority traffic from interactive services and low-priority traffic from bulk transfers. Given that a DC-WAN is an expensive resource, providers often manage it via traffic engineering algorithms that rely on accurate predictions of inter-DC high-priority (delay-sensitive) traffic. In this article, we perform a large-scale measurement study of high-priority inter-DC traffic from Baidu. We measure how inter-DC traffic varies across their global DC-WAN and show that most existing traffic prediction methods either cannot capture the complex traffic dynamics or overlook traffic interrelations among DCs. Building on our measurements, we propose the In terrelated- Te mporal G raph Convolutional Net work (IntegNet) model for inter-DC traffic prediction. In contrast to prior efforts, our model exploits both temporal traffic patterns and inferred co-dependencies between DC pairs. IntegNet forecasts the capacity needed for high-priority traffic demands by accounting for the balance between resource provisioning (i.e., allocating resources exceeding actual demand) and QoS losses (i.e., allocating fewer resources than actual demand). Our experiments show that IntegNet can keep a very limited QoS loss, while also reducing overprovisioning by up to 42.1\% compared to the state-of-the-art and up to 66.2\% compared to the traditional method used in DC-WAN traffic engineering.},
  archive      = {J_TPDS},
  author       = {Zhaohua Wang and Zhenyu Li and Heng Pan and Guangming Liu and Yunfei Chen and Qinghua Wu and Gareth Tyson and Gang Cheng},
  doi          = {10.1109/TPDS.2023.3245092},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1390-1405},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Large-scale measurements and prediction of DC-WAN traffic},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Landlord: Coordinating dynamic software environments to
reduce container sprawl. <em>TPDS</em>, <em>34</em>(5), 1376–1389. (<a
href="https://doi.org/10.1109/TPDS.2023.3241598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containers provide customizable software environments that are independent from the system on which they are deployed. Online services for task execution must often generate containers on the fly to meet user-generated requests. However, as the number of users grows and container environments are changed and updated over time, there is an explosion in the number of containers that must be managed, despite the fact that there is significant overlap among many of the containers in use. We analyze a trace of container launches on the public Binder service and demonstrate the performance and resource usage issues associated with container sprawl. We present Landlord , an algorithm that coalesces related container environments, and show that it can improve container reuse and reduce the number of container builds required in the Binder trace by 40\%. We perform a sensitivity analysis of Landlord using randomized synthetic workloads on a high-energy physics (HEP) software repository and demonstrate that Landlord shows benefits for container management across a wide range of usage patterns. Finally, we compare Landlord to offline clustering, and observe that the continuous churn in software necessitates an online approach.},
  archive      = {J_TPDS},
  author       = {Tim Shaffer and Thanh Son Phung and Kyle Chard and Douglas Thain},
  doi          = {10.1109/TPDS.2023.3241598},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1376-1389},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Landlord: Coordinating dynamic software environments to reduce container sprawl},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general approach to generate test packets with network
configurations. <em>TPDS</em>, <em>34</em>(4), 1362–1375. (<a
href="https://doi.org/10.1109/TPDS.2023.3241433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correctness and reliability of modern networks are often the greatest concerns. A myriad network events like software update, device crash and resource exhaustion, inevitably lead to liveness errors on data plane. This paper focuses on fault detection of the network data plane using test packets. Existing test packet generation techniques are limited in two aspects: i) it is difficult to collect the input data plane snapshot through SNMP or terminals ii) it may rise false negatives due to inconsistent snapshot. In this paper, we propose a new framework, SWIFT, that automatically generates test packets with network configurations. SWIFT minimizes the number of test packets by allowing a packet to go through multiple links or interfaces. For network updates, SWIFT updates test packets in an incremental way to revalidate the network. We evaluate its performance using hundreds of benchmark network configurations. The results show that it takes few seconds to generate test packets to exercise all links and interfaces, and updates the test packets in few seconds for configuration changes. We also deployed a SWIFT prototype in a university network, and successfully detected many network outages.},
  archive      = {J_TPDS},
  author       = {Yahui Li and Han Zhang and Jilong Wang and Zhiliang Wang and Xia Yin and Xingang Shi and Jianping Wu},
  doi          = {10.1109/TPDS.2023.3241433},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1362-1375},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A general approach to generate test packets with network configurations},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reliability-aware multi-objective memetic algorithm for
workflow scheduling problem in multi-cloud system. <em>TPDS</em>,
<em>34</em>(4), 1343–1361. (<a
href="https://doi.org/10.1109/TPDS.2023.3245089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of cloud computing, multi-cloud systems have become common platforms for hosting and executing workflow applications in recent years. However, the complexity of workflow scheduling increases exponentially because of the diversified billing mechanisms, heterogeneous virtual machines, and reliability of multi-cloud systems. This article focuses on a multi-objective workflow scheduling problem in multi-cloud systems (MOWSP-MCS). The makespan, cost, and reliability are considered the optimization objectives from the perspective of users. Compared with the classical multi-objective workflow scheduling in the cloud environment, MOWSP-MCS allows users to apply the backup technique to improve reliability. To solve the MOWSP-MCS, this article proposes a reliability-aware multi-objective memetic algorithm (RA-MOMA) containing a diversification strategy and intensification strategy. In the diversification strategy, several problem-specific genetic operators are introduced to construct the diversified offspring individuals. In the intensification strategy, four problem-specific neighborhood operators are designed based on the critical path and resource utilization rate to improve the quality of the individuals in the archive set. A comprehensive numerical experiment is conducted to evaluate the effectiveness of RA-MOMA. The comparisons with several related algorithms demonstrate the superiority of RA-MOMA for solving the MOWSP-MCS.},
  archive      = {J_TPDS},
  author       = {Shuo Qin and Dechang Pi and Zhongshi Shao and Yue Xu and Yang Chen},
  doi          = {10.1109/TPDS.2023.3245089},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1343-1361},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reliability-aware multi-objective memetic algorithm for workflow scheduling problem in multi-cloud system},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy vs. Efficiency: Achieving both through adaptive
hierarchical federated learning. <em>TPDS</em>, <em>34</em>(4),
1331–1342. (<a href="https://doi.org/10.1109/TPDS.2023.3244198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a decentralized training paradigm, Federated learning (FL) promises data privacy by exchanging model parameters instead of raw local data. However, it is still impeded by the resource limitations of end devices and privacy risks from the ‘curious’ cloud. Yet, existing work predominately ignores that these two issues are non-orthogonal in nature. In this article, we propose a joint design (i.e., AHFL) that accommodates both the efficiency expectation and privacy protection of clients towards high inference accuracy. Based on a cloud-edge-end hierarchical FL framework, we carefully offload the training burden of devices to one proximate edge for enhanced efficiency and apply a two-level differential privacy mechanism for privacy protection. To resolve the conflicts of dynamical resource consumption and privacy risk accumulation, we formulate an optimization problem for choosing configurations under correlated learning parameters (e.g., iterations) and privacy control factors (e.g., noise intensity). An adaptive algorithmic solution is presented based on performance-oriented resource scheduling, budget-aware device selection, and adaptive local noise injection. Extensive evaluations are performed on three different data distribution cases of two real-world datasets, using both a networked prototype and large-scale simulations. Experimental results show that AHFL relieves the end&#39;s resource burden (w.r.t. computation time 8.58\% $\downarrow$ , communication time 59.35\% $\downarrow$ and memory consumption 43.61\% $\downarrow$ ) and has better accuracy (6.34\% $\uparrow$ ) than 3 typical baselines under the limited resource and privacy budgets. The code for our implementation is available at https://github.com/Guoyeting/AHFL .},
  archive      = {J_TPDS},
  author       = {Yeting Guo and Fang Liu and Tongqing Zhou and Zhiping Cai and Nong Xiao},
  doi          = {10.1109/TPDS.2023.3244198},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1331-1342},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Privacy vs. efficiency: Achieving both through adaptive hierarchical federated learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance analysis of machine learning centered workload
prediction models for cloud. <em>TPDS</em>, <em>34</em>(4), 1313–1330.
(<a href="https://doi.org/10.1109/TPDS.2023.3240567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precise estimation of resource usage is a complex and challenging issue due to the high variability and dimensionality of heterogeneous service types and dynamic workloads. Over the last few years, the prediction of resource usage and traffic has received ample attention from the research community. Many machine learning-based workload forecasting models have been developed by exploiting their computational power and learning capabilities. This paper presents the first systematic survey cum performance analysis-based comparative study of diversified machine learning-driven cloud workload prediction models. The discussion initiates with the significance of predictive resource management followed by a schematic description, operational design, motivation, and challenges concerning these workload prediction models. Classification and taxonomy of different prediction approaches into five distinct categories are presented focusing on the theoretical concepts and mathematical functioning of the existing state-of-the-art workload prediction methods. The most prominent prediction approaches belonging to a distinct class of machine learning models are thoroughly surveyed and compared. All five classified machine learning-based workload prediction models are implemented on a common platform for systematic investigation and comparison using three distinct benchmark cloud workload traces via experimental analysis. The essential key performance indicators of state-of-the-art approaches are evaluated for comparison and the paper is concluded by discussing the trade-offs and notable remarks.},
  archive      = {J_TPDS},
  author       = {Deepika Saxena and Jitendra Kumar and Ashutosh Kumar Singh and Stefan Schmid},
  doi          = {10.1109/TPDS.2023.3240567},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1313-1330},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Performance analysis of machine learning centered workload prediction models for cloud},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ProScale: Proactive autoscaling for microservice with
time-varying workload at the edge. <em>TPDS</em>, <em>34</em>(4),
1294–1312. (<a href="https://doi.org/10.1109/TPDS.2023.3238429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying microservice instances on the edge device close to end users can provide on-site processing thus reducing request response time. Each microservice has multiple instances that can process requests in parallel. To achieve high processing efficiency, the number of these instances is scaled according to the workload, which is also known as autoscaling. Previous studies of microservice autoscaling in the edge computing environment lack in-depth consideration of time-varying workload, they assume that the workload of each microservice always depends on that of its upstream. However, through an analysis of Alibaba&#39;s microservice trace with hundreds of millions of records, we find that the assumption is impractical thus hurting autoscaling effectiveness. To solve this problem, we propose ProScale, a prediction-driven proactive autoscaling framework for microservices at the edge. ProScale proactively forecasts the workload for each individual microservice per timeslot. Then it utilizes an efficient online algorithm to leverage the predicting results to determine the instance number for each microservice jointly with making placement decisions. For each microservice instance deployed on the edge device, ProScale handles burst requests using a designed offloading strategy. In addition, ProScale can also balance the load for multiple instances of each microservice. Extensive trace-driven experiments show that ProScale has great scalability. It can reduce average response time by 96.7\% and resource usage by 96.5\% compared with existing strategies and designed baselines.},
  archive      = {J_TPDS},
  author       = {Ke Cheng and Sheng Zhang and Chenghong Tu and Xiaohang Shi and Zhaoheng Yin and Sanglu Lu and Yu Liang and Qing Gu},
  doi          = {10.1109/TPDS.2023.3238429},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1294-1312},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ProScale: Proactive autoscaling for microservice with time-varying workload at the edge},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expediting distributed DNN training with device
topology-aware graph deployment. <em>TPDS</em>, <em>34</em>(4),
1281–1293. (<a href="https://doi.org/10.1109/TPDS.2023.3243261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents TAG, an automatic system to derive optimized DNN training graph and its deployment onto any device topology, for expedited training in device- and topology- heterogeneous ML clusters. We novelly combine both the DNN computation graph and the device topology graph as input to a graph neural network (GNN), and join the GNN with a search-based method to quickly identify optimized distributed training strategies. To reduce communication in a heterogeneous cluster, we further explore a lossless gradient compression technique and solve a combinatorial optimization problem to automatically apply the technique for training time minimization. We evaluate TAG with various representative DNN models and device topologies, showing that it can achieve up to 4.56x training speed-up as compared to existing schemes. TAG can produce efficient deployment strategies for both unseen DNN models and unseen device topologies, without heavy fine-tuning.},
  archive      = {J_TPDS},
  author       = {Shiwei Zhang and Xiaodong Yi and Lansong Diao and Chuan Wu and Siyu Wang and Wei Lin},
  doi          = {10.1109/TPDS.2023.3243261},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1281-1293},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Expediting distributed DNN training with device topology-aware graph deployment},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Node essentiality assessment and distributed collaborative
virtual network embedding in datacenters. <em>TPDS</em>, <em>34</em>(4),
1265–1280. (<a href="https://doi.org/10.1109/TPDS.2023.3242952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network virtualization (NV) has extensive and significant applications in cloud computing and parallel and distributed systems. Virtual network embedding (VNE) is a key issue in NV, which is an effective means to advance systems’ performance. While existing VNE research lacks resource allocation coordination between mappings of different virtual network requests, resulting in insufficient resource utilization and high overhead. In this article, we propose a novel node essentiality evaluation model for data center networks (DCNs), and design an efficient distributed collaborative virtual network embedding. Firstly, we propose a node essentiality evaluation scheme based on dynamic model, which combines the characteristics of network topology and nodes to make the evaluation results more comprehensive. Secondly, we establish the two-stage node importance evaluation criteria for the deviation mean of the data center dynamic model and the variance based on the deviation mean. Furthermore, we investigate a nodal importance assessment method based on the data center dynamic model for perturbation testing. Finally, we design a distributed coordinated VNE algorithm ( CNI-VNE ) which calculates the importance index of physical nodes through topology awareness. The proposed algorithm can increase the coordination between different request mappings, thereby reducing the mapping cost of physical node resources and minimizing the cost of VNE. We use the real Fat-tree DCN of 128 servers and 80 switches as testbed, and evaluate them from indicators such as average reliability, average bandwidth consumption, average energy consumption, and average mapping time. Massive simulation results in different scenarios show that our algorithm achieves the best performance on most indicators compared with the existing state-of-the-art proposals, mapping acceptance and average revenue increased by 19.4\% and 21.3\%, respectively, and DCN reduced bandwidth consumption by about 30\%.},
  archive      = {J_TPDS},
  author       = {Weibei Fan and Fu Xiao and Mengjie Lv and Lei Han and Junchang Wang and Xin He},
  doi          = {10.1109/TPDS.2023.3242952},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1265-1280},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Node essentiality assessment and distributed collaborative virtual network embedding in datacenters},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intermittent fault diagnosis of split-star networks and its
applications. <em>TPDS</em>, <em>34</em>(4), 1253–1264. (<a
href="https://doi.org/10.1109/TPDS.2023.3242089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid increase of the number of processors in multiprocessor systems and the fast expansion of interconnection networks, the reliability of interconnection network is facing severe challenges, where the fast recognition of fault processors is crucial. In practice, most of the processor failures are intermittent faults. In this article, we first determine the intermittent fault diagnosability $t_{I}^{PMC}(S_{n}^{2})$ of $n$ -dimensional split-star network $S_{n}^{2}$ under the PMC model. In addition, we propose a fast intermittent fault probabilistic diagnosis algorithm FIFPDPMC to identify the nodes with intermittent fault in the $n$ -dimensional split-star network $S_{n}^{2}$ under the PMC model, and we calculated the time complexity of the algorithm FIFPDPMC. Then we implement the algorithm FIFPDPMC in the IoT-based wireless sensor network (IoTWSN) and a randomly generated network (RGN) under different number of nodes with intermittent fault, and we evaluate the performance and efficiency of the algorithm FIFPDPMC in terms of accuracy, precision, recall (TPR), F1, G-mean, FPR, TNR and FNR. Experimental results show that, as the number of stages of executing the algorithm FIFPDPMC increases, the number of nodes with intermittent fault being diagnosed by the algorithm FIFPDPMC increases, which implies that the algorithm FIFPDPMC has good performance and efficiency in both IoTWSN and RGN.},
  archive      = {J_TPDS},
  author       = {Jiankang Song and Limei Lin and Yanze Huang and Sun-Yuan Hsieh},
  doi          = {10.1109/TPDS.2023.3242089},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1253-1264},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Intermittent fault diagnosis of split-star networks and its applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BeauForT: Robust byzantine fault tolerance for
client-centric mobile web applications. <em>TPDS</em>, <em>34</em>(4),
1241–1252. (<a href="https://doi.org/10.1109/TPDS.2023.3241963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, part of the web is shifting to a client-centric, decentralized model where web clients become the leading execution environment for application logic and data storage. However, current solutions to build decentralized web applications with multiple distrusting parties often involve a decentralized backend of servers running a BFT protocol between them. Existing consensus protocols using either all-to-all communication, or leader-based gossip suffer from performance degradation in unstable network conditions. In this paper, we present BeauForT, a purely browser-based platform for decentralized BFT consensus in client-centric, community-driven applications. We propose a novel, optimistic, leaderless, gossip-based consensus protocol, tolerating Byzantine replicas, combined with a robust and efficient state-based synchronization protocol. This protocol makes BeauForT well suited for the decentralized client-centric web and its dynamic nature with many network disruptions or node failures.},
  archive      = {J_TPDS},
  author       = {Kristof Jannes and Emad Heydari Beni and Bert Lagaisse and Wouter Joosen},
  doi          = {10.1109/TPDS.2023.3241963},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1241-1252},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {BeauForT: Robust byzantine fault tolerance for client-centric mobile web applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synapse compression for event-based
convolutional-neural-network accelerators. <em>TPDS</em>,
<em>34</em>(4), 1227–1240. (<a
href="https://doi.org/10.1109/TPDS.2023.3239517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manufacturing-viable neuromorphic chips require novel compute architectures to achieve the massively parallel and efficient information processing the brain supports so effortlessly. The most promising architectures for that are spiking/event-based, which enables massive parallelism at low complexity. However, the large memory requirements for synaptic connectivity are a showstopper for the execution of modern convolutional neural networks (CNNs) on massively parallel, event-based architectures. The present work overcomes this roadblock by contributing a lightweight hardware scheme to compress the synaptic memory requirements by several thousand times—enabling the execution of complex CNNs on a single chip of small form factor. A silicon implementation in a 12-nm technology shows that the technique achieves a total memory-footprint reduction of up to 374× compared to the best previously published technique at a negligible area overhead.},
  archive      = {J_TPDS},
  author       = {Lennart Bamberg and Arash Pourtaherian and Luc Waeijen and Anupam Chahar and Orlando Moreira},
  doi          = {10.1109/TPDS.2023.3239517},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1227-1240},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Synapse compression for event-based convolutional-neural-network accelerators},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scheduling algorithms for federated learning with minimal
energy consumption. <em>TPDS</em>, <em>34</em>(4), 1215–1226. (<a
href="https://doi.org/10.1109/TPDS.2023.3240833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has opened the opportunity for collaboratively training machine learning models on heterogeneous mobile or Edge devices while keeping local data private. With an increase in its adoption, a growing concern is related to its economic and environmental cost (as is also the case for other machine learning techniques). Unfortunately, little work has been done to optimize its energy consumption or emissions of carbon dioxide or equivalents, as energy minimization is usually left as a secondary objective. In this paper, we investigate the problem of minimizing the energy consumption of FL training on heterogeneous devices by controlling the workload distribution. We model this as the Minimal Cost FL Schedule problem, a total cost minimization problem with identical, independent, and atomic tasks that have to be assigned to heterogeneous resources with arbitrary cost functions. We propose a pseudo-polynomial optimal solution to the problem based on the previously unexplored Multiple-Choice Minimum-Cost Maximal Knapsack Packing Problem. We also provide four algorithms for scenarios where cost functions are monotonically increasing and follow the same behavior. These solutions are likewise applicable on the minimization of other kinds of costs, and in other one-dimensional data partition problems.},
  archive      = {J_TPDS},
  author       = {Laércio Lima Pilla},
  doi          = {10.1109/TPDS.2023.3240833},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1215-1226},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scheduling algorithms for federated learning with minimal energy consumption},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypergraph-based numerical neural-like p systems for medical
image segmentation. <em>TPDS</em>, <em>34</em>(4), 1202–1214. (<a
href="https://doi.org/10.1109/TPDS.2023.3240174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural-like P systems are membrane computing models inspired by natural computing and are viewed as third-generation neural network models. Although real neurons have complex structures, classical neural-like P systems simplify the structures and corresponding mechanisms to two-dimensional graphs or tree-based firing and forgetting communications, which limit the real applications of these models. In this paper, we propose a hypergraph-based numerical neural-like (HNN) P system containing five types of neurons to describe the high-order correlations among neuron structures. Three new kinds of communication mechanisms among neurons are also proposed to address numerical variables and functions. Based on the new neural-like P system, a tumor/organ segmentation model for medical images is developed. The experimental results indicate that the proposed models outperform the state-of-the-art methods based on two hippocampal datasets and a multiple brain metastases dataset, thus verifying the effectiveness of the HNN P system in correctly segmenting tumors/organs.},
  archive      = {J_TPDS},
  author       = {Jie Xue and Liwen Ren and Bosheng Song and Yujie Guo and Jie Lu and Xiyu Liu and Guanzhong Gong and Dengwang Li},
  doi          = {10.1109/TPDS.2023.3240174},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1202-1214},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Hypergraph-based numerical neural-like p systems for medical image segmentation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging deep reinforcement learning with attention
mechanism for virtual network function placement and routing.
<em>TPDS</em>, <em>34</em>(4), 1186–1201. (<a
href="https://doi.org/10.1109/TPDS.2023.3240404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficacy of Network Function Virtualization (NFV) depends critically on (1) where the virtual network functions (VNFs) are placed and (2) how the traffic is routed. Unfortunately, these aspects are not easily optimized, especially under time-varying network states with different QoS requirements. Given the importance of NFV, many approaches have been proposed to solve the VNF placement and Service Function Chaining (SFC) routing problem. However, those prior approaches mainly assume that the network state is static and known, disregarding dynamic network variations. To bridge that gap, we leverage Markov Decision Process (MDP) to model the dynamic network state transitions. To jointly minimize the delay and cost of NFV providers and maximize the revenue, we first devise a customized Deep Reinforcement Learning (DRL) algorithm for the VNF placement problem. The algorithm uses the attention mechanism to ascertain smooth network behavior within the general framework of network utility maximization (NUM). We then propose attention mechanism-based DRL algorithm for the SFC routing problem, which is to find the path to deliver traffic for the VNFs placed on different nodes. The simulation results show that our proposed algorithms outperform the state-of-the-art algorithms in terms of network utility, delay, cost, and acceptance ratio.},
  archive      = {J_TPDS},
  author       = {Nan He and Song Yang and Fan Li and Stojan Trajanovski and Liehuang Zhu and Yu Wang and Xiaoming Fu},
  doi          = {10.1109/TPDS.2023.3240404},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1186-1201},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Leveraging deep reinforcement learning with attention mechanism for virtual network function placement and routing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On model transmission strategies in federated learning with
lossy communications. <em>TPDS</em>, <em>34</em>(4), 1173–1185. (<a
href="https://doi.org/10.1109/TPDS.2023.3240883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, federated learning (FL) has received tremendous attention in both academia and industry, in which decentralized clients collaboratively complete model training by exchanging model updates with a parameter server through the Internet. Its distributed nature well utilizes the localized data and preserves clients’ privacy, but also incurs heavy communication overhead. Existing studies on model update have mostly focused on the bandwidth constraint of the communication channels. Today&#39;s Internet however is highly unreliable. Simply using Transmission Control Protocol (TCP) would lead to low network utilization under frequent losses. In this paper, we closely examine the optimal transmission strategies in FL over the realistic lossy Internet. We systematically integrate model compression, forward error correction (FEC) and retransmission towards Federated Learning with Lossy Communications (FedLC). We derive the convergence rate of FedLC under non-convex loss with the optimal transmission. We then decompose this non-convex problem and present effective practical solutions. Public datasets are exploited for performance evaluation by varying the packet loss rate from 10\% to 50\%. In a fixed training time budget, FedLC can improve model accuracy by 3.91\% on average or reduce the communication traffic by 34.27\%-47.57\% in comparison with state-of-the-art baselines.},
  archive      = {J_TPDS},
  author       = {Xiaoxin Su and Yipeng Zhou and Laizhong Cui and Jiangchuan Liu},
  doi          = {10.1109/TPDS.2023.3240883},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1173-1185},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On model transmission strategies in federated learning with lossy communications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated learning over coupled graphs. <em>TPDS</em>,
<em>34</em>(4), 1159–1172. (<a
href="https://doi.org/10.1109/TPDS.2023.3240527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are widely used to represent the relations among entities. When one owns the complete data, an entire graph can be easily built, therefore performing analysis on the graph is straightforward. However, in many scenarios, it is impractical to centralize the data due to data privacy concerns. An organization or party only keeps a part of the whole graph data, i.e., graph data is isolated from different parties. Recently, Federated Learning (FL) has been proposed to solve the data isolation issue, mainly for Euclidean data. It is still a challenge to apply FL on graph data because graphs contain topological information which is notorious for its non-IID nature and is hard to partition. In this work, we propose a novel FL framework for graph data, FedCog, to efficiently handle coupled graphs that are a kind of distributed graph data, but widely exist in a variety of real-world applications such as mobile carriers’ communication networks and banks’ transaction networks. We theoretically prove the correctness and security of FedCog. Experimental results demonstrate that our method FedCog significantly outperforms traditional FL methods on graphs. Remarkably, our FedCog improves the accuracy of node classification tasks by up to 14.7\%.},
  archive      = {J_TPDS},
  author       = {Runze Lei and Pinghui Wang and Junzhou Zhao and Lin Lan and Jing Tao and Chao Deng and Junlan Feng and Xidian Wang and Xiaohong Guan},
  doi          = {10.1109/TPDS.2023.3240527},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1159-1172},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Federated learning over coupled graphs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auction-based cluster federated learning in mobile edge
computing systems. <em>TPDS</em>, <em>34</em>(4), 1145–1158. (<a
href="https://doi.org/10.1109/TPDS.2023.3240767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL), allowing data owners to conduct model training without sending their raw data to third-party servers, can enhance data privacy in Mobile Edge Computing (MEC) which brings data processing closer to the data sources. However, the heterogeneity of local data and constrained local resources in MEC bring new challenges hindering the development of FL. To this end, we propose an Auction-based Cluster Federated Learning scheme, called ACFL, comprising a clustered FL framework and an auction-based client selection strategy. Our clustered FL framework first introduces a mean-shift clustering algorithm to FL, which can intelligently cluster clients according to their local data distribution. Then, we select clients from each cluster using an auction mechanism to participate in FL training, which can mitigate the impact of data heterogeneity on model convergence and balance energy consumption. Moreover, we prove the proposed clustered FL framework converges at a sublinear rate. Extensive experiments conducted on real-world datasets demonstrate that the proposed FL scheme outperforms the conventional FL schemes in terms of convergence rate and energy balance.},
  archive      = {J_TPDS},
  author       = {Renhao Lu and Weizhe Zhang and Yan Wang and Qiong Li and Xiaoxiong Zhong and Hongwei Yang and Desheng Wang},
  doi          = {10.1109/TPDS.2023.3240767},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1145-1158},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Auction-based cluster federated learning in mobile edge computing systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The tiny-tasks granularity trade-off: Balancing overhead
versus performance in parallel systems. <em>TPDS</em>, <em>34</em>(4),
1128–1144. (<a href="https://doi.org/10.1109/TPDS.2022.3233712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models of parallel processing systems typically assume that one has $l$ workers and jobs are split into an equal number of $k=l$ tasks. Splitting jobs into $k &amp;gt; l$ smaller tasks, i.e. using “tiny tasks”, can yield performance and stability improvements because it reduces the variance in the amount of work assigned to each worker, but as $k$ increases, the overhead involved in scheduling and managing the tasks begins to overtake the performance benefit. We perform extensive experiments on the effects of task granularity on an Apache Spark cluster, and based on these, develop a four-parameter model for task and job overhead that, in simulation, produces sojourn time distributions that match those of the real system. We also present analytical results which illustrate how using tiny tasks improves the stability region of split-merge systems, and analytical bounds on the sojourn and waiting time distributions of both split-merge and single-queue fork-join systems with tiny tasks. Finally we combine the overhead model with the analytical models to produce an analytical approximation to the sojourn and waiting time distributions of systems with tiny tasks which include overhead. We also perform analogous tiny-tasks experiments on a hybrid multi-processor shared memory system based on MPI and OpenMP which has no load-balancing between nodes. Though no longer strict analytical bounds, our analytical approximations with overhead match both the Spark and MPI/OpenMP experimental results very well.},
  archive      = {J_TPDS},
  author       = {Stefan Bora and Brenton Walker and Markus Fidler},
  doi          = {10.1109/TPDS.2022.3233712},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1128-1144},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The tiny-tasks granularity trade-off: Balancing overhead versus performance in parallel systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the message complexity of fault-tolerant computation:
Leader election and agreement. <em>TPDS</em>, <em>34</em>(4), 1115–1127.
(<a href="https://doi.org/10.1109/TPDS.2023.3239993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the message complexity of two fundamental problems, leader election and agreement in the crash-fault synchronous and fully-connected distributed network. We present randomized (Monte Carlo) algorithms for both the problems and also show non-trivial lower bounds on the message complexity. Our algorithms achieve sublinear message complexity in the so-called implicit version of the two problems when tolerating more than a constant fraction of the faulty nodes. In comparison to the state-of-art, our results improved and extended the works of [Gilbert-Kowalski, SODA’10] (which studied only the agreement problem) in several directions. Specifically, our algorithms tolerate any number of faulty nodes up to $(n -\operatorname{polylog}n)$ . The message complexity (and also the time complexity) of our algorithms is optimal (up to a $\operatorname{polylog}n$ factor). Further, our algorithm works in anonymous networks, where nodes do not know each other. To the best of our knowledge, these are the first sub-linear results for both the leader election and the agreement problem in the crash-fault distributed networks.},
  archive      = {J_TPDS},
  author       = {Manish Kumar and Anisur Rahaman Molla},
  doi          = {10.1109/TPDS.2023.3239993},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1115-1127},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On the message complexity of fault-tolerant computation: Leader election and agreement},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving effectiveness of simulation-based inference in the
massively parallel regime. <em>TPDS</em>, <em>34</em>(4), 1100–1114. (<a
href="https://doi.org/10.1109/TPDS.2023.3238045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation-based Inference (SBI) is a widely used set of algorithms to learn the parameters of complex scientific simulation models. While primarily run on CPUs in High-Performance Compute clusters, these algorithms have been shown to scale in performance when developed to be run on massively parallel architectures such as GPUs. While parallelizing existing SBI algorithms provides us with performance gains, this might not be the most efficient way to utilize the achieved parallelism. This work proposes a new parallelism-aware adaptation of an existing SBI method, namely approximate Bayesian computation with Sequential Monte Carlo(ABC-SMC). This new adaptation is designed to utilize the parallelism not only for performance gain, but also toward qualitative benefits in the learnt parameters. The key idea is to replace the notion of a single ‘step-size’ hyperparameter, which governs how the state space of parameters is explored during learning, with step-sizes sampled from a tuned Beta distribution. This allows this new ABC-SMC algorithm to more efficiently explore the state-space of the parameters being learned. We test the effectiveness of the proposed algorithm to learn parameters for an epidemiology model running on a Tesla T4 GPU. Compared to the parallelized state-of-the-art SBI algorithm, we get similar quality results in $\sim 100$ x fewer simulations and observe $\sim 80$ x lower run-to-run variance across 10 independent trials.},
  archive      = {J_TPDS},
  author       = {Sourabh Kulkarni and Csaba Andras Moritz},
  doi          = {10.1109/TPDS.2023.3238045},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1100-1114},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving effectiveness of simulation-based inference in the massively parallel regime},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A memory-constraint-aware list scheduling algorithm for
memory-constraint heterogeneous muti-processor system. <em>TPDS</em>,
<em>34</em>(4), 1082–1099. (<a
href="https://doi.org/10.1109/TPDS.2022.3229373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective scheduling algorithm is vital for the execution efficiency of applications on Heterogeneous Muti-Processor System (HMPS), especially Memory-Constraint Heterogeneous Muti-Processor System (MCHMPS). Stringent local and external memory constraints have significant impact on the execution performance of applications executed on MCHMPS, predictability is also a critical factor for task scheduling on MCHMPS. Therefore, a novel list scheduling algorithm termed Memory-constraint-aware Improved Predict Priority and Optimistic Processor Selection Scheduling (MIPPOSS), essentially a heuristic search optimization algorithm, is proposed in this paper. In MIPPOSS, a predictive approach is applied for task prioritization and processor selection, and a novel memory-constraint-aware approach is employed in the processor selection phase. MIPPOSS has polynomial complexity and produces better results for application scheduling on target architecture. Randomly generated DAGs and 3 real-world applications experiments, including Cybershake, LIGO, and Montage, show that MIPPOSS outperforms the other five competing algorithms by a large margin.},
  archive      = {J_TPDS},
  author       = {Yu Yao and Yukun Song and Ying Huang and Wei Ni and Duoli Zhang},
  doi          = {10.1109/TPDS.2022.3229373},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1082-1099},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A memory-constraint-aware list scheduling algorithm for memory-constraint heterogeneous muti-processor system},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HE-booster: An efficient polynomial arithmetic acceleration
on GPUs for fully homomorphic encryption. <em>TPDS</em>, <em>34</em>(4),
1067–1081. (<a href="https://doi.org/10.1109/TPDS.2022.3228628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully Homomorphic Encryption (FHE) enables secure offloading of computations to untrusted cloud servers as it allows computing on encrypted data. However, existing well-known FHE schemes suffer from heavy performance overheads. Thus numerous accelerations based on FPGAs, ASICs, and GPUs have been proposed. Compared to FPGAs and ASICs, GPUs have obvious advantages in productivity and development costs. And also, GPUs have already been widely deployed in commercial cloud or supercomputing centers. Therefore, we present HE-Booster, an efficient GPU-based FHE acceleration design. For single-GPU acceleration, a thorough systematic design is exploited to map five common phases in typical FHE schemes to the GPU parallel architecture. In particular, inspired by the regular architecture of NTT/INTT, a novel inter-thread local synchronization is proposed to exploit thread-level parallelism. For multi-GPU acceleration, we propose a scalable parallelization design that exploits data-level parallelism through fine-grained data partition under different representations. Finally, experiments on 1 NVIDIA GPU demonstrate that our work outperforms 251.7×, 78.5× and 164.9× than three mainstream CPU-based libraries HElib, SEAL, and PALISADE, and up to 170.5× speedup is obtained compared to the GPU-accelerated library cuHE. What&#39;s more, performing 8 homomorphic multiplications on 8 GPUs can deliver up to a 7.66× performance boost compared to a single-GPU implementation.},
  archive      = {J_TPDS},
  author       = {Zhiwei Wang and Peinan Li and Rui Hou and Zhihao Li and Jiangfeng Cao and XiaoFeng Wang and Dan Meng},
  doi          = {10.1109/TPDS.2022.3228628},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1067-1081},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HE-booster: An efficient polynomial arithmetic acceleration on GPUs for fully homomorphic encryption},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy preserving <span
class="math inline"><em>n</em></span>n-party scalar product protocol.
<em>TPDS</em>, <em>34</em>(4), 1060–1066. (<a
href="https://doi.org/10.1109/TPDS.2023.3238768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving machine learning enables the training of models on decentralized datasets without the need to reveal the information, both on horizontally and vertically partitioned data. However, it requires specialized techniques and algorithms to perform the necessary computations. The privacy preserving scalar product protocol, which enables the dot product of vectors without revealing them, is one popular example for its versatility. For example it can be used to perform analyses that require counting the number of samples which fulfill certain criteria defined across various sites, such as calculating the information gain at a node in a decision tree. Unfortunately, the solutions currently proposed in the literature focus on two-party scenarios, even though scenarios with a higher number of data parties are becoming more relevant. In this article, we propose a generalization of the protocol for an arbitrary number of parties, based on an existing two-party method. Our proposed solution relies on a recursive resolution of smaller scalar products. After describing our proposed method, we discuss potential scalability issues. Finally, we describe the privacy guarantees and identify any concerns, as well as comparing the proposed method to the original solution in this aspect. Additionally we provide an online repository containing the code.},
  archive      = {J_TPDS},
  author       = {Florian van Daalen and Lianne Ippel and Andre Dekker and Inigo Bermejo},
  doi          = {10.1109/TPDS.2023.3238768},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {1060-1066},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Privacy preserving $n$n-party scalar product protocol},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards correlated data trading for high-dimensional private
data. <em>TPDS</em>, <em>34</em>(3), 1047–1059. (<a
href="https://doi.org/10.1109/TPDS.2023.3237691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The commoditization of private data has become an attractive research topic with the emergence of Big Data era. In this paper, we study the trading of high-dimensional private data with differential privacy guarantee. We propose Cheap , which is a novel Correlated data trading framework for High-dimEnsionAl Private data. Cheap first models data correlations among high-dimensional user attributes, and builds an initial attribute clustering scheme. Combined with this scheme, Cheap devises a novel data perturbation mechanism by solving optimal attribute clustering ( OAC ) problem, in order to improve data utility of traded data and further generate a privacy-preserving high-dimensional dataset with close joint distribution with the original one. It then quantifies privacy loss based on near-optimal attribute cluster scheme due to the NP-hardness of the OAC problem, and further compensates data owners by running auction in a cost-effective way. We evaluate the performance of Cheap on UserBehavior dataset and Obesity dataset, respectively. Our evaluation and analysis demonstrate that Cheap well balances data utility and privacy protection, and achieves all desired economic properties of budget balance, individual rationality and truthfulness.},
  archive      = {J_TPDS},
  author       = {Hui Cai and Yuanyuan Yang and Weibei Fan and Fu Xiao and Yanmin Zhu},
  doi          = {10.1109/TPDS.2023.3237691},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {1047-1059},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards correlated data trading for high-dimensional private data},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimizing the average packet access time of the application
layer for buffered instantly decodable network coding. <em>TPDS</em>,
<em>34</em>(3), 1035–1046. (<a
href="https://doi.org/10.1109/TPDS.2023.3237989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In B-IDNC (buffered instantly decodable network coding), each receiver can cache the non-instantly decodable network coded packets (NIDPs) which contain two wanted packets of the network layer for subsequent network decoding, so that more packets can be recovered at the network layer than the traditional instantly decodable network coding (IDNC). By employing B-IDNC, we consider a radio access network wherein a base station (BS) is required to broadcast a block of packets to a set of receivers. After completing network decoding at the network layer, each receiver can deliver its recovered packets from the network layer to the application layer in order. We consider minimizing the average packet access time of the application layer for B-IDNC. For the optimization problem is intractable, we approximate it to reduce the sum minimum access delay of the application layer across all receivers. The approximate problem is shown to be equivalent to a maximum weight encoding clique problem over the B-IDNC graph. We propose a simple heuristic algorithm based on greedy maximum weight vertex search to solve the approximate problem. Simulation results verify the effectiveness of our proposed algorithm as compared with the existing techniques.},
  archive      = {J_TPDS},
  author       = {Zhonghui Mei},
  doi          = {10.1109/TPDS.2023.3237989},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {1035-1046},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Minimizing the average packet access time of the application layer for buffered instantly decodable network coding},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An instability-resilient renewable energy allocation system
for a cloud datacenter. <em>TPDS</em>, <em>34</em>(3), 1020–1034. (<a
href="https://doi.org/10.1109/TPDS.2023.3235957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Renewable energy supply is a promising solution for datacenters&#39; increasing electricity monetary cost, energy consumption and harmful gas emissions. However, due to the instability of renewable energy, insufficient renewable energy supply may lead to the use of stored energy or brown energy. To handle this problem, in this paper, we propose an instability-resilient renewable energy allocation system. We define a job&#39;s service-level-objective (SLO) as the successful running probability by only using supplied renewable energy. The system allocates jobs with the same SLO level to the same physical machine (PM) group, and powers each PM group with renewable energy generators that have probability no less than its SLO to produce the amount no less than its energy demand. We use a deep learning technique to predict the probability of producing the amount no less than each value of each renewable energy source, and predict the energy demands of each PM area. We formulate an optimization problem to match renewable energy resources with different instabilities to different PM groups for supply, and use reinforcement learning method and linear programming method to solve it. We further propose an energy-driven computing resource assignment method, which adjusts the amount of computing resource of each job based on job deadline and failure probability in each PM group, and a failure prediction based energy saving method. Real trace driven experiments show that our methods achieve much lower SLO violations, total energy monetary cost and total carbon emission compared to other methods and the effectiveness of individual methods.},
  archive      = {J_TPDS},
  author       = {Haiying Shen and Haoyu Wang and Jiechao Gao and Rajkumar Buyya},
  doi          = {10.1109/TPDS.2023.3235957},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {1020-1034},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An instability-resilient renewable energy allocation system for a cloud datacenter},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedMDS: An efficient model discrepancy-aware
semi-asynchronous clustered federated learning framework. <em>TPDS</em>,
<em>34</em>(3), 1007–1019. (<a
href="https://doi.org/10.1109/TPDS.2023.3237752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging distributed machine learning paradigm that protects privacy and tackles the problem of isolated data islands. At present, there are two main communication strategies of FL: synchronous FL and asynchronous FL. The advantages of synchronous FL are the high precision and easy convergence of the model. However, this synchronous communication strategy has the risk of the straggler effect. Asynchronous FL has a natural advantage in mitigating the straggler effect, but there are threats of model quality degradation and server crash. In this paper, we propose a model discrepancy-aware semi-asynchronous clustered FL framework, FedMDS , which alleviates the straggler effect by 1) a clustered strategy based on the delay and direction of the model update and 2) a synchronous trigger mechanism that limits the model staleness. FedMDS leverages the clustered algorithm to reschedule the clients. Each group of clients performs asynchronous updates until the synchronous update mechanism based on the model discrepancy is triggered. We evaluate FedMDS based on four typical federated datasets in a non-IID setting and compare FedMDS to the baselines. The experimental results show that FedMDS significantly improves average test accuracy by more than $+9.2\%$ on the four datasets compared to TA-FedAvg . In particular, FedMDS improves absolute Top-1 test accuracy by $+37.6\%$ on FEMNIST compared to TA-FedAvg . The frequency of the average synchronization waiting time of FedMDS is significantly lower than that of TA-FedAvg on all datasets. Moreover, FedMDS can improve the accuracy and alleviate the straggler effect.},
  archive      = {J_TPDS},
  author       = {Yu Zhang and Duo Liu and Moming Duan and Li Li and Xianzhang Chen and Ao Ren and Yujuan Tan and Chengliang Wang},
  doi          = {10.1109/TPDS.2023.3237752},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {1007-1019},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedMDS: An efficient model discrepancy-aware semi-asynchronous clustered federated learning framework},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HPC hardware design reliability benchmarking with HDFIT.
<em>TPDS</em>, <em>34</em>(3), 995–1006. (<a
href="https://doi.org/10.1109/TPDS.2023.3237777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chips pack ever more, ever smaller transistors. Fault rates increase in turn and become more concerning, particularly at the scale of High-Performance Computing (HPC) systems: on one hand, hardware fault protection is costly - more than 10\% silicon area for floating-point units; on the other, HPC users expect correct application output after the anticipated time of computation, but workloads are seldom bit-reproducible and tolerances in output are allowed for. Benign hardware faults causing errors within these tolerances are therefore acceptable: however, with abstract reliability targets such as ’undetected failures per time,’ current HPC system design does not allow for pursuing trade-offs between reliability and performance with respect to faults. To address the above, we propose a user-centric reliability benchmark to specify HPC system reliability targets, allowing for better performance optimizations in hardware design, while meeting HPC user expectations. Our open-source Hardware Design Fault Injection Toolkit ( HDFIT ) enables - for the first time - end-to-end hardware design reliability experiments: from netlist-level fault injection to application output error. In a proof of concept we present an HPC general matrix multiply (GEMM) reliability study, targeting a series of popular applications, and using HDFIT to benchmark an open-source GEMM accelerator.},
  archive      = {J_TPDS},
  author       = {Patrik Omland and Alessio Netti and Yang Peng and Andrea Baldovin and Michael Paulitsch and Gustavo Espinosa and Jorge Parra and Gereon Hinz and Alois Knoll},
  doi          = {10.1109/TPDS.2023.3237777},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {995-1006},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HPC hardware design reliability benchmarking with HDFIT},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting core maintenance for dynamic hypergraphs.
<em>TPDS</em>, <em>34</em>(3), 981–994. (<a
href="https://doi.org/10.1109/TPDS.2023.3236669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Core maintenance for dynamic hypergraphs has been receiving an increasing attention. However, existing works mainly focus on the insertion/deletion of hyperedges. This article revisits the problem from the view of vertices change. We study core maintenance when the vertices are inserted/deleted into/from specific hyperedges in the hypergraph, which is a challenging task since the deletion of the vertex may increase the core numbers and the insertion of the vertex may decrease the core numbers. We discuss in detail the possible changes of core numbers in different situations. For the insertion/deletion of vertices contained by a single hyperedge, we design sequential algorithms to discover the vertices whose core numbers have changed. Compared with static recomputation (Leng et al. 2013) and LYCLC (Luo et al. 2021) algorithms, our sequential algorithms can accelerate more than 1,000× and 12× at most in the processing time, respectively. For the insertion/deletion of vertices contained by different hyperedges, we find that core numbers of all vertices change 1 at most if these hyperedges form a matching. We design parallel algorithms that divide a matching into different sets based on their core numbers and allot a thread to each set. Experiments show that our parallel algorithms have good stability, scalability, and parallelism. Compared with the parallel static algorithm (Gabert et al. 2021) and the parallel dynamic algorithm GPC (Gabert et al. 2021), our parallel algorithms with 32 threads can accelerate 33× and 22× at most in the processing time, respectively.},
  archive      = {J_TPDS},
  author       = {Qiang-Sheng Hua and Xiaohui Zhang and Hai Jin and Hong Huang},
  doi          = {10.1109/TPDS.2023.3236669},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {981-994},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Revisiting core maintenance for dynamic hypergraphs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). μBench: An open-source factory of benchmark microservice
applications. <em>TPDS</em>, <em>34</em>(3), 968–980. (<a
href="https://doi.org/10.1109/TPDS.2023.3236447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {$\mu$ Bench is an open-source tool for benchmarking cloud/edge computing platforms that run microservice applications. The tool creates dummy microservice applications that can be customized and executed on a Kubernetes cluster. $\mu$ Bench allows users to control fundamental properties of the microservice applications it creates, such as service mesh topology, microservices’ behaviors using a portfolio of stress functions (e.g., for CPU, memory, I/O, network) or implementing new ones, microservice-to-microservice API (HTTP or gRPC), etc. Application performance can be evaluated by stochastic or trace-driven workloads. μBench is aimed at researchers and cloud platform developers who lack real microservice applications to benchmark their findings (e.g., new resource control mechanisms, artificial intelligence-driven orchestration, etc.) or wish to thoroughly evaluate their proposals versus a broad set of heterogeneous applications that μBench can create. In addition to the description of μBench, in this article, we show one possible use of it. We compared advantages and disadvantages of microservice architectures versus monolithic ones, and analyzed the performance impact of key architectural choices, such as service mesh topology and the use of replication. For this analysis, we generated several microservice applications with different properties, and two of them are derived from a real cloud dataset.},
  archive      = {J_TPDS},
  author       = {Andrea Detti and Ludovico Funari and Luca Petrucci},
  doi          = {10.1109/TPDS.2023.3236447},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {968-980},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {μBench: An open-source factory of benchmark microservice applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An unequal caching strategy for shared-memory graph
analytics. <em>TPDS</em>, <em>34</em>(3), 955–967. (<a
href="https://doi.org/10.1109/TPDS.2022.3218885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in computer architecture significantly enhance the computational capacity of multicore systems. It allows large-scale graphs to be processed inside a single machine. Nevertheless, the irregular processing pattern of graph-structured data constrains the hardware resources from being productively utilized. In this paper, we investigate the constraints in two aspects: workload imbalance and parallel inefficiency. When a graph analytics algorithm is multithreaded, the thread time is highly diversified, indicating an uneven work distribution. Also, the intensive thread contention lowers the computing capacity of CPU cores, thereby hindering the effective utilization of CPU resources. To address these challenges, we present a proactive graph caching strategy that unequally segments graph components into cache-able subsets of varying sizes, namely Syze. First, the computational loads of cache-sized subgraphs are estimated. Then, the demanding subgraphs are further subdivided until certain threshold is met. Moreover, during the propagation of updates, a fraction of vertex ID (i.e., several bits) are encoded to facilitate the communication between subgraphs. As a result, Syze is able to balance the workloads amongst the logical cores by shortening the longest thread execution time. Meanwhile, it alleviates thread contention and thus elevates the parallel efficiency of multicores. Compared with well-optimized Ligra, Gemini and GPOP, Syze achieves accelerations by up to $17.76\times$ , $11.67\times$ and $2.81\times$ respectively. Additionally, the side effects of Syze are evaluated, including raised cache misses and memory accesses. They play a trivial role in deciding the overall performance, as their costs are far outweighed by the gains from the even distribution of workloads and the improved utilization of multicores. Author: Please confirm or add details for any funding or financial support for the research of this article. ?&gt;},
  archive      = {J_TPDS},
  author       = {YuAng Chen and Yeh-Ching Chung},
  doi          = {10.1109/TPDS.2022.3218885},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {955-967},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An unequal caching strategy for shared-memory graph analytics},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient GPU implementations of post-quantum signature
XMSS. <em>TPDS</em>, <em>34</em>(3), 938–954. (<a
href="https://doi.org/10.1109/TPDS.2022.3233348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The National Institute of Standards and Technology (NIST) approved XMSS as part of the post-quantum cryptography (PQC) development effort in 2018. XMSS is currently one of only two standardized PQC algorithms, but its performance limits its use. For example, the fastest record for some standardized parameters still takes more than a minute to generate a keypair. In this article, we present the first GPU implementation for XMSS and its variant XMSS $^{\mathsf {MT}}$ . The high parallelism of GPUs is especially effective for reducing latency in key generation and improving throughput for signing and verifying. In order to meet various application scenarios, we provide three parallel XMSS schemes: algorithmic parallelism , multi-keypair data parallelism , and single-keypair data parallelism . For these schemes, we design custom parallel strategies that use more than 10,000 cores for all parameters provided by NIST. In addition, we analyze the availability of most previous serial optimizations and explore numerous techniques to fully exploit GPU performance. Our evaluations are made with the XMSSMT-SHA2_20/2_256 parameter set on a GeForce RTX 3090. The result shows the key generation latency is 3.20 ms, a speedup of 21,899× compared to the GPU ported version, which is also 54× speedup faster than the fastest work (174 ms). When 16384 tasks are executed, the throughput (task/s) for signing/verifying in the single-key and multi-key cases is 311,424/415,100 and 145,100/419,887, respectively. Compared to the throughput for signing/verifying (1695/4000) of the fastest work, we obtain a speedup of 184×/104× and 86×/105× in single-key and multi-key cases, respectively.},
  archive      = {J_TPDS},
  author       = {Ziheng Wang and Xiaoshe Dong and Heng Chen and Yan Kang},
  doi          = {10.1109/TPDS.2022.3233348},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {938-954},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient GPU implementations of post-quantum signature XMSS},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of a quantization-based DNN delta compression
framework for model snapshots and federated learning. <em>TPDS</em>,
<em>34</em>(3), 923–937. (<a
href="https://doi.org/10.1109/TPDS.2022.3230840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved remarkable success in many fields. However, large-scale DNNs also bring storage costs when storing snapshots for preventing clusters’ frequent failures or incur significant communication overheads when transmitting DNNs in the Federated Learning (FL). Recently, several approaches, such as Delta-DNN and LC-Checkpoint, aim to reduce the size of DNNs’ snapshot storage by compressing the difference between two neighboring versions of the DNNs (a.k.a., delta). However, we observe that existing approaches, applying traditional global lossy quantization techniques in DNN&#39;s delta compression, can not fully exploit the data similarity since the parameters’ value ranges vary among layers. To fully explore the similarity of the delta model and improve the compression ratio, we propose a quantization-based local-sensitive delta compression approach, named QD-Compressor, by developing a layer-based local-sensitive quantization scheme and error feedback mechanism. Specifically, the quantizers and number of quantization bits are adaptive among layers based on the value distribution and weighted entropy of the delta&#39;s parameters. To avoid quantization error degrading the performance of the restored model, an alternative error feedback mechanism is designed to dynamically correct the quantization error during the training process. Experiments on multiple popular DNNs and datasets show that QD-Compressor obtains a higher 7×-40× compression ratio in the model snapshot compression scenario than the state-of-the-art approaches. Additionally, QD-Compressor achieves an 11×-15× compression ratio to the residual model of the Federated Learning compression scenario.},
  archive      = {J_TPDS},
  author       = {Haoyu Jin and Donglei Wu and Shuyu Zhang and Xiangyu Zou and Sian Jin and Dingwen Tao and Qing Liao and Wen Xia},
  doi          = {10.1109/TPDS.2022.3230840},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {923-937},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Design of a quantization-based DNN delta compression framework for model snapshots and federated learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GossipFL: A decentralized federated learning framework with
sparsified and adaptive communication. <em>TPDS</em>, <em>34</em>(3),
909–922. (<a href="https://doi.org/10.1109/TPDS.2022.3230938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, federated learning (FL) techniques have enabled multiple users to train machine learning models collaboratively without data sharing. However, existing FL algorithms suffer from the communication bottleneck due to network bandwidth pressure and/or low bandwidth utilization of the participating clients in both centralized and decentralized architectures. To deal with the communication problem while preserving the convergence performance, we introduce a communication-efficient decentralized FL framework GossipFL. In GossipFL, we 1) design a novel sparsification algorithm to enable that each client only needs to communicate with one peer with a highly sparsified model, and 2) propose a new and novel gossip matrix generation algorithm that can better utilize the bandwidth resources while preserving the convergence property. We also theoretically prove that GossipFL has convergence guarantees. We conduct experiments with three convolutional neural networks on two datasets (IID and non-IID) under two distributed environments (14 clients and 100 clients) to verify the effectiveness of GossipFL. Experimental results show that GossipFL takes less communication traffic for 38.5\% and less communication time for $49.8$\% than state-of-the-art solutions while achieving comparative model accuracy.},
  archive      = {J_TPDS},
  author       = {Zhenheng Tang and Shaohuai Shi and Bo Li and Xiaowen Chu},
  doi          = {10.1109/TPDS.2022.3230938},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {909-922},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GossipFL: A decentralized federated learning framework with sparsified and adaptive communication},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CoopEdge+: Enabling decentralized, secure and cooperative
multi-access edge computing based on blockchain. <em>TPDS</em>,
<em>34</em>(3), 894–908. (<a
href="https://doi.org/10.1109/TPDS.2022.3231296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access Edge Computing (MEC) has emerged as a new distributed computing paradigm for its ability to offer low-latency services to users. Suffering from constrained computational resources because of their limited physical sizes, edge servers usually cannot handle all the incoming compute tasks on time when they operate independently. Thus, they need to cooperate by peer-offloading. Incentive and trust are the two major challenges towards to cooperative computing among edge servers operating in a distrusted environment. Another specific challenge in the MEC environment is to facilitate incentive and trust in a decentralized manner. This article proposes CoopEdge+, a novel blockchain-based decentralized platform, to drive and support cooperative multi-access edge computing to tackle these challenges in a systematic manner. On CoopEdge+, an edge server can publish a compute task for other edge servers to contend for. A winner is selected from candidate edge servers as the task executor based on their reputation to perform the compute task. After that, CoopEdge+ employs a random leader election scheme to elect a task recorder without revealing its leadership until its consensus epoch. The task recorder will coordinate a consensus among edge servers to record the task executor&#39;s performance on blockchain. We implement CoopEdge+ based on Hyperledger fabric and evaluate it experimentally against a baseline implementation and three state-of-the-art implementations in a simulated MEC environment. The results validate the usefulness of CoopEdge+ and demonstrate its performance.},
  archive      = {J_TPDS},
  author       = {Liang Yuan and Qiang He and Siyu Tan and Bo Li and Jiangshan Yu and Feifei Chen and Yun Yang},
  doi          = {10.1109/TPDS.2022.3231296},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {894-908},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CoopEdge+: Enabling decentralized, secure and cooperative multi-access edge computing based on blockchain},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bio-ESMD: A data centric implementation for large-scale
biological system simulation on sunway TaihuLight supercomputer.
<em>TPDS</em>, <em>34</em>(3), 881–893. (<a
href="https://doi.org/10.1109/TPDS.2022.3220559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular dynamics (MD) simulations of biological systems are playing an increasingly important role in the research of pathogens and drugs. Most MD methods for biological simulations rely on the listed bonds which interact among specific groups of atoms identified by atom tags (unique atom tags regardless the storage location). However, efficient mapping of tags to atom locations is often challenging on modern many-core processors because data locality can not always be guaranteed for large-scale systems. In this paper, we present Bio-ESMD, a new MD implementation supporting listed bonds. Bio-ESMD is designed and developed based on our previously designed ESMD framework for many-core processors. In Bio-ESMD, we have introduced a data-centric approach for refactoring MD algorithms by reorganizing the cell list data structure to adopt bond lists with guaranteed data locality. Our implementation achieves speedups of over two compared to SW_GROMACS on Sunway TaihuLight. Furthermore, Bio-ESMD can simulate a system of 308.8 million atoms at 1.33 ns/day or 14.44 million atoms at 17.28 ns/day with linear weak scaling efficiency.},
  archive      = {J_TPDS},
  author       = {Xiaohui Duan and Qi Shao and Junben Weng and Bertil Schmidt and Lin Gan and Guohui Li and Haohuan Fu and Wei Xue and Weiguo Liu and Guangwen Yang},
  doi          = {10.1109/TPDS.2022.3220559},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {881-893},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Bio-ESMD: A data centric implementation for large-scale biological system simulation on sunway TaihuLight supercomputer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FCPP+miosix: Scaling aggregate programming to embedded
systems. <em>TPDS</em>, <em>34</em>(3), 869–880. (<a
href="https://doi.org/10.1109/TPDS.2022.3232633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the density of nodes capable of sensing, computing and actuation increases, it becomes increasingly useful to model an entire network of physical devices as a single, continuous space-time computing machine. The emergent behaviour of the whole software system is then induced by local computations deployed within each node and by the dynamics of the information diffusion. A relevant example of this distribution model is given by aggregate programming and its minimal set of functional constructs used to manipulate distributed data structures evolving over space and time, and resulting in robustness to changes. In this paper, we propose the first implementation of the aggregate computing paradigm targeting microcontrollers, by integrating FCPP, a C++ implementation of the paradigm, with Miosix, a modern operating system for microcontrollers with full C++ support. To the best of the author&#39;s knowledge, we are the first to present results on the effectiveness of FCPP in an embedded operating system setting as opposed to a simulation environment, thus considering tight memory and computational constraints and accounting for packet losses due to nonidealities of the radio channel. We implemented and tested on a network of WandStem nodes two benchmark applications: a network connectivity checker for network planning and preventive maintenance, and a decentralised contact tracing application. Additionally, we show that common problems in sensor networks such as neighbour discovery, construction of a graph of the network topology, coarse grain clock synchronisation as well as network monitoring and the collection of statistics (such as memory occupation data) can be easily performed thanks to the expressive semantics of aggregate programming.},
  archive      = {J_TPDS},
  author       = {Giorgio Audrito and Federico Terraneo and William Fornaciari},
  doi          = {10.1109/TPDS.2022.3232633},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {869-880},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FCPP+Miosix: Scaling aggregate programming to embedded systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RLQ: Workload allocation with reinforcement learning in
distributed queues. <em>TPDS</em>, <em>34</em>(3), 856–868. (<a
href="https://doi.org/10.1109/TPDS.2022.3231981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed workload queues are nowadays widely used due to their significant advantages in terms of decoupling, resilience, and scaling. Task allocation to worker nodes in distributed queue systems is typically simplistic (e.g., Least Recently Used) or uses hand-crafted heuristics that require task-specific information (e.g., task resource demands or expected time of execution). When such task information is not available and worker node capabilities are not homogeneous, the existing placement strategies may lead to unnecessarily large execution timings and usage costs. In this work, we formulate the task allocation problem in the Markov Decision Process framework, in which an agent assigns tasks to an available resource, and receives a numerical reward signal upon task completion. Our adaptive and learning-based task allocation solution, Reinforcement Learning based Queues ( RLQ ), is implemented and integrated with the popular Celery task queuing system for Python. We compare RLQ against traditional solutions using both synthetic and real workload traces. On average, using synthetic workloads, RLQ reduces the execution cost by approximately 70\%, the execution time by a factor of at least 3×, and the waiting time by almost 7×. Using real traces, we observe an improvement of about 20\% for execution cost, around 70\% improvement for execution time, and a reduction of approximately 20× in waiting time. We also compare RLQ with a strategy inspired by E-PVM, a state-of-the-art solution used in Google&#39;s Borg cluster manager, showing we are able to outperform it in five out of six scenarios.},
  archive      = {J_TPDS},
  author       = {Alessandro Staffolani and Victor-Alexandru Darvariu and Paolo Bellavista and Mirco Musolesi},
  doi          = {10.1109/TPDS.2022.3231981},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {856-868},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RLQ: Workload allocation with reinforcement learning in distributed queues},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PetaKV: Building efficient key-value store for file system
metadata on persistent memory. <em>TPDS</em>, <em>34</em>(3), 843–855.
(<a href="https://doi.org/10.1109/TPDS.2022.3232382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous works proposed building file systems and organizing the metadata with KV stores because KV stores handle entries of various sizes efficiently and have excellent scalability. The emergence of the byte-addressable persistent memory (PM) enables metadata service to be faster than before by tailoring the KV store for the PM. However, existing PM-based KV stores cannot handle the workloads of file systems’ metadata well because simply depending on hash tables or trees cannot simultaneously provide fast file accessing and efficient directory traversing. In this paper, we exploit the insight of the metadata operations and propose the PetaKV, a KV store tailored for the metadata management of file systems on PM. PetaKV leverages dual hash indexing to achieve fast file put and get operations. Moreover, it cooperates with PM-tailored peta logs to collocate KV entries for each directory, thus supporting efficient directory scans. Our evaluation indicates PetaKV outperforms state-of-art tree-based KV stores on put, get and scan $2.5\times$ , $3.2\times$ , and $2.8\times$ on average, respectively. Moreover, the file system built with PetaKV achieves $1.2\times$ to $6.4\times$ speedup compared to those built with tree-based KV stores on the metadata operations.},
  archive      = {J_TPDS},
  author       = {Yiwen Zhang and Jian Zhou and Xinhao Min and Song Ge and Jiguang Wan and Ting Yao and Daohui Wang},
  doi          = {10.1109/TPDS.2022.3232382},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {843-855},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PetaKV: Building efficient key-value store for file system metadata on persistent memory},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Securing deployed smart contracts and DeFi with distributed
TEE cluster. <em>TPDS</em>, <em>34</em>(3), 828–842. (<a
href="https://doi.org/10.1109/TPDS.2022.3232548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contract technologies can be used to implement almost arbitrary business logic. They can revolutionize many businesses such as payments, insurance, and crowdfunding. The resulting birth of decentralized finance (DeFi) has gained significant momentum. Smart contracts and DeFi are now attractive targets for attacks. An important research question is how to protect deployed smart contracts and DeFi. Smart contracts cannot be modified once deployed, namely vulnerabilities cannot be fixed by patching. In this case, vulnerabilities in deployed contracts and DeFi might cause devastating consequences. In this paper, we put forward SolSaviour, a framework for protecting deployed smart contracts and DeFi. The core of SolSaviour is to build a smart contract protection mechanism based on democratic voting using a distributed trusted execution environment (TEE) cluster. Once a vulnerability in deployed contracts or DeFi is found, SolSaviour can destroy the defective contract and redeploy a patched contract via the distributed TEE cluster. Moreover, SolSaviour can migrate funds and state variables from the destroyed contract to the patched one. Compared with previous work, our approach can protect smart contracts and DeFi in a distributed manner, avoiding reliance on privileged users or trusted third parties. Our experiment results show that SolSaviour can protect smart contracts and complex DeFi protocols with feasible overhead.},
  archive      = {J_TPDS},
  author       = {Zecheng Li and Bin Xiao and Songtao Guo and Yuanyuan Yang},
  doi          = {10.1109/TPDS.2022.3232548},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {828-842},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Securing deployed smart contracts and DeFi with distributed TEE cluster},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IGniter: Interference-aware GPU resource provisioning for
predictable DNN inference in the cloud. <em>TPDS</em>, <em>34</em>(3),
812–827. (<a href="https://doi.org/10.1109/TPDS.2022.3232715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPUs are essential to accelerating the latency-sensitive deep neural network (DNN) inference workloads in cloud datacenters. To fully utilize GPU resources, spatial sharing of GPUs among co-located DNN inference workloads becomes increasingly compelling. However, GPU sharing inevitably brings severe performance interference among co-located inference workloads, as motivated by an empirical measurement study of DNN inference on EC2 GPU instances. While existing works on guaranteeing inference performance service level objectives (SLOs) focus on either temporal sharing of GPUs or reactive GPU resource scaling and inference migration techniques, how to proactively mitigate such severe performance interference has received comparatively little attention. In this paper, we propose iGniter , an interference-aware GPU resource provisioning framework for cost-efficiently achieving predictable DNN inference in the cloud. iGniter is comprised of two key components: (1) a lightweight DNN inference performance model, which leverages the system and workload metrics that are practically accessible to capture the performance interference; (2) A cost-efficient GPU resource provisioning strategy that jointly optimizes the GPU resource allocation and adaptive batching based on our inference performance model, with the aim of achieving predictable performance of DNN inference workloads. We implement a prototype of iGniter based on the NVIDIA Triton inference server hosted on EC2 GPU instances. Extensive prototype experiments on four representative DNN models and datasets demonstrate that iGniter can guarantee the performance SLOs of DNN inference workloads with practically acceptable runtime overhead, while saving the monetary cost by up to $25\%$ in comparison to the state-of-the-art GPU resource provisioning strategies.},
  archive      = {J_TPDS},
  author       = {Fei Xu and Jianian Xu and Jiabin Chen and Li Chen and Ruitao Shang and Zhi Zhou and Fangming Liu},
  doi          = {10.1109/TPDS.2022.3232715},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {812-827},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IGniter: Interference-aware GPU resource provisioning for predictable DNN inference in the cloud},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comments on “IPPTS: An efficient algorithm for scientific
workflow scheduling in heterogeneous computing systems.” <em>TPDS</em>,
<em>34</em>(3), 810–811. (<a
href="https://doi.org/10.1109/TPDS.2022.3232326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IPPTS (Improved Predict Priority Task Scheduling) is a list scheduling algorithm that schedules task graphs on fully connected heterogeneous distributed systems, with an objective of minimizing the overall makespan (i.e., schedule length). With respect to the literature on list scheduling techniques for task graphs, IPPTS improves the task prioritization by considering the “out-degree” of a task. However, we have observed that the IPPTS algorithm contains an ambiguity which introduces the possibility of assigning higher priority to a task compared to its predecessors in a task graph . This priority inversion may lead to the generation of an incorrect schedule due to the violation of precedence-constraints among tasks. In this note, we first highlight this issue using a counter example. Then, we discuss two possible ways to fix the ambiguity in the algorithm.},
  archive      = {J_TPDS},
  author       = {Rajesh Devaraj and Arnab Sarkar},
  doi          = {10.1109/TPDS.2022.3232326},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {810-811},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Comments on “IPPTS: An efficient algorithm for scientific workflow scheduling in heterogeneous computing systems”},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring memory access similarity to improve irregular
application performance for distributed hybrid memory systems.
<em>TPDS</em>, <em>34</em>(3), 797–809. (<a
href="https://doi.org/10.1109/TPDS.2022.3227544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing problem complexity, more irregular applications are deployed on high-performance clusters due to the parallel working paradigm, and yield irregular memory access behaviors across nodes. However, the irregularity of memory access behaviors is not comprehensively studied, which results in low utilization of the integrated hybrid memory system compositing of stacked DRAM and off-chip DRAM. To address this problem, we devise a novel method called Similarity-Managed Hybrid Memory System ( SM-HMS ) to improve the hybrid memory system performance by leveraging the memory access similarity among nodes in a cluster. Within SM-HMS , two techniques are proposed, Memory Access Similarity Measuring and Similarity-based Memory Access Behavior Sharing . To quantify the memory access similarity, memory access behaviors of each node are vectorized, and the distance between two vectors is used as the memory access similarity. The calculated memory access similarity is used to share memory access behaviors precisely across nodes. With the shared memory access behaviors, SM-HMS divides the stacked DRAM into two sections, the sliding window section and the outlier section . The shared memory access behaviors guide the replacement of the sliding window section while the outlier section is managed in the LRU manner. Our evaluation results with a set of irregular applications on various clusters consisting of up to 256 nodes have shown that SM-HMS outperforms the state-of-the-art approaches, Cameo , Chameleon , and Hyrbid2 , on job finish time reduction by up to $58.6\%$ , $56.7\%$ , and $31.3\%$ , with $46.1\%$ , $41.6\%$ , and $19.3\%$ on average, respectively. SM-HMS can also achieve up to $98.6\%$ ( $91.9\%$ on average) of the ideal hybrid memory system performance.},
  archive      = {J_TPDS},
  author       = {Wenjie Liu and Xubin He and Qing Liu},
  doi          = {10.1109/TPDS.2022.3227544},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {797-809},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploring memory access similarity to improve irregular application performance for distributed hybrid memory systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revenue maximizing online service function chain deployment
in multi-tier computing network. <em>TPDS</em>, <em>34</em>(3), 781–796.
(<a href="https://doi.org/10.1109/TPDS.2022.3232205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-tier computing (MC) is a promising architecture that integrates cloud computing, fog computing, and edge computing to provide users with a consistent experience of computing services by fusing computing devices within the network through virtualization technology. Although MC combines powerful computation and communication resources, the massive demand from Service Function Chain (SFC) deployments continues to make it challenging regarding resource constraints, latency satisfaction, and revenue-cost tradeoffs. To this end, in this article, we study an SFC deployment problem in MC and formulate a problem for maximizing the revenue of online SFC deployment under latency, computation resources, and communication resources constraints. To solve this online problem better, we construct a computation and communication resource cost model and transform the original online problem into a deployment cost minimization problem and a request admission problem by an alternating optimization approach. To solve the two subproblems, we propose an online approximation algorithm with a provable competitive ratio for the particular scenario with no latency requirements. Then, based on the cost model, we propose an online heuristic algorithm that adopts a binary search method for the original problem with latency requirements. Simulation experiments show that our two proposed online algorithms have advantages in total revenue, running time, and load balancing compared with other comparison algorithms.},
  archive      = {J_TPDS},
  author       = {Haolin Liu and Saiqin Long and Zhetao Li and Yu Fu and Yong Zuo and Xinglin Zhang},
  doi          = {10.1109/TPDS.2022.3232205},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {781-796},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Revenue maximizing online service function chain deployment in multi-tier computing network},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AGCM-3DLF: Accelerating atmospheric general circulation
model via 3-d parallelization and leap-format. <em>TPDS</em>,
<em>34</em>(3), 766–780. (<a
href="https://doi.org/10.1109/TPDS.2022.3231013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The atmospheric general circulation model (AGCM) has been an important research tool in the study of climate change for decades. As the demand for high-resolution simulation is becoming urgent, the scalability and simulation efficiency is faced with great challenges, especially for the latitude-longitude mesh-based models. In this paper, we propose a highly scalable 3-D atmospheric general circulation model based on leap-format, namely AGCM-3DLF. First, it utilizes a 3-D decomposition method allowing for parallelism release in all three physical dimensions. Then the leap-format difference computation scheme is adopted to maintain computational stability in grid updating and avoid additional filtering at the high latitudes. A novel shifting window communication algorithm is designed for parallelization of the unified model. Furthermore, a series of optimizations are conducted to improve the effectiveness of large-scale simulations. Experiment results in different platforms demonstrate good efficiency and scalability of the model. AGCM-3DLF scales up to the entire CAS-Xiandao1 supercomputer (196,608 CPU cores), attaining the speed of 11.1 simulation-year-per-day (SYPD) at a high resolution of 25KM. In addition, simulations conducted on the Sunway TaihuLight supercomputer exhibit a 1.06 million cores scalability with 36.1\% parallel efficiency.},
  archive      = {J_TPDS},
  author       = {Hang Cao and Liang Yuan and He Zhang and Yunquan Zhang and Baodong Wu and Kun Li and Shigang Li and Minghua Zhang and Pengqi Lu and Junmin Xiao},
  doi          = {10.1109/TPDS.2022.3231013},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {766-780},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AGCM-3DLF: Accelerating atmospheric general circulation model via 3-D parallelization and leap-format},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain-based P2P content delivery with monetary
incentivization and fairness guarantee. <em>TPDS</em>, <em>34</em>(2),
746–765. (<a href="https://doi.org/10.1109/TPDS.2022.3217036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peer-to-peer (P2P) content delivery is up-and-coming to provide benefits comprising cost-saving and scalable peak-demand handling compared with centralized content delivery networks (CDNs), and also complementary to the popular decentralized storage networks such as Filecoin. However, reliable P2P delivery demands proper enforcement of delivery fairness, i.e., the deliverers should be rewarded in line with their in-time delivery. Unfortunately, most existing studies on delivery fairness are on the basis of non-cooperative game-theoretic assumptions that are arguably unrealistic in the ad-hoc P2P setting. We propose an expressive yet still minimalist security requirement for desired fair P2P content delivery, and give two efficient blockchain-enabled and monetary-incentivized solutions ${\mathsf {FairDownload}}$ and ${\mathsf {FairStream}}$ for P2P downloading and P2P streaming scenarios, respectively. Our designs not only ensure delivery fairness where deliverers are paid (nearly) proportional to their in-time delivery, but also guarantee exchange fairness where content consumers and content providers are also fairly treated. The fairness of each party can be assured even when other two parties collude to arbitrarily misbehave. Our protocols provide a general design of fetching content chunk from any specific position so the delivery can be resumed in the presence of unexpected interruption. Further, our systems are efficient in the sense of achieving asymptotically optimal on-chain costs and optimal delivery communication. We implement the prototype and deploy on the Ethereum Ropsten network. Extensive experiments in both LAN and WAN settings are conducted to evaluate the on-chain costs as well as the efficiency of downloading and streaming. Experimental results show the practicality and efficiency of our protocols.},
  archive      = {J_TPDS},
  author       = {Songlin He and Yuan Lu and Qiang Tang and Guiling Wang and Chase Qishi Wu},
  doi          = {10.1109/TPDS.2022.3217036},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {746-765},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Blockchain-based P2P content delivery with monetary incentivization and fairness guarantee},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spiking neural p systems with communication on request and
mute rules. <em>TPDS</em>, <em>34</em>(2), 734–745. (<a
href="https://doi.org/10.1109/TPDS.2022.3228931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural P systems with communication on request (SNQP systems) are neurally inspired computing devices, where a neuron actively seeks spikes from presynaptic neurons instead of passively waiting for spikes. In this work, we consider SNQP systems with mute rules (SNQPM systems), where mute rules have no communication functioning, namely the application of a mute rule only affects the number of spikes in the neuron where the rule resides, without effect on other neurons. It is demonstrated the computation capability of SNQPM systems with only mute rules does not exceed that of register machines with two registers, thereby not Turing universal. SNQPM systems are Turing universal when both mute rules and request rules are employed. Furthermore, two universal SNQPM systems with 7 neurons or 13 neurons are constructed as devices of number generating and function computing, respectively. Comparing to the universal SNQP system with 14 neurons and two types of spikes, SNQPM systems show the capability of trading-off mute rules and the types of spikes.},
  archive      = {J_TPDS},
  author       = {Tingfang Wu and Linqiang Pan},
  doi          = {10.1109/TPDS.2022.3228931},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {734-745},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Spiking neural p systems with communication on request and mute rules},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DELICIOUS: Deadline-aware approximate computing in
cache-conscious multicore. <em>TPDS</em>, <em>34</em>(2), 718–733. (<a
href="https://doi.org/10.1109/TPDS.2022.3228751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing result-accuracy in approximate computing (AC) based real-time systems, without violating power constraints of the underlying hardware, is a challenging problem. Execution of such AC real-time applications can be split into two parts: (i) the mandatory part , execution of which provides a result of acceptable quality, followed by (ii) the optional part , that can be executed partially or fully to refine the initially obtained result in order to increase the result-accuracy, without violating the time-constraint. This article introduces DELICIOUS , a novel hybrid offline-online scheduling strategy for AC real-time dependent tasks. By employing an efficient heuristic algorithm , DELICIOUS first generates a schedule for a task-set with an objective to maximize the results-accuracy, while respecting system-wide constraints. During execution, DELICIOUS then introduces a prudential cache resizing that reduces temperature of the adjacent cores, by generating thermal buffers at the turned off cache ways. DELICIOUS further trades off this thermal benefits by enhancing the processing speed of the cores for a stipulated duration, called V/F Spiking , without violating the power budget of the core, to shorten the execution length of the tasks. This reduced runtime is exploited either to enhance result-accuracy by dynamically adjusting the optional part, or to reduce temperature by enabling sleep mode at the cores. While surpassing the prior art, DELICIOUS offers 80\% result-accuracy with its scheduling strategy, which is further enhanced by 8.3\% in online, while reducing runtime peak temperature by 5.8°C on average, as shown by benchmark based evaluation on a 4-core based multicore.},
  archive      = {J_TPDS},
  author       = {Sangeet Saha and Shounak Chakraborty and Sukarn Agarwal and Rahul Gangopadhyay and Magnus Själander and Klaus McDonald-Maier},
  doi          = {10.1109/TPDS.2022.3228751},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {718-733},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DELICIOUS: Deadline-aware approximate computing in cache-conscious multicore},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Building trust in earth science findings through data
traceability and results explainability. <em>TPDS</em>, <em>34</em>(2),
704–717. (<a href="https://doi.org/10.1109/TPDS.2022.3220539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To trust findings in computational science, scientists need workflows that trace the data provenance and support results explainability. As workflows become more complex, tracing data provenance and explaining results become harder to achieve. In this paper, we propose a computational environment that automatically creates a workflow execution&#39;s record trail and invisibly attaches it to the workflow&#39;s output, enabling data traceability and results explainability. Our solution transforms existing container technology, includes tools for automatically annotating provenance metadata, and allows effective movement of data and metadata across the workflow execution. We demonstrate the capabilities of our environment with the study of SOMOSPIE, an earth science workflow. Through a suite of machine learning modeling techniques, this workflow predicts soil moisture values from the 27 km resolution satellite data down to higher resolutions necessary for policy making and precision agriculture. By running the workflow in our environment, we can identify the causes of different accuracy measurements for predicted soil moisture values in different resolutions of the input data and link different results to different machine learning methods used during the soil moisture downscaling, all without requiring scientists to know aspects of workflow design and implementation.},
  archive      = {J_TPDS},
  author       = {Paula Olaya and Dominic Kennedy and Ricardo Llamas and Leobardo Valera and Rodrigo Vargas and Jay Lofstead and Michela Taufer},
  doi          = {10.1109/TPDS.2022.3220539},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {704-717},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Building trust in earth science findings through data traceability and results explainability},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FSP: Towards flexible synchronous parallel frameworks for
distributed machine learning. <em>TPDS</em>, <em>34</em>(2), 687–703.
(<a href="https://doi.org/10.1109/TPDS.2022.3228733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Myriad of machine learning (ML) algorithms refine model parameters iteratively. Existing synchronous data-parallel frameworks can accelerate training with convergence guarantees. However, the pre-assigned workload-based synchronous design still poses great challenges, since fast workers must wait for slow, straggling ones, especially in a heterogeneous computing cluster. Asynchronous alternatives can bypass this performance bottleneck, but at expense of potentially losing convergence guarantees. This article proposes a new time-based flexible synchronous parallel framework (FSP). It provides strict convergence analysis by consistently updating parameters, as well as significant cost reduction by completely unleashing the power of fast workers. It identifies the optimal synchronization frequency, by online balancing costs of parameters’ update and benefits brought by their freshness. Besides the basic goal of keeping all workers fully CPU-utilized, FSP also aims to keep data spread over the cluster fully utilized, so that they can contribute to convergence with equal opportunities. These proposals are all implemented in a prototype system Flegel, with additional engineering optimizations for further performance enhancement and programming facilitation. Experiments demonstrate that Flegel significantly outperforms recent studies.},
  archive      = {J_TPDS},
  author       = {Zhigang Wang and Yilei Tu and Ning Wang and Lixin Gao and Jie Nie and Zhiqiang Wei and Yu Gu and Ge Yu},
  doi          = {10.1109/TPDS.2022.3228733},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {687-703},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FSP: Towards flexible synchronous parallel frameworks for distributed machine learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-centric client selection for federated learning over
distributed edge networks. <em>TPDS</em>, <em>34</em>(2), 675–686. (<a
href="https://doi.org/10.1109/TPDS.2022.3217271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an efficient data-centric client selection approach, named DICE, to enable federated learning (FL) over distributed edge networks. Prior research focused on assessing the computation and communication ability of the client devices for selection in FL. On-device data quality, in terms of data volume and heterogeneity, across these distributed devices is largely overlooked. The obvious outcome is the selection of an improper subset of clients with poor-quallity data, which inevitably results in an inefficient trained model. With an aim to address this problem, in this work, we design DICE which prioritizes the data quality of the client devices in the selection phase, in addition to their computation and communication abilities, to improve the accuracy of FL. Additionally, in DICE, we introduce the assistance of vicinal edge devices to account for the lack of computation or communication abilities in certain devices without violating the privacy-preserving guarantees of FL. Towards this aim, we propose a scheme to decide the optimal edge device, in terms of latency and workload, to be selected as the helper device. The experimental results show that DICE improves convergence speed for a given level of model accuracy. Further, the simulation results show that DICE reduces delay by at least 16\%, energy consumption by at least 17\%, and packet loss by at least 55\% compared to the existing benchmarks while prioritizing the on-device data quality across clients.},
  archive      = {J_TPDS},
  author       = {Rituparna Saha and Sudip Misra and Aishwariya Chakraborty and Chandranath Chatterjee and Pallav Kumar Deb},
  doi          = {10.1109/TPDS.2022.3217271},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {675-686},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Data-centric client selection for federated learning over distributed edge networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neighbor graph based tensor recovery for accurate internet
anomaly detection. <em>TPDS</em>, <em>34</em>(2), 655–674. (<a
href="https://doi.org/10.1109/TPDS.2022.3227570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting anomalous traffic is a crucial task for network management. Although many anomaly detection algorithms have been proposed recently, constrained by their matrix-based traffic data model, existing algorithms often suffer from low detection accuracy. To fully utilize the multi-dimensional information hidden in the traffic data, this paper uses the tensor model for more accurate Internet anomaly detection. Only considering the low-rank linearity features hidden in the data, current tensor factorization techniques would result in low anomaly detection accuracy. We propose a novel Graph-based Tensor Recovery model (Graph-TR) to well explore both low-rank linearity features as well as the non-linear proximity information hidden in the traffic data for better anomaly detection. We encode the non-linear proximity information of the traffic data by constructing nearest neighbor graphs and incorporate this information into the tensor factorization using the graph Laplacian. Moreover, to facilitate the quick building of neighbor graph, we propose a nearest neighbor searching algorithm with the simple locality-sensitive hashing (LSH). Besides only detecting random anomalies, our algorithm can also effectively detect structured anomalies that appear as bursts. We have conducted extensive experiments using Internet traffic trace data Abilene and GÈANT. Compared with the state of art algorithms on matrix-based anomaly detection and tensor recovery approach, our Graph-TR can achieve higher Accuracy and Recall.},
  archive      = {J_TPDS},
  author       = {Xiaocan Li and Kun Xie and Xin Wang and Gaogang Xie and Kenli Li and Jiannong Cao and Dafang Zhang and Hongbo Jiang and Jigang Wen},
  doi          = {10.1109/TPDS.2022.3227570},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {655-674},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Neighbor graph based tensor recovery for accurate internet anomaly detection},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benzene: Scaling blockchain with cooperation-based sharding.
<em>TPDS</em>, <em>34</em>(2), 639–654. (<a
href="https://doi.org/10.1109/TPDS.2022.3227198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharding has been considered as a prominent approach to enhance the limited performance of blockchain. However, most sharding systems leverage a non-cooperative design, which lowers the fault tolerance resilience due to the decreased mining power as the consensus execution is limited to each separated shard. To this end, we present Benzene, a novel sharding system that enhances the performance by cooperation-based sharding while defending the per-shard security. First, we establish a double-chain architecture for function decoupling. This architecture separates transaction-recording functions from consensus-execution functions, thereby enabling the cross-shard cooperation during consensus execution while preserving the concurrency nature of sharding. Second, we design a cross-shard block verification mechanism leveraging Trusted Execution Environment (TEE), via which miners can verify blocks from other shards during the cooperation process with the minimized overheads. Finally, we design a voting-based consensus protocol for cross-shard cooperation. Transactions in each shard are confirmed by all shards that simultaneously cast votes, consequently achieving an enhanced fault tolerance and lowering the confirmation latency. We implement Benzene and conduct both prototype experiments and large-scale simulations to evaluate the performance of Benzene. Results show that Benzene achieves superior performance than existing sharding/non-sharding blockchain protocols. In particular, Benzene achieves a linearly-improved throughput with the increased number of shards (e.g., 32,370 transactions per second with 50 shards) and maintains a lower confirmation latency than Bitcoin (with more than 50 shards). Meanwhile, Benzene maintains a fixed fault tolerance at 1/3 even with the increased number of shards.},
  archive      = {J_TPDS},
  author       = {Zhongteng Cai and Junyuan Liang and Wuhui Chen and Zicong Hong and Hong-Ning Dai and Jianting Zhang and Zibin Zheng},
  doi          = {10.1109/TPDS.2022.3227198},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {639-654},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Benzene: Scaling blockchain with cooperation-based sharding},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The role of idle waves, desynchronization, and bottleneck
evasion in the performance of parallel programs. <em>TPDS</em>,
<em>34</em>(2), 623–638. (<a
href="https://doi.org/10.1109/TPDS.2022.3221085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of highly parallel applications on distributed-memory systems is influenced by many factors. Analytic performance modeling techniques aim to provide insight into performance limitations and are often the starting point of optimization efforts. However, coupling analytic models across the system hierarchy (socket, node, network) fails to encompass the intricate interplay between the program code and the hardware, especially when execution and communication bottlenecks are involved. In this paper we investigate the effect of bottleneck evasion and how it can lead to automatic overlap of communication overhead with computation. Bottleneck evasion leads to a gradual loss of the initial bulk-synchronous behavior of a parallel code so that its processes become desynchronized. This occurs most prominently in memory-bound programs, which is why we choose memory-bound benchmark and application codes, specifically an MPI-augmented STREAM Triad, sparse matrix-vector multiplication, and a collective-avoiding Chebyshev filter diagonalization code to demonstrate the consequences of desynchronization on two different supercomputing platforms. We investigate the role of idle waves as possible triggers for desynchronization and show the impact of automatic asynchronous communication for a spectrum of code properties and parameters, such as saturation point, matrix structures, domain decomposition, and communication concurrency. Our findings reveal how eliminating synchronization points (such as collective communication or barriers) precipitates performance improvements that go beyond what can be expected by simply subtracting the overhead of the collective from the overall runtime.},
  archive      = {J_TPDS},
  author       = {Ayesha Afzal and Georg Hager and Gerhard Wellein},
  doi          = {10.1109/TPDS.2022.3221085},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {623-638},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The role of idle waves, desynchronization, and bottleneck evasion in the performance of parallel programs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mpi4py.futures: MPI-based asynchronous task execution for
python. <em>TPDS</em>, <em>34</em>(2), 611–622. (<a
href="https://doi.org/10.1109/TPDS.2022.3225481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present mpi4py.futures, a lightweight, asynchronous task execution framework targeting the Python programming language and using the Message Passing Interface (MPI) for interprocess communication. mpi4py.futures follows the interface of the concurrent.futures package from the Python standard library and can be used as its drop-in replacement, while allowing applications to scale over multiple compute nodes. We discuss the design, implementation, and feature set of mpi4py.futures and compare its performance to other solutions on both shared and distributed memory architectures. On a shared-memory system, we show mpi4py.futures to consistently outperform Python&#39;s concurrent.futures with speedup ratios between 1.4X and 3.7X in throughput (tasks per second) and between 1.9X and 2.9X in bandwidth. On a Cray XC40 system, we compare mpi4py.futures to Dask – a well-known Python parallel computing package. Although we note more varied results, we show mpi4py.futures to outperform Dask in most scenarios.},
  archive      = {J_TPDS},
  author       = {Marcin Rogowski and Samar Aseeri and David Keyes and Lisandro Dalcin},
  doi          = {10.1109/TPDS.2022.3225481},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {611-622},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Mpi4py.futures: MPI-based asynchronous task execution for python},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TFormer: A transmission-friendly ViT model for IoT devices.
<em>TPDS</em>, <em>34</em>(2), 598–610. (<a
href="https://doi.org/10.1109/TPDS.2022.3222765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying high-performance vision transformer (ViT) models on ubiquitous Internet of Things (IoT) devices to provide high-quality vision services will revolutionize the way we live, work, and interact with the world. Due to the contradiction between the limited resources of IoT devices and resource-intensive ViT models, the use of cloud servers to assist ViT model training has become mainstream. However, due to the larger number of parameters and floating-point operations (FLOPs) of the existing ViT models, the model parameters transmitted by cloud servers are large and difficult to run on resource-constrained IoT devices. To this end, this article proposes a transmission-friendly ViT model, TFormer, for deployment on resource-constrained IoT devices with the assistance of a cloud server. The high performance and small number of model parameters and FLOPs of TFormer are attributed to the proposed hybrid layer and the proposed partially connected feed-forward network (PCS-FFN). The hybrid layer consists of nonlearnable modules and a pointwise convolution, which can obtain multitype and multiscale features with only a few parameters and FLOPs to improve the TFormer performance. The PCS-FFN adopts group convolution to reduce the number of parameters. The key idea of this article is to propose TFormer with few model parameters and FLOPs to facilitate applications running on resource-constrained IoT devices to benefit from the high performance of the ViT models. Experimental results on the ImageNet-1K, MS COCO, and ADE20K datasets for image classification, object detection, and semantic segmentation tasks demonstrate that the proposed model outperforms other state-of-the-art models. Specifically, TFormer-S achieves 5\% higher accuracy on ImageNet-1K than ResNet18 with 1.4× fewer parameters and FLOPs.},
  archive      = {J_TPDS},
  author       = {Zhichao Lu and Chuntao Ding and Felix Juefei-Xu and Vishnu Naresh Boddeti and Shangguang Wang and Yun Yang},
  doi          = {10.1109/TPDS.2022.3222765},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {598-610},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TFormer: A transmission-friendly ViT model for IoT devices},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Level-based blocking for sparse matrices: Sparse
matrix-power-vector multiplication. <em>TPDS</em>, <em>34</em>(2),
581–597. (<a href="https://doi.org/10.1109/TPDS.2022.3223512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiplication of a sparse matrix with a dense vector (SpMV) is a key component in many numerical schemes and its performance is known to be severely limited by main memory access. Several numerical schemes require the multiplication of a sparse matrix polynomial with a dense vector which is typically implemented as a sequence of SpMVs. This results in low performance and ignores the potential to increase the arithmetic intensity by reusing the matrix data from cache. In this work we use the recursive algebraic coloring engine (RACE) to enable blocking of sparse matrix data across the polynomial computations. In the graph representing the sparse matrix we form levels using a breadth-first search. Locality relations of these levels are then used to improve spatial and temporal locality when accessing the matrix data and to implement an efficient multithreaded parallelization. Our approach is independent of the matrix structure and avoids shortcomings of existing “blocking” strategies in terms of hardware efficiency and parallelization overhead. We quantify the quality of our implementation using performance modelling and demonstrate speedups of up to 3× and 5× compared to an optimal SpMV-based baseline on a single multicore chip of recent Intel and AMD architectures. Various numerical schemes like $s$ -step Krylov solvers, polynomial preconditioners and power clustering algorithms will benefit from our development.},
  archive      = {J_TPDS},
  author       = {Christie Alappat and Georg Hager and Olaf Schenk and Gerhard Wellein},
  doi          = {10.1109/TPDS.2022.3223512},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {581-597},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Level-based blocking for sparse matrices: Sparse matrix-power-vector multiplication},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personalized edge intelligence via federated self-knowledge
distillation. <em>TPDS</em>, <em>34</em>(2), 567–580. (<a
href="https://doi.org/10.1109/TPDS.2022.3225185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is an emerging approach in edge computing for collaboratively training machine learning models among multiple devices, which aims to address limited bandwidth, system heterogeneity, and privacy issues in traditional centralized training. However, the existing federated learning methods focus on learning a shared global model for all devices, which may not always be ideal for different devices. Such situations become even worse when each edge device has its own data distribution or task. In this paper, we study personalized federated learning in which our goal is to train models to perform well for individual clients. We observe that the initialization in each communication round causes the forgetting of historical personalized knowledge. Based on this observation, we propose a novel Personalized Federated Learning (PFL) framework via self-knowledge distillation, named pFedSD. By allowing clients to distill the knowledge of previous personalized models to current local models, pFedSD accelerates the process of recalling the personalized knowledge for the latest initialized clients. Moreover, self-knowledge distillation provides different views of data in feature space to realize an implicit ensemble of local models. Extensive experiments on various datasets and settings demonstrate the effectiveness and robustness of pFedSD.},
  archive      = {J_TPDS},
  author       = {Hai Jin and Dongshan Bai and Dezhong Yao and Yutong Dai and Lin Gu and Chen Yu and Lichao Sun},
  doi          = {10.1109/TPDS.2022.3225185},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {567-580},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Personalized edge intelligence via federated self-knowledge distillation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Securing fine-grained data sharing and erasure in outsourced
storage systems. <em>TPDS</em>, <em>34</em>(2), 552–566. (<a
href="https://doi.org/10.1109/TPDS.2022.3225274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide use of internet-connected services makes massive personal data collected by service providers without the need of our consent. Although the archived data may enable them to provide better service experiences for users, it also presents serious risks to individual privacy, especially when active or unexpected data breaches have become commonplace. To mitigate this issue, several acts and regulations (e.g., the European Union general data protection regulation) have been issued and specified a lot of security requirements for personal data management. Among these various requirements, we mainly focus on the requirement of giving back the access control of personal data to data owners themselves and the right to be forgotten for data erasure. In this article, we provide a cryptographic solution of achieving these two requirements in the setting of outsourced storage. Specifically, we introduce a personal data management framework built upon a novel cryptographic primitive dubbed as forward-secure attribute-based puncturable encryption (FS-DABPE). This primitive simultaneously features of system-wide forward secrecy and practical key management as well as fine-grained access control of the encrypted personal data. Consequently, by locally puncturing, updating and erasing system-wide secret keys, it securely realizes fine-grained personal data sharing and data erasure without interactions. Furthermore, to instantiate the proposed framework, we present a concrete FS-DABPE construction, and prove its security under a well-studied complexity assumption. In addition, we provide a prototype implementation of the concrete construction, and present extensive experimental results that illustrate its feasibility and practicability.},
  archive      = {J_TPDS},
  author       = {Jianghong Wei and Xiaofeng Chen and Jianfeng Wang and Xinyi Huang and Willy Susilo},
  doi          = {10.1109/TPDS.2022.3225274},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {552-566},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Securing fine-grained data sharing and erasure in outsourced storage systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-job intelligent scheduling with cross-device federated
learning. <em>TPDS</em>, <em>34</em>(2), 535–551. (<a
href="https://doi.org/10.1109/TPDS.2022.3224941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a large amount of decentralized data in various (edge) devices of end-users, while the decentralized data aggregation remains complicated for machine learning jobs because of regulations and laws. As a practical approach to handling decentralized data, Federated Learning (FL) enables collaborative global machine learning model training without sharing sensitive raw data. The servers schedule devices to jobs within the training process of FL. In contrast, device scheduling with multiple jobs in FL remains a critical and open problem. In this article, we propose a novel multi-job FL framework, which enables the training process of multiple jobs in parallel. The multi-job FL framework is composed of a system model and a scheduling method. The system model enables a parallel training process of multiple jobs, with a cost model based on the data fairness and the training time of diverse devices during the parallel training process. We propose a novel intelligent scheduling approach based on multiple scheduling methods, including an original reinforcement learning-based scheduling method and an original Bayesian optimization-based scheduling method, which corresponds to a small cost while scheduling devices to multiple jobs. We conduct extensive experimentation with diverse jobs and datasets. The experimental results reveal that our proposed approaches significantly outperform baseline approaches in terms of training time (up to 12.73 times faster) and accuracy (up to 46.4\% higher).},
  archive      = {J_TPDS},
  author       = {Ji Liu and Juncheng Jia and Beichen Ma and Chendi Zhou and Jingbo Zhou and Yang Zhou and Huaiyu Dai and Dejing Dou},
  doi          = {10.1109/TPDS.2022.3224941},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {535-551},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-job intelligent scheduling with cross-device federated learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive performance model of sparse matrix-vector
multiplication to guide kernel optimization. <em>TPDS</em>,
<em>34</em>(2), 519–534. (<a
href="https://doi.org/10.1109/TPDS.2022.3225230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Matrix-Vector Multiplication (SpMV) is important in scientific and industrial applications and remains a well-known challenge for modern CPUs due to high sparsity and irregularity. Many researchers try to improve SpMV performance by designing dedicated data formats and computation patterns. However, out-of-order superscalar CPUs have complex micro-architectures where exist complicated interactions and restrictions among software and hardware factors. It is hard to systematically study the effectiveness of optimization methods on the overall performance, as its benefits may be undermined by other factors. In this paper, we thoroughly study the execution of SpMV on modern CPUs and propose a comprehensive performance model to reveal the critical factors and their relationships. Specifically, we first study the coding characteristics of SpMV kernels to identify key factors worthy of attention. Then we model the execution of SpMV as two overlapped parts: CPU pipeline and memory latency. Both are carefully modeled with related hardware and software factors. We also model SIMD performance with the usage of specific SIMD instructions and vector registers. Experiments show that our model matches the actual execution of real-world processors. Guided by the model, we propose SpV8, a novel SpMV kernel that optimizes critical factors to improve computation efficiency and memory bandwidth. Experiments on Intel/AMD x86 and ARM AArch64 platforms show that SpV8 outperforms several state-of-the-art approaches with large margins, achieving average $3.4\times$ over Intel Math Kernel Library and $1.4\times$ over the best existing approach. Such results indicate that the proposed model is capable of valuable guidance for efficient SpMV optimizations.},
  archive      = {J_TPDS},
  author       = {Tian Xia and Gelin Fu and Chenyang Li and Zhongpei Luo and Lucheng Zhang and Ruiyang Chen and Wenzhe Zhao and Nanning Zheng and Pengju Ren},
  doi          = {10.1109/TPDS.2022.3225230},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {519-534},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A comprehensive performance model of sparse matrix-vector multiplication to guide kernel optimization},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RLTiering: A cost-driven auto-tiering system for two-tier
cloud storage using deep reinforcement learning. <em>TPDS</em>,
<em>34</em>(2), 501–518. (<a
href="https://doi.org/10.1109/TPDS.2022.3224865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud storage boom has prompted providers to offer two storage tiers, i.e., hot and cold tiers, which are respectively purpose-built to provide the lowest cost for frequent and infrequent access patterns. However, for cloud users, it is non-trivial to determine cost-effective tiers because it is hard to obtain future access patterns in advance and is difficult to predict them exactly. The lack of future information poses a risk of increasing costs instead of saving costs. This is not the only challenge encountered when it comes to cost optimization. In this article, we take Amazon S3 as an example to analyze the pricing of two-tier cloud storage and derive several major challenges faced by cost optimization. Then, assuming a priori knowledge of future access patterns, we propose an optimal offline algorithm based on dynamic programming to determine cost-effective tiers for each time slot. Further, to handle online workload arrivals, we formulate the problem using Markov decision processes and propose RLTiering based on deep reinforcement learning. Eventually, the cost performance of RLTiering is evaluated based on real-world traces and prevalent Amazon S3 pricing, and the results show that it achieves significant cost-savings.},
  archive      = {J_TPDS},
  author       = {Mingyu Liu and Li Pan and Shijun Liu},
  doi          = {10.1109/TPDS.2022.3224865},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {501-518},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RLTiering: A cost-driven auto-tiering system for two-tier cloud storage using deep reinforcement learning},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the design and evaluation of an optimal security-and-time
cognizant data placement for dynamic fog environments. <em>TPDS</em>,
<em>34</em>(2), 489–500. (<a
href="https://doi.org/10.1109/TPDS.2022.3223796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog Computing usefully extends Cloud to the edge of the network for the sake of meeting users’ expanding demand for low latency. However, due to its scattered distribution and open architecture, fog nodes are highly vulnerable to security threats, resulting in an inevitable sharp conflict between quick response time and high data security. This conflict motivates the need for effective data placement among fog nodes towards a trade-off between security and time. Existing studies merely offer independent solutions by considering either security or response time. By contrast, we establish a dynamic multi-objective optimization model in this article by optimizing security and response time simultaneously. With this model, we propose an efficient evolutionary algorithm, referred to as Dynamic Interactive Security-and-Time cognizant algorithm ( DIST ), to obtain optimal data placement strategies under Fog environments. To improve efficiency, DIST allows users to gradually incorporate their preference information into the search process so as to find their most preferred solutions without exploring the whole search space. We demonstrate the superiority of DIST by rigorous comparison with the most state-of-art data placement strategy and other well-applied strategies. Experimental results manifest that DIST outperforms other strategies in obtaining solutions with higher data security and shorter response time. Furthermore, DIST is capable of efficiently and continuously tracking the Pareto optimal solution under dynamically changing Fog environments while other existing strategies cannot.},
  archive      = {J_TPDS},
  author       = {Xiaoli Wang and Bharadwaj Veeravalli and Jiaming Song and Honghu Liu},
  doi          = {10.1109/TPDS.2022.3223796},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {489-500},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On the design and evaluation of an optimal security-and-time cognizant data placement for dynamic fog environments},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating deep learning inference via model parallelism
and partial computation offloading. <em>TPDS</em>, <em>34</em>(2),
475–488. (<a href="https://doi.org/10.1109/TPDS.2022.3222509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Internet-of-Things (IoT) and the explosive advance of deep learning, there is an urgent need to enable deep learning inference on IoT devices in Mobile Edge Computing (MEC). To address the computation limitation of IoT devices in processing complex Deep Neural Networks (DNNs), computation offloading is proposed as a promising approach. Recently, partial computation offloading is developed to dynamically adjust task assignment strategy in different channel conditions for better performance. In this paper, we take advantage of intrinsic DNN computation characteristics and propose a novel Fused-Layer-based (FL-based) DNN model parallelism method to accelerate inference. The key idea is that a DNN layer can be converted to several smaller layers in order to increase partial computation offloading flexibility, and thus further create the better computation offloading solution. However, there is a trade-off between computation offloading flexibility as well as model parallelism overhead. Then, we investigate the optimal DNN model parallelism and the corresponding scheduling and offloading strategies in partial computation offloading. In particular, we propose a Particle Swarm Optimization with Minimizing Waiting (PSOMW) method, which explores and updates the FL strategy, path scheduling strategy, and path offloading strategy to reduce time complexity and avoid invalid solutions. Finally, we validate the effectiveness of the proposed method in commonly used DNNs. The results show that the proposed method can reduce the DNN inference time by an average of 12.75 times compared to the legacy No FL (NFL) algorithm, and is very close to the optimal solution achieved by the Brute Force (BF) algorithm with the difference of less than 0.04\%.},
  archive      = {J_TPDS},
  author       = {Huan Zhou and Mingze Li and Ning Wang and Geyong Min and Jie Wu},
  doi          = {10.1109/TPDS.2022.3222509},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {475-488},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating deep learning inference via model parallelism and partial computation offloading},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DRONE: An efficient distributed subgraph-centric framework
for processing large-scale power-law graphs. <em>TPDS</em>,
<em>34</em>(2), 463–474. (<a
href="https://doi.org/10.1109/TPDS.2022.3223068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the ever-increasing volume of graph-structured data such as social networks, graph databases and knowledge graphs requires to be processed efficiently and scalably. These natural graphs commonly found in the real world have highly skewed power-law degree distribution and are called power-law graphs. The subgraph-centric programming model is a promising approach applied in many state-of-the-art distributed graph computing frameworks. However, the performance of subgraph-centric frameworks is limited when processing large-scale power-law graphs. When deployed to the subgraph-centric framework, existing graph partitioning algorithms are not suitable for power-law graphs. In this paper, we present a novel distributed graph computing framework, DRONE (Distributed gRaph cOmputiNg Engine), which leverages the subgraph-centric model and the vertex-cut graph partitioning strategy. DRONE also supports the fault tolerance mechanism to accommodate the increasing scale of machines with negligible overhead (6.48\% on average). We further study the execution workflow of DRONE and propose an efficient and balanced graph partition algorithm (EBV) for DRONE. Experiments show that DRONE reduces the running time on real-world graphs by 25.6\%, on average, compared to the state-of-the-art distributed graph computing frameworks. In addition, the EBV graph partition algorithm reduces the replication factor by at least 21.8\% than other self-based partition algorithms. Our results indicate that DRONE has excellent potential in processing large-scale power-law graphs.},
  archive      = {J_TPDS},
  author       = {Shuai Zhang and Zite Jiang and Xingzhong Hou and Mingyu Li and Mengting Yuan and Haihang You},
  doi          = {10.1109/TPDS.2022.3223068},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {463-474},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DRONE: An efficient distributed subgraph-centric framework for processing large-scale power-law graphs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed multicast traffic engineering for multi-domain
software-defined networks. <em>TPDS</em>, <em>34</em>(2), 446–462. (<a
href="https://doi.org/10.1109/TPDS.2022.3205219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research on SDN multicast traffic engineering mainly focused on intra-domain optimization. However, the main traffic on the Internet is inter-domain, and the selection of border nodes and sharing of network information between domains are usually distributed but ignored in previous works. In this article, we explore multi-domain online distributed multicast traffic engineering (MODMTE). To effectively solve MODMTE, we first prove that MODMTE is inapproximable within $|D_{\max }|$ , which indicates that it is impossible to find any algorithm with a ratio better than $|D_{\max }|$ for MODMTE, and $|D_{\max }|$ is the maximum number of destinations for a multicast tree. Then, we design a $|D_{\max }|$ -competitive distributed algorithm with the ideas of Domain Tree, Dual Candidate Forest Construction, and Forest Rerouting to achieve the tightest performance bound for MODMTE. Experiments on a real SDN with YouTube traffic manifest that the proposed distributed algorithm can reduce more than 30\% of the total cost of bandwidth consumption and rule updates for multicast tree rerouting compared with the state-of-the-art algorithms.},
  archive      = {J_TPDS},
  author       = {Sheng-Hao Chiang and Chih-Hang Wang and De-Nian Yang and Wanjiun Liao and Wen-Tsuen Chen},
  doi          = {10.1109/TPDS.2022.3205219},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {446-462},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed multicast traffic engineering for multi-domain software-defined networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed approaches to butterfly analysis on large
dynamic bipartite graphs. <em>TPDS</em>, <em>34</em>(2), 431–445. (<a
href="https://doi.org/10.1109/TPDS.2022.3221821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tip decomposition has a pivotal role in mining cohesive subgraphs in bipartite graphs by computing the tip number of each vertex in accordance with the non-trivial motif butterfly ((2,2)-biclique). It has been a popular research topic with applications in document clustering, spam group detection, and analysis of affiliation networks. In such applications, the graphs are not only large, but they evolve quickly with new edges being continuously added/deleted. While existing centralized techniques could solve the tip decomposition problem for static bipartite graphs, they are not efficient for maintaining the tip numbers of vertices on large-scale graphs. In this paper, we study butterfly analysis problems on bipartite graphs in a distributed environment with the vertex-centric model. We first extend a centralized butterfly counting algorithm to a distributed version, called DBCA. An ingenious message aggregation strategy is designed to reduce massive redundant messaging and avoid the memory overflow problem while processing large-scale graphs. Based on the results of DBCA, we develop a distributed tip decomposition algorithm (DTDA) to get the tip number of each vertex in parallel. Finally, to maintain the tip numbers of vertices efficiently while graphs evolve over time, we explore a distributed tip maintenance algorithm (DTMA) along with a novel task-split strategy. Specifically, for an updated edge (insertion/deletion), several sub-tasks will be generated in line with the topology structure of the original bipartite graph. To our best knowledge, this is the first study to process the butterfly analysis problems in a distributed environment. In addition, comprehensive experiments have been conducted on real-world bipartite graphs. The experiment results demonstrate that our proposed algorithms are efficient and scalable.},
  archive      = {J_TPDS},
  author       = {Tongfeng Weng and Xu Zhou and Kenli Li and Kian-Lee Tan and Keqin Li},
  doi          = {10.1109/TPDS.2022.3221821},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {431-445},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed approaches to butterfly analysis on large dynamic bipartite graphs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial. <em>TPDS</em>, <em>34</em>(2), 429–430. (<a
href="https://doi.org/10.1109/TPDS.2022.3226843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the editorial for this issue of the publication.},
  archive      = {J_TPDS},
  author       = {Manish Parashar},
  doi          = {10.1109/TPDS.2022.3226843},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {429-430},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Editorial},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A utility-based distributed pattern mining algorithm with
reduced shuffle overhead. <em>TPDS</em>, <em>34</em>(1), 416–428. (<a
href="https://doi.org/10.1109/TPDS.2022.3221210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the arrival of the current digital era and the advancement of information transmission technologies, there has been an unprecedented rise in data. Efficient extraction of useful information from the volumes of data has garnered growing interest from academics and the industry. Data mining research focuses on finding utility patterns in large datasets. But the inherent complications like frequent scans, creation of substantial candidate sets, etc. plague the mining process for large datasets. Distributive architecture-based approaches also prove inefficacious due to high communication overhead over iterations. High communication cost over data exchange both locally and remotely further aggravates the situation. We propose a Communication Cost Effective Utility-based Pattern Mining (CEUPM) algorithm based on the Spark framework to address this issue. Spark accelerates iterative scanning by storing scanned datasets in a memory abstraction called resilient distributed datasets (RDD). RDD operations need a redistribution of data among cluster nodes during processing. To minimize the communication cost incurred during the shuffle process, we adopt a search space division strategy based on data parallelism for a fair and effective task allocation across cluster nodes. Communication overhead is incurred during this redistribution or shuffle process while minimizing costs. Experimental results in four real datasets demonstrate that CEUPM considerably reduces shuffling overhead and outperforms other existing methods in terms of memory usage, communication cost, execution time, and scalability.},
  archive      = {J_TPDS},
  author       = {Sunil Kumar and Krishna Kumar Mohbey},
  doi          = {10.1109/TPDS.2022.3221210},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {416-428},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A utility-based distributed pattern mining algorithm with reduced shuffle overhead},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the efficiency of deadlock detection in MPI
programs through trace compression. <em>TPDS</em>, <em>34</em>(1),
400–415. (<a href="https://doi.org/10.1109/TPDS.2022.3218346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a static deadlock analysis for single-path MPI programs. Deadlock is when processes are blocked indefinitely by a circular communication dependency. A single path program is one that does not decode messages for control flow. The analysis records a program execution in the form of a trace and then determines from that trace whether there exists any feasible deadlocking schedules. The primary contribution is the combining of identical consecutive sends or receives into single macro actions. This simplified trace is analyzed for potential deadlock cycles. An abstract machine identifies infeasible cycles, and those not identified by the machine are encoded as satisfiability problems for an SMT solver to resolve. The action combination reduces the complexity of identifying and filtering cycles before needing the costly SMT solver. This article shows the effectiveness of the action combination in experiments on a benchmark suite comparing to traces without action combination and other state-of-the-art deadlock analyses.},
  archive      = {J_TPDS},
  author       = {Yu Huang and Tao Wang and Zihui Yin and Eric Mercer and Benjamin Ogles},
  doi          = {10.1109/TPDS.2022.3218346},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {400-415},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving the efficiency of deadlock detection in MPI programs through trace compression},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A space-efficient fair cache scheme based on machine
learning for NVMe SSDs. <em>TPDS</em>, <em>34</em>(1), 383–399. (<a
href="https://doi.org/10.1109/TPDS.2022.3221410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-volatile memory express (NVMe) solid-state drives (SSDs) have been widely adopted in multi-tenant cloud computing environments or multi-programming systems. The on-board DRAM cache inside NVMe SSDs can efficiently reduce the disk accesses and extend the lifetime of SSDs. Current SSD cache management research either improves cache hit ratio while ignoring fairness, or improves fairness while sacrificing overall performance. In this paper, we present MLCache, a space-efficient shared cache management scheme for NVMe SSDs. By learning the impact of reuse distance on cache allocation, a workload-generic neural network model is built. At runtime, MLCache continuously monitors the reuse distance distribution for the neural network module to obtain space-efficient allocation decisions. MLCache also proposes an efficient parallel writing back strategy based on hit ratio and response time, to improve fairness. Experimental results show MLCache improves the write hit ratio when compared to baseline, and MLCache strongly safeguards the fairness of SSDs with parallel write-back and maintains a low level of degradation.},
  archive      = {J_TPDS},
  author       = {Weiguang Liu and Jinhua Cui and Tiantian Li and Junwei Liu and Laurence T. Yang},
  doi          = {10.1109/TPDS.2022.3221410},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {383-399},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A space-efficient fair cache scheme based on machine learning for NVMe SSDs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Floating point calculation of the cube function on FPGAs.
<em>TPDS</em>, <em>34</em>(1), 372–382. (<a
href="https://doi.org/10.1109/TPDS.2022.3220039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Specialized arithmetic units allow fast and efficient computation of lesser used mathematical functions. The overall impact of those units would be negligible in a general purpose processor, as added circuitry makes chips more complex despite most software would seldom make use of it. On the opposite side, custom computing machines are built for a specific task, and they can always benefit from specialized units if they are available. In this work, floating point architectures are proposed for computing the cube on Intel and Xilinx FPGAs. Those implementations reduce the cost and latency compared to using simple floating point multiplications and squarers.},
  archive      = {J_TPDS},
  author       = {Roberto R. Osorio},
  doi          = {10.1109/TPDS.2022.3220039},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {372-382},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Floating point calculation of the cube function on FPGAs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerated information dissemination for replica selection
in distributed key-value store systems. <em>TPDS</em>, <em>34</em>(1),
358–371. (<a href="https://doi.org/10.1109/TPDS.2022.3221642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed key-value stores, multiple replica servers are always available for each key-value access operation when the eventual consistency model is employed. Accordingly, the completion times of the key-value access operations generated by an end-user request at different servers may be of great difference, especially when the replica servers are heterogeneous and have time-varying performance. Accordingly, the replica selection algorithm is crucial to cut the response time of end-user requests. The main challenge of making replica selection for each light-weighted key-value access operation is to timely know the status of replica serves. Recently, the adaptive replica selection algorithm C3 suggests guiding the replica selection with the piggybacked information of replica server in the returned “value”. Although C3 has good performance, the poor timeliness of feedback information makes a large performance gap between C3 and the ideal replica selection algorithm. To narrow this gap, the Accelerated Information Dissemination (AID) mechanism is proposed in this paper. Specifically, AID removes the bottleneck of information dissemination at the “slow” servers by letting both “client” and “replica server” store the records about the status of replica servers and both “key” and “value” piggyback multiple records. AID is implemented in Cassandra and evaluated by experiments and large scale simulations. The results show AID can significantly improve the timeliness of feedback information, especially when the number of nodes is large. Accordingly, AID helps C3 to greatly reduce the latency.},
  archive      = {J_TPDS},
  author       = {Wanchun Jiang and Yujia Qiu and Yucheng Chen and Fa Ji and Haiming Xie and Xiangqian Zhou and Jialiang Chen and Jiawei Huang and Jianxin Wang and Yan Li},
  doi          = {10.1109/TPDS.2022.3221642},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {358-371},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerated information dissemination for replica selection in distributed key-value store systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robustness analysis and enhancement of deep reinforcement
learning-based schedulers. <em>TPDS</em>, <em>34</em>(1), 346–357. (<a
href="https://doi.org/10.1109/TPDS.2022.3218649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dependency-aware jobs, such as the big data analytic workflows, are commonly executed on the cloud. They are compiled to directed acyclic graphs, with tasks linked in regarding the dependency. The cloud scheduler, which maintains a large number of resources, is responsible to execute tasks in parallel. To resolve the complex dependencies, Deep Reinforcement Learning (DRL) based schedulers are widely applied. However, we find that the DRL-based schedulers are vulnerable to the perturbations in the input jobs and may generate falsified decisions to benefit a particular job while delaying the others. By perturbation, we mean a slight adjustment to the job&#39;s node features or dependencies, while not changing its functionality. In this paper, we first explore the vulnerability of DRL-based schedulers to job perturbations without accessing the information of the DRL models used in the scheduler. We devise the black-box perturbation system, in which, a proxy model is trained to mimic the DRL-based scheduling policy. We show that the high-faith proxy model can help to craft effective perturbations. The DRL-based schedulers can be as high as 60\% likely to be badly affected by the perturbations. Then, we investigate the solution to improve the robustness of DRL-based schedulers to such perturbations. We propose an adversarial training framework to force the neural model to adapt to the perturbation patterns during training so as to eliminate the potential damage during applications. Experiments show that the adversarial-trained scheduler is more robust, reducing the chance of being affected to 3-fold less and the potential bad effects halved.},
  archive      = {J_TPDS},
  author       = {Shaojun Zhang and Chen Wang and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2022.3218649},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {346-357},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Robustness analysis and enhancement of deep reinforcement learning-based schedulers},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HierFedML: Aggregator placement and UE assignment for
hierarchical federated learning in mobile edge computing. <em>TPDS</em>,
<em>34</em>(1), 328–345. (<a
href="https://doi.org/10.1109/TPDS.2022.3218807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a distributed machine learning technique that enables model development on user equipments (UEs) locally, without violating their data privacy requirements. Conventional FL adopts a single parameter server to aggregate local models from UEs, and can suffer from efficiency and reliability issues – especially when multiple users issue concurrent FL requests . Hierarchical FL consisting of a master aggregator and multiple worker aggregators to collectively combine trained local models from UEs is emerging as a solution to efficient and reliable FL. The placement of worker aggregators and assignment of UEs to worker aggregators plays a vital role in minimizing the cost of implementing FL requests in a Mobile Edge Computing (MEC) network. Cost minimization associated with joint worker aggregator placement and UE assignment problem in an MEC network is investigated in this work. An optimization framework for FL and an approximation algorithm with an approximation ratio for a single FL request is proposed. Online worker aggregator placements and UE assignments for dynamic FL request admissions with uncertain neural network models, where FL requests arrive one by one without the knowledge of future arrivals, is also investigated by proposing an online learning algorithm with a bounded regret. The performance of the proposed algorithms is evaluated using both simulations and experiments in a real testbed with its hardware consisting of server edge servers and devices and software built upon an open source hierarchical FedML (HierFedML) environment. Simulation results show that the performance of the proposed algorithms outperform their benchmark counterparts, by reducing the implementation cost by at least 15\% per FL request. Experimental results in the testbed demonstrate the performance gain using the proposed algorithms using real datasets for image identification and text recognition applications.},
  archive      = {J_TPDS},
  author       = {Zichuan Xu and Dapeng Zhao and Weifa Liang and Omer F. Rana and Pan Zhou and Mingchu Li and Wenzheng Xu and Hao Li and Qiufen Xia},
  doi          = {10.1109/TPDS.2022.3218807},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {328-345},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HierFedML: Aggregator placement and UE assignment for hierarchical federated learning in mobile edge computing},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Loop-the-loops: Fragmented learning over networks for
constrained IoT devices. <em>TPDS</em>, <em>34</em>(1), 316–327. (<a
href="https://doi.org/10.1109/TPDS.2022.3220221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose Timed Loop Gears (TLG), as a distributed method for enabling fragmented learning in Resource-Constrained networked IoT edge devices. TLG identifies atomic operations (gears), such as feed-forward and back-propagation, necessary for training Machine Learning (ML) models. Each of these gears executes on a Fog Node (FN) exclusively for each data point at a time rather than the whole dataset in its entirety. Additionally, the networked Edge Devices (EDs) offload the training data to the fog layer using the Message Queuing Telemetry Transport (MQTT) protocol such that the participating FNs subscribe to incoming training data and store them based on topics, simplifying data sharing. TLG enables the FN to then transfer the partially learned weights to the next suitable FN for further training. This looping of weights is repeated across FNs until the training is complete. Through extensive analysis, we observe that, compared to existing distributed ML training approaches, for $n$ devices, TLG reduces the probability of disruption due to device failure by $n^{2}$ times. Implementation results of our fragmented learning method demonstrate that, although TLG negligibly increases the memory consumption of the IoT devices by $0.8\%$ , it reduces CPU usage by almost $90\%$ . The proposed method proves beneficial for developing and hosting ML models, even on constrained IoT devices, in contrast to existing lightweight ML methods.},
  archive      = {J_TPDS},
  author       = {Pallav Kumar Deb and Anandarup Mukherjee and Digvijay Singh and Sudip Misra},
  doi          = {10.1109/TPDS.2022.3220221},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {316-327},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Loop-the-loops: Fragmented learning over networks for constrained IoT devices},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel training of pre-trained models via chunk-based
dynamic memory management. <em>TPDS</em>, <em>34</em>(1), 304–315. (<a
href="https://doi.org/10.1109/TPDS.2022.3219819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pre-trained model (PTM) is revolutionizing Artificial Intelligence (AI) technology. However, the hardware requirement of PTM training is prohibitively high, making it a game for a small proportion of people. Therefore, we proposed PatrickStar system to lower the hardware requirements of PTMs and make them accessible to everyone. PatrickStar uses the CPU-GPU heterogeneous memory space to store the model data. Different from existing works, we organize the model data in memory chunks and dynamically distribute them in the heterogeneous memory. Guided by the runtime memory statistics collected in a warm-up iteration, chunks are orchestrated efficiently in heterogeneous memory and generate lower CPU-GPU data transmission volume and higher bandwidth utilization. Symbiosis with the Zero Redundancy Optimizer, PatrickStar scales to multiple GPUs on multiple nodes. The system can train tasks on bigger models and larger batch sizes, which cannot be accomplished by existing works. Experimental results show that PatrickStar extends model scales 2.27 and 2.5 times of DeepSpeed, and exhibits significantly higher execution speed. PatricStar also successfully runs the 175B GPT3 training task on a 32 GPU cluster. Our code is available at https://github.com/Tencent/PatrickStar .},
  archive      = {J_TPDS},
  author       = {Jiarui Fang and Zilin Zhu and Shenggui Li and Hui Su and Yang Yu and Jie Zhou and Yang You},
  doi          = {10.1109/TPDS.2022.3219819},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {304-315},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel training of pre-trained models via chunk-based dynamic memory management},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Faber: A hardware/SoftWare toolchain for image registration.
<em>TPDS</em>, <em>34</em>(1), 291–303. (<a
href="https://doi.org/10.1109/TPDS.2022.3218898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image registration is a well-defined computation paradigm widely applied to align one or more images to a target image. This paradigm, which builds upon three main components, is particularly compute-intensive and represents many image processing pipelines’ bottlenecks. State-of-the-art solutions leverage hardware acceleration to speed up image registration, but they are usually limited to implementing a single component. We present Faber, an open-source HW/SW CAD toolchain tailored to image registration. The Faber toolchain comprises HW/SW highly-tunable registration components, supports users with different expertise in building custom pipelines, and automates the design process. In this direction, Faber provides both default settings for entry-level users and latency and resource models to guide HW experts in customizing the different components. Finally, Faber achieves from 1.5× to 54× in speedup and from 2× to 177× in energy efficiency against state-of-the-art tools on a Xeon Gold.},
  archive      = {J_TPDS},
  author       = {Eleonora D&#39;Arnese and Davide Conficconi and Emanuele Del Sozzo and Luigi Fusco and Donatella Sciuto and Marco Domenico Santambrogio},
  doi          = {10.1109/TPDS.2022.3218898},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {291-303},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Faber: A Hardware/SoftWare toolchain for image registration},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the scalability of GPU synchronization primitives.
<em>TPDS</em>, <em>34</em>(1), 275–290. (<a
href="https://doi.org/10.1109/TPDS.2022.3218508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General-purpose GPU applications increasingly use synchronization to enforce ordering between many threads accessing shared data. Accordingly, recently there has been a push to establish a common set of GPU synchronization primitives. However, the expressiveness of existing GPU synchronization primitives is limited. In particular the expensive GPU atomics often used to implement fine-grained synchronization make it challenging to implement efficient algorithms. Consequently, as GPU algorithms scale to millions or billions of threads, existing GPU synchronization primitives either scale poorly or suffer from livelock or deadlock issues because of heavy contention between threads accessing shared synchronization objects. We seek to overcome these inefficiencies by designing more efficient, scalable GPU barriers and semaphores. In particular, we show how multi-level sense reversing barriers and priority mechanisms for semaphores can be designed with the GPUs unique processing model in mind to improve performance and scalability of GPU synchronization primitives. Our results show that the proposed designs significantly improve performance compared to state-of-the-art solutions like CUDA Cooperative Groups and optimized CPU-style synchronization algorithms at medium and high contention levels, scale to an order of magnitude more threads, and avoid livelock in these situations unlike prior open source algorithms. Overall, across three modern GPUs the proposed barrier algorithm improves performance by an average of 33\% over a GPU tree barrier algorithm and improves performance by an average of 34\% over CUDA Cooperative Groups for five full-sized benchmarks at high contention levels; the new semaphore algorithm improves performance by an average of 83\% compared to prior GPU semaphores.},
  archive      = {J_TPDS},
  author       = {Preyesh Dalmia and Rohan Mahapatra and Jeremy Intan and Dan Negrut and Matthew D. Sinclair},
  doi          = {10.1109/TPDS.2022.3218508},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {275-290},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving the scalability of GPU synchronization primitives},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tag-sharer-fusion directory: A scalable coherence directory
with flexible entry formats. <em>TPDS</em>, <em>34</em>(1), 262–274. (<a
href="https://doi.org/10.1109/TPDS.2022.3217956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale chip multiprocessors (CMPs), the scalability of a coherence directory becomes more important as the number of cores increases. However, previously proposed scalable coherence directories typically reduce the directory storage overhead at the cost of one or more aspects of performance, accuracy, and complexity. In this article, we propose the tag-sharer-fusion (TSF) directory, a scalable coherence directory with low hardware complexity, as well as with high performance and accuracy. Each directory entry has just enough bits to store a single sharer pointer and is divided into two primary formats: tag and sharer , where sharer entries store sharers but not tags. Each private block is tracked by a tag entry, and each shared block is tracked by a combination of a tag entry and a sharer entry in the same set. Simulation of a 128-core chip-multiprocessor with the PARSEC and SPLASH-2x benchmarks shows that the TSF directory requires only a quarter of the area of a non-scalable full-map sparse directory to achieve similar performance and network traffic, both with an average overhead within 1\%. The TSF directory outperforms the state-of-the-art Pool and way-combining directory proposals in terms of storage overhead, performance, and network traffic.},
  archive      = {J_TPDS},
  author       = {Yudi Qiu and Jie Jiao and Xiaoyang Zeng and Yibo Fan},
  doi          = {10.1109/TPDS.2022.3217956},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {262-274},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Tag-sharer-fusion directory: A scalable coherence directory with flexible entry formats},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dissecting tensor cores via microbenchmarks: Latency,
throughput and numeric behaviors. <em>TPDS</em>, <em>34</em>(1),
246–261. (<a href="https://doi.org/10.1109/TPDS.2022.3217824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor Cores have been an important unit to accelerate Fused Matrix Multiplication Accumulation (MMA) in all NVIDIA GPUs since Volta Architecture. To program Tensor Cores, users have to use either legacy wmma APIs or current mma APIs. Legacy wmma APIs are more easy-to-use but can only exploit limited features and power of Tensor Cores. Specifically, wmma APIs support fewer operand shapes and can not leverage the new sparse matrix multiplication feature of the newest Ampere Tensor Cores. However, the performance of current programming interface has not been well explored. Furthermore, the computation numeric behaviors of low-precision floating points (TF32, BF16, and FP16) supported by the newest Ampere Tensor Cores are also mysterious. In this paper, we explore the throughput and latency of current programming APIs. We also intuitively study the numeric behaviors of Tensor Cores MMA and profile the intermediate operations including multiplication, addition of inner product, and accumulation. All codes used in this work can be found in https://github.com/sunlex0717/DissectingTensorCores .},
  archive      = {J_TPDS},
  author       = {Wei Sun and Ang Li and Tong Geng and Sander Stuijk and Henk Corporaal},
  doi          = {10.1109/TPDS.2022.3217824},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {246-261},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Dissecting tensor cores via microbenchmarks: Latency, throughput and numeric behaviors},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to schedule multi-server jobs with fluctuated
processing speeds. <em>TPDS</em>, <em>34</em>(1), 234–245. (<a
href="https://doi.org/10.1109/TPDS.2022.3215947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-server jobs are imperative in modern cloud computing systems. A noteworthy feature of multi-server jobs is that, they usually request multiple computing devices simultaneously for their execution. How to schedule multi-server jobs online with a high system efficiency is a topic of great concern. First, the scheduling decisions have to satisfy the service locality constraints. Second, the scheduling decisions needs to be made online without the knowledge of future job arrivals. Third, and most importantly, the actual service rate experienced by a job is usually in fluctuation because of the dynamic voltage and frequency scaling (DVFS) and power oversubscription techniques when multiple types of jobs co-locate. A majority of online algorithms with theoretical performance guarantees are proposed. However, most of them require the processing speeds to be knowable, thereby the job completion times can be exactly calculated. To present a theoretically guaranteed online scheduling algorithm for multi-server jobs without knowing actual processing speeds apriori, in this article, we propose Esdp (Efficient Sampling-based Dynamic Programming), which learns the distribution of the fluctuated processing speeds over time and simultaneously seeks to maximize the cumulative overall utility. The cumulative overall utility is formulated as the sum of the utilities of successfully serving each multi-server job minus the penalty on the operating, maintaining, and energy cost. Esdp is proved to have a polynomial complexity and a logarithmic regret, which is a State-of-the-Art result. We also validate it with extensive simulations and the results show that the proposed algorithm outperforms several benchmark policies with improvements by up to 73\%, 36\%, and 28\%, respectively.},
  archive      = {J_TPDS},
  author       = {Hailiang Zhao and Shuiguang Deng and Feiyi Chen and Jianwei Yin and Schahram Dustdar and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2022.3215947},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {234-245},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Learning to schedule multi-server jobs with fluctuated processing speeds},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The high faulty tolerant capability of the alternating group
graphs. <em>TPDS</em>, <em>34</em>(1), 225–233. (<a
href="https://doi.org/10.1109/TPDS.2022.3217415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The matroidal connectivity and conditional matroidal connectivity are novel indicators to measure the real faulty tolerability. In this paper, for the $n$ -dimensional alternating group graph $AG_{n}$ , the structure properties and (conditional) matroidal connectivity are studied based on the dimensional partition of $E(AG_{n})$ . We prove that for $S\subseteq E(AG_{n})$ under some limitation on the number of faulty edges in each dimensional edge set, if $|S|\leq (n-1)!-1$ , then $AG_{n}-S$ is connected. We study the value of matroidal connectivity and conditional matroidal connectivity of $AG_{n}$ . Furthermore, simulations have been carried out to compare the matroidal connectivity with other types of conditional connectivity in $AG_{n}$ . The simulation result shows that the matroidal connectivity significantly improves these known fault-tolerant capability of alternating group graphs.},
  archive      = {J_TPDS},
  author       = {Hui Zhang and Rong-Xia Hao and Xiao-Wen Qin and Cheng-Kuan Lin and Sun-Yuan Hsieh},
  doi          = {10.1109/TPDS.2022.3217415},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {225-233},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The high faulty tolerant capability of the alternating group graphs},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Replicated versioned data structures for wide-area
distributed systems. <em>TPDS</em>, <em>34</em>(1), 207–224. (<a
href="https://doi.org/10.1109/TPDS.2022.3217969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate the integration of replicated versioned data structures and append-only distributed storage systems. Doing so facilitates high availability and scalability while providing developer access to different versions of program data structures across program executions. Modern distributed systems such as the Internet of Things (IoT) often employ multi-tiered (cloud/edge/sensors) architectures consisting of a wide array of heterogeneous devices generating data frequently. Hence system availability is imperative to avoid data loss, while scalability is required for the efficient operation of the system not only within the same tier but across different tiers as well. Our proposed approach replicates, persists, and versions program data structures such as binary search trees and linked lists for use in distributed IoT applications. The versioning and persistence of these structures aid failure recovery and facilitate system debugging from its inception instead of making such considerations an afterthought. Moreover, our experiments suggest versioned data structures can perform better in applications performing high volumes of temporal queries versus traditional methods of persisting data (e.g., in a database). We empirically evaluate the overheads associated with versioning and storage persistence of program data structures, present experimental results for multiple end-to-end applications, and demonstrate the scalability of this approach.},
  archive      = {J_TPDS},
  author       = {Nazmus Saquib and Chandra Krintz and Rich Wolski},
  doi          = {10.1109/TPDS.2022.3217969},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {207-224},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Replicated versioned data structures for wide-area distributed systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel compute-efficient tridiagonal solver for many-core
architectures. <em>TPDS</em>, <em>34</em>(1), 195–206. (<a
href="https://doi.org/10.1109/TPDS.2022.3214762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tridiagonal solver is an important kernel and is widely supported in mainstream numerical libraries. While parallel algorithms have been studied for many-core architectures, the performance of current algorithms and implementations is still hindered by input size sensitivity and cross-platform portability. In this paper, we propose a novel algorithm WM-pGE for the batched solution of diagonally dominant tridiagonal systems. The algorithm balances the key design objectives, including computation complexity, memory complexity, parallelism, and input size sensitivity, better than existing algorithms. Moreover, an elegant formulation is presented to show the implementation and cross-platform optimization without loss of efficiency and generality, by extracting the platform-dependent works into only four vector operators. The results from our batched tridiagonal experiments show that the proposed algorithm outperforms the prior work PCR-pThomas by 25\% and 12\% on NVIDIA Tesla V100 in single and double precision, respectively. On Intel KNL, our method achieves a 10\% improvement in performance over PCR-pThomas in double precision.},
  archive      = {J_TPDS},
  author       = {Kan Liu and Wei Xue},
  doi          = {10.1109/TPDS.2022.3214762},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {195-206},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A novel compute-efficient tridiagonal solver for many-core architectures},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine-grained performance and cost modeling and optimization
for FaaS applications. <em>TPDS</em>, <em>34</em>(1), 180–194. (<a
href="https://doi.org/10.1109/TPDS.2022.3214783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function-as-a-Service (FaaS) has become a mainstream cloud computing paradigm for developers to build cloud-native applications in recent years. By taking advantage of serverless architecture, FaaS applications bring many desirable benefits, including built-in scalability, high availability, and improved cost-effectiveness. However, predictability and trade-off of performance and cost are still key pitfalls for FaaS applications due to poor infrastructure transparency and lack of performance and cost models that fit the new paradigm. In this study, we therefore fill this gap by proposing formal performance and cost modeling and optimization algorithms, which enable accurate prediction and fine-grained control over the performance and cost of FaaS applications. The proposed model and algorithms provide better predictability and trade-off of performance and cost for FaaS applications, which help developers to make informed decisions on cost reduction, performance improvement, and configuration optimization. We validate the proposed model and algorithms via extensive experiments on AWS. We show that the modeling algorithms can accurately estimate critical metrics, including response time, cost, exit status, and their distributions, regardless of the complexity and scale of the application workflow. Also, the depth-first bottleneck alleviation algorithm for trade-off analysis can effectively solve two optimization problems with fine-grained constraints.},
  archive      = {J_TPDS},
  author       = {Changyuan Lin and Nima Mahmoudi and Caixiang Fan and Hamzeh Khazaei},
  doi          = {10.1109/TPDS.2022.3214783},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {180-194},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fine-grained performance and cost modeling and optimization for FaaS applications},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MRCN: Throughput-oriented multicast routing for customized
network-on-chips. <em>TPDS</em>, <em>34</em>(1), 163–179. (<a
href="https://doi.org/10.1109/TPDS.2022.3217296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relentless proliferation of Big Data and artificial intelligence has compelled computing platform architectures to evolve into heterogeneous multicores for greater energy efficiency. A customized network-on-chip (NoC) supporting interconnection diversity is pivotal for the asymmetric data-access traffic requirements of modern heterogeneous multicore system-on-chip (SoC). A significant portion of on-chip data access comprises single-source multi-destination (SSMD) traffic, which supports barrier synchronization, multi-threading, cache coherency protocols, and deep neural network (DNN) acceleration. By amortizing SSMD traffic, multicast routing is essential for effectively utilizing communication bandwidth. One of the primary concerns in supporting multicast routing in NoCs is to circumvent the additional deadlock conditions caused by branch operations among the active routers. However, it is challenging to implement the throughput-optimized multicast routing in irregular topology-based NoCs because the deadlock conditions become highly complicated, and the Hamiltonian path required to apply the labeling rule may not exist. Two important observations were identified regarding multicast routing in customized NoCs: 1) Even if the NoC lacks a Hamiltonian path, deadlock-freedom can be guaranteed by restricting branch operations to a specific destination. 2) A variable path diversity in a custom topology can be leveraged in routing path allocation and branch. Based on these properties, this study proposes a deadlock-free and throughput-enhanced multicast routing for customized NoC (MRCN). MRCN ensures deadlock freedom by utilizing extended routing and router labeling rules. Furthermore, destination router partitioning and traffic-aware adaptive branching are incorporated to reduce packet routing hops and disperse channel traffic. The effectiveness of MRCN was verified using Noxim, a well-known cycle-accurate NoC simulator, under various topologies and traffic patterns. The simulation revealed that MRCN improved the average latency by 13.98\% and the throughput by 12.16\% under the saturated traffic conditions over the previous multicast routings in customized NoCs.},
  archive      = {J_TPDS},
  author       = {Young Sik Lee and Yong Wook Kim and Tae Hee Han},
  doi          = {10.1109/TPDS.2022.3217296},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {163-179},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MRCN: Throughput-oriented multicast routing for customized network-on-chips},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CIA: A collaborative integrity auditing scheme for cloud
data with multi-replica on multi-cloud storage providers. <em>TPDS</em>,
<em>34</em>(1), 154–162. (<a
href="https://doi.org/10.1109/TPDS.2022.3216614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of cloud storage has solved many pain points of the traditional storage model. However, the issue of cloud storage data integrity - whether the cloud storage provider has kept the data intact - has raised concerns about cloud storage. Integrity auditing of cloud storage data allows users to know the integrity of the outsourced data without downloading it in its entirety. However it is not enough to know its integrity. Multi-replicas, as a common means of redundancy, improves the reliability of cloud storage data. And storing multi-replicas on multi-cloud storage providers (CSPs) enhances this feature. In a multi-replica multi-CSPs scenario, if the independence of CSPs is given full play and auditing is performed among CSPs, not only the introduction of third party auditor (TPA) can be eliminated, but also the tag generation, which is a huge computational overhead, can be removed. Inspired by this, in this paper we propose a new model for remote data integrity auditing: CIA (Collaborative Integrity Auditing). In addition to the reduction in computational overhead, the proposed scheme provides unprecedented support for free data blocking. The proposed scheme employs only hash functions in the calculation, which has a negligible computational overhead compared to the traditional bilinear pairing-based schemes. Theoretical analysis and experimental results show that the proposed scheme provides high efficiency and flexibility with security assurance, and can be used as a lightweight alternative to traditional remote data integrity auditing schemes in multi-replica multi-CSP scenarios.},
  archive      = {J_TPDS},
  author       = {Tengfei Li and Jianfeng Chu and Liang Hu},
  doi          = {10.1109/TPDS.2022.3216614},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {154-162},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CIA: A collaborative integrity auditing scheme for cloud data with multi-replica on multi-cloud storage providers},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SwMPAS-a: Scaling MPAS-a to 39 million heterogeneous cores
on the new generation sunway supercomputer. <em>TPDS</em>,
<em>34</em>(1), 141–153. (<a
href="https://doi.org/10.1109/TPDS.2022.3215002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the computing power of High-Performance Computing (HPC) systems having stepped into the exascale era, more complex problems can be solved with scientific applications on a large scale. However, due to the significant performance gap between computing nodes and storage subsystems, suboptimal design for the Input/Output (I/O) module will significantly impede the efficiency of scientific applications, especially for the ubiquitous atmosphere applications. Two-phase I/O implemented in N-to-1 mode creates a serious bottleneck that hinders the scalability for the Model for Prediction Across Scales-Atmosphere (MPAS-A) on the new generation Sunway supercomputer. To address the I/O problem, we apply a custom data reorganization method to enable N-to-M I/O mode to exploit the parallel file system&#39;s performance and limit the data transfer among MPI ranks to a restricted scope to alleviate communication overhead. Moreover, we have conducted several methods to accelerate the computations, including the redesign for tracer transport, a hybrid buffering scheme, and a three-level parallelization scheme, which allows MPAS-A to use all heterogeneous computing resources efficiently. Experimental results show admirable scalability and efficiency of our I/O method, which achieves speedups of 41× and 58.9× for input and output compared with the raw I/O method on 30,000 MPI ranks. By scaling MPAS-A to 39 million heterogeneous cores, we demonstrate the necessity of a well-constructed I/O module for a real-world atmosphere application. Speed tests show that our optimization methods obtain good results for computations, and MPAS-A achieves a speed of 0.82 Simulated Day per Hour (SDPH) and 0.76 parallel efficiency of strong scaling with 600,000 MPI ranks.},
  archive      = {J_TPDS},
  author       = {Xiaoyu Hao and Tao Fang and Junshi Chen and Jun Gu and Jiawang Feng and Hong An and Chun Zhao},
  doi          = {10.1109/TPDS.2022.3215002},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {141-153},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SwMPAS-A: Scaling MPAS-A to 39 million heterogeneous cores on the new generation sunway supercomputer},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Near-lossless MPI tracing and proxy application
autogeneration. <em>TPDS</em>, <em>34</em>(1), 123–140. (<a
href="https://doi.org/10.1109/TPDS.2022.3215942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traces of MPI communications are used by many performance analysis and visualization tools. Storing exhaustive traces of large-scale MPI applications is infeasible, however, because of their large volume. Aggregated or lossy MPI traces are smaller but provide much less information. In this paper we present Pilgrim, a near-lossless MPI tracing tool that, by using sophisticated compression techniques, generates small trace files at large scales and incurs only moderate overheads. We perform comprehensive studies of various compression techniques used for storing timestamps associated with each call. This timing information is essential for analysis purposes such as skews study. To demonstrate the usefulness of the detailed information stored by Pilgrim, we present a proxy application generator that can generate proxy apps that preserve original communication patterns from the Pilgrim traces.},
  archive      = {J_TPDS},
  author       = {Chen Wang and Yanfei Guo and Pavan Balaji and Marc Snir},
  doi          = {10.1109/TPDS.2022.3215942},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {123-140},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Near-lossless MPI tracing and proxy application autogeneration},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sentinels and twins: Effective integrity assessment for
distributed computation. <em>TPDS</em>, <em>34</em>(1), 108–122. (<a
href="https://doi.org/10.1109/TPDS.2022.3215863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed computing supports large scale and data-intensive computations with the cooperation of a multitude of parties, each responsible for a portion of the workload. Such parties are often not fully reliable and may return incorrect results. In this article, we address the problem of assessing the integrity of the computation results. We provide a comprehensive characterization of two techniques, sentinels and twins , evaluating their effectiveness and synergy. Sentinels are pre-computed tasks whose result is known apriori, and enable checking returned results against a ground truth. Twins are replicated tasks assigned to different workers, and enable cross-checking returned results for a same task. The analysis considers many questions that arise in the design of a concrete integrity assessment strategy and identifies the parameters that have a critical impact on the overall protection. Our model enables to tune the integrity controls so to achieve best effectiveness. The model can be applied to a variety of scenarios and offers guidelines that can find extensive application.},
  archive      = {J_TPDS},
  author       = {Sabrina De Capitani di Vimercati and Sara Foresti and Sushil Jajodia and Stefano Paraboschi and Pierangela Samarati and Roberto Sassi},
  doi          = {10.1109/TPDS.2022.3215863},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {108-122},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Sentinels and twins: Effective integrity assessment for distributed computation},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AESM2 attribute-based encrypted search for multi-owner and
multi-user distributed systems. <em>TPDS</em>, <em>34</em>(1), 92–107.
(<a href="https://doi.org/10.1109/TPDS.2022.3216320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of cloud computing, it is popular for data owners to outsource massive data to the cloud server for data sharing. To protect the privacy of sensitive data, many searchable encryption schemes are proposed. However, most of the existing studies focus on the single-owner model. In practice, users need to query data from distributed owners one by one, which inevitably brings great communication and computation overheads. Moreover, it lacks a secure scheme that realizes the access control requirements of individual owners. In this article, we propose AESM $^{2}$ , a new attribute-based encrypted search with ownership enhancement scheme for multi-owner and multi-user distributed systems. Our design enables users to search data from authorized owners with only one trapdoor. Owners can enforce owner level permission on users and encrypt their data individually with fine-grained attribute level permission. For practical consideration, we further devise an efficient revocation method of the owner level permission for users, where ciphertexts do not need to be updated. We formally define and prove the security of our design. Moreover, we implement a system prototype and analyze the performance from theoretical and experimental aspects. The evaluation results demonstrate that our scheme is effective and efficient.},
  archive      = {J_TPDS},
  author       = {Mingyue Wang and Yinbin Miao and Yu Guo and Hejiao Huang and Cong Wang and Xiaohua Jia},
  doi          = {10.1109/TPDS.2022.3216320},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {92-107},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AESM2 attribute-based encrypted search for multi-owner and multi-user distributed systems},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A distributed network-based runtime verification of full
regular temporal properties. <em>TPDS</em>, <em>34</em>(1), 76–91. (<a
href="https://doi.org/10.1109/TPDS.2022.3215854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a lightweight method, runtime verification aims to check whether one program execution satisfies a desired property. For online runtime verification, the approach efficiency and property expressiveness are two key points restricting its wide application. In this paper, we propose a distributed network-based parallel runtime verification approach to verifying full regular temporal properties for a suitable subset of C (named by Xd-C) programs in an online manner. With this approach, an Xd-C program is translated into an equivalent Modeling, Simulation and Verification Language (MSVL) program, and a desired property is specified as a Propositional Projection Temporal Logic (PPTL) formula; during the program execution, segments of the generated state sequence are verified in parallel by distributed multi-core machines. Experimental results show that, our approach has a speedup of 2.5X-5.0X over the state-of-art runtime verification approaches and supports full regular temporal properties, meaning that our approach can not only take full advantage of computing and storage resources in a distributed network, but also support more expressive properties.},
  archive      = {J_TPDS},
  author       = {Bin Yu and Cong Tian and Xu Lu and Nan Zhang and Zhenhua Duan},
  doi          = {10.1109/TPDS.2022.3215854},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {76-91},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A distributed network-based runtime verification of full regular temporal properties},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Swing: Providing long-range lossless RDMA via PFC-relay.
<em>TPDS</em>, <em>34</em>(1), 63–75. (<a
href="https://doi.org/10.1109/TPDS.2022.3215517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote Direct Memory Access (RDMA) has been widely deployed in datacenters for its high performance. Large-scale high performance cloud services built on geographically distributed datacenters require long-range RDMA for performance requirements. However, existing RDMA solutions can hardly satisfy the stringent requirements of the emerging large-scale high-performance cloud services built on geo-distributed datacenters in terms of throughput and delay. On the one hand, lossless RDMA suffers from a deep buffer and potential suboptimal throughput for inter-datacenter traffic due to delayed response to Priority Flow Control (PFC) messages. On the other hand, lossy RDMA with selective retransmissions suffers from poor performance when multiple flows with different round-trip times (RTTs) coexist in cross-datacenter scenarios. This article proposes Swing , which expands the high-performance lossless RDMA to long-distance links through PFC-Relay. Swing ensures the throughput of long-distance links while minimizing the buffer requirement for long-range RDMA. It enables long-range RDMA without making any modifications to existing in-datacenter networks. The evaluation shows that Swing can reduce the average flow completion time (FCT) by 14\%-66\% in a variety of traffic scenarios.},
  archive      = {J_TPDS},
  author       = {Yanqing Chen and Chen Tian and Jiaqing Dong and Song Feng and Xu Zhang and Chang Liu and Peiwen Yu and Nai Xia and Wanchun Dou and Guihai Chen},
  doi          = {10.1109/TPDS.2022.3215517},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {63-75},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Swing: Providing long-range lossless RDMA via PFC-relay},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tripartite graph aided tensor completion for sparse network
measurement. <em>TPDS</em>, <em>34</em>(1), 48–62. (<a
href="https://doi.org/10.1109/TPDS.2022.3213259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network measurements provide critical inputs for a wide range of network management. Existing network-wide monitoring methods face the challenge of incurring a high measurement cost. Some recent studies show that network-wide measurement data such as end-to-end latency and flow traffic, have hidden spatio-temporal correlations and thus low-rank features. Taking advantage of the low-rank feature, enlightened by tensor model&#39;s strong capability of information representation and extracting, this paper studies a novel sparse measurement scheduling problem which selects a proportion of Origin and Destination (OD) pairs to take measurements in the future time slots, while ensuring the data of the remaining un-measured OD pairs be accurately inferred through tensor completion. It is challenging to find the optimal sampling points (OD pairs) without knowing the structure of the future data and also infer the un-measured data in the presence of noise in the measurement samples. To conquer the challenges, we propose several techniques: a tripartite graph to illustrate the relationship between sample locations and tensor factorization, a graph-based sample selection algorithm, and a graph-based robust tensor completion algorithm. We have conducted extensive experiments based on two real network latency monitoring traces (PlanetLab and Harvard) and two other network monitoring traces (including a traffic trace Abilene and a throughput trace WS-Dream). Our results demonstrate that, even with a sampling ratio of less than 5\%, our scheme can accurately obtain the complete network-wide monitoring data by inferring the missing ones based on the samples taken. To achieve similar recovery performance, the best peer tensor completion algorithm needs a significantly larger number of samples, with the sampling ratio up to 25-150 times ours.},
  archive      = {J_TPDS},
  author       = {Xiaocan Li and Kun Xie and Xin Wang and Gaogang Xie and Kenli Li and Jiannong Cao and Dafang Zhang and Jigang Wen},
  doi          = {10.1109/TPDS.2022.3213259},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {48-62},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Tripartite graph aided tensor completion for sparse network measurement},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scheduling parallel real-time tasks on virtual processors.
<em>TPDS</em>, <em>34</em>(1), 33–47. (<a
href="https://doi.org/10.1109/TPDS.2022.3213024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many popular parallel programming models, e.g., OpenMP (OpenMP, 2013), applications are usually dispatched into several dedicated scheduling entities (named ”threads” in common) for which the processor time of physical platform is provided through the OS schedulers. This behavior requires for a hierarchical scheduling framework, considering each thread as a virtual processor (VP). Moreover, hierarchical scheduling allow separate applications to execute together on a common hardware platform, with each application having the “illusion” of executing on a dedicated component. However, the problem for scheduling parallel real-time tasks on virtual multiprocessor platform has not been addressed yet. An analogous approach to virtual scheduling for parallel real-time tasks is federeted scheudling, where each task exclusively executes on a set of dedicated physical processors. However, federated scheduling suffers significant resource wasting. In this article, we study the scheduling of real-time parallel task on virtual multiprocessors. As a physical processor is shared by virtual processors, tasks effectively share processors with each other. We conduct comprehensive performance evaluation to compare our proposed approach with existing methods of different types. Experiment results show that our approach consistently outperforms existing methods to a considerable extent under a wide range of parameter settings.},
  archive      = {J_TPDS},
  author       = {Xu Jiang and Haochun Liang and Nan Guan and Yue Tang and Lei Qiao and Yi Wang},
  doi          = {10.1109/TPDS.2022.3213024},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {33-47},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scheduling parallel real-time tasks on virtual processors},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asynchronous algorithms for decentralized resource
allocation over directed networks. <em>TPDS</em>, <em>34</em>(1), 16–32.
(<a href="https://doi.org/10.1109/TPDS.2022.3212424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider a class of decentralized resource allocation problems over directed networks, where each node only communicates with its in-neighbors and attempts to minimize its own cost when network-wide resource constraints as well as local capacity limits are satisfied. Decentralized optimization to solve this problem has been a significant focus within engineering research due to its advantages in scalability, robustness, and flexibility. Most existing methods are synchronous while few works are devoted to asynchronously solving the problem. The problem becomes even more challenging when the networks are directed. To address the resource allocation problem when the above issues are considered, we propose a novel decentralized asynchronous algorithm based on the gossip-based communication protocol and epigraph strategy. An important feature of the algorithm is that it is implemented in a completely decentralized manner in the case of asynchronous communication and directed networks. We provide theoretical proof to guarantee the convergence of the proposed algorithm, which indicates that it can successfully allocate the optimal resource. When solving the resource allocation problem over time-varying directed networks, we further discuss a related decentralized asynchronous algorithm according to the random sleep protocol. Numerical examples are given to demonstrate the viability and performance of the algorithms.},
  archive      = {J_TPDS},
  author       = {Qingguo Lü and Xiaofeng Liao and Shaojiang Deng and Huaqing Li},
  doi          = {10.1109/TPDS.2022.3212424},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {16-32},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Asynchronous algorithms for decentralized resource allocation over directed networks},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiated consistency for worldwide gossips.
<em>TPDS</em>, <em>34</em>(1), 1–15. (<a
href="https://doi.org/10.1109/TPDS.2022.3209150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eventual consistency is a consistency model that favors liveness over safety. It is often used in large-scale distributed systems where models ensuring a stronger safety incur performance that are too low to be deemed practical. Eventual consistency tends to be uniformly applied within a system, but we argue a demand exists for differentiated eventual consistency, e.g. in blockchain systems. We propose update-query consistency with primaries and secondaries (UPS) to address this demand. UPS is a novel consistency mechanism that works in pair with our novel two-phase epidemic broadcast protocol gossip primary-secondary (GPS) to offer differentiated eventual consistency and delivery speed. We propose two complementary analyses of the broadcast protocol: a continuous analysis and a discrete analysis based on compartmental models used in epidemiology. Additionally, we propose the formal definition of a scalable consistency metric to measure the consistency trade-off at runtime. We evaluate UPS in two simulated worldwide settings: a one-million-node network and a network emulating that of the Ethereum blockchain. In both settings, UPS reduces inconsistencies experienced by a majority of the nodes and reduces the average message latency for the remaining nodes.},
  archive      = {J_TPDS},
  author       = {Davide Frey and Achour Mostefaoui and Matthieu Perrin and Pierre-Louis Roman and François Taïani},
  doi          = {10.1109/TPDS.2022.3209150},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Differentiated consistency for worldwide gossips},
  volume       = {34},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
