<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcc---266">TCC - 266</h2>
<ul>
<li><details>
<summary>
(2023). Comment on “multi-keyword searchable and verifiable
attribute-based encryption over cloud data.” <em>TCC</em>,
<em>11</em>(4), 3797–3798. (<a
href="https://doi.org/10.1109/TCC.2023.3312918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent article Zhang, et al. 2023, the authors presented an efficient and verifiable multi-keyword attribute-based search scheme over cloud data. In this comment, we show that the key equation design in their scheme has errors, leading to the inability to perform effective searches in the case of multi-keyword. Then we propose a correction to address this issue without compromising the original scheme’s security.},
  archive      = {J_TCC},
  author       = {Wan-Peng Guo and Run-Hua Shi and Xiao-Xu Zhang},
  doi          = {10.1109/TCC.2023.3312918},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3797-3798},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Comment on “Multi-keyword searchable and verifiable attribute-based encryption over cloud data”},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Improving LSM-tree based key-value stores with fine-grained
compaction mechanism. <em>TCC</em>, <em>11</em>(4), 3778–3796. (<a
href="https://doi.org/10.1109/TCC.2023.3329646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LSM-tree-based key-value stores (KV stores) render high-performance read/write services to data-intensive applications. KV stores employ an SSTable-based Coarse-Grained Compaction (CGC) mechanism, which involves a huge amount of data that do not need to be updated, thereby bringing a high write amplification (WA) and long tail latency. To address this issue, we propose a Fine-Grained Compaction (FGC) mechanism anchored on a L og- S tructured p atched- M erge tree (LSpM-tree) - a new data organization that averts rewriting irrelevant data into disks amid compaction. A cluster, the basic unit in FGC, encloses several patches and a redirection table, where each patch has an array of KV regions. We devise three compaction modes powered by the LSpM-tree, and we implement a high-performance key-value store, named FGKV. The extensive experiments show that FGKV improves the random-write throughput by up to 121\%, 36.8\%, 38.6\%, and 15.2\% compared with LevelDB, RocksDB, LDC, and ALDC, respectively. FGKV lowers the WA of the alternative KV stores by up to 50\%. FGKV boosts read performance by up to 122\%, 51.4\%, 96.6\%, and 368\%, respectively, and FGKV curbs the 99th percentile latency of LevelDB, RocksDB, LDC, and ALDC by up to 78.2\%, 77.6\%, 78.3\%, and 73.1\% under YCSB A, respectively. Moreover, FGKV is readily extended to the other KV stores.},
  archive      = {J_TCC},
  author       = {Hui Sun and Guanzhong Chen and Yinliang Yue and Xiao Qin},
  doi          = {10.1109/TCC.2023.3329646},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3778-3796},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Improving LSM-tree based key-value stores with fine-grained compaction mechanism},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AoI-aware partial computation offloading in IIoT with edge
computing: A deep reinforcement learning based approach. <em>TCC</em>,
<em>11</em>(4), 3766–3777. (<a
href="https://doi.org/10.1109/TCC.2023.3328614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of the Industrial Internet of Things, a large amount of industrial data that needs to be processed promptly. Edge computing-based computation offloading can well assist industrial devices to process these data and reduce the overall time overhead. However, there are dependencies among tasks and some tasks have high latency requirements, so completing computation offloading while considering the above factors faces important challenges. In this article, we design a computation offloading method based on a directed acyclic graph task model by modeling task dependencies. In addition to considering traditional optimization objectives in previous computation offloading problems (e.g., latency, energy consumption, etc.), we also propose an age of information (AoI) model to reflect the freshness of information and transform the task offloading problem into an optimization problem for latency, energy consumption, and AoI. To address this issue, we propose a method based on an improved dueling double deep Q-network computation offloading algorithm, named ID3CO. Specifically, it combines the advantages of deep Q-network, double deep Q-network, and dueling deep Q-network algorithms while further utilizing deep residual neural networks to improve convergence. Extensive simulations are conducted to demonstrate that ID3CO outperforms the existing baselines in terms of performance.},
  archive      = {J_TCC},
  author       = {Kai Peng and Peiyun Xiao and Shangguang Wang and Victor C. M. Leung},
  doi          = {10.1109/TCC.2023.3328614},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3766-3777},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {AoI-aware partial computation offloading in IIoT with edge computing: A deep reinforcement learning based approach},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hardware-assisted static and runtime attestation for cloud
deployments. <em>TCC</em>, <em>11</em>(4), 3750–3765. (<a
href="https://doi.org/10.1109/TCC.2023.3327290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is devoted to the problems of static and runtime integrity for cloud deployments. Existing remote attestation solutions for cloud infrastructure do not cover static and dynamic attestation as a whole. They evaluate either the static or dynamic part, not considering the rest. We address this gap by proposing a runtime attestation process based on hardware CET technology, as an enhancement to static attestation enabled by SGX. We show how hardware-assisted protection for control-flow-related attacks can enhance virtual deployment security with minimal tradeoff. Our solution does not significantly increase the processing time. Moreover, a processing time can even be reduced when this mechanism is used as a default protection method against control-flow related attacks.},
  archive      = {J_TCC},
  author       = {Michał Kucab and Piotr Boryło and Piotr Chołda},
  doi          = {10.1109/TCC.2023.3327290},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3750-3765},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Hardware-assisted static and runtime attestation for cloud deployments},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Verifiable cloud-based data publish-subscribe service with
hidden access policy. <em>TCC</em>, <em>11</em>(4), 3737–3749. (<a
href="https://doi.org/10.1109/TCC.2023.3326339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-based publish-subscribe (pub-sub) services provide a decoupling method for publishers and subscribers to effectively exchange targeted information and massive data on the cloud platform. Data publishers implement fine-grained access control to set subscription privileges for outsourced data through an access policy. However, in the context of semi-honest cloud platforms, the publisher&#39;s access policy may be collected, and incomplete or incorrect subscription results may be returned (e.g., to save communication costs). Existing solutions pay little attention to protecting the data publisher&#39;s access policy and cannot provide efficient verification for local results. In this article, we propose a verifiable multi-keyword data publish-subscribe scheme with a hidden access policy (VMP/S). Specifically, VMP/S combines attribute-based keyword search and data aggregation technology to achieve secure fine-grained access control, thereby protecting the privacy of the access policy. Additionally, the scheme provides an effective method for verifying local results by using equal-length verification information to confirm the correctness of feedback subscription data. Furthermore, we introduce a novel verification method for access control to enhance subscription performance efficiency. We demonstrate that VMP/S achieves IND-CKA security and ensures the privacy of the access policy through a comprehensive security analysis. Through experimental simulations, we confirm its effectiveness.},
  archive      = {J_TCC},
  author       = {Chunlin Li and Jinguo Li and Kai Zhang and Yan Yan and Jianting Ning},
  doi          = {10.1109/TCC.2023.3326339},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3737-3749},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Verifiable cloud-based data publish-subscribe service with hidden access policy},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning scheduling policies for co-located workloads in
cloud datacenters. <em>TCC</em>, <em>11</em>(4), 3725–3736. (<a
href="https://doi.org/10.1109/TCC.2023.3319383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-location, which deploys long running applications and batch-processing applications in the same computing cluster, has become a promising way to improve resource utility for large cloud datacenters. However, co-location brings huge challenges to task scheduling because different types of workloads may affect each other. Existing works on task scheduling rarely focus on the scenario of co-location. This article presents Co-ScheRRL, a scheduling algorithm delicately designed for co-located workloads. Co-ScheRRL consists of two major mechanisms: i) a self-attention encoding mechanism which encodes and represents states of the computing cluster as a set of embedding feature vectors; ii) a deep reinforcement learning (DRL) relational reasoning mechanism which calculates and compares different scheduling actions under different co-located workloads pattern via DRL feedback reward signals based on these feature vectors. Our two mechanisms can tackle complicatedly and dynamically varying behaviors of co-located workloads. With the help of these two mechanisms, Co-ScheRRL is able to construct high-quality scheduling policies. Trace-driven simulation demonstrates that Co-ScheRRL outperforms existing scheduling algorithms in terms of makespan by more than 38.4\% and throughput by more than 166.7\%.},
  archive      = {J_TCC},
  author       = {Jialun Li and Danyang Xiao and Jieqian Yao and Yujie Long and Weigang Wu},
  doi          = {10.1109/TCC.2023.3319383},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3725-3736},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Learning scheduling policies for co-located workloads in cloud datacenters},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A complex behavioral interaction analysis method for
microservice systems with bounded buffers. <em>TCC</em>, <em>11</em>(4),
3713–3724. (<a href="https://doi.org/10.1109/TCC.2023.3319038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interaction process in microservice architectures is highly complex, making it very challenging to ensure correct behavioral interactions. The few related works focus only on the verification of interaction soundness in the case of a specific buffer k -value, without considering how to get the suitable buffer k -value. To solve the above problems, this article proposes a method to find the maximum k -value for microservice systems with bounded buffers, which can maximize the analysis of valuable asynchronous interaction paths while avoiding the waste of computer memory resources. Specific contributions include, first, giving the relationship between buffer k -value and asynchronous interaction paths, interaction soundness, and its proof; second, proposing an iterative detection based on the additional added paths algorithm and its correctness proof, which leads to the conclusion that finding the maximum k -value is a decidable problem; finally, validating the proposing methods on ten classical cases of microservice systems, and analyzing effectiveness and performance. The experimental results show that the method can effectively find the maximum k -value of bounded buffers compared with existing methods and thus ensure the correct behavioral interactions of microservice systems.},
  archive      = {J_TCC},
  author       = {Shuo Wang and Zhijun Ding and Ru Yang and Changjun Jiang},
  doi          = {10.1109/TCC.2023.3319038},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3713-3724},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A complex behavioral interaction analysis method for microservice systems with bounded buffers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MARS: A DRL-based multi-task resource scheduling framework
for UAV with IRS-assisted mobile edge computing system. <em>TCC</em>,
<em>11</em>(4), 3700–3712. (<a
href="https://doi.org/10.1109/TCC.2023.3307582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies a dynamic Mobile Edge Computing (MEC) system assisted by Unmanned Aerial Vehicles (UAVs) and Intelligent Reflective Surfaces (IRSs). We propose a scaleable resource scheduling algorithm to minimize the energy consumption of all UEs and UAVs in the MEC system with a variable number of UAVs. We propose a Multi-tAsk Resource Scheduling (MARS) framework based on Deep Reinforcement Learning (DRL) to solve the problem. First, we present a novel Advantage Actor-Critic (A2C) structure with the state-value critic and entropy-enhanced actor to reduce variance and enhance the policy search of DRL. Then, we present a multi-head agent with three different heads in which a classification head is applied to make offloading decisions and a regression head is presented to allocate computational resources, and a critic head is introduced to estimate the state value of the selected action. Next, we introduce a multi-task controller to adjust the agent to adapt to the varying number of UAVs by loading or unloading a part of weights in the agent. Finally, a Light Wolf Search (LWS) is introduced as the action refinement to enhance the exploration in the dynamic action space. The numerical results demonstrate the feasibility and efficiency of the MARS framework.},
  archive      = {J_TCC},
  author       = {Feibo Jiang and Yubo Peng and Kezhi Wang and Li Dong and Kun Yang},
  doi          = {10.1109/TCC.2023.3307582},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3700-3712},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MARS: A DRL-based multi-task resource scheduling framework for UAV with IRS-assisted mobile edge computing system},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective cloud task scheduling optimization based on
evolutionary multi-factor algorithm. <em>TCC</em>, <em>11</em>(4),
3685–3699. (<a href="https://doi.org/10.1109/TCC.2023.3315014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud platforms scheduling resources based on the demand of the tasks submitted by the users, is critical to the cloud provider&#39;s interest and customer satisfaction. In this paper, we propose a multi-objective cloud task scheduling algorithm based on an evolutionary multi-factorial optimization algorithm. First, we choose execution time, execution cost, and virtual machines load balancing as the objective functions to construct a multi-objective cloud task scheduling model. Second, the multi-factor optimization (MFO) technique is applied to the task scheduling problem, and the task scheduling characteristics are combined with the multi-objective multi-factor optimization (MO-MFO) algorithm to construct an assisted optimization task. Finally, a dynamic adaptive transfer strategy is designed to determine the similarity between tasks according to the degree of overlap of the MFO problem and to control the intensity of knowledge transfer. The results of simulation experiments on the cloud task test dataset show that our method significantly improves scheduling efficiency, compared with other evolutionary algorithms (EAs), the scheduling method simplifies the decomposition of complex problems by a multi-factor approach, while using knowledge transfer to share the convergence direction among sub-populations, which can find the optimal solution interval more quickly and achieve the best results among all objective functions.},
  archive      = {J_TCC},
  author       = {Zhihua Cui and Tianhao Zhao and Linjie Wu and A. K. Qin and Jianwei Li},
  doi          = {10.1109/TCC.2023.3315014},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3685-3699},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-objective cloud task scheduling optimization based on evolutionary multi-factor algorithm},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A secure client-side deduplication scheme based on updatable
server-aided encryption. <em>TCC</em>, <em>11</em>(4), 3672–3684. (<a
href="https://doi.org/10.1109/TCC.2023.3311760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The server-aided encryption is widely used in encrypted deduplication systems to protect against brute-force attacks. However, it is non-trivial to update the master key managed by the key server in existing schemes. Once the master key is leaked, all user data are vulnerable to offline brute-force attacks. In this article, we extend the server-aided encryption with the updatable encryption (UE) and a dynamic proof of ownership (PoW) protocol to make it support efficient key updates and can be used in the client-side deduplication. Specifically, we design an updatable server-aided encryption scheme based on UE, which achieves efficient encryption and the user-transparent key update for a system-level master key. Besides, to further enable our updatable server-aided encryption to be applicable to the client-side deduplication, we propose a dynamic PoW protocol based on the Merkle tree. Compared to the state-of-the-art PoW scheme, our PoW protects data privacy and allows multi-time leakages for the target file. Finally, we analyze the security of our scheme and present the performance evaluation. The results show that our scheme provides comprehensive security protection for user data and achieves efficient encryption, PoW, and key update.},
  archive      = {J_TCC},
  author       = {Guanxiong Ha and Chunfu Jia and Yuchen Chen and Hang Chen and Mingyue Li},
  doi          = {10.1109/TCC.2023.3311760},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3672-3684},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A secure client-side deduplication scheme based on updatable server-aided encryption},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint task offloading and service placement for mobile edge
computing: An online two-timescale approach. <em>TCC</em>,
<em>11</em>(4), 3656–3671. (<a
href="https://doi.org/10.1109/TCC.2023.3312283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new computing paradigm, mobile edge computing (MEC) pushes the centralized cloud resources close to the edge network, which significantly reduces the pressure of the backbone network and meets the requirements of emerging mobile applications. To achieve high performance of the MEC system, it is essential to design efficient task offloading and service placement schemes, which are responsible for offloading tasks to the edge servers while considering the heterogeneity and diversity of computation services. Our MEC system aims to maximize the long-term average network utility while maintaining the stability of the edge network. Considering that synchronous manner overlooks the scenarios endowed with asymmetric update frequencies for service placement and task offloading, we propose an online algorithm based on the two-timescale Lyapunov optimization in a stochastic network environment without requiring the future information. By making asynchronous decisions on service placement and task offloading with different control parameters $V$ , we can achieve a time-average sub-optimal solution that is close to the offline optimum. In addition, we introduce the varying control parameter $V(t)$ and $\Omega$ -additive approximation to enhance the robustness of the proposed algorithm within an error $\Omega$ . Finally, rigorous theoretical analysis and extensive trace-driven experimental results show that the proposed algorithm achieves the $[O(1/V), O(V)]$ performance-backlog tradeoff and is more competitive than benchmarks.},
  archive      = {J_TCC},
  author       = {Xin Li and Xinglin Zhang and Tiansheng Huang},
  doi          = {10.1109/TCC.2023.3312283},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3656-3671},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint task offloading and service placement for mobile edge computing: An online two-timescale approach},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An energy-efficient tuning method for cloud servers
combining DVFS and parameter optimization. <em>TCC</em>, <em>11</em>(4),
3643–3655. (<a href="https://doi.org/10.1109/TCC.2023.3308927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging cloud computing applications place a growing demand on resources, leading to increasingly large data centers with significant energy consumption and carbon emissions. Various research conduct optimization methods to improve the energy efficiency of the server in the cloud data center. However, most existing optimization methods are designed for specific applications, thus making it difficult to handle complex cloud environments. In this paper, we propose a general parameter optimization method called MPOD to improve the energy efficiency of cloud servers in real time. MPOD considers issues in the cloud environment, such as SLA guarantee, user privacy, and dynamic workloads. We introduce energy efficiency curves to DVFS, implementing a low-overhead, fast response, and general frequency optimization strategy. Moreover, we design a workload classification framework and three prediction models based on machine learning algorithms to achieve accurate and adaptive Linux kernel parameters optimization. According to the experiment, MPOD can improve the energy efficiency of the server by an average of 30.5\%, 20.1\%, 10.8\% in BenchSEE, SERT and TPC-H, respectively.},
  archive      = {J_TCC},
  author       = {Weiwei Lin and Xiaoxuan Luo and ChunKi Li and Jiechao Liang and Guokai Wu and Keqin Li},
  doi          = {10.1109/TCC.2023.3308927},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3643-3655},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An energy-efficient tuning method for cloud servers combining DVFS and parameter optimization},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On a meta learning-based scheduler for deep learning
clusters. <em>TCC</em>, <em>11</em>(4), 3631–3642. (<a
href="https://doi.org/10.1109/TCC.2023.3308161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has become a dominating type of workloads on AI computing platforms. The performance of such platforms highly depends on how distributed DL jobs are scheduled. Reinforcement learning (RL)-based schedulers have been extensively studied and are capable of modeling interferences between concurrent jobs competing for resources. However, existing RL-based schedulers must learn from large number of samples and adapt to workload changes in real systems, which is a huge cost for production clusters. This paper proposes an intelligent, autonomous scheduler that employs sample-efficient RL for real-world resource scheduling on complex DL clusters. Specifically, we design a closed-loop meta-RL-based worker placement algorithm for DL training jobs. Instead of random exploration, we encourage the scheduler to explore combinatorial subspaces, where the performance model might be inaccurate, to improve the sampling efficiency of the scheduler agent. Extensive experimental results demonstrate that our algorithm outperforms other baselines in terms of average job completion time with 12.29\% to 16.24\% improvements. Further experiments with workload variations yield 15.76\% to 22.13\% improvements.},
  archive      = {J_TCC},
  author       = {Jin Yang and Liang Bao and Wenjing Liu and Rong Yang and Chase Q. Wu},
  doi          = {10.1109/TCC.2023.3308161},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3631-3642},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {On a meta learning-based scheduler for deep learning clusters},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving and forward public key encryption with
field-free multi-keyword search for cloud encrypted data. <em>TCC</em>,
<em>11</em>(4), 3619–3630. (<a
href="https://doi.org/10.1109/TCC.2023.3305370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the excessive growth of data and the rapid development of cloud technology, cloud adoption is expanding rapidly nowadays. To achieve the purpose of privacy protection, the cloud data may be transmitted, stored and retrieved in enciphered form. Public key searchable encryption (PKSE) provides a feasible solution for efficient retrieval over enciphered data without decryption. However, traditional PKSE suffers from some problems, such as keyword guessing (KG) attack and unauthorized ciphertext retrieval. In this paper, we present a practical PKSE scheme named forward public key authenticated encryption with field-free conjunctive keyword search (FW-PAE-FCKS). The scheme enjoys several good properties (e.g., flexible multi-keyword search with no keyword fields, forward ciphertext retrieval) and can effectively withstand the KG attack and the unauthorized ciphertext retrieval. Moreover, the executive overhead of the scheme is very friendly to the user terminals with limited resources as it totally avoids the operations with high computation cost (such as hash-to-point, bilinear pairing) on the user side. Based on the infeasibility assumption of the hash Diffie-Hellman problem, we formally prove its security without using the random oracle. Comparison analysis and experimental results show that it outperforms the existing related schemes.},
  archive      = {J_TCC},
  author       = {Yang Lu and Jiguo Li},
  doi          = {10.1109/TCC.2023.3305370},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3619-3630},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Privacy-preserving and forward public key encryption with field-free multi-keyword search for cloud encrypted data},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-tenant in-memory key-value cache partitioning using
efficient random sampling-based LRU model. <em>TCC</em>, <em>11</em>(4),
3601–3618. (<a href="https://doi.org/10.1109/TCC.2023.3300889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory key-value caches are widely used as a performance-critical layer in web applications, disk-based storage, and distributed systems. The Least Recently Used (LRU) replacement policy has become the de facto standard in those systems since it exploits workload locality well. However, the LRU implementation can be costly due to the rigid data structure in maintaining object priority, as well as the locks for object order updating. Redis as one of the most effective and prevalent deployed commercial systems adopts an approximated LRU policy, where the least recently used item from a small, randomly sampled set of items is chosen to evict. This random sampling-based policy is lightweight and shows its flexibility. We observe that there can exist a significant miss ratio gap between exact LRU and random sampling-based LRU under different sampling size $K$ s. Therefore existing LRU miss ratio curve (MRC) construction techniques cannot be directly applied without loss of accuracy. In this article, we introduce a new probabilistic stack algorithm named KRR to accurately model random sampling based-LRU, and extend it to handle both fixed and variable objects in key-value caches. We present an efficient stack update algorithm that reduces the expected running time of KRR significantly. To improve the performance of the in-memory multi-tenant key-value cache that utilizes random sampling-based replacement, we propose kRedis, a reference locality- and latency-aware memory partitioning scheme. kRedis guides the memory allocation among the tenants and dynamically customizes $K$ to better exploit the locality of each individual tenant. Evaluation results over diverse workloads show that our model generates accurate miss ratio curves for both fixed and variable object size workloads, and enables practical, low-overhead online MRC prediction. Equipped with KRR, kRedis delivers up to a 50.2\% average access latency reduction, and up to a 262.8\% throughput improvement compared to Redis. Furthermore, by comparing with pRedis, a state-of-the-art design of memory allocation in Redis, kRedis shows up to 24.8\% and 61.8\% improvements in average access latency and throughput, respectively.},
  archive      = {J_TCC},
  author       = {Yuchen Wang and Junyao Yang and Zhenlin Wang},
  doi          = {10.1109/TCC.2023.3300889},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3601-3618},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-tenant in-memory key-value cache partitioning using efficient random sampling-based LRU model},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel web attack detection mechanism using maximal-munch
with torrent deep network. <em>TCC</em>, <em>11</em>(4), 3591–3600. (<a
href="https://doi.org/10.1109/TCC.2023.3300083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A web attack is a harmful and deliberate attempt made by one person or group to gain access to another person&#39;s or group&#39;s data collection. Due to the incompatibility of the training algorithm for the Cross-Site Scripting (XSS) detection technique and the heterogeneity of attack load, the website was more frequently impacted by the detection of SQL injection attacks. Also, the language of the online sites has a significant impact on how well the current phishing detection system works, which is still a difficult issue. To address these problems, a novel Praise-Worthy Authentication technique is proposed which accurately detects phishing websites by checking the webpage&#39;s conformance using the hyperlink property. Also, a Maximal-Munch Algorithm-based ANN is proposed to prevent XSS attacks. The URLs associated with each webpage that is dragged will be sorted out to acquire URL parameters, and text patterns are matched at regular intervals to detect the XSS attack. This work also employs a Torrent Deep network with weight-bolster Algorithm to identify SQL injection by hackers, preventing significant network damage that would otherwise cause data leaks and website paralysis. This proposed Web-strafe Detection Framework has considerably increased the security of websites by identifying numerous threats.},
  archive      = {J_TCC},
  author       = {Seema Pillai and Vijayant Verma},
  doi          = {10.1109/TCC.2023.3300083},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3591-3600},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A novel web attack detection mechanism using maximal-munch with torrent deep network},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DTFL: A digital twin-assisted graph neural network approach
for service function chains failure localization. <em>TCC</em>,
<em>11</em>(4), 3573–3590. (<a
href="https://doi.org/10.1109/TCC.2023.3294506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing enables Network Function Virtualization to dynamically provide and deploy network functions (NFs) to meet business-specific requirements. This approach streamlines NFs’ lifecycle management and lowers the cost of Operation Administration and Maintenance. However, these advantages cause Service Function Chain (SFC) failure to grow in both scope and dimensionality, making it difficult to establish a model to locate the failure effectively. In this paper, we propose a complete analysis scheme DTFL (Digital Twin (DT) based for SFC failure localization (FL)) through the following two steps: one is classifying and locating failures, and the other is conducting root cause analysis. We propose transGNN based on the Graph Neural Network and improved graph search model to achieve the classification and location for SFC failures. On this basis, the FNSG-RCA algorithm (failure based graph model) is proposed to analyze failures. We build a prototype based on the cloud platform and experimental results show that this scheme can achieve an accuracy rate of over 98\% in fine-grained classification of 49 failure types. In addition, DTFL delivers desirable performance in RCA, approximately 13\% more accurate than SOTA, the state-of-the-art approach. DTFL improves both RCA accuracy and model deployment efficiency compared with the non-DT approaches.},
  archive      = {J_TCC},
  author       = {Kuo Guo and Jia Chen and Ping Dong and Tao Zou and Jun Zhu and Xu Huang and Shang Liu and Chenxi Liao},
  doi          = {10.1109/TCC.2023.3294506},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3573-3590},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {DTFL: A digital twin-assisted graph neural network approach for service function chains failure localization},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secure multi-party computation-based privacy-preserving
authentication for smart cities. <em>TCC</em>, <em>11</em>(4),
3555–3572. (<a href="https://doi.org/10.1109/TCC.2023.3294621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing concern for identity confidentiality in the Smart City scenario has fostered research on privacy-preserving authentication based on pseudonymization. Pseudonym systems enable citizens to generate pseudo-identities and establish unlinkable anonymous accounts in cloud service providers. The citizen&#39;s identity is concealed, and his/her different anonymous accounts cannot be linked to each other. Unfortunately, current pseudonym systems require a trusted certification authority (CA) to issue the cryptographic components (e.g., credentials, secret keys, or pseudonyms) to citizens. This CA, generally a Smart City governmental entity, has the capability to grant or revoke privacy rights at will, hence posing a serious threat in case of corruption. Additionally, if the pseudonym system enables de-anonymization of misusers, a corrupted CA can jeopardize the citizens’ privacy. This paper presents a novel approach to construct a pseudonym system without a trusted issuer. The CA is emulated by a set of Smart City service providers by means of secure multi-party computation (MPC), which circumvents the requirement of assuming an honest CA. The paper provides a full description of the system, which integrates an MPC protocol and a pseudonym-based signature scheme. The system has been implemented and tested.},
  archive      = {J_TCC},
  author       = {Victor Sucasas and Abdelrahaman Aly and Georgios Mantas and Jonathan Rodriguez and Najwa Aaraj},
  doi          = {10.1109/TCC.2023.3294621},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3555-3572},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure multi-party computation-based privacy-preserving authentication for smart cities},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CLUE: Systems support for knowledge transfer in
collaborative learning with neural nets. <em>TCC</em>, <em>11</em>(4),
3541–3554. (<a href="https://doi.org/10.1109/TCC.2023.3294490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For highly distributed environments such as edge computing, collaborative learning approaches eschew the dependence on a global, shared model, in favor of models tailored for each location. Creating tailored models for individual learning contexts reduces the amount of data transfer, while collaboration among peers provides acceptable model performance. Collaboration assumes, however, the availability of knowledge transfer mechanisms, which are not trivial for deep learning models where knowledge isn&#39;t easily attributed to precise model slices. We present CLUE – a framework that facilitates knowledge transfer for neural networks. CLUE provides new system support for dynamically extracting significant parameters from a helper node&#39;s neural network, and uses this with a multi-model boosting-based approach to improve the predictive performance of the target node. The evaluation of CLUE with different PyTorch and TensorFlow neural network models demonstrates that its knowledge transfer mechanism improves by up to $3.5\times$ how quickly a model adapts to changes, compared to learning in isolation, while affording up to several magnitudes reduction in data movement costs compared to federated learning.},
  archive      = {J_TCC},
  author       = {Harshit Daga and Yiwen Chen and Aastha Agrawal and Ada Gavrilovska},
  doi          = {10.1109/TCC.2023.3294490},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3541-3554},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {CLUE: Systems support for knowledge transfer in collaborative learning with neural nets},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Queue-aware service orchestration and adaptive parallel
traffic scheduling optimization in SDNFV-enabled cloud computing.
<em>TCC</em>, <em>11</em>(4), 3525–3540. (<a
href="https://doi.org/10.1109/TCC.2023.3294239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to software defined network function virtualization (SDNFV), network services can be implemented as service function chains (SFCs) in SDNFV-enabled Cloud Computing. SFCs consist of a series of ordered virtual network functions (VNFs). Due to the dynamic of underlying network state and the unpredictability of network traffic, the traditional SFC orchestrating (SFCO) approaches based on centralized placement and single-path routing lead to low availability of network resources, making it difficult to effectively manage and utilize complex and heterogeneous network resources. To address the above challenges, we propose a queue-aware SFCs orchestrating and adaptive parallel traffic scheduling optimization approach. First, the SFCO problem is modeled as a stochastic optimization problem, and the Lyapunov optimization theory is used to transform and decompose the SFCO problem to decouple the time coupling of optimal decision-making. An automatic decentralized algorithm based on queue model is proposed to orchestrate SFCs using information of local and its immediate one-hop neighbors. Furthermore, an adaptive parallel traffic scheduling optimization algorithm based on deep reinforcement learning is proposed, according to the decision output of the distributed SFC algorithm and current network state, network traffic is allocated to multiple paths for parallel transmission, which improves the availability of network resources and network performance. Experimental results show that, compared with the benchmarks, the average queue depth of the designed approach is reduced by $42.18\% {\sim} 69.97\%$ , the average cost of the designed approach is reduced by $16.1\% {\sim} 55.6\%$ , the average throughput is improved by $2.41\%{\sim} 10.07\%$ , the average link resource utilization rate is improved by about $6.9\%{\sim} 28.3\%$ , the average round-trip delay is shortened by $17.1\%{\sim} 24.1\%$ , and the average packet loss rate is reduced by $39.4\%{\sim} 51.7\%$ .},
  archive      = {J_TCC},
  author       = {Jia Chen and Jing Chen and Kuo Guo},
  doi          = {10.1109/TCC.2023.3294239},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3525-3540},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Queue-aware service orchestration and adaptive parallel traffic scheduling optimization in SDNFV-enabled cloud computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DCDPI: Dynamic and continuous deep packet inspection in
secure outsourced middleboxes. <em>TCC</em>, <em>11</em>(4), 3510–3524.
(<a href="https://doi.org/10.1109/TCC.2023.3293134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure outsourced middleboxes are deployed in network function virtualization services that detect malicious activities on communications, which provides privacy-preserving deep packet inspection (DPI) over encrypted traffic. To boost filtering efficiency of packets, the two-layer middlebox architecture has been adopted in recent DPI systems. Nevertheless, state-of-the-art solutions based on two-layer architecture mainly suffer from two limitations: i) cannot support dynamic rule addition; ii) failed to inspect discontinuous token for rule matching. To address these limitations, this work proposes an efficient, dynamic and continuous DPI (DCDPI) system in secure outsourced middleboxes. To achieve dynamic rule addition with forward privacy, we refine a data structure called virtual binary tree (VBTree) and further introduce a variant of VBTree for DCDPI, termed VBTree+. VBTree+ supports two new desirable features: i) taking the rule action information into consideration; ii) achieving both rule identifier and rule action hiding. By introducing a token continuity check mechanism, DCDPI can effectively identify discontinuous tokens and categorize continuous tokens into one group. The extensive experiment over the real dataset and rule set confirms the practicality and efficiency of DCDPI. Compared to state-of-the-art works with same setting, DCDPI is 18\% $\sim$ 110\% more efficient for a connection establishment between gateway/client and server.},
  archive      = {J_TCC},
  author       = {Minjun Deng and Kai Zhang and Pengfei Wu and Mi Wen and Jianting Ning},
  doi          = {10.1109/TCC.2023.3293134},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3510-3524},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {DCDPI: Dynamic and continuous deep packet inspection in secure outsourced middleboxes},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proactive resource autoscaling scheme based on SCINet for
high-performance cloud computing. <em>TCC</em>, <em>11</em>(4),
3497–3509. (<a href="https://doi.org/10.1109/TCC.2023.3292378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The container resource autoscaling technique provides scalability to cloud services composed of microservice architecture in a cloud-native computing environment. However, the service efficiency is reduced as the scaling is delayed because dynamic loads occur with various workload patterns. Furthermore, estimating the efficient resource size for the workload is difficult, resulting in resource waste and overload. Therefore, this study proposes high-performance resource management (HiPerRM), which stably and elastically manages container resources to ensure service scalability and efficiency even under rapidly changing dynamic loads. HiPerRM forecasts future workloads using a sample convolutional and interaction network (SCINet) model applied with the reversible instance normalization (RevIN) method. HiPerRM generates a resource request with an elastic size based on the forecasted CPU and memory usage, and then efficiently adjusts the pod&#39;s resource request and the number of replicas via HiPerRM&#39;s VPA (Hi-VPA) and HiPerRM&#39;s HPA (Hi-HPA). As a result of evaluating the performance of HiPerRM, the average resource utilization was improved by approximately 3.96–34.06\% compared to conventional autoscaling techniques, even when the resource size was incorrectly estimated for various workloads, and there were relatively fewer overloads.},
  archive      = {J_TCC},
  author       = {Byeonghui Jeong and Jueun Jeon and Young-Sik Jeong},
  doi          = {10.1109/TCC.2023.3292378},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3497-3509},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Proactive resource autoscaling scheme based on SCINet for high-performance cloud computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-adaptive gradient quantization for geo-distributed
machine learning over heterogeneous and dynamic networks. <em>TCC</em>,
<em>11</em>(4), 3483–3496. (<a
href="https://doi.org/10.1109/TCC.2023.3292525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geo-Distributed Machine Learning (Geo-DML) has been proposed to collaborate geographically dispersed data centers (DCs) and train large scale machine learning (ML) models for various applications. While Geo-DML can achieve excellent performance, it also injects massive data traffic into the Wide Area Networks (WANs) in order to exchange gradients during model training process. Such a huge amount of traffic will not only incur network congestion and prolong the training procedure, but also result in straggler problem when DCs are working in heterogeneous network environments. To alleviate these problems, we propose Self-Adaptive Gradient Quantization (SAGQ) for Geo-DML in this work. In SAGQ, each worker DC adopts specific quantization method based on the heterogeneous and dynamic link bandwidth in order to reduce the communication overhead and balance the communication time among worker DCs. By doing so, SAGQ will speed up the Geo-DML training process without sacrificing the ML model performance. Extensive experiments show that compared with the state-of-the-art techniques, SAGQ reduces the Wall-clock time spent to train an ML model by 1.13×–21.31×. In addition, SAGQ can also improve the model accuracy by 0.11\%–2.27\% over baselines.},
  archive      = {J_TCC},
  author       = {Chenyu Fan and Xiaoning Zhang and Yangming Zhao and Yutao Liu and Shui Yu},
  doi          = {10.1109/TCC.2023.3292525},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3483-3496},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Self-adaptive gradient quantization for geo-distributed machine learning over heterogeneous and dynamic networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Priority-driven differentiated performance for NoSQL
database-as-a-service. <em>TCC</em>, <em>11</em>(4), 3469–3482. (<a
href="https://doi.org/10.1109/TCC.2023.3292031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing data stores for native Cloud Computing services brings a number of challenges, especially if the Cloud Provider wants to offer database services capable of controlling the response time for specific customers. These requests may come from heterogeneous data-driven applications with conflicting responsiveness requirements. For instance, a batch processing workload does not require the same level of responsiveness as a time-sensitive one. Their coexistence may interfere with the responsiveness of the time-sensitive workload, such as online video gaming, virtual reality, and cloud-based machine learning. This article presents a modification to the popular MongoDB NoSQL database to enable differentiated per-user/request performance on a priority basis by leveraging CPU scheduling and synchronization mechanisms available within the Operating System. This is achieved with minimally invasive changes to the source code and without affecting the performance and behavior of the database when the new feature is not in use. The proposed extension has been integrated with the access-control model of MongoDB for secure and controlled access to the new capability. Extensive experimentation with realistic workloads demonstrates how the proposed solution is able to reduce the response times for high-priority users/requests, with respect to lower-priority ones, in scenarios with mixed-priority clients accessing the data store.},
  archive      = {J_TCC},
  author       = {Remo Andreoli and Tommaso Cucinotta and Daniel Bristot De Oliveira},
  doi          = {10.1109/TCC.2023.3292031},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3469-3482},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Priority-driven differentiated performance for NoSQL database-as-a-service},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-interactive privacy-preserving frequent itemset mining
over encrypted cloud data. <em>TCC</em>, <em>11</em>(4), 3452–3468. (<a
href="https://doi.org/10.1109/TCC.2023.3291378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent itemset mining is a data mining technique widely used on massive datasets. In cloud computing, the dataset may be encrypted for privacy protection. Therefore, frequent itemset mining over encrypted data is a crucial application in secure cloud computing. In this paper, we propose an effective privacy-preserving framework where the cloud server can directly perform data mining on the encrypted database without interacting with other cloud servers. We first design three security primitives to implement subset determination, accumulation, and comparison in the encrypted domain for frequent itemset mining. Based on the proposed framework, we then propose two secure protocols that allow the cloud server to perform frequent itemset mining on encrypted cloud data with these security primitives. The first protocol leaks no information to the cloud and the second protocol has the advantage of more efficient mining performance. We then present two strategies with parallel algorithms and GPU computing to accelerate the running time. We also analyze the security of our protocols and the computational complexities. Experimental results show that our serial-based protocols achieve shorter running times and higher levels of privacy than previous solutions. Our multi-CPU (or GPU) based parallel protocol can further reduce the practical running time.},
  archive      = {J_TCC},
  author       = {Peijia Zheng and Ziyan Cheng and Xianhao Tian and Hongmei Liu and Weiqi Luo and Jiwu Huang},
  doi          = {10.1109/TCC.2023.3291378},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3452-3468},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Non-interactive privacy-preserving frequent itemset mining over encrypted cloud data},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dependent application offloading in edge computing.
<em>TCC</em>, <em>11</em>(4), 3439–3451. (<a
href="https://doi.org/10.1109/TCC.2023.3290777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task offloading offloads latency-sensitive and computation-intensive applications from resource-constrained terminal devices to relatively resource-rich edge servers to meet users’ demands for latency and energy consumption, which has attracted extensive attention from academia and industry. However, most of the existing researches only considers offloading dependent tasks within a single application or multiple independent applications, while ignoring the dependencies between applications. To this end, this paper proposes an offloading strategy for distributed dependent applications under the condition of limited computing and cache resources. The goal of the proposed strategy is to minimize the weighted sum of latency and energy to complete all applications while solving the offloading and resource allocation problems of dependent applications. However, the dual dependencies between applications and tasks within the application complicate offloading tasks. To accommodate this issue, we represent the dual dependencies as a directed acyclic graph. Then, we design the offloading strategy as follows: First, we transform the formulated non-convex problem into convex optimization subproblems. Second, we iteratively calculate the task priority and obtain the optimal offloading decision of the task according to the priority. Finally, we perform validation on real datasets. Compared with several state-of-the-art methods, our proposed strategy can significantly reduce the weighted sum of latency and energy.},
  archive      = {J_TCC},
  author       = {Junna Zhang and Guoxian Zhang and Xiang Bao and Chuntao Ding and Peiyan Yuan and Xinglin Zhang and Shangguang Wang},
  doi          = {10.1109/TCC.2023.3290777},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3439-3451},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dependent application offloading in edge computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint trajectory and energy consumption optimization based
on UAV wireless charging in cloud computing system. <em>TCC</em>,
<em>11</em>(4), 3426–3438. (<a
href="https://doi.org/10.1109/TCC.2023.3288527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microwave Power Transfer (MPT) is a promising technology to charge sensor devices (SDs) wirelessly in wireless sensor networks, and Cloud Computing (CC) can significantly promote task processing capacity of SDs. However, the propagation loss can dramatically influence the harvested energy and computation performance. So, for wireless sensor networks, we study an unmanned aerial vehicle-assisted cloud wireless charging system with the cooperation of the cloud server and the unmanned aerial vehicle (UAV). First, the UAV acts as the energy transmitter, and we design a quantitative charging scheme according to the energy-aware of SDs’ battery capacity. Second, the cloud server processes the tasks uploaded by SDs with the cooperation of the UAV, and we consider the communication connection between the cloud server and the UAV. Third, we propose the joint resource-trajectory optimization to reduce the energy consumption of UAVs. We put forward the Chaotically Adaptive Beetle Swarm Optimization Based on Cauchy Mutation (CABSOC) assisted block coordinate descent algorithm for addressing this non-convex problem. Numerical results indicate that the proposed solution can significantly improve the energy performance of the UAV. And the energy consumption is reduced by 11\% compared with the solution with network function virtualization (NFV).},
  archive      = {J_TCC},
  author       = {Shanchen Pang and Xiao He and Ching-Hsien Hsu and Chunming Rong and Hailong Zhu and Peiying Zhang},
  doi          = {10.1109/TCC.2023.3288527},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3426-3438},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint trajectory and energy consumption optimization based on UAV wireless charging in cloud computing system},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secure distributed storage orchestration on heterogeneous
cloud-edge infrastructures. <em>TCC</em>, <em>11</em>(4), 3407–3425. (<a
href="https://doi.org/10.1109/TCC.2023.3287653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed storage systems spanning across different cloud data centers have substantially improved availability and flexibility for data storage and retrieval operations. However, stringent latency requirements of emerging applications necessitate optimized selection of storage resources that exhibit smaller delay. Introducing edge resources into distributed storage systems enables data placement closer to its source, but simultaneously increases the complexity of decision-making and orchestration processes for optimal data placement. In this work, we develop mechanisms for storing data across an infrastructure that includes both edge and cloud resources. Our approach focuses on optimizing data integrity, longevity, security, and cost, while leveraging erasure coding when performing the resource allocation. We first present a comprehensive mixed integer linear programming formulation of the storage resource orchestration problem. As the search space for the optimal solution can be vast and the execution time prohibitively large for real size problems, we also propose an innovative multi-agent heuristic approach that uses the rollout, a reinforcement based policy, to balance performance and execution time efficiently. Through various simulation experiments, we evaluate the developed mechanisms and trade-offs involved in our approach. By incorporating data from a multi-cloud provider, we further enhance the validity of the simulations and the conclusions drawn.},
  archive      = {J_TCC},
  author       = {Konstantinos Kontodimas and Polyzois Soumplis and Aristotelis Kretsis and Panagiotis Kokkinos and Marcell Fehér and Daniel E. Lucani and Emmanouel Varvarigos},
  doi          = {10.1109/TCC.2023.3287653},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3407-3425},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure distributed storage orchestration on heterogeneous cloud-edge infrastructures},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Public-key encryption with tester verifiable equality test
for cloud computing. <em>TCC</em>, <em>11</em>(4), 3396–3406. (<a
href="https://doi.org/10.1109/TCC.2023.3287862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public-key encryption with equality test (PKEET) provides cloud servers with an effective way to check the equality of outsourced encrypted data without decryption. This enables PKEET to attract much attention and be widely researched in cloud computing. However, we claim that the existing PKEET schemes suffer from an inherited problem, called message-consistency unverifiability of testers (MCUT). Applying the MCUT problem, outsourcers can fool cloud servers into outputting incorrect testing results of encrypted data, which negates the practicability of PKEET in cloud computing. We investigate the PKEET literature and find the main reason for the MCUT problem is the independence between the messages inserted in the decryption and testing modules in their ciphertexts. To bridge the technical gap between PKEET and its practical applications, we present a new notion, called PKE with tester verifiable equality test (PKE-TVET), which solves the MCUT problem by allowing testers to verify the message consistency in two modules. We then instantiate the PKE-TVET and give a specific construction in the standard model. In our PKE-TVET scheme, the testing module is integrated into the decryption module so that there is only one message inserted in the ciphertext for both decryption and testing. This special setting lets our scheme directly get rid of the MCUT problem. For better applications in actual scenarios, we further extend the scheme to support authorization and tester designation. Finally, we analyze the tradeoff of parameter sizes and computation costs for the security against MCUT attacks in our PKE-TVET scheme.},
  archive      = {J_TCC},
  author       = {Zhen Zhao and Willy Susilo and Baocang Wang and Kai Zeng},
  doi          = {10.1109/TCC.2023.3287862},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3396-3406},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Public-key encryption with tester verifiable equality test for cloud computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A privacy-preserving service framework for traveling
salesman problem-based neural combinatorial optimization network.
<em>TCC</em>, <em>11</em>(4), 3381–3395. (<a
href="https://doi.org/10.1109/TCC.2023.3287552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traveling salesman problem (TSP) is one of the classic combinatorial optimization problems, which can be widely used in intelligent transportation and logistics field. Neural network has shown great potential in combinatorial optimization tasks. However, it faces privacy leakage when a TSP neural combinatorial optimization network and user&#39;s data are directly outsourced to a cloud platform to provide and request service. In order to address the issue, this article proposes a privacy-preserving service framework for a TSP-based neural combinatorial optimization network called PPSF. We first protect the service provider&#39;s model parameters and users’ data by different split methods in multiple cloud servers, providing a secure outsourced mode for the participators in the PPSF framework. Then, in the secure outsourced mode, a series of secure computation protocols are designed for the cloud servers to support performing the secure computing in each service task. Moreover, it can also protect the process that the cloud servers respond to users after data processing and achieve private service result recovery. Finally, we prove that the proposed framework can realize privacy protection for TSP-based combinatorial optimization service and verify its utility and efficiency by experiments.},
  archive      = {J_TCC},
  author       = {Jiao Liu and Xinghua Li and Ximeng Liu and Jiawei Tang and Siqi Ma and Jian Weng},
  doi          = {10.1109/TCC.2023.3287552},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3381-3395},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A privacy-preserving service framework for traveling salesman problem-based neural combinatorial optimization network},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding NFV-enabled vehicle platooning application: A
dependability view. <em>TCC</em>, <em>11</em>(4), 3367–3380. (<a
href="https://doi.org/10.1109/TCC.2023.3286894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to use analytical modeling technique to quantitatively study the dependability of Vehicle Platooning Application, which consists of Multiple Sub-Services (VPP-MSS) to achieve its functionality. Each sub-service (SS), based on network function virtualization technology, is executed in a container. Both SSes and OSes which SSes run on can suffer from software aging after a long and continuous running, reducing VPP-MSS dependability. Rejuvenation techniques are usually used to combat software aging, but they require the support of backup components. Quantitative study of VPP-MSS dependability enables in-depth understanding of the effectiveness of rejuvenation techniques based on analytical models. In contrast to the existing studies, we develop a semi-Markov process (SMP) model to jointly analyze the impact of rejuvenation technique trigger intervals (RTTIs), backup components’ behaviors, time-dependent interactions between various behaviors and the number of active SSes deployed on an OS on the effectiveness of rejuvenation technique. Sensitivity analysis helps identify key parameters for improving the dependability of VPP-MSS. Extensive numerical experiments demonstrate the necessity of considering backup components’ behaviors and investigating non-exponentially distributed failure times. We also determine both the optimal RTTI combination and the optimal combination of SSes and OSes, which can maximize VPP-MSS dependability.},
  archive      = {J_TCC},
  author       = {Jing Bai and Yaru Li and Xiaolin Chang and Fumio Machida and Kishor S. Trivedi},
  doi          = {10.1109/TCC.2023.3286894},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3367-3380},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Understanding NFV-enabled vehicle platooning application: A dependability view},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Energy-efficient federated edge learning in multi-tier
NOMA-enabled HetNet. <em>TCC</em>, <em>11</em>(4), 3355–3366. (<a
href="https://doi.org/10.1109/TCC.2023.3285534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel multi-tier (top, intermediate, and bottom tiers) architecture at the edge of a heterogeneous network (HetNet) where non-orthogonal multiple access (NOMA) provides access to user equipment (UE) to participate in federated edge learning (FEL). The HetNet consists of a macro base station (MBS) and several small base stations (SBSs) where each BS is equipped with an edge server (ES). SBSs use the same system bandwidth to increase the system capacity. The top tier consists of the MBS-ES which works as the global model aggregator while ESs of SBSs and UEs connected with MBS reside in the intermediate tier. Similarly, UEs connected with an SBS-ES of the intermediate tier occupy the bottom tier. ESs of SBSs work as the intermediate model aggregators between the ES of the top tier and the UEs of the bottom tier. To minimize the total energy consumption (EC) for local computing (LC) and uplink transmission (UT) of UEs, we formulate a non-linear programming (NLP) optimization problem, present our solution by decomposing the problem into sub-problems, and propose two sequential algorithms to estimate EC for both LC and UT with less complexity. Our extensively simulated results demonstrate the viability of our proposed work.},
  archive      = {J_TCC},
  author       = {Mohammad Arif Hossain and Nirwan Ansari},
  doi          = {10.1109/TCC.2023.3285534},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3355-3366},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Energy-efficient federated edge learning in multi-tier NOMA-enabled HetNet},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communication-efficient verifiable data streaming protocol
in the multi-user setting. <em>TCC</em>, <em>11</em>(4), 3341–3354. (<a
href="https://doi.org/10.1109/TCC.2023.3285783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifiable data streaming (VDS) protocols enable end users with limited storage space to continuously stream data items to an untrusted cloud server, while preserving the capacity of verifying the integrity of those retrieved data items for downstream tasks. Although there has been plenty of research around the construction of VDS, we observe that they all focus on the scenario of single-user. When deploying these VDS protocols into more common applications that involve multiple users’ data (e.g., network data monitoring and stock trends analysis), the size of the proof used to prove the integrity of retrieved data items grows linearly with the number of involved users. This would bring tremendous communication overhead, especially for lightweight users. To this end, we initiate the study of VDS protocols that are suitable for multi-user (or cross-user) setting. Specifically, we first introduce a new primitive called aggregatable chameleon vector commitment (ACVC) that allows to aggregate multiple proofs from different commitments into a single proof. Then, based on ACVC, we present a communication-efficient VDS protocol for the multi-user setting. That is, when querying data items from multiple users, the size of corresponding proof is constant and independent of the number of involved users. Theoretical analysis indicates that the proposed VDS protocol outperforms previous VDS protocols in terms of communication overhead. We also implement the proposed ACVC, and conduct extensive experiments to demonstrate its practicability.},
  archive      = {J_TCC},
  author       = {Xuan Jing and Meixia Miao and Jianghong Wei and Jianfeng Wang},
  doi          = {10.1109/TCC.2023.3285783},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3341-3354},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Communication-efficient verifiable data streaming protocol in the multi-user setting},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VFLR: An efficient and privacy-preserving vertical federated
framework for logistic regression. <em>TCC</em>, <em>11</em>(4),
3326–3340. (<a href="https://doi.org/10.1109/TCC.2023.3247870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of data volume and computing capability, federated learning, which involves constructing global models over multiple data islands, has demonstrated its advantages and vast prospects in the field of machine learning. However, due to commonly vertically partitioned data, coupled with privacy concerns about data leakage, there are still some challenging issues in traditional federated learning. To tackle these challenges, in this article, we propose an efficient and privacy-preserving vertical federated learning framework for logistic regression, named VFLR, where multiple participants can collaboratively perform global model training and query over their vertically partitioned data. Specifically, we first design a data aggregation matrix construction algorithm, with which the vertically partitioned data can be aggregated for high-accuracy global model training. Then, by utilizing a novel symmetric homomorphic encryption, our framework can ensure that the whole training and query processes do not leak any private information. Moreover, based on the data aggregation matrix, multi-round interactions are not required in VFLR, improving training efficiency significantly. Detailed security analysis shows that VFLR can well protect data and model information from inference attacks. In addition, extensive experiments demonstrate that VFLR has high training and query accuracy and low computation and communication overhead.},
  archive      = {J_TCC},
  author       = {Jiaqi Zhao and Hui Zhu and Fengwei Wang and Rongxing Lu and Ermei Wang and Linfeng Li and Hui Li},
  doi          = {10.1109/TCC.2023.3247870},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {4},
  pages        = {3326-3340},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {VFLR: An efficient and privacy-preserving vertical federated framework for logistic regression},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A privacy-preserving image retrieval scheme based on 16×16
DCT and deep learning. <em>TCC</em>, <em>11</em>(3), 3314–3325. (<a
href="https://doi.org/10.1109/TCC.2023.3286119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, people tend to upload images to cloud servers, which provide storage and retrieval functions. To prevent users’ privacy from leaking to the server, research on cipher-image retrieval has attracted much attention. This work presents a novel encrypted image retrieval method. With this scheme, we perform encryption during the JPEG compression process by applying 16×16 DCT (Discrete Cosine Transform) for blocks’ transformation, followed by coefficients distribution and 8×8 blocks’ permutation. For the retrieval part, when an encrypted query image is sent by an authorized user, the server extracts its DCT histograms as features and inputs them into our trained network model, which incorporates transpose Multilayer perceptron modules ( $Transpose$ $MLP$ ), for retrieval. Experimental results show that our scheme, compared with related schemes, can improve the retrieval performance significantly, when ensuring compression friendliness and no feature information leakage. Moreover, our scheme enables cipher-image retrieval from multiple image owners.},
  archive      = {J_TCC},
  author       = {Zhixun Lu and Qihua Feng and Peiya Li and Kwok-Tung Lo and Feiran Huang},
  doi          = {10.1109/TCC.2023.3286119},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3314-3325},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A privacy-preserving image retrieval scheme based on 16×16 DCT and deep learning},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Branchy deep learning based real-time defect detection under
edge-cloud fusion architecture. <em>TCC</em>, <em>11</em>(3), 3301–3313.
(<a href="https://doi.org/10.1109/TCC.2023.3285654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many machine-learning-based defect detection methods, especially deep learning-based approaches, have high requirements on computing power and network. They lead to time delay, high cost, and energy consumption when applied to deal with the massive data in an autonomous manufacturing enterprise. So efficient detection in the end-edge-cloud architecture is a good solution to overcome the above challenges. A branchy deep learning detection model with early exit ability of inference is proposed, in which the main branch is deployed on the cloud server and the side branches are on edge equipment. The proposed method quickly and effectively detects the category and location of the defect in printed circuit boards since partial computing task is offloaded to the edge nodes. A prototype system is implemented based on a computer as the cloud server and a Raspberry Pi as an edge node in order to verify the feasibility of the proposed method. The experiment result manifests high detection accuracy and fast computing speed.},
  archive      = {J_TCC},
  author       = {Jing Wang and Yi Wu and Yang-Quan Chen},
  doi          = {10.1109/TCC.2023.3285654},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3301-3313},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Branchy deep learning based real-time defect detection under edge-cloud fusion architecture},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). V-recover: Virtual machine recovery when live migration
fails. <em>TCC</em>, <em>11</em>(3), 3289–3300. (<a
href="https://doi.org/10.1109/TCC.2023.3282466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live migration is a critical technology used in cloud infrastructures to transfer running virtual machines (VMs). When live migration fails, as it often does, it is critical that any VMs in transit are not lost. There are two primary live migration techniques – pre-copy and post-copy. Pre-copy transfers a VM&#39;s memory to the destination before its virtual CPUs are transferred, whereas post-copy does the reverse. Both pre-copy and post-copy will lose the VM if the source machine fails during migration. Additionally, post-copy can lose the VM if the destination machine or network fail since the VM&#39;s memory and execution state are split across the source and destination machines. We present V-Recover, an approach to recover a VM when the source, destination, or network fails during live migration. V-Recover consists of two techniques: (1) a forward incremental checkpointing (FIC) mechanism to handle source machine failure during both pre-copy and post-copy, and (2) a reverse incremental checkpointing (RIC) mechanism to handle destination or network failure during post-copy. We present the design, implementation, and evaluation of V-Recover in the KVM/QEMU virtualization platform. Our evaluations show that V-Recover effectively recovers a VM upon migration failure with acceptable overheads on migration metrics and application performance.},
  archive      = {J_TCC},
  author       = {Dinuni Fernando and Jonathan Terner and Ping Yang and Kartik Gopalan},
  doi          = {10.1109/TCC.2023.3282466},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3289-3300},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {V-recover: Virtual machine recovery when live migration fails},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Placement of high availability geo-distributed data centers
in emerging economies. <em>TCC</em>, <em>11</em>(3), 3274–3288. (<a
href="https://doi.org/10.1109/TCC.2023.3280983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data center markets in emerging economies are being built at a furious pace. When high availability is required, as it always is in the modern digital economy, the placement of geo-distributed data centers may be influenced by factors such as technician shortage and under-developed infrastructure, both of which are typical in emerging economies. Although the data center availability subject in general has been well studied, it remains unclear how rapid and unbalanced economic development in emerging economies may affect the availability of geo-distributed data centers and their cost of ownership. In this paper, we incorporate the unbalanced availability of infrastructure and technician into the data center placement. The problem is first formulated as a mixed integer nonlinear program (MINLP). To solve this potentially large scale problem, we transform it into a QCQP, capable of handling heterogeneous workloads. The resulting problem can then be efficiently solved by off-the-shelf optimization toolboxes. With real-life data in China, we show how unbalanced development of infrastructure and technician shortage may affect the placement of data centers, and analyze the tradeoff between cost and availability. Our results indicate that technician shortage and unbalanced network infrastructure will lead to increased cost and distinct data center placement strategies.},
  archive      = {J_TCC},
  author       = {Ruiyun Liu and Weiqiang Sun and Weisheng Hu},
  doi          = {10.1109/TCC.2023.3280983},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3274-3288},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Placement of high availability geo-distributed data centers in emerging economies},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint task offloading and dispatching for MEC with rational
mobile devices and edge nodes. <em>TCC</em>, <em>11</em>(3), 3262–3273.
(<a href="https://doi.org/10.1109/TCC.2023.3280170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access Edge Computing has come forth as a promising paradigm to provide low-latency computing service to mobile end users. Its basic idea is to deploy computation resources at the edge of core networks such as wireless access points, and then users can offload their tasks to nearby edge nodes for processing. Plenty of works have well studied the task offloading problem, aiming to reduce task completion delays. Also, a few recent works have focused on task dispatching among edge nodes to balance their workloads and improve resource utilization. In this work, we jointly consider the task offloading and dispatching problem in an edge computing system with interconnected access points. Furthermore, we assume both end devices and access points are rational, which only care about their own benefits. To solve the joint problem, we first formulate it as a multi-leader multi-follower Stackelberg game, and rigorously prove the existence of a Stackelberg equilibrium. Then, we propose two algorithms for task offloading and dispatching, respectively. Extensive simulations are conducted to show the superiority of our proposed approach. We also demonstrate that an upper bound with a constant approximation ratio is achieved by our approach.},
  archive      = {J_TCC},
  author       = {Tong Liu and Dongyu Guo and Qichao Xu and Honghao Gao and Yanmin Zhu and Yuanyuan Yang},
  doi          = {10.1109/TCC.2023.3280170},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3262-3273},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint task offloading and dispatching for MEC with rational mobile devices and edge nodes},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attacks and improvement of unlinkability of biometric
template protection scheme based on bloom filters. <em>TCC</em>,
<em>11</em>(3), 3251–3261. (<a
href="https://doi.org/10.1109/TCC.2023.3276971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric technologies are being prominently used everywhere. However, the leakage of biometric information can pose a serious security risk, making the protection of biometric templates particularly important and receiving more attention. Rathgeb et al. first proposed the cancelable biometric technology based on Bloom filters in 2013. Bloom filter-based biometrics offer the advantages of alignment-free, fast recognition and high accuracy. An ideal biometric system should also be irreversibility and unlinkability. In this paper, firstly, we propose a reverse reconstruction attack. The biometric data reconstructed from Bloom filters have some strong statistical correlation with the original biometric data, which proves that the original scheme has the linkability defect. Experiments show that for the original Bloom filter-based biometric template protection scheme, we can judge whether two different biometric templates belong to the same user with a success probability of 71.0\%. Secondly, to remedy above defect, we construct a structure-preserving encryption scheme, i.e., the encrypted feature template maintains the structure and length of the original template, making it impossible for an attacker to reconstruct meaningful data from Bloom filter. Finally, an improved biometric template protection scheme based on Bloom filters is proposed by introducing the proposed encryption. Attack experiment shows that the improved scheme can effectively resist the reverse reconstruction attack, the success probability of discrimination is reduced to 50\%, which is the same as the probability of random guessing. Performance evaluation shows that the proposed scheme maintains the biometric performance of the original system and the unprotected system.},
  archive      = {J_TCC},
  author       = {Tanping Zhou and Dong Chen and Wenchao Liu and Xiaoyuan Yang},
  doi          = {10.1109/TCC.2023.3276971},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3251-3261},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Attacks and improvement of unlinkability of biometric template protection scheme based on bloom filters},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reestablishing page placement mechanisms for nested
virtualization. <em>TCC</em>, <em>11</em>(3), 3239–3250. (<a
href="https://doi.org/10.1109/TCC.2023.3276368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Page placement mechanisms have long been used to reduce cache conflict misses. They become more important in clouds where the emerging way-based cache partitioning is used for better workload isolation but at a cost of increased cache conflicts. However, page placement mechanisms become ineffective in virtualized environments, such as clouds, because the real locations of memory pages (i.e., their host physical addresses) are hidden from guest OSs. The paper proposes xPlace as a solution to reestablish page placement mechanisms under the nested virtualization configuration. To keep high portability and low overhead, xPlace follows an approach that creates a synergy between the host and guest VMs, such that the page placement mechanism inside each guest VM becomes effective even if its page placement decisions are made based on the guest physical addresses of memory pages. The paper addresses the technical issues for implementing this approach in the nested virtualization setting, particularly how to create the synergy with the obstacle created by guest hypervisors sitting between the host and guest VMs. Evaluation based on the prototype implementation and diverse real world applications shows that xPlace can greatly reduce cache conflicts and improve application performance in the nested environment.},
  archive      = {J_TCC},
  author       = {Xiaowei Shang and Weiwei Jia and Jianchen Shan and Xiaoning Ding and Cristian Borcea},
  doi          = {10.1109/TCC.2023.3276368},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3239-3250},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Reestablishing page placement mechanisms for nested virtualization},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards an efficient SIMD virtual radio access network
(vRAN) and edge cloud system. <em>TCC</em>, <em>11</em>(3), 3226–3238.
(<a href="https://doi.org/10.1109/TCC.2023.3275576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, virtual Radio Access Network (vRAN) plays a vital role in today&#39;s mobile edge system for its better support for latency-sensitive applications. However, our characterization of vRAN on modern processors depicts a frustrating picture of Single-Instruction Multi-Data (SIMD) acceleration. Specifically, the existing data arrangement processes cannot efficiently utilize the ports in modern processors, which leads to high backend bound and fails to saturate the memory bandwidth between registers and the L1 cache. To tackle the issue, we thoroughly examine the state-of-the-art CPU architecture and observe the idle ports which could be utilized by the process. Motivated by this observation, we propose an “Arithmetic Ports Consciousness Mechanism” (APCM) utilizing these idle ports to eliminate the backend bound and saturate the memory bandwidth. The APCM decreases the data arrangement&#39;s backend bound from 45 $\%$ to 3 $\%$ and promotes its memory bandwidth utilization by 4X-16X. Moreover, we illustrate that the APCM can be utilized to promote the performance of typical mobile edge applications such as network routing, image processing, and AI applications. The CPU time of the data arrangement process time of the selected typical mobile edge applications can be reduced by 55 $\%$ - 95 $\%$ when utilizing the proposed mechanism.},
  archive      = {J_TCC},
  author       = {Jianda Wang and Zhen Wang and Yang Hu},
  doi          = {10.1109/TCC.2023.3275576},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3226-3238},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Towards an efficient SIMD virtual radio access network (vRAN) and edge cloud system},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards strong privacy protection for association rule
mining and query in the cloud. <em>TCC</em>, <em>11</em>(3), 3211–3225.
(<a href="https://doi.org/10.1109/TCC.2023.3269510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently mining frequent itemsets and association rules on the encrypted outsourced data remains a great challenge for the time-consuming ciphertext computations. Nowadays, it has been not well addressed for privacy-preserving frequent itemsets and association rule mining schemes with mining efficiency, dataset, and query confidentiality simultaneously. In this paper, we investigate the study of privacy issues on frequent itemset mining and association rule mining on outsourced data in a two-cloud model, where the data are encrypted and outsourced by multiple owners holding different public keys. We develop several secure computation protocols based on additively homomorphic cryptosystem and additive secret sharing, which enable the clouds could securely mine the frequent itemsets and association rules. Furthermore, we also design two kinds of frequent itemset and association rule query service, i.e., service customers query the cloud-mined results, and service customers query with their own decided threshold. The proposed scheme not only supports the mining process on the data encrypted by multiple public keys without compromising the security of the datasets, query data and query results, but also offline users. In addition, the experimental results show that our query scheme is much more efficient than the state-of-the-art work.},
  archive      = {J_TCC},
  author       = {Lin Liu and Jinshu Su and Ximeng Liu and Rongmao Chen and Xinyi Huang and Guang Kou and Shaojing Fu},
  doi          = {10.1109/TCC.2023.3269510},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3211-3225},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Towards strong privacy protection for association rule mining and query in the cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fully hybrid algorithm for deadline constrained workflow
scheduling in clouds. <em>TCC</em>, <em>11</em>(3), 3197–3210. (<a
href="https://doi.org/10.1109/TCC.2023.3269144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the migration of more and more workflows to clouds, the workflow scheduling in clouds (WSC) becomes a critical problem. Although many algorithms have been presented for WSC, there is still room and need for improvement. This article formulates WSC as a constrained optimization problem that optimizes workflow execution cost within a workflow deadline constraint and proposes a fully hybrid workflow scheduling algorithm, called HPCP-PSO to solve it. Unlike previous works, HPCP-PSO is based on the repeated and alternated execution of two different methods, namely, the heuristic IaaS Cloud Partial Critical Paths (IC-PCP) and meta-heuristic Particle Swarm Optimization (PSO). Moreover, HPCP-PSO incorporates with two novel designs: 1) a new solution encoding strategy not only to sufficiently embody the elasticity of cloud resources, but also to reflect the scheduling relationship between assigned and unassigned tasks; 2) a solution repair strategy on each infeasible lease process to utilize a user-defined deadline more effectively and enhance the solution efficiency of the algorithm. Extensive experiments are conducted on four real-world scientific workflows and the results show that compared with IC-PCP, PSO, and HGSA, the proposed algorithm outperforms them on average by 35.83\%, 70.53\%, and 87.71\% in terms of workflow execution cost.},
  archive      = {J_TCC},
  author       = {Liwen Yang and Yuanqing Xia and Lingjuan Ye and Runze Gao and Yufeng Zhan},
  doi          = {10.1109/TCC.2023.3269144},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3197-3210},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A fully hybrid algorithm for deadline constrained workflow scheduling in clouds},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Algebraic signature-based public data integrity batch
verification for cloud-IoT. <em>TCC</em>, <em>11</em>(3), 3184–3196. (<a
href="https://doi.org/10.1109/TCC.2023.3266593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Internet of Things, the related data are growing explosively. However, IoT devices have limited storage and computing capabilities so that they cannot deal with massive data storage and computing locally. The integration of IoT and cloud is regarded as an effective solution to the above issue, i.e., IoT devices outsource collected data to cloud to enjoy powerful storage and computing resources. Because the data stored in cloud are out of the control of IoT users, some security risks need to be addressed in advance. In this article, we propose a public data integrity verification scheme, called AIVCI, to check the data integrity for Cloud-IoT scenarios. First, based on algebraic signature and homomorphic hash function, AIVCI can efficiently complete data auditing. Second, AIVCI adopts blind technology to prevent the privacy leakage of IoT data and further protect the privacy of IoT users. Third, batch auditing is implemented to improve auditing efficiency and meet realistic demands for Cloud-IoT scenarios. And a new data structure named Improved Divide and Conquer Table (ID&amp;CT) is designed to realize efficient data dynamics. Finally, the security and performance analyses demonstrate that AIVCI is more secure and efficient.},
  archive      = {J_TCC},
  author       = {Yanping Li and Zirui Li and Bo Yang and Yong Ding},
  doi          = {10.1109/TCC.2023.3266593},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3184-3196},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Algebraic signature-based public data integrity batch verification for cloud-IoT},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SR-PEKS: Subversion-resistant public key encryption with
keyword search. <em>TCC</em>, <em>11</em>(3), 3168–3183. (<a
href="https://doi.org/10.1109/TCC.2023.3266459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public key encryption with keyword search (PEKS) provides secure searchable data encryption in cloud storage. Users can outsource encrypted data and keywords to a cloud server, and search target one without disclosing sensitive information. To achieve resistance against off-line keyword guessing attacks, existing practical PEKS schemes employ independent key server(s) to assist users in producing keywords to be encrypted (called server-derived keywords) in an online manner. In this article, we analyze server-aided PEKS schemes and reveal a potential threat: vulnerability against subversion attacks, where algorithms in server-aided PEKS might be maliciously implemented to undermine security. In a subverted encryption implementation, a subliminal channel is established to control randomness generation such that biased ciphertexts covertly leak plaintext information. We further present a specific subversion attack against generation of server-derived keywords to violate keywords’ confidentiality. To address these issues, we propose SR-PEKS, a subversion-resistant PEKS scheme based on cryptographic reverse firewalls (CRF). In SR-PEKS, CRF sanitizes messages transmitted in server-derived keyword generation to resist the presented subversion attack. CRF also participates in a collaborative randomness generation protocol to yield unbiased randomness for encryption, thereby eliminating the subliminal channel. Provable security and high efficiency of SR-PEKS are demonstrated by comprehensive analyses and performance evaluations.},
  archive      = {J_TCC},
  author       = {Changsong Jiang and Chunxiang Xu and Zhao Zhang and Kefei Chen},
  doi          = {10.1109/TCC.2023.3266459},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3168-3183},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SR-PEKS: Subversion-resistant public key encryption with keyword search},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Dynamic GPU scheduling with multi-resource awareness and
live migration support. <em>TCC</em>, <em>11</em>(3), 3153–3167. (<a
href="https://doi.org/10.1109/TCC.2023.3264242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clouds and data centers, GPU servers with multiple GPUs are widely deployed. Current state-of-the-art GPU scheduling policies are “static” in assigning applications to different GPUs. These policies usually ignore the dynamics of the GPU utilization and are often inaccurate in estimating resource demand before assigning/running applications, so there is a large opportunity to further balance the loads and improve GPU utilization. Based on CUDA (Compute Unified Device Architecture), we develop a runtime system called ${\sf DCUDA}$ which supports “dynamic” scheduling of running applications between multiple GPUs. In particular, ${\sf DCUDA}$ takes into consideration multidimensional resources, including computing cores, memory usage, and energy consumption. It first provides a real-time and lightweight method to accurately monitor the resource demand of applications and GPU utilization. Furthermore, it provides a universal migration facility to migrate “running applications” between GPUs with negligible overhead. More importantly, ${\sf DCUDA}$ transparently supports all CUDA applications without changing their source code. Experiments with our prototype system show that ${\sf DCUDA}$ can reduce 78.3\% of overloaded time of GPUs on average. As a result, for different workloads consisting of a wide range of applications we studied, ${\sf DCUDA}$ can reduce the average execution time of general applications by up to 42.1\%, and even up to 67\% for memory-intensive tasks. Besides, ${\sf DCUDA}$ also reduces 13.3\% of energy in light-load scenarios.},
  archive      = {J_TCC},
  author       = {Xiaoyang Wang and Yongkun Li and Fan Guo and Yinlong Xu and John C. S. Lui},
  doi          = {10.1109/TCC.2023.3264242},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3153-3167},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic GPU scheduling with multi-resource awareness and live migration support},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint controller placement and control-service connection in
hybrid-band control. <em>TCC</em>, <em>11</em>(3), 3139–3152. (<a
href="https://doi.org/10.1109/TCC.2023.3264220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By separating the forwarding and control planes, Software-Defined Networking (SDN) facilitates flexible traffic routing and network management for a service network. Because of the impact of controller deployment on message transmission distances and network latency, controller placement problems have drawn many researchers’ attention. However, assumptions in most existing research that all control packets are either transmitted in the service network (i.e., in-band control) or through predetermined control-service connection (i.e., out-of-band control) are not reasonable due to bandwidth resources occupation on the service network or high construction costs. In this paper, we are the first to jointly discuss the controller placement and control-service connection problem for latency minimization in the hybrid-band control mode, which is essentially a bi-level programming optimization problem. Specifically, we introduce auxiliary variables to simplify the above NP-hard problem. Next, Generalized Benders decomposition is used to obtain an optimal solution in theory. In addition, we propose a time-efficient fireworks algorithm with a little latency increment for large-scale networks. Extensive evaluations show that the two proposed algorithms accomplish the desired objectives and respectively achieve up to 35\% and 25\% latency decrement than greedy algorithms.},
  archive      = {J_TCC},
  author       = {Xuewen Dong and Lingtao Xue and Zhiwei Zhang and Yushu Zhang and Teng Li and Zhichao You and Yulong Shen},
  doi          = {10.1109/TCC.2023.3264220},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3139-3152},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint controller placement and control-service connection in hybrid-band control},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PM<span class="math inline"><sup>2</sup></span><!-- -->2VE:
Power metering model for virtualization environments in cloud data
centers. <em>TCC</em>, <em>11</em>(3), 3126–3138. (<a
href="https://doi.org/10.1109/TCC.2023.3262648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtualization technologies provide solutions for cloud computing. Virtual resource scheduling is a crucial task in data centers, and the power consumption of virtual resources is a critical foundation of virtualization scheduling. Containers are the smallest unit of virtual resource scheduling and migration. Although many practical models for estimating the power consumption of virtual machines (VMs) have been proposed, few power estimation models of containers have been put forth. In this article, we propose a fast-training piecewise regression model based on a decision tree for VM power metering and estimate the power of containers configured on the VM by treating the container as a group of processes on the VM. We select appropriate features from the collected metrics of VMs/containers to help our model fit the nonlinear relationship between power and features well. Besides, we optimize the leaf nodes of the regression tree, realizing the effective power metering of virtualization environments. We evaluate the proposed model on 13 tasks in PARSEC and compare it with several commonly used models in data centers. The experimental results prove the effectiveness of the proposed model, and the estimated power of containers is in line with expectations.},
  archive      = {J_TCC},
  author       = {Ziyu Shen and Xusheng Zhang and Zheng Liu and Yun Li},
  doi          = {10.1109/TCC.2023.3262648},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3126-3138},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PM$^{2}$2VE: Power metering model for virtualization environments in cloud data centers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DNN surgery: Accelerating DNN inference on the edge through
layer partitioning. <em>TCC</em>, <em>11</em>(3), 3111–3125. (<a
href="https://doi.org/10.1109/TCC.2023.3258982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep neural networks have substantially improved the accuracy and speed of various intelligent applications. Nevertheless, one obstacle is that DNN inference imposes a heavy computation burden on end devices, but offloading inference tasks to the cloud causes a large volume of data transmission. Motivated by the fact that the data size of some intermediate DNN layers is significantly smaller than that of raw input data, we designed the DNN surgery, which allows partitioned DNN to be processed at both the edge and cloud while limiting the data transmission. The challenge is twofold: (1) Network dynamics substantially influence the performance of DNN partition, and (2) State-of-the-art DNNs are characterized by a directed acyclic graph rather than a chain, so that partition is incredibly complicated. To solve the issues, We design a Dynamic Adaptive DNN Surgery(DADS) scheme, which optimally partitions the DNN under different network conditions. We also study the partition problem under the cost-constrained system, where the resource of the cloud for inference is limited. Then, a real-world prototype based on the selif-driving car video dataset is implemented, showing that compared with current approaches, DNN surgery can improve latency up to 6.45 times and improve throughput up to 8.31 times. We further evaluate DNN surgery through two case studies where we use DNN surgery to support an indoor intrusion detection application and a campus traffic monitor application, and DNN surgery shows consistently high throughput and low latency.},
  archive      = {J_TCC},
  author       = {Huanghuang Liang and Qianlong Sang and Chuang Hu and Dazhao Cheng and Xiaobo Zhou and Dan Wang and Wei Bao and Yu Wang},
  doi          = {10.1109/TCC.2023.3258982},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3111-3125},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {DNN surgery: Accelerating DNN inference on the edge through layer partitioning},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-carbon power-to-hydrogen integrated residential system
over a hybrid cloud framework. <em>TCC</em>, <em>11</em>(3), 3099–3110.
(<a href="https://doi.org/10.1109/TCC.2023.3257995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study pioneers in proposing a hybrid cloud-based framework for designing a totally stand-alone, green residential house, stable over load variations and fluctuations of renewable energy sources (RES). The framework uses wind turbine (WT) and photovoltaic panels (PV) as the main power supplies, while employing a fuel cell (FC), fed with an electrolyzer, as the secondary source of energy. A battery is also used, which together with the FC and electrolyzer, constitute the compensation system in the proposed framework. Taking into account the various types of residential electrical loads, including an electrical vehicle (EV), and applying a deep learning method, the proposed framework makes the day-ahead scheduling of all components in the house based on the forecasted profile of load demand and the energy generated by the RES. The compensation system comes into use to balance the real-time scheduling error caused by the uncertainties of the main sources of power. To ascertain the practicality of the proposed framework for real-life implementation, it is examined on a residential house considering components with authentic technical features. The real-time operation of the suggested residential system is also tested on the SpeadGoat real-time simulator, whose results corroborate the practicability of both the real-time and day-ahead operation of the proposed framework.},
  archive      = {J_TCC},
  author       = {Seyed Ali Mohammad Tajalli and Seyede Zahra Tajalli and Maryam Homayounzadeh and Mohammad-Hassan Khooban},
  doi          = {10.1109/TCC.2023.3257995},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3099-3110},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Zero-carbon power-to-hydrogen integrated residential system over a hybrid cloud framework},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reversible data hiding in shared images based on syndrome
decoding and homomorphism. <em>TCC</em>, <em>11</em>(3), 3085–3098. (<a
href="https://doi.org/10.1109/TCC.2023.3259478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible Data Hiding in Encrypted Images (RDHEI) has drawn increasing concern in multimedia cloud computing scenarios. It embeds secret message into the encrypted carrier while preserving the confidentiality of the image. However, most RDHEI schemes have only one hider and one image carrier which are not efficient for the distributed system with multiple participants. Moreover, if the stego-image is lost, the cover image and the embedded data cannot be restored. To solve above issues, this paper proposes Reversible Data Hiding in Shared Images based on syndrome decoding and homomorphism (RDHSI). In RDHSI, the cover image and secret data are distributed using secret sharing. Multiple data hiders embed shared data into the shared images based on the Syndrome Decoding of Hamming codes and the additive homomorphism of the polynomial. On the receiver side, the lossless cover image and secret data are obtained. The reversibility and privacy preserving of the cover image, the complete extraction of secret data, and the fault-tolerance of the proposed schemes are achieved. The experimental results show that PSNR of the marked image is above 54 dB at the embedding rate of 0.43 bpp . In order to improve the payload of secret data, the Modified RDHSI is further proposed. Generally, the proposed schemes are secure, effective and fault-tolerant.},
  archive      = {J_TCC},
  author       = {Lizhi Xiong and Xiao Han and Ching-Nung Yang and Xinpeng Zhang},
  doi          = {10.1109/TCC.2023.3259478},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3085-3098},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Reversible data hiding in shared images based on syndrome decoding and homomorphism},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FlowPinpoint: Localizing anomalies in cloud-client services
for cloud providers. <em>TCC</em>, <em>11</em>(3), 3070–3084. (<a
href="https://doi.org/10.1109/TCC.2023.3257162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For public cloud providers, it is of great significance to maintain the availability of their cloud services, which requires efficient anomaly diagnosis and recovery. To achieve such properties, the first step is to localize the anomalies, i.e., determining where they happen in the network path of cloud-client services. We propose FlowPinpoint to perform anomaly localization for cloud providers. FlowPinpoint collects statistics of each network flow at the cloud network gateways (i.e., gateway flowlog), where the collected data can reflect the information from both the cloud side and the Internet side. Aggregation and association are conducted on the datacenter-scale gateway flowlogs by Alibaba&#39;s big data computing platform. In order to preclude the disturbance of anomaly-unrelated flowlogs, a two-layer filter is proposed which consists of an indicator-based filter and an isolation forest filter. Finally, the anomaly localization analyzer classifies the flowlogs and determines whether the anomaly is inside the cloud network or not according to the classification results. FlowPinpoint is implemented and tested in the production environment of Alibaba Cloud, and it correctly localizes 1 anomaly inside the cloud and 6 anomalies on the Internet over 4 months.},
  archive      = {J_TCC},
  author       = {Ruopeng Geng and Chongrong Fang and Shiyang Guo and Daxiang Kang and Biao Lyu and Shunmin Zhu and Peng Cheng},
  doi          = {10.1109/TCC.2023.3257162},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3070-3084},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {FlowPinpoint: Localizing anomalies in cloud-client services for cloud providers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HFedMS: Heterogeneous federated learning with memorable data
semantics in industrial metaverse. <em>TCC</em>, <em>11</em>(3),
3055–3069. (<a href="https://doi.org/10.1109/TCC.2023.3254587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL), as a rapidly evolving privacy-preserving collaborative machine learning paradigm, is a promising approach to enable edge intelligence in the emerging Industrial Metaverse. Even though many successful use cases have proved the feasibility of FL in theory, in the industrial practice of Metaverse, the problems of non-independent and identically distributed (non-i.i.d.) data, learning forgetting caused by streaming industrial data, and scarce communication bandwidth remain key barriers to realize practical FL. Facing the above three challenges simultaneously, this article presents a high-performance and efficient system named HFedMS for incorporating practical FL into Industrial Metaverse. HFedMS reduces data heterogeneity through dynamic grouping and training mode conversion ( Dynamic Sequential-to-Parallel Training, STP ). Then, it compensates for the forgotten knowledge by fusing compressed historical data semantics and calibrates classifier parameters ( Semantic Compression and Compensation, SCC ). Finally, the network parameters of the feature extractor and classifier are synchronized in different frequencies ( Layer-wise Alternative Synchronization Protocol, LASP ) to reduce communication costs. These techniques make FL more adaptable to the heterogeneous streaming data continuously generated by industrial equipment, and are also more efficient in communication than traditional methods (e.g., Federated Averaging). Extensive experiments have been conducted on the streamed non-i.i.d. FEMNIST dataset using 368 simulated devices. Numerical results show that HFedMS improves the classification accuracy by at least 6.4\% compared with 8 benchmarks and saves both the overall runtime and transfer bytes by up to 98\%, proving its superiority in precision and efficiency.},
  archive      = {J_TCC},
  author       = {Shenglai Zeng and Zonghang Li and Hongfang Yu and Zhihao Zhang and Long Luo and Bo Li and Dusit Niyato},
  doi          = {10.1109/TCC.2023.3254587},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3055-3069},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {HFedMS: Heterogeneous federated learning with memorable data semantics in industrial metaverse},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A state-aware method for flows with fairness on NVMe SSDs
with load balance. <em>TCC</em>, <em>11</em>(3), 3040–3054. (<a
href="https://doi.org/10.1109/TCC.2023.3253864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, solid-state drives (SSDs) have become the best choice of storage devices because of its brilliant advantages such as small size, low-power consumption, shake resistance, fast access and non-volatility, when compared with hard-disk drives (HDDs). More and more scenarios adopt a multi-SSD architecture to improve performance and expand storage capacity, such as cloud services, database centers, distributed systems and virtualized environments. When multiple users (flows) are competing for shared multiple SSDs concurrently, if the multi-SSD architecture lacks a fairness strategy among multiple users, a user that takes up more resources can affect other users. Meanwhile, if the multi-SSD architecture lacks a load-balance strategy among multiple shared SSDs, some specific SSDs may receive too many I/O requests to degrade the performance and shorten the lifespan. Therefore, we will propose a state-aware method to consider flows with fairness on NVMe SSDs with load balance. According to experimental results, we can show that the proposed method can improve the fairness by 1.15x $\sim$ 1.19x and the load balance by 1.18x $\sim$ 3.15x on average, when compared to other methods.},
  archive      = {J_TCC},
  author       = {Chin-Hsien Wu and Liang-Ting Chen and Ren-Jhen Hsu and Jian-Yu Dai},
  doi          = {10.1109/TCC.2023.3253864},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3040-3054},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A state-aware method for flows with fairness on NVMe SSDs with load balance},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint task offloading and resource allocation for
device-edge-cloud collaboration with subtask dependencies. <em>TCC</em>,
<em>11</em>(3), 3027–3039. (<a
href="https://doi.org/10.1109/TCC.2023.3251561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With more computational intensive applications deployed involving mobile edge computing (MEC), the collaboration among mobile devices, edge and cloud servers becomes an effective mechanism to fully utilize all available distributed computing resources. However, two main challenges have yet to be addressed to enable this three-way collaboration for securing necessary computational resources and further guaranteeing the quality of service (QoS) of task handling. The first challenge is related to the partitioning of an application task into several dependent subtasks and schedule them among the collaborating device-edge-cloud (DEC). The second is focused on the allocation of necessary computing resources of device, edge and cloud servers for effective subtask handling. To this end, we study the joint task offloading and resource allocation for DEC collaboration in this paper by formulating a new optimization problem with the objective of minimizing the task handling latency. To solve this problem, we decompose the original problem into two subproblems, which include the first one of calculating the optimal task partitioning ratio by mathematical analytical method, as well as the second on using the Lagrangian dual (LD) method for obtaining the optimal task offloading and resource allocation policy. Finally, we conduct simulation experiments on a real-life dataset obtained from the central business district (CBD) of Melbourne, Australia, and the experimental results validate the efficacy of our approach in minimizing latency.},
  archive      = {J_TCC},
  author       = {Fangzheng Liu and Jiwei Huang and Xianbin Wang},
  doi          = {10.1109/TCC.2023.3251561},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3027-3039},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint task offloading and resource allocation for device-edge-cloud collaboration with subtask dependencies},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SECMACE+: Upscaling pseudonymous authentication for large
mobile systems. <em>TCC</em>, <em>11</em>(3), 3009–3026. (<a
href="https://doi.org/10.1109/TCC.2023.3250584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The central building block of secure and privacy-preserving Vehicular Communication (VC) systems is a Vehicular Public Key Infrastructure (VPKI), which provides vehicles with multiple anonymized credentials, termed pseudonyms . These pseudonyms are used to ensure VC message authenticity and integrity while preserving vehicle (thus passenger) privacy. In the light of emerging large-scale multi-domain VC environments, the efficiency of the VPKI and, more broadly, its scalability are paramount. By the same token, preventing misuse of the credentials, in particular, Sybil-based misbehavior, and managing “honest-but-curious” VPKI entities are other facets of a challenging problem. In this paper, we leverage the state-of-the-art VPKI system and enhance its functionality towards a highly-available, dynamically-scalable, and resilient design; this ensures that the system remains operational in the presence of benign failures or resource depletion attacks, and that it dynamically scales out , or possibly scales in , according to request arrival rates. Our full-blown implementation on the Google Cloud Platform shows that deploying large-scale and efficient VPKI can be cost-effective: the processing latency to issue 100 pseudonyms is approximately 56 ms. More so, our experiments show that our VPKI system dynamically scales out or scales in according to the rate of pseudonyms requests. We formally assess the achieved security and privacy properties for the credential acquisition process. Overall, our scheme is a comprehensive solution that complements standards and can catalyze the deployment of secure and privacy-protecting VC systems.},
  archive      = {J_TCC},
  author       = {Mohammad Khodaei and Hamid Noroozi and Panos Papadimitratos},
  doi          = {10.1109/TCC.2023.3250584},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {3009-3026},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SECMACE+: Upscaling pseudonymous authentication for large mobile systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-user collusion-resistant searchable encryption for
cloud storage. <em>TCC</em>, <em>11</em>(3), 2993–3008. (<a
href="https://doi.org/10.1109/TCC.2023.3249189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continued development of cloud computing requires technologies protecting users’ data privacy even from the cloud providers themselves, such as Multi-user searchable encryption. It allows data owners to selectively enable users to perform keyword searches over their encrypted data stored at a cloud server. For privacy purposes, it is important to limit what an adversarial server can infer about the encrypted data, even if it colludes with some users. Clearly, in this case it can learn the content of data shared with these “corrupted” users, however, it is important to ensure this collusion does not reveal information about parts of the dataset that are only shared with “uncorrupted” users via cross-user leakage. In this work, we propose three novel multi-user searchable encryption schemes eliminating cross-user leakage. Compared to previous ones, our first two schemes are the first to achieve asymptotically optimal search time . Our third scheme achieves minimal user storage and forward privacy with respect to data sharing, but slightly slower search performance. We formally prove the security of our schemes under reasonable assumptions. Moreover, we implement them for textual documents and tabular databases and evaluate their computation and communication performance with encouraging results.},
  archive      = {J_TCC},
  author       = {Yun Wang and Dimitrios Papadopoulos},
  doi          = {10.1109/TCC.2023.3249189},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2993-3008},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-user collusion-resistant searchable encryption for cloud storage},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient identity-based encryption with equality test in
cloud computing. <em>TCC</em>, <em>11</em>(3), 2983–2992. (<a
href="https://doi.org/10.1109/TCC.2023.3248308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identity-based encryption with equality test (IBEET) provides a feasible way for cloud server partitioning or searching on ciphertexts in the cloud. In that case, the server can judge if two different ciphertexts encrypt the same plaintexts or not. In response to threats posed by quantum computers, lattice-based IBEET schemes have been proposed to make cloud service post-quantum secure. However, those schemes are inefficient and can hardly meet the needs of resource-constrained devices. In this article, an efficient post-quantum IBEET scheme is introduced, which achieves testability by embedding the hash value of plaintext into testing trapdoor instead of encrypting it directly and doubling the size of ciphertext. We also prove that, with the Learning With Errors (LWE) problem assumption, the new scheme is one-way secure against selective identity and chosen ciphertext attacks ( OW-sID-CCA ) in quantum secure model. Furthermore, we evaluate the performance of the new construction and demonstrate its efficiency by showing that it only costs about half storage comparing with other lattice-based IBEET schemes. The execution time of encryption and decryption phrases in the new scheme reduce by 50\%, while the computational cost in test algorithm keeps the same. Therefore, the new proposed IBEET is much more practical for working in post-quantum cloud computing scenarios.},
  archive      = {J_TCC},
  author       = {Zhichao Yang and Debiao He and Longjiang Qu and Qing Ye},
  doi          = {10.1109/TCC.2023.3248308},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2983-2992},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An efficient identity-based encryption with equality test in cloud computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reproducible and portable big data analytics in the cloud.
<em>TCC</em>, <em>11</em>(3), 2966–2982. (<a
href="https://doi.org/10.1109/TCC.2023.3245081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has become a major approach to help reproduce computational experiments. Yet there are still two main difficulties in reproducing batch based Big Data analytics (including descriptive and predictive analytics) in the cloud. The first is how to automate end-to-end scalable execution of analytics including distributed environment provisioning, analytics pipeline description, parallel execution, and resource termination. The second is that an application developed for one cloud is difficult to be reproduced in another cloud, a.k.a. vendor lock-in problem. To tackle these problems, we leverage serverless computing and containerization techniques for automated scalable execution and reproducibility, and utilize the adapter design pattern to enable application portability and reproducibility across different clouds. We propose and develop an open-source toolkit that supports 1) fully automated end-to-end execution and reproduction via a single command, 2) automated data and configuration storage for each execution, 3) flexible client modes based on user preferences, 4) execution history query, and 5) simple reproduction of existing executions in the same environment or a different environment. We did extensive experiments on both AWS and Azure using four Big Data analytics applications that run on virtual CPU/GPU clusters. The experiments show our toolkit can achieve good execution performance, scalability, and efficient reproducibility for cloud-based Big Data analytics.},
  archive      = {J_TCC},
  author       = {Xin Wang and Pei Guo and Xingyan Li and Aryya Gangopadhyay and Carl E. Busart and Jade Freeman and Jianwu Wang},
  doi          = {10.1109/TCC.2023.3245081},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2966-2982},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Reproducible and portable big data analytics in the cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A prediction based replica selection strategy for reducing
tail latency in geo-distributed systems. <em>TCC</em>, <em>11</em>(3),
2954–2965. (<a href="https://doi.org/10.1109/TCC.2023.3244203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of modern applications in geo-distributed systems results in performance fluctuation which is a consequence of long-tail latency. To deliver high-quality services these applications always strive to adapt to the changing situation and an appropriate replica selection strategy is one efficient way to achieve this. Several replica selection strategies have already been developed but none of them are efficient enough to reduce tail latency and adapt to the dynamic environment of the geo-distributed systems. In this article, we present the design and implementation of a prediction based replica selection strategy for reducing tail latency in geo-distributed systems. We have meticulously designed the proposed strategy to adapt to the dynamic behavior of the distributed system. For evaluating its effectiveness in reducing tail latency and adapting to the dynamic behavior of the geo-distributed systems we perform some extensive experiments in a 15 nodes Cassandra cluster that is deployed on Amazon EC2 over 5 geographical regions. For generating test datasets and workloads we use industry-standard Yahoo Cloud Serving Benchmark (YCSB). Our experimental results show that the proposed strategy not only reduces tail latency but also increases the overall throughput of the systems.},
  archive      = {J_TCC},
  author       = {Santa Maria Shithil and Muhammad Abdullah Adnan},
  doi          = {10.1109/TCC.2023.3244203},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2954-2965},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A prediction based replica selection strategy for reducing tail latency in geo-distributed systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SvTPM: SGX-based virtual trusted platform modules for cloud
computing. <em>TCC</em>, <em>11</em>(3), 2936–2953. (<a
href="https://doi.org/10.1109/TCC.2023.3243891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Trusted Platform Modules (vTPMs) are widely used in commercial cloud platforms (e.g., VMware Cloud, Google Cloud, and Microsoft Azure) to provide virtual root-of-trust and security services for virtual machines. Unfortunately, current state-of-the-art vTPM implementations for cloud computing cannot provide strong protection for vTPMs at run-time and suffer from poor performance under binding vTPMs to a physical TPM. In this paper, we propose SvTPM, an SGX-based virtual trusted platform module, which provides complete life cycle protection of vTPMs in the cloud and does not rely on the physical TPM. SvTPM provides strong isolation protection so malicious cloud tenants or even cloud administrators cannot access vTPM&#39;s private keys or any other sensitive data. In this paper, we implement a prototype of SvTPM, which identifies and solves a couple of critical security challenges for vTPM protection with SGX, such as NVRAM rollback attacks, NVRAM binding attacks, and vTPM rollback attacks. SvTPM also shows how to establish trust between vTPM and SGX Platform. Our performance evaluation shows that the NVRAM launch time of SvTPM is $1700\times$ faster than vTPM built upon hardware TPM. In TPM standard command evaluation, we find that SvTPM incurs negligible performance overhead while providing strong isolation and protection. To our knowledge, SvTPM is the first practical work to solve the critical security challenges of securing vTPM using SGX.},
  archive      = {J_TCC},
  author       = {Juan Wang and Jie Wang and Chengyang Fan and Fei Yan and Yueqiang Cheng and Yinqian Zhang and Wenhui Zhang and Mengda Yang and Hongxin Hu},
  doi          = {10.1109/TCC.2023.3243891},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2936-2953},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SvTPM: SGX-based virtual trusted platform modules for cloud computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient integrity checking scheme with full identity
anonymity for cloud data sharing. <em>TCC</em>, <em>11</em>(3),
2922–2935. (<a href="https://doi.org/10.1109/TCC.2023.3242140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud storage services can support data sharing for a group of users. However, the cloud server may lose some of the shared data when attacked. The integrity of stored data is a key issue for cloud storage. However, identity anonymity, an important problem in data integrity, has not been fully investigated yet. Furthermore, the existing schemes of data integrity checking are usually based on public key infrastructure and thus have to perform complex certificate management. In this paper, we propose an efficient integrity checking scheme to achieve full identity anonymity. Additionally, the proposed scheme can reduce the workloads of certificate management and simplify key management. Inspired by the idea of attribute-based signature, we devise a common predicate to manage the identities of legitimate users and disable illegal users from joining the data sharing. By adopting the technique of the monotone span program, the legitimate user can compute the valid authenticators of shared data for all attributes in the common predicate without the need of binding his attribute set to the shared data. Whereupon, the user&#39;s identity remains anonymous to all parties in the data sharing because he can share the data and authenticators without revealing his attribute set to other parties. The extensive experimental results demonstrate that the proposed scheme has less computational and communication overhead compared with the existing schemes while achieving full identity anonymity.},
  archive      = {J_TCC},
  author       = {Ran Ding and Yan Xu and Hong Zhong and Jie Cui and Geyong Min},
  doi          = {10.1109/TCC.2023.3242140},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2922-2935},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An efficient integrity checking scheme with full identity anonymity for cloud data sharing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Hybrid multiple access for network slicing aware mobile
edge computing. <em>TCC</em>, <em>11</em>(3), 2910–2921. (<a
href="https://doi.org/10.1109/TCC.2023.3234543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meeting huge traffic demand with resource constraints imposes a significant challenge for future generation wireless networks. In this work, we propose to utilize limited resources in a dense mobile edge computing (MEC) network to compute user equipment (UE) tasks through a novel hybrid multiple access (HYMA) scheme that employs both non-orthogonal multiple access (NOMA) and orthogonal multiple access (OMA). The main purpose of using HYMA is to reduce the co-channel interference incurred in NOMA by selectively deploying OMA while maintaining the required signal-to-interference ratio. We adopt partial offloading of computing tasks in the MEC network. We also employ network slicing to efficiently utilize the resources of the MEC network to meet different types of application requirements. We prioritize NOMA to increase spectral efficiency as well as energy efficiency. We first formulate a mixed integer nonlinear programming (MINLP) optimization problem to minimize the total energy consumption for both local computing and wireless transmission of the MEC network and propose an algorithm consisting of three parts (UE association, computing resource allocation, and wireless resource and uplink transmission power allocation) to solve the MINLP problem with less computational complexity. We demonstrate the viability of our solution via extensive simulations.},
  archive      = {J_TCC},
  author       = {Mohammad Arif Hossain and Nirwan Ansari},
  doi          = {10.1109/TCC.2023.3234543},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2910-2921},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Hybrid multiple access for network slicing aware mobile edge computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the protection of a high performance load balancer
against SYN attacks**this is an extended journal version of [2].
<em>TCC</em>, <em>11</em>(3), 2897–2909. (<a
href="https://doi.org/10.1109/TCC.2023.3234122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SYN flooding is a simple and effective denial-of-service attack. In this attack, many TCP SYN requests are sent to the targeted server, in an attempt to consume its resources and make it unresponsive to legitimate traffic. While SYN attacks have traditionally targeted web servers, they are also known to be very harmful to intermediate cloud devices, and in particular to stateful load balancers (LBs). Fighting against a SYN attack without negatively affecting legitimate connections is not easy, especially if the LB needs to perform frequent server pool updates during the attack, which is very likely since attacks can often last for many hours or even days. This paper is the first to propose LB schemes that guarantee high throughput of one million connections per second, while supporting a high pool update rate without breaking connections and fighting against a high rate SYN attack. Using an analysis and a proof of concept, we show that the LB can handle up to 10 million fake SYNs per second when the RTT is 10ms, and up to 5 million fake SYNs per second when the RTT is 20ms.},
  archive      = {J_TCC},
  author       = {Reuven Cohen and Matty Kadosh and Alan Lo and Qasem Sayah},
  doi          = {10.1109/TCC.2023.3234122},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2897-2909},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {On the protection of a high performance load balancer against SYN Attacks**This is an extended journal version of [2]},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A privacy-preserving JPEG image retrieval scheme using the
local markov feature and bag-of-words model in cloud computing.
<em>TCC</em>, <em>11</em>(3), 2885–2896. (<a
href="https://doi.org/10.1109/TCC.2022.3233421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of cloud computing attracts a great deal of image owners to upload their images to the cloud server to save the local storage. But privacy becomes a great concern to the owner. A forthright way is to encrypt the images before uploading, which, however, would obstruct the efficient usage of image, such as the Content-Based Image Retrieval (CBIR). In this paper, we propose a privacy-preserving JPEG image retrieval scheme. The image content is protected by a specially-designed image encryption method, which is compatible to JPEG compression and makes no expansion to the final JPEG files. Then, the encrypted JPEG files are uploaded to the cloud, and the cloud can directly extract the features from the encrypted JPEG files for searching similar images. Specifically, big-blocks are first assembled with adjacent 8×8 discrete cosine transform (DCT) coefficient blocks. Then, the big-blocks are permuted and the binary code of DCT coefficients are substituted, so as to disturb the content of image. After receiving the encrypted images, local Markov features are extracted from the encrypted big-blocks, and then the Bag-Of-Words (BOW) model is applied to construct a feature vector with these local features to represent the image, so as to provide the CBIR service to image owner. Experimental results and security analysis demonstrate the retrieval performance and security of our scheme.},
  archive      = {J_TCC},
  author       = {Peipeng Yu and Jian Tang and Zhihua Xia and Zhetao Li and Jian Weng},
  doi          = {10.1109/TCC.2022.3233421},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2885-2896},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A privacy-preserving JPEG image retrieval scheme using the local markov feature and bag-of-words model in cloud computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic combinatorial double auction model for cloud
resource allocation. <em>TCC</em>, <em>11</em>(3), 2873–2884. (<a
href="https://doi.org/10.1109/TCC.2022.3231249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the cloud market, we proposed a Dynamic Combinatorial Double Auction (DCDA) model to improve the social welfare and resource utilization. In the model, cloud-agents represent cloud service providers, and user-agents represent cloud users. They bid for various combinations of resources in a dynamic environment. To overcome the computational complexity of combinatorial auctions, we employed a greedy approximation method to solve the winner determination problem together with a truthful payment scheme. The proposed model is proven to be approximately efficient, incentive compatible, individually rational, and budget-balanced. Considering both parties’ interests and the relative scarcity of cloud resources, this model also ensures fairness and balances resource allocation.},
  archive      = {J_TCC},
  author       = {Qihui Li and Xiaohua Jia and Chuanhe Huang and Haizhou Bao},
  doi          = {10.1109/TCC.2022.3231249},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2873-2884},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A dynamic combinatorial double auction model for cloud resource allocation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DSE-RB: A privacy-preserving dynamic searchable encryption
framework on redactable blockchain. <em>TCC</em>, <em>11</em>(3),
2856–2872. (<a href="https://doi.org/10.1109/TCC.2022.3228867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of various applications of blockchain, blockchain-assisted searchable encryption technology has received wide attention as it can eliminate misbehaviours of malicious servers through the verification and incentive mechanism of blockchain. However, most existing solutions update the encrypted data by means of appending new transactions, which does not scale and wastes resources. In this paper, we explore the potential of redactable blockchain and propose a privacy-preserving dynamic searchable encryption framework (DSE-RB), which is a general scheme that guarantees reliable queries and updates on encrypted data. In particular, we first use transaction-level editing technology to achieve a more flexible update operation of encrypted data without additional transactions while avoiding the waste of storage on the chain. To better support practical applications, we use an index partition method to divide the traditional binary tree index into a plurality of sub-indexes and introduce the concept of polynomials to simplify the whole access control mechanism. We define the security model and conduct repeated experiments on real data sets to test the efficiency. Experimental results and theoretical analysis show the practicability and security of our scheme.},
  archive      = {J_TCC},
  author       = {Mingyue Li and Chunfu Jia and Ruizhong Du and Wei Shao and Guanxiong Ha},
  doi          = {10.1109/TCC.2022.3228867},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2856-2872},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {DSE-RB: A privacy-preserving dynamic searchable encryption framework on redactable blockchain},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anonymous aggregate fine-grained cloud data verification
system for smart health. <em>TCC</em>, <em>11</em>(3), 2839–2855. (<a
href="https://doi.org/10.1109/TCC.2022.3229269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of cloud computing and Internet of Things (IoT), smart health (s-health) is anticipated to enhance healthcare quality significantly. However, data integrity, user anonymity, and authentication concerns have not been adequately addressed in s-health. Remote data integrity checking (RDIC) and digital signature schemes have great potential to address these requirements. Nevertheless, the direct adoption of these schemes suffers from two flaws. First, they incur prohibitively high computation and communication overhead. Second, they leak sensitive health information about patients and do not provide complete anonymity. To address these issues, we introduce $\mathbf {A^{3}B}$ - $\mathbf {RDV}$ , an aggregate anonymous attribute-based remote data verification scheme. In $\mathbf {A^{3}B}$ - $\mathbf {RDV}$ , the integrity of an arbitrary number of cloud data files can be verified at once without downloading the whole data, thereby saving communication and computation resources. Moreover, in $\mathbf {A^{3}B}$ - $\mathbf {RDV}$ , data owners can be authenticated by performing highly efficient operations. Also, $\mathbf {A^{3}B}$ - $\mathbf {RDV}$ provides complete anonymity and supports dishonest-user traceability. We provide security definitions for $\mathbf {A^{3}B}$ - $\mathbf {RDV}$ and prove its security under the hardness assumption of the bilinear Diffie-Hellman (BDH) problem. Performance comparisons and experimental results indicate that $\mathbf {A^{3}B}$ - $\mathbf {RDV}$ is more efficient and expressive than state-of-the-art approaches.},
  archive      = {J_TCC},
  author       = {Mohammad Ali and Mohammad-Reza Sadeghi and Ximeng Liu and Athanasios V. Vasilakos},
  doi          = {10.1109/TCC.2022.3229269},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2839-2855},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Anonymous aggregate fine-grained cloud data verification system for smart health},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computing without borders: The way towards liquid computing.
<em>TCC</em>, <em>11</em>(3), 2820–2838. (<a
href="https://doi.org/10.1109/TCC.2022.3229163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the de-facto technological uniformity fostered by the cloud and edge computing paradigms, resource fragmentation across isolated clusters hinders the dynamism in application placement, leading to suboptimal performance and operational complexity. Building upon and extending these paradigms, we propose a novel approach envisioning a transparent continuum of resources and services on top of the underlying fragmented infrastructure, called liquid computing . Fully decentralized, multi-ownership-oriented and intent-driven, it enables an overarching abstraction for improved applications execution, while at the same time opening up for new scenarios, including resource sharing and brokering. Following the above vision, we present liqo , an open-source project that materializes this approach through the creation of dynamic and seamless Kubernetes multi-cluster topologies. Extensive experimental evaluations have shown its effectiveness in different contexts, both in terms of Kubernetes overhead and compared to other open-source alternatives.},
  archive      = {J_TCC},
  author       = {Marco Iorio and Fulvio Risso and Alex Palesandro and Leonardo Camiciotti and Antonio Manzalini},
  doi          = {10.1109/TCC.2022.3229163},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2820-2838},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Computing without borders: The way towards liquid computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secure similar sequence query over multi-source genomic data
on cloud. <em>TCC</em>, <em>11</em>(3), 2803–2819. (<a
href="https://doi.org/10.1109/TCC.2022.3228906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has been shown promising in enabling various analyses over large-scale genomic data integrated across multiple data sources. However, outsourcing data to remote cloud servers raises data-privacy concerns, therefore demands secure computing measures over the data analyzing process on the untrusted cloud servers. Due to the scale of genomic dataset and the length of each genomic sequence, it is challenging to evaluate data-analysis functions on outsourced genomic data securely and efficiently. In this work, we study the secure similar-sequence-query (SSQ) problem over outsourced genomic data. To address the challenges of security and efficiency, we propose a set of two-party computing protocols in mixed form , which combine secure secret sharing, garbled circuit, and partial homomorphic encryptions together and use them to jointly fulfill the secure SSQ function. Moreover, our scheme supports the fusion of genomic data from multiple data owners to generate a deduplicated-joint genomic dataset, therefore reduces the redundancy in the dataset. The performance improvements of our scheme are validated through extensive experiments on a commercial cloud platform over a real-world genomic dataset.},
  archive      = {J_TCC},
  author       = {Ke Cheng and Yantian Hou and Liangmin Wang},
  doi          = {10.1109/TCC.2022.3228906},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2803-2819},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure similar sequence query over multi-source genomic data on cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decode-and-compare: An efficient verification scheme for
coded distributed edge computing. <em>TCC</em>, <em>11</em>(3),
2784–2802. (<a href="https://doi.org/10.1109/TCC.2022.3228243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, edge computing has demonstrated increasing potential to provide low-latency computing services. Coded edge computing can not only make full use of the resources of heterogeneous edge computing servers, but also significantly reduce the negative effects of slow computing devices on computing time. Nevertheless, since edge servers may be unreliable or untrustworthy, the user will decode and get incorrect computation results even if it uses one incorrect sub-computation result returned by faulty edge servers. In this article, for the existing coded edge computing schemes, we focus on the distributed matrix-matrix multiplication and design a general and efficient Decode-and-Compare Verification (DCV) scheme to verify the correctness of computation results and identify faulty edge servers by utilizing the properties of coded computing itself. The DCV scheme contains two components: (1) computation result verification, i.e., obtain the computation result and verify its correctness, and (2) faulty edge server identification, i.e., identify the faulty edge servers by verifying the correctness of returned sub-computation results. For both the independent and collusion faulty edge server models, we conduct solid theoretical analyses on the required decoding rounds, the coding redundancy and the successful verification probability to demonstrate that the correct computation result can be efficiently verified. We also conduct a lot of experiments on the DCV scheme from different aspects and the results show that it achieves much less computation time to get the correct computation result compared with other potential schemes, including homomorphic encryption and local computation.},
  archive      = {J_TCC},
  author       = {Jin Wang and Zhaobo Lu and Mingjia Fu and Jianping Wang and Kejie Lu and Admela Jukan},
  doi          = {10.1109/TCC.2022.3228243},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2784-2802},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Decode-and-compare: An efficient verification scheme for coded distributed edge computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Passive user authentication utilizing two-dimensional
features for IIoT systems. <em>TCC</em>, <em>11</em>(3), 2770–2783. (<a
href="https://doi.org/10.1109/TCC.2022.3227171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passive user authentication is critical for the secure operation of Industrial Internet of Things (IIoT) systems. By jointly utilizing both the time-varying characteristics of the user sequential operation actions and spatial variation characteristics of channel state information (CSI) caused by these actions, this article proposes a novel two-dimensional passive authentication framework for IIoT systems. In particular, we construct the time-varying operation action sequences from the routine work process of a user and apply the Hidden Markov Model to characterize behavioral biometric characteristics of the user, and also employ the eXtreme Gradient Boosting model to depict the spatial variation characteristics of CSI related to the user. By designing two classifiers corresponding these two characteristics and assigning each classifier an appropriate weight, we propose a two-dimensional user authentication framework for continuous and non-intrusive user authentication in IIoT scenarios. Extensive experiments are conducted to illustrate the authentication performance of the proposed authentication framework in terms of false acceptance rate, false rejection rate and equal-error rate. We further investigate the related authentication efficiency issues like the sensitivity to the weights for classifiers, the sensitivity to authentication time and the capability of resisting against impersonation attacks.},
  archive      = {J_TCC},
  author       = {Guozhu Zhao and Pinchang Zhang and Yulong Shen and Limei Peng and Xiaohong Jiang},
  doi          = {10.1109/TCC.2022.3227171},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2770-2783},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Passive user authentication utilizing two-dimensional features for IIoT systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pooling is not favorable: Decentralize mining power of PoW
blockchain using age-of-work. <em>TCC</em>, <em>11</em>(3), 2756–2769.
(<a href="https://doi.org/10.1109/TCC.2022.3226496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the underlying consensus protocol of Bitcoin and Ethereum blockchains, Proof-of-Work (PoW) features a cryptographic mathematical puzzle whose solution is easy to verify but extremely hard to solve. Under PoW, miners maintain the security of blockchain by devoting computing powers to solve the puzzle; the miner who has solved the puzzle successfully generates a block, along with a reward (e.g., a set of cryptocurrency). The average waiting time to generate a block is inversely proportional to the computing power of the miner. To reduce the average block generation time, a group of individual miners can form a centralized mining pool to aggregate their computing power to solve the puzzle together and share the reward contained in the block. However, if the aggregated computing power of the pool forms a substantial portion of the total computing power in the network, the pooled mining undermines the core spirit of blockchain, i.e., the decentralization, and harms its security. To discourage the pooled mining, we develop a new consensus protocol called Proof-of-Age (PoA) that builds upon the native PoW protocol. The core idea of PoA lies in using Age-of-Work (AoW) to measure the effective mining periods that the miners have devoted to maintaining the security of blockchain. Unlike in the native PoW protocol, in our PoA protocol, miners benefit from its effective mining periods even if they have not successfully mined a block. We first employ a continuous time Markov chain (CTMC) to model the block generation process of the PoA based blockchain. Based on this CTMC model, we then analyze the block generation rates of the mining pool and solo miners respectively. Our analytical results verify that under PoA, the block generation rates of miners in the mining pool are reduced compared to that of solo miners, thereby disincentivizing the pooled mining. Finally, we simulate the mining process in the PoA blockchain to demonstrate the consistency of the analytical results.},
  archive      = {J_TCC},
  author       = {Long Shi and Taotao Wang and Jun Li and Shengli Zhang and Song Guo},
  doi          = {10.1109/TCC.2022.3226496},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2756-2769},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Pooling is not favorable: Decentralize mining power of PoW blockchain using age-of-work},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TeaVisor: Network hypervisor for bandwidth isolation in
SDN-NV. <em>TCC</em>, <em>11</em>(3), 2739–2755. (<a
href="https://doi.org/10.1109/TCC.2022.3225915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce TeaVisor that provides bandwidth isolation guarantee for network virtualization (NV) based on software-defined networking (SDN). SDN-based NV (SDN-NV) offers many benefits to clouds, such as topology and address virtualization while allowing flexible resource provisioning, control, and monitoring on virtual networks. In SDN-NV, however, routing is done by tenants independently; thus, existing studies have difficulties in bandwidth isolation guarantee due to the overloaded link problem. Bandwidth isolation guarantee is essential for providing stable and reliable throughput on network services in SDN-NV. Without bandwidth isolation guarantee, tenants suffer degraded service qualities and significant loss in revenue. To address this problem, we design and implement TeaVisor in three components: path virtualization, bandwidth reservation, and path establishment. Through extensive experiments, TeaVisor shows that bandwidth isolation is guaranteed with near-zero errors, which is three orders of magnitude better than existing studies. In addition, TeaVisor guarantees the minimum and maximum bandwidth at the same time. We also present an overhead analysis of TeaVisor in control traffic and memory consumption.},
  archive      = {J_TCC},
  author       = {Yeonho Yoo and Gyeongsik Yang and Jeunghwan Lee and Changyong Shin and Hoseok Kim and Chuck Yoo},
  doi          = {10.1109/TCC.2022.3225915},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2739-2755},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {TeaVisor: Network hypervisor for bandwidth isolation in SDN-NV},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling sparse store-and-forward bulk data transfers in
inter-datacenter networks with multiple congested links. <em>TCC</em>,
<em>11</em>(3), 2725–2738. (<a
href="https://doi.org/10.1109/TCC.2022.3225977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand of bulk data transfer imposes significant challenges in inter-datacenter networks that span multiple domains. Storing data temporarily at domain borders and forward it at later off-peak hours (SnF) can reduce congestion and improve network utilization. However, performance gains from SnF transfer still requires further quantitative study. In this article, we model deadline-constrained SnF transfers in networks with congested inter-domain links, where traffic has slightly varying peak hour and diurnal patterns. A closed-form estimation for the two-links case is derived to quantify the performance gain of intermediate storage (IS) and edge storage (ES). Our study shows that if bulk data transfer requests arrive with equal probability in a day, increase the allowed waiting time will increase the successful transfer probability linearly. Increasing the aggregated network load, defined as the summed duration of the background traffic peak-hour and data transfer, will result in linear decrease in successful transfer probability, when the network is moderately loaded. Our research also shows that IS has significant advantages over ES under heavy network load, and the maximum benefit occurs when the allowed waiting time is two times the aggregated network load.},
  archive      = {J_TCC},
  author       = {Shengnan Yue and Xiao Lin and Weiqiang Sun and Weisheng Hu},
  doi          = {10.1109/TCC.2022.3225977},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2725-2738},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Modeling sparse store-and-forward bulk data transfers in inter-datacenter networks with multiple congested links},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain-based accountable auditing with multi-ownership
transfer. <em>TCC</em>, <em>11</em>(3), 2711–2724. (<a
href="https://doi.org/10.1109/TCC.2022.3224440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud auditing enables the integrity verification of cloud data without the necessity of data retrieval, which significantly promotes the storage service of cloud computing. Auditing with ownership transfer is a variation where both cloud data and the tags for integrity verification can be transferred. In some scenarios, like joint-stock enterprise acquisition and electronic medical records migration, we argue that auditing and transferring data belonging to multiple owners are significantly important. However, to the best of our knowledge, there exists no such protocol in multi-ownership scenarios in the literature. In this article, we propose a blockchain-based accountable auditing protocol with multi-ownership transfer for the first time. One distinguishable property is the simultaneous achievement of verifiability, accountability and multi-ownership transferability, merely with very little extra cost. Specifically, we construct a novel tag structure based on homomorphic authenticators and compact multi-signatures, enabling integrity verification and multi-ownership transfer. Subsequently, we record the information concerning data generation and ownership transfer on immutable blockchains to make these procedures accountable. Furthermore, we present a comprehensive analysis and extensive experiments to demonstrate the security and efficiency of the proposed protocol.},
  archive      = {J_TCC},
  author       = {Jun Shen and Xiaofeng Chen and Jianghong Wei and Fuchun Guo and Willy Susilo},
  doi          = {10.1109/TCC.2022.3224440},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2711-2724},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Blockchain-based accountable auditing with multi-ownership transfer},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Virtual service embedding with time-varying load and
provable guarantees. <em>TCC</em>, <em>11</em>(3), 2693–2710. (<a
href="https://doi.org/10.1109/TCC.2022.3224399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying services efficiently while satisfying their quality requirements is a major challenge in network slicing. Effective solutions place instances of the services’ virtual network functions (VNFs) at different locations of the cellular infrastructure and manage such instances by scaling them as needed. In this work, we address the above problem and the very relevant aspect of sub-slice reuse among different services. Further, unlike prior art, we account for the services’ finite lifetime and time-varying traffic load. We identify two major sources of inefficiency in service management: (i) the overspending of computing resources due to traffic of multiple services with different latency requirements being processed by the same virtual machine (VM), and (ii) the poor packing of traffic processing requests in the same VM, leading to opening more VMs than necessary. To cope with the above issues, we devise an algorithm, called REShare, that can dynamically adapt to the system&#39;s operational conditions and find an optimal trade-off between the aforementioned opposite requirements. We prove that REShare has low algorithmic complexity and is asymptotic 2-competitive under a non-decreasing load. Numerical results, leveraging real-world scenarios, show that our solution outperforms alternatives, swiftly adapting to time-varying conditions and reducing service cost by over 25\%.},
  archive      = {J_TCC},
  author       = {Gil Einziger and Gabriel Scalosub and Carla Fabiana Chiasserini and Francesco Malandrino},
  doi          = {10.1109/TCC.2022.3224399},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2693-2710},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Virtual service embedding with time-varying load and provable guarantees},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reliability, rental-cost and energy-aware multi-workflow
scheduling on multi-cloud systems. <em>TCC</em>, <em>11</em>(3),
2681–2692. (<a href="https://doi.org/10.1109/TCC.2022.3223869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computationally intensive applications with a wide range of requirements are advancing to cloud computing platforms. However, with the growing demands from users, cloud providers are not always able to provide all the prerequisites of the application. Hence, flexible computation and storage systems, such as multi-cloud systems, emerged as a suitable solution. Different charging mechanisms, vast resource configuration, different energy consumption, and reliability are the key issues for multi-cloud systems. To address these issues, we propose a multi-workflow scheduling framework for multi-cloud systems, intending to lower the monetary cost and energy consumption while enhancing the reliability of application execution. Our proposed platform presents different methods (utilizing resource gaps, the DVFS utilized method, and a task duplication mechanism) to ensure each application&#39;s requirement. The Weibull distribution is used to model task reliability at different resource fault rates and fault behavior. Various synthetic workflow applications are used to perform simulation experiments. The results of the performance evaluation demonstrated that our proposed algorithms outperform (in the terms of resource rental cost, efficient energy consumption, and improved reliability) state-of-the-art algorithms for multi-cloud systems.},
  archive      = {J_TCC},
  author       = {Ahmad Taghinezhad-Niar and Javid Taheri},
  doi          = {10.1109/TCC.2022.3223869},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2681-2692},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Reliability, rental-cost and energy-aware multi-workflow scheduling on multi-cloud systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SLearn: A case for task sampling based learning for cluster
job scheduling. <em>TCC</em>, <em>11</em>(3), 2664–2680. (<a
href="https://doi.org/10.1109/TCC.2022.3222649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to accurately estimate job runtime properties allows a scheduler to effectively schedule jobs. State-of-the-art online cluster job schedulers use history-based learning, which uses past job execution information to estimate the runtime properties of newly arrived jobs. However, with fast-paced development in cluster technology (in both hardware and software) and changing user inputs, job runtime properties can change over time, which lead to inaccurate predictions. In this article, we explore the potential and limitation of real-time learning of job runtime properties, by proactively sampling and scheduling a small fraction of the tasks of each job. Such a task-sampling-based approach exploits the similarity among runtime properties of the tasks of the same job and is inherently immune to changing job behavior. Our analytical and experimental analysis of 3 production traces with different skew and job distribution shows that learning in space can be substantially more accurate. Our simulation and testbed evaluation on Azure of the two learning approaches anchored in a generic job scheduler using 3 production cluster job traces shows that despite its online overhead, learning in space reduces the average Job Completion Time (JCT) by 1.28×, 1.56×, and 1.32× compared to the prior-art history-based predictor. We further analyze the experimental results to give intuitive explanations to why learning in space outperforms learning in time in these experiments. Finally, we show how sampling-based learning can be extended to schedule DAG jobs and achieve similar speedups over the prior-art history-based predictor.},
  archive      = {J_TCC},
  author       = {Akshay Jajoo and Y. Charlie Hu and Xiaojun Lin and Nan Deng},
  doi          = {10.1109/TCC.2022.3222649},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2664-2680},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SLearn: A case for task sampling based learning for cluster job scheduling},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient data as a service in fog computing: An adaptive
multi-agent based approach. <em>TCC</em>, <em>11</em>(3), 2646–2663. (<a
href="https://doi.org/10.1109/TCC.2022.3220811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data as a Service (DaaS) offers an effective provisioning model able to exploit the advantages of cloud computing in terms of accessibility and scalability when data providers need to make their data available to different data consumers. Nevertheless, in settings where data are generated at the edge and they need to be propagated (e.g., Industry 4.0, Smart Cities), DaaS model suffers of some limitations: data transfer from the edge to the cloud – and viceversa – could require a significant time and privacy issues could hamper the possibility to move the data. Goal of this article is to propose a DaaS model based on the Fog Computing paradigm, which combines the advantages of both cloud and edge computing. The proposed solution implements an adaptive multi-agent system where each agent autonomously manages the placement of data in the most convenient location considering the quality of service requirements of the user that it is serving. To guarantee the collaboration of the agents without imposing a centralized control, a reinforcement learning algorithm will be enacted to balance between the local optimum for the single data consumers and the satisfaction of the global requirements of all consumers.},
  archive      = {J_TCC},
  author       = {Giulia Mangiaracina and Pierluigi Plebani and Mattia Salnitri and Monica Vitali},
  doi          = {10.1109/TCC.2022.3220811},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2646-2663},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient data as a service in fog computing: An adaptive multi-agent based approach},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Space-efficient storage structure of blockchain transactions
supporting secure verification. <em>TCC</em>, <em>11</em>(3), 2631–2645.
(<a href="https://doi.org/10.1109/TCC.2022.3220664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of the blockchain size is a major bottleneck hindering its implementations in data-heavy applications. Current efforts improve the distributed storage ways and transactions’ storage mechanisms of blockchain, however, the blockchain distribution and integrity are destroyed. Simplified Payment Verification (SPV) is closely related to blockchain storage, but the current solutions did not explore the privacy-preserving SPV. In this paper, we propose a new storage structure for blockchain transactions, called Coloring Index (CI), to reduce the blockchain&#39;s space occupation. Specifically, we devise an index building algorithm to simply calculate the indices of transactions for the sake of information concealing. By improving the Coloring Embedder for multi-sets query, we can store the indices into the Coloring Embedder to achieve the structured storage of transactions with small space occupation. Using CI, SPV query proceeds without revealing the user&#39;s address, thereby achieving secure data sharing in applications such as the intelligent vehicles’ communications and distributed IoT. We prove CI&#39;s security against malicious full nodes when establishing possible connections between the address and the user. The experiments show that blockchain systems using our CI store one time more transactions than Merkle tree and half more than Bloom filter.},
  archive      = {J_TCC},
  author       = {Xiaoqin Feng and Jianfeng Ma and Huaxiong Wang and Sheng Wen and Yang Xiang and Yinbin Miao},
  doi          = {10.1109/TCC.2022.3220664},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2631-2645},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Space-efficient storage structure of blockchain transactions supporting secure verification},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generic framework for deploying video analytic services on
the edge. <em>TCC</em>, <em>11</em>(3), 2614–2630. (<a
href="https://doi.org/10.1109/TCC.2022.3218813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel distributed model for handling in real-time, edge-based Artificial Intelligence analytics, such as the ones required for smart video surveillance. The novelty of the model relies on decoupling and distributing the services into several decomposed functions which are linked together, creating virtual function chains ( $VFC$ model). The model considers both computational and communication constraints. Theoretical, simulation and experimental results have shown that the $VFC$ model can enable the support of heavy-load services to an edge environment while improving the footprint of the service compared to state-of-the art frameworks. In detail, results on the $VFC$ model have shown that it can reduce the total edge cost, compared with a Monolithic and a Simple Frame Distribution models. For experimenting on a real-case scenario, a testbed edge environment has been developed, where the aforementioned models, as well as a general distribution framework (Spark ©) and an edge-deployement framework (Kubernetes©), have been deployed. A cloud service has also been considered. Experiments have shown that $VFC$ can outperform all alternative approaches, by reducing operational cost and improving the QoS. Finally, a caching and a QoS monitoring service based on Long-Term-Short-Term models are introduced and evaluated.},
  archive      = {J_TCC},
  author       = {Vassilios Tsakanikas and Tasos Dagiuklas},
  doi          = {10.1109/TCC.2022.3218813},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2614-2630},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A generic framework for deploying video analytic services on the edge},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image subset union for compressed image sets in cloud
servers. <em>TCC</em>, <em>11</em>(3), 2603–2613. (<a
href="https://doi.org/10.1109/TCC.2022.3218784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deletion and image insertion, the current commonly used two kinds of image set management, refer to removing compressed images from and adding new photos to a compressed image set, respectively. However, they do not deal with well the problem of the combination of images selected from multiple compressed image sets. To address this issue, in this paper we first propose an image subset union algorithm for compressed image sets. Image subset union aims to integrate image subsets derived from multiple compressed image sets into a new compressed image set. First, we put forward a way to identify the root vertex candidate of the new compressed image set. Second, we classify all the images of image subsets into three categories: images needed to be re-encoded, images needed to be only decoded, and images unneeded to be re-encoded or decoded. Third, we also employ minimum spanning tree production, a proposed vertex layer candidate assignment method, depth- and subtree-constrained minimum spanning tree generation, as well as image re-encoding to build the new compressed image set. Experimental results show that our proposed algorithm can effectively accomplish image subset union with low complexity.},
  archive      = {J_TCC},
  author       = {Wei Wu and Lina Sha},
  doi          = {10.1109/TCC.2022.3218784},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2603-2613},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Image subset union for compressed image sets in cloud servers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AMS: Adaptive multiget scheduling algorithm for distributed
key-value stores. <em>TCC</em>, <em>11</em>(3), 2591–2602. (<a
href="https://doi.org/10.1109/TCC.2022.3218582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed key-value stores provide the Multiget API, where many key-value operations are batched together, to meet the parallel requirement of applications. Correspondingly, reducing the latency of Multigets is crucial for the responsiveness of the distributed key-value stores. The latency of a Multiget depends on both which replica server its key-value operations are scheduled to, i.e., the replica selection for each key-value operation, and when these key-value operations are served, i.e., the scheduling of the service sequence at different replica servers. Existing solutions solely focus on either one of them and accordingly lead to suboptimal latency for Multigets. To address these issues, this article proposes an Adaptive Multiget Scheduling (AMS) algorithm in this article, and specifically, our AMS re-architectures the framework to remove the conflict between replica selection and service sequence scheduling in the existing solution Rein. Based on the new framework, a sophisticated replica selection method is designed. Furthermore, AMS guides both replica selection and service sequence scheduling by the piggybacked information of replica servers, being adaptive to the heterogeneous time-varying server performance. Consequently, AMS can respectively reduce the median, 95th, and 99th percentile latencies of Multigets by a factor of 4, 3.1, and 1.86 compared to the default FIFO algorithm and significantly outperforms Rein.},
  archive      = {J_TCC},
  author       = {Wanchun Jiang and Yujia Qiu and Fa Ji and Yongjia Zhang and Xiangqian Zhou and Jianxin Wang},
  doi          = {10.1109/TCC.2022.3218582},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2591-2602},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {AMS: Adaptive multiget scheduling algorithm for distributed key-value stores},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using microbenchmark suites to detect application
performance changes. <em>TCC</em>, <em>11</em>(3), 2575–2590. (<a
href="https://doi.org/10.1109/TCC.2022.3217947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software performance changes are costly and often hard to detect pre-release. Similar to software testing frameworks, either application benchmarks or microbenchmarks can be integrated into quality assurance pipelines to detect performance changes before releasing a new application version. Unfortunately, extensive benchmarking studies usually take several hours which is problematic when examining dozens of daily code changes in detail; hence, trade-offs have to be made. Optimized microbenchmark suites, which only include a small subset of the full suite, are a potential solution for this problem, given that they still reliably detect the majority of the application performance changes such as an increased request latency. It is, however, unclear whether microbenchmarks and application benchmarks detect the same performance problems and one can be a proxy for the other. In this paper, we explore whether microbenchmark suites can detect the same application performance changes as an application benchmark. For this, we run extensive benchmark experiments with both the complete and the optimized microbenchmark suites of the two time-series database systems InfluxDB and VictoriaMetrics and compare their results to the results of corresponding application benchmarks. We do this for 70 and 110 commits, respectively. Our results show that it is possible to detect application performance changes using an optimized microbenchmark suite if frequent false-positive alarms can be tolerated.},
  archive      = {J_TCC},
  author       = {Martin Grambow and Denis Kovalev and Christoph Laaber and Philipp Leitner and David Bermbach},
  doi          = {10.1109/TCC.2022.3217947},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2575-2590},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Using microbenchmark suites to detect application performance changes},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient and robust committee structure for sharding
blockchain. <em>TCC</em>, <em>11</em>(3), 2562–2574. (<a
href="https://doi.org/10.1109/TCC.2022.3217856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, sharding is deemed a promising way to save traditional blockchain protocols from their low scalability. However, such a technique also brings several potential risks and a huge communication burden. An improper design may give rise to an inconsistent state among different committees. Further, the communication burden arising from cross-shard transactions, unfortunately, reduces the system&#39;s performance. In this article, we first summarize five essential issues that all sharding blockchain designers face. For each issue, we discuss its key challenge and propose our suggested solutions. In order to break the performance bottlenecks, we design a committee structure and propose a reputation mechanism for selecting leaders. The term reputation in our design reflects each node&#39;s honest computation resources. In addition, we present a recovery procedure in case the leader is malicious. Theoretically, we prove that the system is robust under our design. Further simulation results also support this. In addition, the results show that selecting leaders by reputation can dramatically improve the system&#39;s performance.},
  archive      = {J_TCC},
  author       = {Mengqian Zhang and Jichen Li and Zhaohua Chen and Hongyin Chen and Xiaotie Deng},
  doi          = {10.1109/TCC.2022.3217856},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2562-2574},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An efficient and robust committee structure for sharding blockchain},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A2UA: An auditable anonymous user authentication protocol
based on blockchain for cloud services. <em>TCC</em>, <em>11</em>(3),
2546–2561. (<a href="https://doi.org/10.1109/TCC.2022.3216580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regulating illegal activities in cyberspace to balance user privacy and cyberspace governance has been a non-trivial challenge when designing anonymous authentication solutions. For example, while several existing anonymous authentication protocols support accountability, they either risk leaking users’ private keys or incur significant overhead for accountability in each ongoing authentication, including in cloud service-based authentication schemes. Seeking to address these limitations, this article proposes an auditable anonymous user authentication (A2UA) protocol based on blockchain for cloud services. The A2UA protocol mainly employs bilinear pairing, partial authentication factors, dynamic credits and fake-public keys (FPKs) to achieve anonymous mutual authentication between users and cloud service providers, and applies ring signature and blockchain to accomplish two-level accountability while maintaining user privacy. Our analysis results show that the A2UA protocol outperforms several other existing schemes in terms of security, computation and communication costs as well as security and privacy features. Additionally, it has good feasibility in terms of the Ethereum Gas cost as demonstrated in our evaluation.},
  archive      = {J_TCC},
  author       = {Qiuyun Lyu and Hao Li and Zhining Deng and Jingyu Wang and Yizhi Ren and Ning Zheng and Junliang Liu and Huaping Liu and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TCC.2022.3216580},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2546-2561},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A2UA: An auditable anonymous user authentication protocol based on blockchain for cloud services},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved sparrow search algorithm based on normal cloud
model and niche recombination strategy. <em>TCC</em>, <em>11</em>(3),
2529–2545. (<a href="https://doi.org/10.1109/TCC.2022.3216541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the drawback of optimizing process with biased, slower convergence speed and easily falling into local optimum on high-dimensional optimization problems of Sparrow Search Algorithm (SSA), an improved Sparrow Search Algorithm (CRSSA) based on the Normal Cloud Model (NCM) and niche recombination strategy is proposed. Firstly, the position update strategy of the producers with alarm value less than safe value based on the NCM is proposed for avoiding the original update strategy to converge to the center of domain gradually; Secondly, the adaptive parameter $En$ in the NCM can better balance the exploitation and exploration capabilities of the optimization process; Then, the position update strategy of the followers with poor fitness value based on the NCM is presented for enhancing the diversity of population at the end of iteration and avoiding the algorithm to fall into local optimum; Finally, when the algorithm is stagnating, a niche-based recombination strategy is used to further avoid falling into the local optimum, and accelerating the convergence speed. Simulation results show that the CRSSA can not only avoid optimizing with biased of SSA, but keep better accuracy and convergence speed for solving high-dimensional complex optimization problems.},
  archive      = {J_TCC},
  author       = {Baopeng Cheng and Yangwang Fang and Weishi Peng},
  doi          = {10.1109/TCC.2022.3216541},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2529-2545},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Improved sparrow search algorithm based on normal cloud model and niche recombination strategy},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communication-efficient and attack-resistant federated edge
learning with dataset distillation. <em>TCC</em>, <em>11</em>(3),
2517–2528. (<a href="https://doi.org/10.1109/TCC.2022.3215520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Edge Learning considers a large amount of distributed edge nodes collectively train a global gradient-based model for edge computing in the Artificial Internet of Things, which significantly promotes the development of cloud computing. However, current federated learning algorithms take tens of communication rounds transmitting unwieldy model weights under ideal circumstances and hundreds when data is poorly distributed. This drawback directly results in expensive communication overhead for edge devices. Inspired by recent work on dataset distillation and distributed one-shot learning, we propose Distilled One-Shot Federated Learning (DOSFL) to significantly reduce the communication cost while achieving comparable performance. In just one round, each client distills their private dataset, sends the synthetic data to the server, and collectively trains a global model. The distilled data look like noise and are only useful to the specific model weights, i.e., become useless after the model updates. With this weight-less and gradient-less design, the total communication cost of DOSFL is up to three orders of magnitude less than FedAvg while preserving up to 99\% performance of centralized training on both vision and language tasks with different models including CNN, LSTM, Transformer, etc . We demonstrate that an eavesdropping attacker cannot properly train a good model using the leaked distilled data, without knowing the initial model weights. DOSFL serves as an inexpensive method to quickly converge on a performant pre-trained model with less than 0.1\% communication cost of traditional methods.},
  archive      = {J_TCC},
  author       = {Yanlin Zhou and Xiyao Ma and Dapeng Wu and Xiaolin Li},
  doi          = {10.1109/TCC.2022.3215520},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2517-2528},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Communication-efficient and attack-resistant federated edge learning with dataset distillation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MGRM: A multi-segment greedy rewriting method to alleviate
data fragmentation in deduplication-based cloud backup systems.
<em>TCC</em>, <em>11</em>(3), 2503–2516. (<a
href="https://doi.org/10.1109/TCC.2022.3214816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication has been broadly used in Cloud due to its storage space saving ability. Capping methods that rewrite the data chunks of low Container Reference Ratio ( CRR ) containers are developed to alleviate the data fragmentation in Cloud. We analyze and observe from real traces that a number of segments only point to low CRR containers, while some others only contain high CRR containers. This interesting observation is ignored by the existing capping methods. To address this problem, we propose a multi-segment greedy rewriting method named MGRM. MGRM sorts containers of segments in a sequential way. More specifically, given the $i$ th segment currently being processed, MGRM will sort all the containers in the top $i$ th segments. This salient searching feature enables MGRM to select and rewrite the true low-reference container set. Moreover, to achieve a good balance between deduplication ratio and restore performance, MGRM has two working modes: an optimal rewriting mode and a radical rewriting mode. When working in the optimal rewriting mode, MGRM aims to improve the deduplication ratio; when the radical rewriting mode, MGRM strives to improve the restore performance. MGRM adaptively switches the working mode according to workload. Furthermore, unlike the existing capping methods that improve restore performance at the cost of the deduplication ratio, MGRM pays attention to both aspects. Our extensive experimental results show that MGRM achieves high restore performance, coupled with a high deduplication ratio. In particular, compared with the two state-of-art schemes FC and FLC, MGRM improves the deduplication ratio and restore performance by up to 114.83\% and 99.34\%, respectively.},
  archive      = {J_TCC},
  author       = {Datong Zhang and Yuhui Deng and Yi Zhou and Jie Li and Weiheng Zhu and Geyong Min},
  doi          = {10.1109/TCC.2022.3214816},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2503-2516},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MGRM: A multi-segment greedy rewriting method to alleviate data fragmentation in deduplication-based cloud backup systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel scientific power calculations in cloud data center
based on decomposition-coordination directed acyclic graph.
<em>TCC</em>, <em>11</em>(3), 2491–2502. (<a
href="https://doi.org/10.1109/TCC.2022.3211439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the expansion scale of interconnected power systems and refined state perception, scientific power calculations become more complex and diverse. They need faster computation speed and better scalability to support power flow calculation, reactive power optimization, and static/transient stability analysis for unit scheduling. Therefore, this article proposes a novel cloud data center task mapping algorithm of the Stoer-Wagner binary tree (SWBT) to support accelerated executions of these calculations. Firstly, based on the block bordered-diagonal form of the admittance matrix, high-time complexity scientific power calculations are transformed into a unified multi-task decomposition-coordination directed acyclic graph (DC-DAG). And then, the critical tasks in this DC-DAG are found and the virtual machines encapsulating them are matched with physical machines in the data center preferentially. Finally, on CloudSim, a cloud computing platform, the multi-job mixed experiments of 118-13659 bus power systems are carried out. In addition, real-time workload performance is enhanced in two very large real-world power systems. Studies illustrate that SWBT can improve the underlying physical machine resource utilization and reduce data interaction transmission hops to achieve better computing acceleration performance.},
  archive      = {J_TCC},
  author       = {Ting Yang and Xutao Han and Hao Li and Wei Li and Albert Y. Zomaya},
  doi          = {10.1109/TCC.2022.3211439},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2491-2502},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Parallel scientific power calculations in cloud data center based on decomposition-coordination directed acyclic graph},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SE-PIM: In-memory acceleration of data-intensive
confidential computing. <em>TCC</em>, <em>11</em>(3), 2473–2490. (<a
href="https://doi.org/10.1109/TCC.2022.3207145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demand for data-intensive workloads and confidential computing are the prominent research directions shaping the future of cloud computing. Computer architectures are evolving to accommodate the computing of large data. Meanwhile, a plethora of works has explored protecting the confidentiality of the in-cloud computation in the context of hardware-based secure enclaves. However, the approach has faced challenges in achieving efficient large data computation. In this article, we present a novel design, called se-pim , that retrofits Processing-In-Memory (PIM) as a data-intensive confidential computing accelerator. PIM-accelerated computation renders large data computation highly efficient by minimizing data movement. Based on our observation that moving computation closer to memory can achieve efficiency of computation and confidentiality of the processed information simultaneously, we study the advantages of confidential computing inside memory. We construct our findings into a software-hardware co-design called se-pim . Our design illustrates the advantages of PIM-based confidential computing acceleration. We study the challenges in adapting PIM in confidential computing and propose a set of imperative changes, as well as a programming model that can utilize them. Our evaluation shows se-pim can provide a side-channel resistant secure computation offloading and run data-intensive applications with negligible performance overhead compared to the baseline PIM model.},
  archive      = {J_TCC},
  author       = {Kha Dinh Duy and Hojoon Lee},
  doi          = {10.1109/TCC.2022.3207145},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2473-2490},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SE-PIM: In-memory acceleration of data-intensive confidential computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal black-box traceability in decentralized
attribute-based encryption. <em>TCC</em>, <em>11</em>(3), 2459–2472. (<a
href="https://doi.org/10.1109/TCC.2022.3210137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of Cloud-storages, secure sharing of encrypted data with fine-grained access control has become an important challenge. To enforce fine-grained access rights on encrypted data to different users, Attribute-Base Encryption (ABE) is a promising cryptographic tool. But in many situations, some pirate users sell their access privileges for monetary gain. Hence, an efficient Black-Box Traitor Tracing (BB-TT) system is vital to address the corresponding Pirate Decoders (PDs). But, in the existing BB-TTs, the tracing complexity is directly related to the maximum number of users in the system. This article proposes an ABE-based access control system that provides a new Relaxed BB-TT. Our Relaxed BB-TT addresses the complexity problem by exploring the pirate user with an optimal number of decryption queries on the PD. The optimal number of queries leads to the exciting capability of traceability of stateful decoders (i.e., the decoder that can save the states among the Tracer &#39;s queries). Finally, our system provides White-Box traceability and reduces the amount of trust in the authorities. We give formal security proofs and extensive experimental results on different classes of mobile devices, including a laptop and a smartphone.},
  archive      = {J_TCC},
  author       = {Hassan Nasiraee and Maede Ashouri-Talouki and Ximeng Liu},
  doi          = {10.1109/TCC.2022.3210137},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2459-2472},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimal black-box traceability in decentralized attribute-based encryption},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CryptoArcade: A cloud gaming system with blockchain-based
token economy. <em>TCC</em>, <em>11</em>(3), 2445–2458. (<a
href="https://doi.org/10.1109/TCC.2022.3210013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud gaming is a novel service provisioning technology that offloads parts of game software from terminals to powerful cloud infrastructures. However, the commercial charging model for cloud gaming is still in its infancy. In this article, we reveal the deficiencies of existing cloud gaming pricing models and propose CryptoArcade, a token-based cloud gaming system that adopts cryptocurrency as a payment method. Using cryptocurrency, CryptoArcade provides a transparent and resource-aware pricing method, enabling a time irrelevant silent payment on the floating price to protect players’ interests, which avoids the Quality of Experience (QoE) degradation caused by traditional dynamic models. While CryptoArcade can solve the problem of pricing strategies, players still face decision headaches caused by having commission overhead and pre-deposit amounts on blockchains. To better understand players’ trading behaviors in this decision-making, we consider a marketplace where players trade tokens through smart contracts before gaming sessions. Considering the uncertainty of future token consumption, we use Prospect Theory (PT) in modeling and obtain the optimal solution in closed form. When comparing with the benchmark expect utility theory (EUT), we show that with the same external factors, EUT players are more likely to buy tokens than PT ones.},
  archive      = {J_TCC},
  author       = {Sizheng Fan and Juntao Zhao and Rong Zhao and Zehua Wang and Wei Cai},
  doi          = {10.1109/TCC.2022.3210013},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2445-2458},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {CryptoArcade: A cloud gaming system with blockchain-based token economy},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving multi-range queries for secure data
outsourcing services. <em>TCC</em>, <em>11</em>(3), 2431–2444. (<a
href="https://doi.org/10.1109/TCC.2022.3208711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encrypted range query schemes that enable range-based searches over encrypted data have become an effective solution for secure data outsourcing services. However, existing schemes are still inadequate on desired functionality and security. Specifically, supporting efficient multi-range queries while hiding the data ordering leakage remains a challenging research problem. Existing works on order-hiding query schemes only work for encrypted single-range search and incur significant computational overhead due to the protection of the ordering information. In this article, we present a privacy-preserving multi-range query scheme that can address the above problems simultaneously. It not only enables efficient multi-range queries over encrypted data but also guarantees the privacy of ordering information. To protect the ordering leakage, our design adopts an Order-hiding Encoding (OHE) scheme to support multi-range obfuscation. In addition, a novel order-hiding K-Dimensional tree (KD-tree) index structure is designed as the core technique underlying our scheme, which accelerates efficient multi-range queries and obfuscates ordering information of encrypted values. Finally, the formal security analysis confirms that our proposed multi-range query scheme is secure in the random oracle model. The extensive experimental results conducted on real-world datasets demonstrate the practicality of our design.},
  archive      = {J_TCC},
  author       = {Yu Guo and Hongcheng Xie and Mingyue Wang and Xiaohua Jia},
  doi          = {10.1109/TCC.2022.3208711},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2431-2444},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Privacy-preserving multi-range queries for secure data outsourcing services},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Balancing privacy and flexibility of cloud-based personal
health records sharing system. <em>TCC</em>, <em>11</em>(3), 2420–2430.
(<a href="https://doi.org/10.1109/TCC.2022.3208168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things and cloud services have been widely adopted in many applications, and personal health records (PHR) can provide tailored medical care. The PHR data is usually stored on cloud servers for sharing. Weighted attribute-based encryption (ABE) is a practical and flexible technique to protect PHR data. Under a weighted ABE policy, the data user&#39;s attributes will be “scored”, if and only if the score reaches the threshold value, he/she can access the data. However, while this approach offers a flexible access policy, the data owners have difficulty controlling their privacy, especially sharing PHR data in collaborative e-health systems. This article aims to find a balance between privacy and flexibility and proposes an AND-weighted ABE scheme in cloud-based personal health records sharing systems. The proposed scheme can meet both privacy and flexibility. Only when the data user satisfies the scored-based policy and is in the specified organization(s), can the data user access the PHR data. Besides, we give the security proof and the performance evaluation of the proposed scheme. The security proof and performance analysis show that the proposed scheme can efficiently and securely share PHR data in cloud service.},
  archive      = {J_TCC},
  author       = {Yudi Zhang and Fuchun Guo and Willy Susilo and Guomin Yang},
  doi          = {10.1109/TCC.2022.3208168},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2420-2430},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Balancing privacy and flexibility of cloud-based personal health records sharing system},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DiffTREAT: Differentiated traffic scheduling based on RNN in
data centers. <em>TCC</em>, <em>11</em>(3), 2407–2419. (<a
href="https://doi.org/10.1109/TCC.2022.3206593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transmission schemes in data centers are supposed to accurately distinguish flow types for different scheduling. However, prior efforts failed to meet the needs at all levels in a cost-effectively way. Nor the existing schemes proved applicable to all the diverse scenarios or dynamic traffic patterns. Therefore, we proposed Diff erentiated Tr affic sch E duling in d A ta cen T ers (DiffTREAT) based the Recurrent Neural Network (RNN), aiming to simplify the transmission in the dynamic and diverse network scenarios. First, DiffTREAT utilizes deep learning methods for traffic classification and flow size prediction. Second, according to the classified results of flows, DiffTREAT adopts multilevel priority queues to ensure the preferential transmission of latency-sensitive flows while optimizing the overall average flow completion time (FCT). Third, DiffTREAT employs the network cache to increase the capacity of data center networks (DCN), which effectively fights against the traffic burst and improves the throughput of latency-insensitive flows. DiffTREAT has been tested in different topologies in the contexts of diverse network loads and real-world workloads. Experiment results showed that compared with state-of-the-art schemes, DiffTREAT yielded both the lower average flow completion time for latency-sensitive flows and the higher throughput for latency-insensitive flow.},
  archive      = {J_TCC},
  author       = {Ziqi Wei and Qing Li and Keke Zhu and Jianer Zhou and Longhao Zou and Yong Jiang and Xi Xiao},
  doi          = {10.1109/TCC.2022.3206593},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2407-2419},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {DiffTREAT: Differentiated traffic scheduling based on RNN in data centers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cooperative job scheduling and data allocation in
data-intensive parallel computing clusters. <em>TCC</em>,
<em>11</em>(3), 2392–2406. (<a
href="https://doi.org/10.1109/TCC.2022.3206206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data-intensive parallel computing clusters, it is important to provide deadline-guaranteed service to jobs while minimizing resource usage (e.g., network bandwidth and energy). Under the current computing framework (that first allocates data and then schedules jobs), in a busy cluster with many jobs, it is difficult to achieve high data locality (hence low bandwidth consumption), deadline guarantee, and high energy savings simultaneously. We model the problem to simultaneously achieve these three objectives using integer programming. Due to the NP-hardness of the problem, we propose a heuristic Cooperative job Scheduling (CSA) and data Allocation method. CSA novelly reverses the order of data allocation and job scheduling in the current computing framework. Job-scheduling-first enables CSA to proactively consolidate tasks with more common requested data to the same server when conducting deadline-aware scheduling, and also consolidate the tasks to as few servers as possible to maximize energy savings. This facilitates the subsequent data allocation step to allocate a data block to the server that hosts most of this data&#39;s requester tasks, thus maximally enhancing data locality. To achieve the tradeoff between data locality and energy savings with specified weights, CSA has a cooperative recursive refinement process that recursively adjusts the job schedule and data allocation schedule. We further propose two enhancement algorithms (i.e., minimum k-cut data reallocation algorithm and bipartite based task reassignment algorithm) to further improve the performance of CSA through additional data reallocation and task reassignment, respectively. Trace-driven experiments in the simulation and the real cluster show that CSA outperforms other schedulers in supplying deadline-guarantee and resource-efficient services and the effectiveness of each enhancement. Also, the enhancement algorithms are effective in improving CSA .},
  archive      = {J_TCC},
  author       = {Haoyu Wang and Guoxin Liu and Haiying Shen},
  doi          = {10.1109/TCC.2022.3206206},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2392-2406},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cooperative job scheduling and data allocation in data-intensive parallel computing clusters},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PriRanGe: Privacy-preserving range-constrained intersection
query over genomic data. <em>TCC</em>, <em>11</em>(3), 2379–2391. (<a
href="https://doi.org/10.1109/TCC.2022.3205700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genomic data is being produced rapidly by both individuals and enterprises, and outsourcing this ever-increasing data into clouds is promising for cutting the cost of data owners and mining the wealth of genomic data at a larger scale. However, genome carries sensitive information about individuals, and it is challenging to securely and efficiently perform analysis on remotely hosted genomic databases. In this article, we present a privacy-preserving range-constrained intersection query scheme on genomic data. To achieve security and efficiency, we propose a protocol to fulfill range-constrained intersection query, named PriRanGe. With PriRanGe, a client can securely query genomic data in a specific range in a database while keeping this whole process private. The security of our design targets genomic database confidentiality, query range/result confidentiality, and access pattern protection, and the advantage in efficiency is due to most employed primitives are symmetric. We thoroughly evaluated our design by security proof, experimental analysis and comparison to the state-of-the-art works, all of which support the conclusion that this design is both secure and fast.},
  archive      = {J_TCC},
  author       = {Yaxi Yang and Yao Tong and Jian Weng and Yufeng Yi and Yandong Zheng and Leo Yu Zhang and Rongxing Lu},
  doi          = {10.1109/TCC.2022.3205700},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2379-2391},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PriRanGe: Privacy-preserving range-constrained intersection query over genomic data},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable k-FAC training for deep neural networks with
distributed preconditioning. <em>TCC</em>, <em>11</em>(3), 2365–2378.
(<a href="https://doi.org/10.1109/TCC.2022.3205918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The second-order optimization methods, notably the D-KFAC (Distributed Kronecker Factored Approximate Curvature) algorithms, have gained traction on accelerating deep neural network (DNN) training on GPU clusters. However, existing D-KFAC algorithms require to compute and communicate a large volume of second-order information, i.e., Kronecker factors (KFs), before preconditioning gradients, resulting in large computation and communication overheads as well as a high memory footprint. In this article, we propose DP-KFAC, a novel distributed preconditioning scheme that distributes the KF constructing tasks at different DNN layers to different workers. DP-KFAC not only retains the convergence property of the existing D-KFAC algorithms but also enables three benefits: reduced computation overhead in constructing KFs, no communication of KFs, and low memory footprint. Extensive experiments on a 64-GPU cluster show that DP-KFAC reduces the computation overhead by 1.55×-1.65×, the communication cost by 2.79×-3.15×, and the memory footprint by 1.14×-1.47× in each second-order update compared to the state-of-the-art D-KFAC methods. Our codes are available at https://github.com/lzhangbv/kfac_pytorch .},
  archive      = {J_TCC},
  author       = {Lin Zhang and Shaohuai Shi and Wei Wang and Bo Li},
  doi          = {10.1109/TCC.2022.3205918},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2365-2378},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Scalable K-FAC training for deep neural networks with distributed preconditioning},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling ECN for datacenter networks with RTT variations.
<em>TCC</em>, <em>11</em>(3), 2349–2364. (<a
href="https://doi.org/10.1109/TCC.2022.3204988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ECN has been widely employed in production datacenters to deliver high throughput low latency communications. Despite being successful, prior ECN-based transports have an important drawback: they adopt a fixed RTT value in calculating instantaneous ECN marking threshold while overlooking the RTT variations in practice. In this article, we reveal that the current practice of using a fixed high-percentile RTT for ECN threshold calculation can lead to persistent queue buildups, significantly increasing packet latency. On the other hand, directly adopting lower percentile RTTs results in throughput degradation. To handle the problem, we introduce ECN $^\sharp$ , a simple yet effective solution to enable ECN for RTT variations. At its heart, ECN $^\sharp$ inherits the current instantaneous ECN marking (based on a high-percentile RTT) to achieve high throughput and burst tolerance, while further marking packets (conservatively) upon detecting long-term queue buildups to eliminate unnecessary queueing delay without degrading throughput. We implement ECN $^\sharp$ on a Barefoot Tofino switch and evaluate it through extensive testbed experiments and large-scale simulations. Our evaluation confirms that ECN $^\sharp$ can effectively reduce latency without hurting throughput. For example, compared to the current practice, ECN $^\sharp$ achieves up to $23.4\%$ ( $31.2\%$ ) lower average (99th percentile) flow completion time (FCT) for short flows while delivering similar FCT for large flows under production workloads.},
  archive      = {J_TCC},
  author       = {Junxue Zhang and Wei Bai and Kai Chen},
  doi          = {10.1109/TCC.2022.3204988},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2349-2364},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enabling ECN for datacenter networks with RTT variations},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AI-bazaar: A cloud-edge computing power trading framework
for ubiquitous AI services. <em>TCC</em>, <em>11</em>(3), 2337–2348. (<a
href="https://doi.org/10.1109/TCC.2022.3201544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the burgeoning growth of the Internet of Everything and the substantial breakthroughs in deep learning (DL) algorithms, a booming of artificial intelligence (AI) applications keep emerging. Meanwhile, the advance in existing computing paradigms, i.e., cloud computing and edge computing, provide assorted computing solutions to satisfy the increasingly high requirements for ubiquitous AI services. Nevertheless, there are some non-trivial issues in the computing frameworks, including the underutilization of computing power, the self-interest of computing-power trading mechanism, and the inefficiency of AI services management. To tackle the above issues, we propose a computing-power trading framework based on blockchain, also named AI-Bazaar. In AI-Bazaar, the AI consumers play multiple roles and feel free to contribute the computing power rented from the computing-power provider (CPP) for blockchain mining and AI services. Accordingly, we formulate the computing trading problem as a Stackelberg game. Based on the win or learn fast principle (WoLF), we design a profit-balanced multi-agent reinforcement learning (PB-MARL) algorithm to search the AI-Bazaar equilibrium, while finding the balanced profits for AI consumers and CPP. Numerical simulations are carried out to demonstrate the satisfactory performance and effectiveness of the proposed framework.},
  archive      = {J_TCC},
  author       = {Xiaoxu Ren and Chao Qiu and Xiaofei Wang and Zhu Han and Ke Xu and Haipeng Yao and Song Guo},
  doi          = {10.1109/TCC.2022.3201544},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2337-2348},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {AI-bazaar: A cloud-edge computing power trading framework for ubiquitous AI services},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A privacy-preserving outsourcing computing scheme based on
secure trusted environment. <em>TCC</em>, <em>11</em>(3), 2325–2336. (<a
href="https://doi.org/10.1109/TCC.2022.3201401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the key technologies to enable the internet of things (IoT), cloud computing plays a significant role in providing huge computing and storage facilities for large-scale data. Though cloud computing brings great advantages, new issues emerge, such as data security breach and privacy disclosure. In this article, we introduce a novel secure and privacy-preserving outsourcing computing scheme (hereafter referred to as SPOCS) to tackle this issue. In SPOCS, the effective use of Intel Software Guard Extensions (SGX), one of the trusted execution environment (TEE), ensures the confidence and integrity of sensitive data in cloud computing and prevents data loss from causing privacy disclosure. In order to keep malicious cloud service providers (CSPs) from illegally tampering with the outsourcing results, blockchain is employed to ensure the data immutability. Significantly, our proposed scheme achieves anonymity and traceability. In the outsourcing process, smart contracts are applied to make the whole process fully automated without any human involvement. Finally, the security of the proposed scheme is analyzed in terms of its resistance to different attacks. The experiments indicate that our scheme is effective and efficient.},
  archive      = {J_TCC},
  author       = {Zewei Liu and Chunqiang Hu and Ruinian Li and Tao Xiang and Xingwang Li and Jiguo Yu and Hui Xia},
  doi          = {10.1109/TCC.2022.3201401},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2325-2336},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A privacy-preserving outsourcing computing scheme based on secure trusted environment},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Achieving fast convergence and high efficiency using
differential explicit feedback in data center. <em>TCC</em>,
<em>11</em>(3), 2312–2324. (<a
href="https://doi.org/10.1109/TCC.2022.3199779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since most flows are short-lived in data center networks, fast convergence becomes very important to help the short flows effectively utilize high bandwidth. Though current explicit feedback-based transport control protocols (TCPs) provide fast convergence via fine-grained congestion information from customized switches, they unavoidably incur large traffic overhead for widely existing small packets in data center applications, resulting in suboptimal network efficiency. To solve this issue, we propose a datacenter TCP based on D ifferential E xplicit C ongestion N otification, called DECN, to achieve fast convergence without any traffic overhead. Specifically, DECN feeds rate difference between the target and current rate back to the source by using multiple consecutive packets. Besides, we propose an enhanced version DECN* which obtains the optimal number of consecutive packets according to the packet loss rate. The experimental results of NS2 simulation and testbed implementation show that DECN and its enhanced version DECN* achieve comparable fast convergence as XCP without incurring any extra feedback overhead. Compared with the state-of-the-art explicit feedback-based TCPs, they reduce the flow completion time by up to 34\% in typical data center applications.},
  archive      = {J_TCC},
  author       = {Jiawei Huang and Jingling Liu and Ning Jiang and Sen Liu and Jinbin Hu and Jianxin Wang},
  doi          = {10.1109/TCC.2022.3199779},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2312-2324},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Achieving fast convergence and high efficiency using differential explicit feedback in data center},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Congestion-aware critical gradient scheduling for
distributed machine learning in data center networks. <em>TCC</em>,
<em>11</em>(3), 2296–2311. (<a
href="https://doi.org/10.1109/TCC.2022.3197350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed Machine Learning (DML) is proposed not only to accelerate the training of machine learning, but also to solve the inadequate ability for handling a large amount of training data. It adopts multiple computing nodes in data center to collaboratively work in parallel at the cost of high communication overhead. Gradient Compression (GC) is introduced to reduce the communication overhead by reducing the number of synchronized gradients among computing nodes. However, existing GC solutions suffer from varying network congestion. To be specific, when some computing nodes experience high network congestion, their gradient transmission process could be significantly delayed, slowing down the entire training process. To solve the problem, we propose FLASH, a congestion-aware GC solution for DML. FLASH accelerates the training process by jointly considering the iterative approximation of machine learning and dynamic network congestion scenarios. It can maintain good training performance by adaptively adjust and schedule the number of synchronized gradients among computing nodes. We evaluate the effectiveness of FLASH using AlexNet and Resnet18 under different network congestion scenarios. Simulation results show that under the same number of training epochs, FLASH reduces training time 22-71\%, maintains good accuracy, and low loss, compared with the existing memory top-K GC solution.},
  archive      = {J_TCC},
  author       = {Zehua Guo and Jiayu Wang and Sen Liu and Jineng Ren and Yang Xu and Chao Yao},
  doi          = {10.1109/TCC.2022.3197350},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2296-2311},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Congestion-aware critical gradient scheduling for distributed machine learning in data center networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal energy management of internet data center with
distributed energy resources. <em>TCC</em>, <em>11</em>(3), 2285–2295.
(<a href="https://doi.org/10.1109/TCC.2022.3196655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, an increasing number of Internet data centers (IDCs) are trying to apply distributed energy resources (DERs), such as renewable energy, battery energy storage systems (BESS), and conventional generators (CG). However, uncertain renewable energy presents significant challenges to the safe and stable operation of IDCs. A two-stage optimal operation model based on distributionally robust optimization (DRO) is proposed for an IDC with DERs to lower the total cost of the IDC, including operating cost, carbon emission cost, and re-dispatch cost under the worst-case probability distribution of renewable energy. An ambiguity set is formed by combining norm-1 and norm-inf under the given confidence level to capture the possible probability distributions of uncertain renewable energy. The proposed model can be turned into a tractable mixed-integer linear programming problem and efficiently solved by the column-and-constraint generation algorithm. Experimental results show the benefits of DERs integration and workload dispatch. Comparative analysis further demonstrates the superiority and scalability of the proposed model.},
  archive      = {J_TCC},
  author       = {Kaile Zhou and Zhineng Fei and Xinhui Lu},
  doi          = {10.1109/TCC.2022.3196655},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2285-2295},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimal energy management of internet data center with distributed energy resources},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BeDCV: Blockchain-enabled decentralized consistency
verification for cross-chain calculation. <em>TCC</em>, <em>11</em>(3),
2273–2284. (<a href="https://doi.org/10.1109/TCC.2022.3196937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase of data stored on the blockchain, the efficiency of storage and calculation of blockchain has gradually become a bottleneck restricting the development of blockchain. By storing data on multiple chains, blockchains can request data from other chains for calculation and the storage pressure can be alleviated. But the transfer of a large amount of data between chains suffers from low transfer efficiency and poor security. A reasonable design is to perform the calculation on the data storage chain and only transfer the results across chains. However, since the calculation process is invisible, blockchains cannot judge the consistency of calculation results from other chains. In this article, we provide a blockchain-enabled decentralized consistency verification scheme for cross-chain calculation (BeDCV). Considering the decentralized characteristic of blockchain, we adopt the blockchain called supervision chain for decentralized auditing. We modify paillier homomorphic encryption to encrypt data involved in the calculation for correctness verification. Then, we aggregate the ciphertexts of data to generate the audit proof for integrity verification. Besides, we verify whether the data involved in the calculation are real-time by leveraging a counting bloom filter. The supervision chain can check the correctness, integrity, and real-time performance of cross-chain data calculation without revealing any original information about the data. The theoretical and experimental analysis demonstrates that BeDCV can verify the consistency of cross-chain data calculation result effectively, realizing secure and reliable expansion of blockchain.},
  archive      = {J_TCC},
  author       = {Yushu Zhang and Jiajia Jiang and Xuewen Dong and Liangmin Wang and Yong Xiang},
  doi          = {10.1109/TCC.2022.3196937},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2273-2284},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {BeDCV: Blockchain-enabled decentralized consistency verification for cross-chain calculation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BPMS: Blockchain-based privacy-preserving multi-keyword
search in multi-owner setting. <em>TCC</em>, <em>11</em>(3), 2260–2272.
(<a href="https://doi.org/10.1109/TCC.2022.3196712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable encryption (SE) has emerged as a cryptographic primitive that allows data users to search on encrypted data. Most existing SE schemes usually delegate search operations to an intermediary such as a cloud server, which would inevitably result in single-point failure, privacy leakage, and even untrustworthy results. Several blockchain-based SE schemes have been proposed to alleviate these issues; however, they suffer from some issues, such as the support for multi-keyword multi-owner model, query privacy and data storage availability. In this paper, we propose BPMS, blockchain-based privacy-preserving multi-keyword search in multi-owner setting, which supports searching over encrypted data in trustworthy, private and efficient manners. The attribute Bloom filter has been introduced into our BPMS to build indexes, which protects query privacy and improves index generation performance. To guarantee data storage availability, our BPMS leverages the advantages of IPFS (InterPlanetary File System) to store large scale of encrypted data. Security proof and comparative analysis in theory indicate that our BPMS is more secure and efficient. A series of experiments conducted on a real-world dataset further demonstrate that our BPMS is feasible in practice.},
  archive      = {J_TCC},
  author       = {Sheng Gao and Yuqi Chen and Jianming Zhu and Zhiyuan Sui and Rui Zhang and Xindi Ma},
  doi          = {10.1109/TCC.2022.3196712},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2260-2272},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {BPMS: Blockchain-based privacy-preserving multi-keyword search in multi-owner setting},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When RAN intelligent controller in o-RAN meets multi-UAV
enable wireless network. <em>TCC</em>, <em>11</em>(3), 2245–2259. (<a
href="https://doi.org/10.1109/TCC.2022.3193939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) are projected to be utilized in a variety of unexpected applications, including agriculture, firefighting, emergency response, intelligent transportation, and so on. Wireless communication is one of the primary facilitators in bringing UAVs into a new phase in such applications. To realize the vision in fifth-generation (5G) networks, we propose a 5G-integration of the flexible multi-UAV system and the Open Radio Access Network (O-RAN) architecture, named U-ORAN. Although different studies have been proposed to optimize the UAV trajectory and resource allocation in the radio access network (RAN), our work is the first study to investigate the benefits of adopting UAVs in the O-RAN architecture. In U-ORAN, we consider a flying base station system and propose a joint optimization problem of multi-UAV trajectory and offloading tasks (UTOT) in which UTOT can optimize the routes from users to the core network as well as resource allocation to process offloading tasks. We decompose UTOT into two sub-problems and provide learning solutions based on the multi-agent reinforcement learning and online learning methodologies, both of which are well supported by the O-RAN architecture. Our intensive numerical simulations show that the proposed approaches outperform in a variety of settings and validation scenarios.},
  archive      = {J_TCC},
  author       = {Chuan Pham and Foroutan Fami and Kim Khoa Nguyen and Mohamed Cheriet},
  doi          = {10.1109/TCC.2022.3193939},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2245-2259},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {When RAN intelligent controller in O-RAN meets multi-UAV enable wireless network},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid scheduling framework for mixed real-time tasks in
an automotive system with vehicular network. <em>TCC</em>,
<em>11</em>(3), 2231–2244. (<a
href="https://doi.org/10.1109/TCC.2022.3194713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As vehicles integrate more and more autonomous driving functionalities, it becomes more and more important to use vehicular networks to fully guarantee the safety and real-time performance of on-board computing tasks. Current studies on vehicular networks pay much attention to the performance improvement of network communication and resource allocation, while ignoring the fact that automotive on-board computing tasks play a significant role in vehicle safety and need to be elegantly handled in vehicular networks. In this article, we propose a hybrid scheduling framework for meeting all hard real-time task deadlines while minimizing soft real-time task deadline misses. In particular, the proposed scheduler is composed of some local schedulers and a global scheduler, where the former guarantees the schedulability of all hard real-time tasks, and the latter decides the assignment of soft real-time jobs dynamically online. Depending on the remaining processing capability of a vehicular network, arrival jobs are either assigned for further processing or discarded. An approach combining the utilization-based schedulability test and demand-supply analysis is proposed to effectively assign tasks to processors offline. To meet as many soft real-time task deadlines as possible, tasks’ demand and supply bounds are computed at runtime, such that online execution information can be used to compensate for the scheduling loss with the worst-case assumption made by the offline test. Experimental results demonstrate that, compared to a genetic algorithm, our proposed approach needs far less computation to assign more tasks offline. The online scheduling also saves many soft real-time jobs that were to be dropped by the offline algorithm.},
  archive      = {J_TCC},
  author       = {Biao Hu and Yinbin Shi and Zhengcai Cao and MengChu Zhou},
  doi          = {10.1109/TCC.2022.3194713},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2231-2244},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A hybrid scheduling framework for mixed real-time tasks in an automotive system with vehicular network},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A flexible heuristic to schedule distributed analytic
applications in compute clusters. <em>TCC</em>, <em>11</em>(3),
2217–2230. (<a href="https://doi.org/10.1109/TCC.2019.2926977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the problem of scheduling user-defined analytic applications, which we define as high-level compositions of frameworks, their components, and the logic necessary to carry out work. The key idea in our application definition, is to distinguish classes of components, including core and elastic types: the first being required for an application to make progress, the latter contributing to reduced execution times. We show that the problem of scheduling such applications poses new challenges, which existing approaches address inefficiently. Thus, we present the design and evaluation of a novel, flexible heuristic to schedule analytic applications, that aims at high system responsiveness, by allocating resources efficiently. Our algorithm is evaluated using trace-driven simulations and with large-scale real system traces: our flexible scheduler outperforms current alternatives across a variety of metrics, including application turnaround times, and resource allocation efficiency. We also present the design and evaluation of a full-fledged system, which we have called Zoe, that incorporates the ideas presented in this paper, and report concrete improvements in terms of efficiency and performance, with respect to prior generations of our system.},
  archive      = {J_TCC},
  author       = {Francesco Pace and Daniele Venzano and Damiano Carra and Pietro Michiardi},
  doi          = {10.1109/TCC.2019.2926977},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {3},
  pages        = {2217-2230},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A flexible heuristic to schedule distributed analytic applications in compute clusters},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elastic resource management for deep learning applications
in a container cluster. <em>TCC</em>, <em>11</em>(2), 2204–2216. (<a
href="https://doi.org/10.1109/TCC.2022.3194128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for learning from massive datasets is restructuring our economy. Effective learning, however, involves nontrivial computing resources. Most businesses utilize commercial infrastructure providers (e.g., AWS) to host their computing clusters in the cloud, where various jobs compete for available resources. While cloud resource management is a fruitful research field that has made many advances in production, such as Kubernetes and YARN, few efforts have been invested to further optimize the system performance, especially for Deep Learning (DL) training jobs in a container cluster. This work introduces FlowCon, a system that is able to monitor the individual evaluation functions of DL jobs at runtime, and thus to make placement decisions and resource allocations elastically. We present a detailed design and implementation of FlowCon and conduct intensive experiments over various DL models. The results demonstrate that FlowCon significantly improves DL job completion time and resource utilization efficiency when compared to default systems. According to the results, FlowCon can improve the completion time by up to 68.8\% and meanwhile, reduce the makespan by 18.0\%, in the presence of various DL job workloads.},
  archive      = {J_TCC},
  author       = {Ying Mao and Vaishali Sharma and Wenjia Zheng and Long Cheng and Qiang Guan and Ang Li},
  doi          = {10.1109/TCC.2022.3194128},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2204-2216},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Elastic resource management for deep learning applications in a container cluster},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UPOA: A user preference based latency and energy aware
intelligent offloading approach for cloud-edge systems. <em>TCC</em>,
<em>11</em>(2), 2188–2203. (<a
href="https://doi.org/10.1109/TCC.2022.3193709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task offloading has been widely used to extend the battery life of intelligent mobile devices. Existing task offloading approaches, focusing on perfecting the balance between latency and energy consumption, completely ignore the impacts of the user preference caused by low battery anxiety. The existence of low battery anxiety - mobile users’ common fear of losing battery energy, especially when the battery energy is already low - causes users to trade high latency for prolonged battery life. Taking into account the user preference impacts on task offloading, we propose a novel offloading approach called UPOA to obtain refined offloading policies between low latency and energy consumption based on user preferences. In UPOA, we start the study by defining a user preference rule that determines users’ offloading preferences according to battery energy status. Then, we build a fine-grained task offloading model to delineate the task distribution characteristics of each node in its offloading link. Guided by this model, we develop a task prediction algorithm based on the long-short-term-memory neural network model to provide task predictions that facilitate offloading policies. Lastly, we implement a particle-swarm-optimization-based online offloading algorithm. The offloading algorithm provides the best long-term offloading policies by incorporating the user preference determined by our user preference rule and the task predictions generated by our task prediction algorithm. To quantitatively evaluate the performance of UPOA, we conduct extensive experiments in a real-world cloud-edge environment. We compare UPOA with three state-of-the-art offloading approaches, DRA, DRL-E2D, and MUDRL under various conditions. Experimental results demonstrate that UPOA can make effective policies based on user preferences compared with the existing approaches. UPOA reduces average latency by 12.49\% when battery energy is sufficient and extends battery life by 20.14\% when battery energy is low.},
  archive      = {J_TCC},
  author       = {Jingling Yuan and Yao Xiang and Yuhui Deng and Yi Zhou and Geyong Min},
  doi          = {10.1109/TCC.2022.3193709},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2188-2203},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {UPOA: A user preference based latency and energy aware intelligent offloading approach for cloud-edge systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer reinforcement learning for adaptive task offloading
over distributed edge clouds. <em>TCC</em>, <em>11</em>(2), 2175–2187.
(<a href="https://doi.org/10.1109/TCC.2022.3192560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the big data era, resource-constrained mobile devices generate an overwhelmingly large amount of data with complex tasks that demand distributed execution. Offloading computation-intensive tasks to nearby edge clouds is promising to solve this problem. However, mobile end devices cannot handle heterogeneous or delay-sensitive tasks. These end devices are also energy constrained with weak adaptability to environment changes. To address and tackle these problems, we present a two-module transfer reinforcement learning (TRL) framework for adaptive task offloading. A domain adaptation module is used to align heterogeneous characteristics of mobile devices. The TRL makes offloading decisions with a deep reinforcement learning (DRL) module. We evaluate the performance of TRL through real-world experiments on edge clouds. Our experiment results show that TRL reduces the task processing time by a factor of 20\% from using three well known DRL methods. Our method achieved (15.4 $\sim$ 40)\% reduction in task drop rate over these methods. With domain adaptation, the TRL results in (50 $\sim$ 80)\% reduction in model convergence time. These advantages in using the TRL framework make it appealing in real-life edge computing applications.},
  archive      = {J_TCC},
  author       = {Kefan Shuai and Yiming Miao and Kai Hwang and Zhengdao Li},
  doi          = {10.1109/TCC.2022.3192560},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2175-2187},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Transfer reinforcement learning for adaptive task offloading over distributed edge clouds},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BFCRI: A blockchain-based framework for crowdsourcing with
reputation and incentive. <em>TCC</em>, <em>11</em>(2), 2158–2174. (<a
href="https://doi.org/10.1109/TCC.2022.3190275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of cloud computing and the sharing economy, crowdsourcing aroused widespread interest and adoption in providing intelligent and efficient services for humans. The majority of existing works focus on effective crowdsourcing task assignment and privacy protection, mostly relying on central servers and assuming that participants are $honest$ - $and$ - $curious$ and proactive. However, in reality, workers may be unwilling to participate, and there may be malicious behavior among participants, thus harming the enthusiasm and interests of other participants. The central server has weaknesses such as single point of failure. To address above problems, we propose a blockchain-based framework for crowdsourcing with reputation and incentive. We first design a worker selection scheme to select credible and capable workers. We leverage reputation as a metric of workers’ credibility, which is calculated through the improved subjective logic model. Then we utilize contract theory to design incentive mechanisms to attract more workers, especially high-quality workers to participate. Experimental results show that our proposed method can detect and prevent malicious participants and resist malicious collusion when the proportion of malicious participants is no more than 1/3. And encourage more workers to actively, honestly and continuously participate in crowdsourcing.},
  archive      = {J_TCC},
  author       = {Shaojing Fu and XueLun Huang and Lin Liu and Yuchuan Luo},
  doi          = {10.1109/TCC.2022.3190275},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2158-2174},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {BFCRI: A blockchain-based framework for crowdsourcing with reputation and incentive},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Directed search: A new operator in NSGA-II for task
scheduling in IoT based on cloud-fog computing. <em>TCC</em>,
<em>11</em>(2), 2144–2157. (<a
href="https://doi.org/10.1109/TCC.2022.3188926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Internet of Things (IoT) developments have made it one of the most important technologies. The exponential growth of data and increasing the number of latency-sensitive applications has necessitated a new approach to support these applications. The emerging fog computing architecture has partially addressed the issue of latency and other limitations of the IoT-based cloud computing paradigm. In order to achieve high-quality services and high system performance, an appropriate and efficient task scheduling method is needed, in addition, the energy consumption of computing devices should be considered. In this article, a constraint bi-objective optimization problem is designed to minimize the servers’ energy consumption and overall response time simultaneously. Then, to solve this problem, by introducing a recombination operator and modifying NSGA-II, a directed non-dominated sorting genetic algorithm, called D-NSGA-II is proposed. This algorithm can control the selection pressure of agents, and balance the exploration and exploitation abilities of the algorithm using this new operator. To evaluate the performance of this algorithm, it is compared with well-known meta-heuristic algorithms. The experimental results demonstrate the D-NSGA-II has better performance than other algorithms. It can also respond to all requests before their deadline.},
  archive      = {J_TCC},
  author       = {Soghra Mousavi and Sepehr Ebrahimi Mood and Alireza Souri and Mohammad Masoud Javidi},
  doi          = {10.1109/TCC.2022.3188926},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2144-2157},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Directed search: A new operator in NSGA-II for task scheduling in IoT based on cloud-fog computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy and reliability-aware task scheduling for cost
optimization of DVFS-enabled cloud workflows. <em>TCC</em>,
<em>11</em>(2), 2127–2143. (<a
href="https://doi.org/10.1109/TCC.2022.3188672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increasing complexity, the execution of workflow applications on cloud typically involves a large number of virtual machines (VMs), which makes the cost as well as energy consumption a great concern. To alleviate this issue, more and more cloud service providers introduce new pricing policies considering Dynamic Voltage and Frequency Scaling (DVFS), where users are charged on the basis of allocated CPU frequencies together with various combinations of VM configurations and prices. However, the customizable CPU frequencies make resource provisioning and scheduling harder to achieve a cost-optimal solution. The things become even worse, since lowering CPU voltages of VMs will increase their chance of suffering soft errors, which results in a high rate of completion time failures of workflow applications. To address the above problem, this paper proposes a novel task scheduling method for the purpose of cost optimization based on the genetic algorithm. By introducing new genetic operators and frequency scaling scheme for DVFS-enabled cloud workflows, our approach can quickly figure out cost-optimal resource provisioning and task scheduling solutions by allocating tasks to appropriate VMs with specific operating frequencies under energy, reliability, makespan and memory constraints. Extensive experiments on various well-known scientific workflow benchmarks validate the effectiveness of the proposed method. Comparing with state-of-the-art methods, our approach can significantly reduce the overall cost and energy consumption without violating the given constraints.},
  archive      = {J_TCC},
  author       = {E Cao and Saira Musa and Mingsong Chen and Tongquan Wei and Xian Wei and Xin Fu and Meikang Qiu},
  doi          = {10.1109/TCC.2022.3188672},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2127-2143},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Energy and reliability-aware task scheduling for cost optimization of DVFS-enabled cloud workflows},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ICN-based data marketplace model based on a game
theoretic approach using quality-data discovery and profit optimization.
<em>TCC</em>, <em>11</em>(2), 2110–2126. (<a
href="https://doi.org/10.1109/TCC.2022.3188447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of data and machine learning, massive amounts of data produced throughout our society can be rapidly delivered to various applications through a broad spectrum of cloud services. However, the spectrum of applications has vastly different data quality requirements and Willingness-To-Pay(WTP), creating a general and complex problem matching consumer quality requirements and budgets with providers’ data quality and price. This paper proposes the Information-Centric Networking(ICN)-based data marketplace to foster quality-data trading service to address the challenge above. We embed a WTP mechanism into an ICN-based data broker service running on cloud computing; therefore, a data consumer can request its desired data with a data name and quality requirement. By specifying nominal WTPs, data consumers can acquire data of the desired quality at the range of maximum nominal WTP. At the same time, a data broker can offer data of a suitable quality based on the profit-optimized price and the proposed service quality using ground-truth accuracy trained by data. We demonstrate that the data broker&#39;s profit can be almost doubled by using the optimal data size and budget determined by considering the one-leader-multiple-followers Stackelberg game. These results show that a value-added data brokering service can profitably facilitate data trading.},
  archive      = {J_TCC},
  author       = {Eunil Seo and Hyoungshick Kim and Bhaskar Krishnamachari and Erik Elmroth},
  doi          = {10.1109/TCC.2022.3188447},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2110-2126},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An ICN-based data marketplace model based on a game theoretic approach using quality-data discovery and profit optimization},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autothrottle: Satisfying network performance requirements
for containers. <em>TCC</em>, <em>11</em>(2), 2096–2109. (<a
href="https://doi.org/10.1109/TCC.2022.3186397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates how to satisfy network performance requirements that are crucial in achieving the service level objectives (SLOs) in clouds. Traditional techniques for network performance management have a limited ability to satisfy the network SLOs. Our in-depth analysis reveals that the fundamental reason comes from decoupling of the CPU scheduler and the network traffic controller as the current CPU scheduler is not aware of such network requirements but only provides a fair-share amount of CPU to all containers. Thus, the container cannot perform the amount of network processing as needed to satisfy its SLO when the CPU allocation is insufficient. In this article, we propose Autothrottle that dynamically adjusts the CPU allocation for the containers to satisfy their network SLOs. The key element of Autothrottle is a throttle algorithm that autonomously determines the amount of CPU for each container needed to satisfy the requirement. We implement Autothrottle in the Linux kernel and evaluate it with massive real-world workloads such as Apache Kafka. Our evaluation results show that Autothrottle successfully satisfies the given network SLO only with a 2\% gap while the existing scheme achieves 20\% less than the SLO. We further observe that Autothrottle also reduces the CPU overhead in network processing by 19\%, improving the network throughput by 27\% compared to the existing scheme.},
  archive      = {J_TCC},
  author       = {Kyungwoon Lee and Kwanhoon Lee and Hyunchan Park and Jaehyun Hwang and Chuck Yoo},
  doi          = {10.1109/TCC.2022.3186397},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2096-2109},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Autothrottle: Satisfying network performance requirements for containers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated clouds for efficient multitasking in distributed
artificial intelligence applications. <em>TCC</em>, <em>11</em>(2),
2084–2095. (<a href="https://doi.org/10.1109/TCC.2022.3184157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed cloud/edge resources are needed to execute pervasive artificial intelligence tasks, collectively. The AI workload and data sets have variable multitasking granularity, privacy constraints, and communication latency concerns. This article presents a novel federated cloud/edge (FCE ) framework, illustrated by distributed medical image processing across multiple hospital sites. This federated cloud system appeals to train many machine learning models efficiently with workload balancing and reduced communication overheads. We tested the FCE model on a multi-cloud platform recently built at the Chinese University of Hong Kong in Shenzhen. We claim three distinct advantages in using the FCE system. First, our federated cloud system results in 41.3\% reduction in total AI processing time in large-scale ML/DL experiments. Second, high machine model accuracy was achieved at 87\% level in telemedicine experiments. The virtual graph helps reduce internode traffic latencies to avoid ML inference slowdowns. Third, the system can tolerate multiple cloud failures to enter a graceful degradation mode in case of node failures. The scalable performance gains in AI processing speed, model accuracy, and fault tolerance make our federated clouds a truly viable approach to solving massive AI multitasking problems in pervasive AI applications.},
  archive      = {J_TCC},
  author       = {Yuejin Li and Kai Hwang and Kefan Shuai and Zhengdao Li and Albert Zomaya},
  doi          = {10.1109/TCC.2022.3184157},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2084-2095},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Federated clouds for efficient multitasking in distributed artificial intelligence applications},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HATS: HetTask scheduling. <em>TCC</em>, <em>11</em>(2),
2071–2083. (<a href="https://doi.org/10.1109/TCC.2022.3184081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To handle task execution, modern supercomputers employ thousands (or millions) of processors. In such supercomputers, task scheduling has a meaningful impression on system performance. To improve efficiency, task scheduling algorithms aim to decrease the volume of communication and the number of message exchanges. These efforts, however, result in other bottlenecks, such as high-link congestion. In addition, the heterogeneity of processors and networks is another major challenge for schedulers. This paper presents a new algorithm for scheduling called Heterogeneity-Aware Task Scheduling (HATS). The proposed algorithm adopts an updated multi-level hyper-graph partitioning approach. It describes a new method of aggregation in the coarsening step that helps to accurately coarsen the hyper-graph of the task model. The Raccoon Optimization algorithm is then used in the initial partitioning phase, and in the un-coarsening phase, a novel refinement procedure optimises the initial partitions. The experiments on this approach showed that, compared to the other well-known algorithms, the proposed method offers better schedules with lower communication volume and imbalance ratio in a shorter time.},
  archive      = {J_TCC},
  author       = {Sina Zangbari Koohi and Nor Asilah Wati Abdul Hamid and Mohamed Othman and Gafurjan Ibragimov},
  doi          = {10.1109/TCC.2022.3184081},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2071-2083},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {HATS: HetTask scheduling},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards software defined measurement in data centers: A
comparative study of designs, implementation, and evaluation.
<em>TCC</em>, <em>11</em>(2), 2057–2070. (<a
href="https://doi.org/10.1109/TCC.2022.3181890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud data centers are increasingly adopting the Software-Defined Networking (SDN) technologies for their underlying connection and communications. However, as a critical part of daily operations and management of such data centers, the network measurement is essential but has often been constrained by the available resources in the traditional network devices. Thus, how to properly balance the resource consumption while maintain timely and accurate measurement remains a challenge to data center systems. Recent advances in Software-Defined Networking (SDN) have enabled flexible and programmable network measurement, which is referred to as Software Defined Measurement (SDM). A promising trend for SDM is to conduct network traffic measurement on widely deployed Open vSwitches (OVS) in data centers. However, little attention has been paid to the design options for conducting traffic measurement on the OVS. In this study, we set to explore different designs and investigate the corresponding trade-offs among resource consumption, measurement accuracy, implementation complexity, and impact on switching speed. Through extensive experiments and comparisons, we quantitatively show the various trade-offs that the different schemes strike to balance, and demonstrate the feasibility of instrumenting OVS with monitoring capabilities. These results provide valuable insights into which design will best serve different measurement and monitoring needs.},
  archive      = {J_TCC},
  author       = {Zili Zha and An Wang and Yang Guo and Songqing Chen},
  doi          = {10.1109/TCC.2022.3181890},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2057-2070},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Towards software defined measurement in data centers: A comparative study of designs, implementation, and evaluation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective virtual network migration algorithm based
on reinforcement learning. <em>TCC</em>, <em>11</em>(2), 2039–2056. (<a
href="https://doi.org/10.1109/TCC.2022.3180784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual network migration (VNM) helps improve network performance by remapping a subset of virtual nodes or links to physical infrastructure, aligning the resource allocation to the virtual network&#39;s changing conditions. However, existing VNM methods neglect integrating multiple objectives that affect network performance, such as energy, communication, migration, and service level agreement violation (SLAV). It is challenging to make VNM decisions to optimize the overall objective in a large-scale cloud environment. This article establishes a multi-objective optimization model and proposes a multi-objective VNM algorithm called MiOvnm. The MiOvnm employs the double deep $Q$ -learning approach to cope with ample state space. It also applies an action selection method called actfilter to deal with large-scale action space. The MiOvnm finds the migration action with optimal potential reward from the candidate action set. Simulation results demonstrate the superiority of our MiOvnm to the state-of-the-art methods. More specifically, MiOvnm reduces average SLAV, communication cost, and total cost by 24.32\%, 4.95\%, and 12.45\%, respectively. Furthermore, evaluation results in a real-world OpenStack platform reveal that making full use of computation and network resources, the MiOvnm reduces the completion time of computation- and network-intensive benchmarks by 11.35\% and 10.31\%, respectively, with a total cost reduction of 26.02\%.},
  archive      = {J_TCC},
  author       = {Desheng Wang and Weizhe Zhang and Xiao Han and Junren Lin and Yu-Chu Tian},
  doi          = {10.1109/TCC.2022.3180784},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2039-2056},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A multi-objective virtual network migration algorithm based on reinforcement learning},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On-demand virtualization for post-copy OS migration in
bare-metal cloud. <em>TCC</em>, <em>11</em>(2), 2028–2038. (<a
href="https://doi.org/10.1109/TCC.2022.3179485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for bare-metal cloud services has increased rapidly because bare-metal cloud is cost-effective for various types of cloud workloads. However, as the bare-metal cloud does not utilize the abstraction of the virtualization layer, it misses the benefits of virtualization. One important benefits absent in the bare-metal cloud is the live migration of guest operating systems. Migrating an OS and applications in the OS as a single unit provides a convenient way to manage cloud services such as load balancing, fault management, and system maintenance. To enable live migration for bare-metal cloud, several approaches have been proposed but they have limitations; they require OS modifications or impose additional overheads for workloads. This paper suggests an on-demand virtualization technique for post-copy OS migration to improve manageability of the bare-metal cloud services. When live migration is requested, a lightweight virtualization layer is enabled in the host on the fly. After completion of the live migration, the virtualization layer is removed from the host. Therefore, the host returns to a bare-metal system for performance. To implement on-demand virtualization, we modify BitVisor to perform the post-copy migration on the x86 architecture. The elapsed time of on-demand virtualization is negligible. It takes only 20 ms to insert the virtualization layer and 30 ms to remove the one. The downtime of migration is reduced because of the post-copy migration.},
  archive      = {J_TCC},
  author       = {Jaeseong Im and Jongyul Kim and Youngjin Kwon and Seungryoul Maeng},
  doi          = {10.1109/TCC.2022.3179485},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2028-2038},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {On-demand virtualization for post-copy OS migration in bare-metal cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MIRES: Intrusion recovery for applications based on
backend-as-a-service. <em>TCC</em>, <em>11</em>(2), 2011–2027. (<a
href="https://doi.org/10.1109/TCC.2022.3178982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Backend-as-a-Service (BaaS) cloud computing model supports many modern popular mobile applications because it simplifies the development and management of services such as data storage, user authentication, and notifications. However, vulnerabilities and other issues may allow malicious actions on the client side to have impact on the backend, i.e., to corrupt the state of the application in the cloud. To deal with these attacks – after they occur and are successful – it is necessary to remove the direct effects of malicious requests and the effects derived from later operations on corrupted data. We introduce MIRES, the first intrusion recovery service for mobile applications based on the BaaS model. MIRES uses a two-stage recovery process that restores the integrity of the mobile application and minimizes its unavailability. MIRES provides multi-service recovery for applications that use more than one data store. We implemented MIRES for Android and for the Firebase cloud-based BaaS platform. We did experiments on 4 mobile applications which showed that MIRES can revert hundreds to thousands of operations in seconds, with an associated unavailability of the application also in the range of seconds.},
  archive      = {J_TCC},
  author       = {Diogo Vaz and David R. Matos and Miguel L. Pardal and Miguel Correia},
  doi          = {10.1109/TCC.2022.3178982},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {2011-2027},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MIRES: Intrusion recovery for applications based on backend-as-a-service},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and simulating a process mining-influenced
load-balancer for the hybrid cloud. <em>TCC</em>, <em>11</em>(2),
1999–2010. (<a href="https://doi.org/10.1109/TCC.2022.3177668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hybrid cloud inherits the best aspects of both the public and private clouds. One such benefit is maintaining control of data processing in a private cloud whilst having nearly elastic resource availability in the public cloud. However, the public and private cloud combination introduces complexities such as incompatible security and control mechanisms, among others. The result is a reduced consistency of data processing and control policies in the different cloud deployment models. Cloud load-balancing is one control mechanism for routing applications to appropriate processing servers in compliance with the policies of the adopting organization. This article presents a process-mining influenced load-balancer for routing applications and data according to dynamically defined business rules. We use a high-level Colored Petri Net (CPN) to derive a model for the process mining-influenced load-balancer and validate the model employing live data from a selected hospital.},
  archive      = {J_TCC},
  author       = {Kenneth Kwame Azumah and Paulo Romero Martins Maciel and Lene Tolstrup Sørensen and Sokol Kosta},
  doi          = {10.1109/TCC.2022.3177668},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1999-2010},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Modeling and simulating a process mining-influenced load-balancer for the hybrid cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task offloading for deep learning empowered automatic speech
analysis in mobile edge-cloud computing networks. <em>TCC</em>,
<em>11</em>(2), 1985–1998. (<a
href="https://doi.org/10.1109/TCC.2022.3177649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of mobile multimedia services and artificial intelligence applications involving automatic speech analysis (ASA), mobile devices are increasingly unable to handle these computation-intensive tasks generated by users due to the limited computing resource. Besides, the existing cloud computing paradigm is not capable of processing such real-time and delay-sensitive ASA tasks. In this paper, by leveraging mobile edge computing and deep learning (DL), we investigate task offloading for DL-empowered ASA in mobile edge-cloud computing networks to minimize the total time for processing ASA tasks, thereby providing an agile service response. Specifically, to accelerate the processing of ASA tasks, we decompose a convolutional neural network based encoder-decoder model and deploy the encoder at edge servers to extract the features of ASA tasks. Moreover, edge servers derive the user tolerance limit by using a linear regression model for further enhancing the quality of experience of users. Based on some certain network constraints (i.e., user association and edge servers’ storage/computing capacity), we propose a low-complexity and distributed offloading framework to solve the formulated complex problem. Evaluation results demonstrate the effectiveness of the proposed framework on reducing the total time and improving the satisfaction rate of users.},
  archive      = {J_TCC},
  author       = {Xiuhua Li and Zhenghui Xu and Fang Fang and Qilin Fan and Xiaofei Wang and Victor C. M. Leung},
  doi          = {10.1109/TCC.2022.3177649},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1985-1998},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Task offloading for deep learning empowered automatic speech analysis in mobile edge-cloud computing networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pricing model for dynamic resource overbooking in edge
computing. <em>TCC</em>, <em>11</em>(2), 1970–1984. (<a
href="https://doi.org/10.1109/TCC.2022.3175610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Computing (EC) with cloud-like Quality of Service (QoS) can find its wide applications in various resource-constrained smart cities where the resource requirements can be different during peak and off-peak periods. During off-peak periods, there are often many resources that have been requested but not used, which can be reused to obtain higher profit. However, to the best of our knowledge, there is no effective pricing model or overbooking mechanism in EC. To fill in this gap, a novel pricing model for dynamic resource overbooking is proposed in this paper, specifically: 1) To meet the needs of different users in EC, methods of on-demand, daily, auction, and the new spot billing are designed, in which resources can be overbooked. 2) An auction approach with pricing rule and winner determination rule is designed for auction billing, which is proved to guarantee individual rationality, computational efficiency, and truthfulness. 3) To make more use of the auction approach to utilize idle resources, a dynamic resource overbooking mechanism is introduced, including a cancellation policy and a resource prediction method. The mechanism is validated with real-world data-trace. Experimental results show that the dynamic resource overbooking mechanism maximizes the profit of edge nodes with a high QoS Satisfaction ratio of on-demand and daily billing.},
  archive      = {J_TCC},
  author       = {Zhiqing Tang and Fuming Zhang and Xiaojie Zhou and Weijia Jia and Wei Zhao},
  doi          = {10.1109/TCC.2022.3175610},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1970-1984},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Pricing model for dynamic resource overbooking in edge computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Edge AI as a service: Configurable model deployment and
delay-energy optimization with result quality constraints. <em>TCC</em>,
<em>11</em>(2), 1954–1969. (<a
href="https://doi.org/10.1109/TCC.2022.3175725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The breakthrough of artificial intelligence (AI) techniques has accelerated their applications in a wide range of industries, such as security protection, transportation, agriculture, and medical care. With the support of edge computing environments, providing latency guaranteed AI as a Service (AIaaS) can accelerate the deployment of data-intensive and computation-intensive AI applications and reduce the investment cost of the customers. However, the deployment architecture and working mechanism design, and performance optimization problems specific for AIaaS with configurable data quality and model complexity have not been studied in existing works. To address the problem, we propose a configurable model deployment architecture (CMDA) for edge AIaaS and present a flexible working mechanism by enabling the joint configuration of data quality ratios (DQRs) and model complexity ratios (MCRs) for the AI tasks. Along with commonly used resource allocation operations, the manager can improve the energy and delay performance of AI services with the desired quality of results (QoRs). We develop an energy-delay minimization problem under the framework of CMDA and propose a polynomial regression based relaxing method to solve the task configuration subproblem. We conduct experiments and simulations on the ImageNet classification and the common objects in context (COCO) object detection tasks using state-of-the-art deep learning models. We present the corresponding result quality tables (RQTs) and QoR regression models to illustrate the proposed method. The results of single task configuration and multi-task configuration and resource allocation on ImageNet classification and COCO object detection tasks demonstrate that the proposed method can achieve over $5\times$ HDEC improvement compared with non-optimization schemes, and also show that joint configuration of DQR and MCR can achieve over $1.2\times$ HDEC improvement compared with the methods that only configure DQR or MCR.},
  archive      = {J_TCC},
  author       = {Wenyu Zhang and Sherali Zeadally and Wei Li and Haijun Zhang and Jingyi Hou and Victor C. M. Leung},
  doi          = {10.1109/TCC.2022.3175725},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1954-1969},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Edge AI as a service: Configurable model deployment and delay-energy optimization with result quality constraints},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient oblivious random data access scheme in cloud
computing. <em>TCC</em>, <em>11</em>(2), 1940–1953. (<a
href="https://doi.org/10.1109/TCC.2022.3173260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of cloud computing and cloud storage techniques, much attention has been focused on the privacy protection of outsourced data. Existing searchable encryption solutions can ensure the confidentiality and availability of data stored on the cloud. However, searchable encryption is vulnerable to statistical inference attacks, which exploit the disclosure of access patterns on encrypted indexes and encrypted file sets, which has become a potential way to reveal user privacy. Oblivious random access memory (ORAM) is an important means of concealing access patterns, yet its direct use in searchable encryptions is expensive. This paper presents a scheme for efficient and oblivious access to encrypted databases through encrypted indexes. This scheme is a hybrid ORAM scheme, which utilizes semi-homomorphic encryption to perform calculations in the ciphertext domain, overcoming the limitations of the huge overhead associated with Path-ORAM. For excessive amounts of data, semi-homomorphic encryption can significantly reduce communication and storage overhead. Our scheme can achieve high-security encrypted search and update operations at the same time. Moreover, the execution speed of ODS-Tree is 2-8x faster than that of ORAM-based schemes. In addition, the proposed scheme reduces the data block transmission and storage costs compared to existing frameworks.},
  archive      = {J_TCC},
  author       = {Hong Liu and Xiaojing Lu and Shengchen Duan and Yushu Zhang and Yong Xiang},
  doi          = {10.1109/TCC.2022.3173260},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1940-1953},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An efficient oblivious random data access scheme in cloud computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Verifiable privacy-preserving queries on multi-source
dynamic DNA datasets. <em>TCC</em>, <em>11</em>(2), 1927–1939. (<a
href="https://doi.org/10.1109/TCC.2022.3171547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA sharing and querying of personal genomic sequences have becoming more critical than ever with the accumulation of large-scale biomedical data. Quantitative genomic studies heavily rely on multi-source DNA datasets from different institutions, who are reluctant to share the data via cloud centers. The high sensitivity of DNA has compelled the government to restrict its acquisition and usage. One potential solution to tackle the issue is designing a secure query strategy on the encrypted DNA datasets. However, the popular secure DNA query schemes remain defective in verifiability, reliability, and DNA privacy. To relieve the issues, we propose a DNA searchable encryption method named EncGD to achieve verifiable privacy-preserving queries and reliable updates on multi-source dynamic DNA datasets. Specifically, the proposed scheme EncGD is designed by the plaintext-related permutation and substitution primitives, which can enhance the DNA privacy due to the chosen-plaintext attack (CPA)-resist ability. Furthermore, the method realizes the verifiability and reliability by adding known information to block the malicious behaviors of clouds. Experimental results demonstrate the superior efficiency of the proposed method comparing with the two state-of-the-art schemes in terms of time and space costs.},
  archive      = {J_TCC},
  author       = {Dandan Lu and Ming Li and Yi Liao and Guihua Tao and Hongmin Cai},
  doi          = {10.1109/TCC.2022.3171547},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1927-1939},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Verifiable privacy-preserving queries on multi-source dynamic DNA datasets},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). C-wall: Conflict-resistance in privacy-preserving cloud
storage. <em>TCC</em>, <em>11</em>(2), 1911–1926. (<a
href="https://doi.org/10.1109/TCC.2022.3171772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the success of cloud computing, it has been shown its importance to realize various access control models in the cloud storage setting. Chinese Wall is a traditional access control model in business for solving the conflict of interest (CoI) problem, and it would be very interesting to achieve conflict-resistant in cloud storage system. However, the access control model does not ensure the privacy of users, and it may reveal the user&#39;s interest, investment tendency, etc. Therefore, it raises a big challenge to implement the Chinese Wall without compromising the user&#39;s privacy. In this paper, we focus on the Chinese Wall model and apply it to the cloud storage while protecting the access patterns of users. Specifically, we first formulate the tree-based Chinese Wall access control and then propose the Chinese Wall Protocol (called C-Wall). We prove that our C-Wall not only realizes the conflict-resistant but also protects the user&#39;s privacy with universally composable security. Besides, we also apply C-Wall to privacy-preserving cloud storage and propose the C $^{2}$ -Wall, which not only maintains C-Wall&#39;s features, but also ensures the sensitive files from being touched by “honest-but-curious” cloud servers. Furthermore, we evaluate our C $^{2}$ -Wall by theoretical analysis and experimental validation. Experimental results show its effectiveness and efficiency for practical deployment.},
  archive      = {J_TCC},
  author       = {Xiaoguo Li and Tao Xiang and Yi Mu and Fuchun Guo and Zhongyuan Yao},
  doi          = {10.1109/TCC.2022.3171772},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1911-1926},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {C-wall: Conflict-resistance in privacy-preserving cloud storage},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance modeling of metric-based serverless computing
platforms. <em>TCC</em>, <em>11</em>(2), 1899–1910. (<a
href="https://doi.org/10.1109/TCC.2022.3169619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analytical performance models are very effective in ensuring the quality of service and cost of service deployment remain desirable under different conditions and workloads. While various analytical performance models have been proposed for previous paradigms in cloud computing, serverless computing lacks such models that can provide developers with performance guarantees. Besides, most serverless computing platforms still require developers’ input to specify the configuration for their deployment that could affect both the performance and cost of their deployment, without providing them with any direct and immediate feedback. In previous studies, we built such performance models for steady-state and transient analysis of scale-per-request serverless computing platforms (e.g., AWS Lambda, Azure Functions, Google Cloud Functions) that could give developers immediate feedback about the quality of service and cost of their deployments. In this work, we aim to develop analytical performance models for latest trend in serverless computing platforms that use concurrency value and the rate of requests per second for autoscaling decisions. Examples of such serverless computing platforms are Knative and Google Cloud Run (a managed Knative service by Google). The proposed performance model can help developers and providers predict the performance and cost of deployments with different configurations which could help them tune the configuration toward the best outcome. We validate the applicability and accuracy of the proposed performance model by extensive real-world experimentation on Knative and show that our performance model is able to accurately predict the steady-state characteristics of a given workload with minimal amount of data collection.},
  archive      = {J_TCC},
  author       = {Nima Mahmoudi and Hamzeh Khazaei},
  doi          = {10.1109/TCC.2022.3169619},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1899-1910},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Performance modeling of metric-based serverless computing platforms},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling traceable and verifiable multi-user forward secure
searchable encryption in hybrid cloud. <em>TCC</em>, <em>11</em>(2),
1886–1898. (<a href="https://doi.org/10.1109/TCC.2022.3170362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forward secure searchable encryption (FSSE) scheme allows one data user to search on encrypted databases while resisting the file injection attack. The data utilization can be further improved by extending the single-user scenario to the multi-user scenario. However, there are some issues needed to be considered when a data owner shares data with multiple data users. First, the public cloud server can not be completely trusted as it may be dishonest returning incorrect or incomplete results. Second, authorized users may trade their private keys for financial benefit. To our knowledge, state-of-the-art searchable encryption schemes only consider part of the following desirable features: the verifiability of results, the resistance to file injection attacks, the traceability and revocation of malicious users who abuse their private keys in the multi-user setting. Based on these motivations, we first propose enabling traceable and verifiable multi-user FSSE, which achieves the above functionalities. Besides, we carry out the security proof which demonstrates that our scheme can meet the requirements of security. We also assess the performance from theoretical analysis and experimental analysis, which shows that compared with other similar schemes, our scheme has richer functionalities with comparable efficiency.},
  archive      = {J_TCC},
  author       = {Axin Wu and Anjia Yang and Weiqi Luo and Jinghang Wen},
  doi          = {10.1109/TCC.2022.3170362},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1886-1898},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enabling traceable and verifiable multi-user forward secure searchable encryption in hybrid cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resource allocation with workload-time windows for
cloud-based software services: A deep reinforcement learning approach.
<em>TCC</em>, <em>11</em>(2), 1871–1885. (<a
href="https://doi.org/10.1109/TCC.2022.3169157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the workloads and service requests in cloud computing environments change constantly, cloud-based software services need to adaptively allocate resources for ensuring the Quality-of-Service (QoS) while reducing resource costs. However, it is very challenging to achieve adaptive resource allocation for cloud-based software services with complex and variable system states. Most of the existing methods only consider the current condition of workloads, and thus cannot well adapt to real-world cloud environments subject to fluctuating workloads. To address this challenge, we propose a novel Deep Reinforcement learning based resource Allocation method with workload-time Windows (DRAW) for cloud-based software services that considers both the current and future workloads in the resource allocation process. Specifically, an original Deep Q-Network (DQN) based prediction model of management operations is trained based on workload-time windows, which can be used to predict appropriate management operations under different system states. Next, a new feedback-control mechanism is designed to construct the objective resource allocation plan under the current system state through iterative execution of management operations. Extensive simulation results demonstrate that the prediction accuracy of management operations generated by the proposed DRAW method can reach 90.69\%. Moreover, the DRAW can achieve the optimal/near-optimal performance and outperform other classic methods by 3 $\sim$ 13\% under different scenarios.},
  archive      = {J_TCC},
  author       = {Xing Chen and Lijian Yang and Zheyi Chen and Geyong Min and Xianghan Zheng and Chunming Rong},
  doi          = {10.1109/TCC.2022.3169157},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1871-1885},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Resource allocation with workload-time windows for cloud-based software services: A deep reinforcement learning approach},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving content-based similarity detection over
in-the-cloud middleboxes. <em>TCC</em>, <em>11</em>(2), 1854–1870. (<a
href="https://doi.org/10.1109/TCC.2022.3169329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is increasingly popular for cloud providers to offer middlebox service that supports content-based similarity detection for enterprises. However, redirecting network traffic to the cloud for such service raises security concerns. While trusted execution environments such as Intel SGX have emerged as a pragmatic solution for designing secure in-the-cloud middleboxes, it remains challenging to practically support content-based similarity detection. In this paper, we design a secure in-the-cloud middlebox system that can detect content-based similar flows in encrypted traffic dynamically. To cope with the constrained enclave memory, we adopt the caching technique and devise a compact index to increase the cache hit rate for effective similarity detection inside the enclave. We also present a parallel algorithm for performance speedup, with an efficient enclave thread management mechanism. Extensive evaluations demonstrate that the overhead of our system compared to native processing (without SGX) is limited to 2.1×. Meanwhile, our tailored design can achieve up to $14.4\times$ better computational efficiency compared to simply moving the target functionality to the SGX enclave via existing popular library operating systems like Graphene-SGX and Occlum. Our secure system can achieve a normalized similarity detection precision of about 90\%.},
  archive      = {J_TCC},
  author       = {Jing Yao and Xiangyi Meng and Yifeng Zheng and Cong Wang},
  doi          = {10.1109/TCC.2022.3169329},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1854-1870},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Privacy-preserving content-based similarity detection over in-the-cloud middleboxes},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive and scalable caching with erasure codes in
distributed cloud-edge storage systems. <em>TCC</em>, <em>11</em>(2),
1840–1853. (<a href="https://doi.org/10.1109/TCC.2022.3168662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure codes have been widely used to enhance data resiliency with low storage overheads. However, in geo-distributed cloud storage systems, erasure codes may incur high service latency as they require end users to access remote storage nodes to retrieve data. An elegant solution to achieving low latency is to deploy caching services at the edge servers close to end users. In this paper, we propose adaptive and scalable caching schemes to achieve low latency in the cloud-edge storage system. Based on the measured data popularity and network latencies in real time, an adaptive content replacement scheme is proposed to update caching decisions upon the arrival of requests. Theoretical analysis shows that the reduced data access latency of the replacement scheme is at least 50\% of the maximum reducible latency. With the low computation complexity of our design, nearly no extra overheads will be introduced when handling intensive data flows. For further performance improvements without sacrificing its efficiency, an adaptive content adjustment scheme is presented to replace the subset of cached contents that incur the aforementioned performance loss. Driven by real-world data traces, extensive experiments based on Amazon Simple Storage Service demonstrate the effectiveness and efficiency of our design.},
  archive      = {J_TCC},
  author       = {Kaiyang Liu and Jun Peng and Jingrong Wang and Zhiwu Huang and Jianping Pan},
  doi          = {10.1109/TCC.2022.3168662},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1840-1853},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Adaptive and scalable caching with erasure codes in distributed cloud-edge storage systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global virtual data space for unified data access across
supercomputing centers. <em>TCC</em>, <em>11</em>(2), 1822–1839. (<a
href="https://doi.org/10.1109/TCC.2022.3164251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the wide-area high-performance computing environment, heterogeneous storage resources are geographically distributed in different supercomputing centers, which leads to the barriers between applications and data. This article proposes a global virtual data space, named GVDS, to meet the needs of unified data access across supercomputing centers. GVDS integrates the parallel/distributed file systems of supercomputing centers to present a virtual space with tremendous storage capability for users. GVDS organizes users into groups for easy management, which allows users to share, collaborate, and perform computations on the stored data. For failure tolerance, global metadata is replicated and distributed on multiple supercomputing centers, redundant I/O service components are deployed in each supercomputing center. GVDS uses adaptive prefetching, caching, and request merging to improve access performance. Experimental results running on real-world supercomputing centers show that, GVDS can deliver excellent I/O performance running micro-benchmark, real-world traces and applications.},
  archive      = {J_TCC},
  author       = {Bing Wei and Limin Xiao and Hanjie Zhou and Guangjun Qin and Yao Song and Chenhao Zhang},
  doi          = {10.1109/TCC.2022.3164251},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1822-1839},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Global virtual data space for unified data access across supercomputing centers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic multi-metric thresholds for scaling applications
using reinforcement learning. <em>TCC</em>, <em>11</em>(2), 1807–1821.
(<a href="https://doi.org/10.1109/TCC.2022.3163357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-native applications increasingly adopt the microservices architecture, which favors elasticity to satisfy the application performance requirements in face of variable workloads. To simplify the elasticity management, the trend is to create an auto-scaler instance per microservice, which controls its horizontal scalability by using the classic threshold-based policy. Although easy to implement, setting manually the scaling thresholds, which are usually statically-defined on a single metric, may lead to poor scaling decisions when applications are heterogeneous in terms of resource consumption. In this article, we study dynamic multi-metric threshold-based scaling policies, that exploit Reinforcement Learning (RL) to autonomously update the scaling thresholds, one per controlled resource (CPU and memory). The proposed RL approaches (i.e., QL, MB, and DQL Threshold) use different degrees of knowledge about the system dynamics. To model the thresholds’ adaptation actions, we consider two RL-based architectures. In the single-agent architecture, one agent drives the updates of both scaling thresholds. To speed-up the learning, the multi-agent architecture adopts a distinct agent per threshold. Simulation- and prototype-based results show the benefits of the proposed solutions when compared to the state-of-the-art policies and highlight the advantages of multi-agent MB Threshold and DQL Threshold approaches, in terms of deployment objectives and execution times.},
  archive      = {J_TCC},
  author       = {Fabiana Rossi and Valeria Cardellini and Francesco Lo Presti and Matteo Nardelli},
  doi          = {10.1109/TCC.2022.3163357},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1807-1821},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic multi-metric thresholds for scaling applications using reinforcement learning},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Orchestrating image retrieval and storage over a cloud
system. <em>TCC</em>, <em>11</em>(2), 1794–1806. (<a
href="https://doi.org/10.1109/TCC.2022.3162790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since massive numbers of images are now being communicated from, and stored in different cloud systems, faster retrieval has become extremely important. This is more relevant, especially after COVID-19 in bandwidth-constrained environments. However, to the best of our knowledge, a coherent solution to overcome this problem is yet to be investigated in the literature. In this article, by customizing the Progressive JPEG method, we propose a new Scan Script to ensure Faster Image Retrieval. Furthermore, we also propose a new lossy PJPEG architecture to reduce the file size as a solution to overcome our Scan Script&#39;s drawback. In order to achieve an orchestration between them, we improve the scanning of Progressive JPEG&#39;s picture payloads to ensure Faster Image Retrieval using the change in bit pixels of distinct Luma and Chroma components ( $Y$ , $C_{b}$ , and $C_{r}$ ). The orchestration improves user experience even in bandwidth-constrained cases. We evaluate our proposed orchestration in a real-world setting across two continents encompassing a private cloud. Compared to existing alternatives, our proposed orchestration can improve user waiting time by up to 54\% and decrease image size by up to 27\%. Our proposed work is tested in cutting-edge cloud apps, ensuring up to 69\% quicker loading time.},
  archive      = {J_TCC},
  author       = {Jannatun Noor and Md. Nazrul Huda Shanto and Joyanta Jyoti Mondal and Md. Golam Hossain and Sriram Chellappan and A. B. M. Alim Al Islam},
  doi          = {10.1109/TCC.2022.3162790},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1794-1806},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Orchestrating image retrieval and storage over a cloud system},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kubernetes-oriented microservice placement with dynamic
resource allocation. <em>TCC</em>, <em>11</em>(2), 1777–1793. (<a
href="https://doi.org/10.1109/TCC.2022.3161900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microservices and Kubernetes are widely used in the development and operations of cloud-native applications. By providing automated placement and scaling, Kubernetes has become the main tool for managing microservices. However, existing work and Kubernetes fail to consider the dynamic competition and availability of microservices as well as the problem of shared dependency libraries among multiple microservice instances. To this end, this article proposes an integer nonlinear microservice placement model for Kubernetes with the goal of cost minimization. Specifically, we calculate the number of instances based on microservice availability and construct a model in which the total resource demand of multiple microservice instances exceeds the appropriate proportion of node resources when dynamic resource competition exists and the size of the shared dependency library is less than the node storage capacity. Finally, this article solves the microservice placement model using an improved genetic algorithm. The experimental results demonstrate that higher throughput is obtained with the same costs and that the same throughput is obtained with lower costs.},
  archive      = {J_TCC},
  author       = {Zhijun Ding and Song Wang and Changjun Jiang},
  doi          = {10.1109/TCC.2022.3161900},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1777-1793},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Kubernetes-oriented microservice placement with dynamic resource allocation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-effective strong consistency on scalable geo-diverse
data replicas. <em>TCC</em>, <em>11</em>(2), 1764–1776. (<a
href="https://doi.org/10.1109/TCC.2022.3161297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Raft algorithm maintains strong consistency across data replicas in Cloud. This algorithm places nodes, i.e., leader and follower, to serve read/write requests spanning geo-diverse sites. As the workload increases, Raft shall provide proportional scale-out performance. However, traditional scale-out techniques are bottlenecked in Raft with an exponentially increased performance penalty when provisioned sites exhaust local resources. To provide scalability in Raft, this paper presents a cost-effective mechanism that enables elastic auto scaling in Raft, called BW-Raft. BW-Raft extends the original Raft with the following abstractions: (1) secretary nodes that take over expensive log synchronization operations from the leader, relaxing the performance constraint on locks. (2) observer nodes that handle reads only, improving throughput for typical data intensive services. These abstractions are stateless, allowing elastic scale-out on unreliable yet cheap spot instances. In theory, we prove that BW-Raft can preserve the strong consistency guarantee from Raft at scale-out, handling 50X more nodes, compared to the original Raft. We have prototyped the BW-Raft on key-value services and evaluated it with many state-of-the-arts on Amazon EC2 and Alibaba Cloud. Our results show that within the same budget, BW-Raft incurs 5-7X less resource footprint increment than Multi-Raft. Using spot instances, BW-Raft can reduces costs by 84.5\%, compared to Multi-Raft. In the real world experiments, BW-Raft improves goodput of the 95th-percentile SLO by 9X, thus, serves an alternative for distributed service scaling out with strong consistency.},
  archive      = {J_TCC},
  author       = {Yunxiao Du and Zichen Xu and Kanqi Zhang and Jie Liu and Christopher Stewart and Jiacheng Huang},
  doi          = {10.1109/TCC.2022.3161297},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1764-1776},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cost-effective strong consistency on scalable geo-diverse data replicas},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online deployment algorithms for microservice systems with
complex dependencies. <em>TCC</em>, <em>11</em>(2), 1746–1763. (<a
href="https://doi.org/10.1109/TCC.2022.3161684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud and edge computing have been widely adopted in many application scenarios. With the increasing demand of fast iteration and complexity of business logic, it is challenging to achieve rapid development and continuous delivery in such highly distributed cloud and edge computing environment. At present, the microservice-based architecture has been the dominant deployment style, and a microservice system has to evolve agilely to offer stable Quality of Service (QoS) in the situation where user requirement changes frequently. A lot of research have been conducted to optimally re-deploy microservices to adapt to changing requirements. Nevertheless, complex dependencies between microservices and the existence of multiple instances of one single microservice in a microservice system together have not been fully considered in existing work. This article defines SPPMS, the Service Placement Problem in Microservice Systems that feature complex dependencies and multiple instances , as a Fractional Polynomial Problem (FPP). Considering the high computation complexity of FPP, it is then transformed into a Quadratic Sum-of-Ratios Fractional Problem (QSRFP) which is further solved by the our proposed greedy-based algorithms. Experiments demonstrate that our models and algorithms outperform existing approaches in both qualities of the generated solutions and computation speed.},
  archive      = {J_TCC},
  author       = {Xiang He and Zhiying Tu and Markus Wagner and Xiaofei Xu and Zhongjie Wang},
  doi          = {10.1109/TCC.2022.3161684},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1746-1763},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Online deployment algorithms for microservice systems with complex dependencies},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient and robust cloud-based deep learning with
knowledge distillation. <em>TCC</em>, <em>11</em>(2), 1733–1745. (<a
href="https://doi.org/10.1109/TCC.2022.3160129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural networks have shown extraordinary power in various practical learning tasks, especially in object detection, classification, natural language processing. However, deploying such large models on resource-constrained devices or embedded systems is challenging due to their high computational cost. Efforts such as model partition, pruning, or quantization have been used at the expense of accuracy loss. Knowledge distillation is a technique that transfers model knowledge from a well-trained model (teacher) to a smaller and shallow model (student). Instead of using a learning model on the cloud, we can deploy distilled models on various edge devices, significantly reducing the computational cost, memory usage and prolonging the battery lifetime. In this work, we propose a novel neuron manifold distillation (NMD) method, where the student models imitate the teacher&#39;s output distribution and learn the feature geometry of the teacher model. In addition, to further improve the cloud-based learning system reliability, we propose a confident prediction mechanism to calibrate the model predictions. We conduct experiments with different distillation configurations over multiple datasets. Our proposed method demonstrates a consistent improvement in accuracy-speed trade-offs for the distilled model.},
  archive      = {J_TCC},
  author       = {Zeyi Tao and Qi Xia and Songqing Cheng and Qun Li},
  doi          = {10.1109/TCC.2022.3160129},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1733-1745},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An efficient and robust cloud-based deep learning with knowledge distillation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cloud workload turning points prediction via cloud
feature-enhanced deep learning. <em>TCC</em>, <em>11</em>(2), 1719–1732.
(<a href="https://doi.org/10.1109/TCC.2022.3160228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud workload turning point is either a local peak point standing for workload pressure or a local valley point standing for resource waste. Predicting such critical points is important to give warnings to system managers to take precautionary measures aimed at achieving high resource utilization, quality of service (QoS), and profit of the investment. Existing researches mainly focus more on the workload&#39;s future point value prediction only, whereas trend-based turning point prediction is not considered. Moreover, one of the most critical challenges during the prediction is the fact that traditional trend prediction methods which succeed in financial and industrial areas, etc., have a weak ability to represent the cloud features, which means that they cannot describe the highly-variable cloud workloads time series. This article introduces a novel cloud workload turning point prediction approach based on cloud feature-enhanced deep learning. First, we establish a turning point prediction model of cloud server workload considering cloud workload features. Then, a cloud feature-enhanced deep learning model is designed for workload turning point prediction. Experiments on the most famous Google cluster demonstrate the effectiveness of our model compared with state-of-the-art models. To the best of our knowledge, this article is the first systematic research on turning point-based trend prediction of cloud workload time series by cloud feature-enhanced deep learning.},
  archive      = {J_TCC},
  author       = {Li Ruan and Yu Bai and Shaoning Li and Jiaxun Lv and Tianyuan Zhang and Limin Xiao and Haiguang Fang and Chunhao Wang and Yunzhi Xue},
  doi          = {10.1109/TCC.2022.3160228},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1719-1732},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud workload turning points prediction via cloud feature-enhanced deep learning},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Truthful VNFI procurement mechanisms with flexible resource
provisioning in NFV markets. <em>TCC</em>, <em>11</em>(2), 1707–1718.
(<a href="https://doi.org/10.1109/TCC.2022.3157732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of network function virtualization (NFV), more and more enterprises and operators are seeking network service provisioning via service chains of virtual network functions (VNFs), instead of depending on proprietary hardware appliances. Following this trend, an NFV market is emerging, where users can procure different VNF instances (VNFIs) and their combinations from multiple network service providers (NSPs) in a pay-as-you-go way. In such a procurement process, how to guarantee truthfulness while enabling flexible resource provisioning in form of VNFIs is a significant challenge. In this paper, we propose a truthful reverse combinatorial auction-based mechanism to solve the combinatorial VNFI procurement problem. To support flexible resource provisioning, this mechanism allows NSPs to be multi-minded, and determine the provisioning VNFIs according to the auction results. Specifically, we design a heuristic algorithm to determine the winning bids in polynomial time. Furthermore, we devise a critical-payment-based pricing algorithm to induce NSPs to disclose their real costs, aiming to achieve truthfulness. Rigorous theoretical analysis shows the proposed mechanism can guarantee truthfulness, individual rationality and computational efficiency. Simulation results also verify the effectiveness and efficiency of the proposed mechanism.},
  archive      = {J_TCC},
  author       = {Xueyi Wang and Lianbo Ma and Xingwei Wang and Ying Shi and Bo Yi and Min Huang},
  doi          = {10.1109/TCC.2022.3157732},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1707-1718},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Truthful VNFI procurement mechanisms with flexible resource provisioning in NFV markets},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A universal reversible data hiding method in encrypted image
based on MSB prediction and error embedding. <em>TCC</em>,
<em>11</em>(2), 1692–1706. (<a
href="https://doi.org/10.1109/TCC.2022.3155744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image encryption is used for privacy protection in cloud computing. Nowadays, reversible data hiding in encrypted image (RDHEI) has achieved great success with the demand of embedding additional information into encrypted image. The existing algorithms cannot implement large embedding capacity and good reconstructed image quality simultaneously. Besides, the universality of some methods is limited when they are used in images with different textural characteristics. In this work, an RDHEI method based on most significant bit (MSB) prediction and error embedding is proposed. On one hand, all types of prediction errors are considered in the proposed method, therefore all the pixels with prediction errors can be recovered correctly. On the other hand, error blocks are utilized to mark the locations of prediction errors and message blocks are utilized to embed data. Moreover, flag blocks are utilized to distinguish error and message blocks. To solve the problem of misjudgement for flag blocks in the decoding phase, special operations are conducted on error blocks and message blocks. Experimental results demonstrate that, compared with the state-of-the-art RDHEI methods, the proposed method has good universality on well-known databases.},
  archive      = {J_TCC},
  author       = {Guangyong Gao and Shikun Tong and Zhihua Xia and Yun-Qing Shi},
  doi          = {10.1109/TCC.2022.3155744},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1692-1706},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A universal reversible data hiding method in encrypted image based on MSB prediction and error embedding},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A truthful combinatorial auction mechanism towards mobile
edge computing in industrial internet of things. <em>TCC</em>,
<em>11</em>(2), 1678–1691. (<a
href="https://doi.org/10.1109/TCC.2022.3155495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) shows prominent application prospects in the Industrial Internet of Things (IIoT) by allowing resource-restricted IIoT mobile devices (MDs) to offload their tasks to geographical proximity edge clouds. An efficient incentive mechanism should be designed jointly addressing resource allocation and pricing to incentivize MDs (i.e., buyers) and edge clouds (i.e., sellers) to participate in offloading service trading. This article aims to solve the social welfare maximization problem of a personalized MEC computation offloading service market where each edge cloud can allocate different computing and wireless resources to each MD according to the MDs’ delay and energy consumption constraints, and each MD submits bids to edge clouds differently based on the resource allocation of the edge clouds. We propose a truthful combinatorial auction (TCA) mechanism which involves three phases of resource allocation, buyer-seller matching, and payment determination. It should be highlighted that our proposed buyer-seller matching algorithm combines optimal matching and heuristic matching, so it greatly improves the auction effect while ensuring computational efficiency. Considerable theoretical analysis and experimental results prove that the performance of the proposed TCA mechanism is significantly superior to that of other auction mechanisms while holding the desirable properties.},
  archive      = {J_TCC},
  author       = {Yi Su and Wenhao Fan and Yuan’an Liu and Fan Wu},
  doi          = {10.1109/TCC.2022.3155495},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1678-1691},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A truthful combinatorial auction mechanism towards mobile edge computing in industrial internet of things},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiate quality of experience scheduling for deep
learning inferences with docker containers in the cloud. <em>TCC</em>,
<em>11</em>(2), 1667–1677. (<a
href="https://doi.org/10.1109/TCC.2022.3154117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevalence of big-data-driven applications, such as face recognition on smartphones and tailored recommendations from Google Ads, we are on the road to a lifestyle with significantly more intelligence than ever before. Various neural network powered models are running at the back end of their intelligence to enable quick responses to users. Supporting those models requires lots of cloud-based computational resources, e.g., CPUs and GPUs. The cloud providers charge their clients by the amount of resources that they occupy. Clients have to balance the budget and quality of experiences (e.g., response time). The budget leans on individual business owners, and the required Quality of Experience (QoE) depends on usage scenarios of different applications. For instance, an autonomous vehicle requires an real-time response, but unlocking your smartphone can tolerate delays. However, cloud providers fail to offer a QoE-based option to their clients. In this paper, we propose DQoES , differentiated quality of experience scheduler for deep learning inferences. DQoES accepts clients’ specifications on targeted QoEs, and dynamically adjusts resources to approach their targets. Through the extensive cloud-based experiments, DQoES demonstrates that it can schedule multiple concurrent jobs with respect to various QoEs and achieve up to 8x times more satisfied models when compared to the existing system.},
  archive      = {J_TCC},
  author       = {Ying Mao and Weifeng Yan and Yun Song and Yue Zeng and Ming Chen and Long Cheng and Qingzhi Liu},
  doi          = {10.1109/TCC.2022.3154117},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1667-1677},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Differentiate quality of experience scheduling for deep learning inferences with docker containers in the cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring the benefits of resource disaggregation for
service reliability in data centers. <em>TCC</em>, <em>11</em>(2),
1651–1666. (<a href="https://doi.org/10.1109/TCC.2022.3151923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By overcoming the “server box” barrier, resource disaggregation in data centers (DCs) can significantly improve resource utilization. This may then provide a more cost-efficient approach for resource upgrade and expansion. The advantages of resource disaggregation have been explored in earlier research to improve the efficiency of resource usage. This paper investigates the potential benefits of resource disaggregation from the aspect of reliability, which has not been considered before. Resource disaggregation gives rise to a new failure pattern. For example, in a conventional server, the failure of one type of resource leads to the failure of the entire server, so that other types of resources in the same server also become unavailable. After disaggregating, the failure of different types of resources becomes more isolated so that other resources are still available. In this paper, we model the reliability of a resource allocation request in a server-based or disaggregated DC based on whether the request is allocated with only working resources or is also provisioned with backup resources. We then consider a resource allocation problem to maximize the number of requests accepted with guaranteed reliability. This is formulated as an integer linear programming (ILP) problem, and a more straightforward heuristic approach is also proposed. Our numerical studies demonstrate that it may be possible to significantly improve service reliability with this resource disaggregation approach.},
  archive      = {J_TCC},
  author       = {Chao Guo and Xinyu Wang and Gangxiang Shen and Sanjay Kumar Bose and Jiahe Xu and Moshe Zukerman},
  doi          = {10.1109/TCC.2022.3151923},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1651-1666},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Exploring the benefits of resource disaggregation for service reliability in data centers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time FaaS: Towards a latency bounded serverless cloud.
<em>TCC</em>, <em>11</em>(2), 1636–1650. (<a
href="https://doi.org/10.1109/TCC.2022.3151469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, Function-as-a-Service is the most promising concept of serverless cloud computing. It makes possible for developers to focus on application development without any system management effort: FaaS ensures resource allocation, fast response time, schedulability, scalability, resiliency, and upgradability. Applications of 5G, IoT, and Industry 4.0 raise the idea to open cloud-edge computing infrastructures for time-critical applications too, i.e., there is a strong desire to pose real-time requirements for computing systems like FaaS. However, multi-node systems make real-time scheduling significantly complex since guaranteeing real-time task execution and communication is challenging even on one computing node with multi-core processors. In this paper, we present an analytical model and a heuristic partitioning scheduling algorithm suitable for real-time FaaS platforms of multi-node clusters. We show that our task scheduling heuristics could outperform existing algorithms by 55\%. Furthermore, we propose three conceptual designs to enable the necessary real-time communications. We present the architecture of the envisioned real-time FaaS platform, emphasize its benefits and the requirements for the underlying network and nodes, and survey the related work that could meet these demands.},
  archive      = {J_TCC},
  author       = {Márk Szalay and Péter Mátray and László Toka},
  doi          = {10.1109/TCC.2022.3151469},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1636-1650},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Real-time FaaS: Towards a latency bounded serverless cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual optimization of revenue and expense in geo-distributed
data centers using smart grid. <em>TCC</em>, <em>11</em>(2), 1622–1635.
(<a href="https://doi.org/10.1109/TCC.2022.3150985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exorbitant energy expenses can supersede data center profits. Electricity prices often vary across the geographic regions, caused by gaps in the supply-demand, time of use, and production cost factors. Geo-distributed cloud data centers facilitated by a smart grid and enabled by cloud computing can potentially utilize the spatiotemporal diversity of energy prices to reduce operational expenditure and maximize profit. In this article, we solve the data center profit by formulating it as a constrained multi-objective optimization problem. The proposed solution utilizes an evolutionary algorithm-based higher-level heuristic that optimizes data center revenue and expense objectives simultaneously. The proposed technique provides system managers with trade-off solutions suited to varied operational scenarios. Ours is a multi-step approach, utilizing the optimization scheme to obtain Pareto optimal solutions for the request dispatch and resource allocation problem. When broadly evaluated against a comparative resource optimization scheme, our technique increases revenue while lowering expense and collectively yields a higher profit. It exhibits such performance over a broad range of price changes regardless of the data center&#39;s size and utilization level. The extensive simulation results ascertain the effectiveness of the proposed approach across a myriad of system parameters.},
  archive      = {J_TCC},
  author       = {Saifullah Khalid and Ishfaq Ahmad},
  doi          = {10.1109/TCC.2022.3150985},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1622-1635},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dual optimization of revenue and expense in geo-distributed data centers using smart grid},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pricing and budget allocation for IoT blockchain with edge
computing. <em>TCC</em>, <em>11</em>(2), 1608–1621. (<a
href="https://doi.org/10.1109/TCC.2022.3150766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attracted by the inherent security and privacy protection of the blockchain, incorporating blockchain into Internet of Things (IoT) has been widely studied in these years. However, the mining process requires high computational power, which prevents IoT devices from directly participating in blockchain construction. For this reason, edge computing service is introduced to help build the IoT blockchain, where IoT devices could purchase computational resources from the edge servers. In this paper, we consider the case that IoT devices also have other tasks that need the help of edge servers, such as data analysis and data storage. The profits they can get from these tasks is closely related to the amounts of resources they purchased from the edge servers. In this scenario, IoT devices will allocate their limited budgets to purchase different resources from different edge servers, such that their profits can be maximized. Moreover, edge servers will set “best” prices such that they can get the biggest benefits. Accordingly, there raise a pricing and budget allocation problem between edge servers and IoT devices. We model the interaction between edge servers and IoT devices as a multi-leader multi-follower Stackelberg game, whose objective is to reach the Stackelberg Equilibrium (SE). We prove the existence and uniqueness of the SE point, and design efficient algorithms to reach the SE point. In the end, we verify our model and algorithms by performing extensive simulations, and the results show the correctness and effectiveness of our designs.},
  archive      = {J_TCC},
  author       = {Xingjian Ding and Jianxiong Guo and Deying Li and Weili Wu},
  doi          = {10.1109/TCC.2022.3150766},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1608-1621},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Pricing and budget allocation for IoT blockchain with edge computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic performance-optimal offloading of network
functions on programmable switches. <em>TCC</em>, <em>11</em>(2),
1591–1607. (<a href="https://doi.org/10.1109/TCC.2022.3149817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network function virtualization (NFV), network functions (NFs) are chained as a service function chain (SFC) to enhance NF management with low cost and high flexibility. Recent NFV solutions indicate that the packet processing performance of SFCs can be significantly improved by offloading NFs to programmable switches. However, such offloading requires a deep understanding of heterogeneous NF properties (e.g., NF resource consumption and NF performance behaviors) to achieve the maximum SFC performance. Unfortunately, none of existing solutions provide automatic analysis of these NF properties. Thus, network administrators have to manually examine the source codes of NFs and profile various NF properties by hand, which is extremely time-consuming and laborious. In this article, we propose LightNF, a novel system that simplifies NF offloading in programmable networks. LightNF automatically dissects comprehensive NF properties by means of code analysis and performance profiling while eliminating manual efforts. It then leverages its analysis results of NF properties in its SFC placement so as to make the performance-optimal offloading decisions. We have implemented LightNF on Tofino-based hardware programmable switches. We perform extensive experiments to evaluate LightNF with a real-world testbed and large-scale simulation. Our experiments show that LightNF outperforms existing solutions with an orders-of-magnitude reduction in per-packet processing latency and 9.5× improvement in SFC throughput.},
  archive      = {J_TCC},
  author       = {Xiang Chen and Hongyan Liu and Dong Zhang and Zili Meng and Qun Huang and Haifeng Zhou and Chunming Wu and Xuan Liu and Qiang Yang},
  doi          = {10.1109/TCC.2022.3149817},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1591-1607},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Automatic performance-optimal offloading of network functions on programmable switches},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latency-aware task scheduling in software-defined edge and
cloud computing with erasure-coded storage systems. <em>TCC</em>,
<em>11</em>(2), 1575–1590. (<a
href="https://doi.org/10.1109/TCC.2022.3149963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The collaborative edge and cloud computing system has emerged as a promising solution to fulfill the unprecedented high requirements of 5G application scenarios. Due to vendor variations, it is often difficult to manage hardware facilities in such a collaborative system. Moreover, the amount of data generated and tasks requested by end devices are increasing exponentially, which introduces storage and computation bottlenecks. To address these issues, a novel systematic framework called software-defined edge and cloud computing (SD-ECC) is designed to manage the underlying physical resources of edge and cloud layers via software. SD-ECC is combined with an erasure-coded storage system, for which a task scheduling problem is formulated by considering data access and task processing steps. Then, a joint data access and task processing (JDATP) algorithm is proposed to minimize the task response time including data access latency and task processing latency. A practical SD-ECC platform is developed on OpenStack, OpenDaylight, and Kubernetes to conduct experiments with real-world datasets. The experimental results demonstrate that our proposed JDATP algorithm can reduce 20.87\% of the task response time and increase 14.16\% of the remaining storage space on average by comparing it with alternative schemes.},
  archive      = {J_TCC},
  author       = {Jianhang Tang and Mohammad M. Jalalzai and Chen Feng and Zehui Xiong and Yang Zhang},
  doi          = {10.1109/TCC.2022.3149963},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1575-1590},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Latency-aware task scheduling in software-defined edge and cloud computing with erasure-coded storage systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cloud computing based deep compression framework for UHD
video delivery. <em>TCC</em>, <em>11</em>(2), 1562–1574. (<a
href="https://doi.org/10.1109/TCC.2022.3149420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high-definition (UHD) videos are enjoying increased popularity in people&#39;s daily usage because of the good visual experience. However, the data size of UHD videos is 4-16 times larger of HD videos. This will bring many challenges to existing video delivery systems, such as the shortage of network bandwidth resources and longer network transmission latency. In this article, we propose a cloud computing based deep compression framework named Pearl, which utilizes the power of deep learning and cloud computing to compress UHD videos. Pearl compresses UHD videos from two respects: the frame resolution and the colorful information. In pearl, an optimal compact representation of the original UHD video is learned with two deep convolutional neural networks (DCNNs): super resolution CNN (SR-CNN) and colorization CNN (CL-CNN). SR-CNN is used to reconstruct a high resolution video from a low resolution video while CL-CNN is adopted to preserve the color information of the video. Pearl focuses on video content compression in two new directions. Thus, it can be integrated with any existing video compression system. With Pearl, the data size of UHD videos can be significantly reduced. We evaluate the performance of Pearl with a wide variety of network conditions, quality of experience (QoE) metrics, and video properties. In all considered scenarios, Pearl can further compress 84\% of video size and reduce 73\% of network transmission latency.},
  archive      = {J_TCC},
  author       = {Siqi Huang and Jiang Xie and Muhana Magboul Ali Muslam},
  doi          = {10.1109/TCC.2022.3149420},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1562-1574},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A cloud computing based deep compression framework for UHD video delivery},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trust-based secure multi-cloud collaboration framework in
cloud-fog-assisted IoT. <em>TCC</em>, <em>11</em>(2), 1546–1561. (<a
href="https://doi.org/10.1109/TCC.2022.3147226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-Fog-Assisted Internet-of-Things (IoT) is a convincing paradigm to provide users with on-demand and low-latency services through Fog nodes in the edge of multiple clouds (Multi-Cloud). Multi-Cloud is a scalable multi-domain service-oriented net-centric system and can respond to complicated user requirements leveraging Multi-Cloud Service Composition (MCSC). However, in MCSC, user security can be easily compromised by untrusted and curious cloud service providers that may collect and violate the privacy and other essential assets of cloud users. Although many trust-based MCSC solutions have been proposed to seek a trustworthy composite service with highest trust level, most of them are vulnerable to malicious users intending to break through the clouds and inflict serious data leakage or asset damage. Considering these security concerns on both malicious users and untrusted service providers, in this article, we present a trust-based secure multi-cloud collaboration framework for Cloud-Fog-Assisted IoT systems. Specifically, to guarantee the security of users, we develop a role-based trust evaluation method to enhance the trustworthiness of MCSC. To preserve the security of services, we design an efficient user authentication scheme and a secure collaboration scheme to provide collaborative user authentication and access control mechanism for MCSC. We develop a proof of concept implementation for our framework and demonstrate its practicability by performance evaluation with extensive experiments.},
  archive      = {J_TCC},
  author       = {Jiawei Zhang and Teng Li and Zuobin Ying and Jianfeng Ma},
  doi          = {10.1109/TCC.2022.3147226},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1546-1561},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Trust-based secure multi-cloud collaboration framework in cloud-fog-assisted IoT},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nimbus: Towards latency-energy efficient task offloading for
AR services. <em>TCC</em>, <em>11</em>(2), 1530–1545. (<a
href="https://doi.org/10.1109/TCC.2022.3146615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Widespread adoption of mobile augmented reality (AR) and virtual reality (VR) applications depends on their smoothness and immersiveness. Modern AR applications applying computationally intensive computer vision algorithms can burden today&#39;s mobile devices, and cause high energy consumption and/or poor performance. To tackle this challenge, it is possible to offload part of the computation to nearby devices at the edge. However, this calls for smart task placement strategies in order to efficiently use the resources of the edge infrastructure. In this paper, we introduce Nimbus — a task placement and offloading solution for a multi-tier, edge-cloud infrastructure where deep learning tasks are extracted from the AR application pipeline and offloaded to nearby GPU-powered edge devices. Our aim is to minimize the latency experienced by end-users and the energy costs on mobile devices. Our multifaceted evaluation, based on benchmarked performance of AR tasks, shows the efficacy of our solution. Overall, Nimbus reduces the task latency by $\sim 4\times$ and the energy consumption by $\sim$ 77\% for real-time object detection in AR applications. We also benchmark three variants of our offloading algorithm, disclosing the trade-off of centralized versus distributed execution.},
  archive      = {J_TCC},
  author       = {Vittorio Cozzolino and Leonardo Tonetto and Nitinder Mohan and Aaron Yi Ding and Jörg Ott},
  doi          = {10.1109/TCC.2022.3146615},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1530-1545},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Nimbus: Towards latency-energy efficient task offloading for AR services},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online scheduling algorithm for heterogeneous distributed
machine learning jobs. <em>TCC</em>, <em>11</em>(2), 1514–1529. (<a
href="https://doi.org/10.1109/TCC.2022.3143153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning (ML) has played a key role in today&#39;s proliferation of AI services. A typical model of distributed ML is to partition training datasets over multiple worker nodes to update model parameters in parallel, adopting a parameter server or AllReduce architecture. ML training jobs are typically resource elastic, completed using various time lengths with different resource configurations. A fundamental problem in a distributed ML cluster is how to explore the demand elasticity of ML jobs and schedule them with different resource configurations, such that the utilization of resources is maximized and average job completion time is minimized. To address it, we propose an online scheduling algorithm to decide the execution time window, the number and the type of concurrent workers and parameter servers for each job upon its arrival, with a goal of minimizing the weighted average completion time. Our online algorithm consists of (i) an online scheduling framework that groups unprocessed ML training jobs into a batch iteratively, and (ii) a batch scheduling algorithm that configures each ML job to maximize the total weight of scheduled jobs in the current iteration. Our online algorithm guarantees a good parameterized competitive ratio with polynomial time complexity. Extensive evaluations using real-world data demonstrate that it outperforms state-of-the-art schedulers in today&#39;s AI cloud systems.},
  archive      = {J_TCC},
  author       = {Ruiting Zhou and Jinlong Pang and Qin Zhang and Chuan Wu and Lei Jiao and Yi Zhong and Zongpeng Li},
  doi          = {10.1109/TCC.2022.3143153},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1514-1529},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Online scheduling algorithm for heterogeneous distributed machine learning jobs},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling edge-cloud video analytics for robotics
applications. <em>TCC</em>, <em>11</em>(2), 1500–1513. (<a
href="https://doi.org/10.1109/TCC.2022.3142066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging deep learning-based video analytics tasks demand computation-intensive neural networks and powerful computing resources on the cloud to achieve high accuracy. Due to the latency requirement and limited network bandwidth, edge-cloud systems adaptively compress the data to strike a balance between overall analytics accuracy and bandwidth consumption. However, the degraded data leads to another issue of poor tail accuracy , which means the extremely low accuracy of a few semantic classes and video frames. Autonomous robotics applications especially value the tail accuracy performance but suffer using the prior edge-cloud systems. We present Runespoor, an edge-cloud video analytics system to manage the tail accuracy and enable emerging robotics applications. We train and deploy a super-resolution model tailored for the tail accuracy of analytics tasks on the server to significantly improves the performance on hard-to-detect classes and sophisticated frames. During online operation, we use an adaptive data rate controller to further improve the tail performance by instantly adjusting the data rate policy according to the video content. Our evaluation shows that Runespoor improves class-wise tail accuracy by up to 300\%, frame-wise 90\%/99\% tail accuracy by up to 22\%/54\%, and greatly improves the overall accuracy and bandwidth trade-off.},
  archive      = {J_TCC},
  author       = {Yiding Wang and Weiyan Wang and Duowen Liu and Xin Jin and Junchen Jiang and Kai Chen},
  doi          = {10.1109/TCC.2022.3142066},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1500-1513},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enabling edge-cloud video analytics for robotics applications},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning-based joint optimization of
delay and privacy in multiple-user MEC systems. <em>TCC</em>,
<em>11</em>(2), 1487–1499. (<a
href="https://doi.org/10.1109/TCC.2022.3140231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access Edge Computing (MEC) enables mobile users to run various delay-sensitive applications via offloading computation tasks to MEC servers. However, the location privacy and the usage pattern privacy are disclosed to the untrusted MEC servers. The most related work concerning privacy-preserving offloading schemes in MEC either consider an impractical MEC scenario consisting of a single user or take a large amount of computation and communication cost. In this article, we propose a deep reinforcement learning based joint optimization of delay and privacy preservation during offloading for multiple-user wireless powered MEC systems, preserving users’ both location privacy and usage pattern privacy. The main idea is that, to protect both the two kinds of privacy, we propose to disguise users’ offloading decisions and deliberately offloading redundant tasks along with the actual tasks to the MEC servers. On this basis, we further formalize the task offloading as an optimization problem of computation rate and privacy preservation. Then, we design a deep reinforcement learning based offloading algorithm to solve such an non-convex problem, aiming to obtain the better tradeoff between the computation rate and the privacy preservation. Finally, extensive simulation results demonstrate that our algorithm can maintain a high level of computation rate while protecting users’ usage pattern privacy and location privacy, compared with two learning-based methods and two Baselines.},
  archive      = {J_TCC},
  author       = {Ping Zhao and Jiawei Tao and Kangjie Lui and Guanglin Zhang and Fei Gao},
  doi          = {10.1109/TCC.2022.3140231},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1487-1499},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Deep reinforcement learning-based joint optimization of delay and privacy in multiple-user MEC systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ReFlat: A robust access pattern hiding solution for general
cloud query processing based on k-isomorphism and hardware enclave.
<em>TCC</em>, <em>11</em>(2), 1474–1486. (<a
href="https://doi.org/10.1109/TCC.2021.3137351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The access frequency pattern leakage reveals sensitive information over encrypted cloud data, such as query inclinations and interests. Even worse, adversaries can infer the content of storage with the help of auxiliary knowledge. It jeopardizes the mutual trust between the client users and the cloud platform as reported in many cases. In this paper, we study the threats model in which adversaries know both the exact in-memory flow of accessed blocks and the processing boundary of each request. Under these settings, he can precisely observe the access frequency patterns in both aggregated and independent perspectives over queries. We then propose the ReFlat module as a counter solution through the $K$ -duplication obfuscation mechanism. ReFlat securely runs inside the hardware enclave provided by Intel SGX and requires no modifications on query processors. The $K$ -duplication mechanism is further optimized with two working functions to practically deal with point and range queries. Comparing with the state-of-the-art schemes using the similar idea, that is, fake query injection, ReFlat eliminates the security risk of involving intermediate proxy and achieves higher robustness under the proposed threat model. We exhibit comparative experiment results showing that ReFlat exceeds existing schemes providing equal security level in multiple system performance metrics.},
  archive      = {J_TCC},
  author       = {Ziyang Han and Haibo Hu and Qingqing Ye},
  doi          = {10.1109/TCC.2021.3137351},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1474-1486},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {ReFlat: A robust access pattern hiding solution for general cloud query processing based on K-isomorphism and hardware enclave},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Service cost effective and reliability aware job scheduling
algorithm on cloud computing systems. <em>TCC</em>, <em>11</em>(2),
1461–1473. (<a href="https://doi.org/10.1109/TCC.2021.3137323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, increasing number of services are provided to individuals and organizations through cloud computing systems in a pay-as-you-use model. This business service paradigm encounters several cloud Quality of Service (QoS) challenges, such as reliability, cost, and response time. The most common mechanism to improve cloud service reliability is a primary/backup (PB) fault-tolerant technique. However, this reliability enhancement technique inevitably results in multiple replications, which lead to high service cost. In recognition of these challenges, we first build a cloud computing systems resources management architecture. Then, we analyze the cloud service execution reliability on the physical resources of a VM and used a CUDA (Compute Unified Device Architecture)-enabled parallel two-dimensional long short-term memory neural network to predict the software faults of a cloud VM. Third, we propose an effective primary/backup cloud service cost calculation approach. To overcome the cloud service response time constraint, we integrate a response time slack factor into this method. Fourth, we formulate the cloud service reliability and cost aware job scheduling problem, which aims at minimizing the total cloud service cost and rejection rate, and improving the system reliability. Fifthly, a heuristic greedy reliability and cost aware job scheduling (RCJS) algorithm is proposed. Finally, a performance evaluation is conducted and the experimental results demonstrate that our proposed RCJS algorithm significantly outperforms optimal redundant VM placement (OPVMP), MIN-MIN algorithms in terms of average service cost and rejection rate. This algorithm also demonstrates good trade-off of reliability when compared to the other two algorithms and is suitable for cloud services with high reliability and low-cost requirements.},
  archive      = {J_TCC},
  author       = {Xiaoyong Tang and Yi Liu and Zeng Zeng and Bharadwaj Veeravalli},
  doi          = {10.1109/TCC.2021.3137323},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1461-1473},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Service cost effective and reliability aware job scheduling algorithm on cloud computing systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-stage multi-population genetic algorithm with
heuristics for workflow scheduling in heterogeneous distributed
computing environments. <em>TCC</em>, <em>11</em>(2), 1446–1460. (<a
href="https://doi.org/10.1109/TCC.2021.3137881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workflow scheduling in Heterogeneous Distributed Computing Environments (HDCEs) is a NP-hard problem. Although a number of scheduling approaches have been proposed for workflow scheduling in HDCEs, there is still a room and need for improvement. To fill the gaps, this study formulates workflow scheduling problem in HDCEs as a complete, solvable and extensible integer programming mathematical model with precedence and resource constraints that provides a theoretical foundation for developing workflow scheduling strategy. Then, this study develops a novel two-stage multi-population genetic algorithm with heuristics for workflow scheduling. In particular, two-stage multi-population coevolution strategy is employed with designed novel methods for population initialization, genetic operation, individual decoding and improvement. To estimate the validity, extensive experiments are designed and conducted on various scenarios based on real and random workflow applications. The results have shown the practical viability of the proposed algorithm outperforming conventional approaches.},
  archive      = {J_TCC},
  author       = {Yi Xie and Feng-Xian Gui and Wei-Jun Wang and Chen-Fu Chien},
  doi          = {10.1109/TCC.2021.3137881},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1446-1460},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A two-stage multi-population genetic algorithm with heuristics for workflow scheduling in heterogeneous distributed computing environments},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective multi-factorial evolutionary algorithm for
container placement. <em>TCC</em>, <em>11</em>(2), 1430–1445. (<a
href="https://doi.org/10.1109/TCC.2021.3137400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of containerization technology in microservice architecture has become widespread, owing to its potential to support fast deployment of web applications and improve the resource utilization in cloud data centers. Evolutionary algorithms (EAs) have been performed promising on the deployment of applications created using microservices. However, with the growing demand for microservice application, the existing EAs fail to solve the large-scale container placement problem due to the high time complexity and poor scalability. A multi-factorial evolutionary algorithm (MFEA) is proposed in this article, which can evolve multiple optimization problems simultaneously for the container placement problem in heterogeneous cluster environments. First, a system model integrated the heterogeneous clusters, microservices, containers, and four optimization objectives is presented. Then, embedded with local search strategy, a multi-objective container placement MFEA (MOCP-MFEA) algorithm is developed to address the container placement problem. MOCP-MFEA is applied to a variety of container placement problems with different application sizes in heterogeneous cluster environments. Experimental results show that compared with various conventional and evolutionary-based approaches, MOCP-MFEA could shorten optimization time significantly and offer a competitive placement solution for the container placement problem and show a good elasticity in heterogeneous cluster environments. Moreover, the deployment scheme of container to physical machines is crucial to lowering resources wastage.},
  archive      = {J_TCC},
  author       = {Ruochen Liu and Ping Yang and Haoyuan Lv and Weibin Li},
  doi          = {10.1109/TCC.2021.3137400},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1430-1445},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-objective multi-factorial evolutionary algorithm for container placement},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data placement for multi-tenant data federation on the
cloud. <em>TCC</em>, <em>11</em>(2), 1414–1429. (<a
href="https://doi.org/10.1109/TCC.2021.3136577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to privacy concerns of users and law enforcement in data security and privacy, it becomes more and more difficult to share data among organizations. Data federation brings new opportunities to the data-related cooperation among organizations by providing abstract data interfaces. With the development of cloud computing, organizations store data on the cloud to achieve elasticity and scalability for data processing. The existing data placement approaches generally only consider one aspect, which is either execution time or monetary cost, and do not consider data partitioning for hard constraints. In this paper, we propose an approach to enable data processing on the cloud with the data from different organizations. The approach consists of a data federation platform named FedCube and a Lyapunov-based data placement algorithm. FedCube enables data processing on the cloud. We use the data placement algorithm to create a plan in order to partition and store data on the cloud so as to achieve multiple objectives while satisfying the constraints based on a multi-objective cost model. The cost model is composed of two objectives, i.e., reducing monetary cost and execution time. We present an experimental evaluation to show our proposed algorithm significantly reduces the total cost (up to 69.8\%) compared with existing approaches.},
  archive      = {J_TCC},
  author       = {Ji Liu and Lei Mo and Sijia Yang and Jingbo Zhou and Shilei Ji and Haoyi Xiong and Dejing Dou},
  doi          = {10.1109/TCC.2021.3136577},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1414-1429},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Data placement for multi-tenant data federation on the cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PerfSim: A performance simulator for cloud native
microservice chains. <em>TCC</em>, <em>11</em>(2), 1395–1413. (<a
href="https://doi.org/10.1109/TCC.2021.3135757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud native computing paradigm allows microservice-based applications to take advantage of cloud infrastructure in a scalable, reusable, and interoperable way. However, in a cloud native system, the vast number of configuration parameters and highly granular resource allocation policies can significantly impact the performance and deployment cost. For understanding and analyzing these implications in an easy, quick, and cost-effective way, we present PerfSim, a discrete-event simulator for approximating and predicting the performance of cloud native service chains in user-defined scenarios. To this end, we proposed a systematic approach for modeling the performance of microservices endpoint functions by collecting and analyzing their performance and network traces. With a combination of the extracted models and user-defined scenarios, PerfSim can then simulate the performance behavior of all services over a given period and provide an approximation for system KPIs, such as requests’ average response time. Using the processing power of a single laptop, we evaluated both simulation accuracy and speed of PerfSim in 104 prevalent scenarios and compared the simulation results with the identical deployment in a real Kubernetes cluster. We achieved $\scriptstyle \mathtt {\sim }$ 81-99\% simulation accuracy in approximating the average response time of incoming requests and $\scriptstyle \mathtt {\sim }$ 16-1200 times speed-up factor for the simulation.},
  archive      = {J_TCC},
  author       = {Michel Gokan Khan and Javid Taheri and Auday Al-Dulaimy and Andreas Kassler},
  doi          = {10.1109/TCC.2021.3135757},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1395-1413},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PerfSim: A performance simulator for cloud native microservice chains},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). REN: Receiver-driven congestion control using explicit
notification for data center. <em>TCC</em>, <em>11</em>(2), 1381–1394.
(<a href="https://doi.org/10.1109/TCC.2021.3135027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, receiver-driven transport protocols have been proposed to use proactive congestion control to meet the stringent latency requirements of large-scale applications in data center. However, the receiver-driven proposals face the challenges brought by network dynamic. First, when the bursty flows start, the aggressive and blind line-rate transmission in the first RTT easily leads to persistent queue backlog. Second, when some flows finish transmissions, the remaining ones cannot increase their sending rates to seize the available bandwidth. To address these problems, this article presents a new receiver-driven congestion control design, called REN, which uses the under- and over-utilization notifications from switch to handle the dynamic traffic. With the aid of explicit feedback, REN alleviates the traffic burstiness due to aggressive start, mitigates the conservativeness in utilizing available bandwidth, and still retains the receiver-driven feature to achieve ultra-low latency. We implement the prototype of REN using DPDK. The experimental results of real testbed and large-scale NS2 simulation show that REN effectively reduces the average flow completion time (AFCT) by up to 68\% over the state-of-the-art receiver-driven transmission schemes.},
  archive      = {J_TCC},
  author       = {Zhaoyi Li and Jiawei Huang and Jinbin Hu and Weihe Li and Tao Zhang and Jingling Liu and Jianxin Wang and Tian He},
  doi          = {10.1109/TCC.2021.3135027},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1381-1394},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {REN: Receiver-driven congestion control using explicit notification for data center},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost minimizing reservation and scheduling algorithms for
public clouds. <em>TCC</em>, <em>11</em>(2), 1365–1380. (<a
href="https://doi.org/10.1109/TCC.2021.3133464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud Service Providers offer various pricing schemes to charge for their computational resources. Cloud Service Users can opt for on-demand , reserved , or spot instances for their requirements. The overall cost of an application depends on the instances chosen to execute the job. Finding the optimal reservation amount is a significant research problem, and it is more challenging when one uses spot instances with unpredictable prices. We have proposed two algorithms to determine the reservation amount, and for both algorithms, the future spot prices are unknown. But the first algorithm assumes that future demands are known. The second algorithm does not make any such assumption and yet can ensure that the cost of reservation and usage of cloud resources is within a factor $2-\frac{u_h}{e_c}$ of the optimal cost where $u_h$ is the usage cost per hour of the reserved instance and $e_c$ is the average cost per hour for unreserved instances. We have compared our findings with that of some recent works in the literature. We have also given an Integer Linear Programming (ILP) formulation of the problem. Experimental results show that our algorithm differs in cost from ILP by less than 21\%.},
  archive      = {J_TCC},
  author       = {Sharmistha Mandal and Giridhar Maji and Sunirmal Khatua and Rajib K. Das},
  doi          = {10.1109/TCC.2021.3133464},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1365-1380},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cost minimizing reservation and scheduling algorithms for public clouds},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A network load perception based task scheduler for parallel
distributed data processing systems. <em>TCC</em>, <em>11</em>(2),
1352–1364. (<a href="https://doi.org/10.1109/TCC.2021.3132627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In parallel distributed data processing frameworks like Spark and Flink, task scheduling has a great impact on cluster performance. Though task Scheduling has proven to be an NP-complete problem, a large number of researchers have proposed many heuristic rules to obtain approximate optimal solutions. But most of them ignore the fact that the resource requirements of tasks are dynamically changing during its runtime. Considering the overall task entire lives, the CPU utilization is often lower during the data transfer. Especially for most distributed data processing platforms, data transmission is time-consuming, which usually resulting in low overall CPU utilization. Similarly, network throughput during task calculations is also low in some cases. In this article, we propose a network load variation perception based heuristic task scheduling algorithm, and based on this implement a dual-phase pipeline task scheduler (D2PTS) from the perspective of dynamic resource requirements that aims at maximizing cluster resource utilization, as a supplement to existing data-parallel frameworks. D2PTS divides the states of task into two phases: network-intensive and network-free. To improve the overall resource utilities, this article proposes different algorithms to evaluate the execution time of network sensitive and network free phases respectively. When an executing task is in the network-free phase, D2PTS can additionally schedule a new network-intensive task at the right time. Under this scheduling policy, the two tasks sharing the same CPU core can be executed as a coarse-grained pipeline. This execution method can start tasks earlier and improve resource utilization. Finally, we have implemented our model prototype on Spark 2.4.3 and conducted a number of experiments to evaluate the performance of our model. Experimental results show that D2PTS can not only minimize application makespan, but also improve resource utilization.},
  archive      = {J_TCC},
  author       = {Zhuo Tang and Zhanfei Xiao and Li Yang and Kailin He and Kenli Li},
  doi          = {10.1109/TCC.2021.3132627},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1352-1364},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A network load perception based task scheduler for parallel distributed data processing systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective clustering evolutionary algorithm for
multi-workflow computation offloading in mobile edge computing.
<em>TCC</em>, <em>11</em>(2), 1334–1351. (<a
href="https://doi.org/10.1109/TCC.2021.3132175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To cope with the rapid development of the Internet of Things (IoT) and the increasing demand for real-time services, mobile edge computing (MEC) has become a promising solution which extends centralised cloud computing, to provision computing resources, storage and network services closer to the mobile device from the network edge. While computation offloading is a key feature in MEC to enable real-time services, offloading workflow tasks in MEC is an NP-hard problem. Typically, the problem of multi-workflow offloading with multi-objective optimization is still an open and challenging issue. Therefore, this article proposes a multi-objective clustering evolutionary algorithm called MCEA to minimize the cost and energy consumption of multi-workflow execution under the deadline constraint. First, the sub-deadline constraint is added during initialization to generate more initial solutions that satisfy the deadline constraint. Then an adaptive clustering method is adopted to guide individuals to find a suitable mate during crossover operation. Finally, the probabilities of crossover and mutation are dynamically adjusted based on the historical information to control the evolution direction and convergence speed of algorithm. Comprehensive experiments are carried out for complex workflow applications on FogWorkflowSim, which demonstrate that MCEA can achieve better performance than four representative algorithms in three evaluation metrics.},
  archive      = {J_TCC},
  author       = {Lei Pan and Xiao Liu and Zhaohong Jia and Jia Xu and Xuejun Li},
  doi          = {10.1109/TCC.2021.3132175},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1334-1351},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A multi-objective clustering evolutionary algorithm for multi-workflow computation offloading in mobile edge computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Service mapping and scheduling with uncertain processing
time in network function virtualization. <em>TCC</em>, <em>11</em>(2),
1315–1333. (<a href="https://doi.org/10.1109/TCC.2021.3132008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an optimization model for the network service (NS) mapping and scheduling problem with uncertain processing time in network function virtualization. We model processing time uncertainty through the $\Gamma$ -robustness approach, which provides different degrees of robustness against processing time uncertainty. We formulate the problem with the objective to minimize the worst-case makespan over the given uncertainty set. We show the NP-hardness of considered problem. A heuristic that divides the problem into subproblems is presented to tackle it. For the subproblem in which mapping and scheduling decisions are given, we develop an algorithm with polynomial time complexity to calculate the worst-case makespan over the uncertainty set, which has a better scalability than the corresponding mixed integer linear programming (MILP) problem and obtains the same worst-case makespan with the MILP problem. The numerical results show that the proposed model outperforms the conventional model with deterministic parameters in terms of worst-case makespan.},
  archive      = {J_TCC},
  author       = {Yuncan Zhang and Fujun He and Eiji Oki},
  doi          = {10.1109/TCC.2021.3132008},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1315-1333},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Service mapping and scheduling with uncertain processing time in network function virtualization},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Battery-assisted online operation of distributed data
centers with uncertain workload and electricity prices. <em>TCC</em>,
<em>11</em>(2), 1303–1314. (<a
href="https://doi.org/10.1109/TCC.2021.3132174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the online operation of distributed data centers equipped with energy battery. We aim to minimize their long-term operational cost by optimally distributing workload among data centers and operating energy battery. However, future spatio-temporally variant uncertainties in both workload and electricity prices have been the main impediment for a performance-guaranteed online data center operation strategy. To address this issue, we develop a fully distributed online algorithm that decouples workload distribution and battery operation across the network and time by introducing well-designed virtual queues for workload and batteries into the framework of Lyapunov optimization. Theoretically, an analytical gap between the long-term operational cost achieved by our algorithm and the theoretical optimum is provided to corroborate the desirable operation strategy. Extensive simulations using the real-world workload and electricity price data demonstrate the cost-delay tradeoff that our algorithm strikes and validate the theoretical results that we obtained.},
  archive      = {J_TCC},
  author       = {Jun Sun and Shibo Chen and Pengcheng You and Qinmin Yang and Zaiyue Yang},
  doi          = {10.1109/TCC.2021.3132174},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1303-1314},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Battery-assisted online operation of distributed data centers with uncertain workload and electricity prices},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and privacy-preserving similar patients query
scheme over outsourced genomic data. <em>TCC</em>, <em>11</em>(2),
1286–1302. (<a href="https://doi.org/10.1109/TCC.2021.3131287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, genomic data has grown exponentially and is widely used in promising medical and health-related applications, which opens up new opportunities for the field of medicine. Similar patients query (SPQ), which can help physicians formulate an optimal therapy, is one of such popular applications. Despite its popularity, since human genomes are usually highly sensitive, a series of policies have been launched by the government to strictly control its acquisitions and utilization. Thus, how to prevent privacy disclosure becomes of great importance to the flourish of SPQ services. In this article, aiming at the above challenge, we first design a novel genetic BK-tree (GBK-tree) for a genomic database. Then, combined with a random sorting mechanism and some existing encryption techniques, we propose an efficient and privacy-preserving similar patients query scheme over encrypted cloud data, named CASPER. With CASPER, a medical institution can securely outsource its private genomic database to a cloud server, and physicians can request SPQ services from the cloud server while keeping her/his query secret. Detailed security analysis shows that CASPER can preserve privacy in the presence of different threats. Furthermore, extensive performance evaluations demonstrate the high accuracy and efficiency of our proposed scheme.},
  archive      = {J_TCC},
  author       = {Dan Zhu and Hui Zhu and Xiangyu Wang and Rongxing Lu and Dengguo Feng},
  doi          = {10.1109/TCC.2021.3131287},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1286-1302},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient and privacy-preserving similar patients query scheme over outsourced genomic data},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient revocable storage attribute-based encryption with
arithmetic span programs in cloud-assisted internet of things.
<em>TCC</em>, <em>11</em>(2), 1273–1285. (<a
href="https://doi.org/10.1109/TCC.2021.3131686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Revocable storage and efficient description of the access policy are necessary to enhance the practicality of the attribute-based encryption (ABE) in real-life scenarios, such as cloud-assisted Internet of Things (IoT). Nevertheless, existing ABE works fail to balance the two vital factors. In this paper, we construct an efficient revocable storage ciphertext-policy attribute-based encryption with arithmetic span programs (RS-CPABE-ASP). The arithmetic span program (ASP) is elegantly utilized as the access structure to reduce the unnecessary cost for defining access policy. Combining the indirect revocation and the ciphertext update mechanism, our work prevents the revoked user unable to access the newly generated data and the old data that can be accessed before. As shown in the outsourced version of RS-CPABE-ASP, the costly part for users to decrypt the data can be outsourced to powerful cloud servers. In this way, users in our RS-CPABE-ASP are able to access their data in a more efficient way by merely one exponential operation. Finally, we carry out detailed theoretical analysis and experimental simulations to evaluate the performance of our work. The results fairly show that our proposed work is efficient and feasible in cloud-assisted IoT.},
  archive      = {J_TCC},
  author       = {Xin Huang and Hu Xiong and Jinhao Chen and Minghao Yang},
  doi          = {10.1109/TCC.2021.3131686},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1273-1285},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient revocable storage attribute-based encryption with arithmetic span programs in cloud-assisted internet of things},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RRFT: A rank-based resource aware fault tolerant strategy
for cloud platforms. <em>TCC</em>, <em>11</em>(2), 1257–1272. (<a
href="https://doi.org/10.1109/TCC.2021.3126677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The applications that are deployed in the cloud to provide services to the users encompass a large number of interconnected dependent cloud components. Multiple identical components are scheduled to run concurrently in order to handle unexpected failures and provide uninterrupted service to the end user, which introduces resource overhead problem for the cloud service provider. Furthermore such resource-intensive fault tolerant strategies bring extra monetary overhead to the cloud service provider and eventually to the cloud users. In order to address these issues, a novel fault tolerant strategy based on the significance level of each component is developed. The communication topology among the application components, their historical performance, failure rate, failure impact on other components, dependencies among them, etc., are used to rank those application components to further decide on the importance of one component over others. Based on the rank, a Markov Decision Process (MDP) model is presented to determine the number of replicas that varies from one component to another. A rigorous performance evaluation is carried out using some of the most common practically useful metrics such as, recovery time upon a fault, average number of components needed, number of parallel components successfully executed, etc., to quote a few, with similar component ranking and fault tolerant strategies. Simulation results demonstrate that the proposed algorithm reduces the required number of virtual and physical machines by approximately 10\% and 4.2\%, respectively, compared to other similar algorithms.},
  archive      = {J_TCC},
  author       = {Chinmaya Kumar Dehury and Prasan Kumar Sahoo and Bharadwaj Veeravalli},
  doi          = {10.1109/TCC.2021.3126677},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1257-1272},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {RRFT: A rank-based resource aware fault tolerant strategy for cloud platforms},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cloud-edge hosted digital twins for coordinated control of
distributed energy resources. <em>TCC</em>, <em>11</em>(2), 1242–1256.
(<a href="https://doi.org/10.1109/TCC.2022.3191837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel approach for realizing coordinated control of Distributed Energy Resources (DERs) based on cloud-hosted and edge-hosted digital twins (DTs) of DERs. DERs are playing an increasingly important role in supporting the frequency regulation of power systems with massive integration of renewable resources. However, due to the significant differences in DERs’ capability and characteristics, individual and un-coordinated responses from DERs could lead to a less effective overall response with undesirable traits, e.g., slow response, severe overshoots, etc. Therefore, the coordination of DERs is critical to ensure the desirable aggregated overall response. A major shortcoming of conventional centralized or distributed approaches is their significant reliance on real-time communications. This article addresses the challenges by the application of DTs that can be hosted in the cloud for the centralized control approach and the edge for the distributed approach to minimize the need for real-time communications, while being able to achieve the overall coordination among DERs. The proposed DT-based coordinated control is validated using a realistic real-time simulation test setup, and the results demonstrate that the DT-based coordinated control can significantly improve the aggregated DERs’ response, thus offering effective support to the grid during contingency events.},
  archive      = {J_TCC},
  author       = {Jiaxuan Han and Qiteng Hong and Mazheruddin H. Syed and Md Asif Uddin Khan and Guangya Yang and Graeme Burt and Campbell Booth},
  doi          = {10.1109/TCC.2022.3191837},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1242-1256},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud-edge hosted digital twins for coordinated control of distributed energy resources},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cloud-edge architecture with virtualized hardware
functionality for real-time diagnosis of transients in smart grids.
<em>TCC</em>, <em>11</em>(2), 1230–1241. (<a
href="https://doi.org/10.1109/TCC.2023.3241698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Cloud is providing unprecedented opportunities for IoT and WAMS (Wide Area Monitoring Systems) in electrical grid operation. It is an orchestrated environment able to address low latency events through appropriate edge-cloud computing configurations. Transient State Estimation (TSE) is a key monitoring tool for capturing a reliable knowledge of the Smart Grid status in real-time, given the impediments introduced by the increasing penetration of Distributed Energy Resources in the energy mix. Frequency response anomalies, large scale transients, and voltage swings can be captured by TSE for real time or post failure data analytics. This work presents a cloud edge framework for the efficient calculation of TSE which, albeit its benefits, demands high computational resources at the edge (close to the measurement units) along with ultra low latency communications. The framework enables TSE as a service through the coordination of Virtual Machines (VMs) running on virtualized infrastructure and other non-virtualized physical nodes. In order to support the stringent time requirements, part of the TSE is offloaded to dedicated hardware acceleration units (FPGA). The proposed TSE framework is validated using an IEEE 30-bus, and the results show a significant superiority in terms of total latency compared to conventional cloud and edge deployments.},
  archive      = {J_TCC},
  author       = {Nikolaos Tzanis and Eleftherios Mylonas and Panagiotis Papaioannou and Michael Birbas and Alexios Birbas and Christos Tranoris and Spyros Denazis and Alex Papalexopoulos},
  doi          = {10.1109/TCC.2023.3241698},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1230-1241},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud-edge architecture with virtualized hardware functionality for real-time diagnosis of transients in smart grids},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing voltage compliance in distribution network under
cloud and edge computing framework. <em>TCC</em>, <em>11</em>(2),
1217–1229. (<a href="https://doi.org/10.1109/TCC.2022.3149238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by government incentive policies and heightened environmental awareness by individuals, many regions around the world have seen a rapid rise in distributed energy resource (DER) penetration in electricity distribution networks. While high penetration of DER significantly helps facilitate the decarbonization in power and utitlies, it also brings unexpected operational challenges, among which voltage compliance has been a significant concern. To address this issue, the efficient load profile forecast, the operational framework and related strategies are critical challenges that need to be addressed urgently. Hence, this paper presents a cloud-edge computing-based framework to effectively operate the coupled medium-voltage (MV) and low-voltage (LV) distribution network. The high computational efficiency in cloud computing and low data latency in edge computing are presented and explored to coordinate the day-ahead and intraday operations ranging from different framework layers. Under the framework, a customer-level forecasting algorithm is employed to predict both day-ahead and real-time load profiles. Based on the prediction results, an optimization model based on unbalanced-three phase optimal power flow is proposed and solved by an efficient and accurate linearization-based approach that considers the controllability of on-load tap changers, distributed static var generator and the PV inverters. Simulations based on an extensive mocked MV-LV distribution network show the proposed forecasting method is adopted in real-time operations in terms of high accuracy and demonstrate the efficiency of the proposed optimization method in enhancing the voltage compliance in the network.},
  archive      = {J_TCC},
  author       = {Jiangxia Zhong and Bin Liu and Xinghuo Yu and Peter Wong and Zeyu Wang and Chongchong Xu and Xiaojun Zhou},
  doi          = {10.1109/TCC.2022.3149238},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1217-1229},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enhancing voltage compliance in distribution network under cloud and edge computing framework},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coordinated cloud-edge anomaly identification for active
distribution networks. <em>TCC</em>, <em>11</em>(2), 1204–1216. (<a
href="https://doi.org/10.1109/TCC.2022.3155441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensive integration of distributed energy resources transforms the conventional power distribution network into a complex, tightly-coupled cyber-physical system denoted as active distribution network (ADN). In this paper, a data-driven anomaly identification method based on the cloud-edge computing architecture is proposed to reduce the complicated risk of ADN operations. Especially, the proposed method would serve as a widely applicable and efficient line of defense against either cyber or physical anomalies. On one hand, a hierarchical data flow scheme is designed to balance the timeliness and economical requirements of anomaly identification by taking advantage of coordinated cloud-edge computing. On the other hand, an integrated algorithm is proposed on the basis of the proposed data flow architecture in order to identify anomalies with satisfactory robustness and accuracy, even under complicated operating conditions. Results of numerical experiments have validated the computational superiority of the proposed method over the state-of-the-art by simulating a variety of physical faults and cyber attacks.},
  archive      = {J_TCC},
  author       = {Bihuan Li and Dan Yu and Jun Wu and Ping Ju and Zhiyi Li},
  doi          = {10.1109/TCC.2022.3155441},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1204-1216},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Coordinated cloud-edge anomaly identification for active distribution networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cloud-edge orchestrated power dispatching for smart grid
with distributed energy resources. <em>TCC</em>, <em>11</em>(2),
1194–1203. (<a href="https://doi.org/10.1109/TCC.2022.3185170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud and edge computing are gradually used to achieve complex energy operation control and massive information processing in conventional power grid. Meanwhile, with the tremendous number of distributed energy resources and power equipment integrated into the smart grid enabled by cloud and edge computing, the centralized distribution network cannot realize the flexible and realtime energy supply due to the unpredictable and wide distribution of distributed energy resources, which will further deteriorate the stability of smart grid. To solve those problems, this article proposes energy centric smart grid to achieve power dispatching with the help of cloud-edge computing. Our solution uses energy caching and energy multiple addressing of the edge router to eliminate the intermittency of renewables and speed up energy response. For the stability of energy market and to encourage users to participate in power dispatching, a cloud-edge computing-driven energy cache orchestration mechanism is designed. The empirical results show that the response time is greatly reduced to meet the stringent quality of service requirement in smart grid integrated with distributed energy resources.},
  archive      = {J_TCC},
  author       = {Kuan Wang and Jun Wu and Xi Zheng and Jianhua Li and Wu Yang and Athanasios V. Vasilakos},
  doi          = {10.1109/TCC.2022.3185170},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1194-1203},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud-edge orchestrated power dispatching for smart grid with distributed energy resources},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving hybrid cloud framework for real-time
TCL-based demand response. <em>TCC</em>, <em>11</em>(2), 1182–1193. (<a
href="https://doi.org/10.1109/TCC.2022.3142009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Widespread advanced metering infrastructure and wide-area monitoring systems generate a significant amount of electricity load consumption data, which can facilitate eliciting end users’ temperature flexibility for demand response programs. However, the direct delivery of users’ load profiles is a threat to users’ privacy. So this paper proposes a privacy-preserving hybrid cloud framework for TCL-based demand response programs, composed of user private clouds and aggregation cloud. User clouds store users’ load profiles and elicit temperature flexibility by the proposed stable temperature-related regression model. In the aggregation cloud, this paper proposes the slope-priority flexibility aggregation method for the mean-variance analysis of aggregate flexibility and the XGBoost-accelerated disaggregation model for real-time selecting users based on users’ fitting coefficients. Hybrid cloud achieves privacy-preserving by separating flexibility eliciting models and aggregation/disaggregation methods into user private clouds and aggregation cloud. Numerical experiments verify that: 1) in user clouds, the stable regression model achieves less predict errors; 2) in aggregation cloud, the slope-priority method can achieve higher aggregate flexibility, and XGBoost-accelerated disaggregating reduces the solving time by nearly three orders of magnitude.},
  archive      = {J_TCC},
  author       = {Linwei Sang and Qinran Hu and Yinliang Xu and Zaijun Wu},
  doi          = {10.1109/TCC.2022.3142009},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1182-1193},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Privacy-preserving hybrid cloud framework for real-time TCL-based demand response},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MSIAP: A dynamic searchable encryption for
privacy-protection on smart grid with cloud-edge-end. <em>TCC</em>,
<em>11</em>(2), 1170–1181. (<a
href="https://doi.org/10.1109/TCC.2021.3134015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of 5G and the Internet of Things, edge computing and cloud computing with their respective strengths are bound as Cloud-Edge-End Orchestrated (CEEO). The CEEO network integrates artificial intelligence and provides innovative technologies for smart grid applications and services. With massive data transmission on the CEEO network, the trustworthiness of the service node exerts an enormous influence on data privacy. To realize securely share data and decrease the local storage, end-user prefer to encrypt data and upload it to the cloud. Meanwhile, the challenge is how to balance efficiency and security perfectly when users need to find relevant documents containing specific keywords from the CEEO network. In this article, we innovatively propose a searchable encryption scheme that supports multi-keyword subset retrieval, named MSIAP. Specifically, we enhance the Apriori, a data mining algorithm, to mine the relevance of files from massive information and build a multi-level index structure. On this basis, we achieve efficient multi-keyword subset retrieval and dynamic update with insignificant information disclosure in the smart grid. Furthermore, our MSIAP strengthens the present data retrieval methods and enormously reduces the time complexity to accommodate the system of distributed smart grid. Finally, we provide the security analysis and performance evaluations by comparing them with existing works.},
  archive      = {J_TCC},
  author       = {Kai Fan and Qi Chen and Ruidan Su and Kuan Zhang and Haoyang Wang and Hui Li and Yintang Yang},
  doi          = {10.1109/TCC.2021.3134015},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1170-1181},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MSIAP: A dynamic searchable encryption for privacy-protection on smart grid with cloud-edge-end},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An end-cloud collaborated framework for transferable
non-intrusive load monitoring. <em>TCC</em>, <em>11</em>(2), 1157–1169.
(<a href="https://doi.org/10.1109/TCC.2021.3132929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-intrusive load monitoring (NILM) benefits both end users and utilities by perceiving the operation of individual appliances within a household merely based on analytical results of aggregated electrical data. Driven by diverse demands, a basic and practical NILM solution is on call to enable various downstream applications and strengthen the transfer ability to adapt to different real scenarios. Accordingly, an end-cloud collaborated framework for transferable NILM is proposed in this work. First, an end-to-end model with a multi-scale convolutional architecture is designed for identifying the activation of a specified target appliance using current waveform, which can be independently deployed according to actual needs. Furthermore, a transfer learning framework of NILM is established. Primarily, the model pretrained on the cloud is continuously fine tuned on the terminal side in a pseudo-supervised manner, where the group-weighted cross entropy (GWCE) is defined as the loss function. According to the experimental results based on two public datasets, the proposed model structure possesses prominent generalization ability across different appliances and scenarios, and the transfer learning procedure with GWCE can enhance the identification ability of the pretrained model, which is especially effective for unfamiliar scenarios. Provided with a NILM device eligible for terminal-side intelligence, our work is applicable with great prospects in practice.},
  archive      = {J_TCC},
  author       = {Changyu Chen and Guangchao Geng and Heyang Yu and Zixuan Liu and Quanyuan Jiang},
  doi          = {10.1109/TCC.2021.3132929},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1157-1169},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An end-cloud collaborated framework for transferable non-intrusive load monitoring},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cloud-edge-end intelligence for fault-tolerant renewable
energy accommodation in smart grid. <em>TCC</em>, <em>11</em>(2),
1144–1156. (<a href="https://doi.org/10.1109/TCC.2021.3133540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart grid integrates the distributed energy resources such as renewable energy with massive information to facilitate the flow of energy in the industries. The renewable energy accommodation is one of the key issues to achieve the energy efficiency in smart grid, which is difficult to obtain dynamic optimal policies due to the intermittency of renewables. To capture statuses of renewable energy for decision-making, large amounts of information in heterogeneous forms are collected by massive end devices deployed in smart grid. Such information not only provides fruitful features for existing learning based algorithms but also incurs high computation complexity. Besides, such heterogeneous data may also contain missing values, which may result in wrong policies by existing algorithms. In this article, a novel cloud-edge-end orchestrated computing scheme is proposed to efficiently repair missing values and obtain optimal policies in two separate layers. In the first layer, deep learning based algorithms deployed can perceive the characteristics and repair the missing values. In the second layer, deep reinforcement learning based algorithms are employed to obtain optimal policies. Simulations on the real power grid dataset illustrate the effectiveness of proposed fault-tolerant renewable energy accommodation algorithm.},
  archive      = {J_TCC},
  author       = {Xueqing Yang and Xin Guan and Ning Wang and Yongnan Liu and Huayang Wu and Yan Zhang},
  doi          = {10.1109/TCC.2021.3133540},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1144-1156},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud-edge-end intelligence for fault-tolerant renewable energy accommodation in smart grid},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transactive operational framework for internet data centers
in geo-distributed local energy markets. <em>TCC</em>, <em>11</em>(2),
1133–1143. (<a href="https://doi.org/10.1109/TCC.2022.3162556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet data centers (IDCs), which can regulate the spatial and temporal load distribution, and manage on-site energy resources, are promising candidates to enhance the connected distribution network&#39;s operation. The recent emergence of local energy trading provides a new opportunity for IDCs to trade energy with end energy customers in low-voltage distribution networks and enhance their operational energy efficiency, and this has not been well investigated in the literature. Motivated by this, this paper proposes a two-stage transactive operation framework for a group of geo-distributed IDCs to engage in local energy markets. In the first stage, an ex-ante bidding model is proposed, which optimally schedules the IDCs’ cyber-energy resources (computing requests and on-site battery energy storage system) and determines energy trading prices in the local energy markets. The bidding model aims to minimize the cloud service provider (CSP)’s total cost. In the second stage, a real-time energy balancing model is proposed to adjust the IDCs’ energy volumes and faciliate them to offer energy balancing services to the power distribution networks in terms of alleviating energy supply-demand imbalances. The coupling relationship among the IDCs’ operation strategies, energy trading prices, and energy balancing signals are modeled in the framework. Extensive numerical case studies are implemented to demonstrate the effectiveness of the proposed framework. The simulation results show that the framework can reduce a considerable proportion of total operation cost for networked IDCs and can facilitate the CSP to effectively assist power distribution networks in balancing the energy supply and demand in real-time operation.},
  archive      = {J_TCC},
  author       = {Caishan Guo and Fengji Luo and Jiajia Yang and Zexiang Cai},
  doi          = {10.1109/TCC.2022.3162556},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1133-1143},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Transactive operational framework for internet data centers in geo-distributed local energy markets},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint optimization of computing offloading and service
caching in edge computing-based smart grid. <em>TCC</em>,
<em>11</em>(2), 1122–1132. (<a
href="https://doi.org/10.1109/TCC.2022.3163750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous expansion of the power Internet of Things (IoT) and the rapid increase in the number of Smart Devices (SDs), the data generated by SDs has exponentially increased. The traditional cloud-based smart grid cannot meet the low latency and high reliability requirements of emerging applications. By moving computing, data, and services from the centralized cloud to Edge Servers (ESs), edge computing exhibits excellent performance in communication delay and traffic reduction. Simultaneously, service caching also shows attractive advantages in handling the surge in data traffic. In this paper, we consider the joint optimization of computing offloading and service caching in edge computing-based smart grid, and formulate the problem as a Mixed-Integer Non-Linear Program (MINLP), aiming to minimize the task cost of the system. The original problem is decomposed into an equivalent master problem and sub-problem, and a Collaborative Computing Offloading and Resource Allocation Method (CCORAM) is proposed to solve the optimization problem, which includes two low-complexity algorithms. Specifically, a gradient descent allocation algorithm is first proposed to determine the computing resource allocation strategy, and then a game theory-based algorithm is proposed to determine the computing strategy. Simulation results show that CCORAM with low time complexity is very close to the optimal method, and performs much better than other benchmark methods.},
  archive      = {J_TCC},
  author       = {Huan Zhou and Zhenyu Zhang and Dawei Li and Zhou Su},
  doi          = {10.1109/TCC.2022.3163750},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1122-1132},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint optimization of computing offloading and service caching in edge computing-based smart grid},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Carbon-aware load balance control of data centers with
renewable generations. <em>TCC</em>, <em>11</em>(2), 1111–1121. (<a
href="https://doi.org/10.1109/TCC.2022.3150391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing environmental issues, the energy consumption and carbon emissions of data centers have become a major concern. However, in previous works, the cost and carbon reduction potential of geographically dispersed data centers with renewable generations is not fully explored due to the difficulty in matching renewable generations with stochastic incoming jobs. In this paper, the temporal and spatial variability of the carbon footprint and electricity price is revealed. Integrated with the distributed characteristics of renewable generations and geographically dispersed feature of the data centers, the carbon emission reduction potential of data centers with renewable generations is explored, leading to triple uncertainties in electricity price, fuel mix and renewable generation. To navigate such a potential, a virtual queue algorithm is designed, which makes online strategies for job scheduling of the data centers. By introducing historical correction terms, high-precision matching of the workload and renewable generation can be achieved with triple uncertainties. This leads to the economic and environmental friendliness of the proposed mechanism, which can achieve $O(\sqrt{T})$ expected regret and constraint violation. Simulations based on real-world data from several states of Australia demonstrate the effectiveness of the proposed framework in cost and carbon emission reduction with triple uncertainties.},
  archive      = {J_TCC},
  author       = {Wen-Ting Lin and Guo Chen and Huaqing Li},
  doi          = {10.1109/TCC.2022.3150391},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1111-1121},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Carbon-aware load balance control of data centers with renewable generations},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Introduction to special issue on
“cloud-edge-end orchestrated computing for smart grid.” <em>TCC</em>,
<em>11</em>(2), 1107–1110. (<a
href="https://doi.org/10.1109/TCC.2023.3271489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of distributed energy resources (DER) into the smart grid through digitalization has transformed the power grid into a more decentralized system, enhancing energy efficiency and resilience during significant catastrophes. However, the integration of a large number of DERs into the transmission and distribution networks poses reliability challenges due to the intermittent nature of renewable energy sources. To tackle this, smart grids have been using advanced metering infrastructure (AMI) and Internet of Things (IoT) devices for over two decades to improve grid observability and enable near-real-time forecasting of continent-wide anomalies. Cloud-edge-end orchestrated computing can achieve hierarchical management and innovative operational strategies, such as multi-level control and optimization of all-grid-level DERs, voltage and frequency regulation using phasor measurement units (PMU), and special protection schemes (SPS) to detect and prevent potential faults or large-scale cyber-attacks.},
  archive      = {J_TCC},
  author       = {Ruilong Deng and Chee-Wooi Ten and Chaojie Li and Dusit Niyato and Fei Teng},
  doi          = {10.1109/TCC.2023.3271489},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {2},
  pages        = {1107-1110},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Guest editorial: Introduction to special issue on “Cloud-edge-end orchestrated computing for smart grid”},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forward and backward secure searchable encryption scheme
supporting conjunctive queries over bipartite graphs. <em>TCC</em>,
<em>11</em>(1), 1091–1102. (<a
href="https://doi.org/10.1109/TCC.2021.3131176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic searchable encryption, which allows clients to outsource their encrypted data to cloud servers and retain the ability to query and update data, has received wide attention. In the setting, it is essential to ensure that a server infers as little as possible about the content of the outsourced data and the queries it processes. In this article, we propose a forward and backward secure searchable encryption scheme on bipartite graphs (FBSSE-BG) that offers the strongest level of backward security. In particular, we introduce the notion of update counter to construct a new bi-directional index structure, which realizes conjunctive queries over bipartite graphs and supports flexible updates of outsourced data. Besides, to minimize the information revealed to servers, we propose a new oblivious data structure to store the bi-directional index and use a semantically-secure encryption scheme to encrypt node information, such that servers can only observe a series of ORAM locations and encrypted paths. Finally, we prove the security of FBSSE-BG by using the real-world versus ideal-world formalization and provide experimental efficiency evaluations for its implementations.},
  archive      = {J_TCC},
  author       = {Mingyue Li and Chunfu Jia and Ruizhong Du and Wei Shao},
  doi          = {10.1109/TCC.2021.3131176},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {1091-1102},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Forward and backward secure searchable encryption scheme supporting conjunctive queries over bipartite graphs},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Carbon management of multi-datacenter based on
spatio-temporal task migration. <em>TCC</em>, <em>11</em>(1), 1078–1090.
(<a href="https://doi.org/10.1109/TCC.2021.3130644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the massive deployment of geographically distributed data centers (DCs) and the demand surge for cloud services, their high energy consumption and carbon pollution are increasingly problematic. Therefore, mitigating the harmful effects of the carbon footprint of DCs has become a critical challenge. This paper takes the first step toward analyzing the complementary characteristics of global multi-regional renewable energy sources (RES). There is an opportunity to schedule workloads flexibly and track RES to reduce emissions. Integrating DCs and inter-DC network to form a refined emission model for the trade-off between emission cutting effects from scheduling and carbon costs of workload migration, we propose the spatio-temporal task migration mechanism to pursue low carbon in dual-dimension: This paper shifts intensive workloads to locations sufficient in RES at a coarse scale, and adjust the execution time of the workload in response to real-time RES fluctuations at a fine scale. Thus, the emission overage is shifted and offset by RES in a complementary manner; meanwhile, the acceptance of RES is enhanced. Finally, experiments with real-world data show that our method can optimally coordinate demand with RES and mitigate carbon pollution in geographically distributed DCs, and verify the performance and applicability with various-parameter scenarios.},
  archive      = {J_TCC},
  author       = {Ting Yang and Han Jiang and Yucheng Hou and Yinan Geng},
  doi          = {10.1109/TCC.2021.3130644},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {1078-1090},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Carbon management of multi-datacenter based on spatio-temporal task migration},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A provable softmax reputation-based protocol for
permissioned blockchains. <em>TCC</em>, <em>11</em>(1), 1065–1077. (<a
href="https://doi.org/10.1109/TCC.2021.3130244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a hierarchical structure of a permissioned blockchain with three types of participant: providers, collectors, and governors. Providers forward transactions to collectors; collectors upload received transactions to governors after verifying and labeling them; and governors validate a portion of the labeled transactions they receive, pack valid transactions into a block, and append the block to the ledger. This model has various fields of application including data collection from the Internet-of-Things and second-hand markets. Our main contribution is to propose a reputation-based protocol to help governors evaluate the reliability of collectors. Specifically, given a transaction, each governor runs a softmax-based function to calculate a probability for each collector that sent and labeled this transaction. The probabilities, calculated using collectors’ reputations as inputs, represent the likelihood of the lead governor selecting the labeled transaction from collectors to consider for further validation. After the lead governor verifies a transaction, all collectors’ reputations are updated in line with the agreement of their labeling and the validity of the transaction as found by the lead governor. We show, both theoretically and empirically, that our protocol can significantly reduce governors’ verification workloads while maintaining firm liveness and high incentives.},
  archive      = {J_TCC},
  author       = {Hongyin Chen and Zhaohua Chen and Yukun Cheng and Xiaotie Deng and Wenhan Huang and Jichen Li and Hongyi Ling and Mengqian Zhang},
  doi          = {10.1109/TCC.2021.3130244},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {1065-1077},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A provable softmax reputation-based protocol for permissioned blockchains},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic parallel flow algorithms with centralized scheduling
for load balancing in cloud data center networks. <em>TCC</em>,
<em>11</em>(1), 1050–1064. (<a
href="https://doi.org/10.1109/TCC.2021.3129768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BCube is a well-known recursively defined network structure. It provides multiple low-diameter paths and good fault-tolerance for data center networks (DCNs). Its distributed routing algorithm, BCube Source Routing (BSR), can be deployed rapidly and conveniently to build multiple parallel path sets. But in the worst case, BSR may suffer from flow collisions and waste $50\%$ of the capacity of each BCube link. In this paper, to decrease collisions and improve bandwidth utilization, we supplement the BCube topology with a central master computer and design two centralized dynamic parallel flow scheduling algorithms: CDPFS and CDPFSMP, for single-path and multi-path respectively. We focus on finding the least congested path for each flow by analyzing the information about the state of the global network. Furthermore, we allocate those paths to each flow in parallel. The simulation result shows that our proposed algorithms take advantage of BCube structure and deliver high-performance solutions for load balancing problems, which have improved 44.1\% of the throughput in random bijective traffic pattern and 36.2\% of throughput in data shuffle compared with BSR algorithm.},
  archive      = {J_TCC},
  author       = {Wei-Kang Chung and Yun Li and Chih-Heng Ke and Sun-Yuan Hsieh and Albert Y. Zomaya and Rajkumar Buyya},
  doi          = {10.1109/TCC.2021.3129768},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {1050-1064},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic parallel flow algorithms with centralized scheduling for load balancing in cloud data center networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flash: Joint flow scheduling and congestion control in data
center networks. <em>TCC</em>, <em>11</em>(1), 1038–1049. (<a
href="https://doi.org/10.1109/TCC.2021.3129511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flow scheduling and congestion control are two important techniques to reduce flow completion time in data center networks. While existing works largely treat them independently, the interactions between flow scheduling and congestion control are in general overlooked which leads to sub-optimal solutions, especially given that the link capacity is increasing faster than the switch port buffer size. In this paper, we present Flash , a simple yet effective scheme that integrates scheduling and congestion control. Specifically, Flash puts forward a congestion-aware scheduling scheme to determine the priority of flows based on the latest network congestion extent and the flow’s bytes sent. Besides, Flash proposes a priority-based packet dropping scheme in switch port buffers and implements a priority-aware congestion control scheme. Experiment results show that Flash has superior performance: (1) it has 35.8\% lower tail latency than PIAS and performs similar with pFabric in a 10G network without knowing the flow size, (2) in 100G networks with shallow buffers, the information agnostic Flash has 6.8\% lower average FCT than the information-aware pFabric, (3) it outperforms pFabric by 13.5\% in FCT if flow size is also known to Flash .},
  archive      = {J_TCC},
  author       = {Chengxi Gao and Shuhui Chu and Hong Xu and Minxian Xu and Kejiang Ye and Chengzhong Xu},
  doi          = {10.1109/TCC.2021.3129511},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {1038-1049},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Flash: Joint flow scheduling and congestion control in data center networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Outsourcing data processing jobs with lithops. <em>TCC</em>,
<em>11</em>(1), 1026–1037. (<a
href="https://doi.org/10.1109/TCC.2021.3129000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unexpectedly, the rise of serverless computing has also collaterally started the “democratization” of massive-scale data parallelism. This new trend heralded by PyWren pursues to enable untrained users to execute single-machine code in the cloud at massive scale through platforms like AWS Lambda. Driven by this vision, this article presents Lithops , which carries forward the pioneering work of PyWren to better exploit the innate parallelism of à la MapReduce tasks atop several Functions-as-a-Service platforms such as AWS Lambda, IBM Cloud Functions, Google Cloud Functions or Knative. Instead of waiting for a cluster to be up and running in the cloud, Lithops makes easy the task of spawning hundreds and thousands of cloud functions to execute a large job in a few seconds from start. With Lithops, for instance, users can painlessly perform exploratory data analysis from within a Jupyter notebook, while it is the Lithops’s engine which takes care of launching the parallel cloud functions, loading dependencies, automatically partitioning the data, etc. In this article, we describe the design and innovative features of Lithops and evaluate it using several representative applications, including sentiment analysis, Monte Carlo simulations, and hyperparameter tuning. These applications manifest the Lithops’ ability to scale single-machine code computations to thousands of cores. And very importantly, without the need of booting a cold cluster or keeping a warm cluster for occasional tasks.},
  archive      = {J_TCC},
  author       = {Josep Sampé and Marc Sánchez-Artigas and Gil Vernik and Ido Yehekzel and Pedro García-López},
  doi          = {10.1109/TCC.2021.3129000},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {1026-1037},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Outsourcing data processing jobs with lithops},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bottleneck-aware non-clairvoyant coflow scheduling with fai.
<em>TCC</em>, <em>11</em>(1), 1011–1025. (<a
href="https://doi.org/10.1109/TCC.2021.3128360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coflow scheduling is critical to data-parallel applications in data centers. While schemes like Varys can achieve optimal performance, they require a priori information about coflows which is hard to obtain in practice. Existing non-clairvoyant solutions like Aalo generalize least attained service (LAS) scheduling discipline to address this issue. However, they fail to identify the bottleneck flows in a coflow and tend to allocate excessive bandwidth to the non-bottleneck flows, leading to bandwidth wastage and inferior overall performance. To this end, we present Fai that strives to improve the overall coflow performance by accelerating the bottleneck flows without priori knowledge. Fai employs bottleneck-aware scheduling. It adopts loose coordination to update coflow priority and flow rates based on total bytes sent. In addition, Fai detects bottleneck flows based on a flow’s rate and bytes sent, and de-allocates bandwidth for other flows to match the bottleneck rate without affecting the coflow completion time (CCT). The saved bandwidth is then distributed among coflows according to their priority to improve overall performance. Testbed evaluation on a 40-node cluster shows that Fai improves average (P95) CCT by 1.73× (3.43×), compared to Aalo. Large-scale trace-driven simulations also show that Fai outperforms Aalo substantially.},
  archive      = {J_TCC},
  author       = {Libin Liu and Chengxi Gao and Peng Wang and Hongming Huang and Jiamin Li and Hong Xu and Wei Zhang},
  doi          = {10.1109/TCC.2021.3128360},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {1011-1025},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Bottleneck-aware non-clairvoyant coflow scheduling with fai},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sharing-aware task offloading of remote rendering for
interactive applications in mobile edge computing. <em>TCC</em>,
<em>11</em>(1), 997–1010. (<a
href="https://doi.org/10.1109/TCC.2021.3127345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging emerging mobile edge computing and 5G networks, researchers proposed to offload the 3D rendering of interactive applications (e.g. virtual reality and cloud gaming) onto GPU-based edge servers to reduce the user experienced latency. A task offloading problem arises, that is where to offload rendering tasks such that each user will experience tolerable delay and meanwhile the cost of used servers is minimized. The multi-dimensional resource sharing feature of rendering tasks makes the problem challenging. We formulate the task offloading problem into a boolean linear programming. We propose a sharing-aware offloading algorithm which decomposes the problem into two subproblems (user assignment and server packing) and solves them alternately and iteratively. We compare our algorithm with the one without resource sharing in consideration, and the simulations demonstrate that our method can effectively reduce cost as well as satisfy delay requirement.},
  archive      = {J_TCC},
  author       = {Ruitao Xie and Junhong Fang and Junmei Yao and Xiaohua Jia and Kaishun Wu},
  doi          = {10.1109/TCC.2021.3127345},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {997-1010},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Sharing-aware task offloading of remote rendering for interactive applications in mobile edge computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scheduling bag-of-tasks in clouds using spot and burstable
virtual machines. <em>TCC</em>, <em>11</em>(1), 984–996. (<a
href="https://doi.org/10.1109/TCC.2021.3125426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud providers offer several types of Virtual Machines (VMs) in diverse markets, with different guarantees in terms of availability and reliability. Among them, the most popular market models are the on-demand and the spot. On-demand VMs are allocated for a fixed cost per time, and their availability is ensured during the whole execution. On the other hand, in the spot market, VMs are offered with a huge discount, but their availability fluctuates according to cloud’s current demand that can terminate or hibernate a spot VM at any time. Furthermore, to cope with workload variations, cloud providers have also introduced the concept of burstable VMs, which can burst up their CPU performance during a limited period of time. In this work, we present the Burst Hibernation-Aware Dynamic Scheduler (Burst-HADS), a framework that executes Bag-of-Tasks applications with deadline constraints by exploiting both spot and on-demand burstable VMs, aiming at minimizing both the monetary cost and the execution time. Performance results on Amazon EC2 show that Burst-HADS reduces the monetary cost and meets the application deadline even in spot hibernation scenarios, when compared to other approaches from the related literature which uses only spot and non-burstable on-demand instances.},
  archive      = {J_TCC},
  author       = {Luan Teylo and Luciana Arantes and Pierre Sens and Lúcia Maria de A. Drummond},
  doi          = {10.1109/TCC.2021.3125426},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {984-996},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Scheduling bag-of-tasks in clouds using spot and burstable virtual machines},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-keyword searchable and verifiable attribute-based
encryption over cloud data. <em>TCC</em>, <em>11</em>(1), 971–983. (<a
href="https://doi.org/10.1109/TCC.2021.3119407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud data sharing systems, Searchable Encryption (SE) schemes ensure data confidentiality with retrieving, but it faces several issues in practice. First, most of the previous Ciphertext-Policy Attribute-Based Keyword Search (CP-ABKS) systems enable users to initiate search requests with a single keyword, which results in many inaccurate results to be returned, thereby wasting computing and bandwidth resources. Second, untrusted cloud servers may return a small portion of incomplete search results to compress communication overhead. Besides, most CP-ABKS schemes only support an unshared multi-owner setting, which incurs a large amount of computational and storage overhead. Furthermore, when the keyword space is a polynomial, most of the previous schemes suffer from offline keyword guessing attacks. To address these issues, we focus on a multi-keyword search scheme which supports the verification of search results without losing efficiency by combining Ciphertext Policy Attribute-Based Encryption (CP-ABE) technology under the shared multi-owner mechanism. We show the security of our scheme, which achieves selective security against offline keyword guessing attacks and guarantees the unforgeability of signatures. The comparison of experimental results illustrates that our scheme is effective and enjoys superior functionalities than the most relevant solutions.},
  archive      = {J_TCC},
  author       = {Yinghui Zhang and Tian Zhu and Rui Guo and Shengmin Xu and Hui Cui and Jin Cao},
  doi          = {10.1109/TCC.2021.3119407},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {971-983},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-keyword searchable and verifiable attribute-based encryption over cloud data},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CoMCLOUD: Virtual machine coalition for multi-tier
applications over multi-cloud environments. <em>TCC</em>,
<em>11</em>(1), 956–970. (<a
href="https://doi.org/10.1109/TCC.2021.3122445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications hosted in commercial clouds are typically multi-tier and comprise multiple tightly coupled virtual machines (VMs). Service providers (SPs) cater to the users using VM instances with different configurations and pricing depending on the location of the data center (DC) hosting the VMs. However, selecting VMs to host multi-tier applications is challenging due to the trade-off between cost and quality of service (QoS) depending on the placement of VMs. This paper proposes a multi-cloud broker model called CoMCLOUD to select a sub-optimal VM coalition for multi-tier applications from an SP with minimum coalition pricing and maximum QoS. To strike a trade-off between the cost and QoS, we use an ant-colony-based optimization technique. The overall service selection game is modeled as a first-price sealed-bid auction aimed at maximizing the overall revenue of SPs. Further, as the hosted VMs often face demand spikes, we present a parallel migration strategy to migrate VMs with minimum disruption time. Detailed experiments show that our approach can improve the federation profit up to 23\% at the expense of increased latency of approximately 15\%, compared to the baselines.},
  archive      = {J_TCC},
  author       = {Sourav Kanti Addya and Anurag Satpathy and Bishakh Chandra Ghosh and Sandip Chakraborty and Soumya K. Ghosh and Sajal K. Das},
  doi          = {10.1109/TCC.2021.3122445},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {956-970},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {CoMCLOUD: Virtual machine coalition for multi-tier applications over multi-cloud environments},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generic construction of black-box traceable attribute-based
encryption. <em>TCC</em>, <em>11</em>(1), 942–955. (<a
href="https://doi.org/10.1109/TCC.2021.3121684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-based encryption (ABE) has been widely used to provide fine-grained access control to encrypted data in cloud computing. However, in practice, since the attributes are shared by multiple users in the ABE system, it is difficult to identify malicious users which intentionally but implicitly reveal their own ABE secret keys for some purpose. In recent years, a substantial amount of work focused on designing efficient tracing mechanisms to identify such misbehaving users on ABE. In this article, we propose an efficient generic construction of black-box traceable ABE to identify the traitors. Our construction supports (public) adaptive black-box traceability and has short keys and short ciphertexts that are independent of the number of users. Technically, our construction relies on a generic transformation from any attribute-based inner-product functional encryption (ABIPFE) scheme to black-box traceable ABE system. Furthermore, to instantiate our generic construction from standard lattices, we propose the first lattice-based ABIPFE scheme, which is proven weakly selective secure in the standard model.},
  archive      = {J_TCC},
  author       = {Fucai Luo and Saif Al-Kuwari},
  doi          = {10.1109/TCC.2021.3121684},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {942-955},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Generic construction of black-box traceable attribute-based encryption},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latency and energy-aware load balancing in cloud data
centers: A bargaining game based approach. <em>TCC</em>, <em>11</em>(1),
927–941. (<a href="https://doi.org/10.1109/TCC.2021.3121481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid surge in cloud services, cloud load balancing has become a paramount research issue. The major part of a cloud computing system&#39;s operational costs is attributed to energy consumption. Therefore, to provide better QoS, considering the energy minimization factor in load balancing is essential. This paper addresses the latency and energy-aware load balancing problem in a cloud computing system. Specifically, two fundamental performance criteria–response time and energy–for the load balancing problem are considered. To solve this problem, first, the load balancing problem is formulated as an optimization problem. Then it is modeled as a cooperative game so that the solution of the game, called the Nash bargaining solution (NBS), can simultaneously optimize both criteria. The existence and computation of NBS are analyzed theoretically, and an efficient algorithm, called ${{\sf L}}$ atency and ${{\sf E}}$ nergy a ${{\sf W}}$ are load balancIng ${{\sf S}}$ cheme ( ${{\sf LEWIS}}$ ), is proposed to compute the NBS. Further, to assess the efficacy of ${{\sf LEWIS}}$ , it is compared with three other approaches, i.e., ${\mathsf {Coop\_{RT}}}$ , ${\mathsf {Coop\_{EN}}}$ , and ${\mathsf {NCG}}$ , on problem instances of various settings. The experimental results show that ${{\sf LEWIS}}$ not only provides less response time while consuming less energy but also gauntness fairness to the end-users.},
  archive      = {J_TCC},
  author       = {Avadh Kishor and Rajdeep Niyogi and Anthony Theodore Chronopoulos and Albert Y. Zomaya},
  doi          = {10.1109/TCC.2021.3121481},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {927-941},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Latency and energy-aware load balancing in cloud data centers: A bargaining game based approach},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SDTP: Accelerating wide-area data analytics with
simultaneous data transfer and processing. <em>TCC</em>, <em>11</em>(1),
911–926. (<a href="https://doi.org/10.1109/TCC.2021.3119991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the efficient analysis of geo-distributed datasets, cloud providers implement data-parallel jobs across geo-distributed sites (e.g., datacenters and edge clusters), which are generally interconnected by wide-area network links. However, current state-of-the-art geo-distributed data analytic methods fail to make full use of the available network and computing resources. The main reason is that such geo-distributed methods must wait for bottleneck sites to complete the corresponding transmission and computation in each phase. Furthermore, such geo-distributed methods may be impractical to the network bandwidth dynamicity and diverse job parallelism. To this end, we propose a Simultaneous Data Transfer and Processing (SDTP) mechanism to accelerate wide-area data analytics, with the joint consideration of network bandwidth dynamics and job parallelism. In the SDTP, a site can execute the computation, provided that it obtains the required input data. As a result, the input data loading, map, shuffle, and reduce phases at each site need not wait for the completion of the previous phases of other sites. We further improve the SDTP method by offering more accurate time estimation and generalizing the mechanism to dynamic situations. The trace-driven results demonstrate that SDTP can improve the wide-area analytic job response time by 19\% to 72\% compared to other methods.},
  archive      = {J_TCC},
  author       = {Yiting Chen and Lailong Luo and Deke Guo and Ori Rottenstreich and Jie Wu},
  doi          = {10.1109/TCC.2021.3119991},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {911-926},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SDTP: Accelerating wide-area data analytics with simultaneous data transfer and processing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fixed-point iteration approach to spark scalable performance
modeling and evaluation. <em>TCC</em>, <em>11</em>(1), 897–910. (<a
href="https://doi.org/10.1109/TCC.2021.3119943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Companies depend on mining data to grow their business more than ever. To achieve optimal performance of Big Data analytics workloads, a careful configuration of the cluster and the employed software framework is required. The lack of flexible and accurate performance models, however, render this a challenging task. This article fills this gap by presenting accurate performance prediction models based on Stochastic Activity Networks (SANs). In contrast to existing work, the presented models consider multiple work queues, a critical feature to achieve high accuracy in realistic usage scenarios. We first introduce a monolithic analytical model for a multi-queue YARN cluster running DAG-based Big Data applications that models each queue individually. To overcome the limited scalability of the monolithic model, we then present a fixed-point model that iteratively computes the throughput of a single queue with respect to the rest of the system until a fixed-point is reached. The models are evaluated on a real-world cluster running the widely-used Apache Spark framework and the YARN scheduler. Experiments with the common transaction-based TPC-DS benchmark show that the proposed models achieve an average error of only $5.6\%$ in predicting the execution time of the Spark jobs. The presented models enable businesses to optimize their cluster configuration for a given workload and thus to reduce their expenses and minimize service level agreement (SLA) violations. Makespan minimization and per-stage analysis are examined as representative efforts to further assess the applicability of our proposition.},
  archive      = {J_TCC},
  author       = {Soroush Karimian-Aliabadi and Mohammad-Mohsen Aseman-Manzar and Reza Entezari-Maleki and Danilo Ardagna and Bernhard Egger and Ali Movaghar},
  doi          = {10.1109/TCC.2021.3119943},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {897-910},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Fixed-point iteration approach to spark scalable performance modeling and evaluation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Searchable encryption with autonomous path delegation
function and its application in healthcare cloud. <em>TCC</em>,
<em>11</em>(1), 879–896. (<a
href="https://doi.org/10.1109/TCC.2021.3120110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outsourcing medical data to healthcare cloud has become a popular trend. Since medical data of patients contain sensitive personal information, they should be encrypted before outsourcing. However, information retrieval methods based on plaintext cannot be directly applied to encrypted data. In this article, we present a new cryptographic primitive named conjunctive keyword search with secure channel free and autonomous path delegation function (AP-SCF-PECKS), which can be applied in scenarios where patients want to search for and autonomous delegate their private medical information without revealing their private key. Particularly, the proposed solution allows patients to set up multi-hop delegation path with their preferences, and the delegated doctors in the path can search for and access the patient’s private medical information with priority from high to low. Patients can ensure that authorized doctors are always trustworthy, and unauthorized users cannot obtain the private medical information of patients. Moreover, the scheme supports the conjunctive keyword search, secure channel free, and is secure against chosen keyword attack, chosen ciphertext attack, and keyword guessing attack. The security of proposed scheme has been formally proved in the standard model. Finally, the performance evaluations demonstrate that the overhead of proposed scheme are modest for healthcare cloud scenarios.},
  archive      = {J_TCC},
  author       = {Qian Wang and Chengzhe Lai and Rongxing Lu and Dong Zheng},
  doi          = {10.1109/TCC.2021.3120110},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {879-896},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Searchable encryption with autonomous path delegation function and its application in healthcare cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-effective and latency-minimized data placement strategy
for spatial crowdsourcing in multi-cloud environment. <em>TCC</em>,
<em>11</em>(1), 868–878. (<a
href="https://doi.org/10.1109/TCC.2021.3119862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an increasingly mature business model, crowdsourcing, especially spatial crowdsourcing, has played an important role in data collection, disaster response, urban planning and other fields. However, the rapid growth of user scale and massive data collected inevitably brings serious challenges to computing and storage resources. The emergence of cloud computing provides an opportunity to handle such challenges. Its nearly unlimited resource provision capability can provide reliable services for different crowdsourcing applications. Nevertheless, considering the risks of privacy leakage and vendor lock-in using only a single cloud, as well as the additional restrictions caused by the wide geographical distribution of data and associations among workers, the use of multi-cloud seems to be a better choice. In this article, we define a problem to find an effective data placement scheme for spatial crowdsourcing in multi-cloud environment to achieve the cost-effectiveness and minimal latency. We take full account of the interval pricing strategy. Then we analyze the geographical distribution characteristics of data centers through a clustering algorithm, and propose an effective data initialization strategy. Finally, we use a genetic algorithm to further optimize the results. Through experiments on real-world data from cloud providers, the efficiency and effectiveness of our proposed method is verified. Compared with some existing algorithms, the proposed method can significantly reduce the system cost and latency, among which the cost reduction is up to 150 times and the latency reduction is up to twice.},
  archive      = {J_TCC},
  author       = {Pengwei Wang and Zhen Chen and MengChu Zhou and Zhaohui Zhang and Abdullah Abusorrah and Ahmed Chiheb Ammari},
  doi          = {10.1109/TCC.2021.3119862},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {868-878},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cost-effective and latency-minimized data placement strategy for spatial crowdsourcing in multi-cloud environment},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gemini: Enabling multi-tenant GPU sharing based on kernel
burst estimation. <em>TCC</em>, <em>11</em>(1), 854–867. (<a
href="https://doi.org/10.1109/TCC.2021.3119205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen rapid adoption of GPUs in various types of platforms because of the tremendous throughput powered by massive parallelism. However, as the computing power of GPU continues to grow at a rapid pace, it also becomes harder to utilize these additional resources effectively with the support of GPU sharing. In this work, we designed and implemented Gemini , a user-space runtime scheduling framework to enable fine-grained GPU allocation control with support for multi-tenancy and elastic allocation, which are critical for cloud and resource providers. Our key idea is to introduce the concept of kernel burst , which refers to a group of consecutive kernels launched together without being interrupted by synchronous events. Based on the characteristics of kernel burst, we proposed a low overhead event-driven monitor and a dynamic time-sharing scheduler to achieve our goals. Our experiment evaluations using five types of GPU applications show that Gemini enabled multi-tenant and elastic GPU allocation with less than 5\% performance overhead. Furthermore, compared to static scheduling, Gemini achieved 20\% $\sim$ 30\% performance improvement without requiring prior knowledge of applications.},
  archive      = {J_TCC},
  author       = {Hung-Hsin Chen and En-Te Lin and Yu-Min Chou and Jerry Chou},
  doi          = {10.1109/TCC.2021.3119205},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {854-867},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Gemini: Enabling multi-tenant GPU sharing based on kernel burst estimation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards efficient verifiable boolean search over encrypted
cloud data. <em>TCC</em>, <em>11</em>(1), 839–853. (<a
href="https://doi.org/10.1109/TCC.2021.3118692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetric Searchable Encryption (SSE) schemes facilitate searching over encrypted data, and have been extensively explored to improve function, efficiency or security. There are, however, additional functions that we need to consider in a real-world setting. For example, forward and backward privacy are required to adequately secure newly added documents and deleted documents in Dynamic SSE (DSSE) schemes, and support boolean search (that allows users to search over encrypted data using basic boolean operations) to achieve improved efficiency and retrieval accuracy. Therefore, in this article we first construct the Verifiable Boolean Search over encrypted data (VBS), and then improve VBS to achieve Forward and Backward privacy (VBS-FB). Finally, we formally prove the security of our proposed schemes, and evaluate their performance using real-world datasets.},
  archive      = {J_TCC},
  author       = {Feng Li and Jianfeng Ma and Yinbin Miao and Zhiquan Liu and Kim-Kwang Raymond Choo and Ximeng Liu and Robert H. Deng},
  doi          = {10.1109/TCC.2021.3118692},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {839-853},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Towards efficient verifiable boolean search over encrypted cloud data},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Share your data carefree: An efficient, scalable and
privacy-preserving data sharing service in cloud computing.
<em>TCC</em>, <em>11</em>(1), 822–838. (<a
href="https://doi.org/10.1109/TCC.2021.3117998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the powerful computing and storage capabilities of cloud services, data sharing in the cloud has been permeated across various applications including social networks, e-health and crowdsourcing transportation system. Intuitively, outsourcing data to untrusted cloud commonly raises concerns about data privacy breaches. To combat this, one approach is exploiting Broadcast Based Searchable Encryption (BBSE) for secure data sharing. Nevertheless, the latest proposed BBSE is still defective in either security or efficiency. In this article, we propose ESPD, an Efficient, Scalable and Privacy-preserving Data sharing framework over encrypted cloud dataset. Different from previous works, ESPD supports sharing target data to multiple users with distinct secret keys, and keeps a constant ciphertext length with the changes of the amount of system users. This feature significantly improves search efficiency and makes ESPD scalable in real-world scenarios. We show a formal analysis to prove the security of ESPD in terms of file privacy, keyword privacy and trapdoor privacy. Also, extensive experiments on real-world dataset are conducted to indicate the desirable performance of ESPD compared to other similar schemes.},
  archive      = {J_TCC},
  author       = {Jianfei Sun and Guowen Xu and Tianwei Zhang and Hu Xiong and Hongwei Li and Robert H. Deng},
  doi          = {10.1109/TCC.2021.3117998},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {822-838},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Share your data carefree: An efficient, scalable and privacy-preserving data sharing service in cloud computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Network slicing for NOMA-enabled edge computing.
<em>TCC</em>, <em>11</em>(1), 811–821. (<a
href="https://doi.org/10.1109/TCC.2021.3117754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 5G network presents a new horizon with tremendous opportunities for future generation wireless networks. Mobile edge computing (MEC), non-orthogonal multiple access (NOMA), and network slicing (NS) are some of the key enablers for 5G. MEC reduces the latency to a great extent for a wireless network, while NOMA gives access to more users with resource constraints. NS provides users with a better quality of service and network operators with more flexibility. In this work, we propose an NS technique enabled with NOMA for a MEC network. The proposed NS technique improves service latency for MEC users and reduces unnecessary allocation of radio resources in NOMA. The saved resources can be leveraged to accommodate more users, thus increasing the spectral efficiency of the network. We consider different types of services based on the task completion time of users in this work. The primary focus is to optimize the total energy consumption for wireless uplink transmission for the NOMA-enabled sliced MEC network. We also propose a heuristic algorithm as an alternative to reduce the time and computational complexities of the optimization algorithm and simulate the results extensively to show the effectiveness of our proposed algorithm.},
  archive      = {J_TCC},
  author       = {Mohammad Arif Hossain and Nirwan Ansari},
  doi          = {10.1109/TCC.2021.3117754},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {811-821},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Network slicing for NOMA-enabled edge computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RDMA-enabled concurrency control protocols for transactions
in the cloud era. <em>TCC</em>, <em>11</em>(1), 798–810. (<a
href="https://doi.org/10.1109/TCC.2021.3116516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online transaction processing (OLTP) is widely used on modern cloud infrastructures to complete important businesses such as payments and stock exchanges. Remote Direct Memory Access (RDMA) is a technology that enables ultra-low inter-server memory access latency - which is critical for implementing high-performance concurrency control protocols in distributed OLTP. In this article, we develop RCC, the first unified and comprehensive RDMA-enabled distributed transaction processing framework containing six serializable concurrency control protocols—not only the classical protocols ${{\sf NOWAIT}}$ , ${{\sf WAITDIE}}$ , ${{\sf OCC}}$ , but also more advanced ${{\sf MVCC}}$ , ${{\sf SUNDIAL}}$ , and ${{\sf CALVIN}}$ — the deterministic protocol. Our goal is to unbiasedly compare protocols on OLTP workloads in a common execution environment with the concurrency control protocol being the only changeable component. From RCC, we obtained new insights on building RDMA-based protocols. We analyzed stage-wise latency breakdown to develop more efficient hybrid implementations. Moreover, RCC can enumerate all stage-wise hybrid designs under a given workload characteristic. Our results show that throughput-wise hybrid designs are better than RPC or one-sided counterparts by 32.2\% and up to 67\%; three hybrid designs are better than their pure counterparts by up to 17.8\%. RCC can provide performance insights and be used as the common open-sourced infrastructure for fast prototyping new implementations.},
  archive      = {J_TCC},
  author       = {Chao Wang and Xuehai Qian},
  doi          = {10.1109/TCC.2021.3116516},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {798-810},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {RDMA-enabled concurrency control protocols for transactions in the cloud era},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimized dynamic cache instantiation and accurate LRU
approximations under time-varying request volume. <em>TCC</em>,
<em>11</em>(1), 779–797. (<a
href="https://doi.org/10.1109/TCC.2021.3115959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-delivery applications can achieve scalability and reduce wide-area network traffic using geographically distributed caches. However, each deployed cache has an associated cost, and under time-varying request rates (e.g., a daily cycle) there may be long periods when the request rate from the local region is not high enough to justify this cost. Cloud computing offers a solution to problems of this kind, by supporting dynamic allocation and release of resources. In this article, we analyze the potential benefits from dynamically instantiating caches using resources from cloud service providers. We develop novel analytic caching models that accommodate time-varying request rates, transient behavior as a cache fills following instantiation, and selective cache insertion policies. Within the context of a simple cost model, we then develop bounds and compare policies with optimized parameter selections to obtain insights into key cost/performance tradeoffs. We find that dynamic cache instantiation can provide substantial cost reductions, that potential reductions strongly dependent on the object popularity skew, and that selective cache insertion can be even more beneficial in this context than with conventional edge caches. Finally, our contributions also include accurate and easy-to-compute approximations that are shown applicable to LRU caches under time-varying workloads.},
  archive      = {J_TCC},
  author       = {Niklas Carlsson and Derek Eager},
  doi          = {10.1109/TCC.2021.3115959},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {779-797},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimized dynamic cache instantiation and accurate LRU approximations under time-varying request volume},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strong temporal isolation among containers in OpenStack for
NFV services. <em>TCC</em>, <em>11</em>(1), 763–778. (<a
href="https://doi.org/10.1109/TCC.2021.3116183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the problem of temporal isolation among containerized software components running in shared cloud infrastructures is tackled, proposing an approach based on hierarchical real-time CPU scheduling. This allows for reserving a precise share of the available computing power for each container deployed in a multi-core server, so to provide it with a stable performance, independently from the load of other co-located containers. The proposed technique enables the use of reliable modeling techniques for end-to-end service chains that are effective in controlling the application-level performance. An implementation of the technique within the well-known OpenStack cloud orchestration software is presented, focusing on a use-case framed in the context of network function virtualization. The modified OpenStack is capable of leveraging the special real-time scheduling features made available in the underlying Linux operating system through a patch to the in-kernel process scheduler. The effectiveness of the technique is validated by gathering performance data from two applications running in a real test-bed with the mentioned modifications to OpenStack and the Linux kernel. A performance model is developed that tightly models the application behavior under a variety of conditions. Extensive experimentation shows that the proposed mechanism is successful in guaranteeing isolation of individual containerized activities on the platform.},
  archive      = {J_TCC},
  author       = {Tommaso Cucinotta and Luca Abeni and Mauro Marinoni and Riccardo Mancini and Carlo Vitucci},
  doi          = {10.1109/TCC.2021.3116183},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {763-778},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Strong temporal isolation among containers in OpenStack for NFV services},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards enabling residential virtual-desktop computing.
<em>TCC</em>, <em>11</em>(1), 745–762. (<a
href="https://doi.org/10.1109/TCC.2021.3113905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commercial virtual-desktop computing is well established using computers optimized to serve as thin clients connected to centralized computing systems. Some common residential applications impose more stringent requirements on both communication bandwidth and latency than those of typical commercial applications. This article describes an objective study of residential applications accessed through thin-client virtual desktops for the purpose of investigating the feasibility of applying virtual-desktop computing to residential users. New metrics are introduced to quantify user-received application performance. The results suggest that certain commercial solutions with a commodity datacenter server show a strong potential for being adapted to residential virtual-desktop computing.},
  archive      = {J_TCC},
  author       = {Hongying Dong and Aaron T. Kinfe and Jiakai Yu and Qi Liu and Dan Kilper and Ronald D. Williams and Malathi Veeraraghavan},
  doi          = {10.1109/TCC.2021.3113905},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {745-762},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Towards enabling residential virtual-desktop computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint optimization across timescales: Resource placement and
task dispatching in edge clouds. <em>TCC</em>, <em>11</em>(1), 730–744.
(<a href="https://doi.org/10.1109/TCC.2021.3113605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Internet of Things (IoT) data and innovative mobile services has promoted an increasing need for low-latency access to resources such as data and computing services. Mobile edge computing has become an effective computing paradigm to meet the requirement for low-latency access by placing resources and dispatching tasks at the edge clouds near mobile users. The key challenge of such solution is how to efficiently place resources and dispatch tasks in the edge clouds to meet the QoS of mobile users or maximize the platform’s utility. In this article, we study the joint optimization problem of resource placement and task dispatching in mobile edge clouds across multiple timescales under the dynamic status of edge servers. We first propose a two-stage iterative algorithm to solve the joint optimization problem in different timescales, which can handle the varieties among the dynamic of edge resources and/or tasks. We then propose a reinforcement learning (RL) based algorithm which leverages the learning capability of Deep Deterministic Policy Gradient (DDPG) technique to tackle the network variation and dynamic as well. The results from our trace-driven simulations demonstrate that both proposed approaches can effectively place resources and dispatching tasks across two timescales to maximize the total utility of all scheduled tasks.},
  archive      = {J_TCC},
  author       = {Xinliang Wei and A B M Mohaimenur Rahman and Dazhao Cheng and Yu Wang},
  doi          = {10.1109/TCC.2021.3113605},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {730-744},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint optimization across timescales: Resource placement and task dispatching in edge clouds},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed self-adjusting tree networks. <em>TCC</em>,
<em>11</em>(1), 716–729. (<a
href="https://doi.org/10.1109/TCC.2021.3112067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of many data-centric cloud applications critically depends on the performance of the underlying datacenter network. Reconfigurable optical technologies have recently introduced a novel opportunity to improve datacenter network performance, by allowing to dynamically adjust the network topology according to the demand. However, the vision of self-adjusting networks raises the fundamental question how such networks can be efficiently operated in a scalable and distributed manner. This article presents $DiSplayNet$ , the first fully distributed self-adjusting network. $DiSplayNet$ relies on algorithms that perform decentralized and concurrent topological adjustments to account for changes in the demand. We propose two natural metrics to evaluate the performance of distributed self-adjusting networks, the amortized work (the cost of routing on and adjusting the network) and the makespan (the time it takes to serve a set of communication requests). We present a rigorous formal analysis of the work and makespan of $DiSplayNet$ , which can be seen as an interesting generalization of analyses known from sequential self-adjusting datastructures. We complement our theoretical contribution with an extensive trace-driven simulation study, shedding light on the opportunities and limitations of leveraging spatial and temporal locality and concurrency in self-adjusting networks.},
  archive      = {J_TCC},
  author       = {Bruna Soares Peres and Otávio Augusto de Oliveira Souza and Olga Goussevskaia and Chen Avin and Stefan Schmid},
  doi          = {10.1109/TCC.2021.3112067},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {716-729},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Distributed self-adjusting tree networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DynaQ: Enabling protocol-independent service queue isolation
in cloud data centers. <em>TCC</em>, <em>11</em>(1), 704–715. (<a
href="https://doi.org/10.1109/TCC.2021.3110276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Switches in cloud data centers support multiple service queues per port to provide differentiated network performance among different traffic classes. To isolate service queues, recent solutions leverage the power of Explicit Congestion Notification (ECN). However, this causes a fundamental dependency on ECN-based transport protocols, making it hard to use generic transport protocols. To this end, we design DynaQ, a protocol-independent multi-queue management solution that enables service queue isolation with generic transport protocols. The key idea of DynaQ is to adjust the packet dropping threshold of service queues dynamically. Specifically, DynaQ allows a service queue to occupy free buffer space but prevents the queue from hurting other active queues. Our solution requires only a few additional clock cycles to implement on hardware. To evaluate DynaQ comprehensively, we conduct a series of testbed experiments and large-scale simulations. Our evaluation results show that, compared to alternative schemes, DynaQ is the only solution that achieves work-conserving weighted fair sharing and low latency without protocol dependency.},
  archive      = {J_TCC},
  author       = {Gyuyeong Kim and Wonjun Lee},
  doi          = {10.1109/TCC.2021.3110276},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {704-715},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {DynaQ: Enabling protocol-independent service queue isolation in cloud data centers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cloud mining pool aided blockchain-enabled internet of
things: An evolutionary game approach. <em>TCC</em>, <em>11</em>(1),
692–703. (<a href="https://doi.org/10.1109/TCC.2021.3110965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past few years have witnessed an exponential growth of diverse Internet of Things (IoT) devices as well as compelling applications ranging from industrial production to medical care. Dramatic advances in IoT technology not only brought enormous economic opportunities but also challenges (e.g., privacy and security vulnerabilities). Recently, with the appearance of blockchain technology, the integration of IoT and blockchain (BCoT) is considered a promising solution to address these issues. Blockchain provides a secure and scalable data management framework for IoT devices. However, the huge computation and energy cost of the consensus process in blockchain prevents it from being directly applied as a generic platform. To overcome this challenge, in this article, we propose a cloud mining pool-aided BCoT architecture, where the IoT devices can rent the computing resources from the cloud mining pools to offload the mining process. Based on this architecture, we study the mining pool selection problem and analyze the colony behaviors of IoT devices with different pooling strategies. We propose a centralized evolutionary game-based pool selection algorithm for the sake of maximizing the system utility. Considering the non-cooperative relationship among multiple miners, we also propose a lightweight distributed reinforcement learning algorithm, named the ‘WoLF-PHC’ algorithm.},
  archive      = {J_TCC},
  author       = {Tianle Mai and Haipeng Yao and Ni Zhang and Lexi Xu and Mohsen Guizani and Song Guo},
  doi          = {10.1109/TCC.2021.3110965},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {692-703},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud mining pool aided blockchain-enabled internet of things: An evolutionary game approach},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving traceable attribute-based keyword search
in multi-authority medical cloud. <em>TCC</em>, <em>11</em>(1), 678–691.
(<a href="https://doi.org/10.1109/TCC.2021.3109282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud-based electronic medical record (EMR) systems, attribute-based encryption (ABE) has been utilized to protect the confidentiality of EMRs and provide keyword search over the encrypted EMRs. However, existing schemes are designed for a single attribute authority, and lack sufficient user privacy protection. In this article, we introduce TABKS, a privacy-preserving traceable attribute-based keyword search scheme in multi-authority medical cloud. First, we propose an anonymous EMR access control framework with multiple authorities, which provides user anonymity against the untrusted authorities. Second, we achieve traceable attribute-based Boolean keyword search, which enables the authorized user who satisfies the policy to conduct Boolean keyword search over the encrypted EMRs. In this process, TABKS improves the efficiency of legitimate users by partially decrypting the matched results, and also achieves efficient traitor trace by revealing the user identity from the trapdoor. Finally, we prove the security of TABKS against chosen plaintext attack and chosen keyword attack, and conduct extensive experiments with two real-world datasets to show the feasibility of TABKS.},
  archive      = {J_TCC},
  author       = {Qinlong Huang and Guanyu Yan and Yixian Yang},
  doi          = {10.1109/TCC.2021.3109282},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {678-691},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Privacy-preserving traceable attribute-based keyword search in multi-authority medical cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards diversified IoT image recognition services in mobile
edge computing. <em>TCC</em>, <em>11</em>(1), 666–677. (<a
href="https://doi.org/10.1109/TCC.2021.3109385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet of Things (IoT) and emerging Mobile Edge Computing (MEC) technologies, various IoT image recognition services are revolutionizing our lives by providing diverse cognitive assistance. However, most existing related approaches are difficult to meet the diversified needs of users because they believe that the MEC platform is a single layer. In addition, due to the mutual interference between the data, it is not easy for them to extract the discriminative features (DFs) necessary to analyze the input data. To this end, this article proposes an IoT image recognition services framework for different needs in the MEC environment, which consists of Hierarchical Discriminative Feature Extraction (HDFE) and Sub-extractor Deployment (Sub-ED) algorithms. We first propose HDFE, which can avoid mutual interference between data by separately optimizing the data structure, thereby generating an extractor that extracts effective DFs. Then there is Sub-ED, which divides the extractor into a series of sub-extractors and deploys them on appropriate MEC platforms. By doing so, the IoT device can connect to the corresponding MEC platform according to its service types, and use the sub-extract to extract DFs. Then, the MEC platform uploads the extracted feature data to the cloud server for further processing, e.g., feature matching. Finally, the cloud server sends the processed result back to the IoT device. Experimental results show that compared with the state-of-the-art approaches, the proposed framework improves recognition accuracy by about 6\% and reduces network traffic by up to 94\%.},
  archive      = {J_TCC},
  author       = {Chuntao Ding and Ao Zhou and Xiao Ma and Ning Zhang and Ching-Hsien Hsu and Shangguang Wang},
  doi          = {10.1109/TCC.2021.3109385},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {666-677},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Towards diversified IoT image recognition services in mobile edge computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infrastructure-efficient virtual-machine placement and
workload assignment in cooperative edge-cloud computing over backhaul
networks. <em>TCC</em>, <em>11</em>(1), 653–665. (<a
href="https://doi.org/10.1109/TCC.2021.3107596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing provides computing capability at close-user proximity to reduce service latency for end users. To improve the efficiency of edge computing infrastructures, geographically-distributed edge datacenters can co-work with each other and with cloud datacenters, forming a new paradigm referred to as cooperative edge-cloud computing. In this context, applications typically run on a virtual machine (VM) that can be replicated at multiple sites, and thus user traffic can be served at all the sites where corresponding VMs reside. For the performance of many applications, latency is a critical parameter. In this work, taking applications’ latencies as the primary constraint, we model the problem of “VM placement and workload assignment” as a mixed integer linear program and develop heuristic algorithms accordingly. The goal is to minimize the consumption of information technology (IT) infrastructures for placing VMs in cooperative edge-cloud computing, while meeting the heterogeneous latency demands of different applications. Some preliminary results indicate that edge datacenter&#39;s resource efficiency can be optimized by proper cross-site VM placement and workload re-direction.},
  archive      = {J_TCC},
  author       = {Wei Wang and Massimo Tornatore and Yongli Zhao and Haoran Chen and Yajie Li and Abhishek Gupta and Jie Zhang and Biswanath Mukherjee},
  doi          = {10.1109/TCC.2021.3107596},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {653-665},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Infrastructure-efficient virtual-machine placement and workload assignment in cooperative edge-cloud computing over backhaul networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic resource provisioning for iterative workloads on
apache spark. <em>TCC</em>, <em>11</em>(1), 639–652. (<a
href="https://doi.org/10.1109/TCC.2021.3108043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Apache Spark as a popular in-memory data analytic framework has been employed by various applications—such as machine learning, graph computation, and scientific computing, which benefit from the long-running process (e.g., executor) programming model to avoid system I/O overhead. However, existing resource allocation strategies mainly rely on the peak demand, which are normally specified by users. Since the resource usages of long-running applications like iterative computation vary significantly over time, we find that peak demand based resource allocation policies lead to low cloud utilization in production environments. In this article, we present a utilization aware resource provisioning approach for iterative workloads on Apache Spark (i.e., ${iSpark}$ ). It can identify the causes of resource underutilization due to an inflexible resource policy, and elastically adjusts the allocated executors over time according to the real-time resource usage. In general, iterative applications require more computation resources at the beginning stage and their demands for resources diminish as more iterations are completed. iSpark aims to timely scale up or scale down the number of executors in order to fully utilize the allocated resources while taking the dominant factor into consideration. It further preempts the underutilized executors and preserves the cached intermediate data to ensure the data consistency. Testbed evaluations show that iSpark averagely improves the resource utilization of individual executors by 35.2\% compared to vanilla Spark. At the same time, it increases the cluster utilization from 32.1\% to 51.3\% and effectively reduces the overall job completion time by 20.8\% for a set of representative iterative applications. Furthermore, we have extended iSpark to multi-tenancy cloud environments. Specifically, iSpark characterizes a virtual node based on two real-time measured performance statistics: I/O rate and CPU steal time. Thus, we extend the two-dimensional resource constraints (i.e., CPU and MeM) in iSpark to three-dimensional resource constraints (i.e., CPU, MeM and I/O) in the cloud environment. We consider two representative interference scenarios in the cloud: stable interference and dynamic interference. Experimental results on virtual clusters with varying interferences show that iSpark with cloud extension improves the average job completion time by 68\% compared to default Spark resource allocation policies.},
  archive      = {J_TCC},
  author       = {Dazhao Cheng and Yu Wang and Dong Dai},
  doi          = {10.1109/TCC.2021.3108043},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {639-652},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic resource provisioning for iterative workloads on apache spark},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint planning of network slicing and mobile edge computing:
Models and algorithms. <em>TCC</em>, <em>11</em>(1), 620–638. (<a
href="https://doi.org/10.1109/TCC.2021.3107022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access Edge Computing (MEC) facilitates the deployment of critical applications with stringent QoS requirements, latency in particular. This article considers the problem of jointly planning the availability of computational resources at the edge, the slicing of mobile network and edge computation resources, and the routing of heterogeneous traffic types to the various slices. These aspects are intertwined and must be addressed together to provide the desired QoS to all mobile users and traffic types still keeping costs under control. We formulate our problem as a mixed-integer nonlinear program (MINLP) and we define a heuristic, named Neighbor Exploration and Sequential Fixing (NESF), to facilitate the solution of the problem. The approach allows network operators to fine tune the network operation cost and the total latency experienced by users. We evaluate the performance of the proposed model and heuristic against two natural greedy approaches. We show the impact of the variation of all the considered parameters (viz., different types of traffic, tolerable latency, network topology and bandwidth, computation and link capacity) on the defined model. Numerical results demonstrate that NESF is very effective, achieving near-optimal planning and resource allocation solutions in a very short computing time even for large-scale network scenarios.},
  archive      = {J_TCC},
  author       = {Bin Xiang and Jocelyne Elias and Fabio Martignon and Elisabetta Di Nitto},
  doi          = {10.1109/TCC.2021.3107022},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {620-638},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint planning of network slicing and mobile edge computing: Models and algorithms},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-complexity and high-coding-efficiency image deletion for
compressed image sets in cloud servers. <em>TCC</em>, <em>11</em>(1),
608–619. (<a href="https://doi.org/10.1109/TCC.2021.3106103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deletion refers to removing images from a compressed image set in cloud servers, which has always received much attention. However, in some cases images are not successfully deleted, and coding performance still remains to rise. In this paper, we propose a low-complexity and high-coding-efficiency image deletion algorithm. First, all the images are classified into to-be-deleted images, images unneeded to be processed, and images needed to be processed further divided into images needed to be only decoded and images needed to be re-encoded. Then, we also propose a depth- and subtree-constrained minimum spanning tree (DSCMST) heuristics to produce the DSCMST of images needed to be processed. Third, every image unneeded to be processed is added to the just obtained DSCMST as the child of the vertex that is still its parent in the compressed image set. Finally, after the encoding of images needed to be re-encoded, a new compressed image set is constructed, implying the completion of image deletion. Experimental results show that under various circumstances our proposed algorithm can effectively remove any images, including root vertex, internal vertices, and leaf vertices. Moreover, compared with state-of-the-art methods, the proposed algorithm achieves higher coding efficiency while having the minimum complexity.},
  archive      = {J_TCC},
  author       = {Lina Sha and Wei Wu and Bingbing Li},
  doi          = {10.1109/TCC.2021.3106103},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {608-619},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Low-complexity and high-coding-efficiency image deletion for compressed image sets in cloud servers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Storage-saving scheduling policies for clusters running
containers. <em>TCC</em>, <em>11</em>(1), 595–607. (<a
href="https://doi.org/10.1109/TCC.2021.3104662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container technology plays an important role in the virtualization landscape today. A container uses its own file system consisting of a stack of layers, which are stored on the execution server&#39;s disk. Containers running on the same server share the layers they have in common, and this sharing results in valuable savings in server storage space. Many containers can run on a single server, but when their resource demands grow enough, they are distributed across a cluster of nodes/servers by orchestration systems, such as Kubernetes. In this work, we found that for the same amount of containers to run, the storage required is higher for clusters consisting of a larger number of nodes. The severity of this storage overhead depends on the scheduling policy used to select the nodes that run the containers. By comparing different storage-saving scheduling policies that differ from each other in the depth of storage knowledge they leverage to make decisions, our analysis reveals that only deep, layer-level knowledge can effectively counter the growth in storage demand as the cluster size increases. Policies with coarser-grained knowledge achieve limited benefit because they achieve performance that is nearly equal to that of a random, zero-knowledge policy.},
  archive      = {J_TCC},
  author       = {Ludovico Funari and Luca Petrucci and Andrea Detti},
  doi          = {10.1109/TCC.2021.3104662},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {595-607},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Storage-saving scheduling policies for clusters running containers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secure remote cloud file sharing with attribute-based access
control and performance optimization. <em>TCC</em>, <em>11</em>(1),
579–594. (<a href="https://doi.org/10.1109/TCC.2021.3104323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing popularity of remote Cloud File Sharing (CFS) has become a major concern for privacy breach of sensitive data. Aiming at this concern, we present a new resource sharing framework by integrating enterprise-side Attribute-Based Access Control/eXtensible Access Control Markup Language (ABAC/XACML) model, client-side Ciphertext-Policy Attribute-Based Encryption (CP-ABE) scheme, and cloud-side CFS service. Moreover, the framework workflow is provided to support the encrypted-file writing and reading algorithms in accordance with ABAC/XACML-based access policy and attribute credentials. However, an actual problem of realizing this framework is that policy matrix, derived from access policy, seriously affects the performance of existing CP-ABE from Lattice (CP-ABE-L) schemes. To end it, we present an optimal generation algorithm of Small Policy Matrix (SPM), which only consists of small elements, and generates an all-one reconstruction vector. Based on such a matrix, the improved CP-ABE-L scheme is proposed to reduce the cumulative errors to the minimum and prevent the enlargement of error bounds. Furthermore, we give the optimal estimation of system parameters to implement a valid Error Proportion Allocation (EPA). Our experimental results indicate that our scheme has short size of parameters and enjoys efficient computation and storage overloads. Thus, our new framework with optimization methods is conducive to enhancing the security and efficiency of remote work on CFS.},
  archive      = {J_TCC},
  author       = {E Chen and Yan Zhu and Kaitai Liang and Hongjian Yin},
  doi          = {10.1109/TCC.2021.3104323},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {579-594},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure remote cloud file sharing with attribute-based access control and performance optimization},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secure and efficient online fingerprint authentication
scheme based on cloud computing. <em>TCC</em>, <em>11</em>(1), 564–578.
(<a href="https://doi.org/10.1109/TCC.2021.3103546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy protection of biometrics-based on cloud computing is attracting increasing attention. In 2018, Zhu et al. proposed an efficient and privacy-preserving online fingerprint authentication scheme for data outsourcing e-Finga. Under the premise of ensuring user&#39;s fingerprint data privacy and message security authentication, the e-Finga scheme can provide accurate and efficient fingerprint identity authentication services. However, our analysis shows that the temporary fingerprint in this scheme uses the deterministic encryption algorithm, which has the risk of leaking the user&#39;s fingerprint characteristics. Therefore, we propose a temporary fingerprint attack method for the e-Finga scheme. Experiments demonstrate that an adversary can analyze specific secret parameters and fingerprint features when eavesdropping on a user&#39;s temporary fingerprint ciphertext. To counter the temporary fingerprint attack, we propose a secure e-fingerprint scheme– Secure e-finger that uses the learning with errors samples, which has the homomorphic addition property, to encrypt user&#39;s temporary fingerprints. Experiments show that the secure e-finger scheme can resist the temporary fingerprint attack. Compared with the unprotected e-Finga scheme, the client running time is increased by about 6\% percent, the communication cost on the user side only increased by 0.3125\% percent. As a result, our solution can realize secure online fingerprint authentication without losing efficiency. Single user authentication is likely to cause the problem of excessive authority. Based on the Secure e-finger scheme, we propose a threshold scheme based on biological characteristics.},
  archive      = {J_TCC},
  author       = {Yao Liu and Tanping Zhou and Zelun Yue and Wenchao Liu and Yiliang Han and Qi Li and Xiaoyuan Yang},
  doi          = {10.1109/TCC.2021.3103546},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {564-578},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure and efficient online fingerprint authentication scheme based on cloud computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HybCon: A scalable SDN-based distributed cloud architecture
for 5G networks. <em>TCC</em>, <em>11</em>(1), 550–563. (<a
href="https://doi.org/10.1109/TCC.2021.3103935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a time where data traffic is booming, Software-defined Networking (SDN) is becoming the most plausible solution to cope with the conventional networks’ shortcomings. The separation of the control (software) and the data (hardware) planes makes of SDN a technology-enabled to a multitude of new and promising applications and use cases for 5G network technology. Yet, initial SDN deployments considered having a single controller for the entire network. However, this arrangement is deemed to be counterproductive in large-scale and ultra-reliable networks based SDN as the controller might become the system&#39;s bottleneck. To address this issue, several multi-controller architectures were proposed. They are of three types: flat, hierarchical or hybrid. For theses architectures to be potent for 5G core networks, efficient path computation and flow tables update mechanisms are needed. In this paper, we propose HybCon, a hybrid SDN-based distributed Cloud architecture having the control plane distributed over different controller types (i.e., Fog, Edge, SDN). When a path computation request is received, HybCon involves some/all of the controllers and uses a multi-priority queueing model along with node parallelism to expedite the path computation and to cut down the synchronization overhead. Simulation results show that HybCon outperforms existing schemes in terms of path computation time, path setup latency, packet delivery ratio and communication overhead, making it a perfect solution for future 5G networks.},
  archive      = {J_TCC},
  author       = {Djabir Abdeldjalil Chekired and Mohammed Amine Togou and Lyes Khoukhi},
  doi          = {10.1109/TCC.2021.3103935},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {550-563},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {HybCon: A scalable SDN-based distributed cloud architecture for 5G networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-aware routing in fog computing systems.
<em>TCC</em>, <em>11</em>(1), 532–549. (<a
href="https://doi.org/10.1109/TCC.2021.3102996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing enables the execution of IoT applications on compute nodes which reside both in the cloud and at the edge of the network. To achieve this, most fog computing systems route the IoT data on a path which starts at the data source, and goes through various edge and cloud nodes. Each node on this path may accept the data if there are available resources to process this data locally. Otherwise, the data is forwarded to the next node on path. Notably, when the data is forwarded (rather than accepted), the communication latency increases by the delay to reach the next node. To avoid this, we propose a routing mechanism which maintains a history of all nodes that have accepted data of each context in the past. By processing this history, our mechanism sends the data directly to the closest node that tends to accept data of the same context. This lowers the forwarding by nodes on path, and can reduce the communication latency. We evaluate this approach using both prototype- and simulation-based experiments which show reduced communication latency (by up to 23 percent) and lower number of hops traveled (by up to 73 percent), compared to a state-of-the-art method.},
  archive      = {J_TCC},
  author       = {Vasileios Karagiannis and Pantelis A. Frangoudis and Schahram Dustdar and Stefan Schulte},
  doi          = {10.1109/TCC.2021.3102996},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {532-549},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Context-aware routing in fog computing systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating distributed learning in non-dedicated
environments. <em>TCC</em>, <em>11</em>(1), 515–531. (<a
href="https://doi.org/10.1109/TCC.2021.3102593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) models are increasingly trained with distributed workers possessing heterogeneous resources. In such scenarios, model training efficiency may be negatively affected by stragglers —workers that run much slower than others. Efficient model training requires eliminating such stragglers, yet for modern ML workloads, existing load balancing strategies are inefficient and even infeasible. In this article, we propose a novel strategy, called semi-dynamic load balancing , to eliminate stragglers of distributed ML workloads. The key insight is that ML workers shall be load-balanced at iteration boundaries , being non-intrusive to intra-iteration execution. Based on it we further develop LB-BSP, an integrated worker coordination mechanism that adapts workers’ load to their instantaneous processing capabilities—by right-sizing the sample batches at the synchronization barriers. We have designed distinct load tuning algorithms for ML in CPU clusters, in GPU clusters as well as in federated learning setups, based on their respective characteristics. LB-BSP has been implemented as a Python module for ML frameworks like TensorFlow and PyTorch. Our EC2 deployment confirms that LB-BSP is practical, effective and light-weight, and is able to accelerating distributed training by up to 54 percent.},
  archive      = {J_TCC},
  author       = {Chen Chen and Qizhen Weng and Wei Wang and Baochun Li and Bo Li},
  doi          = {10.1109/TCC.2021.3102593},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {515-531},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Accelerating distributed learning in non-dedicated environments},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online/offline rewritable blockchain with auditable
outsourced computation. <em>TCC</em>, <em>11</em>(1), 499–514. (<a
href="https://doi.org/10.1109/TCC.2021.3102031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy-based chameleon hash (PCH) is one of the techniques used for rewriting transaction-level data stored in blockchains. This technique integrates the access policy of the attribute-based encryption (ABE) in the transactions and only allows users with attributes set satisfying the access policy to modify the transactions. However, some operations in the PCH-based rewritable blockchain solution require high computational cost which may impact the performance of user systems, especially on resource-constrained devices. To solve this problem, we propose an online/offline rewritable blockchain with auditable outsourced computation (OO-RB-AOC) scheme. We utilize the ring signature to ensure the credibility of multiple attribute authorities, and adopt the online/offline technique for generating the hash of the rewritable transaction. In addition, expensive computations (e.g., pairings) required for rewriting the transactions can be outsourced to the clouds. The users can rest assured that the computations from the clouds are correct with the audit mechanism of the proposed scheme. On the other hand, the proposed scheme offers a desirable feature for commercial application where the clouds can limit the number of outsourced requests according to the subscription of the users. We also prove the security of the proposed OO-RB-AOC scheme. Finally, we present a theoretical comparison and experimental analysis of the proposed scheme.},
  archive      = {J_TCC},
  author       = {Lifeng Guo and Qianli Wang and Wei-Chuen Yau},
  doi          = {10.1109/TCC.2021.3102031},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {499-514},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Online/Offline rewritable blockchain with auditable outsourced computation},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving and secure cloud computing: A case of
large-scale nonlinear programming. <em>TCC</em>, <em>11</em>(1),
484–498. (<a href="https://doi.org/10.1109/TCC.2021.3099720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The volume of data is increasing rapidly, which poses a great challenge for resource-constrained users to process and analyze. A promising approach for solving computation-intensive tasks over big data is to outsource them to the cloud to take advantage of the cloud’s powerful computing capability. However, it also brings privacy and security issues since the data uploaded to the cloud may contain sensitive and private information which should be protected. In this article, we address this problem and focus on the privacy-preserving and secure outsourcing of large-scale nonlinear programming problems (NLPs) subject to both linear constraints and nonlinear constraints. Large-scale NLPs play an important role in the field of data analytics but have not received enough attention in the context of cloud computing. In our outsourcing protocol, we first apply a secure and efficient transformation scheme at the client side to encrypt the private information of the considered NLP. Then, we use the reduced gradient method and generalized gradient method at the server side to solve the transformed large-scale NLPs under linear constraints and nonlinear constraints, respectively. We provide security analysis of the proposed protocol, and evaluate its performance via a series of experiments. The experimental results show that our protocol can efficiently solve large-scale NLPs and save much time for the client, providing a great potential for real applications.},
  archive      = {J_TCC},
  author       = {Wei Du and Ang Li and Qinghua Li and Pan Zhou},
  doi          = {10.1109/TCC.2021.3099720},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {484-498},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Privacy-preserving and secure cloud computing: A case of large-scale nonlinear programming},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FLCD: A flexible low complexity design of coded distributed
computing. <em>TCC</em>, <em>11</em>(1), 470–483. (<a
href="https://doi.org/10.1109/TCC.2021.3098593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a flexible low complexity design (FLCD) of coded distributed computing (CDC) with empirical evaluation on Amazon Elastic Compute Cloud (Amazon EC2). CDC can expedite MapReduce like computation by trading increased map computations to reduce communication load and shuffle time. A main novelty of FLCD is to utilize the design freedom in defining map and reduce functions to develop asymptotic homogeneous systems to support varying intermediate values (IV) sizes under a general MapReduce framework. Compared to existing designs with constant IV sizes, FLCD offers greater flexibility in adapting to network parameters and significantly reduces the implementation complexity by requiring fewer input files and shuffle groups. The FLCD scheme is the first proposed low-complexity CDC design that can operate on a network with an arbitrary number of nodes and computation load. We perform empirical evaluations of the FLCD by executing the TeraSort algorithm on an Amazon EC2 cluster. This is the first time that theoretical predictions of the CDC shuffle time are validated by empirical evaluations. The evaluations demonstrate a 2.0 to 4.24× speedup compared to conventional uncoded MapReduce, a 12 to 52 percent reduction in total time, and a wider range of operating network parameters compared to existing CDC schemes.},
  archive      = {J_TCC},
  author       = {Nicholas Woolsey and Xingyue Wang and Rong-Rong Chen and Mingyue Ji},
  doi          = {10.1109/TCC.2021.3098593},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {470-483},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {FLCD: A flexible low complexity design of coded distributed computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel privacy-preserving location-based services search
scheme in outsourced cloud. <em>TCC</em>, <em>11</em>(1), 457–469. (<a
href="https://doi.org/10.1109/TCC.2021.3098420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of wireless communications and the pervasiveness of location-aware mobile electronic devices, location-based services (LBS) which can provide a convenient lifestyle for people have attracted considerable interest recently. However, there still exists the privacy disclosure problem of LBS today. To solve this problem, in this article, we present a novel privacy-preserving LBS search scheme in outsourced cloud. In the proposed LBS search scheme, the LBS providers data are first outsourced to the cloud server in an encrypted method. Then, a registered user constructs a query model to obtain accurate LBS query results without divulging his/her location information and query attribute to the LBS provider and the cloud server. Specifically, based on the designed matrix encryption technology, the LBS search scheme can achieve privacy preservation of users query and confidentiality of LBS data in the outsourced cloud server. Through security analysis, we show that our scheme can resist various known security threats. The experimental results further show that our LBS search scheme greatly reduces the communication overhead and provides convenient search experience to the users.},
  archive      = {J_TCC},
  author       = {Dong Li and Jiahui Wu and Junqing Le and Xiaofeng Liao and Tao Xiang},
  doi          = {10.1109/TCC.2021.3098420},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {457-469},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A novel privacy-preserving location-based services search scheme in outsourced cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latency-aware strategies for deploying data stream
processing applications on large cloud-edge infrastructure.
<em>TCC</em>, <em>11</em>(1), 445–456. (<a
href="https://doi.org/10.1109/TCC.2021.3097879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) applications often require the processing of data streams generated by devices dispersed over a large geographical area. Traditionally, these data streams are forwarded to a distant cloud for processing, thus resulting in high application end-to-end latency. Recent work explores the combination of resources located in clouds and at the edges of the Internet, called cloud-edge infrastructure, for deploying Data Stream Processing (DSP) applications. Most previous work, however, fails to scale to very large IoT settings. This paper introduces deployment strategies for the placement of Data Stream Processing (DSP) applications onto cloud-edge infrastructure. The strategies split an application graph into regions and consider regions with stringent time requirements for edge placement. The proposed Aggregate End-to-End Latency Strategy with Region Patterns and Latency Awareness (AELS+RP+LA) decreases the number of evaluated resources when computing an operator&#39;s placement by considering the communication overhead across computing resources. Simulation results show that, unlike the state-of-the-art, Aggregate End-to-End Latency Strategy with Region Patterns and Latency Awareness (AELS+RP+LA) scales to environments with more than 100k resources with negligible impact on the application end-to-end latency.},
  archive      = {J_TCC},
  author       = {Alexandre da Silva Veith and Marcos Dias de Assunção and Laurent Lefèvre},
  doi          = {10.1109/TCC.2021.3097879},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {445-456},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Latency-aware strategies for deploying data stream processing applications on large cloud-edge infrastructure},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantifying satisfaction of security requirements of cloud
software systems. <em>TCC</em>, <em>11</em>(1), 426–444. (<a
href="https://doi.org/10.1109/TCC.2021.3097770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The satisfaction of a software requirement is commonly stated as a Boolean value, that is, a security requirement is either satisfied (true) or not (false). However, a discrete Boolean value to measure the satisfaction level of a security requirement by deployed mechanisms is not very useful. Rather, it would be more effective if we could quantify the level of satisfaction of security requirements on a continuous scale. We propose an approach to achieve this for cloud software systems based on relationships between defense strength, exploitability of vulnerabilities, and attack severity. We extend the concept of entailment relationship from the field of requirements engineering with the satisfiability aspects of security requirements. The proposed approach enables us to systematically structure security concepts into three sets of related descriptions to quantify the satisfaction level of security requirements with the deployed security solutions. To demonstrate the feasibility of the proposed approach, we evaluate the approach in a case study. As a result, security administrators are able to deploy more effective and appropriate security solutions based on their assessment.},
  archive      = {J_TCC},
  author       = {Armstrong Nhlabatsi and Khaled MD Khan and Jin B. Hong and Dong Seong Kim and Rachael Fernandez and Noora Fetais},
  doi          = {10.1109/TCC.2021.3097770},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {426-444},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Quantifying satisfaction of security requirements of cloud software systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentially private combinatorial cloud auction.
<em>TCC</em>, <em>11</em>(1), 412–425. (<a
href="https://doi.org/10.1109/TCC.2021.3097328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud service providers typically provide different types of virtual machines (VMs) to cloud users with various requirements. Thanks to its effectiveness and fairness, auction has been widely applied in this heterogeneous resource allocation. Recently, several strategy-proof combinatorial cloud auction mechanisms have been proposed. However, they fail to protect the bid privacy of users from being inferred from the auction results. In this article, we design a differentially private combinatorial cloud auction mechanism (DPCA) to address this privacy issue. Technically, we employ the exponential mechanism to compute a clearing unit price vector with a probability proportional to the corresponding revenue. We further improve the mechanism to reduce the running time while maintaining high revenues, by computing a single clearing unit price, or a subgroup of clearing unit prices at a time, resulting in the improved mechanisms DPCA-S and its generalized version DPCA-M, respectively. We theoretically prove that our mechanisms can guarantee differential privacy, approximate truthfulness and high revenue. Extensive experimental results demonstrate that DPCA can generate near-optimal revenues at the price of relatively high time complexity, while the improved mechanisms achieve a tunable trade-off between auction revenue and running time.},
  archive      = {J_TCC},
  author       = {Tianjiao Ni and Zhili Chen and Lin Chen and Shun Zhang and Yan Xu and Hong Zhong},
  doi          = {10.1109/TCC.2021.3097328},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {412-425},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Differentially private combinatorial cloud auction},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Popularity-based data placement with load balancing in edge
computing. <em>TCC</em>, <em>11</em>(1), 397–411. (<a
href="https://doi.org/10.1109/TCC.2021.3096467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, edge computing has become an increasingly popular computing paradigm to enable real-time data processing and mobile intelligence. Edge computing allows computing at the edge of the network, where data is generated and distributed at the nearby edge servers to reduce the data access latency and improve data processing efficiency. One of the key challenges in data-intensive edge computing is how to place the data at the edge clouds effectively such that the access latency to the data is minimized. In this paper, we study such a data placement problem in edge computing while different data items have diverse popularity. We propose a popularity based placement method which maps both data items and edge servers to a virtual plane and places or retrieves data based on its virtual coordinate in the plane. We then further propose additional placement strategies to handle load balancing among edge servers via either offloading or data duplication. Simulation results show that our proposed strategies efficiently reduce the average path length of data access and the load-balancing strategies indeed provide an effective relief of storage pressures at certain overloaded servers.},
  archive      = {J_TCC},
  author       = {Xinliang Wei and Yu Wang},
  doi          = {10.1109/TCC.2021.3096467},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {397-411},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Popularity-based data placement with load balancing in edge computing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Searchable public-key encryption with cryptographic reverse
firewalls for cloud storage. <em>TCC</em>, <em>11</em>(1), 383–396. (<a
href="https://doi.org/10.1109/TCC.2021.3095498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to protect data privacy in cloud storage, sensitive data is encrypted before being uploaded to a cloud server. How to retrieve ciphertext safely and effectively has become a problem. Public key encryption with keyword search (PEKS) realizes the retrieval of ciphertexts in clouds without disclosing secret information. However, most PEKS protocols can not resist an keyword guessing attack (KGA) launched by untrusted cloud servers. Meanwhile, these protocols are unable to detect vulnerabilities, resulting in information leakage. In this article, we design a searchable public-key encryption with cryptographic reverse firewalls (SPKE-CRF), and use the JPBC library to implement the protocol. Security analysis shows that the SPKE-CRF protocol can resist a chosen keyword attack (CKA), a KGA, and an algorithm substitution attack (ASA) without secure channels. Performance analysis shows that the SPKE-CRF protocol has a significant communication and computational cost advantage while being resistant to the KGA and ASA from malicious insider attackers in cloud environments. Therefore, our SPKE-CRF protocol is secure and efficient for cloud storage.},
  archive      = {J_TCC},
  author       = {Yuyang Zhou and Zhebin Hu and Fagen Li},
  doi          = {10.1109/TCC.2021.3095498},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {383-396},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Searchable public-key encryption with cryptographic reverse firewalls for cloud storage},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic computation and network chaining in integrated
SDN/NFV cloud infrastructure. <em>TCC</em>, <em>11</em>(1), 367–382. (<a
href="https://doi.org/10.1109/TCC.2021.3094681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational resources are increasingly virtualized to enable computational tasks to be offloaded to remote facilities along the route between the source and destination. The principle that underlies traditional routing, i.e., that only networking resources need to be considered, may no longer be true in a virtualized environment. In this paper, we propose a framework for the efficient utilization of multi-resource infrastructures in which computational resources can be used via the network. Such a framework intrinsically calls for the joint consideration of networking and computational resources. In particular, we focus on unifying the controls in dynamic service chaining and multiple resource management, which are the key technologies in an integrated SDN/NFV architecture. We formulate a multi-path problem for choosing the resources to use in different services. The problem can be viewed as variational inequality using the Lagrange duality and saddle point theory. Based on this, we develop an extragradient-based algorithm that controls and splits the sending rate of each service. We prove that the algorithm converges to the optimal, minimizing the system cost while maximizing service utility. Simulations for diverse scenarios demonstrate that our algorithm achieves high QoS while reducing the system cost by jointly considering dual-resource coupling and service characteristics.},
  archive      = {J_TCC},
  author       = {Yeongjin Kim and Jeongho Kwak and Hyang-Won Lee and Song Chong},
  doi          = {10.1109/TCC.2021.3094681},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {367-382},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic computation and network chaining in integrated SDN/NFV cloud infrastructure},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing OAuth with blockchain technologies for data
portability. <em>TCC</em>, <em>11</em>(1), 349–366. (<a
href="https://doi.org/10.1109/TCC.2021.3094846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To satisfy the requirement of data portability, current service providers (or resource servers) usually provide OAuth-based schemes for third party applications (or clients) to access user data with the user&#39;s consent. To shoulder the costs of maintaining relationships with potential third party applications, a service provider may adopt delegate the task of authentication and authorization to an authorization server. However, current OAuth specification does not specify the interactions between an authorization server and a resource server. To address this limitation, this study proposes the MyDataChain framework to enhance the existing OAuth specification with blockchain technology. The proposed framework utilizes smart contracts to establish the standard interface to support the processes of authorization requesting, granting, and revocation. As blockchain technologies can ensure data integrity, the framework can use the data stored in the blockchain to resolve disputes among different parities. Moreover, as the proposed framework uses the Non-Interactive Zero-Knowledge (NIZK) scheme, the proposed framework can achieve its purpose without storing any personal identifiable or traceable data in the blockchain. Therefore, people cannot utilize information stored in the blockchain to compromise user privacy. Furthermore, this study implements a prototype system using Quorum blockchain technology. The experimental results show that the framework can be realized with existing blockchain technologies. Therefore, this study can provide a feasible privacy preserving means of achieving data portability and providing individuals the rights to be forgotten considering dispute resolution.},
  archive      = {J_TCC},
  author       = {Shi-Cho Cha and Chun-Lin Chang and Yang Xiang and Tzu-Jia Huang and Kuo-Hui Yeh},
  doi          = {10.1109/TCC.2021.3094846},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {349-366},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enhancing OAuth with blockchain technologies for data portability},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Verifiable and dynamic multi-keyword search over encrypted
cloud data using bitmap. <em>TCC</em>, <em>11</em>(1), 336–348. (<a
href="https://doi.org/10.1109/TCC.2021.3093304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable Symmetric Encryption (SSE), which enables users to search over encrypted data without decryption, has gained increasing attention from both academic and industrial fields. However, existing SSE schemes either have low search efficiency or cannot support multi-keyword search, dynamic updates, and result verification simultaneously. To solve these problems, we propose a Verifiable and Dynamic Multi-keyword Search (VDMS) scheme over encrypted data by using the bitmap and RSA accumulator, which provides multi-keyword search over encrypted data in an efficient, verifiable and updated way. The bitmap is used as a data structure to build the indexes, which improves the search efficiency and reduces the storage space of the indexes. The RSA accumulator and bitmap are combined to verify the correctness of results. Formal security analysis proves that our VDMS is adaptively secure against Chosen-Keyword Attacks (CKA), and empirical experiments using a real-world dataset demonstrate that our VDMS is efficient and feasible in practical applications.},
  archive      = {J_TCC},
  author       = {Feng Li and Jianfeng Ma and Yinbin Miao and Qi Jiang and Ximeng Liu and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TCC.2021.3093304},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {336-348},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Verifiable and dynamic multi-keyword search over encrypted cloud data using bitmap},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal algorithm allocation for single robot cloud systems.
<em>TCC</em>, <em>11</em>(1), 324–335. (<a
href="https://doi.org/10.1109/TCC.2021.3093489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a robot to perform a task, several algorithms must be executed, sometimes simultaneously. The algorithms can be executed either on the robot itself or, if desired, on a cloud infrastructure. The term cloud infrastructure refers to hardware, storage, abstracted resources, and network resources associated with cloud computing. Depending on the decision of where the algorithms are executed, the overall execution time and memory required for the robot, change accordingly. The price of a robot depends on its storage capacity and computational power, among other factors. We answer the question of how to maintain a given performance and deploy a cheaper robot (lower resources) by allocating computational tasks to the cloud infrastructure depending on memory, computational power, and communication constraints. Even for a fixed robot, our model provides a way to achieve optimal overall performance. We provide a general model for optimal algorithm allocation decision under certain constraints. We illustrate the model with simulation results. The main advantage of our model is that it provides optimal task allocation simultaneously for memory and time.},
  archive      = {J_TCC},
  author       = {Saeid Alirezazadeh and Luís A. Alexandre},
  doi          = {10.1109/TCC.2021.3093489},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {324-335},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimal algorithm allocation for single robot cloud systems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable fuzzy keyword ranked search over encrypted data on
hybrid clouds. <em>TCC</em>, <em>11</em>(1), 308–323. (<a
href="https://doi.org/10.1109/TCC.2021.3092358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable encryption (SE) is a powerful technology that enables keyword-based search over encrypted data becomes possible. However, most SE schemes focus on exact keyword search which can not tolerate misspellings and typos. Existing fuzzy keyword search schemes only support fuzzy search within a limited similarity threshold $d$ , the storage cost will grow exponentially or the precision of search results will greatly decrease as $d$ increases. Moreover, the current fuzzy keyword ranked search schemes consider only the keyword weight, and disregard the influence of keyword morphology similarity on the ranking. In this article, we propose a scalable fuzzy keyword ranked search scheme over encrypted data under hybrid clouds architecture. We use the edit distance to measure the similarity of keywords and design an edit distance algorithm over encrypted data, in which our scheme achieves fuzzy keyword search for any similarity threshold $d$ with a constant storage size and accurate search results. Furthermore, we design a two-factor ranking function combining keyword weight with keyword morphology similarity, which is utilized to rank the search results and enhance system usability. Extensive experiments are performed to demonstrate the trade-off of efficiency and security of the proposed scheme.},
  archive      = {J_TCC},
  author       = {Hua Zhang and Shaohua Zhao and Ziqing Guo and Qiaoyan Wen and Wenmin Li and Fei Gao},
  doi          = {10.1109/TCC.2021.3092358},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {308-323},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Scalable fuzzy keyword ranked search over encrypted data on hybrid clouds},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MAGNet: Machine learning guided application-aware networking
for data centers. <em>TCC</em>, <em>11</em>(1), 291–307. (<a
href="https://doi.org/10.1109/TCC.2021.3087447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data centers are witnessing fast-growing east-west traffic on their network infrastructure due to the highly distributed data center applications. Motivated by the heterogeneity of such application workloads, we propose in this article an extensible network management architecture called MAGNet which enables application-aware intra-data center networking. The crux of MAGNet is the smart endpoint residing within end-hosts, which is empowered by machine learning combined with lightweight workload tracing to detect workload identities and enable workload-dependent packet tagging. The centralized management plane interface of MAGNet allows network functions to interpret packet tags and perform application-aware packet processing. We demonstrate the feasibility of the architecture via prototype implementation and extensive use case evaluation. Our experiments show that the smart endpoint can fingerprint many real-world applications with 99 percent accuracy only at 1–2 percent additional CPU, and that application-aware data plane can potentially bring substantial benefits in terms of security (e.g., via identity-based microsegmentation), CPU usage (e.g., for intrusion detection) and network latency (e.g., via TCP stack customization).},
  archive      = {J_TCC},
  author       = {Hyunseok Chang and Murali Kodialam and T. V. Lakshman and Sarit Mukherjee and Jacobus Van der Merwe and Zirak Zaheer},
  doi          = {10.1109/TCC.2021.3087447},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {291-307},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MAGNet: Machine learning guided application-aware networking for data centers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel statistical and neural network combined approach for
the cloud spot market. <em>TCC</em>, <em>11</em>(1), 278–290. (<a
href="https://doi.org/10.1109/TCC.2021.3091936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The price of virtual machine instances in the Amazon EC2 spot model is often much lower than in the on-demand counterpart. However, this price reduction comes with a decrease in the availability guarantees. Several mechanisms have been proposed to analyze the spot model in the last years, employing different strategies. To our knowledge, there is no work that accurately captures the trade-off between spot price and availability, for short term analysis, and does long term analysis for spot price tendencies, in favor of user decision making. In this work, we propose (a) a utility-based strategy, that balances cost and availability of spot instances and is targeted to short-term analysis, and (b) a LSTM (Long Short Term Memory) neural network framework for long term spot price tendency analysis. Our experiments show that, for r4.2xlarge, 90 percent of spot bid suggestions ensured at least 5.73 hours of availability in the second quarter of 2020, with a bid price of approximately 38 percent of the on-demand price. The LSTM experiments were able to predict spot prices tendencies for several instance types with very low error. Our LSTM framework predicted an average value of 0.19 USD/hour for the r5.2xlarge instance type (Mean Squared Error $&amp;lt;10^{-6}$ ) for a 7-day period of time, which is about 37 percent of the on-demand price. Finally, we used our combined mechanism on an application that compares thousands of SARS-CoV-2 DNA sequences and show that our approach is able to provide good choices of instances, with low bids and very good availability.},
  archive      = {J_TCC},
  author       = {Gustavo J. Portella and Eduardo Nakano and Genaina N. Rodrigues and Azzedine Boukerche and Alba C. M. A. Melo},
  doi          = {10.1109/TCC.2021.3091936},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {278-290},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A novel statistical and neural network combined approach for the cloud spot market},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive visualization of large turbulent flow as a cloud
service. <em>TCC</em>, <em>11</em>(1), 263–277. (<a
href="https://doi.org/10.1109/TCC.2021.3091387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scientific communities today have community datasets that are continuously created, curated, and maintained for community use. Such datasets are often hosted and shared through cloud-based data repositories. In this work, we propose a lightweight and affordable visualization cloud service that can be deployed as a companion service of a community dataset. Our target visualization use case is parallel flow visualization, which is crucial for understanding planet-scale phenomena such as the Earth’s atmosphere and ocean. As a core research topic of scientific visualization, parallel flow visualization typically uses HPC computing platforms. It is complex to implement with scalability, deploy with efficiency, and is often considered an advanced form of scientific visualization. Because of the heterogeneous nature of cloud platforms, in this work, we use a swarm-based parallel design to replace traditional HPC designs that assume homogeneity and rely upon conventional methods such as Message Passing Interface (MPI). This design enables interactive visualization of large flow fields in a way that is lightweight, efficient and easily deployable as a cloud service. We demonstrate our proposed system using NOAA’s NCEP ensemble data, which captures turbulent planet-scale atmospheric flows in observed forms, as well as in forecast forms for varying time scales. We evaluate the performance and efficacies of our system on Amazon Web Services (AWS) for three use cases, where remote users can use their laptops to (i) interactively explore global atmospheric flow patterns in general, (ii) to specifically compare how a forecast is different from the observation, and (iii) to explore flow patterns in a typical information visualization dashboard.},
  archive      = {J_TCC},
  author       = {Tanner Hobson and James Hammer and Preston Provins and Jian Huang},
  doi          = {10.1109/TCC.2021.3091387},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {263-277},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Interactive visualization of large turbulent flow as a cloud service},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual traceable distributed attribute-based searchable
encryption and ownership transfer. <em>TCC</em>, <em>11</em>(1),
247–262. (<a href="https://doi.org/10.1109/TCC.2021.3090519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose d ual t raceable d istributed a ttribute b ased e ncryption with s ubset k eyword s earch system (DT-DABE-SKS, abbreviated as $\mathcal {DT}$ ) to simultaneously realize data source trace (secure provenance) and user trace (traitor trace) and flexible subset keyword search from polynomial interpolation. Leveraging non-interactive zero-knowledge proof technology, $\mathcal {DT}$ preserves privacy for both data providers and users in normal circumstances, but a trusted authority can disclose their real identities if necessary, such as the providers deceitfully uploading false data or users maliciously leaking secret attribute key. Next, we introduce the new conception of updatable and transferable message-lock encryption (UT-MLE) for block-level dynamic encrypted file update, where the owner does not have to download the whole ciphertext, decrypt, re-encrypt and upload for minor document modifications. In addition, the owner is permitted to transfer file ownership to other system customers with efficient computation in an authenticated manner. A nontrivial integration of $\mathcal {DT}$ and UT-MLE lead to the distributed ABSE with ownership transfer system ( $\mathcal {DTOT}$ ) to enjoy the above merits. We formally define $\mathcal {DT}$ , UT-MLE, and their security model. Then, the instantiations of $\mathcal {DT}$ and UT-MLE, and the formal security proof are presented. Comprehensive comparison and experimental analysis based on real dataset affirm their feasibility.},
  archive      = {J_TCC},
  author       = {Yang Yang and Robert H. Deng and Wenzhong Guo and Hongju Cheng and Xiangyang Luo and Xianghan Zheng and Chunming Rong},
  doi          = {10.1109/TCC.2021.3090519},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {247-262},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dual traceable distributed attribute-based searchable encryption and ownership transfer},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate and efficient monitoring for virtualized SDN in
clouds. <em>TCC</em>, <em>11</em>(1), 229–246. (<a
href="https://doi.org/10.1109/TCC.2021.3089225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents V-Sight, a network monitoring framework for programmable virtual networks in clouds. Network virtualization based on software-defined networking (SDN-NV) in clouds makes it possible to realize programmable virtual networks; consequently, this technology offers many benefits to cloud services for tenants. However, to the best of our knowledge, network monitoring, which is a prerequisite for managing and optimizing virtual networks, has not been investigated in the context of SDN-NV systems. As the first framework for network monitoring in SDN-NV, we identify three challenges: non-isolated and inaccurate statistics, high monitoring delay, and excessive control channel consumption for gathering statistics. To address these challenges, V-Sight introduces three key mechanisms: 1) statistics virtualization for isolated statistics, 2) transmission disaggregation for reduced transmission delay, and 3) pCollector aggregation for efficient control channel consumption. The evaluation results reveal that V-Sight successfully provides accurate and isolated statistics while reducing the monitoring delay and control channel consumption in orders of magnitude. We also show that V-Sight can achieve a data plane throughput close to that of non-virtualized SDN.},
  archive      = {J_TCC},
  author       = {Gyeongsik Yang and Yeonho Yoo and Minkoo Kang and Heesang Jin and Chuck Yoo},
  doi          = {10.1109/TCC.2021.3089225},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {229-246},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Accurate and efficient monitoring for virtualized SDN in clouds},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward secure data computation and outsource for multi-user
cloud-based IoT. <em>TCC</em>, <em>11</em>(1), 217–228. (<a
href="https://doi.org/10.1109/TCC.2021.3087614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has promoted the success of Internet of Things (IoT) with offering abundant storage and computation resources where the data from IoT sensors can be remotely outsourced to the cloud servers, whereas storing, exchanging and processing data collected through IoT sensors via centralised or decentralised cloud servers make cloud-based IoT systems prone to internal or external attacks. To protect IoT data against potential malicious users and adversaries, some cryptographic schemes have been applied to ensure confidentiality and integrity of IoT data. It is however a challenging task to perform any arithmetical computations once data items are encrypted. Fully-homomorphic encryption which is based on lattices can, in principle, provide a solution, but it is unfortunately inefficient in computation and hence cannot be applied to IoT. Fully-homomorphic encryption is feasible when we allow the involvement of a semi-trusted server. However, it is challenging to provide such a system in the situation of distributed environments for shared IoT data. We solve this problem and provide a fully-homomorphic encryption scheme for cloud-based IoT applications. We introduce a new method with the aid of a semi-trusted server that can help compute the homomorphic multiplications without gaining any useful information of the encrypted data. We show how our scheme is applied to multi-user IoT security and prove its semantic security. We also conduct experiments to justify its efficiency and applicability to multi-user cloud-based IoT systems.},
  archive      = {J_TCC},
  author       = {Fatemeh Rezaeibagha and Yi Mu and Ke Huang and Lanxiang Chen and Leyou Zhang},
  doi          = {10.1109/TCC.2021.3087614},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {217-228},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Toward secure data computation and outsource for multi-user cloud-based IoT},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A knowledge-based adaptive discrete water wave optimization
for solving cloud workflow scheduling. <em>TCC</em>, <em>11</em>(1),
200–216. (<a href="https://doi.org/10.1109/TCC.2021.3087642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workflow scheduling in cloud environments has become a significant topic in both commercial and industrial applications. However, it is still an extraordinarily challenge to generate effective and economical scheduling schemes under the deadline constraint especially for the large scale workflow applications. To address the issue, this article investigates the cloud workflow scheduling problem with the aim of minimizing the whole cost of workflow execution whereas maintaining its execution time under a predetermined deadline. A novel knowledge-based adaptive discrete water wave optimization (KADWWO) algorithm is developed based on the problem-specific knowledge of cloud workflow scheduling. In the proposed KADWWO, a discrete propagation operator is designed based on the idle time knowledge of hourly-based cost model to adaptively explore the huge search space. The adaptive refraction operator is employed to avoid stagnation and expand the available resource pool. Meanwhile, the dynamic grouping based breaking operator is designed to exploit the excellent block structure knowledge of task allocation scheme and corresponding resource to intensify the local region and accelerate convergence. Extensive simulation experiments on the well-known scientific workflow demonstrate that the KADWWO approach outperforms several recent state-of-the-art algorithms.},
  archive      = {J_TCC},
  author       = {Shuo Qin and Dechang Pi and Zhongshi Shao and Yue Xu},
  doi          = {10.1109/TCC.2021.3087642},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {200-216},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A knowledge-based adaptive discrete water wave optimization for solving cloud workflow scheduling},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Column generation based service function chaining embedding
in multi-domain networks. <em>TCC</em>, <em>11</em>(1), 185–199. (<a
href="https://doi.org/10.1109/TCC.2021.3084999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization (NFV) achieves cost-effective network service provisioning through exploitation of virtualization and automation by decoupling network functions (software) from dedicated hardware. The software of the various devices can then be hosted by low-cost general computation devices rather than by more expensive dedicated devices. To obtain a specific network service, the traffic flow is steered to go through a specific order of network functions that are hosted by cloud computing, and this network function sequence is known as a service function chaining (SFC). To allocate computation resources for network functions and bandwidth resources between network functions in a physical network is the SFC embedding problem. In this article, we consider the SFC embedding problem in multi-domain networks, where no domain information, like domain topology and network resource, is disclosed among domains. We propose a new optimization algorithm based on column generation method to solve this problem, which is distributedly computed in each domain. To further improve the scalability, we also provide two heuristic algorithms. We selected two networks one large (158 nodes) and one small (18 nodes) to numerically validate the proposed algorithms and demonstrate that the acceptance ratio obtained by the heuristic algorithms is close (within 5.6 percent) to that of the optimal algorithm.},
  archive      = {J_TCC},
  author       = {Rongping Lin and Song Yu and Shan Luo and Xiaoning Zhang and Jingyu Wang and Moshe Zukerman},
  doi          = {10.1109/TCC.2021.3084999},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {185-199},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Column generation based service function chaining embedding in multi-domain networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute-based pseudonymity for privacy-preserving
authentication in cloud services. <em>TCC</em>, <em>11</em>(1), 168–184.
(<a href="https://doi.org/10.1109/TCC.2021.3084538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-based authentication is considered a cornerstone component to achieve scalable fine-grained access control in the fast growing market of cloud-based services. Unfortunately, it also poses a privacy concern. User’s attributes should not be linked to the users’ identity and spread across different organizations. To tackle this issue, several solutions have been proposed such as Privacy Attribute-Based Credentials (Privacy-ABCs), which support pseudonym-based authentication with embedded attributes. Privacy-ABCs allow users to establish anonymous accounts with service providers while hiding the identity of the user under a pseudonym. However, Privacy-ABCs require the selective disclosure of the attribute values towards service providers. Other schemes such as Attribute-Based Signatures (ABS) and mesh signatures do not require the disclosure of attributes; unfortunately, these schemes do not cater for pseudonym generation in their construction, and hence cannot be used to establish anonymous accounts. In this article, we propose a pseudonym-based signature scheme that enables unlinkable pseudonym self-generation with embedded attributes, similarly to Privacy-ABCs, and integrates a secret sharing scheme in a similar fashion to ABS and mesh signature schemes for attribute verification. Our proposed scheme also provides verifiable delegation, enabling users to share attributes according to the service providers’ policies.},
  archive      = {J_TCC},
  author       = {Victor Sucasas and Georgios Mantas and Maria Papaioannou and Jonathan Rodriguez},
  doi          = {10.1109/TCC.2021.3084538},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {168-184},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Attribute-based pseudonymity for privacy-preserving authentication in cloud services},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secure and efficient data deduplication in JointCloud
storage. <em>TCC</em>, <em>11</em>(1), 156–167. (<a
href="https://doi.org/10.1109/TCC.2021.3081702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication can efficiently eliminate data redundancies in cloud storage and reduce the bandwidth requirement of users. However, most previous schemes depending on the help of a trusted key server (KS) are vulnerable and limited because they suffer from revealing information, poor resistance to attacks, great computational overhead, etc. In particular, if the trusted KS fails, the whole system stops working, i.e., single-point-of-failure. In this article, we propose a S ecure and E fficient data D eduplication scheme (named SED) in a JointCloud storage system which provides the global services via collaboration with various clouds. SED also supports dynamic data update and sharing without the help of the trusted KS. Moreover, SED can overcome the single-point-of-failure that commonly occurs in the classic cloud storage system. According to the theoretical analyses, our SED ensures the semantic security in the random oracle model and has strong anti-attack ability such as the brute-force attack resistance and the collusion attack resistance. Besides, SED can effectively eliminate data redundancies with low computational complexity and communication and storage overhead. The efficiency and functionality of SED improves the usability in client-side. Finally, the comparing results show that the performance of our scheme is superior to that of the existing schemes},
  archive      = {J_TCC},
  author       = {Di Zhang and Junqing Le and Nankun Mu and Jiahui Wu and Xiaofeng Liao},
  doi          = {10.1109/TCC.2021.3081702},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {156-167},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure and efficient data deduplication in JointCloud storage},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic deployment and scheduling strategy for dual-service
pooling-based hierarchical cloud service system in intelligent
buildings. <em>TCC</em>, <em>11</em>(1), 139–155. (<a
href="https://doi.org/10.1109/TCC.2021.3078795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the excessive concentration of computing resources in the traditional centralized cloud service system, there will be three prominent problems of management confusion, construction cost and network delay. Therefore, we propose to virtualize regional edge computing resources in intelligent buildings as edge service pooling, then presents a hierarchical cloud platform with dual-service pooling structure and a dynamic strategy for the proposed model. The analytic hierarchy process (AHP) based quality of service (QoS) evaluation mechanism and the dynamic normal distribution selection method are adopted for service deployment. And the dynamic inertia particle swarm optimization (DI-PSO) algorithm is employed to realize task scheduling. Furthermore, the cloud platform and existing terminal server group are used to conduct platform structure comparison experiments, and the popular task scheduling algorithms are selected for simulation experiments. Experimental results of platform measurement show that the average service response time of different services can be improved by about 17.3 to 37.4 percent. The average occupancy ratio of computing resources can be reduced by about 5 percent. The simulation results show that the earliest completion time of single task list can be decreased by 11.3 to 20.9 percent, and the makespan of 100 task lists can be improved by 0.3 times.},
  archive      = {J_TCC},
  author       = {Hongchang Sun and Shengjun Wang and Fengyu Zhou and Lei Yin and Meizhen Liu},
  doi          = {10.1109/TCC.2021.3078795},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {139-155},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic deployment and scheduling strategy for dual-service pooling-based hierarchical cloud service system in intelligent buildings},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Resource management and pricing for cloud computing based
mobile blockchain with pooling. <em>TCC</em>, <em>11</em>(1), 128–138.
(<a href="https://doi.org/10.1109/TCC.2021.3081580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a public blockchain system applying Proof of Work (PoW), the participants need to compete with their computing resources for reward, which is challenging for resource-limited devices. Mobile blockchain is proposed to facilitate the application of blockchain for mobile service, in which the lightweight devices can participate mining by renting resources from the Cloud Computing Service Provider (CCSP), but CCSP usually does not have the information about the demand preference of users. In this article, a contract model is adopted to address the cloud computing resource allocation and pricing problem in the mobile blockchain. In particular, an adverse selection contract solution is proposed to overcome the information asymmetry problem, and resource pooling is introduced to improve the stability of users’ rewards. Simulation results show that the information asymmetry problem is well overcome by adverse selection contract so that CCSP can obtain more utility than linear pricing contracts. Furthermore, the resource pooling could effectively improve the users’ and CCSP&#39;s utilities. When the size of the mining pool is large enough, it can achieve an improvement effect of more than 10 times. The effect of pool size and user type distribution on CCSP&#39;s utility is also studied.},
  archive      = {J_TCC},
  author       = {Junfei Wang and Jing Li and Zhen Gao and Zhu Han and Chao Qiu and Xiaofei Wang},
  doi          = {10.1109/TCC.2021.3081580},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {128-138},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Resource management and pricing for cloud computing based mobile blockchain with pooling},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling simultaneous content regulation and privacy
protection for cloud storage image. <em>TCC</em>, <em>11</em>(1),
111–127. (<a href="https://doi.org/10.1109/TCC.2021.3081564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The population of cloud computing greatly facilitates the sharing of explosively generated image today. While benefiting from the convenient of cloud, the privacy protection mechanism that commonly applied in cloud service makes the spreading of illegal and harmful data very hard to be detected or controlled. Such a realistic threat should be seriously treated, yet is largely overlooked in the literature. To address this issue, we propose the first cloud service framework that can simultaneously provide privacy protection and content regulation for the cloud storage image. In specific, we design a secure multi-party computation (MPC) protocol to protect the data privacy via random projection. By leveraging the distance preserving properties residing in random projection, we propose a privacy-preserving principal component analysis (PCA)-based recognition approach over the random projection domain to achieve content matching while respecting the data privacy. To facilitate the efficiency, we implement our system under the compressive sensing (CS) framework. Due to the compression effect of CS, the proposed cloud service can achieve remarkable reduction on the computation and communication complexity of the content matching process. Theoretical analysis and experimental results both show that our system can achieve privacy assurance and acceptable recognition performance, while with high efficiency.},
  archive      = {J_TCC},
  author       = {Guiqiang Hu and Hongwei Li and Guowen Xu and Xinqiang Ma},
  doi          = {10.1109/TCC.2021.3081564},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {111-127},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enabling simultaneous content regulation and privacy protection for cloud storage image},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online performance modeling and prediction for single-VM
applications in multi-tenant clouds. <em>TCC</em>, <em>11</em>(1),
97–110. (<a href="https://doi.org/10.1109/TCC.2021.3078690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clouds have been adopted widely by many organizations for their supports of flexible resource demands and low cost, which is normally achieved through sharing the underlying hardware among multiple cloud tenants. However, such sharing with the changes in resource contentions in virtual machines (VMs) can result in large variations for the performance of cloud applications, which makes it difficult for ordinary cloud users to estimate the run-time performance of their applications. In this article, we propose online learning methodologies for performance modeling and prediction of applications that run repetitively on multi-tenant clouds (such as on-line data analytic tasks). Here, a few micro-benchmarks are utilized to probe the in-situ perceivable performance of CPU, memory and I/O components of the target VM. Then, based on such profiling information and in-place measured application’s performance, the predictive models can be derived with either Regression or Neural-Network techniques. In particular, to address the changes in the intensity of resource contentions of a VM over time and its effects on the target application, we proposed periodic model retraining where the sliding-window technique was exploited to control the frequency and historical data used for model retraining. Moreover, a progressive modeling approach has been devised where the Regression and Neural-Network models are gradually updated for better adaptation to recent changes in resource contention. With 17 representative applications from PARSEC, NAS Parallel and CloudSuite benchmarks being considered, we have extensively evaluated the proposed online schemes for the prediction accuracy of the resulting models and associated overheads on both a private and public clouds. The evaluation results show that, even on the private cloud with high and radically changed resource contention, the average prediction errors of the considered models can be less than 20 percent with periodic retraining. The prediction errors generally decrease with higher retraining frequencies and more historical data points but incurring higher run-time overheads. Furthermore, with the neural-network progressive models, the average prediction errors can be reduced by about 7 percent with much reduced run-time overheads (up to 265 X ) on the private cloud. For public clouds with less resource contentions, the average prediction errors can be less than 4 percent for the considered models with our proposed online schemes.},
  archive      = {J_TCC},
  author       = {Hamidreza Moradi and Wei Wang and Dakai Zhu},
  doi          = {10.1109/TCC.2021.3078690},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {97-110},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Online performance modeling and prediction for single-VM applications in multi-tenant clouds},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and implementation of smooth renewable power in cloud
data centers. <em>TCC</em>, <em>11</em>(1), 85–96. (<a
href="https://doi.org/10.1109/TCC.2021.3076978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The renewable power has been widely used in modern cloud data centers, which also produce large electricity bills and the negative impacts on environments. However, frequent fluctuation and intermittency of renewable power often cause the challenges in terms of the stability of both electricity grid and data centers, as well as decreasing the utilization of renewable power. Existing schemes fail to alleviate the renewable power fluctuation, which is caused by the essential properties of renewable power. In order to address this problem, we propose an efficient and easy-to-use smooth renewable power-aware scheme, called Smoother, which consists of Flexible Smoothing ( FS ) and Active Delay ( AD ). First, in order to smooth the fluctuation of renewable power, FS carries out the optimized charge/discharge operation via computing the minimum variance of the renewable power that is supplied to data centers per interval. Second, AD improves the utilization of renewable power via actively adjusting the execution time of deferrable workloads. Extensive experimental results via examining the traces of real-world data centers demonstrate that Smoother significantly reduces the negative impact of renewable power fluctuations on data centers and improves the utilization of renewable power by 250.88 percent on average. We have released the source codes for public use.},
  archive      = {J_TCC},
  author       = {Xinxin Liu and Yu Hua and Xue Liu and Ling Yang and Yuanyuan Sun},
  doi          = {10.1109/TCC.2021.3076978},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {85-96},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Design and implementation of smooth renewable power in cloud data centers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel data placement and retrieval service for cooperative
edge clouds. <em>TCC</em>, <em>11</em>(1), 71–84. (<a
href="https://doi.org/10.1109/TCC.2021.3076229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing is a new paradigm in which the computing and storage resources are placed at the edge of the Internet. Data placement and retrieval are fundamental services of mobile edge computing when a network of edge clouds collaboratively provide data services. These services require short-latency and low-overhead implementation in network and computing devices and load balance on edge clouds. However existing methods such as distributed hash tables (DHTs) are not enough to achieve efficient data placement and retrieval services for cooperative edge clouds. This article presents GRED, a novel data placement and retrieval service for mobile edge computing, which is efficient in not only the load balance but also routing path lengths and forwarding table sizes. GRED utilizes the programmable switches to support a virtual-space based DHT with only one overlay hop. Data location can be easily implemented on top of the GRED by associating a virtual position with each data by hashing, and storing the data at the edge server connected to the switch whose position is the nearest to the position of the data in the virtual space. We implement GRED in a P4 prototype, which provides a simple and efficient solution. Results from theoretical analysis, simulations, and experiments show that GRED can efficiently balance the load of edge clouds, and can fast answer data queries due to its low routing stretch.},
  archive      = {J_TCC},
  author       = {Junjie Xie and Chen Qian and Deke Guo and Xin Li and Ge Wang and Honghui Chen},
  doi          = {10.1109/TCC.2021.3076229},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {71-84},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A novel data placement and retrieval service for cooperative edge clouds},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MockFog 2.0: Automated execution of fog application
experiments in the cloud. <em>TCC</em>, <em>11</em>(1), 58–70. (<a
href="https://doi.org/10.1109/TCC.2021.3074988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing is an emerging computing paradigm that uses processing and storage capabilities located at the edge, in the cloud, and possibly in between. Testing and benchmarking fog applications, however, is hard since runtime infrastructure will typically be in use or may not exist, yet. While approaches for the emulation of infrastructure testbeds do exist, their focus is typically the emulation of edge devices. Other approaches also emulate infrastructure within the core network or the cloud, but they miss support for automated experiment orchestration. In this article, we propose to evaluate fog applications on an emulated infrastructure testbed created in the cloud which can be manipulated based on a pre-defined orchestration schedule. Developers can freely design the infrastructure, configure performance characteristics, manage application components, and orchestrate their experiments. We also present our proof-of-concept implementation MockFog 2.0. We use MockFog 2.0 to evaluate a fog-based smart factory application and showcase how its features can be used to study the impact of infrastructure changes and workload variations. With these experiments, we also show that MockFog can achieve good experiment reproducibility, even in a public cloud environment.},
  archive      = {J_TCC},
  author       = {Jonathan Hasenburg and Martin Grambow and David Bermbach},
  doi          = {10.1109/TCC.2021.3074988},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {58-70},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MockFog 2.0: Automated execution of fog application experiments in the cloud},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computation migration oriented resource allocation in mobile
social clouds. <em>TCC</em>, <em>11</em>(1), 44–57. (<a
href="https://doi.org/10.1109/TCC.2021.3074159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of mobile device (e.g., smart phone and bracelet) has spawned a lot of new applications, during which the requirements of applications are increasing, while the capacities of some mobile devices are still limited. Such contradiction drives the emergency of computation migration among mobile edge devices, which is a lack of research currently. In this article, we focus on addressing the computation migration oriented resource allocation problem among mobile edge devices. Specifically, we first construct a framework for Mobile Social Cloud(MSC), in which the mobile devices with rich resources are abstracted as resource suppliers and those resource-lacking devices are abstracted as resource demanders. Then, a mathematical model is formulated and an evolutionary algorithm is proposed to effectively solve this model based on decomposition, dominance, and genetic operations. Moreover, the parallel computing is introduced to further improve the efficiency of the proposed algorithm. The experimental results indicate that the proposed algorithm outperforms the other state-of-the-art methods and it improves the calculation efficiency by about 178 percent (2 cores) and 262 percent (3 cores) by introducing parallel computing.},
  archive      = {J_TCC},
  author       = {Bo Yi and Xingwei Wang and Min Huang and Qiang He and Fuliang Li},
  doi          = {10.1109/TCC.2021.3074159},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {44-57},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Computation migration oriented resource allocation in mobile social clouds},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Securing organization’s data: A role-based authorized
keyword search scheme with efficient decryption. <em>TCC</em>,
<em>11</em>(1), 25–43. (<a
href="https://doi.org/10.1109/TCC.2021.3071304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For better data availability and accessibility while ensuring data secrecy, organizations often tend to outsource their encrypted data to the cloud storage servers, thus bringing the challenge of keyword search over encrypted data. In this article, we propose a novel authorized keyword search scheme using Role-Based Encryption (RBE) technique in a cloud environment. The contributions of this article are multi-fold. First, it presents a keyword search scheme which enables only authorized users, having properly assigned roles, to delegate keyword-based data search capabilities over encrypted data to the cloud providers without disclosing any sensitive information. Second, it supports a multi-organization cloud environment, where the users can be associated with more than one organization. Third, the proposed scheme provides efficient decryption, conjunctive keyword search and revocation mechanisms. Fourth, the proposed scheme outsources expensive cryptographic operations in decryption to the cloud in a secure manner. Fifth, we have provided a formal security analysis to prove that the proposed scheme is semantically secure against Chosen Plaintext and Chosen Keyword Attacks. Finally, our performance analysis shows that the proposed scheme is suitable for practical applications.},
  archive      = {J_TCC},
  author       = {Nazatul Haque Sultan and Maryline Laurent and Vijay Varadharajan},
  doi          = {10.1109/TCC.2021.3071304},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {25-43},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Securing organization’s data: A role-based authorized keyword search scheme with efficient decryption},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Edge-assisted short video sharing with guaranteed
quality-of-experience. <em>TCC</em>, <em>11</em>(1), 13–24. (<a
href="https://doi.org/10.1109/TCC.2021.3067834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a rising star of social apps, short video apps, e.g., TikTok, have attracted a large number of mobile users by providing fresh and short video contents that highly match their watching preferences. Meanwhile, the booming growth of short video apps imposes new technical challenges on the existing computation and communication infrastructure. Traditional solutions maintain all videos on the cloud and stream them to users via contend delivery networks or the Internet. However, they incur huge network traffic and long delay that seriously affects users’ watching experiences. In this article, we propose an edge-assisted short video sharing framework to address these challenges by caching some highly preferred videos at edge servers that can be accessed by users via high-speed network connections. Since edge servers have limited computation and storage resources, we design an online algorithm with provable approximation ratio to decide which videos should be cached at edge servers, without the knowledge of future network quality and watching preferences changes. Furthermore, we improve the performance by jointly considering video fetching and user-edge association. Extensive simulations are conducted to evaluate the proposed algorithms under various system settings, and the results show that our proposals outperform existing schemes.},
  archive      = {J_TCC},
  author       = {Fahao Chen and Peng Li and Deze Zeng and Song Guo},
  doi          = {10.1109/TCC.2021.3067834},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {13-24},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Edge-assisted short video sharing with guaranteed quality-of-experience},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blue-pill oxpecker: A VMI platform for transactional
modification. <em>TCC</em>, <em>11</em>(1), 1–12. (<a
href="https://doi.org/10.1109/TCC.2021.3067829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although multiple techniques have been proposed with the goal of minimizing the semantic gap in virtual machine introspection, most concentrate on passive observation of the internal state, while there are also a number of proposals with which active modification of the VM&#39;s internal state is made possible. However there are issues when modifications are applied, such as keeping a consistent kernel state and avoiding a crash. In this article we propose Oxpecker, a VMI platform for transactional modification. The out-of-VM read access allows an introspector to detect malware in the guest OS (e.g., rootkit) and the transactional write access allows Oxpecker to reliably neutralize the detected threats. To begin a transaction, Oxpecker monitors VM state changes waiting for an idle moment which is free of possible race-conditions in the guest kernel memory. Thereafter, it invokes a VMI client&#39;s callback to proceed with reading/writing in its memory. Upon user request or possible exceptions, transaction is rolled back while the transaction ACID properties are maintained at all times. Oxpecker is implemented and evaluated under different real-world workloads. Additionally and as a practical example, a tool is developed, and open sourced, based on Oxpecker with which guest VM processes could be killed.},
  archive      = {J_TCC},
  author       = {Seyed Mohammad AghamirMohammadAli and Behnam Momeni and Solmaz Salimi and Mehdi Kharrazi},
  doi          = {10.1109/TCC.2021.3067829},
  journal      = {IEEE Transactions on Cloud Computing},
  number       = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Blue-pill oxpecker: A VMI platform for transactional modification},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
