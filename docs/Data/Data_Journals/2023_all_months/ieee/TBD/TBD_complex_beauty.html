<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TBD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tbd---123">TBD - 123</h2>
<ul>
<li><details>
<summary>
(2023). EBoF: Interactive temporal correlation analysis for ensemble
data based on bag-of-features. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(6), 1726–1737. (<a
href="https://doi.org/10.1109/TBDATA.2023.3324482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose eBoF, a novel time-varying ensemble data visualization approach based on the Bag-of-Features (BoF) model. In the eBoF model, we extract a simple and monotone interval from all target variables of ensemble scalar data as a local feature patch. Each local feature of a semantically simple single interval can be defined as a feature patch within the BoF model, with the duration of each interval (i.e., feature patch) serving as its frequency. Feature clusters in ensemble runs are then identified based on the similarity of temporal correlations. eBoF generates clusters along with their probability distributions across all feature patches while preserving the geo-spatial information, which is often lost in traditional topic modeling or clustering algorithms. The probability distribution across different clusters can help to generate reasonable clustering results, evaluated by domain knowledge. We conduct case studies and performance tests to evaluate the eBoF model and gather feedback from domain experts to further refine it. Evaluation results suggest the proposed eBoF can provide insightful and comprehensive evidence on ensemble simulation data analysis.},
  archive  = {J},
  author   = {Zhifei Ding and Jiahao Han and Rongtao Qian and Liming Shen and Siru Chen and Lingxin Yu and Yu Zhu and Richen Liu},
  doi      = {10.1109/TBDATA.2023.3324482},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1726-1737},
  title    = {EBoF: Interactive temporal correlation analysis for ensemble data based on bag-of-features},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial-temporal contrasting for fine-grained urban flow
inference. <em>IEEE Transactions on Big Data</em>, <em>9</em>(6),
1711–1725. (<a
href="https://doi.org/10.1109/TBDATA.2023.3316471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fine-grained urban flow inference (FUFI) problem aims to infer the fine-grained flow maps from coarse-grained ones, benefiting various smart-city applications by reducing electricity, maintenance, and operation costs. Existing models use techniques from image super-resolution and achieve good performance in FUFI. However, they often rely on supervised learning with a large amount of training data, and often lack generalization capability and face overfitting. We present a new solution: S patial- T emporal C ontrasting for Fine-Grained Urban F low Inference (STCF). It consists of (i) two pre-training networks for spatial-temporal contrasting between flow maps; and (ii) one coupled fine-tuning network for fusing learned features. By attracting spatial-temporally similar flow maps while distancing dissimilar ones within the representation space, STCF enhances efficiency and performance. Comprehensive experiments on two large-scale, real-world urban flow datasets reveal that STCF reduces inference error by up to 13.5%, requiring significantly fewer data and model parameters than prior arts.},
  archive  = {J},
  author   = {Xovee Xu and Zhiyuan Wang and Qiang Gao and Ting Zhong and Bei Hui and Fan Zhou and Goce Trajcevski},
  doi      = {10.1109/TBDATA.2023.3316471},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1711-1725},
  title    = {Spatial-temporal contrasting for fine-grained urban flow inference},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling homogeneous GNNs to handle heterogeneous graphs via
relation embedding. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(6), 1697–1710. (<a
href="https://doi.org/10.1109/TBDATA.2023.3313031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph Neural Networks (GNNs) have been generalized to process the heterogeneous graphs by various approaches. Unfortunately, these approaches usually model the heterogeneity via various complicated modules. This article aims to propose a simple yet effective framework to assign adequate ability to the homogeneous GNNs to handle the heterogeneous graphs. Specifically, we propose Relation Embedding based Graph Neural Network (RE-GNN), which employs only one parameter per relation to embed the importance of distinct types of relations and node-type-specific self-loop connections. To optimize these relation embeddings and the model parameters simultaneously, a gradient scaling factor is proposed to constrain the embeddings to converge to suitable values. Besides, we interpret the proposed RE-GNN from two perspectives, and theoretically demonstrate that our RE-GCN possesses more expressive power than GTN (which is a typical heterogeneous GNN, and it can generate meta-paths adaptively). Extensive experiments demonstrate that our RE-GNN can effectively and efficiently handle the heterogeneous graphs and can be applied to various homogeneous GNNs.},
  archive  = {J},
  author   = {Junfu Wang and Yuanfang Guo and Liang Yang and Yunhong Wang},
  doi      = {10.1109/TBDATA.2023.3313031},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1697-1710},
  title    = {Enabling homogeneous GNNs to handle heterogeneous graphs via relation embedding},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards long-tailed recognition for graph classification via
collaborative experts. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(6), 1683–1696. (<a
href="https://doi.org/10.1109/TBDATA.2023.3313029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classification framework via Co llaborative M ulti- e xpert Learning (CoMe) to tackle the problem. To equilibrate the contributions of head and tail classes, we first develop balanced contrastive learning from the view of representation learning, and then design an individual-expert classifier training based on hard class mining. In addition, we execute gated fusion and disentangled knowledge distillation among the multiple experts to promote the collaboration in a multi-expert framework. Comprehensive experiments are performed on seven widely-used benchmark datasets to demonstrate the superiority of our method CoMe over state-of-the-art baselines.},
  archive  = {J},
  author   = {Si-Yu Yi and Zhengyang Mao and Wei Ju and Yong-Dao Zhou and Luchen Liu and Xiao Luo and Ming Zhang},
  doi      = {10.1109/TBDATA.2023.3313029},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1683-1696},
  title    = {Towards long-tailed recognition for graph classification via collaborative experts},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Seq2CASE: Weakly supervised sequence to commentary aspect
score estimation for recommendation. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(6), 1670–1682. (<a
href="https://doi.org/10.1109/TBDATA.2023.3313028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Online users’ feedback has numerous text comments to enrich the review quality on mainstream platforms, such as Yelp and Google Maps. Reading through numerous review comments to speculate the important aspects is tedious and time-consuming. Apparently, there is a huge gap between the numerous commentary text and the crucial aspects for users’ preferences. In this study, we proposed a weakly supervised framework called Sequence to Commentary Aspect Score Estimation (Seq2CASE) to estimate the vital aspect scores from the review comments, since the ground truth of the aspect score is seldom available. The aspect score estimation from Seq2CASE is close to the actual aspect scoring; precisely, the average Mean Absolute Error (MAE) is less than 0.4 for a 5-point grading scale. The performance of Seq2CASE is comparable to or even better than the state-of-the-art supervised approaches in recommendation tasks. We expect this work to be a stepping stone that can inspire more unsupervised studies working on this important but relatively underexploited research.},
  archive  = {J},
  author   = {Chien-Tse Cheng and Yu-Hsun Lin and Chung-Shou Liao},
  doi      = {10.1109/TBDATA.2023.3313028},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1670-1682},
  title    = {Seq2CASE: Weakly supervised sequence to commentary aspect score estimation for recommendation},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PredLife: Predicting fine-grained future activity patterns.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(6), 1658–1669. (<a
href="https://doi.org/10.1109/TBDATA.2023.3310241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Activity pattern prediction is a critical part of urban computing, urban planning, intelligent transportation, and so on. Based on a dataset with more than 10 million GPS trajectory records collected by mobile sensors, this research proposed a CNN-BiLSTM-VAE-ATT-based encoder-decoder model for fine-grained individual activity sequence prediction. The model combines the long-term and short-term dependencies crosswise and also considers randomness, diversity, and uncertainty of individual activity patterns. The proposed results show higher accuracy compared to the ten baselines. The model can generate high diversity results while approximating the original activity patterns distribution. Moreover, the model also has interpretability in revealing the time dependency importance of the activity pattern prediction.},
  archive  = {J},
  author   = {Wenjing Li and Xiaodan Shi and Dou Huang and Xudong Shen and Jinyu Chen and Hill Hiroki Kobayashi and Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
  doi      = {10.1109/TBDATA.2023.3310241},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1658-1669},
  title    = {PredLife: Predicting fine-grained future activity patterns},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning with document-level data augmentation for
aspect-level sentiment classification. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(6), 1643–1657. (<a
href="https://doi.org/10.1109/TBDATA.2023.3310267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-level sentiment classification (ASC) seeks to reveal the emotional tendency of a designated aspect of a text. Some researchers have recently tried to exploit large amounts of document-level sentiment classification (DSC) data available to help improve the performance of ASC models through transfer learning. However, these studies often ignore the difference in sentiment distribution between document-level and aspect-level data without preprocessing the document-level knowledge. Our study provides a transfer learning with document-level data augmentation (TL-DDA) framework to transfer more accurate document-level knowledge to the ASC model by means of document-level data augmentation and attention fusion . First, we use document data selection and text concatenation to produce document-level data with various sentiment distributions. The augmented document data is then utilized for pre-training a well-designed DSC model. Finally, after attention adjustment , we fuse the word attention obtained from this DSC model into the ASC model. Results of experiments utilizing two publicly available datasets suggest that TL-DDA is reliable.},
  archive  = {J},
  author   = {Xiaosai Huang and Jing Li and Jia Wu and Jun Chang and Donghua Liu},
  doi      = {10.1109/TBDATA.2023.3310267},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1643-1657},
  title    = {Transfer learning with document-level data augmentation for aspect-level sentiment classification},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved box embeddings for fine-grained entity typing.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(6), 1631–1642. (<a
href="https://doi.org/10.1109/TBDATA.2023.3310239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Different from traditional vector-based fine-grained entity typing methods, the box-based method is more effective in capturing the complex relationships between entity mentions and entity types. The box-based fine-grained entity typing method projects entity types and entity mentions into high-dimensional box space, where entity types and entity mentions are embedded as d -dimensional hyperrectangles. However, the impacts of entity types are not considered during classification in high-dimensional box space, and the model cannot be optimized precisely when two boxes are completely separated or overlapped in high-dimensional box space. Based on the above shortcomings, an I mproved B ox E mbeddings (IBE) method for fine-grained entity typing is proposed in this work. The IBE not only introduces the impacts of entity types during classification in high-dimensional box space, but also proposes a distance based module to optimize the model precisely when two boxes are completely separated or overlapped in high-dimensional box space. Experimental results on four fine-grained entity typing datasets verify the effectiveness of the proposed IBE, demonstrating that IBE is a state-of-the-art method for fine-grained entity typing.},
  archive  = {J},
  author   = {Yixiu Qin and Yizhao Wang and Jiawei Li and Shun Mao and He Wang and Yuncheng Jiang},
  doi      = {10.1109/TBDATA.2023.3310239},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1631-1642},
  title    = {Improved box embeddings for fine-grained entity typing},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cosine multilinear principal component analysis for
recognition. <em>IEEE Transactions on Big Data</em>, <em>9</em>(6),
1620–1630. (<a
href="https://doi.org/10.1109/TBDATA.2023.3301389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Existing two-dimensional principal component analysis methods can only handle second-order tensors (i.e., matrices). However, with the advancement of technology, tensors of order three and higher are gradually increasing. This brings new challenges to dimensionality reduction. Thus, a multilinear method called MPCA was proposed. Although MPCA can be applied to all tensors, using the square of the F-norm makes it very sensitive to outliers. Several two-dimensional methods, such as Angle 2DPCA, have good robustness but cannot be applied to all tensors. We extend the robust Angle 2DPCA method to a multilinear method and propose Cosine Multilinear Principal Component Analysis (CosMPCA) for tensor representation. Our CosMPCA method considers the relationship between the reconstruction error and projection scatter and selects the cosine metric. In addition, our method naturally uses the F-norm to reduce the impact of outliers. We introduce an iterative algorithm to solve CosMPCA. We provide detailed theoretical analysis in both the proposed method and the analysis of the algorithm. Experiments show that our method is robust to outliers and is suitable for tensors of any order.},
  archive  = {J},
  author   = {Feng Han and Chengcai Leng and Bing Li and Anup Basu and Licheng Jiao},
  doi      = {10.1109/TBDATA.2023.3301389},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1620-1630},
  title    = {Cosine multilinear principal component analysis for recognition},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking missing data: Aleatoric uncertainty-aware
recommendation. <em>IEEE Transactions on Big Data</em>, <em>9</em>(6),
1607–1619. (<a
href="https://doi.org/10.1109/TBDATA.2023.3300547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Historical interactions are the default choice for recommender model training, which typically exhibit high sparsity, i.e., most user-item pairs are unobserved missing data. A standard choice is treating the missing data as negative training samples and estimating interaction likelihood between user-item pairs along with the observed interactions. In this way, some potential interactions are inevitably mislabeled during training, which will hurt the model fidelity, hindering the model to recall the mislabeled items, especially the long-tail ones. In this work, we investigate the mislabeling issue from a new perspective of aleatoric uncertainty , which describes the inherent randomness of missing data. The randomness pushes us to go beyond merely the interaction likelihood and embrace aleatoric uncertainty modeling. Towards this end, we propose a new Aleatoric Uncertainty-aware Recommendation (AUR) framework that consists of a new uncertainty estimator along with a normal recommender model. According to the theory of aleatoric uncertainty, we derive a new recommendation objective to learn the estimator. As the chance of mislabeling reflects the potential of a pair, AUR makes recommendations according to the uncertainty, which is demonstrated to improve the recommendation performance of less popular items without sacrificing the overall performance. We instantiate AUR on three representative recommender models: Matrix Factorization (MF), LightGCN, and VAE from mainstream model architectures. Extensive results on four real-world datasets validate the effectiveness of AUR w.r.t. better recommendation results, especially on long-tail items.},
  archive  = {J},
  author   = {Chenxu Wang and Fuli Feng and Yang Zhang and Qifan Wang and Xunhan Hu and Xiangnan He},
  doi      = {10.1109/TBDATA.2023.3300547},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1607-1619},
  title    = {Rethinking missing data: Aleatoric uncertainty-aware recommendation},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive powerball stochastic conjugate gradient for
large-scale learning. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(6), 1598–1606. (<a
href="https://doi.org/10.1109/TBDATA.2023.3300546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The extreme success of stochastic optimization (SO) in large-scale machine learning problems, information retrieval, bioinformatics, etc., has been widely reported, especially in recent years. As an effective tactic, conjugate gradient (CG) has been gaining its popularity in accelerating SO algorithms. This paper develops a novel type of stochastic conjugate gradient descent (SCG) algorithms from the perspective of the Powerball strategy and the hypergradient descent (HD) technique. The crucial idea behind the resulting methods is inspired by pursuing the equilibrium of ordinary differential equations (ODEs). We elucidate the effect of the Powerball strategy in SCG algorithms. The introduction of HD, on the other side, makes the resulting methods work with an online learning rate. Meanwhile, we provide a comprehension of the theoretical results for the resulting algorithms under non-convex assumptions. As a byproduct, we bridge the gap between the learning rate and powered stochastic optimization (PSO) algorithms, which is still an open problem. Resorting to numerical experiments on numerous benchmark datasets, we test the parameter sensitivity of the proposed methods and demonstrate the superior performance of our new algorithms over state-of-the-art algorithms.},
  archive  = {J},
  author   = {Zhuang Yang},
  doi      = {10.1109/TBDATA.2023.3300546},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1598-1606},
  title    = {Adaptive powerball stochastic conjugate gradient for large-scale learning},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A black-box adversarial attack method via nesterov
accelerated gradient and rewiring towards attacking graph neural
networks. <em>IEEE Transactions on Big Data</em>, <em>9</em>(6),
1586–1597. (<a
href="https://doi.org/10.1109/TBDATA.2023.3296936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent studies have shown that Graph Neural Networks (GNNs) are vulnerable to well-designed and imperceptible adversarial attack. Attacks utilizing gradient information are widely used in the field of attack due to their simplicity and efficiency. However, several challenges are faced by gradient-based attacks: 1) Generate perturbations use white-box attacks (i.e., requiring access to the full knowledge of the model), which is not practical in the real world; 2) It is easy to drop into local optima; and 3) The perturbation budget is not limited and might be detected even if the number of modified edges is small. Faced with the above challenges, this article proposes a black-box adversarial attack method, named NAG-R, which consists of two modules known as N esterov A ccelerated G radient attack module and R ewiring optimization module. Specifically, inspired by adversarial attacks on images, the first module generates perturbations by introducing Nesterov Accelerated Gradient (NAG) to avoid falling into local optima. The second module keeps the fundamental properties of the graph (e.g., the total degree of the graph) unchanged through a rewiring operation, thus ensuring that perturbations are imperceptible. Intensive experiments show that our method has significant attack success and transferability over existing state-of-the-art gradient-based attack methods.},
  archive  = {J},
  author   = {Shu Zhao and Wenyu Wang and Ziwei Du and Jie Chen and Zhen Duan},
  doi      = {10.1109/TBDATA.2023.3296936},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1586-1597},
  title    = {A black-box adversarial attack method via nesterov accelerated gradient and rewiring towards attacking graph neural networks},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-discriminator active adversarial network for
multi-center brain disease diagnosis. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(6), 1575–1585. (<a
href="https://doi.org/10.1109/TBDATA.2023.3294000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-center analysis has attracted increasing attention in brain disease diagnosis, because it provides effective approaches to improve disease diagnostic performance by making use of the information from different centers. However, in practical multi-center applications, data uncertainty is more common than that in single center, which brings challenge to robust modeling of diagnosis. In this article, we proposed a multi-discriminator active adversarial network (MDAAN) to alleviate the uncertainties at the center, feature, and label levels for multi-center brain disease diagnosis. First, we extract the latent invariant representation of the source center and target center to reduce domain shift by adversarial learning strategy. Second, the proposed method adaptively evaluates the contribution of different source centers in fusion by measuring data distribution difference between source and target center. Moreover, only the hard learning samples in target center are identified to label with low sample annotation cost. Finally, we treat the selected samples as the auxiliary domain to alleviate the negative transfer and improve the robustness of the multi-center model. We extensively compare the proposed approach with several state-of-the-art multi-center methods on the five-center schizophrenia dataset, and the results demonstrate that our method is superior to the previous methods in identifying brain disease.},
  archive  = {J},
  author   = {Qi Zhu and Qiming Yang and Mingming Wang and Xiangyu Xu and Yuwu Lu and Wei Shao and Daoqiang Zhang},
  doi      = {10.1109/TBDATA.2023.3294000},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1575-1585},
  title    = {Multi-discriminator active adversarial network for multi-center brain disease diagnosis},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personalized interventions to increase the employment
success of people with disability. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(6), 1561–1574. (<a
href="https://doi.org/10.1109/TBDATA.2023.3291547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {An emerging problem in Disability Employment Services (DES) is recommending to people with disability the right skill to upgrade and the right upgrade level to achieve maximum improvement in their employment success. This problem requires causal reasoning to estimate the individual causal effect of possible factors on the outcome to determine the most effective intervention. In this paper, we propose a causal graph based framework to solve the intervention recommendation problem for survival outcome (job retention time) and non-survival outcome (employment status). For an individual, a personalized causal graph is predicted for them. It indicates which factors affect the outcome and their causal effects at different intervention levels. Based on the causal graph, we can determine the most effective intervention for an individual, i.e., the one that can generate a maximum outcome increase. Experiments with two case studies show that our framework can help people with disability increase their employment success. Evaluations with public datasets also show the advantage of our framework in other applications.},
  archive  = {J},
  author   = {Ha Xuan Tran and Thuc Duy Le and Jiuyong Li and Lin Liu and Jixue Liu and Yanchang Zhao and Tony Waters},
  doi      = {10.1109/TBDATA.2023.3291547},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1561-1574},
  title    = {Personalized interventions to increase the employment success of people with disability},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event extraction by associating event types and argument
roles. <em>IEEE Transactions on Big Data</em>, <em>9</em>(6), 1549–1560.
(<a href="https://doi.org/10.1109/TBDATA.2023.3291563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Event extraction (EE), which acquires structural event knowledge from texts, can be divided into two sub-tasks: event type classification and element extraction (namely identifying triggers and arguments under different role patterns). As different event types always own distinct extraction schemas (i.e., role patterns), previous work on EE usually follows an isolated learning paradigm, performing element extraction independently for different event types. It ignores meaningful associations among event types and argument roles, leading to relatively poor performance for less frequent types/roles. This paper proposes a novel neural association framework for the EE task. Given a document, it first performs type classification via constructing a document-level event graph to associate sentence nodes of different types and adopting a document-awared graph attention network to learn sentence embeddings. Then, element extraction is achieved by building a new schema of argument roles, with a type-awared parameter inheritance mechanism to enhance role preference for extracted elements. As such, our model takes into account type and role associations during EE, enabling implicit information sharing among them. Experimental results show that our approach consistently outperforms most state-of-the-art EE methods in both sub-tasks, especially at least 2.51% and 1.12% improvement of the event trigger identification and argument role classification sub-tasks. Particularly, for types/roles with less training data, the performance is superior to the existing methods.},
  archive  = {J},
  author   = {Qian Li and Shu Guo and Jia Wu and Jianxin Li and Jiawei Sheng and Hao Peng and Lihong Wang},
  doi      = {10.1109/TBDATA.2023.3291563},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1549-1560},
  title    = {Event extraction by associating event types and argument roles},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate time-series forecasting model: Predictability
analysis and empirical study. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(6), 1536–1548. (<a
href="https://doi.org/10.1109/TBDATA.2023.3288693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multivariate time series forecasting has wide applications such as traffic flow prediction, supermarket commodity demand forecasting and etc., and a large number of forecasting models have been developed. Given these models, a natural question has been raised: what theoretical limits of forecasting accuracy can these models achieve? Recent works of urban human mobility prediction have made progress on the maximum predictability that any algorithm can achieve. However, existing approaches on maximum predictability on the multivariate time series fully ignore the interrelationship between multiple variables. In this article, we propose a methodology to measure the upper limit of predictability for multivariate time series with multivariate constraint relations. The key of the proposed methodology is a novel entropy, named Multivariate Constraint Sample Entropy ( McSE ), to incorporate the multivariate constraint relations for better predictability. We conduct a systematic evaluation over eight datasets and compare existing methods with our proposed predictability and find that we get a higher predictability. We also find that the forecasting algorithms that capture the multivariate constraint relation information, such as GNN, can achieve higher accuracy, confirming the importance of multivariate constraint relations for predictability.},
  archive  = {J},
  author   = {Qinpei Zhao and Guangda Yang and Kai Zhao and Jiaming Yin and Weixiong Rao and Lei Chen},
  doi      = {10.1109/TBDATA.2023.3288693},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1536-1548},
  title    = {Multivariate time-series forecasting model: Predictability analysis and empirical study},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards privacy-aware causal structure learning in federated
setting. <em>IEEE Transactions on Big Data</em>, <em>9</em>(6),
1525–1535. (<a
href="https://doi.org/10.1109/TBDATA.2023.3285477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Causal structure learning has been extensively studied and widely used in machine learning and various applications. To achieve an ideal performance, existing causal structure learning algorithms often need to centralize a large amount of data from multiple data sources. However, in the privacy-preserving setting, it is impossible to centralize data from all sources and put them together as a single dataset. To preserve data privacy, federated learning as a new learning paradigm has attached much attention in machine learning in recent years. In this paper, we study a privacy-aware causal structure learning problem in the federated setting and propose a novel federated PC (FedPC) algorithm with two new strategies for preserving data privacy without centralizing data. Specifically, we first propose a novel layer-wise aggregation strategy for a seamless adaptation of the PC algorithm into the federated learning paradigm for federated skeleton learning, then we design an effective strategy for learning consistent separation sets for federated edge orientation. The extensive experiments validate that FedPC is effective for causal structure learning in federated learning setting.},
  archive  = {J},
  author   = {Jianli Huang and Xianjie Guo and Kui Yu and Fuyuan Cao and Jiye Liang},
  doi      = {10.1109/TBDATA.2023.3285477},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1525-1535},
  title    = {Towards privacy-aware causal structure learning in federated setting},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards an energy complexity model for distributed data
processing algorithms. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(6), 1510–1524. (<a
href="https://doi.org/10.1109/TBDATA.2023.3284259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Modern data centers exist as infrastructure in the era of Big Data. Big data processing applications are the major computing workload of data centers. Electricity cost accounts for about 50% of data centers’ operational costs. Therefore, the energy consumed for running distributed data processing algorithms on a data center is starting to attract both academia and industry. Most works study the energy consumption from the hardware perspective and only a few of them from the algorithm perspective. A general and hardware-independent energy evaluation model for the algorithms is in demand. With the model, algorithm designers can evaluate the energy consumption, compare energy consumption features and facilitate energy consumption optimization of distributed data processing algorithms. Inspired by the time complexity model, we propose an energy complexity model for describing the trends that an algorithm&#39;s energy consumption grows with the algorithm&#39;s input size. We argue that a good algorithm, especially for processing Big Data, should have a ‘small’ energy complexity. We define &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$E(n)$&lt;/tex-math&gt;&lt;/inline-formula&gt; to represent the functional relationship that associates an algorithm&#39;s input size &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$n$&lt;/tex-math&gt;&lt;/inline-formula&gt; with its notional energy consumption &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$E$&lt;/tex-math&gt;&lt;/inline-formula&gt; . Based on the well-known abstract Bulk Synchronous Parallel (BSP) computer and programming model, we present a complete &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$E(n)$&lt;/tex-math&gt;&lt;/inline-formula&gt; solution, including abstraction, generalization, quantification, derivation, comparison, analysis, examples, verification, and applications. Comprehensive experimental analysis shows that the proposed energy complexity model is practical, interestingly, and not equivalent to time complexity.},
  archive  = {J},
  author   = {Jie Song and Xingchen Zhao and Chaopeng Guo and Yu Gu and Ge Yu},
  doi      = {10.1109/TBDATA.2023.3284259},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1510-1524},
  title    = {Towards an energy complexity model for distributed data processing algorithms},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-agnostic method: Exposing deepfake using pixel-wise
spatial and temporal fingerprints. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(6), 1496–1509. (<a
href="https://doi.org/10.1109/TBDATA.2023.3284272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deepfake poses a serious threat to the reliability of judicial evidence and intellectual property protection. Existing detection methods either blindly utilize deep learning or use biosignal features, but neither considers spatial and temporal relevance of face features. These methods are increasingly unable to resist the growing realism of fake videos and lack generalization. In this paper, we identify a reliable fingerprint through the consistency of AR coefficients and extend the original PPG signal to 3-dimensional fingerprints to effectively detect fake content. Using these reliable fingerprints, we propose a novel model-agnostic method to expose Deepfake by analyzing temporal and spatial faint synthetic signals hidden in portrait videos. Specifically, our method extracts two types of faint information, i.e., PPG features and AR features, which are used as the basis for forensics in temporal and spatial domains, respectively. PPG allows remote estimation of the heart rate in face videos, and irregular heart rate fluctuations expose traces of tampering. AR coefficients reflect pixel-wise correlation and spatial traces of smoothing caused by up-sampling in the process of generating fake faces. Furthermore, we employ two ACBlock-based DenseNets as classifiers. Our method provides state-of-the-art performance on multiple deep forgery datasets and demonstrates better generalization.},
  archive  = {J},
  author   = {Jun Yang and Yaoru Sun and Maoyu Mao and Lizhi Bai and Siyu Zhang and Fang Wang},
  doi      = {10.1109/TBDATA.2023.3284272},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1496-1509},
  title    = {Model-agnostic method: Exposing deepfake using pixel-wise spatial and temporal fingerprints},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of blockchain-based schemes for data sharing and
exchange. <em>IEEE Transactions on Big Data</em>, <em>9</em>(6),
1477–1495. (<a
href="https://doi.org/10.1109/TBDATA.2023.3293279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data immutability, transparency and decentralization of blockchain make it widely used in various fields, such as Internet of things, finance, energy and healthcare. With the advent of the Big Data era, various companies and organizations urgently need data from other parties for data analysis and mining to provide better services. Therefore, data sharing and data exchange have become an enormous industry. Traditional centralized data platforms face many problems, such as privacy leakage, high transaction costs and lack of interoperability. Introducing blockchain into this field can address these problems, while providing decentralized data storage and exchange, access control, identity authentication and copyright protection. Although many impressive blockchain-based schemes for data sharing or data exchange scenarios have been presented in recent years, there is still a lack of review and summary of work in this area. In this paper, we conduct a detailed survey of blockchain-based data sharing and data exchange platforms, discussing the latest technical architectures and research results in this field. In particular, we first survey the current blockchain-based data sharing solutions and provide a detailed analysis of system architecture, access control, interoperability, and security. We then review blockchain-based data exchange systems and data marketplaces, discussing trading process, monetization, copyright protection and other related topics.},
  archive  = {J},
  author   = {Rui Song and Bin Xiao and Yubo Song and Songtao Guo and Yuanyuan Yang},
  doi      = {10.1109/TBDATA.2023.3293279},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1477-1495},
  title    = {A survey of blockchain-based schemes for data sharing and exchange},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of visual affordance recognition based on deep
learning. <em>IEEE Transactions on Big Data</em>, <em>9</em>(6),
1458–1476. (<a
href="https://doi.org/10.1109/TBDATA.2023.3291558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Visual affordance recognition is an important research topic in robotics, human-computer interaction, and other computer vision tasks. In recent years, deep learning-based affordance recognition methods have achieved remarkable performance. However, there is no unified and intensive survey of these methods up to now. Therefore, this article reviews and investigates existing deep learning-based affordance recognition methods from a comprehensive perspective, hoping to pursue greater acceleration in this research domain. Specifically, this article first classifies affordance recognition into five tasks, delves into the methodologies of each task, and explores their rationales and essential relations. Second, several representative affordance recognition datasets are investigated carefully. Third, based on these datasets, this article provides a comprehensive performance comparison and analysis of the current affordance recognition methods, reporting the results of different methods on the same datasets and the results of each method on different datasets. Finally, this article summarizes the progress of affordance recognition, outlines the existing difficulties and provides corresponding solutions, and discusses its future application trends.},
  archive  = {J},
  author   = {Dongpan Chen and Dehui Kong and Jinghua Li and Shaofan Wang and Baocai Yin},
  doi      = {10.1109/TBDATA.2023.3291558},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1458-1476},
  title    = {A survey of visual affordance recognition based on deep learning},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constraint-driven complexity-aware data science workflow for
AutoBDA. <em>IEEE Transactions on Big Data</em>, <em>9</em>(6),
1438–1457. (<a
href="https://doi.org/10.1109/TBDATA.2023.3256043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Internet of Things, privacy, and technical constraints increase the demand for edge-based data-driven services, which is one of the major goals of Industry 4.0 and Society 5.0. Big data analysis (BDA) is the preferred approach to unleash hidden knowledge. However, BDA consumes excessive resources and time. These limitations hamper the meaningful adoption of BDA, especially the time and situation critical edge use cases, and hinder the goals of Industry 4.0 and Society 5.0. Automating the BDA process at the edge is a cognitive approach to address the aforementioned concerns. Data science workflow is an indispensable challenge for successful automation. Therefore, we conducted a systematic literature survey on data science workflow platforms as the first contribution. Moreover, we learned that the BDA workflow depends on diversified constraints and undergoes rigorous data-mining stages. These caused an increase in the solution space, dynamic constraints, complexity issues, and NP-hardness of BDA workflow. Graphplan is a heuristic AI-planning technique that can address concerns associated with BDA workflow. Therefore, as the second contribution, we adopted the graphplan to generate a workflow for edge-based BDA automation. Experiments demonstrate that the proposed method achieved our objectives.},
  archive  = {J},
  author   = {Akila Siriweera and Incheon Paik and Huawei Huang},
  doi      = {10.1109/TBDATA.2023.3256043},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {1438-1457},
  title    = {Constraint-driven complexity-aware data science workflow for AutoBDA},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quality inference in federated learning with secure
aggregation. <em>IEEE Transactions on Big Data</em>, <em>9</em>(5),
1430–1437. (<a
href="https://doi.org/10.1109/TBDATA.2023.3280406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated learning algorithms are developed both for efficiency reasons and to ensure the privacy and confidentiality of personal and business data, respectively. Despite no data being shared explicitly, recent studies showed that the mechanism could still leak sensitive information. Hence, secure aggregation is utilized in many real-world scenarios to prevent attribution to specific participants. In this paper, we focus on the quality (i.e., the ratio of correct labels) of individual training datasets and show that such quality information could be inferred and attributed to specific participants even when secure aggregation is applied. Specifically, through a series of image recognition experiments, we infer the relative quality ordering of participants. Moreover, we apply the inferred quality information to stabilize training performance, measure the individual contribution of participants, and detect misbehavior.},
  archive  = {J},
  author   = {Balázs Pejó and Gergely Biczók},
  doi      = {10.1109/TBDATA.2023.3280406},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1430-1437},
  title    = {Quality inference in federated learning with secure aggregation},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RGSE: Robust graph structure embedding for anomalous link
detection. <em>IEEE Transactions on Big Data</em>, <em>9</em>(5),
1420–1429. (<a
href="https://doi.org/10.1109/TBDATA.2023.3284270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Anomalous links such as noisy links or adversarial edges widely exist in real-world networks, which may undermine the credibility of the network study, e.g., community detection in social networks. Therefore, anomalous links need to be removed from the polluted network by a detector. Due to the co-existence of normal links and anomalous links, how to identify anomalous links in a polluted network is a challenging issue. By designing a robust graph structure embedding framework, also called RGSE, the link-level feature representations that are generated from both global embedding view and local stable view can be used for anomalous link detection on contaminated graphs. Comparison experiments on a variety of datasets demonstrate that the new model and its variants achieve up to an average 5.2% improvement with respect to the accuracy of anomalous link detection against the traditional graph representation models. Further analyses also provide interpretable evidence to support the model&#39;s superiority.},
  archive  = {J},
  author   = {Zhen Liu and Wenbo Zuo and Dongning Zhang and Xiaodong Feng},
  doi      = {10.1109/TBDATA.2023.3284270},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1420-1429},
  title    = {RGSE: Robust graph structure embedding for anomalous link detection},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Outsourced privacy-preserving data alignment on vertically
partitioned database. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(5), 1408–1419. (<a
href="https://doi.org/10.1109/TBDATA.2023.3284271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the context of real-world secure outsourced computations, private data alignment has been always the essential preprocessing step. However, current private data alignment schemes, mainly circuit-based, suffer from high communication overhead and often need to transfer potentially gigabytes of data. In this paper, we propose a lightweight private data alignment protocol (called SC-PSI) that can overcome the bottleneck of communication. Specifically, SC-PSI involves four phases of computations, including data preprocessing, data outsourcing, private set member (PSM) evaluation and circuit computation (CC). Like prior works, the major overhead of SC-PSI mainly lies in the latter two phases. The improvement is SC-PSI utilizes the function secret sharing technique to develop the PSM protocol, which avoids the multiple rounds of communication to compute intersection set members. Moreover, benefited from our specially designed PSM protocol, SC-PSI does not to execute complex secure comparison circuits in the CC phase. Experimentally, we validate that compared to prior works, SC-PSI can save around 61.39% running time and 89.61% communication overhead.},
  archive  = {J},
  author   = {Zhuzhu Wang and Cui Hu and Bin Xiao and Yang Liu and Teng Li and Zhuo Ma and Jianfeng Ma},
  doi      = {10.1109/TBDATA.2023.3284271},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1408-1419},
  title    = {Outsourced privacy-preserving data alignment on vertically partitioned database},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crime prediction with missing data via spatiotemporal
regularized tensor decomposition. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(5), 1392–1407. (<a
href="https://doi.org/10.1109/TBDATA.2023.3283098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The goal of crime prediction is to forecast the number of crime incidents at each region of a city based on the historical crime data. It has attracted a great deal of attention from both academic and industrial communities due to its considerable significance in improving urban safety and reducing financial losses. Although much progress has been made in this field, most of the existing approaches assume that the historical crime data are complete, which does not hold in many real-world scenarios. Meanwhile, crime incidents are affected by multiple factors and have intricate spatial, temporal, and categorical correlations, which are not fully utilized by the current methods. In this article, we propose a novel tensor decomposition based framework, named TD-Crime, to conduct prediction directly on the incomplete crime data. Specifically, we first organize the crime data as a tensor and then apply the nonnegative CP decomposition to it, which not only provides a natural solution to the missing data problem but also captures the spatial, temporal, and categorical correlations implicitly. Moreover, we attempt to exploit the spatial and temporal correlations explicitly by directly learning from the crime data to further improve the forecasting performance. Finally, we obtain a joint optimization problem and present an efficient alternating optimization scheme to find a satisfactory solution. Extensive experiments on the real-world crime datasets show that TD-Crime can address the crime prediction task effectively under different missing data scenarios.},
  archive  = {J},
  author   = {Weichao Liang and Jie Cao and Lei Chen and Youquan Wang and Jia Wu and Amin Beheshti and Jiangnan Tang},
  doi      = {10.1109/TBDATA.2023.3283098},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1392-1407},
  title    = {Crime prediction with missing data via spatiotemporal regularized tensor decomposition},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy and efficiency of communications in federated split
learning. <em>IEEE Transactions on Big Data</em>, <em>9</em>(5),
1380–1391. (<a
href="https://doi.org/10.1109/TBDATA.2023.3280405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Every day, large amounts of sensitive data are distributed across mobile phones, wearable devices, and other sensors. Traditionally, these enormous datasets have been processed on a single system, with complex models being trained to make valuable predictions. Distributed machine learning techniques such as Federated and Split Learning have recently been developed to protect user data and privacy better while ensuring high performance. Both of these distributed learning architectures have advantages and disadvantages. In this article, we examine these tradeoffs and suggest a new hybrid Federated Split Learning architecture that combines the efficiency and privacy benefits of both. Our evaluation demonstrates how our hybrid Federated Split Learning approach can lower the amount of processing power required by each client running a distributed learning system, and reduce training and inference time while keeping a similar accuracy. We also discuss the resiliency of our approach to deep learning privacy inference attacks and compare our solution to other recently proposed benchmarks.},
  archive  = {J},
  author   = {Zongshun Zhang and Andrea Pinto and Valeria Turina and Flavio Esposito and Ibrahim Matta},
  doi      = {10.1109/TBDATA.2023.3280405},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1380-1391},
  title    = {Privacy and efficiency of communications in federated split learning},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A multi-modal hypergraph neural network via parametric
filtering and feature sampling. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(5), 1365–1379. (<a
href="https://doi.org/10.1109/TBDATA.2023.3278988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the real world, relationships between objects are often complex, involving multiple variables and modes. Hypergraph neural networks possess the capability to capture and represent such intricate relationships by deriving and inheriting their graph-based counterparts. Nevertheless, both graph and hypergraph neural networks suffer from the problem of over-smoothing when multiple graph convolution layers are stacked. To address this issue, this article introduces the Multi-modal Hypergraph Neural Network with Parametric Filtering and Feature Sampling (MHNet) to encode complex hypergraph features and mitigate over-smoothing. The proposed approach uses hypergraph structures to model high-order and multi-modal data correlations, a polynomial hypergraph filter to dynamically extract multi-scale node features through parametric polynomial fitting, and a feature sampling strategy to learn from sparse and labeled samples while avoiding overfitting. Experimental results on four hypergraph datasets and two multi-modal visual datasets demonstrate that the proposed MHNet outperforms state-of-the-art algorithms.},
  archive  = {J},
  author   = {Zijian Liu and Yang Luo and Xitong Pu and Geyong Min and Chunbo Luo},
  doi      = {10.1109/TBDATA.2023.3278988},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1365-1379},
  title    = {A multi-modal hypergraph neural network via parametric filtering and feature sampling},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GCN-ST-MDIR: Graph convolutional network-based
spatial-temporal missing air pollution data pattern identification and
recovery. <em>IEEE Transactions on Big Data</em>, <em>9</em>(5),
1347–1364. (<a
href="https://doi.org/10.1109/TBDATA.2023.3277710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Missing data pattern identification and recovery (MDIR) is vital for accurate air pollution monitoring. To recover the missing air pollution data, GCN-ST-MDIR, a Graph Convolutional Network (GCN)-based MDIR framework, is proposed to identify daily missing data patterns and automatically select the best recovery method. GCN-ST-MDIR presents four novelties: (1) A new graph construction is developed to improve GCN data representation for MDIR using S-T similarity matrix and domain-specific knowledge (e.g., weekend/weekday). (2) A TL component is used to pre-train LSCE and ILSCE models. (3) A GCN structure outputs a selection indicator to determine the dominant missing pattern for daily input. The pre-trained data recovery model&#39;s accuracy is incorporated into the GCN loss function to penalize the wrong indicator. (4) The output of the GCN structure is used as a score to combine LSCE and ILSCE. Results show that the domain-specific S-T regularity and irregularity can be used as the prior information for both GCN and ILSCE/LSCE to enhance feature extraction. Our model considerably improves the recovery performance as compared to the baselines. GCN-ST-MDIR has achieved an accuracy of 88.48% for general missing data recovery with consecutively and sporadically missing data. GCN-ST-MDIR can be extended to many other S-T MDIR challenges.},
  archive  = {J},
  author   = {Yangwen Yu and Victor O. K. Li and Jacqueline C. K. Lam and Kelvin Chan},
  doi      = {10.1109/TBDATA.2023.3277710},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1347-1364},
  title    = {GCN-ST-MDIR: Graph convolutional network-based spatial-temporal missing air pollution data pattern identification and recovery},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised federated adaptation for multi-site brain
disease diagnosis. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(5), 1334–1346. (<a
href="https://doi.org/10.1109/TBDATA.2023.3264109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The multi-site approach has attracted increasing attention in brain disease diagnosis, because it can improve the prediction performance by integrating sample information from different medical institutions. However, its training procedure requires the transmission of subject&#39;s original images or features among sites, which may cause privacy disclosure. In this article, we propose a self-supervised federated adaptation (S2FA) framework for robust multi-site prediction, which can reduce the risk of privacy disclosure. As far as we know, it is the first work to investigate the cross-site brain disease diagnosis, which trains model on source sites and tests on target site, often occurring in clinical practice. First, we implement a decentralized federated optimization strategy, by which each site communicates model parameters periodically. Second, we construct an auxiliary self-supervised model for target site through transferring knowledge from source sites with self-paced learning. Then, a hash mapping is proposed to encode the target feature, simultaneously reducing the risk of privacy information disclosure and alleviating data heterogeneity among sites. Finally, we achieve the cross-site prediction by weighted federated source model and auxiliary target model. Experimental results on multi-site datasets show that the proposed S2FA can accurately identify brain disease. Our codes are available at https://github.com/nuaayqm/S2FA .},
  archive  = {J},
  author   = {Qiming Yang and Qi Zhu and Mingming Wang and Wei Shao and Zheng Zhang and Daoqiang Zhang},
  doi      = {10.1109/TBDATA.2023.3264109},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1334-1346},
  title    = {Self-supervised federated adaptation for multi-site brain disease diagnosis},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Cross-region courier displacement for on-demand delivery
with multi-agent reinforcement learning. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(5), 1321–1333. (<a
href="https://doi.org/10.1109/TBDATA.2023.3262408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {On-demand delivery has become prevailing for people to order meals and groceries online, especially during the pandemic. It is essential to dispatch massive orders to limited couriers to satisfy on-demand delivery users, especially during peak hours. Existing studies mainly focus on order dispatching within a region, and they are challenging to be applied to the cross-region courier displacement problem due to (1) unique practical factors, including regional spatial-temporal demand-supply dynamics and strict delivery time constraints, and (2) the large-scale setting and high-dimensional decision space given massive couriers in on-demand delivery. To address these challenges, in this work, we propose an efficient cross-region courier displacement framework, i.e., C ourier D isplacement R einforcement L earning (short for CDRL ) based on centralized multi-agent actor-critic, which first design the actor-critic network with a time-varying displacement intensity control module to capture demand-supply dynamics and utilize the centralized training and decentralized execution multi-agent framework to address the large-scale coordination. One-month real-world order records collected from one of the biggest on-demand delivery services in the world are utilized to show the performance of our design. The extensive results show that our method offers a 47.97% of increase in balancing supply and demand and reduces idle ride time by 24.62% simultaneously.},
  archive  = {J},
  author   = {Shuai Wang and Shijie Hu and Baoshen Guo and Guang Wang},
  doi      = {10.1109/TBDATA.2023.3262408},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1321-1333},
  title    = {Cross-region courier displacement for on-demand delivery with multi-agent reinforcement learning},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel multi-feature fusion facial aesthetic analysis
framework. <em>IEEE Transactions on Big Data</em>, <em>9</em>(5),
1302–1320. (<a
href="https://doi.org/10.1109/TBDATA.2023.3255582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Machine learning has been used in facial beauty prediction studies. However, the integrity of facial geometric information is not considered in facial aesthetic feature extraction, and the impact of other facial attributes (expression) on aesthetics. We propose a novel multi-feature fusion facial aesthetic analysis framework (NMFA) to overcome this problem. First, we designed a facial shape feature, which is an intuitive, visual quantitative description, based on B-spline. Second, we designed a representative low-dimensional facial structural feature to establish the theoretical basis of the facial structure, based on facial aesthetic structure and expression recognition theory. Next, we designed texture and holistic features based on Gabor and VGG-face network. Finally, we used a multi-feature fusion strategy to fuse them for aesthetic evaluation. Experiments were conducted on four databases. The results revealed that the proposed method realizes the visualization of facial shape features, enriches geometric information, solves the problem of lack of facial geometric information and difficulty to understand, and achieves excellent performance with fewer parameters.},
  archive  = {J},
  author   = {Huanyu Chen and Weisheng Li and Xinbo Gao and Bin Xiao},
  doi      = {10.1109/TBDATA.2023.3255582},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1302-1320},
  title    = {Novel multi-feature fusion facial aesthetic analysis framework},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust low transformed multi-rank tensor completion with
deep prior regularization for multi-dimensional image recovery. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(5), 1288–1301. (<a
href="https://doi.org/10.1109/TBDATA.2023.3254156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we study the robust tensor completion problem in three-dimensional image data, where only partial entries are available and the observed tensor is corrupted by Gaussian noise and sparse noise simultaneously. Compared with the existing tensor nuclear norm minimization for the low-rank component, we propose to use the transformed tensor nuclear norm to explore the global low-rankness of the underlying tensor. Moreover, the plug-and-play (PnP) deep prior denoiser is incorporated to preserve the local details of multi-dimensional images. Besides, the tensor &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell _{1}$&lt;/tex-math&gt;&lt;/inline-formula&gt; norm is utilized to characterize the sparseness of the sparse noise. A symmetric Gauss-Seidel based alternating direction method of multipliers is designed to solve the resulting model under the PnP framework with deep prior denoiser. Extensive numerical experiments on hyperspectral and multispectral images, videos, color images, and magnetic resonance image datasets are conducted to demonstrate the superior performance of the proposed model in comparison with several state-of-the-art models.},
  archive  = {J},
  author   = {Yao Li and Duo Qiu and Xiongjun Zhang},
  doi      = {10.1109/TBDATA.2023.3254156},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1288-1301},
  title    = {Robust low transformed multi-rank tensor completion with deep prior regularization for multi-dimensional image recovery},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing the transferability of adversarial examples based
on nesterov momentum for recommendation systems. <em>IEEE Transactions
on Big Data</em>, <em>9</em>(5), 1276–1287. (<a
href="https://doi.org/10.1109/TBDATA.2023.3248626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The attacker&#39;s malicious behavior of injecting well-designed adversarial examples (i.e., fake users) into recommender systems will severely affect the security of systems. It&#39;s difficult to fully obtain details of victim recommendation models (i.e., black-box model) in practical recommendation scenarios, using the transferability of adversarial examples to achieve black-box attacks is still an effective way. At present, adversarial examples generated by existing gradient-based methods are prone to drop into local minima, making it impossible to achieve the expected attack effect and reducing the transferability. In this article, we propose an attack algorithm that enhances the transferability of adversarial examples based on the Nesterov Momentum for Recommendation Systems (ETANRS). With white-box recommendation surrogate models, we utilize Nesterov momentum to generate better adversarial examples, then inject them into black-box victim models to attack. We utilize the accumulated gradients and pre-determine the update direction of the gradients to keep the optimal value from being lost, thus enhancing the transferability of the adversarial examples. Experimental results demonstrate that our method is better than state-of-the-art gradient-based attack algorithms, which affect recommendation performance.},
  archive  = {J},
  author   = {Fulan Qian and Bei Yuan and Hai Chen and Jie Chen and Defu Lian and Shu Zhao},
  doi      = {10.1109/TBDATA.2023.3248626},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1276-1287},
  title    = {Enhancing the transferability of adversarial examples based on nesterov momentum for recommendation systems},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Epidemic spread modeling for COVID-19 using
cross-fertilization of mobility data. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(5), 1260–1275. (<a
href="https://doi.org/10.1109/TBDATA.2023.3248650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present an individual-centric model for COVID-19 spread in an urban setting. We first analyze patient and route data of infected patients from January 20, 2020, to May 31, 2020, collected by the Korean Center for Disease Control &amp; Prevention (KCDC) and discover how infection clusters develop as a function of time. This analysis offers a statistical characterization of mobility habits and patterns of individuals at the beginning of the pandemic. While the KCDC data offer a wealth of information, they are also by their nature limited. To compensate for their limitations, we use detailed mobility data from Berlin, Germany after observing that mobility of individuals is surprisingly similar in both Berlin and Seoul. Using information from the Berlin mobility data, we cross-fertilize the KCDC Seoul data set and use it to parameterize an agent-based simulation that models the spread of the disease in an urban environment. After validating the simulation predictions with ground truth infection spread in Seoul, we study the importance of each input parameter on the prediction accuracy, compare the performance of our model to state-of-the-art approaches, and show how to use the proposed model to evaluate different what-if counter-measure scenarios.},
  archive  = {J},
  author   = {Anna Schmedding and Riccardo Pinciroli and Lishan Yang and Evgenia Smirni},
  doi      = {10.1109/TBDATA.2023.3248650},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {1260-1275},
  title    = {Epidemic spread modeling for COVID-19 using cross-fertilization of mobility data},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asynchronous parallel incremental block-coordinate descent
for decentralized machine learning. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(4), 1252–1259. (<a
href="https://doi.org/10.1109/TBDATA.2022.3230335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Machine learning (ML) is a key technique for big-data-driven modelling and analysis of massive Internet of Things (IoT) based intelligent and ubiquitous computing. For fast-increasing applications and data amounts, distributed learning is a promising emerging paradigm since it is often impractical or inefficient to share/aggregate data to a centralized location from distinct ones. This paper studies the problem of training an ML model over decentralized systems, where data are distributed over many user devices and the learning algorithm run on-device, with the aim of relaxing the burden at a central entity/server. Although gossip-based approaches have been used for this purpose in different use cases, they suffer from high communication costs, especially when the number of devices is large. To mitigate this, incremental-based methods are proposed. We first introduce incremental block-coordinate descent (I-BCD) for the decentralized ML, which can reduce communication costs at the expense of running time. To accelerate the convergence speed, an asynchronous parallel incremental BCD (API-BCD) method is proposed, where multiple devices/agents are active in an asynchronous fashion. We derive convergence properties for the proposed methods. Simulation results also show that our API-BCD method outperforms state of the art in terms of running time and communication costs.},
  archive  = {J},
  author   = {Hao Chen and Yu Ye and Ming Xiao and Mikael Skoglund},
  doi      = {10.1109/TBDATA.2022.3230335},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1252-1259},
  title    = {Asynchronous parallel incremental block-coordinate descent for decentralized machine learning},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A composable generative framework based on prompt learning
for various information extraction tasks. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(4), 1238–1251. (<a
href="https://doi.org/10.1109/TBDATA.2023.3278977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Prompt learning is an effective paradigm that bridges gaps between the pre-training tasks and the corresponding downstream applications. Approaches based on this paradigm have achieved great transcendent results in various applications. However, it still needs to be answered how to design a general-purpose framework based on the prompt learning paradigm for various information extraction tasks. In this article, we propose a novel composable prompt-based generative framework, which could be applied to a wide range of tasks in the field of information extraction. Specifically, we reformulate information extraction tasks into the form of filling slots in pre-designed type-specific prompts, which consist of one or multiple sub-prompts. A strategy of constructing composable prompts is proposed to enhance the generalization ability in data-scarce scenarios. Furthermore, to fit this framework, we transform relation extraction into the task of determining semantic consistency in prompts. The experimental results demonstrate that our approach surpasses compared baselines on real-world datasets in data-abundant and data-scarce scenarios. Further analysis of the proposed framework is presented, as well as numerical experiments conducted to investigate impact factors of performance on various tasks.},
  archive  = {J},
  author   = {Zhigang Kan and Linhui Feng and Zhangyue Yin and Linbo Qiao and Xipeng Qiu and Dongsheng Li},
  doi      = {10.1109/TBDATA.2023.3278977},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1238-1251},
  title    = {A composable generative framework based on prompt learning for various information extraction tasks},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ATLAS: GAN-based differentially private multi-party data
sharing. <em>IEEE Transactions on Big Data</em>, <em>9</em>(4),
1225–1237. (<a
href="https://doi.org/10.1109/TBDATA.2023.3277716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we study the problem of differentially private multi-party data sharing, where the involved parties assisted by a semi-honest curator collectively generate a shared dataset while satisfying differential privacy. Inspired by the success of data synthesis with the generative adversarial network (GAN), we propose a novel GAN-based differentially private multi-party data sharing approach named ATLAS. In ATLAS, we extend the original GAN to multiple discriminators, and let each party hold a discriminator while the curator holds a generator. To update the generator without compromising each party&#39;s privacy, we decompose the calculation of the generator&#39;s gradient and selectively sanitize the discriminators’ responses . Additionally, we propose two methods to improve the utility of shared data, i.e., the collaborative discriminator filtering (CDF) method and the adaptive gradient perturbation (AGP) method. Specifically, the CDF method utilizes trained discriminators to refine synthetic records, while the AGP method adaptively adjusts the noise scale during training to reduce the impact of deferentially private noise on the final shared data. Extensive experiments on real-world datasets validate the superiority of our ATLAS approach.},
  archive  = {J},
  author   = {Zhenya Wang and Xiang Cheng and Sen Su and Jintao Liang and Haocheng Yang},
  doi      = {10.1109/TBDATA.2023.3277716},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1225-1237},
  title    = {ATLAS: GAN-based differentially private multi-party data sharing},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised nodes-hyperedges embedding for heterogeneous
information network learning. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(4), 1210–1224. (<a
href="https://doi.org/10.1109/TBDATA.2023.3275374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The exploration of self-supervised information mining of heterogeneous datasets has gained significant traction in recent years. Heterogeneous graph neural networks (HGNNs) have emerged as a highly promising method for handling heterogeneous information networks (HINs) due to their superior performance. These networks leverage aggregation functions to convert pairwise relations-based features from raw heterogeneous graphs into embedding vectors. However, real-world HINs contain valuable higher-order relations that are often overlooked but can provide complementary information. To address this issue, we propose a novel method called S elf-supervised N odes- H yperedges E mbedding (SNHE), which leverages hypergraph structures to incorporate higher-order information into the embedding process of HINs. Our method decomposes the raw graph structure into snapshots based on various meta-paths, which are then transformed into hypergraphs to aggregate high-order information within the data and generate embedding representations. Given the complexity of HINs, we develop a dual self-supervised structure that maximizes mutual information in the enhanced graph data space, guides the overall model update, and reduces redundancy and noise. We evaluate our proposed method on various real-world datasets for node classification and clustering tasks, and compare it against state-of-the-art methods. The experimental results demonstrate the efficacy of our method. Our code is available at https://github.com/limengran98/SNHE .},
  archive  = {J},
  author   = {Mengran Li and Yong Zhang and Wei Zhang and Yi Chu and Yongli Hu and Baocai Yin},
  doi      = {10.1109/TBDATA.2023.3275374},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1210-1224},
  title    = {Self-supervised nodes-hyperedges embedding for heterogeneous information network learning},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Searching density-increasing path to local density peaks for
unsupervised anomaly detection. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(4), 1198–1209. (<a
href="https://doi.org/10.1109/TBDATA.2023.3265509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Unsupervised anomaly detection (AD) is a challenging problem in the data mining community. Clustering-based AD methods aim to group normal data points into clusters and then regard a point belonging to none of the clusters as an anomaly. However, they may suffer from the problems of unknown cluster numbers and arbitrary cluster shapes. This paper presents a novel clustering-based AD method named Density-increasing Path (DIP) to tackle these challenges. DIP searches a path for each data point. The path starts at the data point itself, passes through several points with monotonically increasing densities, and ends at a density peak. Further, DIP defines the climbing difficulty of each path by combining the distance and density increment of each step along the path, which can be regarded as the anomaly score of the path starting point. DIP can adaptively decide the number of peaks to address the challenge of unknown cluster numbers. Since DIP requires the path to pass several points rather than directly reaching the peak, it handles arbitrary cluster shapes. We also propose the ensemble DIP to improve prediction accuracy. The experimental results on four synthetic datasets and eleven real-world benchmarks demonstrate that DIP outperforms existing methods.},
  archive  = {J},
  author   = {Jiachen Zhao and Fang Deng and Jiaqi Zhu and Jie Chen},
  doi      = {10.1109/TBDATA.2023.3265509},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1198-1209},
  title    = {Searching density-increasing path to local density peaks for unsupervised anomaly detection},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-consistent graph neural networks for semi-supervised
node classification. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(4), 1186–1197. (<a
href="https://doi.org/10.1109/TBDATA.2023.3266590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph Neural Networks (GNNs), the powerful graph representation technique based on deep learning, have attracted great research interest in recent years. Although many GNNs have achieved the state-of-the-art accuracy on a set of standard benchmark datasets, they are still limited to traditional semi-supervised framework and lack of sufficient supervision information, especially for the large amount of unlabeled data. To overcome this issue, we propose a novel self-consistent graph neural networks (SCGNN) framework to enrich the supervision information from two aspects: the self-consistency of unlabeled data and the label information of labeled data. First, in order to extract the &lt;styled-content style=&quot;color:#000000&quot; xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;self-supervision information&lt;/styled-content&gt; from the numerous unlabeled nodes, we perform graph data augmentation and leverage a self-consistent constraint to maximize the mutual information of the unlabeled nodes across different augmented graph views. The self-consistency can sufficiently utilize the intrinsic structural attributes of the graph to extract the &lt;styled-content style=&quot;color:#000000&quot; xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;self-supervision information&lt;/styled-content&gt; from unlabeled data and improve the subsequent classification result. Second, to further extract supervision information from scarce labeled nodes, we introduce a fusion mechanism to obtain comprehensive node embeddings by fusing node representations of two positive graph views, and optimize the classification loss over labeled nodes to maximize the utilization of label information. We conduct comprehensive empirical studies on six public benchmark datasets in node classification task. In terms of accuracy, SCGNN improves by an average of 2.08% over the best baseline, and specifically by 5.8% on the Disease dataset.},
  archive  = {J},
  author   = {Yanbei Liu and Shichuan Zhao and Xiao Wang and Lei Geng and Zhitao Xiao and Jerry Chun-Wei Lin},
  doi      = {10.1109/TBDATA.2023.3266590},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1186-1197},
  title    = {Self-consistent graph neural networks for semi-supervised node classification},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and secure data sharing scheme on interoperable
blockchain database. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(4), 1171–1185. (<a
href="https://doi.org/10.1109/TBDATA.2023.3265178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Interoperable Blockchain Database (IBD) can enable users to execute transactions for sharing data stored in various blockchains maintained by different organizations or communities in a transparent manner. However, compared to traditional distributed databases, IBD can hardly provide high-level security and scalability, which are caused by many factors, such as system architecture, consensus protocol, and interactive pattern. Among them, the consensus protocol is the most critical factor, since the credibility of consensus nodes inside the corresponding blockchains are difficult to be guaranteed. Additionally, the consensus protocol directly affects the verification efficiency for given transactions in IBD. In this paper, we formally concern the problem of secure data sharing in IBD. We present a scheme named Hybridchain to execute transactions for sharing data securely and efficiently. We first propose a novel concept named Interoperable Consensus Group (ICG) which organizes a set of basic consensus nodes into a group, each of which is responsible for managing at least one local blockchain. Then, we present an interoperable cross-chains consensus protocol to achieve eventual consistency of blockchain transactions. We conduct extensive experiments, and the evaluation results show that our proposed approach achieves superior performance.},
  archive  = {J},
  author   = {Kun Hao and Junchang Xin and Zhiqiong Wang and Zhongming Yao and Guoren Wang},
  doi      = {10.1109/TBDATA.2023.3265178},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1171-1185},
  title    = {Efficient and secure data sharing scheme on interoperable blockchain database},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual uncertainty-guided mixing consistency for
semi-supervised 3D medical image segmentation. <em>IEEE Transactions on
Big Data</em>, <em>9</em>(4), 1156–1170. (<a
href="https://doi.org/10.1109/TBDATA.2023.3258643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {3D semi-supervised medical image segmentation is extremely essential in computer-aided diagnosis, which can reduce the time-consuming task of performing annotation. The challenges with current 3D semi-supervised segmentation algorithms includes the methods, limited attention to volume-wise context information, their inability to generate accurate pseudo labels and a failure to capture important details during data augmentation. This article proposes a dual uncertainty-guided mixing consistency network for accurate 3D semi-supervised segmentation, which can solve the above challenges. The proposed network consists of a Contrastive Training Module which improves the quality of augmented images by retaining the invariance of data augmentation between original data and their augmentations. The Dual Uncertainty Strategy calculates dual uncertainty between two different models to select a more confident area for subsequent segmentation. The Mixing Volume Consistency Module that guides the consistency between mixing before and after segmentation for final segmentation, uses dual uncertainty and can fully learn volume-wise context information. Results from evaluative experiments on brain tumor and left atrial segmentation shows that the proposed method outperforms state-of-the-art 3D semi-supervised methods as confirmed by quantitative and qualitative analysis on datasets. This effectively demonstrates that this study has the potential to become a medical tool for accurate segmentation. Code is available at: https://github.com/yang6277/DUMC .},
  archive  = {J},
  author   = {Chenchu Xu and Yuan Yang and Zhiqiang Xia and Boyan Wang and Dong Zhang and Yanping Zhang and Shu Zhao},
  doi      = {10.1109/TBDATA.2023.3258643},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1156-1170},
  title    = {Dual uncertainty-guided mixing consistency for semi-supervised 3D medical image segmentation},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximate clustering ensemble method for big data.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(4), 1142–1155. (<a
href="https://doi.org/10.1109/TBDATA.2023.3255003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Clustering a big distributed dataset of hundred gigabytes or more is a challenging task in distributed computing. A popular method to tackle this problem is to use a random sample of the big dataset to compute an approximate result as an estimation of the true result computed from the entire dataset. In this paper, instead of using a single random sample, we use multiple random samples to compute an ensemble result as the estimation of the true result of the big dataset. We propose a distributed computing framework to compute the ensemble result. In this framework, a big dataset is represented in the RSP data model as random sample data blocks managed in a distributed file system. To compute the ensemble clustering result, a set of RSP data blocks is randomly selected as random samples and clustered independently in parallel on the nodes of a cluster to generate the component clustering results. The component results are transferred to the master node, which computes the ensemble result. Since the random samples are disjoint and traditional consensus functions cannot be used, we propose two new methods to integrate the component clustering results into the final ensemble result. The first method uses component cluster centers to build a graph and the METIS algorithm to cut the graph into subgraphs, from which a set of candidate cluster centers is found. A hierarchical clustering method is then used to generate the final set of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; cluster centers. The second method uses the clustering-by-passing-messages method to generate the final set of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; cluster centers. Finally, the &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -means algorithm was used to allocate the entire dataset into &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; clusters. Experiments were conducted on both synthetic and real-world datasets. The results show that the new ensemble clustering methods performed better than the comparison methods and that the distributed computing framework is efficient and scalable in clustering big datasets.},
  archive  = {J},
  author   = {Mohammad Sultan Mahmud and Joshua Zhexue Huang and Rukhsana Ruby and Alladoumbaye Ngueilbaye and Kaishun Wu},
  doi      = {10.1109/TBDATA.2023.3255003},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1142-1155},
  title    = {Approximate clustering ensemble method for big data},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). UGCC: Social media user geolocation via cyclic coupling.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(4), 1128–1141. (<a
href="https://doi.org/10.1109/TBDATA.2023.3242961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social media user geolocation is to infer users’ resident locations based on social media data, including user texts and social relationships. Existing methods mainly rely on the textual feature propagation in the social graph to fuse users’ textual and social information. The geolocation accuracy is susceptible to insufficient data sources and inadequate fusion. In this paper, a social media user geolocation algorithm based on cyclic coupling (called UGCC) is proposed. We collapse the social graph based on the neighbor location proximity, which reduces noisy information while enriching social relationships. Unlike existing methods that ignore the social graph&#39;s structure, UGCC measures the probability of users being in the candidate locations according to users’ structural location in the social sub-graph. Finally, we design a cyclic coupling mechanism to fuse the users’ textual and social information, which enables the two kinds of information to enhance each other and geolocate users cooperatively. Compared with ten typical existing methods (such as RELP and HGNN), experimental results show UGCC&#39;s superior performance. On two public datasets, the city-level accuracies of UGCC reach 40.8% and 50.1%; the median errors are 35.1% and 23.4% lower than the state-of-the-art methods.},
  archive  = {J},
  author   = {Yimin Liu and Xiangyang Luo and Zhiyuan Tao and Meng Zhang and Shaoyong Du},
  doi      = {10.1109/TBDATA.2023.3242961},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1128-1141},
  title    = {UGCC: Social media user geolocation via cyclic coupling},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LGM-GNN: A local and global aware memory-based graph neural
network for fraud detection. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(4), 1116–1127. (<a
href="https://doi.org/10.1109/TBDATA.2023.3234529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graphs have been widely adopted to accomplish fraud detection tasks because of their inherently favorable structure to capture the intricate features in many complicated scenarios, especially in some modern e-commerce situations that have various relation attributes like transactions. These works tend to utilize the direct aggregate information about neighbor nodes of the target node or the aggregation of the neighbor information after the conditional filter and mostly use local information but ignore global information. However, in some cases, local abnormal points are detected in the global view, so global information is very important for fraud detection. In this article, we propose a local and global aware memory-based graph neural network for fraud detection (LGM-GNN). It first obtains the preliminary node embedding through relation-aware embedding and then interactively aggregates the local and global memory network to fuse and utilize the local and global information. Finally, the node embeddings of different levels are aggregated through the hierarchical information aggregator. Extensive experiments show our proposed LGM-GNN outperforms other SOAT methods on two real-world fraud detection datasets.},
  archive  = {J},
  author   = {Pengbo Li and Hang Yu and Xiangfeng Luo and Jia Wu},
  doi      = {10.1109/TBDATA.2023.3234529},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1116-1127},
  title    = {LGM-GNN: A local and global aware memory-based graph neural network for fraud detection},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep incremental hashing for semantic image retrieval with
concept drift. <em>IEEE Transactions on Big Data</em>, <em>9</em>(4),
1102–1115. (<a
href="https://doi.org/10.1109/TBDATA.2022.3233457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Hashing methods are widely used for content-based image retrieval due to their attractive time and space efficiencies. Several dynamic hashing methods have been proposed for image retrieval tasks in non-stationary environments. However, concept drift problems in non-stationary environment are seldomly considered which lead to significant deterioration of performance. Therefore, we propose Deep Incremental Hashing (DIH). For the learning part, similarity-preserving object codes of each newly arriving data chunk are computed using the product of its label matrix and a random Gaussian matrix generated offline. A point-wise loss function is then devised to guide the learning of a deep hash neural network. To retain the learned knowledge of former chunks, a weighting-based method is utilized to combine different hash tables trained at different time steps to form a multi-table hashing system. Experimental results on 13 simulated concept drift environments show that DIH adapts to non-stationary data environments well and yields better retrieval performance than existing dynamic hashing methods.},
  archive  = {J},
  author   = {Xing Tian and Wing W. Y. Ng and Huihui Xu},
  doi      = {10.1109/TBDATA.2022.3233457},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1102-1115},
  title    = {Deep incremental hashing for semantic image retrieval with concept drift},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-efficient scheduling of streaming applications in
apache flink on cloud. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(4), 1086–1101. (<a
href="https://doi.org/10.1109/TBDATA.2022.3233031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Stream processing has been gaining extensive attention in the past few years. Apache Flink is a new generation of distributed stream processing engines that can process a great deal of data in real-time with low latency. But the default scheduler of Flink adopts a random task scheduling strategy, which does not consider the cost and load balancing in the cloud environment. In this article, a cost-efficient task scheduling algorithm (CETSA) and a cost-efficient load balancing algorithm (LBA-CE) for Flink are proposed to reduce the job execution cost while optimizing load balancing. First, a cost-efficient model and a load balancing model based on Flink are constructed. Then, the core mechanism of Flink task scheduling is improved based on the cost-efficient model and the improved task scheduler is implemented. In addition, the concept of node adaptation is introduced into cost-efficient scheduling according to the load balancing model, ensuring that the cluster load is balanced as much as possible while reducing the cost in a heterogeneous cluster. Extensive experiments have been performed with Hibench&#39;s Wordcount and Fixwindow workloads in the cloud environment. The experimental results indicate that compared to the baseline scheduling algorithm, the proposed algorithms reduce the cost by about 37.9% and 20.2% on average, and the load deviation of the cluster is reduced by about 23.1% and 24.6% on average, respectively. In summary, the proposed algorithms in this paper can significantly reduce the cost of executing jobs and optimize the load balancing of the cluster in Flink.},
  archive  = {J},
  author   = {Hongjian Li and Jianglin Xia and Wei Luo and Hai Fang},
  doi      = {10.1109/TBDATA.2022.3233031},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1086-1101},
  title    = {Cost-efficient scheduling of streaming applications in apache flink on cloud},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regression analysis of predictions and forecasts of cloud
data center KPIs using the boosted decision tree algorithm. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(4), 1071–1085. (<a
href="https://doi.org/10.1109/TBDATA.2022.3230649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cloud data centers seek to optimize their provision of pooled CPU, bandwidth and storage resources. While over-provision is wasteful, under-provision may lead to violating Service Level Agreements (SLAs) with their consumers; yet the relationship between low-level Key Performance Indicators (KPIs) and SLA violations is not well understood. State-of-the art monitoring systems typically react to service failures after the fact, partly due to unexpected nonlinearities in the aggregated performance data. We seek to provide better modelling of KPIs using predictive algorithms that could be used for the proactive monitoring and adaptation of cloud services. In this paper, we investigate the Boosted Decision Tree (BDT) regression algorithm. We tested the BDT algorithm in a real monitoring framework deployed on a novel Azure cloud test-bed distributed over multiple geolocations, using thousands of robot-user requests to produce huge volumes of KPI data. The BDT algorithm achieved an R-Squared score of 0.9991 at the 0.2 learning rate. This closely predicted the KPI data and outperformed other approaches, such as Ordinary Least Squares and Stochastic Gradient Descent; and is a promising candidate for making short- and long-term predictions for cloud resource allocation.},
  archive  = {J},
  author   = {Thomas Weripuo Gyeera and Anthony J.H. Simons and Mike Stannett},
  doi      = {10.1109/TBDATA.2022.3230649},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1071-1085},
  title    = {Regression analysis of predictions and forecasts of cloud data center KPIs using the boosted decision tree algorithm},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial-attention and demographic-augmented generative
adversarial imputation network for population health data
reconstruction. <em>IEEE Transactions on Big Data</em>, <em>9</em>(4),
1057–1070. (<a
href="https://doi.org/10.1109/TBDATA.2022.3227089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a fundamental component of the public health system, population health monitoring plays an important role in health policy-shaping. However, due to the high-cost nature of traditional data collection approaches, many sparse-sampling-completion algorithms are proposed to solve this problem. Existing data-completion methods are usually based on adjacent-spatial correlations, but this correlation isn&#39;t sufficient to ensure accurate inference when prevalence data for its neighboring areas are also missing due to cost constraints. To tackle this problem, we propose a novel deep-learning-based prevalence inference model called Spatial-attention and Demographic-augmented Generative Adversarial Imputation Network (SDA-GAIN). SDA-GAIN can improve accuracy by learning novel “health semantic space similarities” between cross-space areas. The key insight of SDA-GAIN is that we use the Transformer-based model to learn healthy semantic similarities between areas, and use the GAN-based model to make a high-accuracy completion. We further introduce demographic data to augment the model&#39;s ability to learn a better health semantic representation through using CNN. Extensive experiments show that SDA-GAIN outperforms other state-of-the-art approaches at low sampling rates (lower than 30%) which has a significant benefit on saving sampling costs. Also by visualizing the health semantic similarity learned by SDA-GAIN, the results are very similar to the real situation.},
  archive  = {J},
  author   = {Yujie Feng and Jiangtao Wang and Yasha Wang and Xu Chu},
  doi      = {10.1109/TBDATA.2022.3227089},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1057-1070},
  title    = {Spatial-attention and demographic-augmented generative adversarial imputation network for population health data reconstruction},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of data pricing for data marketplaces. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(4), 1038–1056. (<a
href="https://doi.org/10.1109/TBDATA.2023.3254152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A data marketplace is an online venue that brings data owners, data brokers, and data consumers together and facilitates commoditisation of data amongst them. Data pricing, as a key function of a data marketplace, demands quantifying the monetary value of data. A considerable number of studies on data pricing can be found in literature. This article attempts to comprehensively review the state-of-the-art on existing data pricing studies to provide a general understanding of this emerging research area. Our key contribution lies in a new taxonomy of data pricing studies that unifies different attributes determining data prices. The basis of our framework categorises these studies by the kind of market structure, be it sell-side, buy-side, or two-sided. Then in a sell-side market, the studies are further divided by query type, which defines the way a data consumer accesses data, while in a buy-side market, the studies are divided according to privacy notion, which defines the way to quantify privacy of data owners. In a two-sided market, both privacy notion and query type are used as criteria. We systematically examine the studies falling into each category in our taxonomy. Lastly, we discuss gaps within the existing research and define future research directions.},
  archive  = {J},
  author   = {Mengxiao Zhang and Fernando Beltrán and Jiamou Liu},
  doi      = {10.1109/TBDATA.2023.3254152},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1038-1056},
  title    = {A survey of data pricing for data marketplaces},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of mathematical optimization in data
visualization and visual analytics: A survey. <em>IEEE Transactions on
Big Data</em>, <em>9</em>(4), 1018–1037. (<a
href="https://doi.org/10.1109/TBDATA.2023.3262151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mathematical optimization is the process of determining the set of globally or locally optimal parameters in a finite or infinite search space. It has been extensively employed in the research areas of computer science, engineering, operations research, and economics. The application of mathematical optimization has also been extended to data visualization, where it can enhance data processing, structure visualization, and facilitate exploration. However, the current state of summarization in the application of mathematical optimization in data visualization remains inadequate. In this article, we review and classify the existing techniques for advanced mathematical optimization in the fields of data visualization and visual analytics. The classification is conducted based on a classical visualization pipeline, including data enhancement and transformation, representation and rendering, as well as interactive exploration and analysis. We also discuss various mathematical optimization models and their solution methods to help readers gain a better understanding of the relationship among models, visualization, and application scenarios. We additionally provide an online exploration demo, which could enable users to interactively find relevant articles. Based on the limitations and potential trends revealed in the existing literature, we define future challenges in the cross-disciplinary of mathematical optimization and data visualization.},
  archive  = {J},
  author   = {Guodao Sun and Zihao Zhu and Gefei Zhang and Chaoqing Xu and Yunchao Wang and Sujia Zhu and Baofeng Chang and Ronghua Liang},
  doi      = {10.1109/TBDATA.2023.3262151},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1018-1037},
  title    = {Application of mathematical optimization in data visualization and visual analytics: A survey},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-directional feature fixation-based particle swarm
optimization for large-scale feature selection. <em>IEEE Transactions on
Big Data</em>, <em>9</em>(3), 1004–1017. (<a
href="https://doi.org/10.1109/TBDATA.2022.3232761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Feature selection, which aims to improve the classification accuracy and reduce the size of the selected feature subset, is an important but challenging optimization problem in data mining. Particle swarm optimization (PSO) has shown promising performance in tackling feature selection problems, but still faces challenges in dealing with large-scale feature selection in Big Data environment because of the large search space. Hence, this article proposes a bi-directional feature fixation (BDFF) framework for PSO and provides a novel idea to reduce the search space in large-scale feature selection. BDFF uses two opposite search directions to guide particles to adequately search for feature subsets with different sizes. Based on the two different search directions, BDFF can fix the selection states of some features and then focus on the others when updating particles, thus narrowing the large search space. Besides, a self-adaptive strategy is designed to help the swarm concentrate on a more promising direction for search in different stages of evolution and achieve a balance between exploration and exploitation. Experimental results on 12 widely-used public datasets show that BDFF can improve the performance of PSO on large-scale feature selection and obtain smaller feature subsets with higher classification accuracy.},
  archive  = {J},
  author   = {Jia-Quan Yang and Qi-Te Yang and Ke-Jing Du and Chun-Hua Chen and Hua Wang and Sang-Woon Jeon and Jun Zhang and Zhi-Hui Zhan},
  doi      = {10.1109/TBDATA.2022.3232761},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1004-1017},
  title    = {Bi-directional feature fixation-based particle swarm optimization for large-scale feature selection},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metro OD matrix prediction based on multi-view passenger
flow evolution trend modeling. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(3), 991–1003. (<a
href="https://doi.org/10.1109/TBDATA.2022.3229836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Short-term Origin-Destination(OD) matrix prediction in metro systems aims to predict the number of passenger demands from one station to another during a short time period. That is crucial for dynamic traffic operations, e.g., route recommendation, metro scheduling. However, existing methods need further improvement due to that they fail to take full use of the real-time traffic information and model the complex spatiotemporal correlation of traffic flows. In this paper, a M ulti- V iew P assenger F low (MVPF) evolution trend based OD matrix prediction method is proposed. It consists of two components focusing on individual station and cross-station learning. Specifically, the individual station level part uses Gate Recurrent Unit and Extended Graph Attention Networks combined model to learn the high-level spatiotemporal-dependent representation of each station as the roles of origin and destination respectively, by considering multiple views of real-time traffic information (i.e., Inflow, destination allocation of Inflow, Outflow, origin allocations of Outflow). The cross-station part aims to learn passenger mobility pattern from each origin to destination through defining a transition matrix under spatiotemporal context. Compared with state-of-the-art solutions, MVPF increases the OD prediction performance metric of WMAPE by 2.5% on average. The experimental results demonstrate the superiority of MVPF against other competitors. The source code is available at https://github.com/zfrInSIAT/MVPF-code .},
  archive  = {J},
  author   = {Furong Zheng and Juanjuan Zhao and Jiexia Ye and Xitong Gao and Kejiang Ye and Chengzhong Xu},
  doi      = {10.1109/TBDATA.2022.3229836},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {991-1003},
  title    = {Metro OD matrix prediction based on multi-view passenger flow evolution trend modeling},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SIESTA: A scalable infrastructure of sequential pattern
analysis. <em>IEEE Transactions on Big Data</em>, <em>9</em>(3),
975–990. (<a href="https://doi.org/10.1109/TBDATA.2022.3229092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sequential pattern analysis has become a mature topic with a lot of techniques for a variety of sequential pattern mining-related problems. Moreover, tailored solutions for specific domains, such as business process mining, have been developed. However, there is a gap in the literature for advanced techniques for efficient detection of arbitrary sequences in large collections of activity logs. In this work, we introduce the SIESTA ( S calable i nfrastructur e of s equential pa t tern a nalysis) solution making a threefold contribution: (i) we employ a novel architecture that relies on inverted indices during preprocessing and we introduce an advanced query processor that can detect and explore arbitrary patterns efficiently; (ii) we discuss and evaluate different configurations to optimize both the preprocessing and the querying phase; and (iii) we present evaluation results competing against representatives of the state-of-the-art with a focus on Big Data. The experimental results are particularly encouraging, e.g., when all methods are deployed in a cluster and the volume of the data is increased,SIESTA creates the indices in almost half the time compared to the state-of-the-art Elasticsearch-based solution, while also yielding faster query responses than all its competitors by up to 1 order of magnitude.},
  archive  = {J},
  author   = {Ioannis Mavroudopoulos and Anastasios Gounaris},
  doi      = {10.1109/TBDATA.2022.3229092},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {975-990},
  title    = {SIESTA: A scalable infrastructure of sequential pattern analysis},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physical black-box adversarial attacks through
transformations. <em>IEEE Transactions on Big Data</em>, <em>9</em>(3),
964–974. (<a href="https://doi.org/10.1109/TBDATA.2022.3227318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep learning has shown impressive performance in numerous applications. However, recent studies have found that deep learning models are vulnerable to adversarial attacks, where the attacker adds imperceptible perturbations into benign samples to induce misclassifications. Adversarial attacks in the digital domain focus on constructing imperceptible perturbations. However, they are always less effective in the physical world because the perturbations may be destroyed when captured by the camera. Most physical adversarial attacks require adding invisible adversarial features (e.g., a sticker or a laser) to the target object, which may be noticed by human eyes. In this work, we propose to employ image transformation to generate more natural adversarial samples in the physical world. Concretely, we propose two attack algorithms to satisfy different attack goals: Efficient-AATR employs a greedy strategy to generate adversarial samples with fewer queries; Effective-AATR employs an adaptive particle swarm optimization algorithm to search for the most effective adversarial samples within the given the number of queries. Extensive experiments demonstrate the superiority of our attacks compared with state-of-the-art adversarial attacks under mainstream defenses.},
  archive  = {J},
  author   = {Wenbo Jiang and Hongwei Li and Guowen Xu and Tianwei Zhang and Rongxing Lu},
  doi      = {10.1109/TBDATA.2022.3227318},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {964-974},
  title    = {Physical black-box adversarial attacks through transformations},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A data-driven approach to harvesting latent reduced models
to precondition lossy compression for scientific data. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(3), 949–963. (<a
href="https://doi.org/10.1109/TBDATA.2022.3225959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose and evaluate the idea that data need to be preconditioned prior to compression, such that they can better match the design philosophies of lossy compressors for HPC scientific data. In particular, we aim to identify a reduced model that can be utilized to transform the original data into a more compressible form. We begin with two PDE applications as a proof of concept, in which we demonstrate that a reduced model can indeed reside in the full model output, and can be utilized to improve compression ratios. A mathematical proof is also presented to show how the compression ratio is improved by the reduced model. We further explore more general dimension reduction techniques to extract the reduced model, including principal component analysis, singular value decomposition, and discrete wavelet transform. After preconditioning, the reduced model in conjunction with difference between the reduced model and full model is stored, which results in higher compression ratios. We evaluate the reduced models on ten scientific datasets, and the results show the effectiveness of our approaches. Given that there is no single method that consistently achieves the best performance, we further propose a selection strategy that guides users to select the best reduced model prior to data reduction.},
  archive  = {J},
  author   = {Huizhang Luo and Junqi Wang and Zhenlu Qin and Dan Huang and Qing Liu and Mengchu Zhou and Hong Jiang},
  doi      = {10.1109/TBDATA.2022.3225959},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {949-963},
  title    = {A data-driven approach to harvesting latent reduced models to precondition lossy compression for scientific data},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Localization of inpainting forgery with feature enhancement
network. <em>IEEE Transactions on Big Data</em>, <em>9</em>(3), 936–948.
(<a href="https://doi.org/10.1109/TBDATA.2022.3225194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Inpainting the given region of an image is a typical requirement in computer vision. Conventional inpainting, through exemplar-based or diffusion-based strategies, can create realistic inpainted images at a very low cost. Also, such easy-to-use manipulation poses new security threats. Therefore, the detection of inpainting has attracted considerable attention from researchers. However, the existing methods are typically not suitable for the general detection of various inpainting algorithms. Motivated by this, in this work, an efficient feature enhancement network is proposed to locate the inpainted regions in the digital image. First, we design an artifact enhancement block to effectively capture the traces left by diffusion or exemplar-based inpainting. Then, the VGGNet is used as a feature extractor to describe advanced and low-resolution features. Finally, to take full advantage of enhanced features, we concatenate the features obtained by the feature extractor and the up-sampling operations. Extensive experimental evaluations, covering benchmarking, ablation, robustness, generalization, and efficiency studies, confirm the usefulness of the proposed method. This is especially true on the conventional inpainting dataset, our method obtains an average F1 score 7.63% higher than the second-best method. Theoretical and numerical analyses support the effectiveness of our feature enhancement network in representing the artifacts in inpainted images, exhibiting better potential for real-world forensics than various state-of-the-art strategies.},
  archive  = {J},
  author   = {Yushu Zhang and Zhibin Fu and Shuren Qi and Mingfu Xue and Zhongyun Hua and Yong Xiang},
  doi      = {10.1109/TBDATA.2022.3225194},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {936-948},
  title    = {Localization of inpainting forgery with feature enhancement network},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing keyword search over federated RDF systems.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(3), 918–935. (<a
href="https://doi.org/10.1109/TBDATA.2022.3224749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The issue of keyword search in federated resource description framework(RDF) systems is revisited in this paper. Numerous independent SPARQL endpoints that solely offer SPARQL query interfaces make up a federated RDF system. Others find it challenging to remotely build up the global inverted indices over the full RDF network by downloading the dataset at various SPARQL endpoints. This poses a challenge for existing keyword search approaches. In this paper, we propose a keyword search approach for federated RDF systems without building up global inverted indices on the entire RDF graph. We build an offline schema graph for the federated RDF system. During query evaluation, the full-text search interfaces provided by existing SPARQL endpoints were used to map keywords to vertices in the schema graph. Subsequently, the schema graph was explored to construct SPARQL queries from the keywords. The constructed queries were evaluated over the underlying federated RDF systems. Some cost models were proposed to measure keyword mapping and query construction. On the other hand, to further improve the efficiency, a multiple query optimization strategy was designed to rewrite the queries and share the common computation during evaluation. Extensive experiments on real RDF datasets show that the proposed techniques are effective.},
  archive  = {J},
  author   = {Mingdao Li and Peng Peng and Zhen Tian and Zheng Qin and Zheng Huang and Yi Liu},
  doi      = {10.1109/TBDATA.2022.3224749},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {918-935},
  title    = {Optimizing keyword search over federated RDF systems},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computing significant cliques in large labeled networks.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(3), 904–917. (<a
href="https://doi.org/10.1109/TBDATA.2022.3223644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mining cohesive subgraphs and communities is a fundamental problem in network analysis and has drawn much attention in the last decade. Most existing cohesive subgraph models mainly consider the structural cohesion but ignore the subgraph significance. In this article, we formulate a new model, called statistically significant clique, to mine significant cohesive subgraphs in large vertex-labeled graphs. A statistically significant clique is a complete subgraph with a significance value exceeding a given threshold. The subgraph significance is evaluated by a widely used metric called chi-square statistic. We study the problem of enumerating all maximal statistically significant cliques. The problem is proved to be NP-hard. We propose an efficient branch-and-bound algorithm with several elegant pruning strategies to solve our problem. We conduct extensive experiments on seven large real-world datasets to show the practical efficiency of our algorithms. We also conduct a case study to evaluate the effectiveness of our proposed model.},
  archive  = {J},
  author   = {Yu-Xuan Qiu and Dong Wen and Rong-Hua Li and Lu Qin and Michael Yu and Xuemin Lin},
  doi      = {10.1109/TBDATA.2022.3223644},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {904-917},
  title    = {Computing significant cliques in large labeled networks},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MNL: A highly-efficient model for large-scale dynamic
weighted directed network representation. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(3), 889–903. (<a
href="https://doi.org/10.1109/TBDATA.2022.3218064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A Non-negative Latent-factorization-of-tensors model relying on a N onnegative and M ultiplicative U pdate on I ncomplete T ensors (NMU-IT) algorithm facilitates efficient representation learning to a D ynamic W eighted D irected N etwork (DWDN). However, a NMU-IT algorithm leads to slow model convergence and inefficient selection of hyper-parameters. Aiming to address these challenging issues, this work proposes a Momentum-incorporated Biased Non-negative and Adaptive Latent-factorization-of-tensors (MNL) model. It adopts two-fold ideas: 1) incorporating a generalized momentum method into the NMU-IT algorithm to enable fast model convergence; 2) facilitating hyper-parameter slef-adaptation via Particle Swarm Optimization. Empirical studies on four real DWDNs indicate that the proposed MNL is superior to state-of-the-art models in performing efficient representation learning to a DWDN, which is definitely supported by its high computational efficiency and prediction accuracy for missing links of a DWDN. Moreover, its hyper-parameter-free training enables its high practicability in real scenes.},
  archive  = {J},
  author   = {Minzhi Chen and Chunlin He and Xin Luo},
  doi      = {10.1109/TBDATA.2022.3218064},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {889-903},
  title    = {MNL: A highly-efficient model for large-scale dynamic weighted directed network representation},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DAWN: Domain generalization based network alignment.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(3), 878–888. (<a
href="https://doi.org/10.1109/TBDATA.2022.3218128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Network alignment aims to discover nodes in different networks belonging to the same identity. In recent years, the network alignment problem has aroused significant attentions in both industry and academia. With the rapid growth of information, the sizes of networks are usually very large and in most cases we only focus on the alignment of partial networks. However, under this circumstances, the collected network data may be highly biased, and the training and testing data are no longer i.i.d. (identically and independently distributed). Thus, it is difficult for the trained alignment model to have a good performance in the test set. To bridge this gap, in this paper, we propose a novel D omain gener A lization based net W ork alig N ment approach termed as DAWN. Specifically, in DAWN, we first design a novel invariant feature extraction model which leverages adversarial learning to extract domain-invariant features. Then, we design a novel invariant network alignment model which can achieve global optimum and local optimum simultaneously to learn domain-invariant alignment patterns. Finally, we conduct extensive experiments on the benchmark dataset of Facebook-Twitter, and results show that DAWN can averagely achieve 14.01% higher Hits@k and 10.63% higher MRR@k compared with the state-of-the-art methods.},
  archive  = {J},
  author   = {Shuai Gao and Zhongbao Zhang and Sen Su},
  doi      = {10.1109/TBDATA.2022.3218128},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {878-888},
  title    = {DAWN: Domain generalization based network alignment},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task allocation under geo-indistinguishability via
group-based noise addition. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(3), 860–877. (<a
href="https://doi.org/10.1109/TBDATA.2022.3215467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Locations are usually necessary for task allocation in spatial crowdsourcing, which may put individual privacy in jeopardy without proper protection. Although existing studies have well explored the problem of location privacy protection in task allocation under geo-indistinguishability, they potentially assume the workers could perform any tasks, which might not be practical in reality. Moreover, they usually adopt planar laplacian mechanism to achieve geo-indistinguishability, which will introduce excessive noise due to its randomness and boundlessness. To this end, we propose a task allo CA tio N approach via gr O up-based nois E addition under Geo-I, referred to as CANOE . Its main idea is that each worker uploads the noisy distances between his true location and the obfuscated locations of his preferred tasks instead of uploading his obfuscated location. In particular, to alleviate the total noise when conducting grouping, we put forward an optimized global grouping with adaptive local adjustment method OGAL with convergence guarantee. To collect the noisy distances which are required for subsequent task allocation, we develop a utility-aware obfuscated distance collection method UODC with solid privacy and utility guarantees. We further theoretically analyze the privacy, utility and complexity guarantees of CANOE . Extensive analyses and experiments over two real-world datasets confirm the effectiveness of CANOE .},
  archive  = {J},
  author   = {Pengfei Zhang and Xiang Cheng and Sen Su and Ning Wang},
  doi      = {10.1109/TBDATA.2022.3215467},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {860-877},
  title    = {Task allocation under geo-indistinguishability via group-based noise addition},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel dynamic fusion approach using information entropy
for interval-valued ordered datasets. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(3), 845–859. (<a
href="https://doi.org/10.1109/TBDATA.2022.3215494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Information fusion is capable of fusing and transforming information originated from multiple sources into an integrated representation. As an important representative of information, interval-valued ordered data aims at characterizing inaccurate and ambiguous information. However, existing methods are inappropriate when applied to the multi-source fusion for them. In addition, it is inevitable that in daily-life the sources and attributes of multi-source information systems may update at the same time. Hence there is a need to fuse data as efficiently as possible. Inspired by these deficiencies, we pay attention to the effective and efficient fusion of multi-source interval-valued ordered data in varieties of cases. First, the concepts of fuzzy dominance relation and dominance classes based on it are put forward between any two samples of interval-valued ordered information systems. Second, we define the fuzzy dominating and dominated conditional entropy. Then the fusion model is established and it is multi-source interval-valued ordered data oriented. Furthermore, it is true that there are numerous real-life applications related to concurrent change of both sources and attributes. Consequently, we design four incremental mechanisms and algorithms for fusing multi-source interval-valued data on the ground of the static condition. Eventually, a series of experiments is carried out on twelve datasets to verify that the proposed fusion approach outperforms other comparative methods on efficacy. Meanwhile, our incremental fusion algorithms are efficient compared with the static one for updating multi-source interval-valued data when sources and attributes are in the simultaneous variation.},
  archive  = {J},
  author   = {Weihua Xu and Yanzhou Pan and Xiuwei Chen and Weiping Ding and Yuhua Qian},
  doi      = {10.1109/TBDATA.2022.3215494},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {845-859},
  title    = {A novel dynamic fusion approach using information entropy for interval-valued ordered datasets},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online POI recommendation: Learning dynamic geo-human
interactions in streams. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(3), 832–844. (<a
href="https://doi.org/10.1109/TBDATA.2022.3215134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Online POI recommendation strives to recommend future visitable places to users over time. It is crucial to improve the user experience of location-based social networking applications (e.g., Google Maps, Yelp). While numerous studies focus on capturing user visit preferences, geo-human interactions are ignored. However, users make visit decisions based on the status of geospatial contexts. In the meantime, such visits will change the status of geospatial contexts. Therefore, disregarding such geo-human interactions in streams will result in inferior recommendation performance. To fill this gap, in this article, we propose a novel deep interactive reinforcement learning framework to model geo-human interactions. Specifically, this framework has two main parts: the representation module and the imitation module. The purpose of the representation module is to capture geo-human interactions and convert them into embedding vectors (state). The imitation module is a reinforced agent whose job is to imitate user visit behavior by recommending next-visit POI (action) based on the state. Imitation performance is regarded as a reward signal to optimize the whole interactive framework. When the model converges, the imitation module can precisely perceive users and geospatial contexts to provide accurate POI recommendations. Finally, we conduct extensive experiments to validate the superiority of our framework.},
  archive  = {J},
  author   = {Dongjie Wang and Kunpeng Liu and Hui Xiong and Yanjie Fu},
  doi      = {10.1109/TBDATA.2022.3215134},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {832-844},
  title    = {Online POI recommendation: Learning dynamic geo-human interactions in streams},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable distributed data anonymization for large datasets.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(3), 818–831. (<a
href="https://doi.org/10.1109/TBDATA.2022.3207521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {&lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -Anonymity and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell$&lt;/tex-math&gt;&lt;/inline-formula&gt; -diversity are two well-known privacy metrics that guarantee protection of the respondents of a dataset by obfuscating information that can disclose their identities and sensitive information. Existing solutions for enforcing them implicitly assume to operate in a centralized scenario, since they require complete visibility over the dataset to be anonymized, and can therefore have limited applicability in anonymizing large datasets. In this article, we propose a solution that extends Mondrian (an efficient and effective approach designed for achieving &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -anonymity) for enforcing both &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -anonymity and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell$&lt;/tex-math&gt;&lt;/inline-formula&gt; -diversity over large datasets in a distributed manner, leveraging the parallel computation of multiple workers. Our approach efficiently distributes the computation among the workers, without requiring visibility over the dataset in its entirety. Our data partitioning limits the need for workers to exchange data, so that each worker can independently anonymize a portion of the dataset. We implemented our approach providing parallel execution on a dynamically chosen number of workers. The experimental evaluation shows that our solution provides scalability, while not affecting the quality of the resulting anonymization.},
  archive  = {J},
  author   = {Sabrina De Capitani di Vimercati and Dario Facchinetti and Sara Foresti and Giovanni Livraga and Gianluca Oldani and Stefano Paraboschi and Matthew Rossi and Pierangela Samarati},
  doi      = {10.1109/TBDATA.2022.3207521},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {818-831},
  title    = {Scalable distributed data anonymization for large datasets},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient strong privacy-preserving conjunctive keyword
search over encrypted cloud data. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(3), 805–817. (<a
href="https://doi.org/10.1109/TBDATA.2022.3205668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Searchable symmetric encryption (SSE) supports keyword search over outsourced symmetrically encrypted data. Dynamic searchable symmetric encryption (DSSE), a variant of SSE, further enables data updating. Most DSSE works with conjunctive keyword search primarily consider forward and backward privacy. Ideally, the server should only learn the result sets involving all keywords in the conjunction. However, existing schemes suffer from keyword pair result pattern (KPRP) leakage, revealing the partial result sets containing two of query keywords. We propose the first DSSE scheme to address aforementioned concerns that achieves strong privacy-preserving conjunctive keyword search. Specifically, our scheme can maintain forward and backward privacy and eliminate KPRP leakage, offering a higher level of security. The search complexity scales with the number of documents stored in the database in several existing schemes. However, the complexity of our scheme scales with the update frequency of the least frequent keyword in the conjunction, which is much smaller than the size of the entire database. Besides, we devise a least frequent keyword acquisition protocol to reduce frequent interactions between clients. Finally, we analyze the security of our scheme and evaluate its performance theoretically and experimentally. The results show that our scheme has strong privacy preservation and efficiency.},
  archive  = {J},
  author   = {Chang Xu and Ruijuan Wang and Liehuang Zhu and Chuan Zhang and Rongxing Lu and Kashif Sharif},
  doi      = {10.1109/TBDATA.2022.3205668},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {805-817},
  title    = {Efficient strong privacy-preserving conjunctive keyword search over encrypted cloud data},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedCKE: Cross-domain knowledge graph embedding in federated
learning. <em>IEEE Transactions on Big Data</em>, <em>9</em>(3),
792–804. (<a href="https://doi.org/10.1109/TBDATA.2022.3205705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Representing the structural relations between entities, i.e., knowledge graph embedding, which is a method to learn low-dimensional representations of knowledge, has become an increasingly prevalent research orientation in cognitive and human intelligence. It is significant to study how to interrelate, fuse and embed the knowledge graph data from different domains while considering data not shared. In this paper, we propose a model of cross-domain knowledge graph embedding in federated learning (FedCKE), in which entity/relation embedding between different domains can interact securely in the case that data is not shared. In advance of client model training, we present an inter-domain encrypted entity/relation alignment method using the encrypted sample alignment method in vertical federated learning, which can obtain entity/relation intersections between different domains without revealing any triples structure and additional entities/relations in the respective datasets. On the server, we aggregate the same entity/relation embeddings by the association in conjunction with the parameter-secure aggregation method in horizontal federated learning. Experimental results on three real datasets show that the proposed FedCKE model is able to enhance the embedding of different clients (domains).},
  archive  = {J},
  author   = {Wei Huang and Jia Liu and Tianrui Li and Shenggong Ji and Dexian Wang and Tianqiang Huang},
  doi      = {10.1109/TBDATA.2022.3205705},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {792-804},
  title    = {FedCKE: Cross-domain knowledge graph embedding in federated learning},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Big data for the social good: The drought early-warning
experience report. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(3), 773–791. (<a
href="https://doi.org/10.1109/TBDATA.2022.3191749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data deluge is growing exponentially, but the consumption of the data is not growing at the same pace. DataOps is an emerging family of techniques and tools that harnesses the potential of data continuously whilst incrementally using complex cloud systems orchestration techniques. This paper offers a proof-of-concept implementation of a DataOps pipeline for the social good. Specifically, we prototype—using a combined field-lab Action Research and Design Science approach—a DataOps system to incrementally and iteratively mitigate the devastating effects of droughts for high-risk areas. The context of our study is a game reserve in the Waterberg area in the province Limpopo in South Africa. The objective of this paper is threefold: to (1) develop and study a proof of concept for DataOps, by (2) exploring the applicability of individual software components in a complex and large-scale continuous pipeline, and, finally (3) elaborate on the spatial classification of such components in a new-frontier Drought Early-Warning System (DEWS). As a result, we offer an overview of the challenges and opportunities laid bare by our experimentation in a complex societal scenario while combining Artificial Intelligence and DataOps technologies. We conclude that a combined model of local, regional, and global data performs best on all tests within a stakeholder-acceptable timeframe.},
  archive  = {J},
  author   = {Damian A. Tamburri and Vincent R. van Mierlo and Willem-Jan van den Heuvel},
  doi      = {10.1109/TBDATA.2022.3191749},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {773-791},
  title    = {Big data for the social good: The drought early-warning experience report},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LongArms: Fraud prediction in online lending services using
sparse knowledge graph. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(2), 758–772. (<a
href="https://doi.org/10.1109/TBDATA.2022.3172060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Gang fraud, the major and primary security issue in online lending services, can be efficiently solved by the data-driven paradigm that is recognized as a promising solution for online lending gang fraud prediction. However, it is challenging that such predictions need to detect evolving and increasingly impalpable fraud patterns based on low-quality data, i.e., very preliminary and coarse applicant information. The technical difficulty mainly stems from two factors: the extreme deficiency of information associations and weakness of data labels . In this work, we mainly address the challenges by enhancing the utility of associations (i.e., recovering missing associations and mining underlying associations ) on a knowledge graph. Specifically, we first propose an efficient method of Chinese address disambiguation to recover some critical associations that are broken by the ambiguity of applicant information, e.g., address related information. Then, to mine the implicit associations, we design a novel association representation method, called Adaptive Connected Component Embedding Simplification Scheme (ACCESS), which can adaptively implement embedding for different connected components depending on their sizes. Finally, we adopt the graph clustering algorithms and devised predicting schemes based on the above enhanced associations to predict gang fraud in the case of weakness of data labels. Moreover, we propose a framework called RMCP by integrating the above techniques, which is consists of four steps: Recovering , Mining , Clustering , and Predicting , for efficiently predicting gang fraud. The good performance is validated by the experiments on a real-world dataset from a commercial lending company. Meanwhile, we provide a visual decision support system named LongArms over the RMCP framework.},
  archive  = {J},
  author   = {Cheng Wang and Hangyu Zhu and Ruixin Hu and Rui Li and Changjun Jiang},
  doi      = {10.1109/TBDATA.2022.3172060},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {758-772},
  title    = {LongArms: Fraud prediction in online lending services using sparse knowledge graph},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Who are the best adopters? User selection model for free
trial item promotion. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(2), 746–757. (<a
href="https://doi.org/10.1109/TBDATA.2022.3205334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the increasingly fierce market competition, the free trial has been widely applied as an effective incentive strategy to attract users and promote products. By providing opportunities to experience goods without charge, a free trial offers adopters more direct contact with the products and thus raises their willingness to buy. However, as the key point in the promotion process, how to select proper adopters is rarely explored. Empirically winnowing users by their static demographic attributes is feasible but less effective due to the lack of consideration of personalized demands. In this work, we propose SMILE – a tailored free trial user selection model for finding the best adopters promptly. Based on the reinforcement learning (RL) technique, SMILE can consider long-run profits and rely on user-item interactions to suggest actions. Besides, since selecting adopters from the large user candidates set is time-consuming, we design a balanced tree structure that reformulates the user action space. The experimental analysis on three datasets demonstrates the proposed model&#39;s superiority and elucidates why reinforcement learning and tree structure can improve performance. Our study shows technical feasibility of constructing a more robust and intelligent user selection model and guides for investigating more marketing promotion strategies.},
  archive  = {J},
  author   = {Shiqi Wang and Chongming Gao and Min Gao and Junliang Yu and Zongwei Wang and Hongzhi Yin},
  doi      = {10.1109/TBDATA.2022.3205334},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {746-757},
  title    = {Who are the best adopters? user selection model for free trial item promotion},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient learned spatial index with interpolation function
based learned model. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(2), 733–745. (<a
href="https://doi.org/10.1109/TBDATA.2022.3186857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, researchers have demonstrated that learned index can improve query performance while reducing the storage overhead. It potentially offers an opportunity to address the spatial query processing challenges caused by the surge in location-based services. Although several learned indexes have been proposed to process spatial data, the main idea behind these approaches is to utilize the existing one-dimensional learned models, which requires either converting the spatial data into one-dimensional data or applying the learned model on individual dimensions separately. As a result, these approaches cannot fully leverage or take advantage of the information regarding the spatial distribution of the original spatial data. To this end, in our previous work, we proposed a spatial (multi-dimensional) interpolation function based learned model to develop a spatial learned index and designed efficient range and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; NN query strategies over it. However, there are some limitations in the proposed learned model, such as the prediction accuracy and index building time. In this paper, we address the limitations of our previous work and propose a new spatial learned model by employing the characteristics of the spatial interpolation functions and a novel dynamic encoding technique. Detailed experiments are conducted with real-world datasets. The results indicate that our new proposed learned model is better than our previous one in terms of building time, prediction accuracy, and storage overhead simultaneously, and the new learned spatial index is better than the existing learned spatial indexes in query execution time and index building time.},
  archive  = {J},
  author   = {Songnian Zhang and Suprio Ray and Rongxing Lu and Yandong Zheng},
  doi      = {10.1109/TBDATA.2022.3186857},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {733-745},
  title    = {Efficient learned spatial index with interpolation function based learned model},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporating heterogeneous user behaviors and social
influences for predictive analysis. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(2), 716–732. (<a
href="https://doi.org/10.1109/TBDATA.2022.3193028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Behavior prediction based on historical behavioral data have practical real-world significance. It has been applied in recommendation, predicting academic performance, etc. With the refinement of user data description, the development of new functions, and the fusion of multiple data sources, heterogeneous behavioral data which contain multiple types of behaviors become more and more common. In this paper, we aim to incorporate heterogeneous user behaviors and social influences for behavior predictions. To this end, this paper proposes a variant of Long-Short Term Memory (LSTM) which can consider context information while modeling a behavior sequence, a projection mechanism which can model multi-faceted relationships among different types of behaviors, and a multi-faceted attention mechanism which can dynamically find out informative periods from different facets. Many kinds of behavioral data belong to spatio-temporal data. An unsupervised way to construct a social behavior graph based on spatio-temporal data and to model social influences is proposed. Moreover, a residual learning-based decoder is designed to automatically construct multiple high-order cross features based on social behavior representation and other types of behavior representations. Qualitative and quantitative experiments on real-world datasets have demonstrated the effectiveness of this model.},
  archive  = {J},
  author   = {Haobing Liu and Yanmin Zhu and Chunyang Wang and Jianyu Ding and Jiadi Yu and Feilong Tang},
  doi      = {10.1109/TBDATA.2022.3193028},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {716-732},
  title    = {Incorporating heterogeneous user behaviors and social influences for predictive analysis},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tracking the evolution of clusters in social media streams.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(2), 701–715. (<a
href="https://doi.org/10.1109/TBDATA.2022.3204207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Tracking the evolution of clusters in social media streams is becoming increasingly important for many applications, such as early detection and monitoring of natural disasters or pandemics. In contrast to clustering on a static set of data, streaming data clustering does not have a global view of the complete data. The local (or partial) view in a high-speed stream makes clustering a challenging task. In this paper, we propose a novel density peak based algorithm, TStream , for tracking the evolution of clusters and outliers in social media streams, via the evolutionary actions of cluster adjustment, emergence, disappearance, split, and merge. TStream is based on a temporal decay model and text stream summarisation. The decay model captures the decreasing importance of textual documents over time. The stream summarisation compactly represents them with the help of cells ( aka micro-clusters) in the memory. We also propose a novel efficient index called shared dependency tree ( aka SD-Tree) based on the ideas of density peak and shared dependency. It maintains the dynamic dependency relationships in TStream and thereby improves the overall efficiency. We conduct extensive experiments on five real datasets. TStream outperforms the existing state-of-the-art solutions based on MStream , MStreamF , EDMStream , OSGM , and EStream , in terms of cluster mapping measure (CMM) by up to 17.8%, 18.6%, 6.9%, 16.4%, and 20.1%, respectively. It is also significantly more efficient than MStream , MStreamF , OSGM , and EStream , in terms of response time and throughput.},
  archive  = {J},
  author   = {Tarique Anwar and Surya Nepal and Cecile Paris and Jian Yang and Jia Wu and Quan Z. Sheng},
  doi      = {10.1109/TBDATA.2022.3204207},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {701-715},
  title    = {Tracking the evolution of clusters in social media streams},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IEMask r-CNN: Information-enhanced mask r-CNN. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(2), 688–700. (<a
href="https://doi.org/10.1109/TBDATA.2022.3187413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The instance segmentation task is relatively difficult in computer vision, which requires not only high-quality masks but also high-accuracy instance category classification. Mask R-CNN has been proven to be a feasible method. However, due to the Feature Pyramid Network (FPN) structure lack useful channel information, global information and low-level texture information, and mask branch cannot obtain useful local-global information, Mask R-CNN is prevented from obtaining high-quality masks and high-accuracy instance category classification. Therefore, we proposed the Information-enhanced Mask R-CNN, called IEMask R-CNN. In the FPN structure of IEMask R-CNN, the information-enhanced FPN will enhance the useful channel information and the global information of the feature maps to solve the issues that the high-level feature map loses useful channel information and inaccurate of instance category classification, meanwhile the bottom-up path enhancement with adaptive feature fusion will ultilize the precise positioning signal in the lower layer to enhance the feature pyramid. In the mask branch of IEMask R-CNN, an encoding-decoding mask head will strength local-global information to gain a high-quality mask. Without bells and whistles, IEMask R-CNN gains significant gains of about 2.60%, 4.00%, 3.17% over Mask R-CNN on MS COCO2017, Cityscapes and LVIS1.0 benchmarks respectively.},
  archive  = {J},
  author   = {Xiuli Bi and Jinwu Hu and Bin Xiao and Weisheng Li and Xinbo Gao},
  doi      = {10.1109/TBDATA.2022.3187413},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {688-700},
  title    = {IEMask R-CNN: Information-enhanced mask R-CNN},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel overlapping community detection algorithm on GPU.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(2), 677–687. (<a
href="https://doi.org/10.1109/TBDATA.2022.3180360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Community detection is one of the most representative graph mining applications, which is often assembled as a concurrent graph partition application to explore the maximum modularity (or gained modularity) of each community. However, many branch divergence operations create significant obstacles to unleashing GPU&#39;s high throughput and memory bandwidth, which are needed in community detection applications to divide the vertices into different communities. In this paper, we present Lugger, a GPU-based overlapping community detection algorithm that reduces GPU&#39;s branch divergence via the customer-designed cache-aware parallel searching technique. In Lugger, we first design a cache-aware parallel searching policy using the B-Tree structure. Then, we set the B-Tree node matches with the GPU cache line to meet the coalesced memory access manner and avoid the branch divergence in warps. Moreover, we design a positive node splitting scheme to reduce the lock operation and idle threads when building the B-Tree structure. In addition, we implement a warp-centric thread assignment strategy to make sure the workloads across threads are balanced. We implement the proposed algorithm on NVIDIA GPU and evaluate the performance on eight large graphs (up to &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\text{3}~M$&lt;/tex-math&gt;&lt;/inline-formula&gt; vertices and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\text{117}~M$&lt;/tex-math&gt;&lt;/inline-formula&gt; edges) with ground-truth communities. The experimental results show that Lugger can outperform the state-of-the-art works on scalability and detection quality.},
  archive  = {J},
  author   = {Zhigao Zheng and Xuanhua Shi and Hai Jin},
  doi      = {10.1109/TBDATA.2022.3180360},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {677-687},
  title    = {Parallel overlapping community detection algorithm on GPU},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cost-driven top-k queries optimization approach on
federated RDF systems. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(2), 665–676. (<a
href="https://doi.org/10.1109/TBDATA.2022.3156090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {RDF (Resource Description Framework) is a model widely used to construct knowledge bases, while SPARQL (SPARQL Protocol and RDF Query Language) is the standardized structured query language to manipulate RDF data. Recently, many data providers have published their RDF datasets in their own autonomous sites and provided SPARQL query interfaces, called RDF sources . In order to integrate multiple RDF sources, researchers put forward the federated RDF system to support the federated SPARQL queries. However, existing studies can only support efficient basic queries but not top-k queries. Toward this end, we propose a cost-driven top-k queries optimization approach in federated RDF systems, which can support both top-k queries for single variable ordering and expression ordering. Firstly, we propose an optimized query decomposition method to decompose the federated query into multiple subqueries. Secondly, while considering the top-k operator, we propose a cost model to evaluate the query cost and join cost of subqueries. The optimal query plan can be obtained by the costed-based query plan generation algorithm. Finally, combined with the characteristics of top-k queries, an incremental query plan execution strategy is developed to minimize the total query cost. Experimental results show that the proposed method is effective, efficient and scalable.},
  archive  = {J},
  author   = {Ningchao Ge and Zheng Qin and Peng Peng and Mingdao Li and Lei Zou and Keqin Li},
  doi      = {10.1109/TBDATA.2022.3156090},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {665-676},
  title    = {A cost-driven top-K queries optimization approach on federated RDF systems},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An overall evaluation on benefits of competitive influence
diffusion. <em>IEEE Transactions on Big Data</em>, <em>9</em>(2),
653–664. (<a href="https://doi.org/10.1109/TBDATA.2021.3084468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Influence maximization (IM) is a representative and classic problem that has been studied extensively before. The most important application derived from the IM problem is viral marketing. Take us as a promoter, we want to get benefits from the influence diffusion in a given social network, where each influenced (activated) user is associated with a benefit. However, there is often competing information initiated by our rivals that diffuses in the same social network at the same time. Consider such a scenario, a user is influenced by both our information and our rivals’ information. Here, the benefit from this user should be weakened to a certain degree. How to quantify the degree of weakening? Based on that, we propose an overall evaluation on benefits of influence (OEBI) problem. We prove the objective function of the OEBI problem is not monotone, not submodular, and not supermodular. Fortunately, we can decompose this objective function into the difference of two submodular functions and adopt a modular-modular procedure to approximate it with a data-dependent approximation guarantee. Because of the difficulty to compute the exact objective value, we design a group of unbiased estimators by exploiting the idea of reverse influence sampling, which can improve time efficiency significantly without losing its approximation ratio. Finally, numerical experiments on real datasets verified the effectiveness of our approaches regardless of performance and efficiency.},
  archive  = {J},
  author   = {Jianxiong Guo and Yapu Zhang and Weili Wu},
  doi      = {10.1109/TBDATA.2021.3084468},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {653-664},
  title    = {An overall evaluation on benefits of competitive influence diffusion},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dense or sparse: Elastic SPMM implementation for optimal
big-data processing. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(2), 637–652. (<a
href="https://doi.org/10.1109/TBDATA.2022.3199197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Many real-world graph datasets can be represented using a sparse matrix format, and they are widely used for various big-data applications. The multiplication of two sparse matrices (SPMM) is a major kernel for various machine learning algorithms when using a sparsely expressed dataset. Apache Spark, a general-purpose big-data processing engine, includes the SPMM operation in its linear algebra package. The default Spark SPMM implementation, however, always converts a right sparse matrix to a dense format before performing multiplication, which can result in significant performance overhead for diverse SPMM scenarios. To address a limitation of the current Spark implementation, we describe an SPMM implementation that keeps the right matrix in a Compressed Sparse Column (CSC) format and propose an SPMM task latency prediction model based on a Deep Neural Network (DNN) architecture. Using the SPMM latency prediction model, we implement an elastic SPMM implementation recommendation service, which we name DoS ( D ense o r S parse). The proposed DoS recommends an optimal SPMM implementation method of either transforming a right matrix to a dense format or keeping it as a sparse format during the multiplication. Through evaluation of the proposed system using a real-world graph reveals that the proposed service can improve the SPMM latency of default Spark implementation by 2.2 times while shortening the overall execution time.},
  archive  = {J},
  author   = {Unho Choi and Kyungyong Lee},
  doi      = {10.1109/TBDATA.2022.3199197},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {637-652},
  title    = {Dense or sparse: Elastic SPMM implementation for optimal big-data processing},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BitAnalysis: A visualization system for bitcoin wallet
investigation. <em>IEEE Transactions on Big Data</em>, <em>9</em>(2),
621–636. (<a href="https://doi.org/10.1109/TBDATA.2022.3188660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Bitcoin is gaining ever increasing popularity. However, professional skills are required if people want to check bitcoin transaction information from the blockchain. As pointed out in a recent study, there is a lack of tools to support effective interactive investigation of bitcoin transactions. Therefore, we present a novel visualization system, BitAnalysis , for interactive bitcoin wallet investigation. The analytical and visualization functions of BitAnalysis are defined and developed by following the advice and requirements of a group of entrepreneurs and regulators of bitcoin-related business. BitAnalysis provides a rich set of functions and intuitive visual interfaces for the users, such as law-enforcement officers and regulators, to effectively visualize and analyze the transactions of a bitcoin wallet (i.e., a cluster of bitcoin addresses) and its related wallets, to track the flow of bitcoins, and to identify wallet correlation using our novel clustering functions. To achieve these functions, we have designed new visualization techniques for presenting bitcoin transactions information and introduced the connection diagram and bitcoin flow map as new ways of analyzing, tracking and monitoring the trading activities of a cluster of closely related wallets. We also present an extensive user study that validated the effectiveness and usability of BitAnalysis .},
  archive  = {J},
  author   = {Yujing Sun and Hao Xiong and Siu Ming Yiu and Kwok Yan Lam},
  doi      = {10.1109/TBDATA.2022.3188660},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {621-636},
  title    = {BitAnalysis: A visualization system for bitcoin wallet investigation},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling spatial trajectories using coarse-grained
smartphone logs. <em>IEEE Transactions on Big Data</em>, <em>9</em>(2),
608–620. (<a href="https://doi.org/10.1109/TBDATA.2022.3204759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Every user carries their smartphones wherever they go – a crucial aspect ignored by the current models for spatial recommendations. In detail, the current approaches learn the points-of-interest (POI) preferences of a user via the standard spatial features, i.e. the POI coordinates and the social network, and thus ignore the features related to the smartphone usage of a user. Moreover, with growing privacy concerns, users refrain from sharing their exact geographical coordinates as well as their social media activity. In this paper, we present ReVAMP , a sequential POI recommendation approach that uses smartphone app-usage logs to identify the mobility preferences of a user. Our work aligns with the recent psychological studies of online urban users which show that their spatial mobility behavior is largely influenced by the activity of their smartphone apps. Specifically, our proposal of coarse-grained data refers to data logs collected in a privacy-conscious manner consisting only of (a) category of smartphone app-used and (b) category of check-in location. Thus, ReVAMP is not privy to precise geo-coordinates, social networks, and the specific app being used. Buoyed by the efficacy of self-attention models, ReVAMP learns the POI preferences of a user using two forms of positional encodings – absolute and relative – with each extracted from the inter-check-in dynamics in the check-in sequence of a user. Extensive experiments across two large-scale datasets from China show that ReVAMP outperforms the state-of-the-art sequential POI recommendation approaches and can be extended to app- and POI-category prediction.},
  archive  = {J},
  author   = {Vinayak Gupta and Srikanta Bedathur},
  doi      = {10.1109/TBDATA.2022.3204759},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {608-620},
  title    = {Modeling spatial trajectories using coarse-grained smartphone logs},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AF-GCN: Attribute-fusing graph convolution network for
recommendation. <em>IEEE Transactions on Big Data</em>, <em>9</em>(2),
597–607. (<a href="https://doi.org/10.1109/TBDATA.2022.3192598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph Convolution Networks (GCNs) are playing important role and widely used in recommendation systems. This is benefited from their capability of capturing the collaborative signals of higher-order neighbors by exploiting the graph structure. GCN-based methods have made great success in improving recommending performance, but still suffer from the severe problem of data sparsity. An effective solution to alleviate the data sparsity is to introduce attribute information. However, existing GCN-based methods hardly capture the complex attribute information of users and items and the complicated relationships between users, items, and attributes simultaneously. To address the above problems, we propose a novel attribute-fusing graph convolution network model called AF-GCN. Specifically, we first propose an attention-based attribute fusion strategy by taking account of different effects of attributes. Then, we construct a complex graph containing four kinds of nodes. Finally, we design a particular Laplacian matrix, which leverages the attribute information through graph structure to learn user and item representations better. Extensive experimental results on three real-world datasets demonstrate that the proposed AF-GCN significantly outperforms state-of-the-art methods. The source codes of this work are available at https://github.com/xiaorui-mnaire/af-gcn .},
  archive  = {J},
  author   = {Guowei Yue and Rui Xiao and Zhongying Zhao and Chao Li},
  doi      = {10.1109/TBDATA.2022.3192598},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {597-607},
  title    = {AF-GCN: Attribute-fusing graph convolution network for recommendation},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-learning based classification for moving object
trajectories in mobile IoT. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(2), 584–596. (<a
href="https://doi.org/10.1109/TBDATA.2022.3195861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The proliferation and ubiquity of GPS-enabled mobile Internet of Things (IoT) devices (e.g., drones and unmanned robotic devices) across many disciplines has generated substantial interest in the analysis and mining of trajectory data for enhancing public services such as air pollution monitoring and road safety. Among all trajectory analysis techniques in mobile IoT, classifying moving device trajectories is a fundamental research problem due to its importance for numerous important mobile IoT applications such as travel demand analysis and animal migration patterns. However, existing classification methods for moving object trajectories either are dynamic programming problems (such as similarity-based methods with dynamic time wrapping or Fréchet distance) with a quadratic time complexity in all cases, or need abundant labeled samples to training classification model, thus limit the scalability of these methods for given resource-constrained mobile IoT devices. In this study, we propose a deep learning-based approach for trajectory classification in mobile IoT with linear time complexity by firstly encoding a trajectory as a vector via deep representation learning, then utilizing a meta-learning based approach to learn the ability of classifying trajectory with few annotated samples. Experiments on a massive trajectory dataset show that the proposed approach outperforms state-of-the-art baselines consistently and significantly.},
  archive  = {J},
  author   = {Yuanyi Chen and Peng Yu and Wenwang Chen and Zengwei Zheng and Minyi Guo},
  doi      = {10.1109/TBDATA.2022.3195861},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {584-596},
  title    = {Meta-learning based classification for moving object trajectories in mobile IoT},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Community detection based on multiobjective particle swarm
optimization and graph attention variational autoencoder. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(2), 569–583. (<a
href="https://doi.org/10.1109/TBDATA.2022.3164916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Community detection is an important research direction in complex network analysis that can help us discover valuable network structures. The community detection algorithms based on multiobjective particle swarm optimization encode community membership of nodes in particles and employ evolutionary strategies to search for the optimal community division. Existing algorithms face two challenges: (1) they are inapplicable to large networks because the evolution process is time-consuming; (2) they are easy to fall into local optima. In this paper, we propose a novel algorithm that combines a label-propagation-based multiobjective particle swarm optimization algorithm with a graph attention variational autoencoder to realize community detection. On the one hand, the label propagation strategy is involved in the update of a swarm&#39;s particles to speed up its evolution. The optimal solutions found by the particle swarm optimization algorithm are embedded into the objective of the autoencoder to improve the embedding vectors’ quality. On the other hand, the embedding vectors are used to improve the solutions of the particle swarm optimization algorithm to avoid its early convergence. The experiments on artificial and real-world networks demonstrate the feasibility and effectiveness of our algorithm compared with some state-of-the-art algorithms.},
  archive  = {J},
  author   = {Kun Guo and Zhanhong Chen and Xu Lin and Ling Wu and Zhi-Hui Zhan and Yuzhong Chen and Wenzhong Guo},
  doi      = {10.1109/TBDATA.2022.3164916},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {569-583},
  title    = {Community detection based on multiobjective particle swarm optimization and graph attention variational autoencoder},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal feature selection with efficient spouses discovery.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(2), 555–568. (<a
href="https://doi.org/10.1109/TBDATA.2022.3178472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Causal feature selection has recently attracted much more attention because it can improve the interpretability of predictive models. However, the existing causal feature selection framework needs to discover the PC (i.e., parents and children) of each variable in the PC of a target variable for spouses discovery, which is time-consuming on high-dimensional data. To tackle this issue, we propose a novel C ausal F eature S election framework with efficient spouses discovery, called CFS. Specifically, by exploiting the dependency change property between a variable and its non-PC, the proposed framework only discovers the PC of the variables in some children of the target variable for spouses discovery. Furthermore, based on the proposed CFS framework and existing PC discovery algorithms, we propose four new causal feature selection algorithms. Using benchmark Bayesian networks and real-world datasets, we experimentally validated the efficiency and accuracy of the proposed algorithms compared with seven state-of-the-art causal feature selection algorithms.},
  archive  = {J},
  author   = {Zhaolong Ling and Bo Li and Yiwen Zhang and Qingren Wang and Kui Yu and Xindong Wu},
  doi      = {10.1109/TBDATA.2022.3178472},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {555-568},
  title    = {Causal feature selection with efficient spouses discovery},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining hierarchical information of CNNs for scene
classification of VHR remote sensing images. <em>IEEE Transactions on
Big Data</em>, <em>9</em>(2), 542–554. (<a
href="https://doi.org/10.1109/TBDATA.2022.3196314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Scene classification of very high resolution (VHR) images is an active research subject in remote sensing community, and it has provided data or decision supports for many practical applications. Although existing CNN-based methods have achieved good classification results, they have not fully exploited rich potential information contained in pre-trained models. In this paper, a novel framework termed hierarchical features fusion of convolutional neural network (HFFCNN) is developed for scene classification of VHR images. On the whole, the HFFCNN covers two parallel modules to severally process convolutional features and fully connected (FC) features. At the first module, an adaptive spatial-wise attention based multi-scale nonlinear bag-of-visual-words (ASA-MNBoVW) model is designed to encoding convolutional feature maps, and the responses of discriminative regions are highlighted without introducing any additional parameters. For the second module, a weighted image pyramid structure is adopted to reveal geometric information and spatial layouts by aggregating local image patch-based FC features. Finally, these hierarchical features are combined for mutually complementing, and a linear classifier is adopted to predict semantic labels. Experimental results organized on two challenging data sets prove that the developed HFFCNN approach obtains more dramatic performance of scene classification than some state-of-the-art methods in terms of OAs.},
  archive  = {J},
  author   = {Kejie Xu and Peifang Deng and Hong Huang},
  doi      = {10.1109/TBDATA.2022.3196314},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {542-554},
  title    = {Mining hierarchical information of CNNs for scene classification of VHR remote sensing images},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-attention graph convolution residual network for
traffic data completion. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(2), 528–541. (<a
href="https://doi.org/10.1109/TBDATA.2022.3181068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Complete and accurate traffic data is critical in urban traffic management, planning and operation. In fact, real-world traffic data contains missing values due to multiple factors, such as device outages and communication errors. For traffic data completion task, most of the existing methods are matrix/tensor completion methods, which usually enforce low rank constraint on traffic data matrix/tensor. But they neglect the graph structure of traffic data, resulting in low completion performance. Recently, graph convolutional networks have achieved remarkable results in traffic data forecasting due to their abilities of feature extraction and nonlinear fitting on arbitrarily graph-structured data. However, there are few studies based on graph neural networks for traffic data completion task. In this paper, we propose a traffic data completion model based on graph convolutional network model to impute missing values from the perspective of deep learning. This model utilizes graph convolution to model the local spatial dependency. As for global spatial dependency and temporal dependency, this model incorporates self-attention mechanism, which is applied in the spatial and temporal dimensions respectively. The experimental results on the two real-time datasets demonstrate that the proposed model outperforms the baseline methods significantly under arbitrarily missing scenarios.},
  archive  = {J},
  author   = {Yong Zhang and Xiulan Wei and Xinyu Zhang and Yongli Hu and Baocai Yin},
  doi      = {10.1109/TBDATA.2022.3181068},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {528-541},
  title    = {Self-attention graph convolution residual network for traffic data completion},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TUSQ: Targeted high-utility sequence querying. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(2), 512–527. (<a
href="https://doi.org/10.1109/TBDATA.2022.3175428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Significant efforts have been expended in the research and development of a database management system (DBMS) that has a wide range of applications for managing an enormous collection of multisource, heterogeneous, complex, or growing data. Besides the primary function (i.e., create, delete, and update), a practical and impeccable DBMS can interact with users through information selection, that is, querying with their targets. Previous querying algorithms, such as frequent itemset querying and sequential pattern querying (SPQ) have focused on the measurement of frequency, which does not involve the concept of utility, which is helpful for users to discover more informative patterns. To apply the querying technology for wider applications, we incorporate utility into target-oriented SPQ and formulate the task of targeted utility-oriented sequence querying. To address the proposed problem, we develop a novel algorithm, namely targeted high-utility sequence querying (TUSQ), based on two novel upper bounds (suffix remain utility and terminated descendants utility) as well as a vertical last instance table. For further efficiency, TUSQ relies on a projection technology utilizing a compact data structure called the targeted chain. An extensive experimental study conducted on several real and synthetic datasets shows that the proposed algorithm outperformed the designed baseline algorithm in terms of runtime, memory consumption, and candidate filtering.},
  archive  = {J},
  author   = {Chunkai Zhang and Quanjian Dai and Zilin Du and Wensheng Gan and Jian Weng and Philip S. Yu},
  doi      = {10.1109/TBDATA.2022.3175428},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {512-527},
  title    = {TUSQ: Targeted high-utility sequence querying},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporating prior knowledge in local differentially
private data collection for frequency estimation. <em>IEEE Transactions
on Big Data</em>, <em>9</em>(2), 499–511. (<a
href="https://doi.org/10.1109/TBDATA.2022.3190033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Local differential privacy (LDP) is a prevalent measure of privacy protection as it provides rigorous privacy guarantees and has been widely studied for statistical analysis, especially in frequency estimation. As a representative LDP-enabled frequency estimation algorithm, Google&#39;s Randomized Aggregation Privacy-Preserving Ordinal Response (RAPPOR) has been put into practice. However, it achieves sub-optimal utility due to the following limitations. Firstly, the adoption of the MD5 hash function inevitably results in the hash collision. Secondly, the application of the randomized response technique leads to randomness. To improve the practical effectiveness of RAPPOR and the utility of frequency-based services, we propose an LDP-enabled frequency estimation method called PK-RAPPOR, in which we devise an effective re-encoding hash function (RE-HF) incorporating prior knowledge (PK) about the rough frequency ranking of items. RE-HF divides items into several cohorts based on the PK and generates a unique hash value set for each item. Compared with the original RAPPOR, the hash collision can be eliminated for items from different cohorts, and the effect of randomness can be decreased by the overlapping of items from the same cohorts. We validate our proposed method with theoretical analysis and demonstrate its effectiveness with experiments on both synthetic and real-world datasets.},
  archive  = {J},
  author   = {Xue Chen and Cheng Wang and Jipeng Cui and Qing Yang and Teng Hu and Changjun Jiang},
  doi      = {10.1109/TBDATA.2022.3190033},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {499-511},
  title    = {Incorporating prior knowledge in local differentially private data collection for frequency estimation},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SZ3: A modular framework for composing prediction-based
error-bounded lossy compressors. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(2), 485–498. (<a
href="https://doi.org/10.1109/TBDATA.2022.3201176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Today&#39;s scientific simulations require a significant reduction of data volume because of extremely large amounts of data they produce and the limited I/O bandwidth and storage space. Error-bounded lossy compression has been considered one of the most effective solutions to the above problem. In practice, however, the best-fit compression method often needs to be customized or optimized in particular because of diverse characteristics in different datasets and various user requirements on the compression quality and performance. In this paper, we address this issue with a novel modular, composable compression framework named SZ3. Our contributions are four-folds. (1) We develop SZ3 which features an innovative modular abstraction for the prediction-based compression framework, such that compression modules can be plugged in easily to create new compressors based on characteristics of data and user requirements. (2) We create a new compression pipeline by SZ3 for GAMESS data, which significantly improves the compression ratios over state-of-the-art compressors. (3) We develop an adaptive compression pipeline by SZ3 for APS data with minimal efforts, which leads to the best rate-distortion among all existing error-bounded lossy compressors for any bit-rate. (4) We compare the sustainability of SZ3 with leading error-bounded prediction-based compressors, and then demonstrate the necessity of diverse pipelines by integrating and evaluating several compression pipelines on diverse scientific datasets from multiple disciplines. Experiments show that SZ3 incurs very limited overhead in compressor integration and our customized compression pipelines lead to up to 20% improvement in compression ratios under the same data distortion, when compared with the best existing approach.},
  archive  = {J},
  author   = {Xin Liang and Kai Zhao and Sheng Di and Sihuan Li and Robert Underwood and Ali M. Gok and Jiannan Tian and Junjing Deng and Jon C. Calhoun and Dingwen Tao and Zizhong Chen and Franck Cappello},
  doi      = {10.1109/TBDATA.2022.3201176},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {485-498},
  title    = {SZ3: A modular framework for composing prediction-based error-bounded lossy compressors},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PrivTDSI: A local differentially private approach for truth
discovery via sampling and inference. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(2), 471–484. (<a
href="https://doi.org/10.1109/TBDATA.2022.3186175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Truth discovery is an effective way to identify the aggregated truth of each task among multiple observed data drawn from different workers of varying reliabilities. However, existing studies are insufficient to protect individuals’ privacy, as they either just guarantee the weaker versions of local differential privacy (LDP) or potentially assume that the tasks are independent. In this paper, we, for the first time, investigate the problem of truth discovery while achieving the rigorous LDP for each worker with continuous inputs without the independence assumption. We present a locally differentially private truth discovery approach called PrivTDSI based on sampling and inference with solid privacy and utility guarantees. In PrivTDSI , the server first determines which values of each worker should be sampled according to a sample proportion and sends the indexes of these values to each worker. Then, each worker adds noise into the sampled values for privacy protection and uploads them to the server. After receiving the noisy sampled values from all the workers, the server first infers the unsampled values and then conducts truth discovery based on both the noisy sampled values and the inferred values. In particular, to determine the sample proportion, we formulate a constrained nonlinear programming problem and give a closed-form solution to this problem. Moreover, to determine which values of each worker should be sampled while avoiding the situation where the values of some workers or tasks might not be sampled at all, we develop a two-stage sampling method called TOSS . Furthermore, to infer the unsampled values accurately, we design a quality-aware inference method based on matrix factorization called QualityMF . Experimental results on two real-world datasets and a synthetic dataset demonstrate the effectiveness of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;${PrivTDSI}$&lt;/tex-math&gt;&lt;/inline-formula&gt; .},
  archive  = {J},
  author   = {Pengfei Zhang and Xiang Cheng and Sen Su and Binyuan Zhu},
  doi      = {10.1109/TBDATA.2022.3186175},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {471-484},
  title    = {PrivTDSI: A local differentially private approach for truth discovery via sampling and inference},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network embedding via deep prediction model. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(2), 455–470. (<a
href="https://doi.org/10.1109/TBDATA.2022.3194643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Network-structured data becomes ubiquitous in daily life and is growing at a rapid pace. It presents great challenges to feature engineering due to the high non-linearity and sparsity of the data. The local and global structure of the real-world networks can be reflected by dynamical transfer behaviors among nodes. This paper proposes a network embedding framework to capture the transfer behaviors on structured networks via deep prediction models. We first design a degree-weight biased random walk model to capture the transfer behaviors on the network. Then a deep network embedding method is introduced to preserve the transfer possibilities among the nodes. A network structure embedding layer is added into conventional deep prediction models, including Long Short-Term Memory Network and Recurrent Neural Network, to utilize the sequence prediction ability. To keep the local network neighborhood, we further perform a Laplacian supervised space optimization on the embedding feature representations. Experimental studies are conducted on various datasets including social networks, citation networks, biomedical network, collaboration network and language network. The results show that the learned representations can be effectively used as features in a variety of tasks, such as clustering, visualization, classification, reconstruction and link recovery, and achieve promising performance compared with state-of-the-arts.},
  archive  = {J},
  author   = {Xin Sun and Zenghui Song and Yongbo Yu and Junyu Dong and Claudia Plant and Christian Böhm},
  doi      = {10.1109/TBDATA.2022.3194643},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {455-470},
  title    = {Network embedding via deep prediction model},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards efficient synchronous federated training: A survey
on system optimization strategies. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(2), 437–454. (<a
href="https://doi.org/10.1109/TBDATA.2022.3177222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The increasing demand for privacy-preserving collaborative learning has given rise to a new computing paradigm called federated learning (FL), in which clients collaboratively train a machine learning (ML) model without revealing their private training data. Given an acceptable level of privacy guarantee, the goal of FL is to minimize the time-to-accuracy of model training. Compared with distributed ML in data centers, there are four distinct challenges to achieving short time-to-accuracy in FL training, namely the lack of information for optimization, the tradeoff between statistical and system utility, client heterogeneity, and large configuration space. In this paper, we survey recent works in addressing these challenges and present them following a typical training workflow through three phases: client selection, configuration, and reporting. We also review system works including measurement studies and benchmarking tools that aim to support FL developers.},
  archive  = {J},
  author   = {Zhifeng Jiang and Wei Wang and Bo Li and Qiang Yang},
  doi      = {10.1109/TBDATA.2022.3177222},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {437-454},
  title    = {Towards efficient synchronous federated training: A survey on system optimization strategies},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on heterogeneous graph embedding: Methods,
techniques, applications and sources. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(2), 415–436. (<a
href="https://doi.org/10.1109/TBDATA.2022.3177455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Heterogeneous graphs (HGs) also known as heterogeneous information networks have become ubiquitous in real-world scenarios; therefore, HG embedding, which aims to learn representations in a lower-dimension space while preserving the heterogeneous structures and semantics for downstream tasks (e.g., node/graph classification, node clustering, link prediction), has drawn considerable attentions in recent years. In this survey, we perform a comprehensive review of the recent development on HG embedding methods and techniques. We first introduce the basic concepts of HG and discuss the unique challenges brought by the heterogeneity for HG embedding in comparison with homogeneous graph representation learning; and then we systemically survey and categorize the state-of-the-art HG embedding methods based on the information they used in the learning process to address the challenges posed by the HG heterogeneity. In particular, for each representative HG embedding method, we provide detailed introduction and further analyze its pros and cons; meanwhile, we also explore the transformativeness and applicability of different types of HG embedding methods in the real-world industrial environments for the first time. In addition, we further present several widely deployed systems that have demonstrated the success of HG embedding techniques in resolving real-world application problems with broader impacts. To facilitate future research and applications in this area, we also summarize the open-source code, existing graph learning platforms and benchmark datasets. Finally, we explore the additional issues and challenges of HG embedding and forecast the future research directions in this field.},
  archive  = {J},
  author   = {Xiao Wang and Deyu Bo and Chuan Shi and Shaohua Fan and Yanfang Ye and Philip S. Yu},
  doi      = {10.1109/TBDATA.2022.3177455},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {415-436},
  title    = {A survey on heterogeneous graph embedding: Methods, techniques, applications and sources},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data privacy threat modelling for autonomous systems: A
survey from the GDPR’s perspective. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(2), 388–414. (<a
href="https://doi.org/10.1109/TBDATA.2022.3227336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Artificial Intelligence-based applications have been increasingly deployed in every field of life including smart homes, smart cities, healthcare services, and autonomous systems where personal data is collected across heterogeneous sources and processed using ”black-box” algorithms in opaque centralised servers. As a consequence, preserving the data privacy and security of these applications is of utmost importance. In this respect, a modelling technique for identifying potential data privacy threats and specifying countermeasures to mitigate the related vulnerabilities in such AI-based systems plays a significant role in preserving and securing personal data. Various threat modelling techniques have been proposed such as STRIDE, LINDDUN, and PASTA but none of them is sufficient to model the data privacy threats in autonomous systems. Furthermore, they are not designed to model compliance with data protection legislation like the EU/UK General Data Protection Regulation (GDPR), which is fundamental to protecting data owners’ privacy as well as to preventing personal data from potential privacy-related attacks. In this article, we survey the existing threat modelling techniques for data privacy threats in autonomous systems and then analyse such techniques from the viewpoint of GDPR compliance. Following the analysis, We employ STRIDE and LINDDUN in autonomous cars, a specific use-case of autonomous systems, to scrutinise the challenges and gaps of the existing techniques when modelling data privacy threats. Prospective research directions for refining data privacy threats &amp; GDPR-compliance modelling techniques for autonomous systems are also presented.},
  archive  = {J},
  author   = {Naila Azam and Lito Michala and Shuja Ansari and Nguyen Binh Truong},
  doi      = {10.1109/TBDATA.2022.3227336},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {388-414},
  title    = {Data privacy threat modelling for autonomous systems: A survey from the GDPR&#39;s perspective},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking embedded unsupervised feature selection: A simple
joint approach. <em>IEEE Transactions on Big Data</em>, <em>9</em>(1),
380–387. (<a href="https://doi.org/10.1109/TBDATA.2022.3178715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, various embedded methods for unsupervised feature selection have been put forward. However, most of them adopt a two-step strategy, i.e., selecting &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; top-ranked dimensions according to a learned order of all features, then conducting K-means clustering for evaluation. This commonly used strategy usually results in a group of sub-optimal features, because the selected &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; top-ranked features are seldom the desired top- &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; dimensions. To address this problem, we rethink the two steps in a joint manner and propose a simple yet effective approach called U nsupervised F eature S election with S eparability ( UFS&lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^{\mathbf{2}}$&lt;/tex-math&gt;&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mn mathvariant=&quot;bold&quot;&gt;2&lt;/mml:mn&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=&quot;chang-ieq4-3178715.gif&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;/&gt;&lt;/inline-formula&gt; ) to simultaneously select features and cluster data. More specifically, a binary vector is seamlessly integrated into K-means to select an exact number of features for clustering. Different from previous embedded methods involving &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$l_{2,1}$&lt;/tex-math&gt;&lt;/inline-formula&gt; -norm, our joint model explicitly uses the parameter &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; (i.e., the number of selected features). Afterwards, a customized term for the binary vector is designed to maximize the separability among selected feature dimensions. In order to solve the formulated 0-1 integer programming problem, an iterative algorithm is developed. Finally, we evaluate the proposed approach extensively on different datasets. Despite the relative simplicity, UFS &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$^{2}$&lt;/tex-math&gt;&lt;/inline-formula&gt; remarkably and generally outperforms state-of-the-art baselines.},
  archive  = {J},
  author   = {Heng Chang and Jun Guo and Wenwu Zhu},
  doi      = {10.1109/TBDATA.2022.3178715},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {380-387},
  title    = {Rethinking embedded unsupervised feature selection: A simple joint approach},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed dual averaging based data clustering. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(1), 372–379. (<a
href="https://doi.org/10.1109/TBDATA.2022.3146169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multiagent distributed clustering scheme is proposed herein to process data which are collected by dispersed sensors that are not under centralized control. Two methods based on distributed dual averaging (DDA) algorithm are proposed, which are able to incorporate network structure and do not require exchange of centroid estimates, which makes it appealing for security conscious applications. The first method provides the framework for distributed clustering using the DDA algorithm with predefined regularization parameter. The second method, called Adaptive DDA (ADDA), relaxes the condition concerning a priori knowledge about the centroids, assumed in the first method, without losing clustering performance. This is achieved by properly regularizing the problem where a data-driven approach is used to determine the regularization parameter. The proposed methods are further extended via the proposed Bin method to scenario where processing agents store unbalanced amount of data with non-IID class distribution. Experiments are conducted on both real-life and synthetic data. Numerical results show the efficacy of the proposed approaches compared to state-of-art centralized algorithm and other distributed approaches.},
  archive  = {J},
  author   = {Mykola Servetnyk and Carrson C. Fung},
  doi      = {10.1109/TBDATA.2022.3146169},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {372-379},
  title    = {Distributed dual averaging based data clustering},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to realize efficient and scalable graph embeddings via
an entropy-driven mechanism. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(1), 358–371. (<a
href="https://doi.org/10.1109/TBDATA.2022.3164575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph embedding is becoming widely adopted as an efficient way to learn graph representations required to solve graph analytics problems. However, most existing graph embedding methods, owing to computation-efficiency challenges for large-scale graphs, generally employ a one-size-fits-all strategy to extract information, resulting in a large amount of redundant or inaccurate representations. In this work, we propose HuGE+, an efficient and scalable graph embedding method enabled by an entropy-driven mechanism. Specifically, HuGE+ leverages hybrid-property heuristic random walk to capture node features, which considers both information content of nodes and the number of common neighbors in each walking step. More importantly, to guarantee information effectiveness of sampling, HuGE+ adopts two heuristic methods to decide the random walk length and the number of walks per node, respectively. Extensive experiments on real-world graphs demonstrate that HuGE+ achieves both efficiency and performance advantages over recent popular graph embedding approaches. For three downstream graph tasks, our approach not only offers &gt;10% average gains, but also exhibits 23×–127× speedup over existing sampling-based methods. In addition, HuGE+ significantly reduces memory footprint by an average of 68.9%, facilitating training for billion-node-scale graph embeddings.},
  archive  = {J},
  author   = {Peng Fang and Fang Wang and Zhan Shi and Hong Jiang and Dan Feng and Xianghao Xu and Wei Yin},
  doi      = {10.1109/TBDATA.2022.3164575},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {358-371},
  title    = {How to realize efficient and scalable graph embeddings via an entropy-driven mechanism},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A drift-sensitive distributed LSTM method for short text
stream classification. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(1), 341–357. (<a
href="https://doi.org/10.1109/TBDATA.2022.3164239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Real-world applications especially in the fields of social media have produced massive short text streams. Unlike traditional normal texts, these data present the characteristics of short length, high-volume, high-velocity and variable data distribution etc, which lead to the issues of data sparsity and concept drift. It is hence very challenging for existing short text classification algorithms. Therefore, we propose a flexible Long Short-Term Memory (LSTM) ensemble network based short text stream classification approach, which is implemented in a distributed mode while maintaining the high-accuracy advantage of deep learning models. More specifically, external resource based short text embedding using a pretrained embedding model and CNN is first proposed for the solution to the data sparsity of short texts. Second, to adapt to the high-volume and high-velocity short text streams, a flexible LSTM network is developed and implemented in a distributed mode for classifying short text data streams. Third, a concept drift factor is introduced for adapting to the concept drifts caused by the changing of data distributions. Finally, experiments conducted on three real short text data sets demonstrate that as compared with several state-of-the-art short text (stream) classification approaches, the proposed approach can classify short text streams effectively and efficiently while adapting to concept drifts.},
  archive  = {J},
  author   = {Peipei Li and Yingying Liu and Yang Hu and Yuhong Zhang and Xuegang Hu and Kui Yu},
  doi      = {10.1109/TBDATA.2022.3164239},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {341-357},
  title    = {A drift-sensitive distributed LSTM method for short text stream classification},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalized deep learning algorithm based on NMF for
multi-view clustering. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(1), 328–340. (<a
href="https://doi.org/10.1109/TBDATA.2022.3163584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-view clustering research is a hot topic in the field of data mining, where complementary information between views can better describe data objects and improve the clustering performance. Non-negative matrix factorization (NMF) based multi-view clustering algorithm suffers from weak feature extraction, slow convergence speed and low accuracy. To solve these problems, this paper proposes a generalized deep learning multi-view clustering (GDLMC) algorithm based on NMF. Firstly, via decoupling the elements in the matrix, the matrix elements are non-negatively restricted using an activation function with a non-negative value domain, and the elements are updated employing stochastic gradient descent with learning rate guidance. Then, the corresponding gradients when the elements update are transformed into generalized weights and generalized biases, followed by combining the generalized weights and generalized biases with activation functions to construct generalized deep learning (GDL). Further, GDL is adopted to learn the corresponding low-dimensional matrix of each view and consensus matrix for obtaining the GDLMC algorithm. In addition, the detailed reasoning of the GDLMC algorithm are given. Finally, extensive experiments are conducted on four public datasets including regular and large-scale datasets, and the experimental results show that GDLMC has significant advantages.},
  archive  = {J},
  author   = {Dexian Wang and Tianrui Li and Ping Deng and Jia Liu and Wei Huang and Fan Zhang},
  doi      = {10.1109/TBDATA.2022.3163584},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {328-340},
  title    = {A generalized deep learning algorithm based on NMF for multi-view clustering},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised discrete multiple-length hashing for image
retrieval. <em>IEEE Transactions on Big Data</em>, <em>9</em>(1),
312–327. (<a href="https://doi.org/10.1109/TBDATA.2022.3161905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Hashing can facilitate efficient retrieval and storage for large-scale images due to the binary representation. In the real applications, the trade-off between retrieval accuracy and speed is essential for designing a hashing framework, which is reflected by variable hash code lengths. In light of this, the existing hashing methods need to train different models for different lengths of hash codes, leading to considerable training time cost and hashing flexibility reduction. Given that a sample can be represented by various hash codes with different lengths, there are some helpful relationships that can boost the performance of hashing methods. However, the existing hashing methods do not fully utilize these relationships. To address the aforementioned issues, we propose a new model, known as supervised discrete multiple-length hashing (SDMLH), to simultaneously learn hash codes with multiple lengths. In this proposed SDMLH method, three types of information are respectively derived, from the hash codes with different lengths. The original features of the samples, and the label, are applied for hash learning. Unlike the existing hashing methods, SDMLH can fully employ the assistance among hash codes with different lengths and learn them in one step. Furthermore, given a hash length meeting the demand of users, we propose a hash fusion strategy to obtain the hash code with this desirable length by fusing the multiple-length hash codes. This obtained hash code outperforms the one learned directly. In addition, SDMLH can generate the hash code of any length that is shorter than the sum length of given multiple hash codes with the fusion strategy. To the best of our knowledge, SDMLH is one of the first attempts for learning multiple-length hash codes simultaneously. We conduct extensive experiments based on three benchmark datasets, demonstrating the superiority of this proposed method.},
  archive  = {J},
  author   = {Xiushan Nie and Xingbo Liu and Jie Guo and Letian Wang and Yilong Yin},
  doi      = {10.1109/TBDATA.2022.3161905},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {312-327},
  title    = {Supervised discrete multiple-length hashing for image retrieval},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anomaly detection in catalog streams. <em>IEEE Transactions
on Big Data</em>, <em>9</em>(1), 294–311. (<a
href="https://doi.org/10.1109/TBDATA.2022.3161925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Detecting anomalies with high accuracy and real time from large amounts of streaming data is a challenge for many real-world applications, such as smart city, astronomical observations, and remote sensing. This article focuses on a special kind of stream, catalog stream, whose high-level catalog structure can be used to analyze the stream effectively. We first formulate the anomaly detection in catalog streams as a constrained optimization problem based on a catalog stream matrix. Then, a novel filtering-identifying based anomaly detection algorithm ( FIAD ) is proposed, which includes two complementary strategies, true event identifying and false alarm filtering, data-oriented general method and domain-oriented specific method together, to detect truly valuable anomalies. Furthermore, different kinds of attention windows are developed to provide corresponding data for various algorithm components. A scalable and lightweight catalog stream processing framework CSPF is designed to support and implement the proposed method efficiently. A prototype system is developed to evaluate the proposed algorithm. Extensive experiments are conducted on the catalog stream data sets from an operational super large field-of-view high-cadence astronomy observation. The experimental results show that the proposed method can achieve a false-positive rate as low as 0.04%, reduces the false alarms by 98.6% compared with the existing methods, and the latency to handle each catalog is 2.1 seconds (much less than the required 15 seconds). Furthermore, a total of 36 transient candidates, including seven microlensing events, 27 superflares, and two dual-superflares, are detected from 21.67 million stars (involving 1.09 million catalogs) from one observation season.},
  archive  = {J},
  author   = {Chen Yang and Zhihui Du and Xiaofeng Meng and Xukang Zhang and Xinli Hao and David A. Bader},
  doi      = {10.1109/TBDATA.2022.3161925},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {294-311},
  title    = {Anomaly detection in catalog streams},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view multi-task campaign embedding for cold-start
conversion rate forecasting. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(1), 280–293. (<a
href="https://doi.org/10.1109/TBDATA.2022.3162150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In online advertising, it is critical for advertisers to forecast conversion rate (CVR) of campaigns. Previous work on campaign forecasting concentrates on the time-series analysis which depend on the availability of a length of history. However, these approaches become inadequate for cold-start campaigns which lack for the observation of past. In this work, we attempt to mitigate this challenge by learning an unsupervised and composite campaign embedding to capture multi-view semantic relationships on campaign information, and consequently forecasting the cold-start campaigns using the nearest neighbor campaigns. Specifically, we propose a novel embedding framework which simultaneously extracts and fuses heterogeneous knowledge from multiple views of campaign data in a multi-task learning fashion, to learn the semantic relationship of ad message, conversion rule, and audience targeting. We develop a hierarchical attention mechanism to refine the embedding model at two levels - an intra-view attention to improve context aggregation, and an inter-task attention to balance task importance. Finally, we adopt the k-NN regression model to predict the CVR based on the neighboring campaigns in the embedding space which encodes the multi-view campaign proximity. We conduct extensive experiments on a real-world advertising campaign dataset. The results demonstrate the effectiveness of the proposed embedding method for CVR forecasting in cold-start scenarios.},
  archive  = {J},
  author   = {Zijun Yao and Deguang Kong and Miao Lu and Xiao Bai and Jian Yang and Hui Xiong},
  doi      = {10.1109/TBDATA.2022.3162150},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {280-293},
  title    = {Multi-view multi-task campaign embedding for cold-start conversion rate forecasting},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GGNN: Graph-based GPU nearest neighbor search. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(1), 267–279. (<a
href="https://doi.org/10.1109/TBDATA.2022.3161156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Approximate nearest neighbor (ANN) search in high dimensions is an integral part of several computer vision systems and gains importance in deep learning with explicit memory representations. Since PQT (Wieschollek et al. , 2016), FAISS (Johnson et al. , 2021), and SONG (Zhao et al. , 2020) started to leverage the massive parallelism offered by GPUs, GPU-based implementations are a crucial resource for today’s state-of-the-art ANN methods. While most of these methods allow for faster queries, less emphasis is devoted to accelerating the construction of the underlying index structures. In this paper, we propose a novel GPU-friendly search structure based on nearest neighbor graphs and information propagation on graphs. Our method is designed to take advantage of GPU architectures to accelerate the hierarchical construction of the index structure and for performing the query. Empirical evaluation shows that GGNN significantly surpasses the state-of-the-art CPU- and GPU-based systems in terms of build-time, accuracy and search speed.},
  archive  = {J},
  author   = {Fabian Groh and Lukas Ruppert and Patrick Wieschollek and Hendrik P. A. Lensch},
  doi      = {10.1109/TBDATA.2022.3161156},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {267-279},
  title    = {GGNN: Graph-based GPU nearest neighbor search},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SGAE: Stacked graph autoencoder for deep clustering.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(1), 254–266. (<a
href="https://doi.org/10.1109/TBDATA.2022.3160477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Unsupervised clustering is a crucial issue in data mining and pattern recognition. Based on deep learning paradigms, deep clustering algorithms have been studied extensively and obtained superior performance in various applications. However, most of previous methods did not use helpful information from neighborhood relations to form group-separated space, and the feature embedding is usually distorted during the training process. To tackle the former limitation, we develop a graph convolution based unsupervised learning algorithm named Stacked Graph Autoencoder (SGAE). Specifically, SGAE utilizes the message passing mechanism to aggregate information from neighbors and obtain a meaningful and separated latent representation. Since the adjacency matrix is unavailable in clustering tasks, a graph construction approach with two pruning strategies is introduced to generate a transition matrix. To reduce the distortion caused by the multi-layered network training process, we further propose a topological structure preservation mechanism. It uses the constructed adjacency graph as supervised information, to maintain the relationship between nodes in the original space. Experiments on several popular benchmark datasets show that SGAE achieves significant improvements compared to unsupervised and semi-supervised deep clustering methods.},
  archive  = {J},
  author   = {Shunxin Xiao and Shiping Wang and Wenzhong Guo},
  doi      = {10.1109/TBDATA.2022.3160477},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {254-266},
  title    = {SGAE: Stacked graph autoencoder for deep clustering},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DSTAGCN: Dynamic spatial-temporal adjacent graph
convolutional network for traffic forecasting. <em>IEEE Transactions on
Big Data</em>, <em>9</em>(1), 241–253. (<a
href="https://doi.org/10.1109/TBDATA.2022.3156366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Capturing complex and dynamic spatial-temporal dependencies of traffic data is of great importance for accurate and real-time traffic forecasting in intelligent transportation systems. The spatial-temporal dependency between traffic locations is often dynamic, which means the correlation between the traffic status of different locations changes jointly over their spatial distance and the time slice they are in. Most of the existing Graph Convolutional Network-based methods usually capture spatial and temporal dependencies separately and then combine them in a parallel or serial mechanism to capture the spatial-temporal features. They always utilize the predefined static graph structure to capture both local correlations and global dependencies in the same time slice. These methods are incapable of directly learning dynamic spatial-temporal dependency across time slices. Meanwhile, it is challenging to learn the spatiotemporal correlation knowledge among traffic locations only by using neural networks. To address these issues, we propose a novel Dynamic Spatial-Temporal Adjacent Graph Convolutional Network (DSTAGCN), which connects the latest time slice with each past time slice to construct the spatial-temporal graph. DSTAGCN can directly learn the global spatial dependency across time and simultaneously capture the spatial-temporal dependencies through graph convolution. Since fuzzy theory make it possible to represent uncertain relationships, a simplified fuzzy neural network that integrates fuzzy systems and neural networks is designed to help generating the graph adjacency matrix representing the dynamic adjacency correlations. Experiments on public datasets show that our method outperforms baselines with fast convergence.},
  archive  = {J},
  author   = {Qi Zheng and Yaying Zhang},
  doi      = {10.1109/TBDATA.2022.3156366},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {241-253},
  title    = {DSTAGCN: Dynamic spatial-temporal adjacent graph convolutional network for traffic forecasting},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metagraph-based life pattern clustering with big human
mobility data. <em>IEEE Transactions on Big Data</em>, <em>9</em>(1),
227–240. (<a href="https://doi.org/10.1109/TBDATA.2022.3155752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Life pattern clustering is essential for abstracting the groups&#39; characteristics of daily life patterns and activity regularity. Based on millions of GPS records, this research proposes a framework on the life pattern clustering which can efficiently identify the groups that have similar life patterns. The proposed method can retain original features of individual life pattern data without aggregation. Metagraph-based data structure is proposed for presenting the diverse life pattern. Spatial-temporal similarity includes significant places semantics, time-sequential properties and frequency are integrated into this data structure, which captures the uncertainty of an individual and the diversities between individuals. Non-negative-factorization-based method is utilized for reducing the dimension. The results show that our proposed method can effectively identify the groups that have similar life pattern in long term and takes advantage in computation efficiency and representational capacity compared with the traditional methods. We reveal the representative life pattern groups and analyze the group characteristics of human life patterns during different periods and different regions. We believe our work helps in future infrastructure planning, services improvement and policy making related to urban and transportation, thus promoting a humanized and sustainable city.},
  archive  = {J},
  author   = {Wenjing Li and Haoran Zhang and Jinyu Chen and Peiran Li and Yuhao Yao and Xiaodan Shi and Mariko Shibasaki and Hill Hiroki Kobayashi and Xuan Song and Ryosuke Shibasaki},
  doi      = {10.1109/TBDATA.2022.3155752},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {227-240},
  title    = {Metagraph-based life pattern clustering with big human mobility data},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Exploring multi-dimension user-item interactions with
attentional knowledge graph neural networks for recommendation. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(1), 212–226. (<a
href="https://doi.org/10.1109/TBDATA.2022.3154778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {It is commonly agreed that a recommender system should use not only explicit information (i.e., historical user-item interactions) but also implicit information (i.e., incidental information) to deal with the problem of data sparsity and cold start. The knowledge graph (KG), due to its expressive structural and semantic representation capabilities, has been increasingly used for capturing auxiliary information for recommender systems, such as the recent development of graph neural network (GNN) based models for KG-aware recommendation. Nevertheless, these models have the shortcoming of insufficient node interactions or improper node weights during information propagation, which limits the performance of recommender systems. To address this issue, we propose a Multi-dimension Interaction based attentional Knowledge Graph Neural Network (MI-KGNN) for enhanced KG-aware recommendation. MI-KGNN characterizes similarities between users and items through information propagation and aggregation in knowledge graphs. As such, it can optimize the updating direction of node representation by fully exploring multi-dimension interactions among nodes during information propagation. In addition, MI-KGNN introduces a dual attention mechanism, which allows users and items to jointly determine the weight of neighbor nodes. As a result, MI-KGNN can effectively capture and represent both structural (i.e., the topology of interactions) and semantic information (i.e., the weight of interactions) in the knowledge graph. Experimental results show that the proposed model significantly outperforms baseline methods for top-K recommendation. Specifically, the recall rate is increased by 5.78%, 6.66%, and 3.22% on three public datasets, compared with the best performance of existing methods.},
  archive  = {J},
  author   = {Zhu Wang and Zilong Wang and Xiaona Li and Zhiwen Yu and Bin Guo and Liming Chen and Xingshe Zhou},
  doi      = {10.1109/TBDATA.2022.3154778},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {212-226},
  title    = {Exploring multi-dimension user-item interactions with attentional knowledge graph neural networks for recommendation},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STGAN: Spatio-temporal generative adversarial network for
traffic data imputation. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(1), 200–211. (<a
href="https://doi.org/10.1109/TBDATA.2022.3154097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The traffic data corrupted by noise and missing entries often lead to the poor performance of Intelligent Transportation Systems (ITS), such as the bad congestion prediction and route guidance. How to efficiently impute the traffic data is an urgent problem. As a classic deep learning method, Generative Adversarial Network (GAN) achieves remarkable success in image recovery fields, which opens up a new way for the traffic data imputation. In this paper, we propose a novel spatio-temporal GAN model for the traffic data imputation (STGAN). Firstly, we design the generative loss and center loss, which not only minimizes the reconstructed errors of the imputed entries, but also ensures each imputed entry and its neighbors conform to the local spatio-temporal distribution. Then, the discriminator uses the convolution neural network classifier to judge whether the imputed matrix conforms to the global spatio-temporal distribution. As for the network architecture of the generator, we introduce the skip-connection to keep all well preserved data unchanged, and employ the dilated convolution to capture the spatio-temporal correlation in the traffic data. The experimental results show that our proposed method obviously outperforms other competitive traffic data imputation methods.},
  archive  = {J},
  author   = {Ye Yuan and Yong Zhang and Boyue Wang and Yuan Peng and Yongli Hu and Baocai Yin},
  doi      = {10.1109/TBDATA.2022.3154097},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {200-211},
  title    = {STGAN: Spatio-temporal generative adversarial network for traffic data imputation},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RPT: Toward transferable model on heterogeneous researcher
data via pre-training. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(1), 186–199. (<a
href="https://doi.org/10.1109/TBDATA.2022.3152386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the growth of the academic engines, the mining and analysis acquisition of massive researcher data, such as collaborator recommendation and researcher retrieval, has become indispensable for improving the quality and intelligence of services. However, most of the existing studies for researcher data mining focus on a single task for a particular application scenario and learning a task-specific model, which is usually unable to transfer to out-of-scope tasks. In this paper, we propose a multi-task self-supervised learning-based researcher data pre-training model named RPT, which is efficient to accomplish multiple researcher data mining tasks. Specifically, we divide the researchers’ data into semantic document sets and community graph. We design the hierarchical Transformer and the local community encoder to capture information from the two categories of data, respectively. Then, we propose three self-supervised learning objectives to train the whole model. For RPT’s main task, we leverage contrastive learning to discriminate whether these captured two kinds of information belong to the same researcher. In addition, two auxiliary tasks, named hierarchical masked language model and community relation prediction for extracting semantic and community information, are integrated to improve pre-training. Finally, we also propose two transfer modes of RPT for fine-tuning in different scenarios. We conduct extensive experiments to evaluate RPT, results on three downstream tasks verify the effectiveness of pre-training for researcher data mining.},
  archive  = {J},
  author   = {Ziyue Qiao and Yanjie Fu and Pengyang Wang and Meng Xiao and Zhiyuan Ning and Denghui Zhang and Yi Du and Yuanchun Zhou},
  doi      = {10.1109/TBDATA.2022.3152386},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {186-199},
  title    = {RPT: Toward transferable model on heterogeneous researcher data via pre-training},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A privacy and efficiency-oriented data sharing mechanism
for IoTs. <em>IEEE Transactions on Big Data</em>, <em>9</em>(1),
174–185. (<a href="https://doi.org/10.1109/TBDATA.2022.3148181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the volume of data increasing in the Internet of Things, a new business mode, where data owners share their own data to others for rewards, has emerged. Therefore, how to motivate data owners to participate in the data trading process is the main challenge. So far, lots of works focus on the motivation mechanism designing and ensure a fair distribution of profits among data owners. However, some security and privacy issues are still not well solved and the data owners are still unwilling to participate in the process. Especially, when a data provider claims rewards with its real identity for the shared data, the linkage between its real identity and the shared data will expose the participator&#39;s private information included in the shared data, such as location information. To protect user&#39;s privacy in the scenario, a privacy and efficiency-oriented data sharing mechanism for IoTs is proposed in this paper. We first propose a blockchain-based data sharing framework in which the behavior of all participants will be supervised. Then, in order to hide the real identities of data providers during the data sharing process, an anonymous certificate-based data sharing policy is proposed. At last, two novel non-interactive zero-knowledge proofs are designed to hide the identities of qualified data providers while claiming rewards to the system. Through security analysis and performance evaluation, the feasibility and effectiveness of the data sharing scheme are illustrated.},
  archive  = {J},
  author   = {Chao Wang and Shuo Wang and Xiaoman Cheng and Yunhua He and Ke Xiao and Shujia Fan},
  doi      = {10.1109/TBDATA.2022.3148181},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {174-185},
  title    = {A privacy and efficiency-oriented data sharing mechanism for IoTs},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight matrix factorization for recommendation with
local differential privacy in big data. <em>IEEE Transactions on Big
Data</em>, <em>9</em>(1), 160–173. (<a
href="https://doi.org/10.1109/TBDATA.2021.3139125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The proliferation of various items recommended by Internet-based systems has resulted in the exponential growth of the number of ratings in the big data era. Recent advances in matrix factorization have made it an effective way to process these ratings for recommendations. However, we confront a challenge in deploying a matrix factorization model for recommendations in big data that arises from the typically resource-constrained local devices regarding their storage space and computing capacity. This paper proposes a novel lightweight matrix factorization for recommendations. Our scheme deploys shard grandients training on user local internet of things(IoT) devices, which makes it possible for users to train big data model locally. We design a two-phase solution to protect the security of users’ data and reduce the dimension of the items. Moreover, we optimize the proposed scheme by introducing a stabilization mechanism to decrease the scale of the perturbed gradients. Some experimental results are given with two real online datasets, MovieLens and LibimSeTi. The theoretical analysis and experimental results demonstrate that compared with other methods, the proposed scheme achieves a good performance in terms of security, accuracy, and efficiency.},
  archive  = {J},
  author   = {Hao Zhou and Geng Yang and Yang Xiang and Yunlu Bai and Weiya Wang},
  doi      = {10.1109/TBDATA.2021.3139125},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {160-173},
  title    = {A lightweight matrix factorization for recommendation with local differential privacy in big data},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A structured approach towards big data identification.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(1), 147–159. (<a
href="https://doi.org/10.1109/TBDATA.2021.3139069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Big data is a ”relative” concept. It is the combination of data, application, and platform properties. The term big data has been used with almost every problem involving large size, real time, and heterogeneous data. However, these data attributes are not enough to identify big data by ignoring the application and platform properties for finding processing thresholds. The equivocated identification of big data can lead to an inefficient use of optimization techniques, resulting into global inefficiency, reduced system performance, increasing power consumption, requiring greater effort on the part of the programming team, and misallocation of the hardware resources required for the task. In this regard, a structured approach has been presented for identification of big data. The approach is based on three equations that categorize the Volume, Velocity, and Variety characteristics by relating data, application, and platform properties. The 3Vs identification is necessary for enabling the relevant optimization techniques. In addition to 3Vs identification, it is required to discriminate whether the big data is due to 1V, 2Vs or 3Vs, as the involvement of more Vs increases the problem complexity. In this regard, the classification of big data into strong , moderate or weak level has been proposed . To evaluate the proposed methods, a set of well-known applications have been experimented and categorized, depicting a saving of up to 58% main memory and 44% disk reads, as well as prescribing lower clock rate, lesser cores, sequential programming, and non adaptive processing &amp; storage formats. Moreover, four case studies reported as big data have been analyzed according to the proposed system. The proposed method is able to categorize two case studies as weak low big data presenting only volume, the third case is weak medium due to velocity, whereas in the fourth case no V is involved. Also, the proposed equations reduce the computation and human resources up to 75% of Spark cluster execution. In this manner, the proposed work can save the unnecessary investments by relevant prescriptions. Furthermore, the proposed equations can be integrated into different tools for assisting selective offloading of big data workloads to appropriate software and hardware solutions.},
  archive  = {J},
  author   = {Hameeza Ahmed and Muhammad Ali Ismail},
  doi      = {10.1109/TBDATA.2021.3139069},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {147-159},
  title    = {A structured approach towards big data identification},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). What and why? Towards duo explainable fauxtography detection
under constrained supervision. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(1), 133–146. (<a
href="https://doi.org/10.1109/TBDATA.2021.3130165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fauxtography is a category of multi-modal posts that spread misleading information on various big data online social platforms that generate billions of posts on a daily basis (e.g., Facebook, Twitter, Reddit). A fauxtography post usually consists of an image, a text description and comments from its readers. In this paper, we focus on explaining fauxtography posts by identifying what specific component and why that component of a post leads to the fauxtography (i.e., duo explanations). This problem is motivated by the limitations of current fauxtography detection solutions that only focus on the detection but ignore the important explanation aspect of their results. Two critical challenges exist in solving our problem: i) it is difficult to accurately identify the “guilty” component of a fauxtography post given the fact that different components of the post and their associations could all lead to the fauxtography; ii) it is expensive and time-consuming to obtain a good training set with fine-grained labels of fauxtography posts in terms of explainability, making it challenging to develop fully supervised explainable solutions. To address the above challenges, we develop a D uo Ex plainable F auxtography Detection Framework under a C onstrained Supervision (DExFC) to generate duo explanations from both content and comment parts of fauxtography posts. We evaluate the DExFC by creating real-world datasets from different online social media platforms (Twitter and Reddit). The results show that DExFC not only detects the fauxtography posts more accurately than the state-of-the-art solutions but also provides well-justified explanations to its results without the full supervision.},
  archive  = {J},
  author   = {Ziyi Kou and Daniel Zhang and Lanyu Shang and Dong Wang},
  doi      = {10.1109/TBDATA.2021.3130165},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {133-146},
  title    = {What and why? towards duo explainable fauxtography detection under constrained supervision},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting and understanding student learning performance
using multi-source sparse attention convolutional neural networks.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(1), 118–132. (<a
href="https://doi.org/10.1109/TBDATA.2021.3125204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Predicting and understanding student learning performance has been a long-standing task in learning science, which can benefit personalized teaching and learning. This study shows that the progress towards this task can be accelerated by using learning record data to feed a deep learning model that considers the intrinsic course association and the structured features. We proposed a multi-source sparse attention convolutional neural network (MsaCNN) to predict the course grades in a general formulation. MsaCNN adopts multi-scale convolution kernels on student grade records to capture structured features, a global attention strategy to discover the relationship between courses, and multiple input-heads to integrate multi-source features. All achieved features are then poured into a softmax classifier towards an end-to-end supervised deep learning model. Conducting insights into higher education on real-world university datasets, the results show that MsaCNN achieves better performance than traditional methods and delivers an interpretation of student performance by virtue of the resulted course relationships. Inspired by this interpretation, we created an association map for all mentioned courses, followed by evaluating the map with a questionnaire survey. This study provides computer-aided system tools and discovers the course-space map from the educational data, potentially facilitating the personalized learning progress.},
  archive  = {J},
  author   = {Yupei Zhang and Rui An and Shuhui Liu and Jiaqi Cui and Xuequn Shang},
  doi      = {10.1109/TBDATA.2021.3125204},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {118-132},
  title    = {Predicting and understanding student learning performance using multi-source sparse attention convolutional neural networks},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual analysis of multidimensional big data: A scalable
lightweight bundling method for parallel coordinates. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(1), 106–117. (<a
href="https://doi.org/10.1109/TBDATA.2021.3123982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Varied edge bundling methods have been used to reduce visual clutter in parallel coordinates plots (PCP). However, existing edge-bundled PCP do not scale well for visual analysis of multidimensional big data and often overplot the bundles in the area near the axes. In this study, we propose a scalable lightweight bundling method to support visual analysis of multidimensional big data in PCP. It helps the users discover trends and detect outliers in the data by bundling the edges between each two adjacent axes independently. We integrate human judgments into the two-dimensional data binning by novel interactions to accelerate the clustering process of the data. We use the frequency-based representation to render the clusters as histogram-like bundles to reveal the distribution of the data and eliminate the overplotting of the bundles. Based on our method, we build a lightweight web-based visual analytics system for exploring multidimensional big data in PCP. The scalability analysis of our method shows that its clustering time increases linearly with the size of the data. Its rendering time is independent of the size of the data. It can cluster and visualize 1 million data records with 6 dimensions in about 1 second in web-based visualization without pre-computation of the data or hardware-accelerated rendering. We conduct two case studies and a user study to compare our method with classic PCP and two state-of-the-art edge-bundled PCP. The results show that our method is more efficient and effective for visually analyzing multidimensional big data.},
  archive  = {J},
  author   = {Wenqiang Cui and Girts Strazdins and Hao Wang},
  doi      = {10.1109/TBDATA.2021.3123982},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {106-117},
  title    = {Visual analysis of multidimensional big data: A scalable lightweight bundling method for parallel coordinates},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical recovery of missing air pollution data via
improved long-short term context encoder network. <em>IEEE Transactions
on Big Data</em>, <em>9</em>(1), 93–105. (<a
href="https://doi.org/10.1109/TBDATA.2021.3123819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Due to equipment and transmission failures, data loss presents a key challenge to air quality monitoring. This paper attempts to recover missing air quality data from an air quality database. Leveraging adaptive updating convolutional neural networks (CNNs), we propose a novel Long-short term context encoder (ILSCE) model, which can simultaneously capture any temporal-spatial correlation and periodic variation identified from an air quality dataset. In addition, our model applies a new mechanism to automatically update both the air quality data and their corresponding masks in every single layer of CNN. Our proposed method presents three novelties. First, it hierarchically recovers any missing air quality values. Second, domain specific weekday/weekend and seasonal information are incorporated into the training model. Third, model performance is enhanced by an additional regularization term that captures the correlation between different air pollutants, thereby considering both background ambient pollution and local emissions. Our experimental study shows these three newly proposed features allow the ILSCE model to significantly outperform existing state-of-the-art imputation methods in air pollution data recovery. Furthermore, as data loss becomes more severe, with more missing data and more consecutively missing data, the superior recovery performance and greater robustness of our model become more prominent.},
  archive  = {J},
  author   = {Yangwen Yu and Victor O.K. Li and Jacqueline C.K. Lam},
  doi      = {10.1109/TBDATA.2021.3123819},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {93-105},
  title    = {Hierarchical recovery of missing air pollution data via improved long-short term context encoder network},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid riemannian graph-embedding metric learning for image
set classification. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(1), 75–92. (<a
href="https://doi.org/10.1109/TBDATA.2021.3113084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the continuously increasing amount of video data, image set classification has recently received widespread attention in the CV&amp;PR community. However, the intra-class diversity and inter-class ambiguity of representations remain an open challenge. To tackle this issue, several methods have been put forward to perform multiple geometry-aware image set modelling and learning. Although the extracted complementary geometric information is beneficial for decision making, the sophisticated computational paradigm (e.g., scatter matrices computation and iterative optimisation) of such algorithms is counterproductive. As a countermeasure, we propose an effective hybrid Riemannian metric learning framework in this paper. Specifically, we design a multiple graph embedding-guided metric learning framework for the sake of fusing these complementary kernel features, obtained via the explicit RKHS embeddings of the Grassmannian manifold, SPD manifold, and Gaussian embedded Riemannian manifold, into a unified subspace for classification. Furthermore, the involved optimisation problem of the developed model can be solved in terms of a series of sub-problems, achieving improved efficiency theoretically and experimentally. Substantial experiments are carried out to evaluate the efficacy of our approach. The experimental results suggest the superiority of it over the state-of-the-art methods.},
  archive  = {J},
  author   = {Ziheng Chen and Tianyang Xu and Xiao-Jun Wu and Rui Wang and Josef Kittler},
  doi      = {10.1109/TBDATA.2021.3113084},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {75-92},
  title    = {Hybrid riemannian graph-embedding metric learning for image set classification},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical lifelong machine learning with “watchdog.”
<em>IEEE Transactions on Big Data</em>, <em>9</em>(1), 63–74. (<a
href="https://doi.org/10.1109/TBDATA.2021.3110862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most existing lifelong machine learning works focus on how to exploit previously accumulated experiences (e.g., knowledge library) from earlier tasks, and transfer it to learn a new task. However, when a lifelong learning system encounters a large pool of candidate tasks, the knowledge among various coming tasks are imbalance, and the system should intelligently choose the next one to learn. In this paper, an effective “human cognition” strategy is taken into consideration via actively sorting the importance of new tasks in the process of unknown-to-known, and preferentially selecting the most valuable task with more information to learn. To be specific, we assess the importance of each new coming task (e.g., unknown or not) as an outlier detection issue, and propose to employ a “watchdog” knowledge library to reconstruct each task under &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\ell _0$&lt;/tex-math&gt;&lt;/inline-formula&gt; -norm constraint. The coming candidate tasks are then sorted depending on the sparse reconstruction scores in a descending order, which is referred to as a “watchdog” mechanism. Following this, we design a hierarchical knowledge library for the lifelong learning framework to encode new task with higher reconstruction score, where the library consists of two-level task descriptors, i.e., a high-dimensional one with low-rank constraint and a low-dimensional one. Both “watchdog” knowledge library and hierarchy knowledge library can be optimized with knowledge from both previously learned tasks and current task automatically. For model optimization, we explore an alternating method to iteratively update our proposed framework with a guaranteed convergence. Experimental results on several existing benchmarks demonstrate that our proposed model outperforms various state-of-the-art task selection methods.},
  archive  = {J},
  author   = {Gan Sun and Yang Cong and Changjun Gu and Xu Tang and Zhengming Ding and Haibin Yu},
  doi      = {10.1109/TBDATA.2021.3110862},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {63-74},
  title    = {Hierarchical lifelong machine learning with “Watchdog”},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noiseless privacy: Definition, guarantees, and applications.
<em>IEEE Transactions on Big Data</em>, <em>9</em>(1), 51–62. (<a
href="https://doi.org/10.1109/TBDATA.2021.3104021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we define noiseless privacy, as a non-stochastic rival to differential privacy, requiring that the outputs of a mechanism (i.e., function composition of a privacy-preserving mapping and a query) attain only a few values while varying the data of an individual (the logarithm of the number of the distinct values is bounded by the privacy budget). Therefore, the output of the mechanism is not fully informative of the data of the individuals in the dataset. We prove several guarantees for noiselessly-private mechanisms. The information content of the output about the data of an individual, even if an adversary knows all the other entries of the private dataset, is bounded by the privacy budget. The zero-error capacity of memory-less channels using noiselessly private mechanisms for transmission is upper bounded by the privacy budget. The performance of a non-stochastic hypothesis-testing adversary is bounded again by the privacy budget. Assuming that an adversary has access to a stochastic prior on the dataset, we prove that the estimation error of the adversary for individual entries of the dataset is lower bounded by a decreasing function of the privacy budget. In this case, we also show that the maximal leakage is bounded by the privacy budget. In addition to privacy guarantees, we prove that noiselessly-private mechanisms admit composition theorem and post-processing does not weaken their privacy guarantees. We prove that quantization or binning can ensure noiseless privacy if the number of quantization levels is appropriately selected based on the sensitivity of the query and the privacy budget. Finally, we illustrate the privacy merits of noiseless privacy using multiple datasets in energy, transport, and finance.},
  archive  = {J},
  author   = {Farhad Farokhi},
  doi      = {10.1109/TBDATA.2021.3104021},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {51-62},
  title    = {Noiseless privacy: Definition, guarantees, and applications},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximal quasi-cliques mining in uncertain graphs. <em>IEEE
Transactions on Big Data</em>, <em>9</em>(1), 37–50. (<a
href="https://doi.org/10.1109/TBDATA.2021.3093355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cohesive subgraph mining is a fundamental problem in the field of graph data analysis. Many existing cohesive graph mining algorithms are mainly tailored to deterministic graphs. Real-world graphs, however, are often not deterministic, but uncertain in nature. Applications of such uncertain graphs include protein-protein interactions networks with experimentally inferred links and sensor networks with uncertain connectivity links. In this article, we study the problem of mining cohesive subgraphs from an uncertain graph. Specifically, we introduce a new &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$(\alpha,\gamma)$&lt;/tex-math&gt;&lt;/inline-formula&gt; -quasi-clique model to model the cohesive subgraphs in an uncertain graph, and propose a basic enumeration algorithm to find all maximal &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$(\alpha,\gamma)$&lt;/tex-math&gt;&lt;/inline-formula&gt; -quasi-cliques. We also develop an advanced enumeration algorithm based on several novel pruning rules, including early termination and candidate set reduction. To further improve the efficiency, we propose several optimization techniques. Extensive experiments on five real-world datasets demonstrate that our solutions are almost three times faster than the baseline approach.},
  archive  = {J},
  author   = {Lianpeng Qiao and Rong-Hua Li and Zhiwei Zhang and Ye Yuan and Guoren Wang and Hongchao Qin},
  doi      = {10.1109/TBDATA.2021.3093355},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {37-50},
  title    = {Maximal quasi-cliques mining in uncertain graphs},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-ratio lossy compression: Exploring the autoencoder to
compress scientific data. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(1), 22–36. (<a
href="https://doi.org/10.1109/TBDATA.2021.3066151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Scientific simulations on high-performance computing (HPC) systems can generate large amounts of floating-point data per run. To mitigate the data storage bottleneck and lower the data volume, it is common for floating-point compressors to be employed. As compared to lossless compressors, lossy compressors, such as SZ and ZFP, can reduce data volume more aggressively while maintaining the usefulness of the data. However, a reduction ratio of more than two orders of magnitude is almost impossible without seriously distorting the data. In deep learning, the autoencoder technique has shown great potential for data compression, in particular with images. Whether the autoencoder can deliver similar performance on scientific data, however, is unknown. In this article, we for the first time conduct a comprehensive study on the use of autoencoders to compress real-world scientific data and illustrate several key findings on using autoencoders for scientific data reduction. We implement an autoencoder-based compression prototype to reduce floating-point data. Our study shows that the out-of-the-box implementation needs to be further tuned in order to achieve high compression ratios and satisfactory error bounds. Our evaluation results show that, for most of the test datasets, the tuned autoencoder outperforms SZ by up to 4X, and ZFP by up to 50X in compression ratios, respectively. Our practices and lessons learned in this work can direct future optimizations for using autoencoders to compress scientific data.},
  archive  = {J},
  author   = {Tong Liu and Jinzhen Wang and Qing Liu and Shakeel Alibhai and Tao Lu and Xubin He},
  doi      = {10.1109/TBDATA.2021.3066151},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {22-36},
  title    = {High-ratio lossy compression: Exploring the autoencoder to compress scientific data},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social media driven big data analysis for disaster situation
awareness: A tutorial. <em>IEEE Transactions on Big Data</em>,
<em>9</em>(1), 1–21. (<a
href="https://doi.org/10.1109/TBDATA.2022.3158431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Situational awareness tries to grasp the important events and circumstances in the physical world through sensing, communication, and reasoning. Tracking the evolution of changing situations is an essential part of this awareness and is crucial for providing appropriate resources and help during disasters. Social media, particularly Twitter, is playing an increasing role in this process in recent years. However, extracting intelligence from the available data involves several challenges, including (a) filtering out large amounts of irrelevant data, (b) fusion of heterogeneous data generated by the social media and other sources, and (c) working with partially geo-tagged social media data in order to deduce the needs of the affected people. Spatio-temporal analysis of the data plays a key role in understanding the situation, but is available only sparsely because only a small fraction of people post relevant text and of those very few enable location tracking. In this paper, we provide a comprehensive survey on data analytics to assess situational awareness from social media big data.},
  archive  = {J},
  author   = {Amitangshu Pal and Junbo Wang and Yilang Wu and Krishna Kant and Zhi Liu and Kento Sato},
  doi      = {10.1109/TBDATA.2022.3158431},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {1-21},
  title    = {Social media driven big data analysis for disaster situation awareness: A tutorial},
  volume   = {9},
  year     = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
