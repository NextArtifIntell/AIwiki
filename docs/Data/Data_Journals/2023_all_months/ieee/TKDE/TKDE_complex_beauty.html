<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TKDE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tkde---927">TKDE - 927</h2>
<ul>
<li><details>
<summary>
(2023). Video visual relation detection with contextual knowledge
embedding. <em>TKDE</em>, <em>35</em>(12), 13083–13095. (<a
href="https://doi.org/10.1109/TKDE.2023.3270328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video visual relation detection (VidVRD) aims at abstracting structured relations in the form of $&amp;lt; $ subject-predicate-object $&amp;gt;$ from videos. The triple formation makes the search space extremely huge and the distribution unbalanced. Usually, existing works predict the relationships from visual, spatial, and semantic cues. Among them, semantic cues are responsible for exploring the semantic connections between objects, which is crucial to transfer knowledge across relations. However, most of these works extract semantic cues via simply mapping the object labels to classified features, which ignore the contextual surroundings, resulting in poor performance for low-frequency relations. To alleviate these issues, we propose a novel network, termed Contextual Knowledge Embedded Relation Network (CKERN), to facilitate VidVRD through establishing contextual knowledge embeddings for detected object pairs in relations from two aspects: commonsense attributes and prior linguistic dependencies. Specifically, we take the pair as a query to extract relational facts in the commonsense knowledge base, then encode them to explicitly construct semantic surroundings for relations. In addition, the statistics of object pairs with different predicates distilled from large-scale visual relations are taken into account to represent the linguistic regularity of relations. Extensive experimental results on benchmark datasets demonstrate the effectiveness and robustness of our proposed model.},
  archive      = {J_TKDE},
  author       = {Qianwen Cao and Heyan Huang},
  doi          = {10.1109/TKDE.2023.3270328},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {13083-13095},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Video visual relation detection with contextual knowledge embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransCP: A transformer pointer network for generic entity
description generation with explicit content-planning. <em>TKDE</em>,
<em>35</em>(12), 13070–13082. (<a
href="https://doi.org/10.1109/TKDE.2023.3271971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study neural data-to-text generation to generate a sentence to describe a target entity based on its attributes. Specifically, we address two problems of the encoder-decoder framework for data-to-text generation: i) how to encode a non-linear input (e.g., a set of attributes); and ii) how to order the attributes in the generated description. Existing studies focus on the encoding problem but do not address the ordering problem, i.e., they learn the content-planning implicitly. The other approaches focus on two-stage models but overlook the encoding problem. To address the two problems at once, we propose a model named TransCP to explicitly learn content-planning and integrate them into a description generation model in an end-to-end fashion. We propose a novel Transformer-based Pointer Network with gated residual attention and importance masking to learn a content-plan. To integrate the content-plan with a description generator, we propose a tracking mechanism to trace the extent to which the content-plan is exposed in the previous decoding time-step. This helps the description generator select the attributes to be mentioned in proper order. Experimental results show that our model consistently outperforms state-of-the-art baselines by up to 2\% and 3\% in terms of BLEU score on two real-world datasets.},
  archive      = {J_TKDE},
  author       = {Bayu Distiawan Trisedya and Jianzhong Qi and Haitao Zheng and Flora D. Salim and Rui Zhang},
  doi          = {10.1109/TKDE.2023.3271971},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {13070-13082},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TransCP: A transformer pointer network for generic entity description generation with explicit content-planning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards adaptive information fusion in graph convolutional
networks. <em>TKDE</em>, <em>35</em>(12), 13055–13069. (<a
href="https://doi.org/10.1109/TKDE.2023.3271772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have gained great popularity in tackling various analytic tasks on graph and network data. However, some recent studies raise concerns about whether GCNs can optimally integrate node features and topological structures in a complex graph. In this paper, we first present an experimental investigation. Surprisingly, our experimental results clearly show that the capability of the state-of-the-art GCNs in fusing node features and topological structures is distant from optimal or even satisfactory. The weakness may severely hinder the capability of GCNs in some classification tasks, since GCNs may not be able to adaptively learn some deep correlation information between topological structures and node features. Can we remedy the weakness and design a new type of GCNs that can retain the advantages of the state-of-the-art GCNs and, at the same time, enhance the capability of fusing topological structures and node features substantially? We tackle the challenge and propose an A daptive M ulti-channel G raph C onvolutional N etwork for semi-supervised classification ( AM-GCN ). The central idea is that we extract the specific and common embeddings from node features, topological structures, and their combinations simultaneously, and use the attention mechanism to learn adaptive importance weights of the embeddings. However, considering that the input topology and feature structure in AM-GCN are still predefined and fixed, once the properties of graph structures are not consistent with tasks, the fusion performance of AM-GCN will be hindered from the beginning. Therefore, we need to adjust the structure and further propose the L abel P ropagation guided M ulti-channel G raph C onvolutional N etwork ( LPM-GCN ). LPM-GCN introduces edge weights learning on both topology and feature spaces to improve structural homophily, which can better promote the fusion process of graph convolutional networks. Our extensive experiments on benchmark data sets clearly show that our proposed models extract the most correlated information from both node features and topological structures substantially, and improves the classification accuracy with a clear margin.},
  archive      = {J_TKDE},
  author       = {Meiqi Zhu and Xiao Wang and Chuan Shi and Yibo Li and Junping Du},
  doi          = {10.1109/TKDE.2023.3271772},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {13055-13069},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards adaptive information fusion in graph convolutional networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TERL: Two-stage ensemble reinforcement learning paradigm for
large-scale decentralized decision making in transportation simulation.
<em>TKDE</em>, <em>35</em>(12), 13043–13054. (<a
href="https://doi.org/10.1109/TKDE.2023.3272688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transportation simulation is non-trivial due to the co-existence of thousands of heterogeneous decision makers (or vehicles). Such large-scale decision making is intrinsically a complex decentralized problem, the resolution of which is at the forefront of transportation simulation. Despite many physical or mathematical models proposed to date, underlying them is usually a set of universal rules plus some random perturbations to characterize vehicular movements. Inspired by the decision-making mechanism of rational human beings (i.e., learning iteratively from experience), this study proposes a novel two-stage ensemble reinforcement learning (TERL) paradigm for large-scale decentralized decision making in transportation simulation, in order to enhance the computational efficiency and thus scalability of RL to practical applications at scale. After establishing a problem-specific Markov decision process, the first stage utilizes clustering to group heterogeneous vehicles into quasi-homogeneous clusters. A representative RL model (or agent) is employed for each cluster where the included vehicles share and jointly optimize the policy parameters. The second stage develops an ensemble control strategy based on representative RL models for vehicles isolated as noise during clustering. While vehicles are simultaneously simulated, RL models are separately trained with cluster-specific experience replay. Application of TERL to the classical multi-user dynamic route choice problem in a real-world network of the Gusu District in Suzhou, China demonstrates the effectiveness of the proposed approach in deriving desirable simulation results, compared with the classical shortest path model and the dynamic user equilibrium model.},
  archive      = {J_TKDE},
  author       = {Ziyuan Gu and Xun Yang and Qi Zhang and Wenwu Yu and Zhiyuan Liu},
  doi          = {10.1109/TKDE.2023.3272688},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {13043-13054},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TERL: Two-stage ensemble reinforcement learning paradigm for large-scale decentralized decision making in transportation simulation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal heterogeneous information network embedding via
semantic evolution. <em>TKDE</em>, <em>35</em>(12), 13031–13042. (<a
href="https://doi.org/10.1109/TKDE.2023.3287260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world networks are often heterogeneous and constantly changing over time. Evolution reveals the trend of network development, which is vital for predicting its future state, and network embedding can effectively learn the information from it. Nevertheless, previous works only consider the impact of meta-path instances or node neighbors on the network dynamics but ignore the relationship between them, and hence the hidden semantic information is missed, which will result in performance deterioration. Therefore, we propose a novel temporal heterogeneous information network embedding method (SemE), which abstracts the instance of the meta-path as semantic units and then considers the interaction between them to discover deeper semantic information. Specifically, we first construct semantic networks by the Ethernet topology and the interaction between semantic units. The semantic units are sampled based on a pre-designed meta-path-guided random walk. To further capture the semantic evolution of the semantic network, we learn the embedding of nodes by the attention-Hawkes process. Finally, we generate the final embedding by aggregating the structure, semantic and temporal information with the attention mechanism. Experiments on three real-world temporal heterogeneous information networks show that SemE performs better than competitive counterparts.},
  archive      = {J_TKDE},
  author       = {Wei Zhou and Hong Huang and Ruize Shi and Xiran Song and Xue Lin and Xiao Wang and Hai Jin},
  doi          = {10.1109/TKDE.2023.3287260},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {13031-13042},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Temporal heterogeneous information network embedding via semantic evolution},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal graph cube. <em>TKDE</em>, <em>35</em>(12),
13015–13030. (<a
href="https://doi.org/10.1109/TKDE.2023.3270460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data warehouse and OLAP (Online Analytical Processing) are effective tools for decision support on traditional relational data and static multidimensional network data. However, many real-world multidimensional networks are often modeled as temporal multidimensional networks, where the edges in the network are associated with temporal information. Such temporal multidimensional networks typically cannot be handled by traditional data warehouse and OLAP techniques. To fill this gap, we propose a novel data warehouse model, named $\mathsf {Temporal{ }\; Graph{ }\; Cube}$ , to support OLAP queries on temporal multidimensional networks. Through supporting OLAP queries in any time range, users can obtain summarized information of the network in the time range of interest, which cannot be derived by using traditional static graph OLAP techniques. We propose a segment-tree based indexing technique to speed up the OLAP queries, and also develop an index-updating technique to maintain the index when the temporal multidimensional network evolves over time. In addition, we also propose a novel concept called $\mathsf {similarity{ }\; of{ }\; snapshots}$ which shows a strong correlation with the efficiency of indexing technique and can provide a good reference on the necessity of building the index. The results of extensive experiments on two large real-world datasets demonstrate the effectiveness and efficiency of the proposed method.},
  archive      = {J_TKDE},
  author       = {Guoren Wang and Yue Zeng and Rong-Hua Li and Hongchao Qin and Xuanhua Shi and Yubin Xia and Xuequn Shang and Liang Hong},
  doi          = {10.1109/TKDE.2023.3270460},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {13015-13030},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Temporal graph cube},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). TDN: Triplet distributor network for knowledge graph
completion. <em>TKDE</em>, <em>35</em>(12), 13002–13014. (<a
href="https://doi.org/10.1109/TKDE.2023.3272568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional Knowledge Graph Completion (KGC) methods typically map entities and relations to a unified space through the shared mapping matrix, and then interact with entities and relations to infer the missing items in the knowledge graph. Although this shared mapping matrix considers the suitability of all triplets, it neglects the specificity of each triplet. To solve this problem, we dynamically learn one information distributor for each triplet to exchange its specific information. In this paper, we propose a novel Triplet Distributor Network (TDN) for the knowledge graph completion task. Specifically, we adaptively learn one Triplet Distributor (TD) for each triplet to assist the interaction between the entity and relation. Furthermore, on the basis of TD, we creatively design the information exchange layer to dynamically propagate the information of the entity and relation, thus mutually enhancing entity and relation representations. Except for several commonly-used knowledge graph datasets, we still implement the link prediction task on the social-relational and medical datasets to test the proposed method. Experimental results demonstrate that the proposed method performs better than existing state-of-the-art KGC methods. The source codes of this paper are available at https://github.com/TDN for Knowledge Graph Completion.git.},
  archive      = {J_TKDE},
  author       = {Jiapu Wang and Boyue Wang and Junbin Gao and Xiaoyan Li and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TKDE.2023.3272568},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {13002-13014},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TDN: Triplet distributor network for knowledge graph completion},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Seesaw counting filter: A dynamic filtering framework for
vulnerable negative keys. <em>TKDE</em>, <em>35</em>(12), 12987–13001.
(<a href="https://doi.org/10.1109/TKDE.2023.3273709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bloom filter is an efficient data structure for filtering negative keys (keys not in a given set) with substantially small space. However, in real-world applications, there widely exist vulnerable negative keys, which will bring high costs if not being properly filtered, especially when positive keys are added/deleted dynamically. Such problem gets more severe when keys within one set are dynamically added or deleted. Recently, there are works focusing on handling such (vulnerable) negative keys by incorporating learning techniques. These learning-based filters fail to work as the learning techniques can hardly handle incremental insertions or deletions. To address the problem, we propose S ee S aw C ounting F ilter (SSCF), which is innovated with encapsulating the vulnerable negative keys into a unified counter array named seesaw counter array, and dynamically modulating (or varying) the applied hash functions to guard the encapsulated keys from being misidentified. Moreover, we design ada-SSCF to handle the scenarios where the vulnerable negative keys cannot be obtained in advance. We extensively evaluate our SSCF, which shows that SSCF outperforms the cutting-edge filters by $3\times$ on averages regarding accuracy while ensuring a low operation latency. All source codes are in (SSCF-authors).},
  archive      = {J_TKDE},
  author       = {Meng Li and Deyi Chen and Haipeng Dai and Rongbiao Xie and Siqiang Luo and Rong Gu and Tong Yang and Guihai Chen},
  doi          = {10.1109/TKDE.2023.3273709},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12987-13001},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Seesaw counting filter: A dynamic filtering framework for vulnerable negative keys},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SEAnet: A deep learning architecture for data series
similarity search. <em>TKDE</em>, <em>35</em>(12), 12972–12986. (<a
href="https://doi.org/10.1109/TKDE.2023.3270264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key operation for massive data series collection analysis is similarity search. According to recent studies, SAX-based indexes offer state-of-the-art performance for similarity search tasks. However, their performance lags under high-frequency, weakly correlated, excessively noisy, or other dataset-specific properties. In this work, we propose Deep Embedding Approximation (DEA), a novel family of data series summarization techniques based on deep neural networks. Moreover, we describe SEAnet, a novel architecture especially designed for learning DEA, that introduces the Sum of Squares preservation property into the deep network design. We further enhance SEAnet with SEAtrans encoder. Finally, we propose novel sampling strategies, SEAsam and SEAsamE, that allow SEAnet to effectively train on massive datasets. Comprehensive experiments on 7 diverse synthetic and real datasets verify the advantages of DEA learned using SEAnet in providing high-quality data series summarizations and similarity search results.},
  archive      = {J_TKDE},
  author       = {Qitong Wang and Themis Palpanas},
  doi          = {10.1109/TKDE.2023.3270264},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12972-12986},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SEAnet: A deep learning architecture for data series similarity search},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reveal your images: Gradient leakage attack against unbiased
sampling-based secure aggregation. <em>TKDE</em>, <em>35</em>(12),
12958–12971. (<a
href="https://doi.org/10.1109/TKDE.2023.3271432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, some Unbiased Gradient Sampling-based (UGS) methods have been proposed to enhance the security and efficiency of federated learning through crafted unbiased random transformation and sampling, such as MinMax Sampling in SIGMOD ’22. In this paper, we propose a novel attack, GLAUS, to show that UGS is not as secure as claimed in these works and is still vulnerable to the gradient leakage attack (GLA). Specifically, we demonstrate an idea to approximately infer the gradient for GLA in the context of the UGS scenario where the real gradient is not available. Once the gradient is approximately obtained, the security of the UGS frameworks is downgraded to that of the original federated learning. The approximate gradient is refined by the following steps: 1) narrow the gradient searching range to the finite set; 2) obtain the magnitude of each gradient value approximately; 3) revise the gradient signs . Versus the failure of existing attacks, extensive experiments on six datasets show that our attack is effective in reconstructing private datapoints with pixel-wise accuracy on four network sizes and three image resolutions. Finally, we show how to defend against GLAUS while maintaining the high efficiency of UGS and only introducing an additional step to hide the sampled gradient indices.},
  archive      = {J_TKDE},
  author       = {Yilong Yang and Zhuo Ma and Bin Xiao and Yang Liu and Teng Li and Junwei Zhang},
  doi          = {10.1109/TKDE.2023.3271432},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12958-12971},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reveal your images: Gradient leakage attack against unbiased sampling-based secure aggregation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Reinforced imitative graph learning for mobile user
profiling. <em>TKDE</em>, <em>35</em>(12), 12944–12957. (<a
href="https://doi.org/10.1109/TKDE.2023.3270238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile user profiling refers to the efforts of extracting users’ characteristics from mobile activities. In order to capture the dynamic varying of user characteristics for generating effective user profiling, we propose an imitation-based mobile user profiling framework. Considering the objective of teaching an autonomous agent to imitate user mobility based on the user&#39;s profile, the user profile is the most accurate when the agent can perfectly mimic the user behavior patterns. The profiling framework is formulated into a reinforcement learning task, where an agent is a next-visit planner, an action is a POI that a user will visit next, and the state of the environment is a fused representation of a user and spatial entities. An event in which a user visits a POI will construct a new state, which helps the agent predict users’ mobility more accurately. In the framework, we introduce a spatial Knowledge Graph (KG) to characterize the semantics of user visits over connected spatial entities. Additionally, we develop a mutual-updating strategy to quantify the state that evolves over time. Along these lines, we develop a reinforcement imitative graph learning framework for mobile user profiling. Finally, we conduct extensive experiments to demonstrate the superiority of our approach.},
  archive      = {J_TKDE},
  author       = {Dongjie Wang and Pengyang Wang and Yanjie Fu and Kunpeng Liu and Hui Xiong and Charles E. Hughes},
  doi          = {10.1109/TKDE.2023.3270238},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12944-12957},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reinforced imitative graph learning for mobile user profiling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Progressive hard negative masking: From global uniformity to
local tolerance. <em>TKDE</em>, <em>35</em>(12), 12932–12943. (<a
href="https://doi.org/10.1109/TKDE.2023.3269795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised contrastive learning has recently become increasingly popular due to its amazing performance without the need for costly annotations. However, indiscriminate sampling of negative pairs is accompanied by the uniformity-tolerance dilemma, which is especially serious in node-level graph contrastive learning due to the smoothing property of graph convolutional operators. Previous negative mining strategies that either overly emphasize hard negatives or rely on precise distribution estimation can make minor improvements or even degrade the performance in such a case. In this article, we investigate the role of hard negatives in the uniformity-tolerance dilemma and propose a novel contrastive objective with a progressive hard negative masking scheme. The proposed objective, as an asymptotically-tightened lower bound of mutual information, is theoretically and empirically demonstrated to be capable of allowing higher local tolerance and stronger contrastive effects, thus leading to higher-quality embedding distributions and considerable performance improvement in downstream node classification tasks.},
  archive      = {J_TKDE},
  author       = {Qingqiang Sun and Wenjie Zhang and Xuemin Lin},
  doi          = {10.1109/TKDE.2023.3269795},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12932-12943},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Progressive hard negative masking: From global uniformity to local tolerance},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prefix coding scheme supporting direct access without
auxiliary space. <em>TKDE</em>, <em>35</em>(12), 12917–12931. (<a
href="https://doi.org/10.1109/TKDE.2023.3271111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entropy coding is a widely used technique for lossless data compression. The entropy coding schemes supporting the direct access capability on the encoded stream have been investigated in recent years. However, all prior schemes require auxiliary space to support the direct access ability. This paper proposes a rearranging method for prefix codes to support a certain level of direct access to the encoded stream without requiring additional data space. Then, an efficient decoding algorithm is proposed based on lookup tables. The simulation results show that when the encoded stream does not allow additional space, the number of bits per access read of the proposed method is above two orders of magnitude less than the conventional method. In contrast, the alternative solution consumes at least one more bit per symbol on average than the proposed method to support direct access. This indicates that the proposed scheme can achieve a good trade-off between space usage and access performance. In addition, if a small amount of additional storage space is allowed (it is approximately 0.057\% in the simulation), the number of bits per access read in our proposal can be significantly reduced by 90\%.},
  archive      = {J_TKDE},
  author       = {Na Wang and Wei Yan and Hao Jiang and Sian-Jheng Lin and Yunghsiang S. Han},
  doi          = {10.1109/TKDE.2023.3271111},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12917-12931},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Prefix coding scheme supporting direct access without auxiliary space},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online learning with incremental feature space and bandit
feedback. <em>TKDE</em>, <em>35</em>(12), 12902–12916. (<a
href="https://doi.org/10.1109/TKDE.2023.3272313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning is a fundamental paradigm for learning from continuous data stream. Tradition online learning approaches usually assume that the feature space of data stream is fixed and the incoming instance can always get the true label after making its prediction. However, in many real-world applications, such as the personalized recommender systems, the feature space may keep expanding due to the accumulation of user behaviors. Besides, we may only get bandit feedback, i.e., we only know whether the prediction is correct or not. To solve this important but rarely studied problem, we propose a novel algorithm LIFBF, together with its two variants LIFBF-I and LIFBF-II, to learn from data stream with incremental feature space and bandit feedback. Specifically, when an instance arrives with augmented features, we first utilize the exploration-exploitation strategy to guess its best label, then, a new loss function considering both bandit feedback and guessed label is proposed. Finally, we design a highly dynamic multi-class classifier, which updates the shared and augmented features by adopting the passive-aggressive rule and structural risk minimization principle, respectively. We theoretically analyze the cumulative loss bound of LIFBF. Besides, empirical studies on various datasets further validate the effectiveness of our proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Shilin Gu and Tingjin Luo and Ming He and Chenping Hou},
  doi          = {10.1109/TKDE.2023.3272313},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12902-12916},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online learning with incremental feature space and bandit feedback},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OneSketch: A generic and accurate sketch for data streams.
<em>TKDE</em>, <em>35</em>(12), 12887–12901. (<a
href="https://doi.org/10.1109/TKDE.2023.3278028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a generic sketch algorithm capable of achieving more accuracy in the following five tasks: finding top- $k$ frequent items, finding heavy hitters, per-item frequency estimation, and heavy changes in the time and spatial dimension. The state-of-the-art (SOTA) sketch solution for multiple measurement tasks is ElasticSketch (ES). However, the accuracy of its frequency estimation has room for improvement. The reason for this is that ES suffers from overestimation errors in the light part, which introduces errors when querying both frequent and infrequent items. To address these problems, we propose a generic sketch, OneSketch, designed to minimize overestimation errors. To achieve the design goal, we propose four key techniques, which embrace hash collisions and minimize possible errors by handling highly recurrent item replacements well. Experimental results show that OneSketch clearly outperforms 12 SOTA schemes. For example, compared with ES, OneSketch achieves more than 10× lower Average Absolute Error on finding top- $k$ frequent items and heavy hitters, as well as 48.3\% and 38.4\% higher F1 Scores on two heavy changes under 200 KB memory, respectively.},
  archive      = {J_TKDE},
  author       = {Zhuochen Fan and Ruixin Wang and Yalun Cai and Ruwen Zhang and Tong Yang and Yuhan Wu and Bin Cui and Steve Uhlig},
  doi          = {10.1109/TKDE.2023.3278028},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12887-12901},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {OneSketch: A generic and accurate sketch for data streams},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OMG: Towards effective graph classification against label
noise. <em>TKDE</em>, <em>35</em>(12), 12873–12886. (<a
href="https://doi.org/10.1109/TKDE.2023.3271677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph classification is a fundamental problem with diverse applications in bioinformatics and chemistry. Due to the intricate procedures of manual annotations in graphical domains, there may be abundant noisy labels of graphs in practice, resulting in poor performance for existing supervised methods. Thus, it is necessary and urgent to study the problem of graph classification with label noise. However, this problem is challenging due to the overfitting of noisy data as well as complicated relational structures of graphs. To handle this problem, we present a simple but effective approach called c O upled M ix for G raph Contrast (OMG), which combines coupled Mixup with graph contrastive learning in the feature space. On the one hand, to improve the model generalization, we take convex combination of sample pairs in the feature space for positive pair construction. On the other hand, to accomplish effective optimization, we offer challenging negatives by multiple sample Mixup with different emphasis. To further reduce the impact of noisy data, we develop a neighbour-aware noise removal strategy, which promotes the smoothness in the neighbourhood of samples following the principle of curriculum learning. Extensive experiments on a range of benchmark datasets demonstrate the superiority of our proposed OMG.},
  archive      = {J_TKDE},
  author       = {Nan Yin and Li Shen and Mengzhu Wang and Xiao Luo and Zhigang Luo and Dacheng Tao},
  doi          = {10.1109/TKDE.2023.3271677},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12873-12886},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {OMG: Towards effective graph classification against label noise},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). No DBA? No regret! Multi-armed bandits for index tuning of
analytical and HTAP workloads with provable guarantees. <em>TKDE</em>,
<em>35</em>(12), 12855–12872. (<a
href="https://doi.org/10.1109/TKDE.2023.3271664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating physical database design has remained a long-term interest in database research due to substantial performance gains afforded by optimised structures. Despite significant progress, a majority of today&#39;s commercial solutions are highly manual, requiring offline invocation by database administrators (DBAs). This status quo is untenable: identifying representative static workloads is no longer realistic; and physical design tools remain susceptible to the query optimiser&#39;s cost misestimates. Furthermore, modern application environments like hybrid transactional and analytical processing (HTAP) systems render analytical modelling next to impossible. We propose a self-driving approach to online index selection that does not depend on the DBA and query optimiser, and instead learns the benefits of viable structures through strategic exploration and direct performance observation. We view the problem as one of sequential decision making under uncertainty, specifically within the bandit learning setting. Multi-armed bandits balance exploration and exploitation to provably guarantee average performance that converges to policies that are optimal with perfect hindsight. Our comprehensive empirical evaluation against a state-of-the-art commercial tuning tool demonstrates up to 75\% speed-up in analytical processing environments and 59\% speed-up in HTAP environments. Lastly, our bandit framework outperforms a Monte Carlo tree search (MCTS)-based database optimiser, providing up to 24\% speed-up.},
  archive      = {J_TKDE},
  author       = {R. Malinga Perera and Bastian Oetomo and Benjamin I. P. Rubinstein and Renata Borovica-Gajic},
  doi          = {10.1109/TKDE.2023.3271664},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12855-12872},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {No DBA? no regret! multi-armed bandits for index tuning of analytical and HTAP workloads with provable guarantees},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multi-view bipartite graph clustering with coupled noisy
feature filter. <em>TKDE</em>, <em>35</em>(12), 12842–12854. (<a
href="https://doi.org/10.1109/TKDE.2023.3268215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised bipartite graph learning has been a hot topic in multi-view clustering, to tackle the restricted scalability issue of traditional full graph clustering in large-scale applications. However, the existing bipartite graph clustering paradigm pays little attention to the adverse impact of noisy features on learning process. To further facilitate this part of research, apart from simply reweighting features to depress the noisy ones, we take the first step towards analyzing the induced adverse impact via theoretical and experimental investigations. One crucial finding in this article is that the existence of noisy features will incur “anchor shift” phenomenon, which deviates from the potential representations of anchors and then degrades performance. To this end, we propose a coupled noisy feature filter mechanism with automatically finding feature importance to remedy the anchor shift issue in this article. Apart from leveraging features, we theoretically analyze the bounds of proposed feature-adaptive bipartite graph&#39;s fuzzy membership. Specifically, distinguishing features’ discrimination will increase the fuzzy membership to achieve soft partitions against the potential inaccurate absolute relationships. With the afore-mentioned merits, our proposed multi-view bipartite graph clustering with coupled noisy feature filter model (MVBGC-NFF) provides novel and interesting insights on the feature level of anchor shift. The effectiveness and efficiency of MVBGC-NFF are demonstrated on synthetic and real-world datasets with improved clustering performance, increasing fuzzy membership, and filtering noisy features. The code is available on https://github.com/liliangnudt/MVBGC-NFF .},
  archive      = {J_TKDE},
  author       = {Liang Li and Junpu Zhang and Siwei Wang and Xinwang Liu and Kenli Li and Keqin Li},
  doi          = {10.1109/TKDE.2023.3268215},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12842-12854},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view bipartite graph clustering with coupled noisy feature filter},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-stream concept drift self-adaptation using graph
neural network. <em>TKDE</em>, <em>35</em>(12), 12828–12841. (<a
href="https://doi.org/10.1109/TKDE.2023.3272911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is the phenomenon where the data distribution in a data stream changes over time. It is a ubiquitous problem in the real-world, for example, a traffic accident would cause a jam in a certain period, leading to a distribution change in traffic speed. Most research in the concept drift field focuses on single data stream, however, few of them consider multi-stream environments which are more in line with the application needs. To fill this gap, we propose a multi-stream prediction setting and a multi-stream concept drift self-adaptation framework using graph neural network, named SAGN. In SAGN, we reconsider the learning procedure of GNN-based predictors from an aspect of concept drift adaptation for multi-stream. By this design, the prediction task is converted into online streaming data tasks in sub-graphs. Each sub-graph corresponds to an adaptation target and will be updated over time. In this way, locally we can overcome drift in each sub-graph by a designed adaptation technique, and globally the correlation between different data streams is well-preserved as a graph structure. Therefore, whether drift occurs or not, in one or several streams, SAGN can provide consistently accurate prediction results. We comprehensively tested SAGN on both synthetic and real-world, drift and non-drift data in the multi-step prediction task. The experiment results show that SAGN is able to achieve state-of-the-art performance in most cases.},
  archive      = {J_TKDE},
  author       = {Ming Zhou and Jie Lu and Yiliao Song and Guangquan Zhang},
  doi          = {10.1109/TKDE.2023.3272911},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12828-12841},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-stream concept drift self-adaptation using graph neural network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiplex graph representation learning via dual correlation
reduction. <em>TKDE</em>, <em>35</em>(12), 12814–12827. (<a
href="https://doi.org/10.1109/TKDE.2023.3268069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the superior capacity for analyzing the multiplex graph data, self-supervised multiplex graph representation learning (SMGRL) has received much interest. However, existing SMGRL methods are still limited by the following issues: (i) they generally ignore the noisy information within each graph and the common information among different graphs, thus weakening the effectiveness of SMGRL, and (ii) they conduct negative sample encoding and complex pretext tasks for contrastive learning, thus weakening the efficiency of SMGRL. To solve these issues, in this work, we propose a new framework to conduct effective and efficient SMGRL. Specifically, the proposed method investigates the intra-graph and inter-graph decorrelation losses, respectively, for reducing the impact of noisy information within each graph and capturing the common information among different graphs, to achieve the effectiveness. Moreover, the proposed method does not need negative samples for the SMGRL and designs a simple pretext task, to achieve the efficiency. We further theoretically justify that our method achieves the maximal mutual information instead of directly conducting contrastive learning and theoretically justify that our method actually minimizes the multiplex graph information bottleneck, which guarantees the effectiveness. In addition, an extension for semi-supervised scenarios is proposed to fit the case that a few labels are provided in reality. Extensive experimental results verify the effectiveness and efficiency of the proposed method with respect to various downstream tasks.},
  archive      = {J_TKDE},
  author       = {Yujie Mo and Yuhuan Chen and Yajie Lei and Liang Peng and Xiaoshuang Shi and Changan Yuan and Xiaofeng Zhu},
  doi          = {10.1109/TKDE.2023.3268069},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12814-12827},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multiplex graph representation learning via dual correlation reduction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MSDS: A novel framework for multi-source data selection
based cross-network node classification. <em>TKDE</em>, <em>35</em>(12),
12799–12813. (<a
href="https://doi.org/10.1109/TKDE.2023.3277957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of multi-source cross-network node classification, which aims to classify unlabeled nodes in a target network by leveraging the knowledge learned from the rich labeled nodes in multiple source networks. The existing multi-source transfer learning approaches generally fail to model the structural information of networks, and the current cross-network node classification models mainly neglect that not all source networks can boost the task performance in the target network. Thus, none can be directly applied to the multi-source cross-network node classification task. To this end, in this paper, we propose a novel multi-source data selection (MSDS) based framework for cross-network node classification, which integrates multi-source transfer learning with network embedding to learn label-discriminative and network-invariant node representations. In MSDS, we first propose the multi-source network data selection, which applies three distances to jointly select the transferable source networks to well alleviate the problem of suboptimal solution or even negative transfer. In addition, we devise a new feature information alignment technique to make node vector representations network-invariant. Moreover, we incorporate aggregated structural information and feature information to make node representations label-discriminative. Extensive experiments on real-world datasets demonstrate that the proposed approaches outperform the state-of-the-art non-transfer and single-source transfer approaches in terms of classification accuracy.},
  archive      = {J_TKDE},
  author       = {Hui He and Hongwei Yang and Weizhe Zhang and Yan Wang and Zhaonian Zou and Tao Li},
  doi          = {10.1109/TKDE.2023.3277957},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12799-12813},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MSDS: A novel framework for multi-source data selection based cross-network node classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MINING: Multi-granularity network alignment based on
contrastive learning. <em>TKDE</em>, <em>35</em>(12), 12785–12798. (<a
href="https://doi.org/10.1109/TKDE.2023.3273782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network alignment aims to discover nodes in different networks belonging to the same identity. In recent years, the network alignment problem has aroused significant attentions in both industry and academia. However, the continuous exploding of network data brings two challenges in solving the network alignment problem, i.e., large network scale and scarce labeled data. To bridge this gap, in this paper we propose a novel approach termed as M ulti-granular I ty N etwork al I gnment based on co N trastive learnin G (MINING). Specifically, in MINING, we first design multi-granularity alignment framework to solve the issue of large network scale. Then, we design intra- and inter-network contrastive learning to solve the issue of scarce labeled data. Moreover, we provide theoretical proofs to demonstrate the effectiveness of MINING. Finally, we conduct extensive experiments on the benchmark datasets of Facebook-Twitter, AMiner-LinkedIn and DBpedia $_{\text{ZH}}$ -DBpedia $_{\text{EN}}$ , and results show that MINING can averagely achieve 15.93\% higher $\operatorname{Hits@}k$ and 14.82\% higher $\operatorname{MRR@}k$ compared with the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Zhongbao Zhang and Shuai Gao and Sen Su and Li Sun and Ruiyang Chen},
  doi          = {10.1109/TKDE.2023.3273782},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12785-12798},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MINING: Multi-granularity network alignment based on contrastive learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Matching knowledge graphs in entity embedding spaces: An
experimental study. <em>TKDE</em>, <em>35</em>(12), 12770–12784. (<a
href="https://doi.org/10.1109/TKDE.2023.3272584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity alignment (EA) identifies equivalent entities that locate in different knowledge graphs (KGs), and has attracted growing research interests over the last few years with the advancement of KG embedding techniques. Although a pile of embedding-based EA frameworks have been developed, they mainly focus on improving the performance of entity representation learning , while largely overlook the subsequent stage that matches KGs in entity embedding spaces . Nevertheless, accurately matching entities based on learned entity representations is crucial to the overall alignment performance, as it coordinates individual alignment decisions and determines the global matching result. Hence, it is essential to understand how well existing solutions for matching KGs in entity embedding spaces perform on present benchmarks, as well as their strengths and weaknesses. To this end, in this article we provide a comprehensive survey and evaluation of matching algorithms for KGs in entity embedding spaces in terms of effectiveness and efficiency on both classic settings and new scenarios that better mirror real-life challenges. Based on in-depth analysis, we provide useful insights into the design trade-offs and good paradigms of existing works, and suggest promising directions for future development.},
  archive      = {J_TKDE},
  author       = {Weixin Zeng and Xiang Zhao and Zhen Tan and Jiuyang Tang and Xueqi Cheng},
  doi          = {10.1109/TKDE.2023.3272584},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12770-12784},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Matching knowledge graphs in entity embedding spaces: An experimental study},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interoperability in blockchain: A survey. <em>TKDE</em>,
<em>35</em>(12), 12750–12769. (<a
href="https://doi.org/10.1109/TKDE.2023.3275220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a systematic and comprehensive survey on blockchain interoperability, where interoperability is defined as the ability of blockchains to flexibly transfer assets, share data, and invoke smart contracts across a mix of public, private, and consortium blockchains without any changes to the underlying blockchain systems. Analyzing the vast landscape of both research papers and industry projects, we classify the existing works into five categories, namely, (1) sidechains, (2) notary schemes, (3) hashed time lock contracts (HTLC), (4) relays, and (5) blockchain agnostic protocols. We analyze the existing works under a taxonomy that consists of system and safety characteristics, such as decentralization, direction of communication, locking mechanism, verification mechanism, trust, safety, liveness, and atomicity. Different from other surveys, we are the first to evaluate the performance of some representative interoperability approaches between Bitcoin and Ethereum covering sidechains, notary schemes, and HTLCs. Even though the performance of cross-chain transactions is low (typically fewer than 10 transactions per second), the main reason is the underlying blockchain (e.g., Bitcoin and Ethereum) and not the interoperability approach. Finally, we discuss existing challenges and possible research directions in blockchain interoperability. For example, we identify challenges in interoperability across permissioned and permissionless blockchains, in interacting with scripting blockchains, in security and privacy.},
  archive      = {J_TKDE},
  author       = {Kunpeng Ren and Nhut-Minh Ho and Dumitrel Loghin and Thanh-Toan Nguyen and Beng Chin Ooi and Quang-Trung Ta and Feida Zhu},
  doi          = {10.1109/TKDE.2023.3275220},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12750-12769},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Interoperability in blockchain: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inconsistent matters: A knowledge-guided dual-consistency
network for multi-modal rumor detection. <em>TKDE</em>, <em>35</em>(12),
12736–12749. (<a
href="https://doi.org/10.1109/TKDE.2023.3275586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rumor spreaders are increasingly utilizing multimedia content to attract the attention and trust of news consumers. Though quite a few rumor detection models have exploited the multi-modal data, they seldom consider the inconsistent semantics between images and texts, and rarely spot the inconsistency among the post contents and background knowledge. In addition, they commonly assume the completeness of multiple modalities and thus are incapable of handling handle missing modalities in real-life scenarios. Motivated by the intuition that rumors in social media are more likely to have inconsistent semantics, a novel Knowledge-guided Dual-consistency Network is proposed to detect rumors with multimedia contents. It uses two consistency detection subnetworks to capture the inconsistency at the cross-modal level and the content-knowledge level simultaneously. It also enables robust multi-modal representation learning under different missing visual modality conditions, using a special token to discriminate between posts with visual modality and posts without visual modality. Extensive experiments on three public real-world multimedia datasets demonstrate that our framework can outperform the state-of-the-art baselines under both complete and incomplete modality conditions.},
  archive      = {J_TKDE},
  author       = {Mengzhu Sun and Xi Zhang and Jianqiang Ma and Sihong Xie and Yazheng Liu and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3275586},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12736-12749},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Inconsistent matters: A knowledge-guided dual-consistency network for multi-modal rumor detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving generalizability of graph anomaly detection models
via data augmentation. <em>TKDE</em>, <em>35</em>(12), 12721–12735. (<a
href="https://doi.org/10.1109/TKDE.2023.3271771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph anomaly detection (GAD) has wide applications in real-world networked systems. In many scenarios, people need to identify anomalies on new (sub)graphs, but they may lack labels to train an effective detection model. Since recent semi-supervised GAD methods, which can leverage the available labels as prior knowledge, have achieved superior performance than unsupervised methods, one natural idea is to directly adopt a trained semi-supervised GAD model to the new (sub)graphs for testing. However, we find that existing semi-supervised GAD methods suffer from poor generalization issues, i.e., well-trained models could not perform well on an unseen area (i.e., not accessible in training) of the graph. Motivated by this, we formally define the problem of generalized graph anomaly detection that aims to effectively identify anomalies on both the training-domain graph(s) and the unseen test graph(s). Nevertheless, it is a challenging task since only limited labels are available, and the normal data distribution may differ between training and testing data. Accordingly, we propose a data augmentation method named AugAN ( Aug mentation for A nomaly and N ormal distributions) to enrich training data and adopt a customized episodic training strategy for learning with the augmented data. Extensive experiments verify the effectiveness of AugAN in improving model generalizability.},
  archive      = {J_TKDE},
  author       = {Shuang Zhou and Xiao Huang and Ninghao Liu and Huachi Zhou and Fu-Lai Chung and Long-Kai Huang},
  doi          = {10.1109/TKDE.2023.3271771},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12721-12735},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Improving generalizability of graph anomaly detection models via data augmentation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HANM: Hierarchical additive noise model for many-to-one
causality discovery. <em>TKDE</em>, <em>35</em>(12), 12708–12720. (<a
href="https://doi.org/10.1109/TKDE.2023.3277757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering causal relationships among observed variables is a new research focus in the area of data mining. Methods based on the additive noise model have been proved to be efficient in the identification of cause-effect pairs. However, when trying to determine many-to-one causality, additive noise models often fail to identify the causal direction due to the complex interrelationships and interactions even though the generation of each causal relation follows the additive noise model, and become unreliable in practical applications. In this work, to identify the causal direction, we propose a Hierarchical Additive Noise Model (HANM) to convert many-to-one causality into an approximate one-to-one causality by generalizing multiple factors into an intermediate variable with a variational approach, and use asymmetry in the forward model and backward model of HANM to identify causal direction. Experiments using synthetic data show that many-to-one causality can be effectively identified through asymmetry with our proposed HANM and the accuracy of HANM is higher than the best existing model. By applying the model to real-world data, it can be seen that HANM can greatly augment the application scope of functional causal models for causal discovery.},
  archive      = {J_TKDE},
  author       = {Boxiang Zhao and Shuliang Wang and Lianhua Chi and Chuanfeng Zhao and Hanning Yuan and Qi Li and Xiaojia Liu and Jing Geng and Ye Yuan},
  doi          = {10.1109/TKDE.2023.3277757},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12708-12720},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HANM: Hierarchical additive noise model for many-to-one causality discovery},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GSim: A graph neural network based relevance measure for
heterogeneous graphs. <em>TKDE</em>, <em>35</em>(12), 12693–12707. (<a
href="https://doi.org/10.1109/TKDE.2023.3271425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graphs, which contain nodes and edges of multiple types, are prevalent in various domains, including bibliographic networks, social media, and knowledge graphs. As a fundamental task in analyzing heterogeneous graphs, relevance measure aims to calculate the relevance between two objects of different types, which has been used in many applications such as web search, recommendation, and community detection. Most of existing relevance measures focus on homogeneous networks where objects are of the same type, and a few measures are developed for heterogeneous graphs, but they often need the pre-defined meta-path. Defining meaningful meta-paths requires much domain knowledge, which largely limits their applications, especially on schema-rich heterogeneous graphs like knowledge graphs. Recently, the Graph Neural Network (GNN) has been widely applied in many graph mining tasks, but it has not been applied for measuring relevance yet. To address the aforementioned problems, we propose a novel GNN-based relevance measure, namely GSim. Specifically, we first theoretically analyze and show that GNN is effective for measuring the relevance of nodes in the graph. We then propose a context path-based graph neural network (CP-GNN) to automatically leverage the semantics in heterogeneous graphs. Moreover, we exploit CP-GNN to support relevance measures between two objects of any type. Extensive experiments demonstrate that GSim outperforms existing measures. (Coda and data is available at this link https://github.com/RManLuo/GSim ).},
  archive      = {J_TKDE},
  author       = {Linhao Luo and Yixiang Fang and Moli Lu and Xin Cao and Xiaofeng Zhang and Wenjie Zhang},
  doi          = {10.1109/TKDE.2023.3271425},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12693-12707},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GSim: A graph neural network based relevance measure for heterogeneous graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FLUID: Towards efficient continuous transaction processing
in DAG-based blockchains. <em>TKDE</em>, <em>35</em>(12), 12679–12692.
(<a href="https://doi.org/10.1109/TKDE.2023.3272312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most blockchain-based application scenarios, a complete application logic consists of multiple continuous transactions, in which the initiation of one transaction depends on the confirmation result of the previous one. This mandates that continuous transactions must be processed in the correct order. Unfortunately, existing chain-based blockchains fail to effectively support continuous transaction processing due to considerable latency in confirming continuous transactions. Recent studies shifted from chain-based blockchains to Directed Acyclic Graph (DAG) based blockchains, which reduced transaction confirmation latencies. However, DAG-based blockchains store transactions in an out-of-order manner that leads to unordered transaction processing. To address this challenge, we propose FLUID, a new DAG-based blockchain that supports continuous transaction processing while delivering high performance. The fundamental idea of FLUID is to design a transaction dependency tracking structure to ensure that continuous transactions can be processed in the correct order. FLUID utilizes a conflict resolution mechanism to provide instant confirmation and to support concurrent transaction processing with lower latencies. In addition, FLUID builds a checkpoint-based verification mechanism to achieve deterministic consensus on transaction processing results in the DAG. Extensive experiments demonstrate that our proposed FLUID can improve the throughput over state-of-the-art OHIE by 66\% with two orders of magnitude lower latencies.},
  archive      = {J_TKDE},
  author       = {Junpei Ni and Jiang Xiao and Shijie Zhang and Bo Li and Baochun Li and Hai Jin},
  doi          = {10.1109/TKDE.2023.3272312},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12679-12692},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FLUID: Towards efficient continuous transaction processing in DAG-based blockchains},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Finding materialized models for model reuse. <em>TKDE</em>,
<em>35</em>(12), 12663–12678. (<a
href="https://doi.org/10.1109/TKDE.2023.3270923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Materialized model query aims to find the most appropriate materialized model as the initial model for model reuse. It is the precondition of model reuse, and has recently attracted much attention. Nonetheless, the existing methods suffer from the need to provide source data, limited range of applications, and inefficiency since they do not construct a suitable metric to measure the target-related knowledge of materialized models. To address this, we present ${\sf MMQ}$ , a source-data free, general, efficient, and effective materialized model query framework. It uses a Gaussian mixture-based metric called separation degree to rank materialized models. For each materialized model, ${\sf MMQ}$ first vectorizes the samples in the target dataset into probability vectors by directly applying this model, then utilizes Gaussian distribution to fit for each class of probability vectors, and finally uses separation degree on the Gaussian distributions to measure the target-related knowledge of the materialized model. Moreover, we propose an improved ${\sf MMQ}$ ( ${\sf I\text{-}MMQ}$ ), which significantly reduces the query time while retaining the query performance of ${\sf MMQ}$ . Extensive experiments on a range of practical model reuse workloads demonstrate the effectiveness and efficiency of ${\sf MMQ}$ .},
  archive      = {J_TKDE},
  author       = {Minjun Zhao and Lu Chen and Keyu Yang and Yuntao Du and Yunjun Gao},
  doi          = {10.1109/TKDE.2023.3270923},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12663-12678},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Finding materialized models for model reuse},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Filter transfer learning algorithm for missing data
imputation in wastewater treatment process. <em>TKDE</em>,
<em>35</em>(12), 12649–12662. (<a
href="https://doi.org/10.1109/TKDE.2023.3270118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data imputation is a critical data processing procedure in wastewater treatment process. However, the existing imputation methods cannot stand the missing data with high proportions that frequently happens due to unmaintained instruments or detection failures. Transfer Learning aims to learn much reliable information for the target domain with previous learned knowledge from a source domain, which provides a framework for solving such problem. This paper proposes a filter transfer learning algorithm (FTLA) for missing data imputation with high proportions. First, a knowledge acquisition strategy is developed to extract the source knowledge, including independent knowledge from historical datasets and parallel knowledge in terms of related datasets. The missing data is then interpreted through source knowledge comprehensively. Second, a filter transfer learning algorithm is designed to achieve target knowledge that mimics the tendency of the missing data. This algorithm can avoid serious negative transfer by using the extended Kalman filter to filtrate source knowledge. Third, a knowledge rolling mechanism is established to perform the imputation online with target knowledge, which can maintain the reliable imputation for missing data with high proportions. Finally, several comparative experiments of wastewater data are provided to demonstrate the merits of missing data imputation with FTLA.},
  archive      = {J_TKDE},
  author       = {Honggui Han and Mengmeng Li and Junfei Qiao and Qing Yang and Yongzhen Peng},
  doi          = {10.1109/TKDE.2023.3270118},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12649-12662},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Filter transfer learning algorithm for missing data imputation in wastewater treatment process},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient maximal biclique enumeration on large uncertain
bipartite graphs. <em>TKDE</em>, <em>35</em>(12), 12634–12648. (<a
href="https://doi.org/10.1109/TKDE.2023.3272110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the problem of maximal biclique enumeration on large uncertain bipartite graphs. Given an uncertain bipartite graph $\mathcal {G}=(U,V,E,p)$ , a probability threshold $\tau$ , and two size constraints $\alpha$ and $\beta$ , we aim to efficiently enumerate all maximal $\tau$ -bicliques in $\mathcal {G}$ , where a maximal $\tau$ -biclique $B(L,R)$ is a complete subgraph of $\mathcal {G}$ with (1) the probability of $B$ is no less than $\tau$ , (2) $|L| \geq \alpha$ and $|R| \geq \beta$ , and (3) $B$ is a maximal complete subgraph satisfying (1) and (2). This problem has many applications, such as biclustering of gene expression data, fraud detection, similar group identification, etc. Despite the wide range of applications, to the best of our knowledge, we note that there are no efficient and scalable solutions to this problem in the literature. This problem is computationally challenging due to its #P-completeness. In this article, we propose a competitive branch-and-bound method, namely ${\sf MBEN}$ , which explores the search space in a depth-first manner with a variety of pruning techniques. To improve the performance of ${\sf MBEN}$ , we propose several novel and efficient search processing optimizations. First, we always select the side with fewer candidates to expand the search space. With this search strategy, we have a chance to prune the fruitless branches early. Second, we devise an advanced pruning technique by considering the size pruning and probability pruning at the same time to boost the pruning capacity. Last, we implement ${\sf MBEN}$ with pre-allocated arrays and pointer maintaining techniques such that the frequent work sets creating operations can be substituted by array element switching operations. In addition, we introduce useful graph reduction techniques to further accelerate the computation. Comprehensive performance studies on 10 real datasets demonstrate that our proposals can significantly outperform the baseline methods by more than two orders of magnitude.},
  archive      = {J_TKDE},
  author       = {Jianhua Wang and Jianye Yang and Ziyi Ma and Chengyuan Zhang and Shiyu Yang and Wenjie Zhang},
  doi          = {10.1109/TKDE.2023.3272110},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12634-12648},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient maximal biclique enumeration on large uncertain bipartite graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficiently supporting multi-level serializability in
decentralized database systems. <em>TKDE</em>, <em>35</em>(12),
12618–12633. (<a
href="https://doi.org/10.1109/TKDE.2023.3277969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In decentralized database systems, it is reported that serializability could still produce unexpected transaction orderings, leading to the stale read anomaly. To eliminate this anomaly, strict serializability imposes an additional ordering constraint, called the real-time order, which is required to be preserved among serializable transactions. Yet, preserving the real-time order in strict serializability often causes the performance to drop significantly. Because a weaker data consistency often yields better performance, in this paper, we model serializability from different consistency perspectives to properly leverage the performance and consistency. To do this, we first define a group of orderings, based on which we formulate multi-level serializability by preserving a certain set of ordering constraints among transactions. We then propose a bidirectional timestamp adjustment algorithm (abbreviated as BDTA) to support multi-level serializability with various optimizations. Our special design makes ordering constraints among transactions be preserved simply by adjusting timestamp intervals. Finally, we conduct extensive experiments to show the necessity of introducing multi-level serializability and confirm that BDTA achieves up to 1.19 × better performance than the state-of-the-art concurrency control algorithms.},
  archive      = {J_TKDE},
  author       = {Zhanhao Zhao and Hongyao Zhao and Qiyu Zhuang and Wei Lu and Haixiang Li and Meihui Zhang and Anqun Pan and Xiaoyong Du},
  doi          = {10.1109/TKDE.2023.3277969},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12618-12633},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficiently supporting multi-level serializability in decentralized database systems},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual consistency-enhanced semi-supervised sentiment analysis
towards COVID-19 tweets. <em>TKDE</em>, <em>35</em>(12), 12605–12617.
(<a href="https://doi.org/10.1109/TKDE.2023.3270940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of COVID-19, numerous people present their opinions through social networks. It is thus highly desired to conduct sentiment analysis towards COVID-19 tweets to learn the public&#39;s attitudes, and facilitate the government to make proper guidelines for avoiding the social unrest. Although many efforts have studied the text-based sentiment classification from various domains (e.g., delivery and shopping reviews), it is hard to directly use these classifiers for the sentiment analysis towards COVID-19 tweets due to the domain gap. In fact, developing the sentiment classifier for COVID-19 tweets is mainly challenged by the limited annotated training dataset, as well as the diverse and informal expressions of user-generated posts. To address these challenges, we construct a large-scale COVID-19 dataset from Weibo and propose a dual COnsistency-enhanced semi-superVIseD network for Sentiment Anlaysis (COVID-SA). In particular, we first introduce a knowledge-based augmentation method to augment data and enhance the model&#39;s robustness. We then employ BERT as the text encoder backbone for both labeled data, unlabeled data, and augmented data. Moreover, we propose a dual consistency (i.e., label-oriented consistency and instance-oriented consistency) regularization to promote the model performance. Extensive experiments on our self-constructed dataset and three public datasets show the superiority of COVID-SA over state-of-the-art baselines on various applications.},
  archive      = {J_TKDE},
  author       = {Teng Sun and Liqiang Jing and Yinwei Wei and Xuemeng Song and Zhiyong Cheng and Liqiang Nie},
  doi          = {10.1109/TKDE.2023.3270940},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12605-12617},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual consistency-enhanced semi-supervised sentiment analysis towards COVID-19 tweets},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep isolation forest for anomaly detection. <em>TKDE</em>,
<em>35</em>(12), 12591–12604. (<a
href="https://doi.org/10.1109/TKDE.2023.3270293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Isolation forest (iForest) has been emerging as arguably the most popular anomaly detector in recent years due to its general effectiveness across different benchmarks and strong scalability. Nevertheless, its linear axis-parallel isolation method often leads to (i) failure in detecting hard anomalies that are difficult to isolate in high-dimensional/non-linear-separable data space, and (ii) notorious algorithmic bias that assigns unexpectedly lower anomaly scores to artefact regions. These issues contribute to high false negative errors. Several iForest extensions are introduced, but they essentially still employ shallow, linear data partition, restricting their power in isolating true anomalies. Therefore, this paper proposes deep isolation forest. We introduce a new representation scheme that utilises casually initialised neural networks to map original data into random representation ensembles, where random axis-parallel cuts are subsequently applied to perform the data partition. This representation scheme facilitates high freedom of the partition in the original data space (equivalent to non-linear partition on subspaces of varying sizes), encouraging a unique synergy between random representations and random partition-based isolation. Extensive experiments show that our model achieves significant improvement over state-of-the-art isolation-based methods and deep detectors on tabular, graph and time series datasets; our model also inherits desired scalability from iForest.},
  archive      = {J_TKDE},
  author       = {Hongzuo Xu and Guansong Pang and Yijie Wang and Yongjun Wang},
  doi          = {10.1109/TKDE.2023.3270293},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12591-12604},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep isolation forest for anomaly detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data lakes: A survey of functions and systems.
<em>TKDE</em>, <em>35</em>(12), 12571–12590. (<a
href="https://doi.org/10.1109/TKDE.2023.3270101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data lakes are becoming increasingly prevalent for Big Data management and data analytics. In contrast to traditional ‘schema-on-write’ approaches such as data warehouses, data lakes are repositories storing raw data in its original formats and providing a common access interface. Despite the strong interest raised from both academia and industry, there is a large body of ambiguity regarding the definition, functions and available technologies for data lakes. A complete, coherent picture of data lake challenges and solutions is still missing. This survey reviews the development, architectures, and systems of data lakes. We provide a comprehensive overview of research questions for designing and building data lakes. We classify the existing approaches and systems based on their provided functions for data lakes, which makes this survey a useful technical reference for designing, implementing and deploying data lakes. We hope that the thorough comparison of existing solutions and the discussion of open research challenges in this survey will motivate the future development of data lake research and practice.},
  archive      = {J_TKDE},
  author       = {Rihan Hai and Christos Koutras and Christoph Quix and Matthias Jarke},
  doi          = {10.1109/TKDE.2023.3270101},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12571-12590},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Data lakes: A survey of functions and systems},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-graph embedding with trainable proximity for graph
alignment. <em>TKDE</em>, <em>35</em>(12), 12556–12570. (<a
href="https://doi.org/10.1109/TKDE.2023.3270119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph alignment, also known as network alignment, has many applications in data mining tasks. It aims to find the node correspondence across disjoint graphs. With recent representation learning advancements, embedding-based graph alignment has become a hot topic. Existing embedding-based methods focus either on structural proximity across graphs or on the positional proximity within a single graph. However, only considering the structural similarity will make the position relation of nodes not clear enough, which makes it easy to misalign the nodes close in distance, while only considering the position proximity of a single graph will make the node embeddings from different graphs in different subspaces. To mitigate this issue, we propose a novel model CEGA for C ross-graph E mbedding-based G raph A lignment, which can generate node embeddings to reflect structural proximity and positional proximity simultaneously. Meanwhile, we make the proximity trainable thus it can be learned to best suit the alignment task at hand automatically. We show that CEGA outperforms existing graph alignment methods in accuracy under unsupervised scenarios through extensive experiments on public benchmarks.},
  archive      = {J_TKDE},
  author       = {Wei Tang and Haifeng Sun and Jingyu Wang and Qi Qi and Huangxun Chen and Li Chen},
  doi          = {10.1109/TKDE.2023.3270119},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12556-12570},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-graph embedding with trainable proximity for graph alignment},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COLTR: Semi-supervised learning to rank with co-training and
over-parameterization for web search. <em>TKDE</em>, <em>35</em>(12),
12542–12555. (<a
href="https://doi.org/10.1109/TKDE.2023.3270750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While learning to rank (LTR) has been widely used in web search to prioritize most relevant webpages among the retrieved contents subject to the input queries, the traditional LTR models fail to deliver decent performance due to two main reasons: 1) the lack of well-annotated query-webpage pairs with ranking scores to cover search queries of various popularity, and 2) ill-trained models based on a limited number of training samples with poor generalization performance. To improve the performance of LTR models, tremendous efforts have been done from above two aspects, such as enlarging training sets with pseudo-labels of ranking scores by self-training, or refining the features used for LTR through feature extraction and dimension reduction. Though LTR performance has been marginally increased, we still believe these methods could be further improved in the newly-fashioned “interpolating regime”. Specifically, instead of lowering the number of features used for LTR models, our work proposes to transform original data with random Fourier feature, so as to over-parameterize the downstream LTR models (e.g., GBRank or LightGBM) with features in ultra-high dimensionality and achieve superb generalization performance. Furthermore, rather than self-training with pseudo-labels produced by the same LTR model in a “self-tuned” fashion, the proposed method incorporates the diversity of prediction results between the listwise and pointwise LTR models while co-training both models with a cyclic labeling-prediction pipeline in a “ping-pong” manner. We deploy the proposed Co-trained and Over-parameterized LTR system COLTR at Baidu search and evaluate COLTR with a large number of baseline methods. The results show that COLTR could achieve $\Delta NDCG_{4}$ = 3.64\% $\sim$ 4.92\%, compared to baselines, under various ratios of labeled samples. We also conduct a 7-day A/B Test using the realistic web traffics of Baidu Search, where we can still observe significant performance improvement around $\Delta NDCG_{4}$ = 0.17\% $\sim$ 0.92\% in real-world applications. COLTR performs consistently both in online and offline experiments.},
  archive      = {J_TKDE},
  author       = {Yuchen Li and Haoyi Xiong and Qingzhong Wang and Linghe Kong and Hao Liu and Haifang Li and Jiang Bian and Shuaiqiang Wang and Guihai Chen and Dejing Dou and Dawei Yin},
  doi          = {10.1109/TKDE.2023.3270750},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12542-12555},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {COLTR: Semi-supervised learning to rank with co-training and over-parameterization for web search},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative semantic aggregation and calibration for
federated domain generalization. <em>TKDE</em>, <em>35</em>(12),
12528–12541. (<a
href="https://doi.org/10.1109/TKDE.2023.3271851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains. The existing DG methods usually exploit the fusion of shared multi-source data to train a generalizable model. However, tremendous data is distributed across lots of places nowadays that can not be shared due to privacy policies. In this paper, we tackle the problem of federated domain generalization where the source datasets can only be accessed and learned locally for privacy protection. We propose a novel framework called Collaborative Semantic Aggregation and Calibration (CSAC) to enable this challenging problem. To fully absorb multi-source semantic information while avoiding unsafe data fusion, we conduct data-free semantic aggregation by fusing the models trained on the separated domains layer-by-layer. To address the semantic dislocation problem caused by domain shift, we further design cross-layer semantic calibration with an attention mechanism to align each semantic level and enhance domain invariance. We unify multi-source semantic learning and alignment in a collaborative way by repeating the semantic aggregation and calibration alternately, keeping each dataset localized, and the data privacy is carefully protected. Extensive experiments show the significant performance of our method in addressing this challenging problem.},
  archive      = {J_TKDE},
  author       = {Junkun Yuan and Xu Ma and Defang Chen and Fei Wu and Lanfen Lin and Kun Kuang},
  doi          = {10.1109/TKDE.2023.3271851},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12528-12541},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Collaborative semantic aggregation and calibration for federated domain generalization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clique identification in signed graphs: A balance theory
based model. <em>TKDE</em>, <em>35</em>(12), 12513–12527. (<a
href="https://doi.org/10.1109/TKDE.2023.3272636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clique, as a fundamental model for graph analysis, is widely investigated in the literature. However, with the emergence of various graph data, such as signed graph, novel clique model is desired to better capture the cohesiveness within these graphs. Different from unsigned graphs, where only one type of edge exists, in signed graphs, nodes can be connected either positively or negatively (e.g., friend or enemy). In this article, we propose a novel clique model, called signed $k$ -clique, which aims to find cohesive subgraphs in signed networks based on the classic clique model and balance theory. Given a signed graph $G$ , an induced subgraph $S$ is a signed $k$ -clique if $|S| \geq k$ and $S$ is a clique without any unbalanced triangle. Moreover, we propose and investigate two fundamental problems, i.e., maximal signed $k$ -clique enumeration and maximum signed $k$ -clique identification, both of which are shown to be NP-hard. For maximal signed $k$ -clique enumeration, novel balance graph based search framework and optimization techniques are proposed to eliminate the limitations in the developed baseline. For maximum signed $k$ -clique identification, different upper bound based techniques are developed to early terminate the search. Furthermore, the support of finding top- $\gamma$ results is also discussed. Finally, comprehensive experiments on seven real-world datasets are conducted to demonstrate the efficiency and effectiveness of the proposed techniques. Compared with the baseline, the optimized algorithm can achieve up to four orders of magnitude speedup.},
  archive      = {J_TKDE},
  author       = {Renjie Sun and Yanping Wu and Xiaoyang Wang and Chen Chen and Wenjie Zhang and Xuemin Lin},
  doi          = {10.1109/TKDE.2023.3272636},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12513-12527},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Clique identification in signed graphs: A balance theory based model},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Channel attention for sensor-based activity recognition:
Embedding features into all frequencies in DCT domain. <em>TKDE</em>,
<em>35</em>(12), 12497–12512. (<a
href="https://doi.org/10.1109/TKDE.2023.3277839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During recent years, channel attention has attracted great interest in deep learning community. Despite significant success, it has been rarely exploited in ubiquitous human activity recognition (HAR) scenario. To decrease computational overhead, the channel attention often uses global averaging pooling (GAP) to compress each channel into a simple scalar. It is well known that GAP is equal to the lowest frequency component. Despite obvious lightweight advantage, such compression process inevitably causes severe information loss. In this paper, we propose a novel multi-frequency channel attention framework for activity recognition tasks. Considering various sensing frequencies of human activities, an intuition solution is to convert the time series from time domain to frequency domain. Instead of GAP, the discrete cosine transform (DCT) is used to compress channels. We prove that GAP can be seen as a special case of DCT, which uses the lowest frequency component only and leaves out all other frequency components unused. DCT is able to better compress channels by fully exploiting other frequency components discarded by GAP. Despite multiple frequency components used, each channel will still be represented by a scalar in order to maintain the same computational overhead. Using two frequency screening criteria, our method is able to achieve state-of-the-art results on four benchmark HAR datasets. Extensive ablation studies are conducted, which provides a better interpretability of deep model behaviors. Finally, actual inference is evaluated on an embedded platform.},
  archive      = {J_TKDE},
  author       = {Shige Xu and Lei Zhang and Yin Tang and Chaolei Han and Hao Wu and Aiguo Song},
  doi          = {10.1109/TKDE.2023.3277839},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12497-12512},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Channel attention for sensor-based activity recognition: Embedding features into all frequencies in DCT domain},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Balanced and unbalanced triangle count in signed networks.
<em>TKDE</em>, <em>35</em>(12), 12491–12496. (<a
href="https://doi.org/10.1109/TKDE.2023.3272657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle count is a frequently used network statistic, possessing high computational cost. Moreover, this task gets even more complex in the case of signed networks which consist of unbalanced and balanced triangles. In this work, we propose a fast I ncremental T riangle C ounting (ITC) algorithm for counting all types of triangles, including balanced and unbalanced. The proposed algorithm updates the count of different types of triangles for newly added nodes and edges only instead of recalculating the same triangle multiple times for the entire network repeatedly. Thus, the proposed ITC algorithm also works for dynamic networks. The experimental results show that the proposed method is practically efficient having run time complexity of $O(m k_{{\max}})$ , where $m$ represents the number of edges and $k_{{\max}}$ represents the maximum degree of the given signed network.},
  archive      = {J_TKDE},
  author       = {Aikta Arya and Pradumn Kumar Pandey and Akrati Saxena},
  doi          = {10.1109/TKDE.2023.3272657},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12491-12496},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Balanced and unbalanced triangle count in signed networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Automatic database knob tuning: A survey. <em>TKDE</em>,
<em>35</em>(12), 12470–12490. (<a
href="https://doi.org/10.1109/TKDE.2023.3266893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knob tuning plays an important role in database optimization, which tunes knob settings to optimize the database performance or improve resource utilization. However, there are several common challenges in knob tuning. First, databases have hundreds of configuration knobs, and it is hard to determine the knobs that cause the performance/resource bottlenecks. Second, most knobs are of continuous values and cause large search space, where heuristic knob tuning may not find high-performance knob settings within limited time. Third, it is increasingly tricky to conduct knob tuning with the proliferation of cloud services, where we need to tune a large number of database instances for various scenarios (e.g., different applications, datasets, and hardware). Recently, many learning-based knob tuning methods are proposed to alleviate those problems. The core idea of learning-based knob tuning is that, with the help of machine learning techniques, it is reasonable to collect knob tuning data, leverage these data to train a knob tuning model, and utilize the tuning model to recommend knob settings for new similar scenarios, so as to achieve the optimization objectives. In this paper, we provide a comprehensive survey on database knob tuning. The pipeline of knob tuning includes knob selection , feature selection , tuning methods , and transfer techniques . First, for knob selection , we introduce the main categories of database knobs and summarize existing knob selection algorithms. Second, for feature selection , we introduce commonly-used tuning features and explain existing feature selection techniques (e.g., runtime metric selection and workload encoding). Third, for tuning methods , we compare four classes of tuning methods, i.e., heuristic methods, Bayesian-optimization methods, deep-learning methods, and reinforcement-learning methods. In particular, we summarize the challenges and discuss how existing methods address those challenges. Moreover, we discuss some transfer techniques that utilize historically well-trained tuning models in new scenarios. Fourth, we discuss the implementation of automatic knob tuning methods in typical systems (e.g., commercial relational databases and Big Data analytics systems). Lastly, we provide some research challenges and future research opportunities. We believe this survey can help researchers better understand the knob tuning problems and existing approaches and further encourage them to solve the remaining problems in automatic knob tuning.},
  archive      = {J_TKDE},
  author       = {Xinyang Zhao and Xuanhe Zhou and Guoliang Li},
  doi          = {10.1109/TKDE.2023.3266893},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12470-12490},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Automatic database knob tuning: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic context pattern generation for entity set
expansion. <em>TKDE</em>, <em>35</em>(12), 12458–12469. (<a
href="https://doi.org/10.1109/TKDE.2023.3275211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity Set Expansion (ESE) is a valuable task that aims to find entities of the target semantic class described by given seed entities. Various Natural Language Processing (NLP) and Information Retrieval (IR) downstream applications have benefited from ESE due to its ability to discover knowledge. Although existing corpus-based ESE methods have achieved great progress, they still rely on corpora with high-quality entity information annotated, because most of them need to obtain the context patterns through the position of the entity in a sentence. Therefore, the quality of the given corpora and their entity annotation has become the bottleneck that limits the performance of such methods. To overcome this dilemma and make the ESE models free from the dependence on entity annotation, our work aims to explore a new ESE paradigm, namely corpus-independent ESE. Specifically, we devise a context pattern generation module that utilizes autoregressive language models (e.g., GPT-2) to automatically generate high-quality context patterns for entities. In addition, we propose the GAPA, a novel ESE framework that leverages the aforementioned G ener A ted PA tterns to expand target entities. Extensive experiments and detailed analyses on three widely used datasets demonstrate the effectiveness of our method. All the codes of our experiments are available at https://github.com/geekjuruo/GAPA .},
  archive      = {J_TKDE},
  author       = {Yinghui Li and Shulin Huang and Xinwei Zhang and Qingyu Zhou and Yangning Li and Ruiyang Liu and Yunbo Cao and Hai-Tao Zheng and Ying Shen},
  doi          = {10.1109/TKDE.2023.3275211},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12458-12469},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Automatic context pattern generation for entity set expansion},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anti-money laundering by group-aware deep graph learning.
<em>TKDE</em>, <em>35</em>(12), 12444–12457. (<a
href="https://doi.org/10.1109/TKDE.2023.3272396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anti-money laundering (AML) is a classical data mining problem in finance applications. As well known, money laundering (ML) is critical to the effective operation of transnational and organized crime, which affects a country&#39;s economy, government, and social wellbeings. Financial services organizations facilitate the movement of money and have been enlisted by governments to assist with the detection and prevention of money laundering, which is a key tool in the fight to reduce crime and create sustainable economic development. In the application of AML, user identity and financial behavior data are widely used to detect laundering transactions. In recent years, an increasing number of money laundering activities have been conducted by organized criminal gangs while most existing works still treat the actions of each account as independent identity behavior without considering the group-level conspired interactions. Therefore, in this paper, we propose a group-aware deep graph learning-based approach for organized money-laundering detection. In particular, we design a community-centric encoder to represent the nodes and attributes in user transaction graphs and derive the adjacent gang behaviors. Then, we devise a scheme of local enhancement to accommodate nodes with similar transaction features, which are aggregated into gangs for downstream detection. Extensive experiments on the real-world dataset from one of the largest bank card alliances worldwide show that our proposed method outperforms state-of-the-art methods in both offline and online modes, showing the effectiveness of money laundering detection with group-aware deep graph learning.},
  archive      = {J_TKDE},
  author       = {Dawei Cheng and Yujia Ye and Sheng Xiang and Zhenwei Ma and Ying Zhang and Changjun Jiang},
  doi          = {10.1109/TKDE.2023.3272396},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12444-12457},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Anti-money laundering by group-aware deep graph learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new approach for semi-external topological sorting on big
graphs. <em>TKDE</em>, <em>35</em>(12), 12430–12443. (<a
href="https://doi.org/10.1109/TKDE.2023.3274528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new approach for semi-external topological sorting algorithm on big directed acyclic graph(DAG). Topological sorting aims to find an ordering of each node in DAG, which satisfies $u$ precedes $v$ in the ordering for each edge $(u,v)$ in DAG. Topological sorting is an important subroutine for scheduling and other external graph algorithms. But, the internal topological sorting algorithm cannot handle big DAGs and the I/O complexity of total external topological sorting is too high for practical applications. Therefore, we pay attention to the semi-external topological sorting for big DAGs in this paper. We find that the existing semi-external topological sorting algorithm is mainly based on constructing a DFS-Tree in internal memory. However, this DFS-based algorithm is natively more difficult than topological sorting, because DFS-Tree determines a strict total order, while topological order is only a partial order. Therefore, a partial order level order is proposed in this paper. Based on the level order , we propose a new semi-external topological sorting algorithm. Next, two optimizations, NodeRemove and EdgeRemove , are proposed to reduce the CPU and I/O cost. In addition, we also propose a batch algorithm. Finally, we perform experimental studies using real and synthetic datasets to confirm the efficiency of our approach. According to the experimental results, our algorithms are better than the previous DFS-based algorithms.},
  archive      = {J_TKDE},
  author       = {Tianpeng Gao and Jianzhong Li and Hengzhao Ma},
  doi          = {10.1109/TKDE.2023.3274528},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12430-12443},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A new approach for semi-external topological sorting on big graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial attacks for black-box recommender systems via
copying transferable cross-domain user profiles. <em>TKDE</em>,
<em>35</em>(12), 12415–12429. (<a
href="https://doi.org/10.1109/TKDE.2023.3272652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As widely used in data-driven decision-making, recommender systems have been recognized for their capabilities to provide users with personalized services in many user-oriented online services, such as E-commerce (e.g., Amazon, Taobao, etc.) and Social Media sites (e.g., Facebook and Twitter). Recent works have shown that deep neural networks-based recommender systems are highly vulnerable to adversarial attacks, where adversaries can inject carefully crafted fake user profiles (i.e., a set of items that fake users have interacted with) into a target recommender system to promote or demote a set of target items. Instead of generating users with fake profiles from scratch, in this article, we introduce a novel strategy to obtain “fake” user profiles via copying cross-domain user profiles, where a reinforcement learning based black-box attacking framework (CopyAttack+) is developed to effectively and efficiently select cross-domain user profiles from the source domain to attack the target system. Moreover, we propose to train a local surrogate system for mimicking adversarial black-box attacks in the source domain, so as to provide transferable signals with the purpose of enhancing the attacking strategy in the target black-box recommender system. Comprehensive experiments on three real-world datasets are conducted to demonstrate the effectiveness of the proposed attacking framework.},
  archive      = {J_TKDE},
  author       = {Wenqi Fan and Xiangyu Zhao and Qing Li and Tyler Derr and Yao Ma and Hui Liu and Jianping Wang and Jiliang Tang},
  doi          = {10.1109/TKDE.2023.3272652},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12415-12429},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adversarial attacks for black-box recommender systems via copying transferable cross-domain user profiles},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Additive feature attribution explainable methods to craft
adversarial attacks for text classification and text regression.
<em>TKDE</em>, <em>35</em>(12), 12400–12414. (<a
href="https://doi.org/10.1109/TKDE.2023.3270581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) models have significantly improved the performance of text classification and text regression tasks. However, DL models are often strikingly vulnerable to adversarial attacks. Many researchers have aimed to develop adversarial attacks against DL models in realistic black-box settings (i.e., assuming no model knowledge is accessible to attackers). These attacks typically operate with a two-phase framework: (1) sensitivity estimation through gradient-based or deletion-based methods to evaluate the sensitivity of each token to the prediction of the target model, and (2) perturbation execution to craft adversarial examples based on the estimated token sensitivity. However, gradient-based and deletion-based methods used to estimate sensitivity often face issues of capturing token directionality and overlapping token sensitivities, respectively. In this study, we propose a novel eXplanation-based method for Adversarial Text Attacks (XATA) that leverages additive feature attribution explainable methods, namely LIME or SHAP, to measure the sensitivity of input tokens when crafting black-box adversarial attacks on DL models performing text classification or text regression. We evaluated XATA&#39;s attack performance on DL models executing text classification on the IMDB Movie Review, Yelp Reviews-Polarity, and Amazon Reviews-Polarity datasets and DL models conducting text regression on the My Personality, Drug Review, and CommonLit Readability datasets. The proposed XATA outperformed the existing gradient-based and deletion-based adversarial attack baselines in both tasks. These findings indicate that the ever-growing research focused on improving the explainability of DL models with additive feature attribution explainable methods can provide attackers with weapons to launch targeted adversarial attacks.},
  archive      = {J_TKDE},
  author       = {Yidong Chai and Ruicheng Liang and Sagar Samtani and Hongyi Zhu and Meng Wang and Yezheng Liu and Yuanchun Jiang},
  doi          = {10.1109/TKDE.2023.3270581},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12400-12414},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Additive feature attribution explainable methods to craft adversarial attacks for text classification and text regression},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). Adaptive graph convolution methods for attributed graph
clustering. <em>TKDE</em>, <em>35</em>(12), 12384–12399. (<a
href="https://doi.org/10.1109/TKDE.2023.3278721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph clustering is a challenging task as it requires to jointly model graph structure and node attributes. Although recent advances in graph convolutional networks have shown the effectiveness of graph convolution in combining structural and content information, there is limited understanding of how to properly apply it for attributed graph clustering. Previous methods commonly use a fixed and low order graph convolution, which only aggregates information of few-hop neighbours and hence cannot fully capture the cluster structures of diverse graphs. In this paper, we first propose an adaptive graph convolution method (AGC) for attributed graph clustering, which exploits high-order graph convolutions to capture global cluster structures and adaptively selects an appropriate order $k$ via intra-cluster distance. While AGC can find a reasonable $k$ and avoid over-smoothing, it is not sensitive to the gradual decline of clustering performance as $k$ increases. To search for a better $k$ , we further propose an improved adaptive graph convolution method (IAGC) that not only observes the variation of intra-cluster distance, but also considers the inconsistencies of filtered features with graph structure and raw features, respectively. We establish the validity of our methods by theoretical analysis and extensive experiments on various benchmark datasets.},
  archive      = {J_TKDE},
  author       = {Xiaotong Zhang and Han Liu and Qimai Li and Xiao-Ming Wu and Xianchao Zhang},
  doi          = {10.1109/TKDE.2023.3278721},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12384-12399},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive graph convolution methods for attributed graph clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive ensemble clustering with boosting BLS-based
autoencoder. <em>TKDE</em>, <em>35</em>(12), 12369–12383. (<a
href="https://doi.org/10.1109/TKDE.2023.3271120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble clustering has an advantage in producing a more promising and robust clustering result by combining multiple partitions strategically. The quality of both base partitions and co-association matrix plays an essential role in improving the consensus partition. However, the current ensemble clustering methods have several limitations: 1) The noise in high-dimensional feature space is ignored; 2) The independent base partition generation process does not pay attention to ambiguous samples; 3) The co-association matrix and the weights of base partitions commonly lack of theoretical optimization. In order to address these issues, we propose an adaptive ensemble clustering framework with boosting BLS-based autoencoder (BoostAEC). In the generation step, a boosting BLS-based autoencoder (BoostBLSAE) is designed to generate base partitions sequentially, which learns compressed feature subspaces for ambiguous samples and adaptively evaluates the corresponding weights of reliability. In the integration step, we construct a fuzzy membership function to capture the inter-cluster correlation and explicitly propose a consensus objective function to optimize the unified co-association matrix by considering the weighted base partitions. Extensive experiments on various real-world datasets demonstrate the superior performance of BoostAEC to the state-of-the-art ensemble clustering methods.},
  archive      = {J_TKDE},
  author       = {Yifan Shi and Kaixiang Yang and Zhiwen Yu and C. L. Philip Chen and Huanqiang Zeng},
  doi          = {10.1109/TKDE.2023.3271120},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12369-12383},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive ensemble clustering with boosting BLS-based autoencoder},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive survey on multi-view clustering.
<em>TKDE</em>, <em>35</em>(12), 12350–12368. (<a
href="https://doi.org/10.1109/TKDE.2023.3270311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of information gathering and extraction technology has led to the popularity of multi-view data, which enables samples to be seen from numerous perspectives. Multi-view clustering (MVC), which groups data samples by leveraging complementary and consensual information from several views, is gaining popularity. Despite the rapid evolution of MVC approaches, there has yet to be a study that provides a full MVC roadmap for both stimulating technical improvements and orienting research newbies to MVC. In this article, we review recent MVC techniques with the purpose of exhibiting the concepts of popular methodologies and their advancements. This survey not only serves as a unique MVC comprehensive knowledge for researchers but also has the potential to spark new ideas in MVC research. We summarise a large variety of current MVC approaches based on two technical mechanisms: heuristic-based multi-view clustering (HMVC) and neural network-based multi-view clustering (NNMVC). We end with four technological approaches within the category of HMVC: nonnegative matrix factorisation, graph learning, latent representation learning, and tensor learning. Deep representation learning and deep graph learning are two technical methods that we demonstrate in NNMVC. We also show 15 publicly available multi-view datasets and examine how representative MVC approaches perform on them. In addition, this study identifies the potential research directions that may require further investigation in order to enhance the further development of MVC.},
  archive      = {J_TKDE},
  author       = {Uno Fang and Man Li and Jianxin Li and Longxiang Gao and Tao Jia and Yanchun Zhang},
  doi          = {10.1109/TKDE.2023.3270311},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12350-12368},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A comprehensive survey on multi-view clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerated fuzzy c-means clustering based on new affinity
filtering and membership scaling. <em>TKDE</em>, <em>35</em>(12),
12337–12349. (<a
href="https://doi.org/10.1109/TKDE.2023.3273274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy C-Means (FCM) is a widely used clustering method. However, FCM and its many accelerated variants have low efficiency in the mid-to-late stage of the clustering process. In this stage, all samples are involved in updating their non-affinity centers, and the membership grades of most samples, whose assignments remain unchanged, are still updated by calculating the sample-center distances. All these factors lead to the algorithms converging slowly. In this paper, a new affinity filtering technique is developed to recognize a complete set of non-affinity centers for each sample with low computations. Then, a new membership scaling technique is suggested to set the membership grades between each sample and its non-affinity centers to 0 and maintain the fuzzy membership grades for others. By integrating these two techniques, FCM based on new affinity filtering and membership scaling (AMFCM) is proposed to accelerate the whole convergence process of FCM. Numerous experimental results performed on synthetic and real-world data sets have shown the feasibility and efficiency of the proposed algorithm. Compared with state-of-the-art algorithms, AMFCM is significantly faster and more effective. For example, AMFCM reduces the number of FCM iterations by 80 $\%$ on average.},
  archive      = {J_TKDE},
  author       = {Dong Li and Shuisheng Zhou and Witold Pedrycz},
  doi          = {10.1109/TKDE.2023.3273274},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12337-12349},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Accelerated fuzzy C-means clustering based on new affinity filtering and membership scaling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). Traffic accident risk prediction via multi-view multi-task
spatio-temporal networks. <em>TKDE</em>, <em>35</em>(12), 12323–12336.
(<a href="https://doi.org/10.1109/TKDE.2021.3135621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal traffic incidents such as traffic accidents have become a significant health and development threat with the rapid urbanization of many countries. Thus it is critically important to accurately forecast the traffic accident risks of different areas in a city, which has attracted increasing research interest in the research area of urban computing. The challenges of accurate traffic risk forecasting are three-fold. First, traffic accident data in some areas of a city is sparse, especially for a fine-grained prediction, which may cause the zero inflation problem during model training. Second, the spatio-temporal correlations of the traffic accidents occurring in different areas are rather complex and non-linear, which is difficult to capture by existing shallow models like regression. Third, the occurrence of traffic accidents can be significantly affected by various context features including weather, POI and road network features. It is non-trivial to capture the complex associations between the diverse context features and traffic accident risks for building an accurate prediction model. To address the above challenges, this paper proposes a Multi-View Multi-Task Spatio-Temporal Networks (MVMT-STN) model to forecast fine- and coarse-grained traffic accident risks of a city simultaneously. Specifically, to address the data sparsity issue in a fine-grained prediction, we adopt a multi-task learning framework to jointly forecast both fine- and coarse-grained traffic accident risks by considering their spatial associations. For each granularity prediction, we design the channel-wise CNN and multi-view GCN to capture the local geographic dependency and global semantic dependency, respectively. In order to obtain the diverse impacts of the context features on traffic accidents, we also introduce a fusion learning module that integrates the channel-wise and multi-view features learned from different types of the external factors. We conduct extensive experiments over two large real traffic accident datasets. The results show that MVMT-STN improves the performance of traffic accident risk prediction in both fine- and coarse-grained prediction by a large margin compared with existing state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Senzhang Wang and Jiaqiang Zhang and Jiyue Li and Hao Miao and Jiannong Cao},
  doi          = {10.1109/TKDE.2021.3135621},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12323-12336},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Traffic accident risk prediction via multi-view multi-task spatio-temporal networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Markov-driven graph convolutional networks for social
spammer detection. <em>TKDE</em>, <em>35</em>(12), 12310–12322. (<a
href="https://doi.org/10.1109/TKDE.2022.3150669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing popularity of social media, malicious users (spammers) unfairly overpower legitimate users with unwanted or fake content to achieve their illegal purposes, which encourages research on spammer detection. The existing spammer detection methods can be characterized into feature-based detection and propagation-based detection. However, feature-based methods (e.g., GCN) cannot capture the user’s following relations, while propagation-based methods cannot utilize the rich text features. To this end, we consider combining these two methods and propose an Adaptive Reward Markov Random Field (ARMRF) layer. ARMRF layer models three intuitions on user label relations and assign them different learnable rewards. Besides, we learn the reward weights by stacking the ARMRF layer on top of GCN for end-to-end training, and we call the stacked model ARMGCN. To further improve the expressive power of ARMGCN, we propose the Markov-Driven Graph Convolutional Network (MDGCN), which integrates conditional random fields (CRF) and ARMGCN. CRF establishes the label joint probability distribution conditioned features for learning user dependencies, and the distribution can be optimized by a variational EM algorithm. We extensively evaluate the proposed method on two real-world Twitter datasets, and the experimental results demonstrate that MDGCN outperforms the state-of-the-art baselines. In addition, the ARMRF layer is model-independent, so it can be integrated with existing advanced detection methods to improve detection performance further.},
  archive      = {J_TKDE},
  author       = {Leyan Deng and Chenwang Wu and Defu Lian and Yongji Wu and Enhong Chen},
  doi          = {10.1109/TKDE.2022.3150669},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12310-12322},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Markov-driven graph convolutional networks for social spammer detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning discriminative text representation for streaming
social event detection. <em>TKDE</em>, <em>35</em>(12), 12295–12309. (<a
href="https://doi.org/10.1109/TKDE.2021.3119686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event detection on social platforms can help people perceive essential events and make actionable decisions. Existing document-pivot streaming social event detection methods generally embed documents and perform text clustering. They face the challenges of constantly changing context and unknown event categories and struggle by designing compound text representation methods and various similarity measures. However, phased, well-designed methods are excessively fragile and unable to utilize the potential of text representations fully. Meanwhile, their complex threshold settings result in clustering-based event detection suffering the pain of ever-changing environments. We propose a text representation learning method namely Text Sim ilarity C ontrastive L earning N eural N etwork (Text-SimCLNN) to tackle these challenges. Text-SimCLNN uses smaller parts to learn the similarity probability of text pairs from semantic and structural perspectives, effectively bridging the gap between text representation learning and similarity measure in streaming event detection. Event discovery and merging in streams can be easily performed based on the learned representations, and we use various techniques to speed up such processes. Furthermore, we introduce an online update mechanism that uses heterogeneous graphs to generate high-quality samples to enable stable and reliable inductive learning. Extensive experiments on two real-world datasets demonstrate that our method far exceeds state-of-the-art (SOTA).},
  archive      = {J_TKDE},
  author       = {Chaodong Tong and Huailiang Peng and Xu Bai and Qiong Dai and Ruitong Zhang and Yangyang Li and Hanjie Xu and Xian-Ming Gu},
  doi          = {10.1109/TKDE.2021.3119686},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12295-12309},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning discriminative text representation for streaming social event detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IoT botnet detection based on anomalies of multiscale time
series dynamics. <em>TKDE</em>, <em>35</em>(12), 12282–12294. (<a
href="https://doi.org/10.1109/TKDE.2022.3157636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a solution for detecting botnet attacks on the Internet of Things (IoT) by identifying anomalies in the temporal dynamics of their devices. Given their limited computing capabilities, IoT devices are more vulnerable to attacks than conventional computers. In this scenario, botnets have a high degree of severity since they are used to trigging distributed denial-of-service attacks, which are amplified by a large number of IoT devices. Thus, solutions aiming to identify and mitigate the damage caused by botnets in IoT are urgent and essential. We evaluate the number of packets a device transmits, following a multiscale ordinal patterns transformation, and use Isolation Forest for anomaly detection. By investigating how devices evolve, we can distinguish between normal and anomalous behaviors. We apply the proposed solution to detect two major botnets for IoT: Mirai and Bashlite. We evaluated our model throughout two experimental setups. The first, using a single model for all devices, reaching 99.5\% of accuracy and 99.6\% of specificity, and the second, by tuning a model per device, reaching 100\% of accuracy. These results show that, with the proper transformation, it is possible to use simple methods for detecting anomalies in IoT devices’ behaviors.},
  archive      = {J_TKDE},
  author       = {João B. Borges and João P. S. Medeiros and Luiz P. A. Barbosa and Heitor S. Ramos and Antonio A. F. Loureiro},
  doi          = {10.1109/TKDE.2022.3157636},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12282-12294},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {IoT botnet detection based on anomalies of multiscale time series dynamics},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying anomalies while preserving privacy.
<em>TKDE</em>, <em>35</em>(12), 12264–12281. (<a
href="https://doi.org/10.1109/TKDE.2021.3129633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying anomalies in data is vital in many domains, including medicine, finance, and national security. However, privacy concerns pose a significant roadblock to carrying out such an analysis. Since existing privacy definitions do not allow good accuracy when doing outlier analysis, the notion of sensitive privacy has been recently proposed to deal with this problem. Sensitive privacy makes it possible to analyze data for anomalies with practically meaningful accuracy while providing a strong guarantee similar to differential privacy, which is the prevalent privacy standard today. In this work, we relate sensitive privacy to other important notions of data privacy so that one can port the technical developments and private mechanism constructions from these related concepts to sensitive privacy. Sensitive privacy critically depends on the underlying anomaly model. We develop a novel n-step lookahead mechanism to efficiently answer arbitrary outlier queries, which provably guarantees sensitive privacy if we restrict our attention to common a class of anomaly models. We also provide general constructions to give sensitively private mechanisms for identifying anomalies and show the conditions under which the constructions would be optimal.},
  archive      = {J_TKDE},
  author       = {Hafiz Asif and Jaideep Vaidya and Periklis A. Papakonstantinou},
  doi          = {10.1109/TKDE.2021.3129633},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12264-12281},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Identifying anomalies while preserving privacy},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid-order anomaly detection on attributed networks.
<em>TKDE</em>, <em>35</em>(12), 12249–12263. (<a
href="https://doi.org/10.1109/TKDE.2021.3117842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection on attributed networks has received an increasing amount of attention in recent years. Despite the success, most of the existing methods only focus on detecting the abnormal nodes while fail to detect the abnormal subgraphs. In this paper, we define a new problem of hybrid-order anomaly detection on attributed networks, which aims to detect both of the abnormal nodes and subgraphs. To this end, a new deep learning model called Hybrid-Order Graph Attention Network (HO-GAT) is developed, which is able to simultaneously detect the abnormal nodes and motif instances in an attributed network. In order to model the mutual influence between nodes and motif instances, the learning procedures of the node representation and the motif instance representation are integrated into a unified graph attention network with a novel hybrid-order self-attention mechanism. After learning the node representation and the motif instance representation, two decoders are respectively designed to reconstruct the attribute information of the nodes and motif instances, and the hybrid-order topological structure among nodes and motif instances. And finally, the reconstruction errors are utilized as the abnormal score of nodes and motif instances respectively. Extensive experiments conducted on real-world datasets have confirmed the effectiveness of the HO-GAT method.},
  archive      = {J_TKDE},
  author       = {Ling Huang and Ye Zhu and Yuefang Gao and Tuo Liu and Chao Chang and Caixing Liu and Yong Tang and Chang-Dong Wang},
  doi          = {10.1109/TKDE.2021.3117842},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12249-12263},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hybrid-order anomaly detection on attributed networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generative evolutionary anomaly detection in dynamic
networks. <em>TKDE</em>, <em>35</em>(12), 12234–12248. (<a
href="https://doi.org/10.1109/TKDE.2021.3129057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in dynamic networks aims to find network elements (e.g., nodes, edges, subgraphs, change points) with significantly different behaviors from the vast majority, it can also devote to community detection and evolution and prediction tasks. Most existing methods focus on one specific task, that is, only detect anomalies of one type of element isolated, so they lose the ability to model the correlation and driving mechanism between different abnormal behavior. Considering that the anomaly detection of one type of element is helpful to other types of elements, i.e., the temporal evolution hidden the dynamic networks are driven by indivisible behavior patterns. So in this paper, we propose a unified Generation model to analyze the dynamic network for Exploring the Abnormal Behaviors of different Scales (GEABS). It can model the relation and catch different levels (node, community and network) of anomaly with a joint statistical network model and detect the community structure and its evolution. Specifically, we denote the parameters of node popularity, community membership to generate the dynamic network with stochastic block model (SBM), we also describe the varying of node and community by dynamic process. With a well-designed generative mechanism, it can detect the change point on network level, temporal evolution on community level and abnormal behavior on node level synchronously, besides, it also detects the community structure effectively. We also propose an effective optimization algorithm with variational inference. Experimental results show that the GEABS achieves better performance on abnormal behavior and community structure compared with baselines.},
  archive      = {J_TKDE},
  author       = {Pengfei Jiao and Tianpeng Li and Yingjie Xie and Yinghui Wang and Wenjun Wang and Dongxiao He and Huaming Wu},
  doi          = {10.1109/TKDE.2021.3129057},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12234-12248},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Generative evolutionary anomaly detection in dynamic networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generative and contrastive self-supervised learning for
graph anomaly detection. <em>TKDE</em>, <em>35</em>(12), 12220–12233.
(<a href="https://doi.org/10.1109/TKDE.2021.3119326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection from graph data has drawn much attention due to its practical significance in many critical applications including cybersecurity, finance, and social networks. Existing data mining and machine learning methods are either shallow methods that could not effectively capture the complex interdependency of graph data or graph autoencoder methods that could not fully exploit the contextual information as supervision signals for effective anomaly detection. To overcome these challenges, in this paper, we propose a novel method, Self-Supervised Learning for Graph Anomaly Detection ( SL-GAD ). Our method constructs different contextual subgraphs (views) based on a target node and employs two modules, generative attribute regression and multi-view contrastive learning for anomaly detection. While the generative attribute regression module allows us to capture the anomalies in the attribute space, the multi-view contrastive learning module can exploit richer structure information from multiple subgraphs, thus abling to capture the anomalies in the structure space, mixing of structure, and attribute information. We conduct extensive experiments on six benchmark datasets and the results demonstrate that our method outperforms state-of-the-art methods by a large margin.},
  archive      = {J_TKDE},
  author       = {Yu Zheng and Ming Jin and Yixin Liu and Lianhua Chi and Khoa T. Phan and Yi-Ping Phoebe Chen},
  doi          = {10.1109/TKDE.2021.3119326},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12220-12233},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Generative and contrastive self-supervised learning for graph anomaly detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GAN-based anomaly detection for multivariate time series
using polluted training set. <em>TKDE</em>, <em>35</em>(12),
12208–12219. (<a
href="https://doi.org/10.1109/TKDE.2021.3128667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series anomaly detection has great potentials in many practical applications such as structural health monitoring, intelligent operation and maintenance, quantitative trading, etc . Extreme unbalanced training set and noise interference make it challenging to accurately capture the distribution of normal data and then detect anomalies. Recently, dozens of AutoEncoder (AE) and Generative Adversarial Network (GAN) based methods have been proposed to learn the latent representation of normal data and then detect anomalies based on reconstruction error. However, existing AE-based approaches are lack of effective regularization method specially designed for anomaly detection tasks thus easily overfitting while GAN-based approaches are mostly trained under the hypothesis of pollution-free training set, which means the training set is all composed of normal samples and that is hard to satisfy in practice. To tackle these problems, in this paper we propose a GAN based anomaly detection method for multivariate time series named FGANomaly (letter F is for Filter). The core idea is to filter possible anomalous samples with pseudo-labels before training the discriminator thus to capture the distribution of normal data as precise as possible. In addition, we design a novel training objective for the generator, which leads the generator to concentrate more on plausible normal data and ignore anomalies. We conducted comprehensive experiments on four public datasets, and the experimental results show the superiority of our method over baselines in both performance and robustness.},
  archive      = {J_TKDE},
  author       = {Bowen Du and Xuanxuan Sun and Junchen Ye and Ke Cheng and Jingyuan Wang and Leilei Sun},
  doi          = {10.1109/TKDE.2021.3128667},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12208-12219},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GAN-based anomaly detection for multivariate time series using polluted training set},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end transferable anomaly detection via multi-spectral
cross-domain representation alignment. <em>TKDE</em>, <em>35</em>(12),
12194–12207. (<a
href="https://doi.org/10.1109/TKDE.2021.3118111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection (AD) aims to distinguish abnormal instances from what is defined as normal, which strongly correlates with the safe and robust applications of machine learning. A well-performed anomaly detector often relies on the training on massive labeled data, while it is of high cost to annotate data in practice. Fortunately, this dilemma can be solved by transferring the knowledge of a label-rich dataset (source domain) to assist the learning on the label-scarce dataset (target domain), which is known as domain adaptation in transfer learning. In this paper, we propose a Multi-spectral Cross-domain Representation Alignment (MsRA) method for the anomaly detection in the domain adaptation setting, where we can only access normal source data and limited normal target data. Specifically, MsRA first constructs multi-spectral feature representations by fusing different frequency components of the original features, which mitigates the information scarcity due to limited target training data by capturing richer input pattern information. Then we employ the adversarial training strategy to learn domain-invariant features and force the features of normal data to be more compact by the center clustering. Finally, the distance of each sample to the prototype of normal class can be used as its anomaly score, where the prototype is the center of both source and target data. In this way, we achieve anomaly detection in an end-to-end manner, without two-stage training for feature extraction and anomaly detection. Comprehensive experiments on cross-domain anomaly detection benchmarks validate the effectiveness of MsRA.},
  archive      = {J_TKDE},
  author       = {Shuang Li and Shugang Li and Mixue Xie and Kaixiong Gong and Jianxin Zhao and Chi Harold Liu and Guoren Wang},
  doi          = {10.1109/TKDE.2021.3118111},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12194-12207},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {End-to-end transferable anomaly detection via multi-spectral cross-domain representation alignment},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). ECOD: Unsupervised outlier detection using empirical
cumulative distribution functions. <em>TKDE</em>, <em>35</em>(12),
12181–12193. (<a
href="https://doi.org/10.1109/TKDE.2022.3159580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection refers to the identification of data points that deviate from a general data distribution. Existing unsupervised approaches often suffer from high computational cost, complex hyperparameter tuning, and limited interpretability, especially when working with large, high-dimensional datasets. To address these issues, we present a simple yet effective algorithm called ECOD (Empirical-Cumulative-distribution-based Outlier Detection), which is inspired by the fact that outliers are often the “rare events” that appear in the tails of a distribution. In a nutshell, ECOD first estimates the underlying distribution of the input data in a nonparametric fashion by computing the empirical cumulative distribution per dimension of the data. ECOD then uses these empirical distributions to estimate tail probabilities per dimension for each data point. Finally, ECOD computes an outlier score of each data point by aggregating estimated tail probabilities across dimensions. Our contributions are as follows: (1) we propose a novel outlier detection method called ECOD , which is both parameter-free and easy to interpret; (2) we perform extensive experiments on 30 benchmark datasets, where we find that ECOD outperforms 11 state-of-the-art baselines in terms of accuracy, efficiency, and scalability; and (3) we release an easy-to-use and scalable (with distributed support) Python implementation for accessibility and reproducibility.},
  archive      = {J_TKDE},
  author       = {Zheng Li and Yue Zhao and Xiyang Hu and Nicola Botta and Cezar Ionescu and George H. Chen},
  doi          = {10.1109/TKDE.2022.3159580},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12181-12193},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ECOD: Unsupervised outlier detection using empirical cumulative distribution functions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative feature mining based on frequency information
and metric learning for face forgery detection. <em>TKDE</em>,
<em>35</em>(12), 12167–12180. (<a
href="https://doi.org/10.1109/TKDE.2021.3117003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery detection has received considerable attention due to security concerns about abnormal faces generated by face forgery technology. While recent researches have made prominent progress, they still suffer from two limitations: a) the learned features supervised by softmax loss are insufficiently discriminative, since the softmax loss fails to explicitly boost inter-class separability and intra-class compactness; b) hand-crafted features are unable to effectively mine forgery patterns from frequency domain. To address the two problems, this paper proposes a novel frequency-aware discriminative feature learning framework. Specifically, we design an innovative single-center loss which compresses mere intra-class variations of natural faces while encouraging inter-class differences between natural and manipulated faces in the embedding space. Supervised by such a loss, more discriminative features can be learned with less optimization difficulty. As for frequency-related features, a frequency feature adaptively generated module is developed to capture frequency clues in a data-driven manner. Besides, to better fuse the features of both RGB domain and frequency domain, this paper devises a fusion module based on positional correlation of features. The effectiveness and superiority of our framework have been proved by extensive experiments and our approach achieves state-of-the-art performance in both in-dataset and cross-dataset evaluation.},
  archive      = {J_TKDE},
  author       = {Jiaming Li and Hongtao Xie and Lingyun Yu and Xingyu Gao and Yongdong Zhang},
  doi          = {10.1109/TKDE.2021.3117003},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12167-12180},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discriminative feature mining based on frequency information and metric learning for face forgery detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Concept drift-based runtime reliability anomaly detection
for edge services adaptation. <em>TKDE</em>, <em>35</em>(12),
12153–12166. (<a
href="https://doi.org/10.1109/TKDE.2021.3127224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the rapidly increasing need of computation-intensive and latency-sensitive applications, mobile edge computing (MEC) has attracted tremendous attention from both academia and industry. However, the runtime reliability of edge services fluctuates over time due to the dynamics in their internal states and the external environment. This causes the distribution of edge services’ reliability data streams to vary in the form of concept drift. Severe negative reliability drifts indicate that an edge service may be suffering from a performance anomaly or a runtime failure. To ensure the stable operation of edge services, we propose A-Detection, a concept drift-based runtime reliability anomaly detection approach for edge services adaptation. We integrate reservoir sampling and singular value decomposition (SVD) for large-scale streaming data sampling and feature extraction. Jensen Shannon (JS) divergence is utilized to develop a dissimilarity metric of data stream distribution, called FDC, for runtime edge service reliability anomaly detection. When an anomaly is detected in a running edge service, checkpoint-retry is combined with computation offloading to implement runtime reliability adaptation. Extensive experimental results verify and demonstrate the effectiveness and efficiency of A-Detection.},
  archive      = {J_TKDE},
  author       = {Lei Wang and Shuhan Chen and Qiang He},
  doi          = {10.1109/TKDE.2021.3127224},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12153-12166},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Concept drift-based runtime reliability anomaly detection for edge services adaptation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CollaborEM: A self-supervised entity matching framework
using multi-features collaboration. <em>TKDE</em>, <em>35</em>(12),
12139–12152. (<a
href="https://doi.org/10.1109/TKDE.2021.3134806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity Matching (EM) aims to identify whether two tuples refer to the same real-world entity and is well-known to be labor-intensive. It is a prerequisite to anomaly detection, as comparing the attribute values of two matched tuples from two different datasets provides one effective way to detect anomalies. Existing EM approaches, due to insufficient feature discovery or error-prone inherent characteristics, are not able to achieve stable performance. In this paper, we present ${{\sf CollaborEM}}$ , a self-supervised entity matching framework via multi-features collaboration. It is capable of (i) obtaining reliable EM results with zero human annotations and (ii) discovering adequate tuples’ features in a fault-tolerant manner. ${{\sf CollaborEM}}$ consists of two phases, i.e., automatic label generation (ALG) and collaborative EM training (CEMT). In the first phase, ALG is proposed to generate a set of positive tuple pairs and a set of negative tuple pairs. ALG guarantees the high quality of the generated tuples, and hence ensures the training quality of the subsequent CEMT. In the second phase, CEMT is introduced to learn the matching signals by discovering graph features and sentence features of tuples collaboratively. Extensive experimental results over eight real-world EM benchmarks show that ${{\sf CollaborEM}}$ outperforms all the existing unsupervised EM approaches and is comparable or even superior to the state-of-the-art supervised EM methods.},
  archive      = {J_TKDE},
  author       = {Congcong Ge and Pengfei Wang and Lu Chen and Xiaoze Liu and Baihua Zheng and Yunjun Gao},
  doi          = {10.1109/TKDE.2021.3134806},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12139-12152},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CollaborEM: A self-supervised entity matching framework using multi-features collaboration},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CityNeuro: Towards location and time prediction for urban
abnormal events. <em>TKDE</em>, <em>35</em>(12), 12125–12138. (<a
href="https://doi.org/10.1109/TKDE.2022.3193128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban abnormal events constitute a significant threat to social order and public safety. It is of vital importance for emergency treatment if the location and time of abnormal events could be predicted before they happen. However, forecasting the occurrence of urban abnormal events is extremely challenging due to various influencing factors. First, the spatiotemporal environment in urban space is associated with complicated and dynamic attributes, which all potentially affect the happening of urban emergency events. Second, historical events also influence the occurrence of future events, and the impacts vary across urban regions and time due to dynamic regional relations. In this paper, we propose a framework called CityNeuro that incorporates both environmental and historical influence for location and time prediction of urban abnormal events. On the one hand, we identify important environmental factors by analyzing real-world datasets and constructing essential spatiotemporal features accordingly. On the other hand, we propose using neural region states to capture important historical information with a novel spatiotemporal information propagation mechanism. To the best of our knowledge, we are the first to forecast the precise location and time of individual urban abnormal events. Extensive experiments on real-world datasets demonstrate the advantages of our model compared with state-of-the-art spatiotemporal prediction methods.},
  archive      = {J_TKDE},
  author       = {Mingyang Zhang and Tong Li and Pan Hui},
  doi          = {10.1109/TKDE.2022.3193128},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12125-12138},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CityNeuro: Towards location and time prediction for urban abnormal events},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BisSiam: Bispectrum siamese network based contrastive
learning for UAV anomaly detection. <em>TKDE</em>, <em>35</em>(12),
12109–12124. (<a
href="https://doi.org/10.1109/TKDE.2021.3118727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a surging number of unmanned aerial vehicles (UAVs) are pervasively utilized in many areas. However, the increasing number of UAVs may cause privacy and security issues such as voyeurism and espionage. It is critical for individuals or organizations to manage their behaviors and proactively prevent the misbehaved invasion of unauthorized UAVs through effective anomaly detection. The UAV anomaly detection framework needs to cope with complex signals in the noisy-prone environments and to function with very limited labeled samples. This paper proposes BisSiam , a novel framework that is capable of identifying UAV presence, types and operation modes. BisSiam converts UAVs signals to bispectrum as the input and exploits a siamese network based contrastive learning model to learn the vector encoding. A sampling mechanism is proposed for optimizing the sample size involved in the model training whilst ensuring the model accuracy without compromising the training efficiency. Finally, we present a similarity-based fingerprint matching mechanism for detecting unseen UAVs without the need of retraining the whole model. Experiment results show that our approach outperforms other baselines and can reach 92.85\% accuracy of UAV type detection in unsupervised learning scenarios. 91.4\% accuracy can be achieved when BisSiam is used for detecting the UAV type of the out-of-sample UAVs.},
  archive      = {J_TKDE},
  author       = {Taotao Li and Zhen Hong and Qianming Cai and Li Yu and Zhenyu Wen and Renyu Yang},
  doi          = {10.1109/TKDE.2021.3118727},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12109-12124},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BisSiam: Bispectrum siamese network based contrastive learning for UAV anomaly detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anomaly rule detection in sequence data. <em>TKDE</em>,
<em>35</em>(12), 12095–12108. (<a
href="https://doi.org/10.1109/TKDE.2021.3139086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing sequence data usually leads to the discovery of interesting patterns and then anomaly detection. In recent years, numerous frameworks and methods have been proposed to discover interesting patterns in sequence data as well as detect anomalous behavior. However, existing algorithms mainly focus on frequency-driven analytics, and they are challenging to be applied in real-world settings. In this work, we present a new anomaly detection framework called DUOS that enables Discovery of Utility-aware Outlier Sequential rules from a set of sequences. In this pattern-based anomaly detection algorithm, we incorporate both the anomalousness and utility of a group, and then introduce the concept of utility-aware outlier sequential rule (UOSR). We show that this is a more meaningful way for detecting anomalies. Besides, we propose some efficient pruning strategies w.r.t. upper bounds for mining UOSR, as well as the outlier detection. An extensive experimental study conducted on several real-world datasets shows that the proposed DUOS algorithm has a better effectiveness and efficiency. Finally, DUOS outperforms the baseline algorithm and has a suitable scalability.},
  archive      = {J_TKDE},
  author       = {Wensheng Gan and Lili Chen and Shicheng Wan and Jiahui Chen and Chien-Ming Chen},
  doi          = {10.1109/TKDE.2021.3139086},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12095-12108},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Anomaly rule detection in sequence data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Anomaly detection in dynamic graphs via transformer.
<em>TKDE</em>, <em>35</em>(12), 12081–12094. (<a
href="https://doi.org/10.1109/TKDE.2021.3124061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting anomalies for dynamic graphs has drawn increasing attention due to their wide applications in social networks, e-commerce, and cybersecurity. Recent deep learning-based approaches have shown promising results over shallow methods. However, they fail to address two core challenges of anomaly detection in dynamic graphs: the lack of informative encoding for unattributed nodes and the difficulty of learning discriminate knowledge from coupled spatial-temporal dynamic graphs. To overcome these challenges, in this paper, we present a novel T ransformer-based A nomaly D etection framework for DY namic graphs ( TADDY ). Our framework constructs a comprehensive node encoding strategy to better represent each node’s structural and temporal roles in an evolving graphs stream. Meanwhile, TADDY captures informative representation from dynamic graphs with coupled spatial-temporal patterns via a dynamic graph transformer model. The extensive experimental results demonstrate that our proposed TADDY framework outperforms the state-of-the-art methods by a large margin on six real-world datasets.},
  archive      = {J_TKDE},
  author       = {Yixin Liu and Shirui Pan and Yu Guang Wang and Fei Xiong and Liang Wang and Qingfeng Chen and Vincent CS Lee},
  doi          = {10.1109/TKDE.2021.3124061},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12081-12094},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Anomaly detection in dynamic graphs via transformer},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive memory networks with self-supervised learning for
unsupervised anomaly detection. <em>TKDE</em>, <em>35</em>(12),
12068–12080. (<a
href="https://doi.org/10.1109/TKDE.2021.3139916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly detection aims to build models to effectively detect unseen anomalies by only training on the normal data. Although previous reconstruction-based methods have made fruitful progress, their generalization ability is limited due to two critical challenges. First, the training dataset only contains normal patterns, which limits the model generalization ability. Second, the feature representations learned by existing models often lack representativeness which hampers the ability to preserve the diversity of normal patterns. In this paper, we propose a novel approach called Adaptive Memory Network with Self-supervised Learning (AMSL) to address these challenges and enhance the generalization ability in unsupervised anomaly detection. Based on the convolutional autoencoder structure, AMSL incorporates a self-supervised learning module to learn general normal patterns and an adaptive memory fusion module to learn rich feature representations. Experiments on four public multivariate time series datasets demonstrate that AMSL significantly improves the performance compared to other state-of-the-art methods. Specifically, on the largest CAP sleep stage detection dataset with 900 million samples, AMSL outperforms the second-best baseline by 4\%+ in both accuracy and F1 score. Apart from the enhanced generalization ability, AMSL is also more robust against input noise.},
  archive      = {J_TKDE},
  author       = {Yuxin Zhang and Jindong Wang and Yiqiang Chen and Han Yu and Tao Qin},
  doi          = {10.1109/TKDE.2021.3139916},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12068-12080},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive memory networks with self-supervised learning for unsupervised anomaly detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Adaptive label propagation for group anomaly detection in
large-scale networks. <em>TKDE</em>, <em>35</em>(12), 12053–12067. (<a
href="https://doi.org/10.1109/TKDE.2022.3176478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concentrates on group anomalies in general large-scale networks. Existing algorithms on group anomalies mainly focus on homogeneous or bipartite networks, and thus are difficult to apply to heterogeneous networks directly. Moreover, these algorithms follow the non-overlapping hypothesis of groups implicitly, which is improper in many scenarios. For example, fraud users in Alibaba E-commerce platform may join more than one organization at the same time. In this paper, we introduce a novel algorithm called Adaptive Label Propagation (ALP) to solve these problems. ALP is designed based on label propagation (LP) frameworks, for the reason that LP-based frameworks are simple in thought and easy to scale. ALP is able to find overlapping groups by label propagation with belonging coefficients, and can be applied to heterogeneous networks for its design of adaptive neighbor weighting. Assigning different weights to neighbors in label propagation is a challenging task. Inspired by the combinatorial multi-armed bandit mechanism, ALP views the neighbors of each node as arms to be selected, and iteratively updates their weights by evaluating their expected rewards in following iterations. Experiments are conducted on four real-world networks (including two bipartite ones and two heterogeneous ones). The results show that LP-based methods are effective for detecting group anomalies, and the comparison results with several state-of-the-art label propagation based community detection methods show the effectiveness of the proposed method.},
  archive      = {J_TKDE},
  author       = {Zhao Li and Xia Chen and Junshuai Song and Jun Gao},
  doi          = {10.1109/TKDE.2022.3176478},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12053-12067},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive label propagation for group anomaly detection in large-scale networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active learning for human-in-the-loop customs inspection.
<em>TKDE</em>, <em>35</em>(12), 12039–12052. (<a
href="https://doi.org/10.1109/TKDE.2022.3144299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the human-in-the-loop customs inspection scenario, where an AI-assisted algorithm supports customs officers by recommending a set of imported goods to be inspected. If the inspected items are fraudulent, the officers can levy extra duties. The updated decisions are used as additional training data for successive iterations. Inspecting only the likely fraudulent items may lead to an immediate gain in revenue, yet it does not bring new insights for learning dynamic trade patterns. In contrast, including uncertain items in the inspection helps gradually acquire new knowledge that will be used as supplementary training resources to update the system. Based on multiyear customs declaration logs obtained from three countries, we demonstrate that some degree of exploration is necessary to cope with domain shifts in trade data. The results show that a hybrid strategy of jointly selecting likely fraudulent and uncertain items will eventually outperform the exploitation-only strategy.},
  archive      = {J_TKDE},
  author       = {Sundong Kim and Tung-Duong Mai and Sungwon Han and Sungwon Park and D.K. Thi Nguyen and Jaechan So and Karandeep Singh and Meeyoung Cha},
  doi          = {10.1109/TKDE.2022.3144299},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12039-12052},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Active learning for human-in-the-loop customs inspection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive survey on graph anomaly detection with deep
learning. <em>TKDE</em>, <em>35</em>(12), 12012–12038. (<a
href="https://doi.org/10.1109/TKDE.2021.3118815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalies are rare observations (e.g., data records or events) that deviate significantly from the others in the sample. Over the past few decades, research on anomaly mining has received increasing interests due to the implications of these occurrences in a wide range of disciplines - for instance, security, finance, and medicine. For this reason, anomaly detection, which aims to identify these rare observations, has become one of the most vital tasks in the world and has shown its power in preventing detrimental events, such as financial fraud, network intrusions, and social spam. The detection task is typically solved by identifying outlying data points in the feature space, which, inherently, overlooks the relational information in real-world data. At the same time, graphs have been prevalently used to represent the structural/relational information, which raises the graph anomaly detection problem - identifying anomalous graph objects (i.e., nodes, edges and sub-graphs) in a single graph, or anomalous graphs in a set/database of graphs. Conventional anomaly detection techniques cannot tackle this problem well because of the complexity of graph data (e.g., irregular structures, relational dependencies, node/edge types/attributes/directions/multiplicities/weights, large scale, etc.). However, thanks to the advent of deep learning in breaking these limitations, graph anomaly detection with deep learning has received a growing attention recently. In this survey, we aim to provide a systematic and comprehensive review of the contemporary deep learning techniques for graph anomaly detection. Specifically, we provide a taxonomy that follows a task-driven strategy and categorizes existing work according to the anomalous graph objects that they can detect. We especially focus on the challenges in this research area and discuss the key intuitions, technical details as well as relative strengths and weaknesses of various techniques in each category. From the survey results, we highlight 12 future research directions spanning unsolved and emerging problems introduced by graph data, anomaly detection, deep learning and real-world applications. Additionally, to provide a wealth of useful resources for future studies, we have compiled a set of open-source implementations, public datasets, and commonly-used evaluation metrics. With this survey, our goal is to create a “one-stop-shop” that provides a unified understanding of the problem categories and existing approaches, publicly available hands-on resources, and high-impact open challenges for graph anomaly detection using deep learning.},
  archive      = {J_TKDE},
  author       = {Xiaoxiao Ma and Jia Wu and Shan Xue and Jian Yang and Chuan Zhou and Quan Z. Sheng and Hui Xiong and Leman Akoglu},
  doi          = {10.1109/TKDE.2021.3118815},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {12012-12038},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A comprehensive survey on graph anomaly detection with deep learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Abnormal samples oversampling for anomaly detection based on
uniform scale strategy and closed area. <em>TKDE</em>, <em>35</em>(12),
11999–12011. (<a
href="https://doi.org/10.1109/TKDE.2021.3130595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The samples representing abnormal situation is usually very few in the dataset, which makes it difficult to learn the features of abnormal samples by machine-learning-based methods. To improve the accuracy of anomaly detection, the number of abnormal samples should be expanded to ensure the balance of the dataset. In this paper, a discrete synthetic minority oversampling technique (D-SMOTE) is proposed to generate new samples. A closed area is constructed using the three nearest abnormal samples in the dataset. The new samples are then uniformly interpolated in a closed area. By this means, the problem of the imbalance for the original dataset is handled, thus improving the data quality. Based on the expanded datasets, a two-dimensional convolutional neural network (2D CNN) is constructed to detect abnormal samples. In experiments, three cases and different machine learning methods are considered for comparison. Several indexes including accuracy, precision, confusion matrix, F1-score, and Recall have been used to evaluate the detection effectiveness. The results show that the abnormal samples can be detected accurately using oversampling data obtained from the proposed D-SMOTE method.},
  archive      = {J_TKDE},
  author       = {Anqi Shangguan and Guo Xie and Lingxia Mu and Rong Fei and Xinhong Hei},
  doi          = {10.1109/TKDE.2021.3130595},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {11999-12011},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Abnormal samples oversampling for anomaly detection based on uniform scale strategy and closed area},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A2DJP: A two graph-based component fused learning framework
for urban anomaly distribution and duration joint-prediction.
<em>TKDE</em>, <em>35</em>(12), 11984–11998. (<a
href="https://doi.org/10.1109/TKDE.2022.3176650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern intelligent transportation system (ITS) has greatly benefitted people&#39;s daily life. However, the chanciness and suddenness of urban anomalies may greatly restrict the trouble-free operations of ITS. To be aware of future urban anomalies and their possible influences, great efforts have been achieved on these two aspects, but comprehensive predictions of urban anomalies including the predictions of distributions and durations, are still beingless. And the spatiotemporal cascade self/mutual exciting influences among anomalies have never been considered in previous studies. In this paper, we propose a novel Anomaly Distribution and Duration Joint-Prediction (A2DJP) algorithm to simultaneously filtrate urban subregions and estimate the duration of corresponding potential anomalies in the future. To capture the spatiotemporal correlations between urban traffics and anomalies, we use a modified Graph Convolution Network and Long Short-Term Memory integrated network. To learn the cascade correlations among anomalies themselves, we devise a novel Spatiotemporal neural Hawkes Process model, which contains a Hawkes Process (HP) based GCN and HP-based LSTM to extract the anomaly-wise spatiotemporal cascading correlations. By fusing the spatiotemporal correlations between traffics and anomalies, we then simultaneously predict the distributions and durations of future anomalies. Extensive experiments on real-world datasets demonstrate that our proposed method significantly outperforms state-of-the-art solutions.},
  archive      = {J_TKDE},
  author       = {Kun Wang and Zhengyang Zhou and Xu Wang and Pengkun Wang and Qi Fang and Yang Wang},
  doi          = {10.1109/TKDE.2022.3176650},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {11984-11998},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A2DJP: A two graph-based component fused learning framework for urban anomaly distribution and duration joint-prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Guest editorial introduction to the special issue on
anomaly detection in emerging data-driven applications: Theory,
algorithms, and applications. <em>TKDE</em>, <em>35</em>(12),
11982–11983. (<a
href="https://doi.org/10.1109/TKDE.2023.3301582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are delighted to present this special issue on Anomaly Detection in Emerging Data-Driven Applications: Theory, Algorithms, and Applications. Anomaly detection plays an important part of knowledge and data engineering, such as cybersecurity, fintech, healthcare, public security and AI safety. However, large amounts of data have been generated through different types of objects, and it brings new challenges for anomaly detection research. The purpose of this special issue is to provide a forum for researchers and practitioners to present their latest research findings and engineering experiences in the theoretical foundations, empirical studies, and novel applications.},
  archive      = {J_TKDE},
  author       = {Jianxin Li and Lifang He and Hao Peng and Peng Cui and Charu C. Aggarwal and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3301582},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {12},
  pages        = {11982-11983},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Guest editorial introduction to the special issue on anomaly detection in emerging data-driven applications: Theory, algorithms, and applications},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TIRA: Truth inference via reliability aggregation on
object-source graph. <em>TKDE</em>, <em>35</em>(11), 11967–11981. (<a
href="https://doi.org/10.1109/TKDE.2022.3225308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing platforms collect massive dirty claims that are provided by sources for crowdsourced objects, which prompts truth inference to be proposed for crowdsourcing data denoising. Although current graph-based truth-inference methods achieve remarkable success by capturing complex crowdsourcing relationships, they typically suffer from two challenges: 1) They fail to obtain complete crowdsourcing relationships because of the structural limitations of crowdsourcing relationship graphs; 2) Their vector initialization methods for objects and sources are disturbed by claim noise, which limits them from obtaining correct object and source semantics. To cope with these challenges, we propose a novel T ruth- I nference method via R eliability A ggregation (TIRA) on an object-source graph. Specifically, we propose a hierarchical graph auto-encoder to adapt to a reasonable object-source graph, which enables TIRA to capture complete crowdsourcing relationships from multiple perspectives. To better guide TIRA, we design a vector initialization method based on source reliabilities to map the denoised claims to a representation space of objects and sources. Finally, TIRA aggregates the reliability information on an object-source graph to generate object embeddings for truth inference. We conducted extensive experiments on 12 real-world datasets. The experimental results demonstrate that our method significantly outperforms 12 state-of-the-art baselines in terms of the $accuracy$ and $weighted\_{F}1$ .},
  archive      = {J_TKDE},
  author       = {Gongqing Wu and Xingrui Zhuo and Liangzhu Zhou and Xianyu Bao and Richang Hong and Xindong Wu},
  doi          = {10.1109/TKDE.2022.3225308},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11967-11981},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TIRA: Truth inference via reliability aggregation on object-source graph},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-varying gaussian markov random fields learning for
multivariate time series clustering. <em>TKDE</em>, <em>35</em>(11),
11950–11966. (<a
href="https://doi.org/10.1109/TKDE.2022.3232331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series (MTS) clustering is an important technique for discovering co-evolving patterns and interpreting group characteristics in many areas including economics, bioinformatics, data science, etc. Although time series clustering has been widely studied in the past decades, no enough attention has been paid to capture time-varying correlation patterns in MTS. In this article, we propose a novel clustering approach for MTS data based on time-varying features. We introduce a time-varying Gaussian Markov Random Fields (T-GMRF) model to describe the correlation structure between MTS variables, and formulate the time-varying feature extraction problem as a convex optimization problem, which can be solved by a T-GMRF learning algorithm based on random block coordinate descent. We further apply a principal component analysis (PCA) based method on GMRF sequences to obtain low-dimensional feature vectors, and adopt a multi-density based clustering approach to form the cluster assignments. We conduct extensive experiments to compare the proposed T-GMRF method with 11 clustering algorithms based on 33 open MTS datasets, which show that T-GMRF significantly outperforms the state-of-the-arts with performance improvement up to 16\%-64.5\% on a variety of clustering performance metrics. The source codes of T-GMRF are publicly available at GitHub.},
  archive      = {J_TKDE},
  author       = {Wangxiang Ding and Wenzhong Li and Zhijie Zhang and Chen Wan and Jianhui Duan and Sanglu Lu},
  doi          = {10.1109/TKDE.2022.3232331},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11950-11966},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Time-varying gaussian markov random fields learning for multivariate time series clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The design and implementation of UniKV for mixed key-value
storage workloads. <em>TKDE</em>, <em>35</em>(11), 11935–11949. (<a
href="https://doi.org/10.1109/TKDE.2023.3234510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent key-value (KV) stores are mainly designed based on the Log-Structured Merge-tree (LSM-tree), yet they suffer from large read and write amplifications, especially when KV stores grow in size. Existing design optimizations for LSM-tree-based KV stores often make certain trade-offs and fail to simultaneously improve both the read and write performance on large KV stores without sacrificing scan performance. We design UniKV, which unifies the key design ideas of hash indexing and the LSM-tree in a single system. Specifically, UniKV leverages data locality to differentiate the indexing management of KV pairs. It also develops multiple techniques (e.g., merge with partial KV separation, dynamic range partitioning) to tackle the issues caused by unifying the indexing techniques, so as to simultaneously improve the performance in reads and writes. Furthermore, it proposes a parallel optimization scheme to manage partitions in parallel and develops multiple strategies to optimize the scan performance. Experiments show that UniKV significantly outperforms several state-of-the-art KV stores (e.g., LevelDB, RocksDB, PebblesDB and Titan) in overall throughput under read-write mixed workloads.},
  archive      = {J_TKDE},
  author       = {Qiang Zhang and Yongkun Li and Patrick P. C. Lee and Yinlong Xu},
  doi          = {10.1109/TKDE.2023.3234510},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11935-11949},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {The design and implementation of UniKV for mixed key-value storage workloads},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tangent space based alternating projections for nonnegative
low rank matrix approximation. <em>TKDE</em>, <em>35</em>(11),
11917–11934. (<a
href="https://doi.org/10.1109/TKDE.2022.3224052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a new alternating projection method to compute nonnegative low rank matrix approximation for nonnegative matrices. In the nonnegative low rank matrix approximation method, the projection onto the manifold of fixed rank matrices can be expensive as the singular value decomposition is required. We propose to use the tangent space of the point in the manifold to approximate the projection onto the manifold in order to reduce the computational cost. We show that the sequence generated by the alternating projections onto the tangent spaces of the fixed rank matrices manifold and the nonnegative matrix manifold, converge linearly to a point in the intersection of the two manifolds where the convergent point is sufficiently close to optimal solutions. This convergence result based inexact projection onto the manifold is new and is not studied in the literature. Numerical examples in data clustering, pattern recognition and hyperspectral data analysis are given to demonstrate that the performance of the proposed method is better than that of nonnegative matrix factorization methods in terms of computational time and accuracy.},
  archive      = {J_TKDE},
  author       = {Guangjing Song and Michael K. Ng and Tai-Xiang Jiang},
  doi          = {10.1109/TKDE.2022.3224052},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11917-11934},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Tangent space based alternating projections for nonnegative low rank matrix approximation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SGSI – a scalable GPU-friendly subgraph isomorphism
algorithm. <em>TKDE</em>, <em>35</em>(11), 11899–11916. (<a
href="https://doi.org/10.1109/TKDE.2022.3230744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inherent hardness of subgraph isomorphism, the performance is often a bottleneck in various real-world applications. We address this by designing an efficient subgraph isomorphism algorithm leveraging features of GPU architecture. Existing GPU-based solutions adopt two-step output scheme, performing the same join twice in order to write intermediate results concurrently. They also lack GPU architecture-aware optimizations that allow scaling to large graphs. In this article, we propose a S calable G PU-friendly s ubgraph i somorphism algorithm, SGSI . SGSI incorporates a Prealloc-Combine strategy based on the vertex-oriented framework, which avoids joining-twice in existing solutions. It uses a GPU-friendly data structure (called PCSR ) to represent an edge-labeled graph. We also study fine-grained load balance strategies and discuss how to handle enormous graphs that cannot be resident in GPU memory. A partition-based pipeline framework is proposed. Extensive experiments on both synthetic and real graphs show that SGSI outperforms the state-of-the-art algorithms by up to several orders of magnitude and has a good scalability with graph size scaling to billions of edges.},
  archive      = {J_TKDE},
  author       = {Li Zeng and Lei Zou and M. Tamer Özsu},
  doi          = {10.1109/TKDE.2022.3230744},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11899-11916},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SGSI – a scalable GPU-friendly subgraph isomorphism algorithm},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Set reconciliation using ternary and invertible bloom
filters. <em>TKDE</em>, <em>35</em>(11), 11885–11898. (<a
href="https://doi.org/10.1109/TKDE.2023.3237009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Set reconciliation between different hosts to hold the same dataset is an important prerequisite in numerous distributed applications. Sending the entire dataset to achieve set reconciliation is inefficient if the majority of the data owned by each host is the same. Each host is required to send exclusive elements uniquely included in its set to the other to minimize communication complexity. This paper proposes an efficient algorithm for a host to identify its exclusive elements using the recursive comparison of ternary Bloom filters, each representing the signature of a subset of elements and used for filtering out subsets with identical elements. Hence, subsets with exclusive elements are identified, and elements included in the identified subsets are programmed to an invertible Bloom filter (IBF) to be sent to the other host. Thus, the number of elements programmed in the IBF is significantly reduced. Simulation results show that the proposed algorithm provides excellent performance compared with existing set reconciliation algorithms under the constraint of the same amount of data communication.},
  archive      = {J_TKDE},
  author       = {Seungeun Lee and Hayoung Byun and Hyesook Lim},
  doi          = {10.1109/TKDE.2023.3237009},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11885-11898},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Set reconciliation using ternary and invertible bloom filters},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic and structural view fusion modeling for social
recommendation. <em>TKDE</em>, <em>35</em>(11), 11872–11884. (<a
href="https://doi.org/10.1109/TKDE.2022.3230972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies have shown that user-item interaction data and social relation data can be jointly used for enhancing the performance of social recommendation. However, limited research has a focus on investigating how to deeply exploit different views of social interaction structures and rating behavior differences for further improving social recommendation. To this end, in this paper, we propose to integrate information from both semantic and structural views for social recommendation. Specifically, we first design a collective intelligence-based strategy to reveal high-quality implicit relations for both users and items. Then, by reformulating all available nodes and relations as a heterogeneous graph, we define multiple semantic metapaths to capture diverse preferences for comprehensive user and item representations. While various metapaths enlarge the representation capacity of users and items, they also introduce noise and irrelevant information. We recall that, for the user-item interaction graph, different structure sizes (e.g., local and global structures) provide diverse and complementary information for recommendation. Motivated by this, we propose a semantic and structural view fusion framework for social recommendation (S4Rec), which consists of a deep graph model and a wide attentive SVD (Singular Value Decomposition) model for rating prediction by taking the local and global structure as input and aggregating messages along the predefined metapaths. Finally, the two predicted results are adaptively fused to achieve the final both accurate and stable prediction. In addition, we treat the user&#39;s rating behavior difference as the relative position difference problem in the embedding space, and model it with TransH to improve the generalization ability of the main rating model. Extensive experiments on three open datasets demonstrate the superiority of our framework compared with state-of-the-art methods. Particularly, our model outperforms other baselines under different sparsity conditions, further validating the effectiveness on cold-start users. We release the source code at https://github.com/lcwy220/Social-Recommendation .},
  archive      = {J_TKDE},
  author       = {Kun Yuan and Guannan Liu and Junjie Wu and Hui Xiong},
  doi          = {10.1109/TKDE.2022.3230972},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11872-11884},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semantic and structural view fusion modeling for social recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised hypergraph representation learning for
sociological analysis. <em>TKDE</em>, <em>35</em>(11), 11860–11871. (<a
href="https://doi.org/10.1109/TKDE.2023.3235312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern sociology has profoundly uncovered many convincing social criteria for behavioral analysis. Unfortunately, many of them are too subjective to be measured and very challenging to be presented in online social networks (OSNs) for the large data volume and complicated environments to be explored. On the other hand, data mining techniques can better find data patterns but many of them leave behind unnatural understanding to humans. Although there are some works trying to integrate social observations for specific tasks, they are still hard to be applied to more general cases. In this paper, we propose a fundamental methodology to support the further fusion of data mining techniques and sociological behavioral criteria. Our highlights are three-fold: First, we propose an effective hypergraph awareness and a fast line graph construction framework. The hypergraph can more profoundly indicate the interactions between individuals and their environments because each edge in the hypergraph (a.k.a hyperedge) contains more than two nodes, which is perfect to describe social. A line graph treats each social environment as a super node with the underlying influence between different environments. In this way, we go beyond traditional pair-wise relations and explore richer patterns under various sociological criteria; Second, we propose a novel hypergraph-based neural network to learn social influence flowing from users to users, users to environments, environment to users, and environments to environments. The neural network can be learned via a task-free method, making our model very flexible to support various data mining tasks and sociological analysis; Third, we propose both qualitative and quantitive solutions to effectively evaluate the most common sociological criteria like social conformity, social equivalence, environmental evolving and social polarization. Our extensive experiments show that our framework can better support both data mining tasks for online user behaviors and sociological analysis.},
  archive      = {J_TKDE},
  author       = {Xiangguo Sun and Hong Cheng and Bo Liu and Jia Li and Hongyang Chen and Guandong Xu and Hongzhi Yin},
  doi          = {10.1109/TKDE.2023.3235312},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11860-11871},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Self-supervised hypergraph representation learning for sociological analysis},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust label and feature space co-learning for multi-label
classification. <em>TKDE</em>, <em>35</em>(11), 11846–11859. (<a
href="https://doi.org/10.1109/TKDE.2022.3232114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification remains a challenging task for high-dimensional data samples and their labels both increase the complexity of training models. In this paper, we propose a Robust Label and Feature Space Co-Learning method, referred to as RLFSCL, for multi-label classification. Different from traditional multi-label classification methods which focus on feature space learning through regression directly between data samples and labels, our proposed method can further learn robust low rank label space from this traditional regression method. Therefore, our RLFSCL can learn better low rank feature and label representations simultaneously in original noisy and high dimensional spaces. Experimental comparison on five benchmark datasets, including Rcv1s5, Cal500, and Corel16k4 shows that the proposed RLFSCL algorithm outperforms state-of-the-art multi-label classification methods. The code of RLFSCL is made available on https://github.com/JingChuanTang/RLFSCL .},
  archive      = {J_TKDE},
  author       = {Zhifeng Liu and Chuanjing Tang and Stanley Ebhohimhen Abhadiomhen and Xiang-Jun Shen and Yangyang Li},
  doi          = {10.1109/TKDE.2022.3232114},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11846-11859},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust label and feature space co-learning for multi-label classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting embedding based graph analyses: Hyperparameters
matter! <em>TKDE</em>, <em>35</em>(11), 11830–11845. (<a
href="https://doi.org/10.1109/TKDE.2022.3230743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embeddings have been widely used for many graph analysis tasks. Mainstream factorization-based and graph-sampling-based embedding learning schemes both involve many hyperparameters and design choices. However, existing techniques often adopt some heuristics for these hyperparameters and design choices with little investigation into their impact, making it unclear what is the exact performance gains of these techniques on graph analysis tasks. Against this background, this paper presents a systematic study on the impact of an extensive list of hyperparameters for both factorization-based and graph-sampling-based graph embedding techniques for homogeneous graphs. We design generalized factorization-based and graph-sampling-based techniques involving these hyperparameters, and conduct a comprehensive set of experiments with over 3,000 embedding models trained and evaluated per dataset. We reveal that much of the performance gains are indeed due to optimal hyperparameter settings/design choices rather than the sophistication of embedding models; appropriate hyperparameter settings for typical embedding techniques can outperform a sizeable collection of 18 state-of-the-art graph embedding techniques by 0.30-35.41\% across different tasks. Moreover, we find that there is no one-size-fits-all hyperparameter setting across tasks, but we can indeed provide a list of task-specific practical recommendations for these hyperparameter settings/design choices, which we believe can serve as important guidelines for future research on embedding based graph analyses.},
  archive      = {J_TKDE},
  author       = {Dingqi Yang and Bingqing Qu and Rana Hussein and Paolo Rosso and Philippe Cudré-Mauroux and Jie Liu},
  doi          = {10.1109/TKDE.2022.3230743},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11830-11845},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Revisiting embedding based graph analyses: Hyperparameters matter!},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023g). Rethink the linearizability constraints of raft for
distributed systems. <em>TKDE</em>, <em>35</em>(11), 11815–11829. (<a
href="https://doi.org/10.1109/TKDE.2023.3235399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the deployment of modern hardware such as Flash-based SSDs and the high-speed network in distributed systems, the distributed consensus and consistency module (e.g., Raft) is typically the most time-consuming part. The reason lies in that Raft introduces some very strict constraints to ensure the linearizability. Therefore, in this paper, we rethink these constraints in-depth and find that some of them are not necessary, and can be broken to accelerate the performance significantly without breaking the linear consistency for distributed systems. An improved distributed consensus algorithm called BUC-Raft (Breaking Unnecessary Constraints of Raft) is proposed in this paper and implemented in an industry-level distributed system. The experimental results suggest that both the write and the read performance can be accelerated significantly by BUC-Raft.},
  archive      = {J_TKDE},
  author       = {Yangyang Wang and Zikai Wang and Yunpeng Chai and Xin Wang},
  doi          = {10.1109/TKDE.2023.3235399},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11815-11829},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Rethink the linearizability constraints of raft for distributed systems},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Reinforcement learning based path exploration for
sequential explainable recommendation. <em>TKDE</em>, <em>35</em>(11),
11801–11814. (<a
href="https://doi.org/10.1109/TKDE.2023.3237741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in path-based explainable recommendation systems have attracted increasing attention thanks to the rich information from knowledge graphs. Most existing explainable recommendations only utilize static knowledge graphs and ignore the dynamic user-item evolutions, leading to less convincing and inaccurate explanations. Although some works boost the performance and explainability of recommendations through modeling the user&#39;s temporal sequential behavior, most of them either only focus on modeling the user&#39;s sequential interactions within a path or independently and separately of the recommendation mechanism. Moreover, some path-based explainable recommendations use random selection or traditional machine learning methods to decrease the volume of explainable paths, which cannot guarantee high quality of the explainable paths for the recommendation. To deal with the problem, recent path exploration use reinforcement learning to improve diversity and quality. However, unsupervised training leads to low-efficiency path exploration. Therefore, we propose a novel T emporal M eta-path Guided E xplainable R ecommendation leveraging R einforcement L earning ( TMER-RL ), which utilizes supervised reinforcement learning to explore item-item paths between consecutive items with attention mechanisms to sequentially model dynamic user-item evolutions on a dynamic knowledge graph for the explainable recommendation. Extensive evaluations of TMER-RL on two real-world datasets show state-of-the-art performance compared to recent strong baselines.},
  archive      = {J_TKDE},
  author       = {Yicong Li and Hongxu Chen and Yile Li and Lin Li and Philip S. Yu and Guandong Xu},
  doi          = {10.1109/TKDE.2023.3237741},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11801-11814},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reinforcement learning based path exploration for sequential explainable recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Process discovery on deviant traces and other stranger
things. <em>TKDE</em>, <em>35</em>(11), 11784–11800. (<a
href="https://doi.org/10.1109/TKDE.2022.3232207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the need to understand and formalise business processes into a model has grown over the last years, the process discovery research field has gained more and more importance, developing two different classes of approaches to model representation: procedural and declarative. Orthogonally to this classification, the vast majority of works envisage the discovery task as a one-class supervised learning process guided by the traces that are recorded into an input log. In this work instead, we focus on declarative processes and embrace the less-popular view of process discovery as a binary supervised learning task, where the input log reports both examples of the normal system execution, and traces representing a “stranger” behaviour according to the domain semantics. We therefore deepen how the valuable information brought by both these two sets can be extracted and formalised into a model that is “optimal” according to user-defined goals. Our approach, namely NegDis , is evaluated w.r.t. other relevant works in this field, and shows promising results regarding both the performance and the quality of the obtained solution.},
  archive      = {J_TKDE},
  author       = {Federico Chesani and Chiara Di Francescomarino and Chiara Ghidini and Daniela Loreti and Fabrizio Maria Maggi and Paola Mello and Marco Montali and Sergio Tessaris},
  doi          = {10.1109/TKDE.2022.3232207},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11784-11800},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Process discovery on deviant traces and other stranger things},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Patient health representation learning via correlational
sparse prior of medical features. <em>TKDE</em>, <em>35</em>(11),
11769–11783. (<a
href="https://doi.org/10.1109/TKDE.2022.3230454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting the correlations between medical features is essential to the success of healthcare data analysis. However, most existing methods are either suffering large estimation variance for data insufficiency or inflexible in terms of demanding task-specific medical knowledge. In this paper, we propose a novel patient health representation learning framework dubbed SAFARI . SAFARI learns a compact representation by imposing a clinical-fact-inspired task-agnostic correlational sparsity prior to the correlations of medical feature pairs. Specifically, we learn the compact representation by solving the bi-level optimization problem, which involves solving the high-level inter-group correlations and the nested lower-level intra-group correlations. We leverage the Laplacian kernel as a robust metric for feature grouping and graph neural networks for solving the bi-level optimization problem following the optimal value reformulation paradigm. Experiments on five datasets of various inputs and tasks demonstrate the efficacy of SAFARI . The discovered findings are also consistent with our insights and medical literature, which can provide valuable clinical explanations.},
  archive      = {J_TKDE},
  author       = {Xinyu Ma and Yasha Wang and Xu Chu and Liantao Ma and Wen Tang and Junfeng Zhao and Ye Yuan and Guoren Wang},
  doi          = {10.1109/TKDE.2022.3230454},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11769-11783},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Patient health representation learning via correlational sparse prior of medical features},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel hub labeling maintenance with high efficiency in
dynamic small-world networks. <em>TKDE</em>, <em>35</em>(11),
11751–11768. (<a
href="https://doi.org/10.1109/TKDE.2023.3236632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortest path computation is a fundamental operation in many application domains and is especially challenging in frequently evolving small-world networks (i.e., graphs in which many nodes can be reached from every other node by a small number of hops). Index-based methods, especially ones based on 2-hop labeling are often used for high query efficiency. However, the evolvements of small-world networks in many realistic scenarios pose the challenge of efficient maintenance of the shortest path index. In this work, we adopt the state-of-the-art Parallel Shortest-distance Labeling (PSL) as the underlying 2-hop labeling construction method, and design algorithms to support its efficient update given edge weight changes (increase and decrease). Specifically, we focus on weighted PSL (WPSL) and propose a propagation-based update mechanism for both synchronous and asynchronous propagation. We also identify the curse of pruning power in the edge weight increase case, and solve it with a balance between index size and effectiveness. Followed by, we extend the asynchronous propagation method to Pruned Landmark Labeling (PLL) for faster index maintenance and query processing with a smaller index size. Finally, we further optimize the index performance by reducing the index size through graph contraction and accelerating the index update through parallelized mix index update. Our experimental results on real-life and synthetic networks demonstrate the superiority of our algorithms over the relevant baselines on index maintenance.},
  archive      = {J_TKDE},
  author       = {Mengxuan Zhang and Lei Li and Goce Trajcevski and Andreas Züfle and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2023.3236632},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11751-11768},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Parallel hub labeling maintenance with high efficiency in dynamic small-world networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Outdoor position recovery from heterogeneous telco cellular
data. <em>TKDE</em>, <em>35</em>(11), 11736–11750. (<a
href="https://doi.org/10.1109/TKDE.2022.3232361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed unprecedented amounts of data generated by telecommunication (Telco) cellular networks. For example, measurement records (MRs) are generated to report the connection states between mobile devices and Telco networks, e.g., received signal strength. MR data have been widely used to localize outdoor mobile devices for human mobility analysis, urban planning, and traffic forecasting. Existing works using first-order sequence models such as the Hidden Markov Model (HMM) attempt to capture spatio-temporal locality in underlying mobility patterns for lower localization errors. The HMM approaches typically assume stable mobility patterns of the underlying mobile devices. Yet real MR datasets exhibit heterogeneous mobility patterns due to mixed transportation modes of the underlying mobile devices and uneven distribution of the positions associated with MR samples. Thus, the existing solutions cannot handle these heterogeneous mobility patterns. To this end, we propose a multi-task learning-based deep neural network (DNN) framework, namely ${{\sf PRNet}}$ $^+$ , to incorporate outdoor position recovery and transportation mode detection. To make sure that ${{\sf PRNet}}$ $^+$ can work, we develop a feature extraction module to precisely learn local-, short- and long-term spatio-temporal locality from heterogeneous MR samples. Extensive evaluation on eight datasets collected at three representative areas in Shanghai indicates that ${{\sf PRNet}}$ ${^{+}}$ greatly outperforms state-of-the-arts by lower localization errors.},
  archive      = {J_TKDE},
  author       = {Yige Zhang and Weixiong Rao and Kun Zhang and Lei Chen},
  doi          = {10.1109/TKDE.2022.3232361},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11736-11750},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Outdoor position recovery from heterogeneous telco cellular data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OPR-miner: Order-preserving rule mining for time series.
<em>TKDE</em>, <em>35</em>(11), 11722–11735. (<a
href="https://doi.org/10.1109/TKDE.2022.3224963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering frequent trends in time series is a critical task in data mining. Recently, order-preserving matching was proposed to find all occurrences of a pattern in a time series, where the pattern is a relative order (regarded as a trend) and an occurrence is a sub-time series whose relative order coincides with the pattern. Inspired by the order-preserving matching, the existing order-preserving pattern (OPP) mining algorithm employs order-preserving matching to calculate the support, which leads to low efficiency. To address this deficiency, this paper proposes an algorithm called efficient frequent OPP miner (EFO-Miner) to find all frequent OPPs. EFO-Miner is composed of four parts: a pattern fusion strategy to generate candidate patterns, a matching process for the results of sub-patterns to calculate the support of super-patterns, a screening strategy to dynamically reduce the size of prefix and suffix arrays, and a pruning strategy to further dynamically prune candidate patterns. Moreover, this paper explores the order-preserving rule (OPR) mining and proposes an algorithm called OPR-Miner to discover strong rules from all frequent OPPs using EFO-Miner. Experimental results verify that OPR-Miner gives better performance than other competitive algorithms. More importantly, clustering and classification experiments further validate that OPR-Miner achieves good performance.},
  archive      = {J_TKDE},
  author       = {Youxi Wu and Xiaoqian Zhao and Yan Li and Lei Guo and Xingquan Zhu and Philippe Fournier-Viger and Xindong Wu},
  doi          = {10.1109/TKDE.2022.3224963},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11722-11735},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {OPR-miner: Order-preserving rule mining for time series},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Opinion summarization via submodular information measures.
<em>TKDE</em>, <em>35</em>(11), 11708–11721. (<a
href="https://doi.org/10.1109/TKDE.2023.3235337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on opinion summarization for constructing subjective and concise summaries representing essential opinions of online text reviews. As previous works rarely focus on the relationship between opinions, topics, and sentences, we propose a set of new requirements for Opinion-Topic-Sentence, which are essential for performing opinion summarization. We prove that Opinion-Topic-Sentence can be theoretically analyzed by submodular information measures. Thus, our proposed method can reduce redundant information, strengthen the relevance to given topics, and informatively represent the underlying emotional variations. While conventional methods require human-labeled topics for extractive summarization, we use unsupervised topic modeling methods to generate topic features. We propose four submodular functions and two optimization algorithms with proven performance bounds that can maximize opinion summarization&#39;s utility. An automatic evaluation metric, Topic-based Opinion Variance, is also derived to compensate for ROUGE-based metrics of opinion summarization evaluation. Four large, diversified, and representative corpora, OPOSUM, Opinosis, Yelp, and Amazon reviews, are used in our study. The results on these online review texts corroborate the efficacy of our proposed metric and framework.},
  archive      = {J_TKDE},
  author       = {Yang Zhao and Tommy W. S. Chow},
  doi          = {10.1109/TKDE.2023.3235337},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11708-11721},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Opinion summarization via submodular information measures},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online feature screening for data streams with concept
drift. <em>TKDE</em>, <em>35</em>(11), 11693–11707. (<a
href="https://doi.org/10.1109/TKDE.2022.3232752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screening feature selection methods are often used as a preprocessing step for reducing the number of variables before training a model. Traditional screening methods only focus on dealing with complete high dimensional datasets. However, modern datasets not only have higher dimensions and larger sample size, but also have properties such as streaming input, sparsity, and concept drift. Therefore a considerable number of online feature selection methods were introduced to handle these kinds of problems in recent years. Online screening methods are one of the categories of online feature selection methods. The methods that we propose in this paper are capable of handling all three situations mentioned above, in classification settings. Our experiments show that the proposed methods can generate the same feature importance as their offline versions with faster speed and less storage requirements. Furthermore, the results show that online screening methods with integrated model adaptation have a higher true feature detection rate than without model adaptation on data streams exhibiting concept drift. Among the three large real datasets that potentially have concept drift, online screening methods with model adaptation show advantages in either saving computation time and space, reducing model complexity, or improving prediction accuracy.},
  archive      = {J_TKDE},
  author       = {Mingyuan Wang and Adrian Barbu},
  doi          = {10.1109/TKDE.2022.3232752},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11693-11707},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online feature screening for data streams with concept drift},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). On the substructure countability of graph neural networks.
<em>TKDE</em>, <em>35</em>(11), 11681–11692. (<a
href="https://doi.org/10.1109/TKDE.2022.3223471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the empirical success of Graph Neural Networks (GNNs) on graph-related tasks, it is intriguing to investigate their theoretical power on these tasks. In this paper, we focus on GNNs’ theoretical power on substructure counting, a fundamental yet challenging task in many applications. Previous works have proven that the 2-dimensional Weisfeiler-Leman algorithm (2-WL) equivalent GNNs can only count limited substructures. However, the substructure counting ability of theoretically more powerful and computationally tractable GNNs remains unclear. In this paper, we study conditions for substructures to be theoretically countable by k-WL equivalent GNNs, and then focus on 3-WL equivalent ones, which are currently the most theoretically powerful instances with practical computational cost. Further, we propose an algorithm to determine the countability of substructures for 3-WL equivalent GNNs. Our results reveal that 3-WL equivalent GNNs can count considerably more substructures than 2-WL equivalent ones. However, the proportion of countable patterns and prediction performance decrease as the pattern size increases. Therefore, we propose a Layer Permutation Pooling (LPP) model for better substructure counting performance. LPP first decomposes the data graph into subgraphs. Then we propose a layer permutation scheme to represent each decomposed subgraph as a set of matrices. Finally, LPP utilizes a neural network to conduct predictions on matrices. We compare LPP with several state-of-the-art GNNs on various datasets. Experimental results show that LPP outperforms baselines by 84\% on average with the RMSE metric.},
  archive      = {J_TKDE},
  author       = {Wenwen Xia and Yuchen Li and Shenghong Li},
  doi          = {10.1109/TKDE.2022.3223471},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11681-11692},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On the substructure countability of graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Near-optimal scheduling for crowdsourced transit system
with skip-stop tactic. <em>TKDE</em>, <em>35</em>(11), 11668–11680. (<a
href="https://doi.org/10.1109/TKDE.2022.3223553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient bus scheduling is a crucial component for the improvement of public transit services. Without systematic optimization and efficient shift arrangement, the bus scheduling system may suffer from poor vehicle loading rate or crowd onboard, resulting in wasted energy and passenger dissatisfaction. In this paper, we consider a crowdsourced bus service system (on a fixed route) that receives user requests as input and computes the scheduling of buses with flexible departure time and skip-stop to minimize the travel time of users. We first show that the general problem of computing the optimal scheduling is NP-hard. On the other hand, for the case when skip-stop is not adopted, we propose the Optimized Departure Time (ODT) algorithm that computes optimal scheduling. Our algorithm is built on an innovative reduction of the problem to some variants of the k-clustering problem and an efficient application of dynamic programming. On top of ODT, we further improve the effectiveness of the solution by utilizing the power of skip-stop tactic, named ODTS. Our experimental results demonstrate that ODT and ODTS dramatically outperform existing algorithms for the bus scheduling problem in terms of effectiveness and efficiency. Moreover, the solutions given by ODTS are very close to the optimum.},
  archive      = {J_TKDE},
  author       = {Hanlin Li and Xiaowei Wu and Leong Hou U and Kun Pang Kou},
  doi          = {10.1109/TKDE.2022.3223553},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11668-11680},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Near-optimal scheduling for crowdsourced transit system with skip-stop tactic},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view partial multi-label learning via
graph-fusion-based label enhancement. <em>TKDE</em>, <em>35</em>(11),
11656–11667. (<a
href="https://doi.org/10.1109/TKDE.2022.3232482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view partial multi-label learning (MVPML) aims to learn a multi-label predictive model from the training examples, each of which is presented by multiple feature vectors while associated with a set of candidate labels where only a subset is correct. Generally, existing techniques work simply by identifying the ground-truth label via aggregating the features from all views to train a final classifier, but ignore the cause of the incorrect labels in the candidate label sets, i.e., the diverse property of the representation from different views leads to the incorrect labels which form the candidate labels alone with the essential supervision. In this paper, a novel MVPML approach is proposed to learn the predictive model and the incorrect-labeling model jointly by incorporating the graph-fusion-based topological structure of the feature space. Specifically, the latent label distribution and the incorrect labels are identified simultaneously in a unified framework under the supervision of candidate labels. In addition, a common topological structure of the feature space from all views is learned via the graph fusion for further capturing the latent label distribution. Experimental results on the real-world datasets clearly validate the effectiveness of the proposed approach for solving multi-view partial multi-label learning problems.},
  archive      = {J_TKDE},
  author       = {Ning Xu and Yong-Di Wu and Congyu Qiao and Yi Ren and Minxue Zhang and Xin Geng},
  doi          = {10.1109/TKDE.2022.3232482},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11656-11667},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view partial multi-label learning via graph-fusion-based label enhancement},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multi-view fuzzy classification with subspace clustering
and information granules. <em>TKDE</em>, <em>35</em>(11), 11642–11655.
(<a href="https://doi.org/10.1109/TKDE.2022.3231929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning becomes increasingly attractive and promising because multimodal or multi-view data are commonly encountered in real-world applications. In this study, we develop a novel multi-view Takagi–Sugeno–Kang (TSK) fuzzy system framework to handle classification problems for such data. We propose an anchor and graph subspace clustering strategy to discover and represent the actual latent data distribution for each view separately. In this way, the discriminate anchors (landmarks) are learned to capture the main structure of the multi-view data. This strategy also provides a computationally efficient clustering algorithm with respect to the number of instances. These resulting anchors are formed as the prototypes of information granules (IGs) for fuzzy modeling. Then we construct an information-granule-based multi-view TSK fuzzy classification model inherited from the natural interpretability of fuzzy rule-based systems. Concretely, the relationship between the multi-view input and label output spaces is depicted by IGs-oriented fuzzy rules. The experimental studies involve various commonly used benchmark datasets, which indicate that our proposed method achieves comparable or better performance compared to the state-of-the-art algorithms.},
  archive      = {J_TKDE},
  author       = {Xingchen Hu and Xinwang Liu and Witold Pedrycz and Qing Liao and Yinghua Shen and Yan Li and Siwei Wang},
  doi          = {10.1109/TKDE.2022.3231929},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11642-11655},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view fuzzy classification with subspace clustering and information granules},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multi-task weakly supervised learning for
origin–destination travel time estimation. <em>TKDE</em>,
<em>35</em>(11), 11628–11641. (<a
href="https://doi.org/10.1109/TKDE.2023.3236060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Travel time estimation from GPS trips is of great importance to order duration, ridesharing, taxi dispatching, etc. However, the dense trajectory is not always available due to the limitation of data privacy and acquisition, while the origin-destination (OD) type of data, such as NYC taxi data, NYC bike data, and Capital Bikeshare data, is more accessible. To address this issue, this paper starts to estimate the OD trips travel time combined with the road network. Subsequently, a M ulti-task W eakly S upervised L earning Framework for T ravel T ime E stimation (MWSL-TTE) has been proposed to infer transition probability between roads segments, and the travel time on road segments and intersection simultaneously. Technically, given an OD pair, the transition probability intends to recover the most possible route. And then, the output of travel time is equal to the summation of all segments’ and intersections’ travel time in this route. A novel route recovery function has been proposed to iteratively maximize the current routes’ co-occurrence probability, and minimize the discrepancy between routes’ probability distribution and the inverse distribution of routes’ estimation loss. Moreover, the expected log-likelihood function based on a weakly-supervised framework has been deployed in optimizing the travel time from road segments and intersections concurrently. We conduct experiments on a wide range of real-world taxi datasets in Xi’an and Chengdu and demonstrate our method&#39;s effectiveness on route recovery and travel time estimation.},
  archive      = {J_TKDE},
  author       = {Hongjun Wang and Zhiwen Zhang and Zipei Fan and Jiyuan Chen and Lingyu Zhang and Ryosuke Shibasaki and Xuan Song},
  doi          = {10.1109/TKDE.2023.3236060},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11628-11641},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-task weakly supervised learning for Origin–Destination travel time estimation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale ensemble booster for improving existing TSD
classifiers. <em>TKDE</em>, <em>35</em>(11), 11610–11627. (<a
href="https://doi.org/10.1109/TKDE.2022.3230709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Series Classification (TSC) is an essential task in Time Series Data (TSD) analysis. Ensemble-based approaches now achieve the best performance on TSC tasks. However, integrating numerous different models makes them highly suffer from heavy preprocessing. Even worse, non-deep-learning ensemble-based methods suffer from substantial computational costs due to lacking GPU acceleration. Multi-scale information in TSD can improve TSC performance. However, Existing TSD classifiers employing multi-scale information struggle with heavy preprocessing and cannot help other TSD classifiers obtain multi-scale feature extraction capabilities. Inspired by these, we proposed a performance enhancement framework called multi-scale ensemble booster (MEB), helping existing TSD classifiers achieve performance leaps. In MEB, we proposed an easy-to-combine network structure without changing any of their structure and hyperparameters, only needed to set one hyperparameter, consisting of multi-scale transformation and multi-output decision fusion. Then, a probability distribution co-evolution strategy is proposed to attain the optimal label probability distribution. We conducted numerous ablation experiments of MEB on 128 univariate datasets and 29 multivariate datasets and comparative experiments with 11 state-of-the-art methods, which demonstrated the significant performance improvement ability of MEB and the most advanced performance of the model enhanced by MEB, respectively. Furthermore, to figure out why MEB can improve model performance, we provided a chain of interpretability analyses. https://github.com/foryichuanqi/Multi-Scale-Ensemble-Booster-for-Improving-Existing-Time-Series-Data-Classifiers .},
  archive      = {J_TKDE},
  author       = {Linchuan Fan and Yi Chai and Xiaolong Chen},
  doi          = {10.1109/TKDE.2022.3230709},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11610-11627},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-scale ensemble booster for improving existing TSD classifiers},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple-instance learning from unlabeled bags with pairwise
similarity. <em>TKDE</em>, <em>35</em>(11), 11599–11609. (<a
href="https://doi.org/10.1109/TKDE.2022.3232141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiple-instance learning (MIL), each training example is represented by a bag of instances. A training bag is either negative if it contains no positive instances or positive if it has at least one positive instance. Previous MIL methods generally assume that training bags are fully labeled. However, the exact labels of training examples may not be accessible, due to security, confidentiality, and privacy concerns. Fortunately, it could be easier for us to access the pairwise similarity between two bags (indicating whether two bags share the same label or not) and unlabeled bags, as we do not need to know the underlying label of each bag. In this paper, we provide the first attempt to investigate MIL from only similar-dissimilar-unlabeled bags. To solve this new MIL problem, we first propose a strong baseline method that trains an instance-level classifier by employing an unlabeled-unlabeled learning strategy. Then, we also propose to train a bag-level classifier based on a convex formulation and theoretically derive a generalization error bound for this method. Comprehensive experimental results show that our instance-level classifier works well, while our bag-level classifier even has better performance.},
  archive      = {J_TKDE},
  author       = {Lei Feng and Senlin Shu and Yuzhou Cao and Lue Tao and Hongxin Wei and Tao Xiang and Bo An and Gang Niu},
  doi          = {10.1109/TKDE.2022.3232141},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11599-11609},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multiple-instance learning from unlabeled bags with pairwise similarity},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). Modeling user demand evolution for next-basket prediction.
<em>TKDE</em>, <em>35</em>(11), 11585–11598. (<a
href="https://doi.org/10.1109/TKDE.2022.3231018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users’ purchase behaviors are complex and dynamic, which are usually driven by various personal demands evolving with time. According to psychology and economic theories, user demands can be satisfied with a sequence of purchase behaviors, resulting in a basket of items. However, most of the existing works simply predict the next basket from a shallow perspective of (purchase) sequence data modeling without deep insight into the underlying factors which drive user purchase behaviors. In fact, filling a basket with multiple items is a process to incrementally satisfy a user&#39;s demand. Therefore, the key challenges to predict a user&#39;s next basket lie in (1) how to track the changes of the user&#39;s demand, and (2) how to satisfy her demand at a given moment. To this end, we propose an Evolving DEmand SAtisfaction (EvoDESA) model to model a user&#39;s demand evolution for next-basket prediction. In EvoDESA, a demand evolution module learns the dynamics of user demand over a sequence of basket-purchase behaviors. Then, a next-basket planning module effectively packs an optimal combination of items to best satisfy the user&#39;s current demand. Extensive experiments on three real-world transaction datasets demonstrate the considerable superiority of EvoDESA over the state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Shoujin Wang and Yan Wang and Liang Hu and Xiuzhen Zhang and Qi Zhang and Quan Z. Sheng and Mehmet A. Orgun and Longbing Cao and Defu Lian},
  doi          = {10.1109/TKDE.2022.3231018},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11585-11598},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling user demand evolution for next-basket prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MetaVIM: Meta variationally intrinsic motivated
reinforcement learning for decentralized traffic signal control.
<em>TKDE</em>, <em>35</em>(11), 11570–11584. (<a
href="https://doi.org/10.1109/TKDE.2022.3232711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic signal control aims to coordinate traffic signals across intersections to improve the traffic efficiency of a district or a city. Deep reinforcement learning (RL) has been applied to traffic signal control recently and demonstrated promising performance where each traffic signal is regarded as an agent. However, there are still several challenges that may limit its large-scale application in the real world. On the one hand, the policy of the current traffic signal is often heavily influenced by its neighbor agents, and the coordination between the agent and its neighbors needs to be considered. Hence, the control of a road network composed of multiple traffic signals is naturally modeled as a multi-agent system, and all agents’ policies need to be optimized simultaneously. On the other hand, once the policy function is conditioned on not only the current agent&#39;s observation but also the neighbors’, the policy function would be closely related to the training scenario and cause poor generalizability because the agents in various scenarios often have heterogeneous neighbors. To make the policy learned from a training scenario generalizable to new unseen scenarios, a novel Meta Variationally Intrinsic Motivated (MetaVIM) RL method is proposed to learn the decentralized policy for each intersection that considers neighbor information in a latent way. Specifically, we formulate the policy learning as a meta-learning problem over a set of related tasks, where each task corresponds to traffic signal control at an intersection whose neighbors are regarded as the unobserved part of the state. Then, a learned latent variable is introduced to represent the task&#39;s specific information and is further brought into the policy for learning. In addition, to make the policy learning stable, a novel intrinsic reward is designed to encourage each agent&#39;s received rewards and observation transition to be predictable only conditioned on its own history. Extensive experiments conducted on CityFlow demonstrate that the proposed method substantially outperforms existing approaches and shows superior generalizability.},
  archive      = {J_TKDE},
  author       = {Liwen Zhu and Peixi Peng and Zongqing Lu and Yonghong Tian},
  doi          = {10.1109/TKDE.2022.3232711},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11570-11584},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MetaVIM: Meta variationally intrinsic motivated reinforcement learning for decentralized traffic signal control},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Low-rank tensor learning for incomplete multiview
clustering. <em>TKDE</em>, <em>35</em>(11), 11556–11569. (<a
href="https://doi.org/10.1109/TKDE.2022.3230964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multiview clustering (IMVC) is an effective way to identify the underlying structure of incomplete multiview data. Most existing algorithms based on matrix factorization, graph learning or subspace learning have at least one of the following limitations: (1) the global and local structures of high-dimensional data are not effectively explored simultaneously; (2) the high-order correlations among multiple views are ignored. In this article, we propose a low-rank tensor learning (LRTL) method that learns a consensus low-dimensional embedding matrix for IMVC. We first take advantage of the self-expressiveness property of high-dimensional data to construct sparse similarity matrices for individual views under low-rank and sparsity constraints. Individual low-dimensional embedding matrices can be obtained from the sparse similarity matrices using spectral embedding techniques. This approach simultaneously explores the global and local structures of incomplete multiview data. Then, we present a multiview embedding matrix fusion model that incorporates individual low-dimensional embedding matrices into a third-norm tensor to achieve a consensus low-dimensional embedding matrix. The fusion model exploits complementary information by finding the high-order correlations among multiple views. In addition, the computational cost of an improved fusion strategy is dramatically reduced. Extensive experimental results demonstrate that the proposed LRTL method outperforms several state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Jie Chen and Zhu Wang and Hua Mao and Xi Peng},
  doi          = {10.1109/TKDE.2022.3230964},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11556-11569},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Low-rank tensor learning for incomplete multiview clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging long short-term user preference in conversational
recommendation via multi-agent reinforcement learning. <em>TKDE</em>,
<em>35</em>(11), 11541–11555. (<a
href="https://doi.org/10.1109/TKDE.2022.3225109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommender systems (CRS) endow traditional recommender systems with the capability of dynamically obtaining users’ short-term preferences for items and attributes through interactive dialogues. There are three core challenges for CRS, including the intelligent decisions for what attributes to ask, which items to recommend, and when to ask or recommend, at each conversation turn. Previous methods mainly leverage reinforcement learning (RL) to learn conversational recommendation policies for solving one or two of these three decision-making problems in CRS with separated conversation and recommendation components. These approaches restrict the scalability and generality of CRS and fall short of preserving a stable training procedure. In the light of these challenges, we tackle these three decision-making problems in CRS as a unified policy learning task. In order to leverage different features that are important to each sub-problem and facilitate better unified policy learning in CRS, we propose two novel multi-agent RL-based frameworks, namely Independent and Hierarchical Multi-Agent UNIfied COnversational RecommeNders (IMA-UNICORN and HMA-UNICORN), respectively. In specific, two low-level agents enrich the state representations for attribute prediction and item recommendation, by combining the long-term user preference information from the historical interaction data and the short-term user preference information from the conversation history. A high-level meta agent is responsible for coordinating the low-level agents to adaptively make the final decision. Experimental results on four benchmark CRS datasets and a real-world E-Commerce application show that the proposed frameworks significantly outperform state-of-the-art methods. Extensive analyses further demonstrate the superior scalability of the MARL frameworks on the multi-round conversational recommendation.},
  archive      = {J_TKDE},
  author       = {Yang Deng and Yaliang Li and Bolin Ding and Wai Lam},
  doi          = {10.1109/TKDE.2022.3225109},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11541-11555},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Leveraging long short-term user preference in conversational recommendation via multi-agent reinforcement learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label-enhanced graph neural network for semi-supervised node
classification. <em>TKDE</em>, <em>35</em>(11), 11529–11540. (<a
href="https://doi.org/10.1109/TKDE.2022.3231660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have been widely applied in the semi-supervised node classification task, where a key point lies in how to sufficiently leverage the limited but valuable label information. Most of the classical GNNs solely use the known labels for computing the classification loss at the output. In recent years, several methods have been designed to additionally utilize the labels at the input. One part of the methods augment the node features via concatenating or adding them with the one-hot encodings of labels, while other methods optimize the graph structure by assuming neighboring nodes tend to have the same label. To bring into full play the rich information of labels, in this article we present a label-enhanced learning framework for GNNs, which first models each label as a virtual center for intra-class nodes and then jointly learns the representations of both nodes and labels. Our approach could not only smooth the representations of nodes belonging to the same class, but also explicitly encode the label semantics into the learning process of GNNs. Moreover, a training node selection technique is provided to eliminate the potential label leakage issue and guarantee the model generalization ability. Finally, an adaptive self-training strategy is proposed to iteratively enlarge the training set with more reliable pseudo labels and distinguish the importance of each pseudo-labeled node during the model training process. Experimental results on both real-world and synthetic datasets demonstrate our approach can not only consistently outperform the state-of-the-arts, but also effectively smooth the representations of intra-class nodes.},
  archive      = {J_TKDE},
  author       = {Le Yu and Leilei Sun and Bowen Du and Tongyu Zhu and Weifeng Lv},
  doi          = {10.1109/TKDE.2022.3231660},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11529-11540},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Label-enhanced graph neural network for semi-supervised node classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kill two birds with one stone: A multi-view
multi-adversarial learning approach for joint air quality and weather
prediction. <em>TKDE</em>, <em>35</em>(11), 11515–11528. (<a
href="https://doi.org/10.1109/TKDE.2023.3236423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and timely air quality and weather predictions are of great importance to urban governance and human livelihood. Though many efforts have been made for air quality or weather prediction, most of them simply employ one another as feature input, which ignores the inner-connection between two predictive tasks. On one hand, the accurate prediction of one task can help improve another task&#39;s performance. On the other hand, geospatially distributed air quality and weather monitoring stations provide additional hints for city-wide spatiotemporal dependency modeling. Inspired by the above two insights, in this paper, we propose a multi-view multi-adversarial approach, entitled MasterGNN $^{+}$ , to jointly predict air quality and weather conditions. First, we devise a multi-view graph learning block to model spatial autocorrelation based on geographical distance and environmental context. Then, a dedicated evolved recurrent network is proposed to dynamically capture the long-range and independent temporal autocorrelation for each monitoring station and time slot. After that, we develop a multi-adversarial graph learning framework to against observation noise propagation introduced by spatiotemporal modeling. Moreover, we present an adaptive training strategy by formulating multi-adversarial learning as a multi-task learning problem. Finally, extensive experiments on two real-world datasets show that MasterGNN $^{+}$ achieves the best performance compared with seven baselines on both air quality and weather prediction tasks.},
  archive      = {J_TKDE},
  author       = {Jindong Han and Hao Liu and Hengshu Zhu and Hui Xiong},
  doi          = {10.1109/TKDE.2023.3236423},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11515-11528},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Kill two birds with one stone: A multi-view multi-adversarial learning approach for joint air quality and weather prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Independent relation representation with line graph for
cross-lingual entity alignment. <em>TKDE</em>, <em>35</em>(11),
11503–11514. (<a
href="https://doi.org/10.1109/TKDE.2022.3232138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-lingual entity alignment, which is an important task in the field of graph mining, aims to find equivalent entity pairs from two knowledge graphs. Recent methods show that relation representation can be used to improve entity representation and entity alignment. However, relation representation is learned dependently on entity representation, and both representations are learned from node-centered knowledge graphs. This dependency of relation on entity results in poor relation representation and leads to limited enhancement of entity representation. Therefore, to address this challenge, we propose novel relation-aware line graph neural networks for cross-lingual entity alignment (RALG). More specifically, we first propose to learn relation representation with heterogeneous line graphs independently from entities. The constructed heterogeneous line graphs can capture the correlation of relations explicitly. Second, we design a new way of aggregation in the form of triples to strengthen the relevance between entities and their corresponding relations. Experiments conducted on real-world datasets show that independent learning of relation representation with line graphs can represent relations better, and our method achieves better performance than the state-of-the-art methods for entity alignment.},
  archive      = {J_TKDE},
  author       = {Yuhong Zhang and Jianqing Wu and Kui Yu and Xindong Wu},
  doi          = {10.1109/TKDE.2022.3232138},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11503-11514},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Independent relation representation with line graph for cross-lingual entity alignment},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hyperbolic temporal network embedding. <em>TKDE</em>,
<em>35</em>(11), 11489–11502. (<a
href="https://doi.org/10.1109/TKDE.2022.3232398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal networks arise in various real-world scenarios, including social networks, user-item networks, traffic networks, financial transaction networks, etc. Modeling the dynamics of temporal networks is of importance as it describes how the networks evolve, which helps to understand and predict the behavior of the systems. There has been a lot of research on temporal network representation learning so far. Nonetheless, most of them are based on euclidean geometry, which fails to encode the underlying hierarchical layout or scale-free property of the real-world temporal network. Encouragingly, hyperbolic geometry excels in preserving both node similarity and network hierarchies. In the preliminary work, we proposed a hyperbolic temporal graph network (HTGN) on the Poincaré ball model, taking advantage of the exponential capacity and hierarchical awareness of hyperbolic geometry. HTGN moves the temporal network embedding into hyperbolic space and employs the hyperbolic graph neural network and hyperbolic gated recurrent neural network to capture spatial and temporal dynamics, respectively. In addition, two modules were further put forward to advance the performance: (1) hyperbolic temporal contextual self-attention to watch historical states and (2) hyperbolic temporal consistency to enforce the embeddings changing gradually. In this work, we further design a lightweight and efficient hyperbolic graph convolutional module that enables HTGN to scale to large-size graphs easily and flexibly handle datasets with different densities. Moreover, we investigate the hyperbolic temporal network embedding in the Lorentz model of hyperbolic geometry with regard to its numerical stability and optimization advantages. Extensive experiments demonstrate the effectiveness of the proposals as they consistently outperform the competing baselines on small-, medium-, and large-scale datasets.},
  archive      = {J_TKDE},
  author       = {Menglin Yang and Min Zhou and Hui Xiong and Irwin King},
  doi          = {10.1109/TKDE.2022.3232398},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11489-11502},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hyperbolic temporal network embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous graph neural network with multi-view
representation learning. <em>TKDE</em>, <em>35</em>(11), 11476–11488.
(<a href="https://doi.org/10.1109/TKDE.2022.3224193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph neural networks (GNNs)-based methods have been widely adopted for heterogeneous graph (HG) embedding, due to their power in effectively encoding rich information from a HG into the low-dimensional node embeddings. However, previous works usually easily fail to fully leverage the inherent heterogeneity and rich semantics contained in the complex local structures of HGs. On the one hand, most of the existing methods either inadequately model the local structure under specific semantics, or neglect the heterogeneity when aggregating information from the local structure. On the other hand, representations from multiple semantics are not comprehensively integrated to obtain node embeddings with versatility. To address the problem, we propose a Heterogeneous Graph Neural Network for HG embedding within a Multi-View representation learning framework (named MV-HetGNN), which consists of a view-specific ego graph encoder and auto multi-view fusion layer. MV-HetGNN thoroughly learns complex heterogeneity and semantics in the local structure to generate comprehensive and versatile node representations for HGs. Extensive experiments on three real-world HG datasets demonstrate the significant superiority of our proposed MV-HetGNN compared to the state-of-the-art baselines in various downstream tasks, e.g., node classification, node clustering, and link prediction.},
  archive      = {J_TKDE},
  author       = {Zezhi Shao and Yongjun Xu and Wei Wei and Fei Wang and Zhao Zhang and Feida Zhu},
  doi          = {10.1109/TKDE.2022.3224193},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11476-11488},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Heterogeneous graph neural network with multi-view representation learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GoldFinger: Fast &amp; approximate jaccard for efficient KNN
graph constructions. <em>TKDE</em>, <em>35</em>(11), 11461–11475. (<a
href="https://doi.org/10.1109/TKDE.2022.3232689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose GoldFinger , a new compact and fast-to-compute binary representation of datasets to approximate Jaccard&#39;s index. We illustrate the effectiveness of GoldFinger on the emblematic big data problem of K-Nearest-Neighbor (KNN) graph construction and show that GoldFinger can drastically accelerate a large range of existing KNN algorithms with little to no overhead. As a side effect, we also show that the compact representation of the data protects users’ privacy for free by providing k -anonymity and l -diversity. Our extensive evaluation of the resulting approach on several realistic datasets shows that our approach reduces computation times by up to 78.9\% compared to raw data while only incurring a negligible to moderate loss in terms of KNN quality. We also show that GoldFinger can be applied to KNN queries (a widely-used search technique) and delivers speedups of up to $\times 3.55$ over one of the most efficient approaches to this problem.},
  archive      = {J_TKDE},
  author       = {Rachid Guerraoui and Anne-Marie Kermarrec and Guilhem Niot and Olivier Ruas and François Taïani},
  doi          = {10.1109/TKDE.2022.3232689},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11461-11475},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GoldFinger: Fast &amp; approximate jaccard for efficient KNN graph constructions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global ontology entities embeddings. <em>TKDE</em>,
<em>35</em>(11), 11449–11460. (<a
href="https://doi.org/10.1109/TKDE.2023.3235779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontologies are among the most widely used types of knowledge representation formalisms. The application of deep learning techniques in the field of ontology engineering has reinforced the need to learn and generate representations of ontological data. This allows ontologies to be exploited by such models, and thus automate various ontology engineering tasks, where most of the existing tools and machine learning approaches require a numerical feature vectors associated with each concept. This paper outlines a novel approach for learning global ontology entities embeddings by exploiting the structure and the various taxonomic and semantic relationships present in ontologies, taking into account all the information present in the ontological graph and carried by the OWL/RDF triples. Thus, producing global ontology entities embeddings capturing the global ontological graph semantics and similarities enclosed in the source ontology. Three different neural network models have been proposed based on two architectures: multi-input and multi-output, trained using the contrastive estimation technique. The evaluation on OWL/RDF ontologies and word semantic similarity tasks using various graph and WordNet based similarity measures, show that our approach yields competitive results outperforming the state-of-the-art ontology and word embedding models.},
  archive      = {J_TKDE},
  author       = {Achref Benarab and Jianguo Sun and Fahad Rafique and Allaoua Refoufi},
  doi          = {10.1109/TKDE.2023.3235779},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11449-11460},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Global ontology entities embeddings},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global combination and clustering based differential privacy
mixed data publishing. <em>TKDE</em>, <em>35</em>(11), 11437–11448. (<a
href="https://doi.org/10.1109/TKDE.2023.3237822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of information technology, a large amount of high-value data have been generated. To exploit the potential value of Big Data and at the same time to protect individuals’ sensitive information, a global combination and clustering based differential privacy (DP) mixed data publishing method is proposed in this paper. The main idea of the proposed method is to improve the truthfulness of the published data as well as to enhance the utility by shifting the sensitivity of query function from a single record to a group of records using $k$ -median clustering algorithm. Specifically, to improve the accuracy and utility of categorical attributes, a global combination method is proposed to take the correlation among categorical attributes into account. The proposed combination method takes all categorical attributes as a unit and then applies the exponential mechanism to improve the data utility. Then we combine it with the $k$ -median clustering with differential privacy to publish the mixed data. Theoretical analysis shows that the proposed method satisfies $\varepsilon$ -differential privacy. Experimental results on real datasets illustrate that the proposed method has a much lower information loss and time overhead than the state-of-the-art approach for the same parameters.},
  archive      = {J_TKDE},
  author       = {Lanxiang Chen and Lingfang Zeng and Yi Mu and Leilei Chen},
  doi          = {10.1109/TKDE.2023.3237822},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11437-11448},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Global combination and clustering based differential privacy mixed data publishing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric affinity propagation for clustering with network
knowledge. <em>TKDE</em>, <em>35</em>(11), 11419–11436. (<a
href="https://doi.org/10.1109/TKDE.2023.3237630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering data into meaningful subsets is a major task in scientific data analysis. To date, various strategies ranging from model-based approaches to data-driven schemes, have been devised for efficient and accurate clustering. One important class of clustering methods that is of a particular interest is the class of exemplar-based approaches. This interest primarily stems from the amount of compressed information encoded in these exemplars that effectively reflect the major characteristics of the corresponding clusters. Affinity propagation (AP) has proven to be a powerful exemplar-based approach that refines the set of optimal exemplars by iterative pairwise message updates. However, a critical limitation is its inability to capitalize on known networked relations between data points often available for various scientific datasets. To address this shortcoming, we propose Geometric-AP, a novel clustering algorithm that effectively extends the original AP to take advantage of the network topology. Geometric-AP obeys network constraints and uses max-sum belief propagation to leverage the available network topology for generating smooth clusters over the network. Extensive performance assessment shows that Geometric-AP leads to a significant quality enhancement of the clustering results when compared to existing schemes. Especially, we demonstrate that Geometric-AP performs extremely well even in cases where the original AP fails drastically.},
  archive      = {J_TKDE},
  author       = {Omar Maddouri and Xiaoning Qian and Byung-Jun Yoon},
  doi          = {10.1109/TKDE.2023.3237630},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11419-11436},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Geometric affinity propagation for clustering with network knowledge},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding minimum connected subgraphs with ontology
exploration on large RDF data. <em>TKDE</em>, <em>35</em>(11),
11403–11418. (<a
href="https://doi.org/10.1109/TKDE.2022.3225076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the following problem: given a knowledge graph (KG) and a set of input vertices (representing concepts or entities) and edge labels, we aim to find the smallest connected subgraphs containing all of the inputs. This problem plays a key role in KG-based search engines and natural language question answering systems, and it is a natural extension of the Steiner tree problem, which is known to be NP-hard. We present RECON, a system for finding approximate answers. RECON aims at achieving high accuracy with instantaneous response (i.e., sub-second/millisecond delay) over KGs with hundreds of millions edges without resorting to expensive computational resources. Furthermore, when no answer exists due to disconnection between concepts and entities, RECON refines the input to a semantically similar one based on the ontology, and attempts to find answers with respect to the refined input. We conduct a comprehensive experimental evaluation of RECON. In particular we compare it with five existing approaches for finding approximate Steiner trees. Our experiments on four large real and synthetic KGs show that RECON significantly outperforms its competitors and incurs a much smaller memory footprint.},
  archive      = {J_TKDE},
  author       = {Xiangnan Ren and Neha Sengupta and Xuguang Ren and Junhu Wang and Olivier Curé},
  doi          = {10.1109/TKDE.2022.3225076},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11403-11418},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Finding minimum connected subgraphs with ontology exploration on large RDF data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast multi-view clustering via ensembles: Towards
scalability, superiority, and simplicity. <em>TKDE</em>,
<em>35</em>(11), 11388–11402. (<a
href="https://doi.org/10.1109/TKDE.2023.3236698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant progress, there remain three limitations to the previous multi-view clustering algorithms. First, they often suffer from high computational complexity, restricting their feasibility for large-scale datasets. Second, they typically fuse multi-view information via one-stage fusion, neglecting the possibilities in multi-stage fusions. Third, dataset-specific hyperparameter-tuning is frequently required, further undermining their practicability. In light of this, we propose a fast m ulti-v i ew c lustering via e nsembles (FastMICE) approach.Particularly, the concept of random view groups is presented to capture the versatile view-wise relationships, through which the hybrid early-late fusion strategy is designed to enable efficient multi-stage fusions. With multiple views extended to many view groups, three levels of diversity (w.r.t. features, anchors, and neighbors, respectively) are jointly leveraged for constructing the view-sharing bipartite graphs in the early-stage fusion. Then, a set of diversified base clusterings for different view groups are obtained via fast graph partitioning, which are further formulated into a unified bipartite graph for final clustering in the late-stage fusion. Notably, FastMICE has almost linear time and space complexity, and is free of dataset-specific tuning. Experiments on 22 multi-view datasets demonstrate its advantages in scalability (for extremely large datasets), superiority (in clustering performance), and simplicity (to be applied) over the state-of-the-art. Code available: https://github.com/huangdonghere/FastMICE .},
  archive      = {J_TKDE},
  author       = {Dong Huang and Chang-Dong Wang and Jian-Huang Lai},
  doi          = {10.1109/TKDE.2023.3236698},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11388-11402},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast multi-view clustering via ensembles: Towards scalability, superiority, and simplicity},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairness-aware maximal clique in large graphs: Concepts and
algorithms. <em>TKDE</em>, <em>35</em>(11), 11368–11387. (<a
href="https://doi.org/10.1109/TKDE.2022.3232165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cohesive subgraph mining on attributed graphs is a fundamental problem in graph data analysis. Existing cohesive subgraph mining algorithms on attributed graphs do not consider the fairness of attributes in the subgraph. In this article, we, for the first time, introduce fairness into the widely-used clique model to mine fairness-aware cohesive subgraphs. In particular, we propose three novel fairness-aware maximal clique models on attributed graphs, called weak fair clique, strong fair clique and relative fair clique, respectively. To enumerate all weak fair cliques, we develop an efficient backtracking algorithm called $\mathsf{WFCEnum}$ equipped with a novel colorful $k$ -core based pruning technique. We also propose an efficient enumeration algorithm called $\mathsf{SFCEnum}$ to find all strong fair cliques based on a new attribute-alternatively-selection search technique. To further improve the efficiency, we also present several non-trivial ordering techniques for both weak and strong fair clique enumerations. To enumerate all relative fair cliques, we design an enhanced colorful $k$ -core based pruning technique for 2D attributes, and develop two efficient search algorithms: $\mathsf{RFCRefineEnum}$ and $\mathsf{RFCAlterEnum}$ for arbitrary dimension attributes. The results of extensive experiments on four real-world graphs demonstrate the efficiency, scalability and effectiveness of the proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Qi Zhang and Rong-Hua Li and Minjia Pan and Yongheng Dai and Qun Tian and Guoren Wang},
  doi          = {10.1109/TKDE.2022.3232165},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11368-11387},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fairness-aware maximal clique in large graphs: Concepts and algorithms},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ExamGAN and twin-ExamGAN for exam script generation.
<em>TKDE</em>, <em>35</em>(11), 11354–11367. (<a
href="https://doi.org/10.1109/TKDE.2022.3233046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the learning management system (LMS) has been widely used in different educational stages from primary to tertiary education for student administration, documentation, tracking, reporting, and delivery of educational courses, training programs, or learning and development programs. Towards effective learning outcome assessment, the exam script generation problem has attracted many attentions recently. But the research in this field is still in its early stage. Two essential issues have been ignored largely by existing solutions. First, given a course, it is unknown yet how to generate an quality exam script which concurrently has (i) the proper difficulty level, (ii) the coverage of essential knowledge points, (iii) the capability to distinguish academic performances between students, and (iv) the student scores in normal distribution. Second, while frequently encountered in practice, it is unknown so far how to generate a pair of high quality exam scripts which are equivalent in assessment (i.e., the student scores are comparable by taking either of them) but have significantly different sets of questions. To fill the gap, this paper proposes ExamGAN (Exam Script Generative Adversarial Network) to generate high quality exam scripts, and then extends ExamGAN to T-ExamGAN (Twin-ExamGAN) to generate a pair of high quality exam scripts. Based on extensive experiments on three benchmark datasets, it has verified the superiority of proposed solutions in various aspects against the state-of-the-art. Moreover, we have conducted a case study which demonstrated the effectiveness of proposed solution in the real teaching scenarios.},
  archive      = {J_TKDE},
  author       = {Zhengyang Wu and Ke Deng and Judy Qiu and Yong Tang},
  doi          = {10.1109/TKDE.2022.3233046},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11354-11367},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ExamGAN and twin-ExamGAN for exam script generation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating edge credibility in evolving noisy social
networks. <em>TKDE</em>, <em>35</em>(11), 11342–11353. (<a
href="https://doi.org/10.1109/TKDE.2022.3223403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the massive surge of evolving social network analysis in popularity, existing research usually represent the observed social interactions among individuals as completely credible edges. However, due to information inaccuracy, individual non-response and dropout, and sampling biases in observations, the evolving noisy social network that coexists true edges and spurious edges is pervasive in actual applications, where the ignoration of credibility otherness of observed edges could lead to the wrong estimates of social properties and misleading conclusions. To discover credible edge information to shape correct social interactions among individuals, we propose a universal and explainable multiple-neighbor evolutional filtering method ( MEFM ) to evaluate how credible of observed edges to ‘truly’ exist in the evolving noisy social network. MEFM consists of an evolutional extractor and a filtering evaluator. To resist the noisy disturbance, the evolutional extractor exploits the evolutional states of edges from the perspective of evolution mechanisms within multiple-neighbor ranges, which applies different link prediction algorithms to fit the evolution mechanism in the formation of each edge. Further, the filtering evaluator reconstructs Kalman filter to predict and refine the evolutional states of edges based on their evolving local structures. As a result, MEFM combines the evolutional extractor and the filtering evaluator to analyze the evolutional fluctuations of the observed edges to evaluate their credibility. Extensive experiments on real-world datasets demonstrate that our proposed MEFM can effectively and reasonably evaluate edge credibility in evolving noisy social networks.},
  archive      = {J_TKDE},
  author       = {Huan Wang and Ziwen Cui and Shun Liu and Qiufen Ni and Zhiguo Gong},
  doi          = {10.1109/TKDE.2022.3223403},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11342-11353},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Evaluating edge credibility in evolving noisy social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficiently counting triangles for hypergraph streams by
reservoir-based sampling. <em>TKDE</em>, <em>35</em>(11), 11328–11341.
(<a href="https://doi.org/10.1109/TKDE.2023.3236335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraph streams provide an efficient model to express and preserve complex connections in various applications. Triangles in a hypergraph can be formed by vertices and hyperedges. The specific counts of triangles are important to analyze various applications. Due to the huge costs of counting triangles based on the whole datasets, a sampling-and-estimating framework has low overhead while obtaining a relatively accurate result. However, existing sampling algorithms focus on pairwise graph streams, they estimate the counts of triangles formed by vertices with large estimation errors and can not be applied to count triangles formed by hyperedges. Therefore, this paper first proposes a sampling-and-estimating framework that produces hyperedge samples using a reservoir with static capacity to estimate the total counts of triangles by inferring the probabilities of forming the triangles respectively. Furthermore, to improve the estimation accuracy, this paper proposes another sampling-and-estimating framework to produce samples in the form of hyperedge pairs which can be used to compute the probabilities of the formations of triangles more accurately and then estimate the total triangle counts with smaller estimation variances. The extensive experiments based on real-world datasets confirm the efficiency and accuracy of our proposed frameworks for counting triangles in different types of hypergraphs at a small sampling ratio.},
  archive      = {J_TKDE},
  author       = {Lingling Zhang and Zhiwei Zhang and Guoren Wang and Ye Yuan and Kangfei Zhao},
  doi          = {10.1109/TKDE.2023.3236335},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11328-11341},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficiently counting triangles for hypergraph streams by reservoir-based sampling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-view preference learning for adaptive recommendation.
<em>TKDE</em>, <em>35</em>(11), 11316–11327. (<a
href="https://doi.org/10.1109/TKDE.2023.3236370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While recommendation systems have been widely deployed, most existing approaches only capture user preferences in the macro-view , i.e., the user&#39;s general interest across all kinds of items. However, in real-world scenarios, user preferences could vary with items of different natures, which we call the micro-view . Both views are crucial for fully personalized recommendation, where an underpinning macro-view governs a multitude of finer-grained preferences in the micro-view. To model the dual views, in this paper, we propose a novel model called Dual-View Adaptive Recommendation (DVAR). In DVAR, we formulate the micro-view based on item categories, and further integrate it with the macro-view. Moreover, DVAR is designed to be adaptive, which is capable of automatically adapting to the dual-view preferences in response to different input users and item categories. To the best of our knowledge, this is the first attempt to integrate user preferences in macro- and micro- views in an adaptive way, without relying on additional side information such as text reviews. Finally, we conducted extensive quantitative and qualitative evaluations on several real-world datasets. Empirical results not only show that DVAR can significantly outperform other state-of-the-art recommendation systems, but also demonstrate the benefit and interpretability of the dual views.},
  archive      = {J_TKDE},
  author       = {Zhongzhou Liu and Yuan Fang and Min Wu},
  doi          = {10.1109/TKDE.2023.3236370},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11316-11327},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual-view preference learning for adaptive recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual structural consistency preserving community detection
on social networks. <em>TKDE</em>, <em>35</em>(11), 11301–11315. (<a
href="https://doi.org/10.1109/TKDE.2022.3230502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection on social networks is a fundamental and crucial task in the research field of social computing. Here we propose DSCPCD —a dual structural consistency preserving community detection method to uncover the hidden community structure, which is designed regarding two criteria: 1) users interact with each other in a manner combining uncertainty and certainty; 2) original explicit network (two linked users are friends) and potential implicit network (two linked users have common friends) should have a consistent community structure, i.e., dual structural consistency . Particularly, DSCPCD formulates each user in a social network as an individual in an evolutionary game associated with community-aware payoff settings, where the community state evolves under the guidance of replicator dynamics. To further seek each user&#39;s membership, we develop a happiness index to measure all users’ satisfaction towards two community structures in explicit and implicit networks, meanwhile, the dual community structural consistency between the two networks is also characterized. Specifically, each user is assumed to maximize the happiness bounded by the evolutionary community state. We evaluate DSCPCD on several real-world and synthetic datasets, and the results show that it can yield substantial performance gains in terms of detection accuracy over several baselines.},
  archive      = {J_TKDE},
  author       = {Yuyao Wang and Jie Cao and Zhan Bu and Jia Wu and Youquan Wang},
  doi          = {10.1109/TKDE.2022.3230502},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11301-11315},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual structural consistency preserving community detection on social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributionally robust learning with stable adversarial
training. <em>TKDE</em>, <em>35</em>(11), 11288–11300. (<a
href="https://doi.org/10.1109/TKDE.2022.3224056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms with empirical risk minimization are vulnerable under distributional shifts due to the greedy adoption of all the correlations found in training data. There is an emerging literature on tackling this problem by minimizing the worst-case risk over an uncertainty set. However, existing methods mostly construct ambiguity sets by treating all variables equally regardless of the stability of their correlations with the target, resulting in the overwhelmingly-large uncertainty set and low confidence of the learner. In this paper, we propose a novel Stable Adversarial Learning (SAL) algorithm that leverages heterogeneous data sources to construct a more practical uncertainty set and conduct differentiated robustness optimization, where covariates are differentiated according to the stability of their correlations with the target. We theoretically show that our method is tractable for stochastic gradient-based optimization and provide the performance guarantees for our method. Empirical studies on both simulation and real datasets validate the effectiveness of our method in terms of uniformly good performance across unknown distributional shifts.},
  archive      = {J_TKDE},
  author       = {Jiashuo Liu and Zheyan Shen and Peng Cui and Linjun Zhou and Kun Kuang and Bo Li},
  doi          = {10.1109/TKDE.2022.3224056},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11288-11300},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Distributionally robust learning with stable adversarial training},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density-based clustering by means of bridge point
identification. <em>TKDE</em>, <em>35</em>(11), 11274–11287. (<a
href="https://doi.org/10.1109/TKDE.2022.3232315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density-based clustering focuses on defining clusters consisting of contiguous regions characterized by similar densities of points. Traditional approaches identify core points first, whereas more recent ones initially identify the cluster borders and then propagate cluster labels within the delimited regions. Both strategies encounter issues in presence of multi-density regions or when clusters are characterized by noisy borders. To overcome the above issues, we present a new clustering algorithm that relies on the concept of bridge point. A bridge point is a point whose neighborhood includes points of different clusters. The key idea is to use bridge points, rather than border points, to partition points into clusters. We have proved that a correct bridge point identification yields a cluster separation consistent with the expectation. To correctly identify bridge points in absence of a priori cluster information we leverage an established unsupervised outlier detection algorithm. Specifically, we empirically show that, in most cases, the detected outliers are actually a superset of the bridge point set. Therefore, to define clusters we spread cluster labels like a wildfire until an outlier, acting as a candidate bridge point, is reached. The proposed algorithm performs statistically better than state-of-the-art methods on a large set of benchmark datasets and is particularly robust to the presence of intra-cluster multiple densities and noisy borders.},
  archive      = {J_TKDE},
  author       = {Luca Colomba and Luca Cagliero and Paolo Garza},
  doi          = {10.1109/TKDE.2022.3232315},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11274-11287},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Density-based clustering by means of bridge point identification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Densest periodic subgraph mining on large temporal graphs.
<em>TKDE</em>, <em>35</em>(11), 11259–11273. (<a
href="https://doi.org/10.1109/TKDE.2022.3233788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Densest subgraphs are often interpreted as communities , based on a basic assumption that the connections inside a community are much denser than those between communities. In a graph with temporal information, a densest periodic subgraph is the most densely connected periodic behavior which needs to be captured. Unfortunately, the existing work do not model the densest periodic subgraph in temporal graphs, and the current algorithms for mining the densest subgraph cannot be applied to detect the densest periodic subgraph in the temporal networks. To tackle this problem, we propose a novel model, called the densest $\sigma$ -periodic subgraph, which presents the densest periodic subgraph whose period size is $\sigma$ . We prove that finding the densest $\sigma$ -periodic subgraph can be solved in polynomial time, but it is still challenging because the naive algorithm needs to repeatedly invoke a maximum flow algorithm for many periodic subgraphs. To compute the densest $\sigma$ -periodic subgraph efficiently, we first develop an effective pruning technique based on the degeneracy of the graph to significantly prune the number of the periodic subgraphs. Then, we present a more efficient algorithm that can reduce the computations for the degeneracy and maximum flow. Next, we develop a greedy algorithm that can compute the approximate densest $\sigma$ -periodic subgraph and achieve an approximation ratio of 1/2. Finally, the results of extensive experiments on several real-life datasets demonstrate the efficiency, scalability, and effectiveness of our algorithms.},
  archive      = {J_TKDE},
  author       = {Hongchao Qin and Rong-Hua Li and Ye Yuan and Yongheng Dai and Guoren Wang},
  doi          = {10.1109/TKDE.2022.3233788},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11259-11273},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Densest periodic subgraph mining on large temporal graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-city multi-granular adaptive transfer learning for
traffic flow prediction. <em>TKDE</em>, <em>35</em>(11), 11246–11258.
(<a href="https://doi.org/10.1109/TKDE.2022.3232185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic prediction is one of the most important techniques in building a smart city. Many works, especially deep learning models, have made great progress in traffic prediction based on rich historical data. However, many cities still suffer from the problem of data scarcity in many aspects. Some works use transfer learning to solve this kind of problem, but what and how to transfer is still an important problem. In this article, we propose a novel Cross-city Multi-Granular Adaptive Transfer Learning method named MGAT for traffic prediction with only a few data in the target city. We first use the meta-learning algorithm to train the model on multiple source cities to get a good initialization. And at the same time, the multi-granular regional characteristics of each source city will be obtained based on our model structure. Then we design an Adaptive Transfer module mainly composed of Spatial-Attention and Multi-head Attention mechanism to automatically select the most appropriate features from the multi-granular features trained from multiple source cities, to achieve the best transfer effect. We conduct extensive experiments on two kinds of real-world traffic datasets cross several cities. Experimental results with other state-of-the-art models demonstrate the effectiveness of the proposed model.},
  archive      = {J_TKDE},
  author       = {Jiqian Mo and Zhiguo Gong},
  doi          = {10.1109/TKDE.2022.3232185},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11246-11258},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-city multi-granular adaptive transfer learning for traffic flow prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cover trees revisited: Exploiting unused distance and
direction information. <em>TKDE</em>, <em>35</em>(11), 11231–11245. (<a
href="https://doi.org/10.1109/TKDE.2022.3231781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cover tree (CT) and its improved version are hierarchical data structures that simplified navigating nets while maintaining good runtime guarantees. They can perform nearest neighbor search in logarithmic time and provide efficient computation in practice. In this article, we revisit cover trees for nearest neighbor search, and propose a more competitive method. The central idea of our method is to fully exploit the unused distance and direction information. More specially, our method introduces three novel concepts/techniques: (i) range list, (ii) quadrant information, and (iii) vectorial angle cosine. These techniques are seamlessly integrated into our suggested data structure and search algorithms. As an extra bonus, we explore approximate nearest neighbor and $k$ nearest neighbor based on the proposed techniques, and present algorithms for handling updates. Extensive experimental results, based on both real and synthetic datasets, consistently demonstrate that our method is attractive and competitive, compared against existing cover tree structures for nearest neighbor search and its variants.},
  archive      = {J_TKDE},
  author       = {Zhi-Jie Wang and Mengdie Nie and Kaiqi Zhao and Zhe Quan and Bin Yao},
  doi          = {10.1109/TKDE.2022.3231781},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11231-11245},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cover trees revisited: Exploiting unused distance and direction information},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive proxy kernel stein path alignment for
cross-domain cold-start recommendation. <em>TKDE</em>, <em>35</em>(11),
11216–11230. (<a
href="https://doi.org/10.1109/TKDE.2022.3233789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Domain Recommendation has been popularly studied to utilize different domain knowledge to solve the cold-start problem in recommender systems. In this paper, we focus on the Cross-Domain Cold-Start Recommendation ( CDCSR ) problem. That is, how to leverage the information from a source domain, where items are ’warm’, to improve the recommendation performance of a target domain, where items are ’cold’. It has two main challenges, i.e., (1) how to efficiently reduce the discrepancy between the latent embedding distribution across domains and (2) how to generate more robust and stable cold item embeddings. To address these two challenges, we propose CPKSPA , a cross-domain recommendation framework for the CDCSR problem. CPKSPA contains three modules, i.e., rating prediction module, embedding distribution alignment module, and contrastive augmentation module. To start with, we first utilize the rating prediction module to model user-item interactions. To solve the first challenge, we propose proxy Stein path alignment with typical-subgroup discovering algorithm in the embedding distribution alignment module. To tackle the second challenge, we propose the contrastive augmentation module which adopts contrastive augmentation learning to generate more stable and robust cold item embeddings. Our empirical study on Douban and Amazon datasets demonstrates that CPKSPA significantly outperforms the state-of-the-art models.},
  archive      = {J_TKDE},
  author       = {Weiming Liu and Xiaolin Zheng and Jiajie Su and Longfei Zheng and Chaochao Chen and Mengling Hu},
  doi          = {10.1109/TKDE.2022.3233789},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11216-11230},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Contrastive proxy kernel stein path alignment for cross-domain cold-start recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complex knowledge base question answering: A survey.
<em>TKDE</em>, <em>35</em>(11), 11196–11215. (<a
href="https://doi.org/10.1109/TKDE.2022.3223858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Early studies mainly focused on answering simple questions over KBs and achieved great success. However, their performances on complex questions are still far from satisfactory. Therefore, in recent years, researchers propose a large number of novel methods, which looked into the challenges of answering complex questions. In this survey, we review recent advances in KBQA with the focus on solving complex questions, which usually contain multiple subjects, express compound relations, or involve numerical operations. In detail, we begin with introducing the complex KBQA task and relevant background. Then, we present two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. Specifically, we illustrate their procedures with flow designs and discuss their difference and similarity. Next, we summarize the challenges that these two categories of methods encounter when answering complex questions, and explicate advanced solutions as well as techniques used in existing work. After that, we discuss the potential impact of pre-trained language models (PLMs) on complex KBQA. To help readers catch up with SOTA methods, we also provide a comprehensive evaluation and resource about complex KBQA task. Finally, we conclude and discuss several promising directions related to complex KBQA for future research.},
  archive      = {J_TKDE},
  author       = {Yunshi Lan and Gaole He and Jinhao Jiang and Jing Jiang and Wayne Xin Zhao and Ji-Rong Wen},
  doi          = {10.1109/TKDE.2022.3223858},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11196-11215},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Complex knowledge base question answering: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Complex event summarization using multi-social attribute
correlation. <em>TKDE</em>, <em>35</em>(11), 11180–11195. (<a
href="https://doi.org/10.1109/TKDE.2022.3227906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex social event summarization is a problem which has been shown having great utility for real-world applications, including crisis management, rumor control and government policy tracking. In recent years there has been significant research effort spent on effectively extracting meaningful textual descriptions of an event. However, in many critical situations, social events are complex and context-sensitive, which demands the online summarization of social events in an integrated manner. In this paper, we propose the first online complex social event summarization approach, namely SOMA, which summarizes the complex social events over multiple attributes including media content and contexts simultaneously. Specifically, we first propose a deep learning model that comprehensively summarizes events in regards to the text description and locations that they appear in, by utilizing their hidden connections in posts. We then propose a summary generator over time, text and location to achieve a maximal coverage of the summary over the original social event and minimal redundancy of the summary. Furthermore, we propose a location estimation method to address the location sparsity issue of complex events by mining the correlation between text and location. The evaluation over four real-event datasets and three benchmark datasets shows that our proposed approach outperforms the existing solutions for event summarizaiton in terms of effectiveness and efficiency.},
  archive      = {J_TKDE},
  author       = {Xi Chen and Xiangmin Zhou and Jeffrey Chan and Lei Chen and Timos Sellis and Yanchun Zhang},
  doi          = {10.1109/TKDE.2022.3227906},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11180-11195},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Complex event summarization using multi-social attribute correlation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cohesive subgraph discovery over uncertain bipartite graphs.
<em>TKDE</em>, <em>35</em>(11), 11165–11179. (<a
href="https://doi.org/10.1109/TKDE.2023.3234567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose the $(\alpha,\beta,\eta)$ -core model, which is the first cohesive subgraph model on uncertain bipartite graphs. To capture the uncertainty of relationships/edges, $\eta$ -degree is adopted to measure the vertex engagement level, which is the largest integer $k$ such that the probability of a vertex having at least $k$ neighbors is not less than $\eta$ . Given degree constraints $\alpha$ and $\beta$ , and a probability threshold $\eta$ , the $(\alpha,\beta,\eta)$ -core requires that each vertex on the upper or lower level have $\eta$ -degree no less than $\alpha$ or $\beta$ , respectively. An $(\alpha,\beta,\eta)$ -core can be obtained by iteratively removing the vertices with $\eta$ -degrees below the degree constraints. Apart from the online computation algorithm, we propose a probability-aware index to strike a balance between time and space costs. To efficiently build such an index, we design a top-down index construction algorithm to allow computation sharing. Then, we show how to parallelize our query algorithms and index construction algorithms. In addition, we study community search on uncertain bipartite graphs by adopting the $(\alpha,\beta,\eta)$ -core model. Extensive experiments are conducted on 13 datasets to validate the efficiency and effectiveness of our proposed techniques.},
  archive      = {J_TKDE},
  author       = {Kai Wang and Gengda Zhao and Wenjie Zhang and Xuemin Lin and Ying Zhang and Yizhang He and Chunxiao Li},
  doi          = {10.1109/TKDE.2023.3234567},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11165-11179},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cohesive subgraph discovery over uncertain bipartite graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal inference for knowledge graph based recommendation.
<em>TKDE</em>, <em>35</em>(11), 11153–11164. (<a
href="https://doi.org/10.1109/TKDE.2022.3231352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph (KG), as a side-information, tends to be utilized to supplement the collaborative filtering (CF) based recommendation model. By mapping items with the entities in KGs, prior studies mostly extract the knowledge information from the KGs and inject it into the representations of users and items. Despite their remarkable performance, they fail to model the user preference on attribute in the KG, since they ignore that (1) the structure information of KG may hinder the user preference learning, and (2) the user&#39;s interacted attributes will result in the bias issue on the similarity scores. With the help of causality tools, we construct the causal-effect relation between the variables in KG-based recommendation and identify the reasons causing the mentioned challenges. Accordingly, we develop a new framework, termed Knowledge Graph-based Causal Recommendation (KGCR), which implements the deconfounded user preference learning and adopts counterfactual inference to eliminate bias in the similarity scoring. Ultimately, we evaluate our proposed model on three datasets, including Amazon-book, LastFM, and Yelp2018 datasets. By conducting extensive experiments on the datasets, we demonstrate that KGCR outperforms several state-of-the-art baselines, such as KGNN-LS (Wang et al., 2019), KGAT (Wang et al., 2019) and KGIN (Wang et al., 2021).},
  archive      = {J_TKDE},
  author       = {Yinwei Wei and Xiang Wang and Liqiang Nie and Shaoyu Li and Dingxian Wang and Tat-Seng Chua},
  doi          = {10.1109/TKDE.2022.3231352},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11153-11164},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Causal inference for knowledge graph based recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal inference for leveraging image-text matching bias in
multi-modal fake news detection. <em>TKDE</em>, <em>35</em>(11),
11141–11152. (<a
href="https://doi.org/10.1109/TKDE.2022.3231338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal fake news detection has drawn considerable attention with the development of online social media. Existing methods primarily conduct direct cross-modal fusion, while ignoring the image-text matching degree which may introduce unexpected bias. This work studies an unexplored problem in multi-modal fake news detection – how to deconfound and leverage the image-text matching bias to improve the performance of fake news detection. The key lies in two aspects: how to remove the confounding effect of the image-text matching bias during training, and how to utilize the bias in the inference stage since the news with mismatched image and text is more likely to be fake. To achieve our goal, we formulate the fake news detection task as a causal graph that reflects the cause-effect factors, and propose a novel framework – C ausal Inference for L everaging I mage-text M atching B ias ( CLIMB ) in multi-modal fake news detection. To our best knowledge, this is the first work that considers the image-text matching degree into the fake news detection task with the approach of causal inference. CLIMB can be applied to any fake news detection models with visual and textual features as inputs. Extensive experiments on two real-world datasets validate the effectiveness of CLIMB.},
  archive      = {J_TKDE},
  author       = {Linmei Hu and Ziwei Chen and Ziwang Zhao and Jianhua Yin and Liqiang Nie},
  doi          = {10.1109/TKDE.2022.3231338},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11141-11152},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Causal inference for leveraging image-text matching bias in multi-modal fake news detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BurstSketch: Finding bursts in data streams. <em>TKDE</em>,
<em>35</em>(11), 11126–11140. (<a
href="https://doi.org/10.1109/TKDE.2022.3223686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Burst is a common pattern in data streams which is characterized by a sudden increase in terms of arrival rate followed by a sudden decrease. Burst detection has attracted extensive attention from the research community. To detect bursts accurately in real time, we propose a novel sketch, namely BurstSketch, which consists of two stages. Stage 1 uses the technique Running Track to select potential burst items efficiently. Stage 2 monitors the potential burst items and captures the key features of burst pattern by a technique called Snapshotting. We further propose an optimization, namely Dynamic Buckets, which can improve the accuracy of BurstSketch. We provide theoretical error bounds for Stage 1, Stage 2 and the optimized version. Experimental results show that, compared with the strawman solution, Burstsketch achieves 2.00 to 11.63 times higher F1 score, and 1.56 times higher throughput. We also integrate BurstSketch into Apache Flink, and show that using BurstSketch can be faster than simply using the built-in APIs provided by Apache Flink.},
  archive      = {J_TKDE},
  author       = {Ruijie Miao and Zheng Zhong and Jiarui Guo and Zikun Li and Tong Yang and Bin Cui},
  doi          = {10.1109/TKDE.2022.3223686},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11126-11140},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BurstSketch: Finding bursts in data streams},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BL-GAN: Semi-supervised bug localization via generative
adversarial network. <em>TKDE</em>, <em>35</em>(11), 11112–11125. (<a
href="https://doi.org/10.1109/TKDE.2022.3225329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various automated bug localization technologies have recently emerged that require adequate bug-fix records available to train a predictive model. However, many projects in practice might not provide these necessities, especially for new projects in the first release, due to the expensive human effort for constructing a large amount of bug-fix records. Aiming to capture the potential relevance distribution between the bug report and code file from a limited number of available bug-fix records, we present the first semi-supervised bug localization model named BL-GAN in this paper. For this purpose, the promising Generative Adversarial Network is introduced in BL-GAN, in which synthetic bug-fix records close to the real ones are constructed by searching the project directory tree to generate file paths instead of traversing the contents of all code files. For processing bug reports, the proposed BL-GAN adopts an attention-based Transformer architecture to capture semantic and sequence information. In order to capture the proprietary structural information in code files, BL-GAN incorporates a novel multilayer Graph Convolutional Network to process the source code in a graphical view. Extensive experiments on large-scale real-world datasets reveal that our model BL-GAN significantly outperforms the state-of-the-art on all evaluation measures.},
  archive      = {J_TKDE},
  author       = {Ziye Zhu and Hanghang Tong and Yu Wang and Yun Li},
  doi          = {10.1109/TKDE.2022.3225329},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11112-11125},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BL-GAN: Semi-supervised bug localization via generative adversarial network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bidirectional string anchors for improved text indexing and
top-<span class="math inline"><em>K</em></span> similarity search.
<em>TKDE</em>, <em>35</em>(11), 11093–11111. (<a
href="https://doi.org/10.1109/TKDE.2022.3231780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimizers sampling mechanism is a popular mechanism for string sampling. However, minimizers sampling mechanisms lack good guarantees on the expected size of their samples for different combinations of their input parameters. Furthermore, indexes constructed over minimizers samples lack good worst-case guarantees for on-line pattern searches. In response, we propose bidirectional string anchors (bd-anchors), a new string sampling mechanism. Given an integer $\ell$ , our mechanism selects the lexicographically smallest rotation in every length- $\ell$ fragment. We show that, like minimizers samples, bd-anchors samples are approximately uniform, locally consistent, and computable in linear time. Furthermore, our experiments demonstrate that the bd-anchors sample sizes decrease proportionally to $\ell$ ; and that these sizes are competitive to or smaller than the minimizers sample sizes. We theoretically justify these results by analyzing the expected size of bd-anchors samples. We also prove that computing a total order on the input alphabet which minimizes the bd-anchors sample size is NP-hard. We next highlight the benefits of bd-anchors in two important applications: text indexing and top- $K$ similarity search. For the first application, we develop an index for performing on-line pattern searches in near-optimal time, and show experimentally that a simple implementation of our index is consistently faster for on-line pattern searches than an analogous implementation of a minimizers-based index; we also show that it is substantially faster than two classic text indexes. For the second application, we develop a heuristic for top- $K$ similarity search under edit distance, and show experimentally that it is generally as accurate as the state-of-the-art tool for the same purpose but more than one order of magnitude faster .},
  archive      = {J_TKDE},
  author       = {Grigorios Loukides and Solon P. Pissis and Michelle Sweering},
  doi          = {10.1109/TKDE.2022.3231780},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11093-11111},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Bidirectional string anchors for improved text indexing and top-$K$ similarity search},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Balanced clique computation in signed networks: Concepts and
algorithms. <em>TKDE</em>, <em>35</em>(11), 11079–11092. (<a
href="https://doi.org/10.1109/TKDE.2022.3225562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clique is one of the most fundamental models for cohesive subgraph mining in network analysis. Existing clique model mainly focuses on unsigned networks. However, in real world, many applications are modeled as signed networks with positive and negative edges. As the signed networks hold their own properties different from the unsigned networks, the existing clique model is inapplicable for the signed networks. Motivated by this, we propose the balanced clique model that considers the most fundamental and dominant theory, structural balance theory, for signed networks. Following the balanced clique model, we study the m aximal b alanced c lique e numeration problem ( ${\mathsf {MBCE}}$ ) which computes all the maximal balanced cliques in a given signed network. Moreover, in some applications, users prefer a unique and representative balanced clique with maximum size rather than all balanced cliques. Thus, we also study the m aximum b alanced c lique s earch problem ( ${\mathsf {MBCS}}$ ) which computes the balanced clique with maximum size. We show that ${\mathsf {MBCE}}$ problem and ${\mathsf {MBCS}}$ problem are both NP-Hard. For the ${\mathsf {MBCE}}$ problem, a straightforward solution is to treat the signed network as two unsigned networks and leverage the off-the-shelf techniques for unsigned networks. However, such a solution is inefficient for large signed networks. To address this problem, in this paper, we first propose a new maximal balanced clique enumeration algorithm by exploiting the unique properties of signed networks. Based on the new proposed algorithm, we devise two optimization strategies to further improve the efficiency of the enumeration. For the ${\mathsf {MBCS}}$ problem, we first propose a baseline solution. To overcome the huge search space problem of the baseline solution, we propose a new search framework based on search space partition. To further improve the efficiency of the new framework, we propose multiple optimization strategies regarding to redundant search branches and invalid candidates. We conduct extensive experiments on large real datasets. The experimental results demonstrate the efficiency, effectiveness and scalability of our proposed algorithms for ${\mathsf {MBCE}}$ problem and ${\mathsf {MBCS}}$ problem.},
  archive      = {J_TKDE},
  author       = {Zi Chen and Long Yuan and Xuemin Lin and Lu Qin and Wenjie Zhang},
  doi          = {10.1109/TKDE.2022.3225562},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11079-11092},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Balanced clique computation in signed networks: Concepts and algorithms},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimal hybrid bi-component series-parallel structure for
time series forecasting. <em>TKDE</em>, <em>35</em>(11), 11067–11078.
(<a href="https://doi.org/10.1109/TKDE.2022.3231008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and forecasting of real-world systems have become one of the most critical needs in different science kinds. Among various factors considered in selecting an appropriate forecasting tool, accuracy is known as the most important criterion. Therefore, the most critical issue in recent forecasting studies is related to improving forecasting accuracy. These studies can be generally categorized into two categories of proposing new single models and developing hybrid models. It has been proven from both theoretical and empirical points of view that combining different models can generate superior results and improve single models&#39; predictive performance. However, despite the popularity and the widespread use of hybrid models, some influential factors, such as the structure of hybridization, number of components, type of components, etc., affect hybrid models&#39; performance that must be appropriately chosen by designers. It is the most challenging subject in the literature of time series forecasting in two recent decades. Several researchers examine different combinations of these factors in order to conclude which one is better. In this way, several different papers have been published in hybridization literature; however, none can prove that its proposed structure is universally better than others. Thus, this paper&#39;s primary purpose is to propose an optimal hybrid structure for time series forecasting. The proposed structure&#39;s main idea is to simultaneously use remarkable features of series and parallel structures and lift their limitations by hybridization of these two methodologies. In this way, in some parts of the modeling process, parallel structure is used, and in some other parts, the series structure is used. In this paper, a bi-component hybrid model of statistical classic and artificial intelligence models is presented as the initial implementation of the proposed methodology. Its performance is theoretically and empirically evaluated. The optimality of the proposed structure is mathematically demonstrated from the theoretical point of view. It is universally proven that the constructed hybrid model based on the proposed structure will achieve the best performance among all other hybrid models constructed based on series and parallel structures by the same conditions, e.g., number and type of components. In addition, the empirical results of ten benchmark data sets with different characteristics indicate that the profitability of the proposed structure is statistically significant.},
  archive      = {J_TKDE},
  author       = {Zahra Hajirahimi and Mehdi Khashei},
  doi          = {10.1109/TKDE.2022.3231008},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11067-11078},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An optimal hybrid bi-component series-parallel structure for time series forecasting},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient data structure for dynamic graph on GPUs.
<em>TKDE</em>, <em>35</em>(11), 11051–11066. (<a
href="https://doi.org/10.1109/TKDE.2023.3235941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest to offload dynamic graph computation to GPU and resort to its high parallel processing ability and larger memory bandwidths compared with CPUs. The existing GPU graph systems usually use compressed sparse row (CSR) as the de-facto structure. However, CSR has a critical weakness for dynamic change due to the large overhead of re-balance process after update. GPMA+ is a state-of-art dynamic PMA-based structure that uses PMA structure and segment-oriented parallel update procedure to address the dynamic weakness of CSR, but it still has a bottleneck on the array expansion. In this paper, we propose an leveled structure (called LPMA) instead of continue array to retain low time complexity and high parallel update and lift the expansion bottleneck of GPMA+. More specifically, we propose a series of optimization techniques, including bottom-up update, top-down update and on-demand hybrid update strategies as well as consistence-guaranteed parallel processing for update-query mixed workloads. We theoretically analyze the benefits of LPMA compared in terms of re-balance cost during updates. Extensive experiments on four large real-life graphs prove the superiority of LPMA compared with the-state-of-arts.},
  archive      = {J_TKDE},
  author       = {Lei Zou and Fan Zhang and Yinnian Lin and Yanpeng Yu},
  doi          = {10.1109/TKDE.2023.3235941},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11051-11066},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An efficient data structure for dynamic graph on GPUs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified framework for adversarial attacks on multi-source
domain adaptation. <em>TKDE</em>, <em>35</em>(11), 11039–11050. (<a
href="https://doi.org/10.1109/TKDE.2022.3230825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source domain adaptation studies the knowledge transferability from multiple labeled source domains to an unlabeled target domain under a distribution shift. However, little effort has been devoted to studying the adversarial vulnerability of multi-source domain adaptation approaches. Specifically, most existing techniques focus on learning the domain-invariant representation to mitigate the distribution shift across domains. In this paper, we theoretically show that the domain-invariant representation cannot guarantee the success of multi-source domain adaptation, when no labeled samples are available in the target domain. This result motivates us to propose a unified framework ( AdaptAttack ) for data poisoning adversarial attacks on multi-source domain adaptation. The key idea is to maliciously manipulate the label-informed data distributions of source domains by injecting perceptibly unnoticeable noise into the source data. In addition, it requires that the generated adversarial attacks are invisible to multi-source domain adaptation algorithms, i.e., the source classification errors and marginal discrepancies across domains are not negatively affected. Extensive experiments on public domain adaptation benchmarks confirm the effectiveness and computational efficiency of our proposed AdaptAttack framework in both white-box and black-box attack scenarios.},
  archive      = {J_TKDE},
  author       = {Jun Wu and Jingrui He},
  doi          = {10.1109/TKDE.2022.3230825},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11039-11050},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A unified framework for adversarial attacks on multi-source domain adaptation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). A survey on aspect-based sentiment analysis: Tasks,
methods, and challenges. <em>TKDE</em>, <em>35</em>(11), 11019–11038.
(<a href="https://doi.org/10.1109/TKDE.2022.3230975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important fine-grained sentiment analysis problem, aspect-based sentiment analysis (ABSA), aiming to analyze and understand people&#39;s opinions at the aspect level, has been attracting considerable interest in the last decade. To handle ABSA in different scenarios, various tasks are introduced for analyzing different sentiment elements and their relations, including the aspect term, aspect category, opinion term, and sentiment polarity. Unlike early ABSA works focusing on a single sentiment element, many compound ABSA tasks involving multiple elements have been studied in recent years for capturing more complete aspect-level sentiment information. However, a systematic review of various ABSA tasks and their corresponding solutions is still lacking, which we aim to fill in this survey. More specifically, we provide a new taxonomy for ABSA which organizes existing studies from the axes of concerned sentiment elements, with an emphasis on recent advances of compound ABSA tasks. From the perspective of solutions, we summarize the utilization of pre-trained language models for ABSA, which improved the performance of ABSA to a new stage. Besides, techniques for building more practical ABSA systems in cross-domain/lingual scenarios are discussed. Finally, we review some emerging topics and discuss some open challenges to outlook potential future directions of ABSA.},
  archive      = {J_TKDE},
  author       = {Wenxuan Zhang and Xin Li and Yang Deng and Lidong Bing and Wai Lam},
  doi          = {10.1109/TKDE.2022.3230975},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11019-11038},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on aspect-based sentiment analysis: Tasks, methods, and challenges},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust and generalized framework for adversarial graph
embedding. <em>TKDE</em>, <em>35</em>(11), 11004–11018. (<a
href="https://doi.org/10.1109/TKDE.2023.3235944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding is essential for graph mining tasks. With the prevalence of graph data in real-world applications, many methods have been proposed in recent years to learn high-quality graph embedding for various types of graphs, among which the Generative Adversarial Networks (GAN) based methods attract increasing attention among researchers. However, most GAN-based generator-discriminator frameworks randomly generate the negative samples from the original graph distributions to enhance the training process of the discriminator without considering the noise. In addition, most of these methods only focus on the explicit graph structures and cannot fully capture complex semantics of edges such as various relationships or asymmetry. In order to address these issues, we propose a robust and generalized framework named AGE. It generates fake neighbors as the enhanced negative samples from the implicit distribution, and enables the discriminator and generator to jointly learn robust and generalized node representations. Based on this framework, we propose three models to handle three types of graph data and derive the corresponding optimization algorithms, namely the UG-AGE and DG-AGE for undirected and directed homogeneous graphs, respectively, and the HIN-AGE for heterogeneous information networks. Extensive experiments show that our methods consistently and significantly outperform existing state-of-the-art methods across multiple graph mining tasks.},
  archive      = {J_TKDE},
  author       = {Jianxin Li and Xingcheng Fu and Shijie Zhu and Hao Peng and Senzhang Wang and Qingyun Sun and Philip S. Yu and Lifang He},
  doi          = {10.1109/TKDE.2023.3235944},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {11004-11018},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A robust and generalized framework for adversarial graph embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A neural expectation-maximization framework for noisy
multi-label text classification. <em>TKDE</em>, <em>35</em>(11),
10992–11003. (<a
href="https://doi.org/10.1109/TKDE.2022.3223067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification (MLTC) has a wide range of real-world applications. Neural networks recently promoted the performance of MLTC models. Training these neural-network models relies on sufficient accurately labelled data. However, manually annotating large-scale multi-label text classification datasets is expensive and impractical for many applications. Weak supervision techniques have thus been developed to reduce the cost of annotating text corpus. However, these techniques introduce noisy labels into the training data and may degrade the model performance. This paper aims to deal with such noise-label problems in MLTC in both single-instance and multi-instance settings. We build a novel Neural Expectation-Maximization Framework (nEM) that combines neural networks with probabilistic modelling. The nEM framework produces text representations using neural-network text encoders and is optimized with the Expectation-Maximization algorithm. It naturally considers the noisy labels during learning by iteratively updating the model parameters and estimating the distribution of the ground-truth labels. We evaluate our nEM framework in multi-instance noisy MLTC on a benchmark relation extraction dataset constructed by distant supervision and in single-instance noisy MLTC on synthetic noisy datasets constructed by keywords supervision and label flipping. The experimental results demonstrate that nEM significantly improves upon baseline models in both single-instance and multi-instance noisy MLTC tasks. The experiment analysis suggests that our nEM framework efficiently reduces the noisy labels in MLTC datasets and significantly improves model performance.},
  archive      = {J_TKDE},
  author       = {Junfan Chen and Richong Zhang and Jie Xu and Chunming Hu and Yongyi Mao},
  doi          = {10.1109/TKDE.2022.3223067},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {10992-11003},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A neural expectation-maximization framework for noisy multi-label text classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-type transferable method for missing link prediction
in heterogeneous social networks. <em>TKDE</em>, <em>35</em>(11),
10981–10991. (<a
href="https://doi.org/10.1109/TKDE.2022.3233481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous social networks, which are characterized by diverse interaction types, have resulted in new challenges for missing link prediction. Most deep learning models tend to capture type-specific features to maximize the prediction performances on specific link types. However, the types of missing links are uncertain in heterogeneous social networks; this restricts the prediction performances of existing deep learning models. To address this issue, we propose a multi-type transferable method ( $MTTM$ ) for missing link prediction in heterogeneous social networks, which exploits adversarial neural networks to remain robust against type differences. It comprises a generative predictor and a discriminative classifier. The generative predictor can extract link representations and predict whether the unobserved link is a missing link. To generalize well for different link types to improve the prediction performance, it attempts to deceive the discriminative classifier by learning transferable feature representations among link types. In order not to be deceived, the discriminative classifier attempts to accurately distinguish link types, which indirectly helps the generative predictor judge whether the learned feature representations are transferable among link types. Finally, the integrated $MTTM$ is constructed on this minimax two-player game between the generative predictor and discriminative classifier to predict missing links based on transferable feature representations among link types. Extensive experiments show that the proposed $MTTM$ can outperform state-of-the-art baselines for missing link prediction in heterogeneous social networks.},
  archive      = {J_TKDE},
  author       = {Huan Wang and Ziwen Cui and Ruigang Liu and Lei Fang and Ying Sha},
  doi          = {10.1109/TKDE.2022.3233481},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {10981-10991},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A multi-type transferable method for missing link prediction in heterogeneous social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight and accurate spatial-temporal transformer for
traffic forecasting. <em>TKDE</em>, <em>35</em>(11), 10967–10980. (<a
href="https://doi.org/10.1109/TKDE.2022.3233086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the forecasting problem for traffic with dynamic, possibly periodical, and joint spatial-temporal dependency between regions. Given the aggregated inflow and outflow traffic of regions in a city from time slots 0 to $t - 1$ , we predict the traffic at time $t$ for any region. Prior arts in the area often considered the spatial and temporal dependencies in a decoupled manner, or were rather computationally intensive in training with a large number of hyper-parameters which needed tuning. We propose ST-TIS, a novel, lightweight and accurate S patial- T emporal T ransformer with i nformation fusion and region s ampling for traffic forecasting. ST-TIS extends the canonical Transformer with information fusion and region sampling. The information fusion module captures the complex spatial-temporal dependency between regions. The region sampling module is to improve the efficiency and prediction accuracy, cutting the computation complexity for dependency learning from $O(n^{2})$ to $O(n\sqrt{n})$ , where $n$ is the number of regions. With far fewer parameters than state-of-the-art deep learning models, ST-TIS&#39;s offline training is significantly faster in terms of tuning and computation (with a reduction of up to $90\%$ on training time and network parameters). Notwithstanding such training efficiency, extensive experiments show that ST-TIS is substantially more accurate in online prediction than state-of-the-art approaches (with an average improvement of $9.5\%$ on RMSE, and $12.4\%$ on MAPE compared to STDN and DSAN).},
  archive      = {J_TKDE},
  author       = {Guanyao Li and Shuhan Zhong and Xingdong Deng and Letian Xiang and S.-H. Gary Chan and Ruiyuan Li and Yang Liu and Ming Zhang and Chih-Chieh Hung and Wen-Chih Peng},
  doi          = {10.1109/TKDE.2022.3233086},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {10967-10980},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A lightweight and accurate spatial-temporal transformer for traffic forecasting},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A game-theoretic federated learning framework for data
quality improvement. <em>TKDE</em>, <em>35</em>(11), 10952–10966. (<a
href="https://doi.org/10.1109/TKDE.2022.3230959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a promising distributed machine learning paradigm that has been playing a significant role in privacy-preserving machine learning tasks. However, alongside all its achievements, the framework has limitations. First, traditional frameworks assume that all clients want to improve model accuracy and so participation is voluntary. However, in reality, clients usually want to be appropriately compensated for the data and resources they will need to commit to the training process before contributing. Second, today&#39;s frameworks allow clients to perturb their parameter updates locally, which introduces a great deal of noise to the trained model and can seriously impact model accuracy. To address these concerns, we have developed a private reward game that incentivizes clients to contribute high-quality data to the training process. The game converges to a Nash equilibrium under the guarantee of joint differential privacy, and each client maximizes their reward following an equilibrium strategy. The noise injected into the model is reduced by introducing a centralized differential privacy model that aggregates the parameters and compensates clients via a data trading market. Experimental simulations show the rationales behind and effectiveness of the proposed game approach. Additionally, we present comparisons between different training models to demonstrate the performance of the proposed approach in real-world scenarios.},
  archive      = {J_TKDE},
  author       = {Lefeng Zhang and Tianqing Zhu and Ping Xiong and Wanlei Zhou and Philip S. Yu},
  doi          = {10.1109/TKDE.2022.3230959},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {10952-10966},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A game-theoretic federated learning framework for data quality improvement},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for accurate community detection on signed
networks using adversarial learning. <em>TKDE</em>, <em>35</em>(11),
10937–10951. (<a
href="https://doi.org/10.1109/TKDE.2022.3231104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a framework for embedding-based community detection on signed networks, namely A dversarial learning of B alanced triangle for C ommunity detection, in short ${{\sf ABC}}$ . It first represents all the nodes of a signed network as vectors in low-dimensional embedding space and conducts a clustering algorithm (e.g., k -means) on vectors, thereby detecting a community structure in the network. When performing the embedding process, ${{\sf ABC}}$ learns only the edges belonging to balanced triangles whose edge signs follow the balance theory, significantly excluding noise edges in learning. To address the sparsity of balanced triangles in a signed network, ${{\sf ABC}}$ learns not only the edges in balanced real -triangles but those in balanced virtual -triangles that do not actually exist but are produced by our generator. Finally, ${{\sf ABC}}$ employs adversarial learning to generate more-realistic balanced virtual-triangles with less noise edges. Through extensive experiments using seven real-world networks, we validate the effectiveness of (1) learning edges belonging to balanced real/virtual-triangles and (2) employing adversarial learning for signed network embedding. We show that ${{\sf ABC}}$ consistently and significantly outperforms the state-of-the-art community detection methods in all datasets.},
  archive      = {J_TKDE},
  author       = {David Y. Kang and Woncheol Lee and Yeon-Chang Lee and Kyungsik Han and Sang-Wook Kim},
  doi          = {10.1109/TKDE.2022.3231104},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {10937-10951},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A framework for accurate community detection on signed networks using adversarial learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A crowd-enabled approach for privacy-enhanced and
personalized safe route planning for fixed or flexible destinations.
<em>TKDE</em>, <em>35</em>(11), 10922–10936. (<a
href="https://doi.org/10.1109/TKDE.2023.3234703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring travelers’ safety on roads has become a research challenge in recent years. We introduce a novel safe route planning problem and develop an efficient solution to ensure travelers’ safety on roads. Though few research attempts have been made in this regard, all of them assume that people share their sensitive travel experiences with a centralized entity for finding the safest routes, which is not ideal in practice for privacy reasons. Furthermore, existing works formulate safe route planning in ways that do not meet a traveler&#39;s need for safe travel on roads. Our approach finds the safest routes within a user-specified distance threshold based on the personalized travel experience of the knowledgeable crowd without involving any centralized computation. We develop a privacy-preserving model to quantify the travel experience of a user into personalized safety scores. Our algorithms, direct and iterative for finding the safest route further enhance user privacy by minimizing the exposure of personalized safety scores with others. Our safe route planner can find the safest routes for individuals and groups by considering both a fixed and a set of flexible destination locations. Extensive experiments using real datasets show that our approach finds the safest route in seconds. Compared to the direct algorithm, our iterative algorithm requires 43\% less exposure of personalized safety scores.},
  archive      = {J_TKDE},
  author       = {Fariha Tabassum Islam and Tanzima Hashem and Rifat Shahriyar},
  doi          = {10.1109/TKDE.2023.3234703},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {10922-10936},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A crowd-enabled approach for privacy-enhanced and personalized safe route planning for fixed or flexible destinations},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A contemporary and comprehensive survey on streaming tensor
decomposition. <em>TKDE</em>, <em>35</em>(11), 10897–10921. (<a
href="https://doi.org/10.1109/TKDE.2022.3230874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor decomposition has been demonstrated to be successful in a wide range of applications, from neuroscience and wireless communications to social networks. In an online setting, factorizing tensors derived from multidimensional data streams is however nontrivial due to several inherent problems of real-time stream processing. In recent years, many research efforts have been dedicated to developing online techniques for decomposing such tensors, resulting in significant advances in streaming tensor decomposition or tensor tracking. This topic is emerging and enriches the literature on tensor decomposition, particularly from the data stream analystics perspective. Thus, it is imperative to carry out an overview of tensor tracking to help researchers and practitioners understand its developments and achievements, summarise the current trends and advances, and identify challenging problems. In this article, we provide a contemporary and comprehensive survey on different types of tensor tracking techniques. We particularly categorize the state-of-the-art methods into three main groups: streaming CP decompositions, streaming Tucker decompositions, and streaming decompositions under other tensor formats (i.e., tensor-train, t-SVD, and BTD). In each group, we further divide the existing algorithms into sub-categories based on their main optimization framework and model architectures. Finally, we present several applications, research challenges, open problems, and potential directions of tensor tracking in the future.},
  archive      = {J_TKDE},
  author       = {Le Trung Thanh and Karim Abed-Meraim and Nguyen Linh Trung and Adel Hafiane},
  doi          = {10.1109/TKDE.2022.3230874},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {11},
  pages        = {10897-10921},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A contemporary and comprehensive survey on streaming tensor decomposition},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical contrastive learning enhanced heterogeneous
graph neural network. <em>TKDE</em>, <em>35</em>(10), 10884–10896. (<a
href="https://doi.org/10.1109/TKDE.2023.3264691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph neural networks (HGNNs) as an emerging technique have shown superior capacity of dealing with heterogeneous information network (HIN). However, most HGNNs follow a semi-supervised learning manner, which notably limits their wide use in reality since labels are usually scarce in real applications. Recently, contrastive learning, a self-supervised method, becomes one of the most exciting learning paradigms and shows great potential when there are no labels. In this paper, we study the problem of self-supervised HGNNs and propose a novel co-contrastive learning mechanism for HGNNs, named HeCo. Different from traditional contrastive learning which only focuses on contrasting positive and negative samples, HeCo employs cross-view contrastive mechanism. Specifically, two views of a HIN (network schema and meta-path views) are proposed to learn node embeddings, so as to capture both of local and high-order structures simultaneously. Then the cross-view contrastive learning, as well as a view mask mechanism, is proposed, which is able to extract the positive and negative embeddings from two views. This enables the two views to collaboratively supervise each other and finally learn high-level node embeddings. Moreover, to further boost the performance of HeCo, two additional methods are designed to generate harder negative samples with high quality. The essence of HeCo is to make positive samples from different views close to each other by cross-view contrast, and learn the factors invariant to two proposed views. However, besides the invariant factors, view-specific factors complementally provide the diverse structure information between different nodes, which also should be contained into the final embeddings. Therefore, we need to further explore each view independently and propose a modified model, called HeCo++. Specifically, HeCo++ conducts hierarchical contrastive learning, including cross-view and intra-view contrasts, which aims to enhance the mining of respective structures. Extensive experiments conducted on a variety of real-world networks show the superior performance of the proposed methods over the state-of-the-arts.},
  archive      = {J_TKDE},
  author       = {Nian Liu and Xiao Wang and Hui Han and Chuan Shi},
  doi          = {10.1109/TKDE.2023.3264691},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10884-10896},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical contrastive learning enhanced heterogeneous graph neural network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly supervised concept map generation through task-guided
graph translation. <em>TKDE</em>, <em>35</em>(10), 10871–10883. (<a
href="https://doi.org/10.1109/TKDE.2023.3252588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the rapid development of concept map generation techniques due to their advantages in providing well-structured summarization of knowledge from free texts. Traditional unsupervised methods do not generate task-oriented concept maps, whereas deep generative models require large amounts of training data. In this work, we present GT-D2G (Graph Translation-based Document To Graph), an automatic concept map generation framework that leverages generalized NLP pipelines to derive semantic-rich initial graphs, and translates them into more concise structures under the weak supervision of downstream task labels. The concept maps generated by GT-D2G can provide interpretable summarization of structured knowledge for the input texts, which are demonstrated through human evaluation and case studies on three real-world corpora. Further experiments on the downstream task of document classification show that GT-D2G beats other concept map generation methods. Moreover, we specifically validate the labeling efficiency of GT-D2G in the label-efficient learning setting and the flexibility of generated graph sizes in controlled hyper-parameter studies.},
  archive      = {J_TKDE},
  author       = {Jiaying Lu and Xiangjue Dong and Carl Yang},
  doi          = {10.1109/TKDE.2023.3252588},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10871-10883},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Weakly supervised concept map generation through task-guided graph translation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Meta auxiliary learning for top-k recommendation.
<em>TKDE</em>, <em>35</em>(10), 10857–10870. (<a
href="https://doi.org/10.1109/TKDE.2022.3223155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are playing a significant role in modern society to alleviate the information/choice overload problem, since Internet users may feel hard to identify the most favorite items or products from millions of candidates. Thanks to the recent successes in computer vision, auxiliary learning has become a powerful means to improve the performance of a target (primary) task. Even though helpful, the auxiliary learning scheme is still less explored in recommendation models. To integrate the auxiliary learning scheme, we propose a novel meta auxiliary learning framework to facilitate the recommendation model training, i.e., user and item latent representations. Specifically, we construct two self-supervised learning tasks, regarding both users and items, as auxiliary tasks to enhance the representation effectiveness of users and items. Then the auxiliary and primary tasks are further modeled as a meta learning paradigm to adaptively control the contribution of auxiliary tasks for improving the primary recommendation task. This is achieved by an implicit gradient method guaranteeing less time complexity compared with conventional meta learning methods. Via a comparison using four real-world datasets with a number of state-of-the-art methods, we show that the proposed model outperforms the best existing models on the Top-K recommendation by 3\% to 23\%.},
  archive      = {J_TKDE},
  author       = {Ximing Li and Chen Ma and Guozheng Li and Peng Xu and Chi Harold Liu and Ye Yuan and Guoren Wang},
  doi          = {10.1109/TKDE.2022.3223155},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10857-10870},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Meta auxiliary learning for top-K recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Directed acyclic graph learning on attributed heterogeneous
network. <em>TKDE</em>, <em>35</em>(10), 10845–10856. (<a
href="https://doi.org/10.1109/TKDE.2023.3266453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the directed acyclic graph (DAG) among causal variables is a fundamental pre-task in causal discovery. Available DAG learning solutions canonically focus on homogeneous nodes with multiple variables and assume i.i.d. samples, how to learn DAG on typical attributed heterogeneous network (AHN) composed with different types of inter-dependent nodes and diverse attributes is a practical but more difficult task. In this paper, we propose HetDAG to identify DAG among nodes from heterogeneous network. HetDAG first embeds different types of node attributes and aggregates these embeddings as the node&#39;s raw representation. Then it uses contrastive learning with prior network structure to explore latent relationships between nodes and update the representation. Next, HetDAG introduces an attention-based DAG learning module that takes node representations as input to search DAG and orient edges between nodes. To the best of our knowledge, HetDAG is the first study to learn DAG on heterogeneous networks. Extensive experiments on both semi-synthetic and real data show that HetDAG can learn DAG in an efficacy way and outperforms the state-of-the-art approaches. The results on real biological networks confirm that HetDAG can find out the causal relations between lncRNAs and miRNAs.},
  archive      = {J_TKDE},
  author       = {Jiaxuan Liang and Jun Wang and Guoxian Yu and Wei Guo and Carlotta Domeniconi and Maozu Guo},
  doi          = {10.1109/TKDE.2023.3266453},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10845-10856},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Directed acyclic graph learning on attributed heterogeneous network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DP2-pub: Differentially private high-dimensional data
publication with invariant post randomization. <em>TKDE</em>,
<em>35</em>(10), 10831–10844. (<a
href="https://doi.org/10.1109/TKDE.2023.3265605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large amount of high-dimensional and heterogeneous data appear in practical applications, which are often published to third parties for data analysis, recommendations, targeted advertising, and reliable predictions. However, publishing these data may disclose personal sensitive information, resulting in an increasing concern on privacy violations. Privacy-preserving data publishing has received considerable attention in recent years. Unfortunately, the differentially private publication of high dimensional data remains a challenging problem. In this paper, we propose a differentially private high-dimensional data publication mechanism (DP2-Pub) that runs in two phases: a Markov-blanket-based attribute clustering phase and an invariant post randomization (PRAM) phase. Specifically, splitting attributes into several low-dimensional clusters with high intra-cluster cohesion and low inter-cluster coupling helps obtain a reasonable allocation of privacy budget, while a double-perturbation mechanism satisfying local differential privacy facilitates an invariant PRAM to ensure no loss of statistical information and thus significantly preserves data utility. We also extend our DP2-Pub mechanism to the scenario with a semi-honest server which satisfies local differential privacy. We conduct extensive experiments on four real-world datasets and the experimental results demonstrate that our mechanism can significantly improve the data utility of the published data while satisfying differential privacy.},
  archive      = {J_TKDE},
  author       = {Honglu Jiang and Haotian Yu and Xiuzhen Cheng and Jian Pei and Robert Pless and Jiguo Yu},
  doi          = {10.1109/TKDE.2023.3265605},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10831-10844},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DP2-pub: Differentially private high-dimensional data publication with invariant post randomization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DEMOS: Clustering by pruning a density-boosting cluster tree
of density mounts. <em>TKDE</em>, <em>35</em>(10), 10814–10830. (<a
href="https://doi.org/10.1109/TKDE.2023.3266451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing clustering algorithms require presetting cluster number and often fail to capture complex shapes. Herein, we propose a clustering algorithm by pruning a density-boosting cluster tree of density mounts—DEnsity MOuntains Separation clustering algorithm (DEMOS). A cluster is assumed to be a density-connected area with multiple (or a single) density mounts (i.e., single-peak clusters) and a relatively large dis-connectivity from density-connected areas of higher densities. Based on this assumption, DEMOS can easily detect the number of clusters and robustly reconstruct their complex shapes. It first builds the dataset into a peak graph, where each density peak represents a density mount. A multi-valley-link-based connectivity estimation method is embedded to efficiently estimate the connectivity between density peaks during peak graph building. Then, by applying a new linkage metric designed based on our assumption, DEMOS builds density mounts into a reasonably density-boosting cluster tree. After obtaining a robust center detection in a clarity-enhancing decision graph (i.e., a two-dimensional plot for detecting centers), DEMOS prunes the cluster tree into final clusters to finish clustering. Experimental results on both synthetic and real datasets demonstrated the effectiveness of DEMOS and its applicability to large-scale data clustering.},
  archive      = {J_TKDE},
  author       = {Junyi Guan and Sheng Li and Xiaojun Chen and Xiongxiong He and Jiajia Chen},
  doi          = {10.1109/TKDE.2023.3266451},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10814-10830},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DEMOS: Clustering by pruning a density-boosting cluster tree of density mounts},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Single-site perishable inventory management under
uncertainties: A deep reinforcement learning approach. <em>TKDE</em>,
<em>35</em>(10), 10807–10813. (<a
href="https://doi.org/10.1109/TKDE.2023.3250249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online lot sizing for perishable materials in an uncertain environment is a fundamental problem for inventory planning and has been studied for several decades. In this article, we study a novel setting of the l ot s izing problem, considering p erishable materials, m ultiple suppliers, u ncertain demands and lead time (LS-PMU), which captures the inventory planning task in real life better than existing lot sizing problems. We present theoretical results of the best possible competitive ratio an online algorithm can achieve for the LS-PMU problem. We then develop a reinforcement learning-based algorithm called RL4LS to intelligently choose the supplier and decide the order quantity in each time period. We conduct extensive experiments on both real and synthetic datasets to verify that RL4LS outperforms existing algorithms in terms of effectiveness and efficiency, e.g., RL4LS improves the effectiveness by 44\% and runs two orders of magnitude faster than the state-of-the-art algorithm IBFA .},
  archive      = {J_TKDE},
  author       = {Kaixin Wang and Cheng Long and Darrell Joshua Ong and Jie Zhang and Xue-Ming Yuan},
  doi          = {10.1109/TKDE.2023.3250249},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10807-10813},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Single-site perishable inventory management under uncertainties: A deep reinforcement learning approach},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Efficient community search in edge-attributed graphs.
<em>TKDE</em>, <em>35</em>(10), 10790–10806. (<a
href="https://doi.org/10.1109/TKDE.2023.3267550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a graph, searching for a community containing a query vertex is a fundamental problem and has found many applications. Most existing community search models are based on non-attributed or vertex-attributed graphs. In many real-world graphs, however, the edges carry the richest information to describe the interactions between vertices; hence, it is important to take the information into account in community search. In this paper, we conduct a pioneer study on the community search on edge-attributed graphs. We proposed the Edge-Attributed Community Search (EACS) problem, which aims to extract a subgraph that contains the given query vertex while its edges have the maximum attribute similarity. We prove that the EACS problem is NP-hard and propose both exact and 2-approximation algorithms to address EACS. Our exact algorithms run up to 2320.34 times faster than the baseline solution. Our approximate algorithms further improve the efficiency by up to 2.93 times. We conducted extensive experiments to demonstrate the efficiency and effectiveness of our algorithms.},
  archive      = {J_TKDE},
  author       = {Ling Li and Yuhai Zhao and Siqiang Luo and Guoren Wang and Zhengkui Wang},
  doi          = {10.1109/TKDE.2023.3267550},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10790-10806},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient community search in edge-attributed graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HOFD: An outdated fact detector for knowledge bases.
<em>TKDE</em>, <em>35</em>(10), 10775–10789. (<a
href="https://doi.org/10.1109/TKDE.2023.3248223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge bases (KBs), which store high-quality information, are crucial for many applications, such as enhancing search results and serving as external sources for data cleaning. Not surprisingly, there exist outdated facts in most KBs due to the rapid change of information. Naturally, it is important to keep KBs up-to-date. Traditional wisdom has investigated the problem of using reference data (such as new facts extracted from the news) to detect outdated facts in KBs. However, existing approaches can only cover a small percentage of facts in KBs. In this paper, we propose HOFD , a novel human-in-the-loop approach for outdated fact detection in KBs. HOFD trains a binary classifier using features such as historical update frequency and update time of a fact to compute the likelihood of a fact in a KB to be outdated. Then, HOFD interacts with humans to verify whether a fact with high likelihood is indeed outdated. In addition, HOFD also uses logical rules to detect more outdated facts based on human feedback. The outdated facts detected by the logical rules will also be fed back to train the ML model further for data augmentation . Extensive experiments on real-world KBs, such as Yago and DBpedia, show the effectiveness of our solution.},
  archive      = {J_TKDE},
  author       = {Shuang Hao and Chengliang Chai and Guoliang Li and Nan Tang and Ning Wang and Xiang Yu},
  doi          = {10.1109/TKDE.2023.3248223},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10775-10789},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HOFD: An outdated fact detector for knowledge bases},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust bidirectional poly-matching. <em>TKDE</em>,
<em>35</em>(10), 10762–10774. (<a
href="https://doi.org/10.1109/TKDE.2023.3266480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental problem in many scenarios is to match entities across two data sources. It is frequently presumed in prior work that entities to be matched are of comparable granularity. In this work, we address one-to-many or poly-matching in the scenario where entities have varying granularity. A distinctive feature of our problem is its bidirectional nature, where the ‘one’ or the ‘many’ could come from either source arbitrarily. Moreover, to deal with diverse entity representations that give rise to noisy similarity values, we incorporate novel notions of receptivity and reclusivity into a robust matching objective. As the optimal solution to the resulting formulation is proven computationally intractable, we propose more scalable yet still performant heuristics. Experiments on multiple real-life datasets showcase the effectiveness and outperformance of our proposed algorithms over baselines.},
  archive      = {J_TKDE},
  author       = {Ween Jiann Lee and Maksim Tkachenko and Hady W. Lauw},
  doi          = {10.1109/TKDE.2023.3266480},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10762-10774},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust bidirectional poly-matching},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multi-scale adaptive graph neural network for multivariate
time series forecasting. <em>TKDE</em>, <em>35</em>(10), 10748–10761.
(<a href="https://doi.org/10.1109/TKDE.2023.3268199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series (MTS) forecasting plays an important role in the automation and optimization of intelligent applications. It is a challenging task, as we need to consider both complex intra-variable dependencies and inter-variable dependencies. Existing works only learn temporal patterns with the help of single inter-variable dependencies. However, there are multi-scale temporal patterns in many real-world MTS. Single inter-variable dependencies make the model prefer to learn one type of prominent and shared temporal patterns. In this article, we propose a multi-scale adaptive graph neural network (MAGNN) to address the above issue. MAGNN exploits a multi-scale pyramid network to preserve the underlying temporal dependencies at different time scales. Since the inter-variable dependencies may be different under distinct time scales, an adaptive graph learning module is designed to infer the scale-specific inter-variable dependencies without pre-defined priors. Given the multi-scale feature representations and scale-specific inter-variable dependencies, a multi-scale temporal graph neural network is introduced to jointly model intra-variable dependencies and inter-variable dependencies. After that, we develop a scale-wise fusion module to effectively promote the collaboration across different time scales, and automatically capture the importance of contributed temporal patterns. Experiments on six real-world datasets demonstrate that MAGNN outperforms the state-of-the-art methods across various settings.},
  archive      = {J_TKDE},
  author       = {Ling Chen and Donghui Chen and Zongjiang Shang and Binqing Wu and Cen Zheng and Bo Wen and Wei Zhang},
  doi          = {10.1109/TKDE.2023.3268199},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10748-10761},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-scale adaptive graph neural network for multivariate time series forecasting},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximizing mutual information across feature and topology
views for representing graphs. <em>TKDE</em>, <em>35</em>(10),
10735–10747. (<a
href="https://doi.org/10.1109/TKDE.2023.3264512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, maximizing mutual information has emerged as a powerful tool for unsupervised graph representation learning. Existing methods are typically effective in capturing graph information from the topology view but consistently ignore the node feature view. To circumvent this problem, we propose a novel method by exploiting mutual information maximization across feature and topology views. Specifically, we first construct the feature graph to capture the underlying structure of nodes in feature spaces by measuring the distance between pairs of nodes. Then we use a cross-view representation learning module to capture both local and global information content across feature and topology views on graphs. To model the information shared by the feature and topology spaces, we develop a common representation learning module by using mutual information maximization and reconstruction loss minimization. Here, minimizing reconstruction loss forces the model to learn the shared information of feature and topology spaces. To explicitly encourage diversity between graph representations from the same view, we also introduce a disagreement regularization to enlarge the distance between representations from the same view. Experiments on synthetic and real-world datasets demonstrate the effectiveness of integrating feature and topology views. In particular, compared with the previous supervised methods, the proposed method achieves comparable or even better performance under the unsupervised representation and linear evaluation protocol.},
  archive      = {J_TKDE},
  author       = {Xiaolong Fan and Maoguo Gong and Yue Wu and Hao Li},
  doi          = {10.1109/TKDE.2023.3264512},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10735-10747},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Maximizing mutual information across feature and topology views for representing graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Preference-aware group task assignment in spatial
crowdsourcing: Effectiveness and efficiency. <em>TKDE</em>,
<em>35</em>(10), 10722–10734. (<a
href="https://doi.org/10.1109/TKDE.2023.3266735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the diffusion of online mobile devices with geo-location capabilities, the infrastructure necessary for real-world deployment of Spatial Crowdsourcing (SC), where so-called mobile workers are assigned location-sensitive tasks, is in place. Some SC tasks cannot be completed by a single worker due to their complexity, but rather must be assigned to and completed by a group of users. Achieving such group assignments that satisfy all group members evenly is an open challenge. To address this challenge, we propose a novel preference-aware group task assignment framework encompassing two components: Mutual Information-based Preference Modeling (MIPM) and Preference-aware Group Task Assignment (PGTA). The MIPM component learns the preferences of groups contrastively by maximizing the mutual information between workers and worker groups based on worker-task and group-task interaction data and by using an attention mechanism to weight group members adaptively. In addition, curriculum negative sampling is adopted to generate a small number of negative workers for each worker group, following the principles of curriculum learning. Next, the PGTA component offers an optimal task assignment algorithm that employs tree decomposition to assign tasks to appropriate worker groups, with the aim of maximizing the number of task assignments while prioritizing more interested groups when assigning tasks. The task assignment framework also features preference-constrained pruning of unpromising worker groups to speed up the assignment process. Finally, we report extensive experiments that offer evidence of the effectiveness and practicality of the paper&#39;s proposal.},
  archive      = {J_TKDE},
  author       = {Yan Zhao and Jiaxin Liu and Yunchuan Li and Dalin Zhang and Christian S. Jensen and Kai Zheng},
  doi          = {10.1109/TKDE.2023.3266735},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10722-10734},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Preference-aware group task assignment in spatial crowdsourcing: Effectiveness and efficiency},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Top-<span class="math inline"><em>k</em></span> community
similarity search over large-scale road networks. <em>TKDE</em>,
<em>35</em>(10), 10710–10721. (<a
href="https://doi.org/10.1109/TKDE.2023.3243177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the urbanization and development of infrastructure, the community search over road networks has become increasingly important in many real applications such as urban/city planning, social study on local communities, and community recommendations by real estate agencies. In this article, we propose a novel problem, namely top-$k$k community similarity search ( $Top\text{-}kCS^{2}$ ) over road networks, which efficiently and effectively obtains $k$ spatial communities that are the most similar to a given query community in road-network graphs. In order to efficiently and effectively tackle the $Top\text{-}kCS^{2}$ problem, in this paper, we will design an effective similarity measure between spatial communities, and propose a framework for retrieving $Top\text{-}kCS^{2}$ query answers, which integrates offline pre-processing and online computation phases. Moreover, we also consider a variant, namely continuous top-$k$k community similarity search ( $CTop\text{-}kCS^{2}$ ), where the query community continuously moves along a query line segment. We develop an efficient algorithm to split query line segment into intervals, incrementally obtain similar candidate communities for each interval, and refine actual $CTop\text{-}kCS^{2}$ query answers. Extensive experiments have been conducted on real and synthetic data sets to confirm the efficiency and effectiveness of our proposed $Top\text{-}kCS^{2}$ and $CTop\text{-}kCS^{2}$ approaches under various parameter settings.},
  archive      = {J_TKDE},
  author       = {Niranjan Rai and Xiang Lian},
  doi          = {10.1109/TKDE.2023.3243177},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10710-10721},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Top-$k$ community similarity search over large-scale road networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Summarizing provenance of aggregate query results in
relational databases. <em>TKDE</em>, <em>35</em>(10), 10695–10709. (<a
href="https://doi.org/10.1109/TKDE.2023.3265840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data provenance is any information about the origin of a piece of data and the process that led to its creation. Most database provenance work has focused on creating models and semantics to query and generate this provenance information. While comprehensive, provenance information remains large and overwhelming, making it hard for data provenance systems to support data exploration. We present a new approach to provenance exploration that builds on data summarization techniques. We contribute novel summarization schemes for the provenance of aggregation queries and techniques for the fast generation of these summarization schemes. We introduce two types of summaries for aggregate queries. Impact summaries take into account the impact of specific groups of tuples in the provenance of the query on an aggregate result, and comparative summaries allow users to compare the provenance of two aggregate results. We also present algorithms for efficient computation of these summaries, implement optimizations using data sampling and feature selection, and conduct experiments and a user survey to show the feasibility and relevance of our approaches.},
  archive      = {J_TKDE},
  author       = {Omar AlOmeir and Eugenie Y. Lai and Mostafa Milani and Rachel Pottinger},
  doi          = {10.1109/TKDE.2023.3265840},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10695-10709},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Summarizing provenance of aggregate query results in relational databases},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). C2IMUFS: Complementary and consensus learning-based
incomplete multi-view unsupervised feature selection. <em>TKDE</em>,
<em>35</em>(10), 10681–10694. (<a
href="https://doi.org/10.1109/TKDE.2023.3266595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view unsupervised feature selection (MUFS) has been demonstrated as an effective technique to reduce the dimensionality of multi-view unlabeled data. The existing methods assume that all of views are complete. However, multi-view data are usually incomplete, i.e., a part of instances are presented on some views but not all views. Besides, learning the complete similarity graph, as an important promising technology in existing MUFS methods, cannot achieve due to the missing views. In this paper, we propose a complementary and consensus learning-based incomplete multi-view unsupervised feature selection method (C $^{2}$ IMUFS) to address the aforementioned issues. Concretely, C $^{2}$ IMUFS integrates feature selection into an extended weighted non-negative matrix factorization model equipped with adaptive learning of view-weights and a sparse $\ell _{2,p}$ -norm, which can offer better adaptability and flexibility. By the sparse linear combinations of multiple similarity matrices derived from different views, a complementary learning-guided similarity matrix reconstruction model is presented to obtain the complete similarity graph in each view. Furthermore, C $^{2}$ IMUFS learns a consensus clustering indicator matrix across different views and embeds it into a spectral graph term to preserve the local geometric structure. Comprehensive experimental results on real-world datasets demonstrate the effectiveness of C $^{2}$ IMUFS compared with state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Yanyong Huang and Zongxin Shen and Yuxin Cai and Xiuwen Yi and Dongjie Wang and Fengmao Lv and Tianrui Li},
  doi          = {10.1109/TKDE.2023.3266595},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10681-10694},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {C2IMUFS: Complementary and consensus learning-based incomplete multi-view unsupervised feature selection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SketchNE: Embedding billion-scale networks accurately in one
hour. <em>TKDE</em>, <em>35</em>(10), 10666–10680. (<a
href="https://doi.org/10.1109/TKDE.2023.3250703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study large-scale network embedding with the goal of generating high-quality embeddings for networks with more than 1 billion vertices and 100 billion edges. Recent attempts LightNE and NetSMF propose to sparsify and factorize the (dense) NetMF matrix for embedding large networks, where NetMF is a theoretically-grounded network embedding method. However, there is a trade-off between their embeddings’ quality and scalability due to their expensive memory requirements, making embeddings less effective under real-world memory constraints. Therefore, we present the SketchNE model, a scalable, effective, and memory-efficient network embedding solution developed for a single machine with CPU only. The main idea of SketchNE is to avoid the explicit construction and factorization of the NetMF matrix either sparsely or densely when producing the embeddings through the proposed sparse-sign randomized single-pass SVD algorithm. We conduct extensive experiments on nine datasets of various sizes for vertex classification and link prediction, demonstrating the consistent outperformance of SketchNE over state-of-the-art baselines in terms of both effectiveness and efficiency. SketchNE costs only 1.0 hours to embed the Hyperlink2012 network with 3.5 billion vertices and 225 billion edges on a CPU-only single machine with embedding superiority (e.g., a 282\% relative HITS@10 gain over LightNE).},
  archive      = {J_TKDE},
  author       = {Yuyang Xie and Yuxiao Dong and Jiezhong Qiu and Wenjian Yu and Xu Feng and Jie Tang},
  doi          = {10.1109/TKDE.2023.3250703},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10666-10680},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SketchNE: Embedding billion-scale networks accurately in one hour},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online learning from incomplete and imbalanced data streams.
<em>TKDE</em>, <em>35</em>(10), 10650–10665. (<a
href="https://doi.org/10.1109/TKDE.2023.3250472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with streaming data has attracted extensive research interest in recent years. Existing online learning approaches have specific assumptions regarding data streams, such as requiring fixed or varying feature spaces with explicit patterns and balanced class distributions. While the data streams generated in many real scenarios commonly have arbitrarily incomplete feature spaces and dynamic imbalanced class distributions, making existing approaches be unsuitable for real applications. To address this issue, this paper proposes a novel O nline L earning from I ncomplete and I mbalanced D ata S treams (OLI $^{2}$ DS) algorithm. OLI $^{2}$ DS has a two-fold main idea: 1) it follows the empirical risk minimization principle to identify the most informative features of incomplete feature spaces, and 2) it develops a dynamic cost strategy to handle imbalanced class distributions in real-time by transforming F-measure optimization into a weighted surrogate loss minimization. To evaluate OLI $^{2}$ DS, we compare it with state-of-the-art related algorithms in three kinds of experiments. First, we adopt 14 real datasets to simulate three scenarios of incomplete feature spaces, i.e., trapezoidal, feature evolvable, and capricious data streams. Second, based on a benchmark online analyzer, we generate 13 datasets to simulate incomplete data streams with different imbalance ratios. Third, we analyze concept drift in two simulated scenes, i.e., online learning and data stream mining, and verify the adaption of OLI $^{2}$ DS on repeated concept drifts and variable imbalance ratios. The results demonstrate that OLI $^{2}$ DS achieves a significantly better performance than its rivals. Besides, a real-world case study on movie review classification is conducted to elaborate on our OLI $^{2}$ DS algorithm&#39;s effectiveness. Code is released at https://github.com/youdianlong/OLI2DS .},
  archive      = {J_TKDE},
  author       = {Dianlong You and Jiawei Xiao and Yang Wang and Huigui Yan and Di Wu and Zhen Chen and Limin Shen and Xindong Wu},
  doi          = {10.1109/TKDE.2023.3250472},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10650-10665},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online learning from incomplete and imbalanced data streams},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting malicious accounts in online developer communities
using deep learning. <em>TKDE</em>, <em>35</em>(10), 10633–10649. (<a
href="https://doi.org/10.1109/TKDE.2023.3237838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online developer communities like GitHub allow a massive number of developers to collaborate. However, the openness of the communities makes them vulnerable to different types of malicious attacks, since attackers can easily join these communities and interact with legitimate users. In this work, we propose GitSec, a deep learning-based solution for detecting malicious accounts in online developer communities. GitSec distinguishes malicious accounts from legitimate ones based on the account profiles, dynamic activity characteristics, as well as social interactions. First, GitSec introduces two user activity sequences and applies a parallel neural network design with an attention mechanism to process the sequences. Second, GitSec constructs two graphs to represent the interactions between users according to their repository operations. Especially, graph neural networks and structural hole theory are employed to deal with the two constructed graphs. Third, GitSec makes use of the descriptive features to enhance the detection performance. The final judgement is made by a decision maker implemented by a supervised machine learning-based classifier. Based on the real-world data of GitHub users, our comprehensive evaluations show that GitSec achieves a better performance than state-of-the-art solutions, with an AUC value of 0.916.},
  archive      = {J_TKDE},
  author       = {Qingyuan Gong and Yushan Liu and Jiayun Zhang and Yang Chen and Qi Li and Yu Xiao and Xin Wang and Pan Hui},
  doi          = {10.1109/TKDE.2023.3237838},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10633-10649},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Detecting malicious accounts in online developer communities using deep learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NMTF-LTM: Towards an alignment of semantics for lifelong
topic modeling. <em>TKDE</em>, <em>35</em>(10), 10616–10632. (<a
href="https://doi.org/10.1109/TKDE.2023.3267496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at mining high quality topics by accumulating and utilizing semantic knowledge for a stream of documents, lifelong topic modeling (LTM) has attracted more and more attentions recently. However, the permutation of topics may change over time, resulting in a semantic misalignment between the topic representations of document chunks across the stream. Such a misalignment deteriorates the model performances of various downstream tasks, while it has been overlooked by the existing lifelong topic models. Towards addressing the misalignment of semantics, we formulate LTM as a problem of non-negative matrix tri-factorization (NMTF) and propose a consolidation framework (i.e., NMTF-LTM) to enforce an alignment in a mapped topic space. In addition, a distributed parallel algorithm, namely PNMTF-LTM, is developed to meet the real-time requirement for large-scale stream processing. Empirical results show that our method can not only obtain a superior alignment of semantics without loss of topic quality, but also achieve effective speedup when deployed to a high performance computing cluster.},
  archive      = {J_TKDE},
  author       = {Zhiqi Lei and Hai Liu and Jiaxing Yan and Yanghui Rao and Qing Li},
  doi          = {10.1109/TKDE.2023.3267496},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10616-10632},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {NMTF-LTM: Towards an alignment of semantics for lifelong topic modeling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scaling stratified stochastic gradient descent for
distributed matrix completion. <em>TKDE</em>, <em>35</em>(10),
10603–10615. (<a
href="https://doi.org/10.1109/TKDE.2023.3253791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stratified SGD (SSGD) is the primary approach for achieving serializable parallel SGD for matrix completion. State-of-the-art parallelizations of SSGD fail to scale due to large communication overhead. During an SGD epoch, these methods send data proportional to one of the dimensions of the rating matrix. We propose a framework for scalable SSGD through significantly reducing the communication overhead via exchanging point-to-point messages utilizing the sparsity of the rating matrix. We provide formulas to represent the essential communication for correctly performing parallel SSGD and we propose a dynamic programming algorithm for efficiently computing them to establish the point-to-point message schedules. This scheme, however, significantly increases the number of messages sent by a processor per epoch from $\mathcal {O}(K)$ to $\mathcal {O}(K^{2})$ for a $K$ -processor system which might limit the scalability. To remedy this, we propose a Hold-and-Combine strategy to limit the upper-bound on the number of messages sent per processor to $\mathcal {O}(K\lg \!K)$ . We also propose a hypergraph partitioning model that correctly encapsulates reducing the communication volume. Experimental results show that the framework successfully achieves a scalable distributed SSGD through significantly reducing the communication overhead. Our code is publicly available at: github.com/nfabubaker/CESSGD},
  archive      = {J_TKDE},
  author       = {Nabil Abubaker and M. Ozan Karsavuran and Cevdet Aykanat},
  doi          = {10.1109/TKDE.2023.3253791},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10603-10615},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Scaling stratified stochastic gradient descent for distributed matrix completion},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairness in graph mining: A survey. <em>TKDE</em>,
<em>35</em>(10), 10583–10602. (<a
href="https://doi.org/10.1109/TKDE.2023.3265598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph mining algorithms have been playing a significant role in myriad fields over the years. However, despite their promising performance on various graph analytical tasks, most of these algorithms lack fairness considerations. As a consequence, they could lead to discrimination towards certain populations when exploited in human-centered applications. Recently, algorithmic fairness has been extensively studied in graph-based applications. In contrast to algorithmic fairness on independent and identically distributed (i.i.d.) data, fairness in graph mining has exclusive backgrounds, taxonomies, and fulfilling techniques. In this survey, we provide a comprehensive and up-to-date introduction of existing literature under the context of fair graph mining. Specifically, we propose a novel taxonomy of fairness notions on graphs, which sheds light on their connections and differences. We further present an organized summary of existing techniques that promote fairness in graph mining. Finally, we discuss current research challenges and open questions, aiming at encouraging cross-breeding ideas and further advances.},
  archive      = {J_TKDE},
  author       = {Yushun Dong and Jing Ma and Song Wang and Chen Chen and Jundong Li},
  doi          = {10.1109/TKDE.2023.3265598},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10583-10602},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fairness in graph mining: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure-aware subspace clustering. <em>TKDE</em>,
<em>35</em>(10), 10569–10582. (<a
href="https://doi.org/10.1109/TKDE.2023.3249765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering has attracted much attention because of its ability to group unlabeled high-dimensional data into multiple subspaces. Existing graph-based subspace clustering methods focus on either the sparsity of data affinity or the low rank of data affinity. Thus, the quality of data affinity plays an essential role in the performance of subspace clustering. However, the real-world data are generally high-dimensional, complex, and heterogeneous multi-source data, so that the data affinity learned by these methods cannot be completely dependent. Moreover, since these approaches always ignore the intrinsic structure of data, their grouping effect is relatively low. In this paper, we propose a novel unsupervised algorithm, called Structure-Aware Subspace Clustering (SASC), to address the above issues. SASC considers local and global correlation structures simultaneously to capture the intrinsic structure. Further, it integrates the captured structure into representation learning to gain a relatively precise data affinity. It is powerful to promote an all-around grouping effect and enhances the robustness and applicability of subspace clustering. Experiments on various benchmark datasets, including bioinformatics, handwritten digit, object image, and speech signal, demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_TKDE},
  author       = {Simin Kou and Xuesong Yin and Yigang Wang and Songcan Chen and Tieming Chen and Zizhao Wu},
  doi          = {10.1109/TKDE.2023.3249765},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10569-10582},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Structure-aware subspace clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-sensitive online adaptive kernel learning for
large-scale imbalanced classification. <em>TKDE</em>, <em>35</em>(10),
10554–10568. (<a
href="https://doi.org/10.1109/TKDE.2023.3266648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced classification is a challenging task in the fields of machine learning, data mining and pattern recognition. Cost-sensitive online algorithms are very important methods for large-scale imbalanced classification problems. At present, most of the cost-sensitive classification algorithms focus on the accuracy of the minority class and ignore the accuracy of the majority class. In order to better balance the accuracy between the minority class and the majority class, in this article, a misclassification cost is presented to ensure that the cost-sensitive online algorithm can better deal with the imbalanced classification problems without signifificantly reducing the accuracy of the majority class. Based on the proposed misclassification cost, a novel cost-sensitive online adaptive kernel learning algorithm is proposed to boost the adaptability of kernel function when data arrives one by one. According to the essential characteristics of the imbalanced binary classification, a cost-sensitive online adaptive kernel learning algorithm is given to handle the large-scale imbalanced multi-class classification problems. Theoretical analysis of the proposed algorithms are provided. Extensive experiments demonstrate that compared with the state-of-the-art imbalanced classification algorithms, the proposed algorithms can significantly improve the classification performances on most of the large-scale imbalanced data sets.},
  archive      = {J_TKDE},
  author       = {Yingying Chen and Zijie Hong and Xiaowei Yang},
  doi          = {10.1109/TKDE.2023.3266648},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10554-10568},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cost-sensitive online adaptive kernel learning for large-scale imbalanced classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Counterfactual graph learning for anomaly detection on
attributed networks. <em>TKDE</em>, <em>35</em>(10), 10540–10553. (<a
href="https://doi.org/10.1109/TKDE.2023.3250523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph anomaly detection is attracting remarkable multidisciplinary research interests ranging from finance, healthcare, and social network analysis. Recent advances on graph neural networks have substantially improved the detection performance via semi-supervised representation learning. However, prior work suggests that deep graph-based methods tend to learn spurious correlations. As a result, they fail to generalize beyond training data distribution. In this article, we aim to identify structural and contextual anomaly nodes in an attributed graph. Based on our preliminary data analyses, spurious correlations can be eliminated with causal subgraph interventions. Therefore, we propose a new graph-based anomaly detection model that can learn causal relations for anomaly detection while generalizing to new environments. To handle situations with varying environments, we steer the generative model to manufacture synthetic environment features, which are exerted on realistic subgraphs to generate counterfactual subgraphs. Further, these counterfactual subgraphs help a few-shot anomaly detection model learn transferable and causal relations across different environments. The experiments on three real-world attributed graphs show that the proposed approach achieves the best performance compared to the state-of-the-art baselines and learns robust causal representations resistant to noises and spurious correlations.},
  archive      = {J_TKDE},
  author       = {Chunjing Xiao and Xovee Xu and Yue Lei and Kunpeng Zhang and Siyuan Liu and Fan Zhou},
  doi          = {10.1109/TKDE.2023.3250523},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10540-10553},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Counterfactual graph learning for anomaly detection on attributed networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive skeleton construction for accurate DAG learning.
<em>TKDE</em>, <em>35</em>(10), 10526–10539. (<a
href="https://doi.org/10.1109/TKDE.2023.3265015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed acyclic graph (DAG) learning plays a key role in causal discovery and many machine learning tasks. Learning a DAG from high-dimensional data always faces scalability problems. A local-to-global DAG learning approach can be scaled to high-dimensional data, however, existing local-to-global DAG learning algorithms employ either the AND-rule or the OR-rule for constructing a DAG skeleton. Simply using either rule, existing local-to-global methods may learn an inaccurate DAG skeleton, leading to unsatisfactory DAG learning performance. To tackle this problem, in this paper, we propose an A daptive D AG L earning (ADL) algorithm. The novel contribution of ADL is that it can simultaneously and adaptively use the AND-rule and the OR-rule to construct an accurate global DAG skeleton. We conduct extensive experiments on both benchmark and real-world datasets, and the experimental results show that ADL is significantly better than some existing local-to-global and global DAG learning algorithms.},
  archive      = {J_TKDE},
  author       = {Xianjie Guo and Kui Yu and Lin Liu and Peipei Li and Jiuyong Li},
  doi          = {10.1109/TKDE.2023.3265015},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10526-10539},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive skeleton construction for accurate DAG learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised adaptive bipartite graph embedding.
<em>TKDE</em>, <em>35</em>(10), 10514–10525. (<a
href="https://doi.org/10.1109/TKDE.2023.3267505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional graph embedding methods, graph construction is sensitive to high-dimensional data with noise and outliers, making an effective exploration of the neighborhood structure of the data difficult. Besides, with these methods, constructing graphs and reducing dimensions are disconnected and cannot be mutually optimized. To address these problems, we propose an unsupervised dimensionality reduction method based on bipartite graph, named unsupervised adaptive bipartite graph embedding (UABGE). First, the anchors are generated from the raw data by K-means or random sampling. Second, the bipartite graph, which is constructed between the samples and the anchors in the low-dimensional subspace, utilizes the adaptive allocation method to assign neighbors for each sample, so that the local structure of high-dimensional data can be captured effectively. Third, we present an objective function that combines bipartite graph construction and projection matrix learning to achieve mutual optimization between them, which can be solved with an alternating optimization algorithm. Finally, the computational complexity and the convergence of the algorithm are analyzed. Experimental results on synthetic data and publicly available datasets illustrate the effectiveness of the proposed method.},
  archive      = {J_TKDE},
  author       = {Jianyong Zhu and Xinyun Chen and Hui Yang and Feiping Nie},
  doi          = {10.1109/TKDE.2023.3267505},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10514-10525},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised adaptive bipartite graph embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EShare+: A data-driven balancing mechanism for bike sharing
systems considering both quality of service and maintenance.
<em>TKDE</em>, <em>35</em>(10), 10497–10513. (<a
href="https://doi.org/10.1109/TKDE.2023.3253725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of sharing economy, we have access to massive sharing systems such as Uber, Airbnb, and bike sharing nowadays. The sharing economy, at its core, is to achieve efficient use of resources. However, the actual usage of shared resources is still unclear to us. Little measurement or analysis, if any, has been conducted to investigate the resource usage patterns with the large-scale data collected from these sharing systems. In this paper, we first analyze the shared bike usage patterns in three typical bike sharing systems based on 140-month multi-event data. From our data-driven analysis, we found that the most used 20\% of shared bikes account for 45\% of total usage, while the least used 20\% of bikes account for less than 1\% of usage. To efficiently utilize shared bikes, we propose a usage balancing design called eShare+ to improve the bike sharing systems by considering both the quality of service and bike maintenance, which includes three key components: (i) a statistical model based on archived data to infer historical usage; (ii) an entropy and contextual LSTM-based prediction model with both real-time and archived data to infer future usage; (iii) a model-driven optimal calibration engine for bike selection to dynamically balance usage. We develop an ID swapping-based evaluation methodology to measure the efficiency of eShare+ with data from three large-scale bike sharing systems including 84,000 bikes and 3,300 service stations. Our results show that eShare+ not only fully utilizes shared bikes with efficient maintenance but also improves the quality of service. In addition, eShare+ also has the potential to be applicable to different fleet sizes.},
  archive      = {J_TKDE},
  author       = {Shuai Wang and Xin Zhu and Guang Wang and Yunhuai Liu and Tian He and Desheng Zhang},
  doi          = {10.1109/TKDE.2023.3253725},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10497-10513},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {EShare+: A data-driven balancing mechanism for bike sharing systems considering both quality of service and maintenance},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PrigSim: Towards privacy-preserving graph similarity search
as a cloud service. <em>TKDE</em>, <em>35</em>(10), 10478–10496. (<a
href="https://doi.org/10.1109/TKDE.2023.3266449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are widely used to model complex structured data in many applications. With the proliferation of cloud computing, it is popular to store and query graphs in the cloud. Among others, graph similarity search, which aims to retrieve from a graph database graphs similar to a query graph, has received wide attentions and benefited various domains such as cheminformatics, computer vision, and more. Deploying graph similarity search services on the cloud, however, raises critical privacy concerns on the information-rich graphs. In this article, we initiate the first study on privacy-preserving graph similarity search in cloud computing. We design, implement, and evaluate PrigSim, a novel system allowing the cloud to host an outsourced encrypted graph database and support secure graph similarity search, where the graph similarity is measured by the well-known metric called graph edit distance. PrigSim is built from a customized and delicate synergy of insights on graph modelling, lightweight cryptography, and data encoding and padding, providing protections for the confidentiality of data content associated with graphs, as well as hiding the connections among vertices. Extensive experiments demonstrate that the security design of PrigSim is accuracy-preserving, and presents modest performance overheads (with $9\times$ - $15\times$ higher query latency than the plaintext baseline).},
  archive      = {J_TKDE},
  author       = {Songlei Wang and Yifeng Zheng and Xiaohua Jia and Hejiao Huang and Cong Wang},
  doi          = {10.1109/TKDE.2023.3266449},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10478-10496},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PrigSim: Towards privacy-preserving graph similarity search as a cloud service},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Semi-supervised entity alignment with global alignment and
local information aggregation. <em>TKDE</em>, <em>35</em>(10),
10464–10477. (<a
href="https://doi.org/10.1109/TKDE.2023.3238993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity alignment is a vital task in knowledge fusion, which aims to align entities from different knowledge graphs and merge them into one single graph. Existing entity alignment models focus on local features and try to minimize the distance between pairs of pre-aligned entities. Despite their success, these models heavily rely on the number of existing pre-aligned entity pairs and the topology information from the rest large set of unaligned entities is still largely unexplored. To overcome the limitation of existing models, we propose a model, termed Global Alignment and Local Information Aggregation, or GALA. GALA constructs global features for the knowledge graphs to be aligned using entity embeddings. It aligns the entities in the graphs by forcing their global features to match with each other and progressively updating the entity embeddings by aggregating local information from the other network. Empirical studies on commonly-used KG alignment data sets confirm the effectiveness of the proposed model.},
  archive      = {J_TKDE},
  author       = {Xuefeng Zhang and Richong Zhang and Junfan Chen and Jaein Kim and Yongyi Mao},
  doi          = {10.1109/TKDE.2023.3238993},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10464-10477},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised entity alignment with global alignment and local information aggregation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-sensitive learning for medical insurance fraud
detection with temporal information. <em>TKDE</em>, <em>35</em>(10),
10451–10463. (<a
href="https://doi.org/10.1109/TKDE.2023.3240431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fraudulent activities within the U.S. healthcare system cost billions of dollars each year and harm the wellbeing of many qualifying beneficiaries. The implementation of an effective fraud detection method has become imperative to secure the welfare of the general public. In this article, we focus on the problem of fraud detection using the current year&#39;s Medicare claims data from the perspective of utilizing temporal information from the previous years. We group the data into temporal trajectories of the key covariates and base our feature engineering around these trajectories. For effective feature engineering on the temporal data, we propose to use the functional principal component analysis (FPCA) method for analyzing the temporal covariates’ trajectory as well as the distributional FPCA for extracting features from the empirical probability density curve of the covariates. Moreover, we introduce the framework of cost-sensitive learning for analyzing the Medicare database to allow for asymmetrical losses in the confusion matrix, such that the classification rule reflects the realistic tradeoff between the fixed cost and the fraud cost. The issue of class imbalance in the database is tackled through the random undersampling scheme. Our results confirm that the trained classifier has a reasonably good prediction performance and a significant percentage of cost savings can be achieved by taking into account the financial cost.},
  archive      = {J_TKDE},
  author       = {Haolun Shi and Mohammad A. Tayebi and Jian Pei and Jiguo Cao},
  doi          = {10.1109/TKDE.2023.3240431},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10451-10463},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cost-sensitive learning for medical insurance fraud detection with temporal information},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient location-based skyline queries with secure r-tree
over encrypted data. <em>TKDE</em>, <em>35</em>(10), 10436–10450. (<a
href="https://doi.org/10.1109/TKDE.2023.3253883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supporting efficient and secure location-based skyline queries on encrypted data, such as private data outsourced to cloud-based systems, remains an ongoing challenge for efficiency due to significant computational costs in the ciphertext domain. To accelerate privacy-preserving skyline queries, the secure index intuitively contributes to an increase in efficiency. However, designing such a secure index is a challenge while protecting the unlinkability of queries. Meanwhile, there exist little work that can commendably assure efficiency and security. In this paper, we demonstrate SecSky, an efficient solution for supporting secure location-based skyline queries through the secure index. To support SecSky, we devise a novel unified structure, named secure R-tree (SR-tree) index, without privacy leakage (especially indirect privacy). Subsequently, we propose a novel secure location-based dominance protocol, which is utilized to calculate the dominance relationship on the SR-tree. Using this protocol as the building block, our secure location-based skyline query protocol integrates SR-tree, permutation and perturbation techniques to facilitate query processing so as to dramatically reduce the computational overhead. Meanwhile, our proposed solution avoids compromising the privacy of datasets, queries, dominance relationship and skyline results. Finally, we analyze the complexity and security of SecSky. Findings from the experimental evaluation show that our proposed scheme outperforms several other protocols by at least 3 orders of magnitude in terms of query efficiency.},
  archive      = {J_TKDE},
  author       = {Zuan Wang and Xiaofeng Ding and Junfeng Lu and Liang Zhang and Pan Zhou and Kim-Kwang Raymond Choo and Hai Jin},
  doi          = {10.1109/TKDE.2023.3253883},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10436-10450},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient location-based skyline queries with secure R-tree over encrypted data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised anomaly detection via neural process.
<em>TKDE</em>, <em>35</em>(10), 10423–10435. (<a
href="https://doi.org/10.1109/TKDE.2023.3266755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many deep (semi-) supervised neural network-based methods have been proposed for anomaly detection, tackling the issue of limited labeled data. They have shown good performance but still face two major challenges. First, insufficient labeled data limits their flexibility. Second, measuring the uncertainty of the prediction, especially when dealing with objects deviating largely from training data, has not been well studied. Another common reason preventing them from prevailing is that they learn a determined function to make predictions from the input. This usually makes the predicted results uncertain and lacks robustness. To address these problems, we propose a novel framework, incorporating the neural process into the semi-supervised anomaly detection paradigm and efficiently using unlabeled data and a handful of labeled data in training. Different from other methods, ours is equivalent to modeling the distribution of functions representing anomalous patterns according to the labeled data rather than learning a single determined function for anomaly detection. Our approach improves the flexibility and robustness under the condition of insufficient training data, and can measure the uncertainty of prediction results. Extensive experiments under real-world datasets demonstrate that our proposed method can significantly improve anomaly detection performance compared to several cutting-edge benchmarks.},
  archive      = {J_TKDE},
  author       = {Fan Zhou and Guanyu Wang and Kunpeng Zhang and Siyuan Liu and Ting Zhong},
  doi          = {10.1109/TKDE.2023.3266755},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10423-10435},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised anomaly detection via neural process},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random deep graph matching. <em>TKDE</em>, <em>35</em>(10),
10411–10422. (<a
href="https://doi.org/10.1109/TKDE.2022.3221084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph matching endeavors to find corresponding nodes across two or more graphs, which plays a fundamental role in many vision and pattern matching tasks. However, existing graph matching algorithms often meet abnormal graphs with missing node features and suffer from numerous cluttered outliers in practical applications. To address these, we propose a novel deep graph matching method called Random Deep Graph Matching (RDGM). Different from the deterministic affinity inference in existing deep graph matching methods, RDGM performs message passing in a random manner during model training through randomly masking some available node features in the source or target graph, so that the affinity inference between nodes is insensitive to specific neighborhoods. In addition, a hierarchical attention graph neural network framework is devised in the node embedding process of RDGM, which can obtain more sufficient high-order structural information to reduce the impact of latent noise on affinity learning. Extensive experiments suggest that the proposed RDGM outperforms state-of-the-art graph matching methods, and demonstrates strong robustness and generalization performance.},
  archive      = {J_TKDE},
  author       = {Yu Xie and Zhiguo Qin and Maoguo Gong and Bin Yu and Jiye Liang},
  doi          = {10.1109/TKDE.2022.3221084},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10411-10422},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Random deep graph matching},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust multi-view clustering through partition integration
on stiefel manifold. <em>TKDE</em>, <em>35</em>(10), 10397–10410. (<a
href="https://doi.org/10.1109/TKDE.2023.3253244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering aims at integrating information from different views to improve clustering performance. Recent methods integrate multiple view-specific partition matrices to seek a consensus one and have demonstrated promising clustering performance in various applications. However, the clustering performance of such methods heavily relies on the consensus partition matrix estimated by the arithmetic mean in euclidean space and thus is highly susceptible to noise corruption. To this end, this article proposes to learn a consensus partition matrix through the geometric mean on the manifold to achieve robust clustering. Specifically, the multiple view-specific partition matrices can be regarded as points residing in the Stiefel manifold and enable a manifold-based integration. Consequently, the view-specific partition matrices are integrated by estimating a consensus partition matrix as the center point on the Stiefel manifold. Such a partition integration boils down to the Fréchet mean problem on a manifold, which is solved by the intrinsic manifold-based optimization and proves effective in providing a more robust estimation against noise. Experimental results on seven benchmark datasets demonstrate the effectiveness and noise-robustness of our proposed method in comparison to eight competitive methods.},
  archive      = {J_TKDE},
  author       = {Yu Hu and Endai Guo and Zhi Xie and Xinwang Liu and Hongmin Cai},
  doi          = {10.1109/TKDE.2023.3253244},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10397-10410},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust multi-view clustering through partition integration on stiefel manifold},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A VAE-based user preference learning and transfer framework
for cross-domain recommendation. <em>TKDE</em>, <em>35</em>(10),
10383–10396. (<a
href="https://doi.org/10.1109/TKDE.2023.3253168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core idea of cross-domain recommendation is to alleviate the problem of data scarcity. Previous methods have made brilliant successes. However, many of them mainly focus on learning an ideal mapping function across-domains, ignoring the user preferences within a specific domain, which leads to suboptimal results. In this paper, we propose a Cross-Domain Recommendation Variational AutoEncoder framework (CDRVAE), a novel extension of a variational autoencoder on cross-domain recommendations for user behaviour distribution modeling. It applies a new hybrid architecture of VAE as the backbone and simultaneously constructs two information flows, within-domain and cross-domain modeling. For the former, an asymmetric codec structure is designed to reconstruct preference distribution from domain-specific latent factors. To relieve the posterior collapse dilemma, a combined prior is employed to increase the distribution complexity. The equivalent transition by a transformation matrix and the unobserved interaction generation by cross-domain reconstruction contribute to the latter. We combine all the above components for the more accurate and reliable user features. Extensive experiments are conducted on three public benchmark datasets to validate the effectiveness of the proposed CDRVAE. Experimental results demonstrate that CDRVAE is consistently superior to other state-of-the-art alternative baseline models.},
  archive      = {J_TKDE},
  author       = {Tong Zhang and Chen Chen and Dan Wang and Jie Guo and Bin Song},
  doi          = {10.1109/TKDE.2023.3253168},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10383-10396},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A VAE-based user preference learning and transfer framework for cross-domain recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the performance intricacies of persistent memory aware
storage engines. <em>TKDE</em>, <em>35</em>(10), 10365–10382. (<a
href="https://doi.org/10.1109/TKDE.2023.3248643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As key components of DBMSs, various storage engines and index structures have been proposed based on incorrect assumptions before PMem hardware is publicly available. Recent studies reveal that there is a significant performance gap in evaluating index structures on real PMem platforms as compared to DRAM-based emulators. However, a comprehensive evaluation for those PMem-aware database storage engines on real PMem hardware is still missing. Meanwhile, dynamic memory management is more important on PMem systems because PMem is slower than DRAM and unfriendly to random small-writes, and ensuring crash-consistency for the metadata of PMem allocators introduces extra overhead. Therefore, it is essential to understand the performance intricacies of PMem-aware database storage engines from the perspective of PMem allocators. This paper presents a systematic evaluation of three PMem-aware database storage engines using representative workloads and a unified benchmarking framework that is integrated with four PMem allocators. Besides the commonly used metrics, the impact of different hardware configurations (such as NUMA and eADR) on performance is also considered. Through in-depth analysis, we reveal caveats and pitfalls on using or designing PMem-aware storage engines and important insights that can serve as guidelines for future development of PMem allocators and other related components.},
  archive      = {J_TKDE},
  author       = {Zhiwen Chen and Wenkui Che and Daokun Hu and Xin He and Jianhua Sun and Hao Chen},
  doi          = {10.1109/TKDE.2023.3248643},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10365-10382},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On the performance intricacies of persistent memory aware storage engines},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint network topology inference via structural fusion
regularization. <em>TKDE</em>, <em>35</em>(10), 10351–10364. (<a
href="https://doi.org/10.1109/TKDE.2023.3264971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint network topology inference represents a canonical problem of jointly learning multiple graph Laplacian matrices from heterogeneous graph signals. In such a problem, a widely employed assumption is that of a simple common component shared among multiple graphs. However, in practice, a more intricate topological pattern, comprising simultaneously of homogeneous and heterogeneous components, would exhibit in multiple graphs. In this paper, we propose a general graph estimator based on a novel structural fusion regularization that enables us to jointly learn multiple graphs with such complex topological patterns, and enjoys rigorous theoretical guarantees. Specifically, in the proposed regularization term, the structural similarity among graphs is characterized by a Gram matrix, which enables us to flexibly model different types of network structural similarities through different Gram matrix choices. Algorithmically, the regularization term, coupling the parameters together, makes the formulated optimization problem intractable, and thus, we develop an implementable algorithm based on the alternating direction method of multipliers (ADMM) to solve it. Theoretically, non-asymptotic statistical analysis is provided, which precisely characterizes the minimum sample size required for the consistency of the graph estimator. This analysis also provides high-probability bounds on the estimation error as a function of graph structural similarities and other key problem parameters. Finally, the superior performance of the proposed method is demonstrated through simulated and real data examples.},
  archive      = {J_TKDE},
  author       = {Yanli Yuan and De Wen Soh and Kun Guo and Zehui Xiong and Tony Q. S. Quek},
  doi          = {10.1109/TKDE.2023.3264971},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10351-10364},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Joint network topology inference via structural fusion regularization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STD: A seasonal-trend-dispersion decomposition of time
series. <em>TKDE</em>, <em>35</em>(10), 10339–10350. (<a
href="https://doi.org/10.1109/TKDE.2023.3268125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decomposition of a time series is an essential task that helps to understand its very nature. It facilitates the analysis and forecasting of complex time series expressing various hidden components such as the trend, seasonal components, cyclic components and irregular fluctuations. Therefore, it is crucial in many fields for forecasting and decision-making processes. In recent years, many methods of time series decomposition have been developed, which extract and reveal different time series properties. Unfortunately, they neglect a very important property, i.e., time series variance. To deal with heteroscedasticity in time series, the method proposed in this work – a seasonal-trend-dispersion decomposition (STD) – extracts the trend, seasonal component and component related to the dispersion of the time series. We define STD decomposition in two ways: with and without an irregular component. We show how STD can be used for time series analysis and forecasting.},
  archive      = {J_TKDE},
  author       = {Grzegorz Dudek},
  doi          = {10.1109/TKDE.2023.3268125},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10339-10350},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {STD: A seasonal-trend-dispersion decomposition of time series},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contact tracing over uncertain indoor positioning data.
<em>TKDE</em>, <em>35</em>(10), 10324–10338. (<a
href="https://doi.org/10.1109/TKDE.2023.3270031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pandemics often cause dramatic losses of human lives and impact our societies in many aspects such as public health, tourism, and economy. To contain the spread of an epidemic like COVID-19, efficient and effective contact tracing is important, especially in indoor venues where the risk of infection is higher. In this work, we formulate and study a novel query called Indoor Contact Query ( ICQ ) over raw, uncertain indoor positioning data that digitalizes people&#39;s movements indoors. Given a query object $o$ , e.g., a person confirmed to be a virus carrier, an ICQ analyzes uncertain indoor positioning data to find objects that most likely had close contact with $o$ for a long period of time. To process ICQ , we propose a set of techniques. First, we design an enhanced indoor graph model to organize different types of data necessary for ICQ . Second, for indoor moving objects, we devise methods to determine uncertain regions and to derive positioning samples missing in the raw data. Third, we propose a query processing framework with a close contact determination method, a search algorithm, and the acceleration strategies. We conduct extensive experiments on synthetic and real datasets to evaluate our proposals. The results demonstrate the efficiency and effectiveness of our proposals.},
  archive      = {J_TKDE},
  author       = {Tiantian Liu and Huan Li and Hua Lu and Muhammad Aamir Cheema and Harry Kai-Ho Chan},
  doi          = {10.1109/TKDE.2023.3270031},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10324-10338},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Contact tracing over uncertain indoor positioning data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised rumor detection based on propagation tree VAE.
<em>TKDE</em>, <em>35</em>(10), 10309–10323. (<a
href="https://doi.org/10.1109/TKDE.2023.3267821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide spread of rumors inflicts damages on social media platforms. Detecting rumors has become an emerging problem concerning the public and government. A crucial problem for rumors detection on social media is the lack of reliably pre-annotated dataset to train classification models. To solve this problem, we propose an unsupervised model that detects rumors by measuring how well the tweets follow the normal patterns. However, the problem is challenging in how to automatically discover the normal patterns of tweets. To tackle the challenge, we first propose a novel tree variational autoencoder model that reconstructs the sentiment labels along the propagation tree of a factual tweet. Then we propose a cross-alignment method to align the multiple modalities, i.e., tree structure and propagation features, and output the final prediction results. We conduct extensive experiments on a real-world dataset collected from Weibo. The experiments show that the proposed method significantly outperforms the state-of-the-art unsupervised methods and adapts better to the concept drift than state-of-the-art supervised methods.},
  archive      = {J_TKDE},
  author       = {Lanting Fang and Kaiyu Feng and Kaiqi Zhao and Aiqun Hu and Tao Li},
  doi          = {10.1109/TKDE.2023.3267821},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10309-10323},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised rumor detection based on propagation tree VAE},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pushing ML predictions into DBMSs. <em>TKDE</em>,
<em>35</em>(10), 10295–10308. (<a
href="https://doi.org/10.1109/TKDE.2023.3269592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, many approaches have been suggested to execute ML workloads on a DBMS. However, most of them have looked at in-DBMS ML from a training perspective, whereas ML inference has been largely overlooked. We think that this is an important gap to fill for two main reasons: (1) in the near future, every application will be infused with some sort of ML capability; (2) behind every web page, application, and enterprise there is a DBMS, whereby in-DBMS inference is an appealing solution both for efficiency (e.g., less data movement), performance (e.g., cross-optimizations between relational operators and ML) and governance. In this article, we study whether DBMSs are a good fit for prediction serving. We introduce a technique for translating trained ML pipelines containing both featurizers (e.g., one-hot encoding) and models (e.g., linear and tree-based models) into SQL queries, and we compare in-DBMS performance against popular ML frameworks such as Sklearn and ml.net . Our experiments show that, when pushed inside a DBMS, trained ML pipelines can have performance comparable to ML frameworks in several scenarios, while they perform quite poorly on text featurization and over (even simple) neural networks.},
  archive      = {J_TKDE},
  author       = {Matteo Paganelli and Paolo Sottovia and Kwanghyun Park and Matteo Interlandi and Francesco Guerra},
  doi          = {10.1109/TKDE.2023.3269592},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10295-10308},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Pushing ML predictions into DBMSs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced multi-task learning and knowledge graph-based
recommender system. <em>TKDE</em>, <em>35</em>(10), 10281–10294. (<a
href="https://doi.org/10.1109/TKDE.2023.3251897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the m ulti-task learning for k nowledge graph-based r ecommender system, termed MKR, has shown its promising performance and has attracted increasing interest, because a recommendation task and a knowledge graph embedding (KGE) task can help each other to improve the recommendation. However, MKR still has two difficult issues. The first is how fully to capture users’ historical behavior pattern in the recommendation task and how fully to utilize deep multi-relation semantic information in the KGE task. The second is how to deal with datasets with different sparsity. Tackling these challenging issues, this paper proposes an enhanced MKR (EMKR) approach with two novelties. First, we propose to utilize the attention mechanism to aggregate users’ historical behavior for more accurately mining preferences in the recommendation task, and utilize the relation-aware graph convolutional neural network to fully capture the deep multi-relation neighborhood features in the KGE task, so as to address the first issue. Second, a two-part modeling strategy is proposed for a better representation of users in the recommendation task to expand the expressive ability of the model for adapting to datasets with different sparsity, so as to address the second issue. Extensive experiments are conducted on widely-used datasets and 11 approaches are used for comparison. The results show that the proposed EMKR can achieve substantial gains over the compared state-of-the-art approaches, especially in the situation where user-item interactions are sparse.},
  archive      = {J_TKDE},
  author       = {Min Gao and Jian-Yu Li and Chun-Hua Chen and Yun Li and Jun Zhang and Zhi-Hui Zhan},
  doi          = {10.1109/TKDE.2023.3251897},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10281-10294},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhanced multi-task learning and knowledge graph-based recommender system},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot semantic relation prediction across heterogeneous
graphs. <em>TKDE</em>, <em>35</em>(10), 10265–10280. (<a
href="https://doi.org/10.1109/TKDE.2023.3251951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic relation prediction aims to mine the implicit relationships between objects in heterogeneous graphs, which consist of different types of objects and different types of links. In real-world scenarios, new semantic relations constantly emerge and they typically appear with only a few labeled data. Since a variety of semantic relations exist in multiple heterogeneous graphs, the transferable knowledge can be mined from some existing semantic relations to help predict the new semantic relations with few labeled data. This inspires a novel problem of few-shot semantic relation prediction across heterogeneous graphs. However, the existing methods cannot solve this problem because they not only require a large number of labeled samples as input, but also focus on a single graph with a fixed heterogeneity. Targeting this novel and challenging problem, in this paper, we propose a Meta-learning based Graph neural network for Semantic relation prediction, named MetaGS. First, MetaGS decomposes the graph structure between objects into multiple normalized subgraphs, then adopts a two-view graph neural network to capture local heterogeneous information and global structure information of these subgraphs. Second, MetaGS aggregates the information of these subgraphs with a hyper-prototypical network, which can learn from existing semantic relations and adapt to new semantic relations. Third, using the well-initialized two-view graph neural network and hyper-prototypical network, MetaGS can effectively learn new semantic relations from different graphs while overcoming the limitation of few labeled data. Extensive experiments on three real-world datasets have demonstrated the superior performance of MetaGS over the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Pengfei Ding and Yan Wang and Guanfeng Liu and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2023.3251951},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10265-10280},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Few-shot semantic relation prediction across heterogeneous graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive consensus clustering for multiple k-means via base
results refining. <em>TKDE</em>, <em>35</em>(10), 10251–10264. (<a
href="https://doi.org/10.1109/TKDE.2023.3264970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus clustering, which learns a consensus clustering result from multiple weak base results, has been widely studied. However, conventional consensus clustering methods only focus on the ensemble process while ignoring the quality improvement of the base results, and thus they just use the fixed base results for consensus learning. In this paper, we provide an alternative idea to improve the final consensus clustering performance by considering the base results refining. In our framework, we adaptively refine the base results in the process of the ensemble. In more detail, on one hand, we ensemble multiple K-means results to learn the consensus one by considering the consensus and diversity; on the other hand, we apply the consensus result to design a graph filter to learn a more cluster-friendly embedding for refining the base K-means results. In our framework, the consensus learning and base results refining are integrated into one unified objective function so that these two tasks can be boosted by each other. Then we design an effective iterative algorithm to optimize the carefully designed objective function. The extensive experiments on benchmark data sets demonstrate that the proposed method can outperform both the single clustering and the state-of-the-art consensus clustering methods. The codes of this paper are released in http://Doctor-Nobody.github.io/codes/ACMK.zip .},
  archive      = {J_TKDE},
  author       = {Peng Zhou and Liang Du and Xuejun Li},
  doi          = {10.1109/TKDE.2023.3264970},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10251-10264},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive consensus clustering for multiple K-means via base results refining},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning region similarities via graph-based deep metric
learning. <em>TKDE</em>, <em>35</em>(10), 10237–10250. (<a
href="https://doi.org/10.1109/TKDE.2023.3253802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region similarity learning plays an essential role in applications such as business site selection, region recommendation, and urban planning. Earlier studies mainly represent regions as bags of points of interest (POIs) for region similarity comparisons, which cannot fully exploit the spatial features of the regions. Recently, researchers propose to use deep neural networks to exploit spatial features such as POI geo-coordinates and categories, which have produced more accurate and robust region similarity learning results. However, many useful features such as the height and size of a POI, and the distance and relative importance between the POIs, are still overlooked in these methods. To take advantage of such features, we propose to represent regions as graphs, where nodes are POIs with rich features such as height, size, and hexagonal coordinates, while edges are the relationships between POIs formulated by their road network distances. To capture POIs’ importance, we weigh them by their height and size. Since there is limited availability of ground-truth region similarity data, we propose a contrastive learning-based multi-relational graph neural network (C-MPGCN) for region similarity learning based on the graph representations. To generate data for model training, we propose a soft graph edit distance (SGED) based algorithm to generate triples of similar and dissimilar graphs of a given graph (representing a given region) based on the POI weights. Experimental results show that C-MPGCN outperforms the state-of-the-art methods for region similarity learning consistently with an improvement of at least 8.6\% and 9.4\% in terms of MRR and HR@1, respectively.},
  archive      = {J_TKDE},
  author       = {Yunxiang Zhao and Jianzhong Qi and Bayu D. Trisedya and Yixin Su and Rui Zhang and Hongguang Ren},
  doi          = {10.1109/TKDE.2023.3253802},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10237-10250},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning region similarities via graph-based deep metric learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining high utility itemsets using prefix trees and utility
vectors. <em>TKDE</em>, <em>35</em>(10), 10224–10236. (<a
href="https://doi.org/10.1109/TKDE.2023.3256126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High utility itemsets can reveal combinations of items that have a high profit, expense, or importance. Mining high utility itemsets in a database with $n$ items generally results in a huge search space, composed of $2^{n}$ itemsets, and heavy utility calculations for the explored itemsets. Previous algorithms using prefix tree structures perform two phases, namely candidate generation and testing. To avoid generating candidate itemsets, one-phase algorithms use list or hyper-link structures and have been proven to be superior to two-phase algorithms. However, it should be noted that a prefix tree is still an efficient structure for itemset mining problems, and especially algorithms using prefix trees such as FP-Growth have shown excellent performance for mining frequent itemsets. This paper proposes Hamm, a High-performance AlgorithM for Mining high utility itemsets. Hamm employs a novel TV (prefix Tree and utility Vector) structure and mines high utility itemsets in one phase without candidate generation. We also develop an efficient optimization which is incorporated into Hamm as a component. Using prefix trees and utility vectors, Hamm outperforms state-of-the-art algorithms on various databases in experiments. Experimental results also show that the proposed optimization remarkably reduces the search space and speeds up Hamm.},
  archive      = {J_TKDE},
  author       = {Jun-Feng Qu and Philippe Fournier-Viger and Mengchi Liu and Bo Hang and Chunyang Hu},
  doi          = {10.1109/TKDE.2023.3256126},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10224-10236},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Mining high utility itemsets using prefix trees and utility vectors},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-oriented visual question answering: The e-VQA dataset
and benchmark. <em>TKDE</em>, <em>35</em>(10), 10210–10223. (<a
href="https://doi.org/10.1109/TKDE.2023.3267036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering (VQA) is a challenging task that reasons over questions on images with knowledge. A prerequisite for VQA is the availability of annotated datasets, while the available datasets have several limitations. 1) The diversity of questions and answers are limited to a few question categories and certain concepts (e.g., objects, relations, actions.) with somewhat mechanical answers. 2) The availability of background knowledge or context information has been disregarded with just images, questions and answers being provided. 3) The timeliness of knowledge has not been examined, though some works may introduce factual or commonsense knowledge bases, e.g., ConceptNet, DBPedia. In this paper, we provide an Event-oriented Visual Question Answering (E-VQA) dataset including free-form questions and answers for real-world event concepts, which provides context information of events as domain knowledge in addition to images. E-VQA consists of 2,690 social media images, 9,088 questions, 5,479 answers, and 1,157 news media articles for references being annotated to 182 real-world events, covering a wide range of topics, such as armed conflicts and attacks, disasters and accidents, law and crime. For comparisons, we investigate 10 state-of-the-art VQA methods as benchmarks.},
  archive      = {J_TKDE},
  author       = {Zhenguo Yang and Jiale Xiang and Jiuxiang You and Qing Li and Wenyin Liu},
  doi          = {10.1109/TKDE.2023.3267036},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10210-10223},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Event-oriented visual question answering: The E-VQA dataset and benchmark},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimal algorithm for finding champions in tournament
graphs. <em>TKDE</em>, <em>35</em>(10), 10197–10209. (<a
href="https://doi.org/10.1109/TKDE.2023.3267345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A tournament graph is a complete directed graph, which can be used to model a round-robin tournament between $n$ players. In this paper, we address the problem of finding a champion of the tournament, also known as Copeland winner, which is a player that wins the highest number of matches. In detail, we aim to investigate algorithms that find the champion by playing a low number of matches. Solving this problem allows us to speed up several Information Retrieval and Recommender System applications, including question answering, conversational search, etc. Indeed, these applications often search for the champion inducing a round-robin tournament among the players by employing a machine learning model to estimate who wins each pairwise comparison. Our contribution, thus, allows finding the champion by performing a low number of model inferences. We prove that any deterministic or randomized algorithm finding a champion with constant success probability requires $\Omega (\ell n)$ comparisons, where $\ell$ is the number of matches lost by the champion. We then present an asymptotically-optimal deterministic algorithm matching this lower bound without knowing $\ell$ , and we extend our analysis to three variants of the problem. Lastly, we conduct a comprehensive experimental assessment of the proposed algorithms on a question answering task on public data. Results show that our proposed algorithms speed up the retrieval of the champion up to $13\times$ with respect to the state-of-the-art algorithm that perform the full tournament.},
  archive      = {J_TKDE},
  author       = {Lorenzo Beretta and Franco Maria Nardini and Roberto Trani and Rossano Venturini},
  doi          = {10.1109/TKDE.2023.3267345},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10197-10209},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An optimal algorithm for finding champions in tournament graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Expanding the edge: Enabling efficient winograd CNN
inference with deep reuse on edge device. <em>TKDE</em>,
<em>35</em>(10), 10181–10196. (<a
href="https://doi.org/10.1109/TKDE.2023.3269017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning on edge devices is becoming increasingly important, especially with the explosion of IoT devices. For example, the total number of devices connected to IoT reaches 29 billion in 2022. Convolutional neural networks (CNNs), as common deep learning representatives, are among the most popular neural networks in knowledge and data engineering. However, CNN employs a high degree of computing. In comparison to the training phase, the inference process is more frequently done on low-power computing equipments, such as edge devices. The limited computing resource and high computation pressure limit the effective use of CNN algorithms at the edge. Fortunately, a minimal filtering algorithm called Winograd can reduce convolution calculations by minimizing multiplication operations. We find that Winograd convolution can be accelerated further by deep reuse technique, which reuses the similar data and computation processes. In this paper, we propose a new inference method, called DREW, which combines deep reuse with Winograd for further accelerating CNNs. DREW handles three difficulties. First, it can detect the similarities from the complex minimal filtering patterns by clustering. Second, it reduces the online clustering cost in a reasonable range. Third, it provides an adjustable method in clustering granularity balancing the performance and accuracy. We perform evaluation on Raspberry PI and NVIDIA Jetson AGX Xavier edge devices, and experiments show that on five popular networks, 1) DREW further accelerates the Winograd convolution by an average of 8.27× speedup. Even for the highly parallel Winograd implementation, DREW still can provide 2.21× speedup. 2) When DREW is applied to end-to-end Winograd CNN inferences, DREW achieves 5.94× the average performance speedup with no ( $&amp;lt; $ 0.4\%) accuracy loss. 3) Energy consumption is an important factor for inference in practice. DREW reduces the number of convolution operations to 10\% of the original operations, thus achieving up to 60\% energy-efficiency benefits than the original Winograd inference.},
  archive      = {J_TKDE},
  author       = {Feng Zhang and Ruofan Wu and Jiawei Guan and Zhen Zheng and Xiaoguang Guo and Xiao Zhang and Xiaoyong Du and Xipeng Shen},
  doi          = {10.1109/TKDE.2023.3269017},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10181-10196},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Expanding the edge: Enabling efficient winograd CNN inference with deep reuse on edge device},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Semi-supervised variational user identity linkage via
noise-aware self-learning. <em>TKDE</em>, <em>35</em>(10), 10166–10180.
(<a href="https://doi.org/10.1109/TKDE.2023.3250245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User identity linkage, which aims to link identities of a natural person across different social platforms, has attracted increasing research interest recently. Existing approaches usually first embed the identities as deterministic vectors in a shared latent space, and then learn a classifier based on the available annotations. However, the formation and characteristics of real-world social platforms are full of uncertainties, which makes these deterministic embedding based methods sub-optimal. Besides, semi-supervised models utilize the unlabeled data to help capture the intrinsic data distribution. However, the existing semi-supervised linkage methods heavily rely on the heuristically defined similarity measurements to incorporate the innate closeness between labeled and unlabeled samples. Such manually designed assumptions may not be consistent with the actual linkage signals and further introduce the noises. To address the mentioned limitations, in this paper we propose a novel Noise-aware Semi-supervised Variational User Identity Linkage (NSVUIL) model. Specifically, we first propose a novel supervised linkage module to incorporate the available annotations. Each social identity is represented by a Gaussian distribution in the Wasserstein space to simultaneously preserve the fine-grained social profiles and model the uncertainty of identities. Then, a noise-aware self-learning module is designed to faithfully augment the few available annotations, which is capable of filtering noises from the pseudo-labels generated by the supervised module. The filtered reliable candidates are added into the labeled set to provide enhanced training guidance for the next training iteration. Empirically, we evaluate the NSVUIL model over multiple real-world datasets, and the experimental results demonstrate its superiority.},
  archive      = {J_TKDE},
  author       = {Chaozhuo Li and Senzhang Wang and Jie Xu and Zheng Liu and Hao Wang and Xing Xie and Lei Chen and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3250245},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10166-10180},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised variational user identity linkage via noise-aware self-learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal aggregation and propagation graph neural networks
for dynamic representation. <em>TKDE</em>, <em>35</em>(10), 10151–10165.
(<a href="https://doi.org/10.1109/TKDE.2023.3265271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal graphs exhibit dynamic interactions between nodes over continuous time, whose topologies evolve with time elapsing. The whole temporal neighborhood of nodes reveals the varying preferences of nodes. However, previous works usually generate dynamic representation with limited neighbors for simplicity, which results in both inferior performance and high latency of online inference. Therefore, in this paper, we propose a novel method of temporal graph convolution with the whole neighborhood, namely Temporal Aggregation and Propagation Graph Neural Networks (TAP-GNN). Specifically, we first analyze the computational complexity of the dynamic representation problem by unfolding the temporal graph in a message-passing paradigm. The expensive complexity motivates us to design the AP (aggregation and propagation) block, which significantly reduces the repeated computation of historical neighbors. The final TAP-GNN supports online inference in the graph stream scenario, which incorporates the temporal information into node embeddings with a temporal activation function and a projection layer besides several AP blocks. Experimental results on various real-life temporal networks show that our proposed TAP-GNN outperforms existing temporal graph methods by a large margin in terms of both predictive performance and online inference latency.},
  archive      = {J_TKDE},
  author       = {Tongya Zheng and Xinchao Wang and Zunlei Feng and Jie Song and Yunzhi Hao and Mingli Song and Xingen Wang and Xinyu Wang and Chun Chen},
  doi          = {10.1109/TKDE.2023.3265271},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10151-10165},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Temporal aggregation and propagation graph neural networks for dynamic representation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards more general loss and setting in unsupervised domain
adaptation. <em>TKDE</em>, <em>35</em>(10), 10140–10150. (<a
href="https://doi.org/10.1109/TKDE.2023.3266785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present an analysis of unsupervised domain adaptation with a series of theoretical and algorithmic results. We derive a novel Rényi- $\alpha$ divergence-based generalization bound, which is tailored to domain adaptation algorithms with arbitrary loss functions in a stochastic setting. Moreover, our theoretical results provide new insights into the assumptions for successful domain adaptation: the closeness between the conditional distributions of the domains and the Lipschitzness on the source domain. With these assumptions, we reveal the following: if their conditional generation distributions are close, the Lipschitzness property of the target domain can be transferred from the Lipschitzness on the source domain, without knowing the exact target distribution. Motivated by our analysis and assumptions, we further derive practical principles for deep domain adaptation: 1) Rényi-2 adversarial training for marginal distributions matching and 2) Lipschitz regularization for the classifier. Our experimental results on both synthetic and real-world datasets support our theoretical findings and the practical efficiency of the proposed principles.},
  archive      = {J_TKDE},
  author       = {Changjian Shui and Ruizhi Pu and Gezheng Xu and Jun Wen and Fan Zhou and Christian Gagné and Charles X. Ling and Boyu Wang},
  doi          = {10.1109/TKDE.2023.3266785},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10140-10150},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards more general loss and setting in unsupervised domain adaptation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CGF: A category guidance based PM<span
class="math inline"><sub>2.5</sub></span> sequence forecasting training
framework. <em>TKDE</em>, <em>35</em>(10), 10125–10139. (<a
href="https://doi.org/10.1109/TKDE.2023.3253703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PM $_{2.5}$ concentration forecasting is important yet challenging. First, complicated local fluctuations in PM $_{2.5}$ concentrations disturb modeling global trends. Second, forecasting errors are often accumulated through an autoregressive process. To contend with the two challenges, we propose a C ategory G uidance based PM ${_{2.5}}$ sequence F orecasting training framework (CGF) to enhance the performance of existing PM ${_{2.5}}$ concentration forecasting models. CGF contains a Category based Representation Learning (CRL) module and a Category based Self-paced Learning (CSL) module, both of which utilize PM ${_{2.5}}$ category information that is easily obtained and publicly available. First, CRL employs category information to guide forecasting models to produce more robust hidden representations that are insensitive to local fluctuations, thus alleviating the negative impact of local fluctuations. Second, CSL adaptively selects real PM ${_{2.5}}$ concentration values versus autoregressive PM ${_{2.5}}$ forecast values when training forecasting models, helping alleviate error accumulations. The CGF framework is applied to existing PM ${_{2.5}}$ forecasting models, and the experimental results on two real-world datasets demonstrate that CGF is able to consistently improve the accuracy of existing forecasting models. Furthermore, to validate the generality of CGF, we conduct extensional experiments in two other time-series prediction tasks, including exchange rate forecasting and electricity forecasting. The experimental results also verify the effectiveness of CGF.},
  archive      = {J_TKDE},
  author       = {Haomin Yu and Jilin Hu and Xinyuan Zhou and Chenjuan Guo and Bin Yang and Qingyong Li},
  doi          = {10.1109/TKDE.2023.3253703},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10125-10139},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CGF: A category guidance based PM$_{2.5}$ sequence forecasting training framework},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature-level deeper self-attention network with contrastive
learning for sequential recommendation. <em>TKDE</em>, <em>35</em>(10),
10112–10124. (<a
href="https://doi.org/10.1109/TKDE.2023.3250463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation, which aims to recommend next item that the user will likely interact in a near future, has become essential in various Internet applications. Existing methods usually consider the transition patterns between items, but ignore the transition patterns between features of items. We argue that only the item-level sequences cannot reveal the full sequential patterns, while explicit and implicit feature-level sequences can help extract the full sequential patterns. Meanwhile, the item-level sequential recommendation also suffers from limited supervised signal issues. In this article, we propose a novel model Feature-level Deeper Self-Attention Network with Contrastive Learning (FDSA-CL) for sequential recommendation. Specifically, FDSA-CL first integrates various heterogeneous features of items into feature-level sequences with different weights through a vanilla attention mechanism. After that, FDSA-CL applies separated self-attention blocks on item-level sequences and feature-level sequences, respectively, to model item transition patterns and feature transition patterns. Moreover, we propose contrastive learning and item feature recommendation tasks to capture the embedding commonality and further utilize the beneficial interaction among the two levels, so as to alleviate the sparsity of the supervised signal and extract the most critical information. Finally, we jointly optimize the above tasks. We evaluate the proposed model using two real-world datasets and experimental results show that our model significantly outperforms the state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Yongjing Hao and Tingting Zhang and Pengpeng Zhao and Yanchi Liu and Victor S. Sheng and Jiajie Xu and Guanfeng Liu and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2023.3250463},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10112-10124},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Feature-level deeper self-attention network with contrastive learning for sequential recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge graph augmented network towards multiview
representation learning for aspect-based sentiment analysis.
<em>TKDE</em>, <em>35</em>(10), 10098–10111. (<a
href="https://doi.org/10.1109/TKDE.2023.3250499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment analysis. To better comprehend long complicated sentences and obtain accurate aspect-specific information, linguistic and commonsense knowledge are generally required in this task. However, most current methods employ complicated and inefficient approaches to incorporate external knowledge, e.g., directly searching the graph nodes. Additionally, the complementarity between external knowledge and linguistic information has not been thoroughly studied. To this end, we propose a knowledge graph augmented network ( KGAN ), which aims to effectively incorporate external knowledge with explicitly syntactic and contextual information. In particular, KGAN captures the sentiment feature representations from multiple different perspectives, i.e. , context-, syntax- and knowledge-based. First, KGAN learns the contextual and syntactic representations in parallel to fully extract the semantic features. Then, KGAN integrates the knowledge graphs into the embedding space, based on which the aspect-specific knowledge representations are further obtained via an attention mechanism. Last, we propose a hierarchical fusion module to complement these multi-view representations in a local-to-global manner. Extensive experiments on five popular ABSA benchmarks demonstrate the effectiveness and robustness of our KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN achieves a new record of state-of-the-art performance among all datasets.},
  archive      = {J_TKDE},
  author       = {Qihuang Zhong and Liang Ding and Juhua Liu and Bo Du and Hua Jin and Dacheng Tao},
  doi          = {10.1109/TKDE.2023.3250499},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10098-10111},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Knowledge graph augmented network towards multiview representation learning for aspect-based sentiment analysis},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing spaced repetition schedule by capturing the
dynamics of memory. <em>TKDE</em>, <em>35</em>(10), 10085–10097. (<a
href="https://doi.org/10.1109/TKDE.2023.3251721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spaced repetition, namely, learners review items in a given schedule, has been proven powerful for memorization and practice of skills. Most current spaced repetition methods focus on either predicting student recall or designing an optimal review schedule, thus omitting the integrity of the spaced repetition system. In this work, we propose a novel spaced repetition schedule framework by capturing the dynamics of memory, which alternates memory prediction and schedule optimization to improve the efficiency of learners’ reviews. First, the framework collects logs from students’ reviews and builds memory models with Markov property to capture the dynamics of memory. Then, the spaced repetition optimization is transformed a stochastic shortest path problem and solved via the value iteration method. We also construct a new benchmark dataset for spaced repetition, which is the first to contain time-series information during learners’ memorization. Experimental results on the collected data from the real world and the simulated environment demonstrate that the proposed approach reduces 64\% error and 17\% cost in predicting recall rates and optimizing schedules compared to several baselines. We have publicly released the dataset containing 220 million rows and codes used in this paper at: https://github.com/maimemo/SSP-MMC-Plus .},
  archive      = {J_TKDE},
  author       = {Jingyong Su and Junyao Ye and Liqiang Nie and Yilong Cao and Yongyong Chen},
  doi          = {10.1109/TKDE.2023.3251721},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10085-10097},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Optimizing spaced repetition schedule by capturing the dynamics of memory},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MM-FRec: Multi-modal enhanced fashion item recommendation.
<em>TKDE</em>, <em>35</em>(10), 10072–10084. (<a
href="https://doi.org/10.1109/TKDE.2023.3266423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies on fashion item recommendation mainly focused on incorporating the visual signals of items to boost the user preference learning, while overlooking the semantic attributes (e.g., material and brand) of fashion items that also contain important cues about items’ properties and users’ preference. To bridge this gap, we aim to comprehensively explore the attribute and vision modalities of items to improve the fashion item recommendation performance. However, this is non-trivial due to the latent visual-semantic consistency, various relation types, and unique attributes with insufficient samples. To address these challenges, we propose a Multi-Modal enhanced Fashion item Recommendation scheme (MM-FRec). Specifically, to cope with the multi-modal data, we introduce a relation-oriented graph as well as a vision-oriented graph, and design MM-FRec with three key components: attribute-enhanced latent representation learning, visual representation learning, and multi-modal enhanced preference modeling. To deal with the various relation types, we present a new relation-aware propagation method for adaptively aggregating the information from neighbor nodes to promote the user and item representation learning. To cope with the unique attributes, we introduce the deep multi-task learning strategy in the relation-aware confidence assignment. Extensive experiments on a real-world dataset demonstrate the superiority of our model over state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Xuemeng Song and Chun Wang and Changchang Sun and Shanshan Feng and Min Zhou and Liqiang Nie},
  doi          = {10.1109/TKDE.2023.3266423},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10072-10084},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MM-FRec: Multi-modal enhanced fashion item recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hiding from centrality measures: A stackelberg game
perspective. <em>TKDE</em>, <em>35</em>(10), 10058–10071. (<a
href="https://doi.org/10.1109/TKDE.2023.3267854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centrality measures can rank nodes in a social network according to their importance. However, in many cases, a node may want to avoid being highly ranked by such measures, e.g., as is the case with terrorist networks. In this work, we study a confrontation between the seeker—the party analyzing a social network using centrality measures—and the evader—a node attempting to decrease its ranking according to such measures. We analyze the possible outcomes of modifying, i.e., adding or removing, a single edge by the evader, showing that even without complete knowledge about the network, the effects of the modification on the evader&#39;s ranking can often be predicted. We study the computational complexity of finding a set of modifications that reduce the evader&#39;s centrality ranking in an optimal way, proving that these decision problems are NP-complete. Moreover, we provide a 2-approximation for the degree centrality, and logarithmic approximation boundaries for the closeness and betweenness centralities. Finally, we define and investigate a Stackelberg game between the seeker and the evader, providing a Mixed Integer Linear Programming formulation of finding an equilibrium. Altogether, we provide a thorough analysis of the strategic aspects of hiding from centrality measures in social networks.},
  archive      = {J_TKDE},
  author       = {Marcin Waniek and Jan Woźnica and Kai Zhou and Yevgeniy Vorobeychik and Tomasz P. Michalak and Talal Rahwan},
  doi          = {10.1109/TKDE.2023.3267854},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10058-10071},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hiding from centrality measures: A stackelberg game perspective},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GODDAG: Generating origin-destination flow for new cities
via domain adversarial training. <em>TKDE</em>, <em>35</em>(10),
10048–10057. (<a
href="https://doi.org/10.1109/TKDE.2023.3268409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Origin-destination (OD) flow data, which reflects population mobility patterns in the city, is very important in many urban applications, such as urban planning and public resource allocation, etc. However, due to the high cost of money and time during device deployment and social surveys, it is challenging to obtain OD flow data, especially in developing cities and emerging cities where historical OD flow data is scarce. Therefore, it is necessary to investigate a method that can generate OD flow in cities where OD flow data are not available. The research on modeling population mobility in the city has a long history. Traditional gravity models, etc., are too simple to model the complex population mobility; recently proposed machine learning models and deep learning models are not applicable in cities where data are scarce because the parameters must be fitted with abundant data. To solve the problem of difficult access to OD flow data, we propose a method to learn mobility knowledge with ample data in the source city and generate OD flow data in new cities named GODDAG ( G enerating O rigin- D estination Flow via D omain A dversarial Trainin g ). Our proposed method consists of two parts, one is a GNN (graph neural networks) based mobility model generating OD flow between every two regions based on regional attributes such as census and POI distribution, and the other is a domain adversarial training strategy to make the model have better transfer ability between different cities. Extensive experiments are conducted on two real-world datasets to prove the validity of our methods.},
  archive      = {J_TKDE},
  author       = {Can Rong and Jie Feng and Jingtao Ding},
  doi          = {10.1109/TKDE.2023.3268409},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10048-10057},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GODDAG: Generating origin-destination flow for new cities via domain adversarial training},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LiteWSEC: A lightweight framework for web-scale spectral
ensemble clustering. <em>TKDE</em>, <em>35</em>(10), 10035–10047. (<a
href="https://doi.org/10.1109/TKDE.2023.3267167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral Clustering (SC) is an effective clustering method for its excellent performance in partitioning non-linearly distributed data. On the other hand, Ensemble Clustering (EC), a different clustering technology, can promote cluster quality by ensembling the results of base clusterings. In this work, we concentrate on an EC framework that utilizes SC as the base method. Nevertheless, SC suffers from scalability due to its high computational complexity in constructing the Laplacian graph and computing the corresponding eigendecomposition. In the past decades, many efforts have been made to it. However, SC suffers from the scalability issue in processing extensive data, especially in web-scale scenarios. Additionally, EC requires multiple clustering results as the ensemble bases, which further aggravates resource consumption. To address this issue, LiteWSEC, a simple yet efficient Lightweight Framework for Web-scale Spectral Ensemble Clustering, is proposed to cluster web-scale data with limited resource requirements. It adopts the Web-scale Spectral Clustering (WSC) as the base method, which has minimal space overhead without computing overall embedding explicitly. LiteWSEC is highly flexible in the memory requirement, which is adaptive to the available resource. It can partition web-scale data (e.g., $n $ = 8,000 k) in an resource-limited host (e.g., memory is restricted to 1 GB). Experiments on real-world, large-scale, and web-scale datasets demonstrate both the efficiency and effectiveness of LiteWSEC over state-of-the-art SC and EC methods.},
  archive      = {J_TKDE},
  author       = {Geping Yang and Sucheng Deng and Can Chen and Yiyang Yang and Zhiguo Gong and Xiang Chen and Zhifeng Hao},
  doi          = {10.1109/TKDE.2023.3267167},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10035-10047},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LiteWSEC: A lightweight framework for web-scale spectral ensemble clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimum recall-based loss function for imbalanced time
series classification. <em>TKDE</em>, <em>35</em>(10), 10024–10034. (<a
href="https://doi.org/10.1109/TKDE.2023.3268994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with imbalanced time series classification problems. In particular, we propose to learn time series classifiers that maximize the minimum recall of the classes rather than the accuracy. Consequently, we manage to obtain classifiers which tend to give the same importance to all the classes. Unfortunately, for most of the traditional classifiers, learning to maximize the minimum recall of the classes is not trivial (if possible), since it can distort the nature of the classifiers themselves. Neural networks, in contrast, are classifiers that explicitly define a loss function, allowing it to be modified. Given that the minimum recall is not a differentiable function, and therefore does not allow the use of common gradient-based learning methods, we apply and evaluate several smooth approximations of the minimum recall function. A thorough experimental evaluation shows that our approach improves the performance of state-of-the-art methods used in imbalanced time series classification, obtaining higher recall values for the minority classes, incurring only a slight loss in accuracy.},
  archive      = {J_TKDE},
  author       = {Josu Ircio and Aizea Lojo and Usue Mori and Simon Malinowski and Jose A. Lozano},
  doi          = {10.1109/TKDE.2023.3268994},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10024-10034},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Minimum recall-based loss function for imbalanced time series classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empowering a* algorithm with neuralized variational
heuristics for fastest route recommendation. <em>TKDE</em>,
<em>35</em>(10), 10011–10023. (<a
href="https://doi.org/10.1109/TKDE.2023.3269084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fastest route recommendation (FRR) is crucial for intelligent transportation systems. The existing methods treat it as a pathfinding problem on dynamic graphs, and extend A* algorithm with neuralized travel time estimators as cost functions. However, they fail to provide effective heuristic cost due to the neglect of its admissibility and the utilization of noise path information, resulting in sub-optimal results and inefficiency. Besides, path sequentiality is also ignored, affecting algorithm accuracy as well. In this paper, we propose a variational inference based fastest route recommendation method, which follows the framework of A* algorithm and provides effective costs for routing. Specifically, we first adopt a sequential estimator to accurately estimate the travel time of a specific path. More importantly, we design a variational inference based estimator, which models the distribution of travel time between two nodes and provides an effective heuristic cost with high probability of being admissible. We further take advantage of adversarial learning to enrich the fastest path information. To the best of our knowledge, we are the first to use variational estimator to consider the admissibility of heuristics in FRR. Extensive experiments are conducted on two real-world datasets. The results verify the performance advantage of our proposed method.},
  archive      = {J_TKDE},
  author       = {Minrui Xu and Jiajie Xu and Rui Zhou and Jianxin Li and Kai Zheng and Pengpeng Zhao and Chengfei Liu},
  doi          = {10.1109/TKDE.2023.3269084},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {10011-10023},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Empowering a* algorithm with neuralized variational heuristics for fastest route recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Competition and cooperation: Global task assignment in
spatial crowdsourcing. <em>TKDE</em>, <em>35</em>(10), 9998–10010. (<a
href="https://doi.org/10.1109/TKDE.2023.3251443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online spatial crowdsourcing platforms provide popular O2O services in people&#39;s daily. Users submit real-time tasks through the Internet and require the platform to immediately assign workers to serve them. However, the imbalance distribution of tasks and workers leads to the rejection of some tasks, which reduces the profit of the platform. In this paper, we propose that similar platforms can form an alliance to make full use of the global service supply through cooperation. We name the problem as G lobal T ask A ssignment (GTA), in which platforms are allowed to hire idle workers from other platforms to improve the profit of all the platforms together. Different from relevant works, the decision-makers in GTA are platforms rather than individual workers, which can better assign workers in all platforms and improve the overall profit. We design an auction-based incentive mechanism (AIM), to motivate platforms to rent idle workers to other platforms so that increase their own profit. Based on the mechanism, we propose a greedy-based assignment algorithm (BaseGTA), in which platforms greedily maximizes their current profit. We further propose a prediction-based assignment algorithm (ImpGTA), in which platforms make decisions based on the spatial-temporal distribution in the future time. Experimental results show that platforms using our algorithms can achieve higher profit than the existing studies.},
  archive      = {J_TKDE},
  author       = {Boyang Li and Yurong Cheng and Ye Yuan and Changsheng Li and Qianqian Jin and Guoren Wang},
  doi          = {10.1109/TKDE.2023.3251443},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9998-10010},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Competition and cooperation: Global task assignment in spatial crowdsourcing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PLAME: Piecewise-linear approximate measure for additive
kernel SVM. <em>TKDE</em>, <em>35</em>(10), 9985–9997. (<a
href="https://doi.org/10.1109/TKDE.2023.3253263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive Kernel SVM has been extensively used in many applications, including human activity detection and pedestrian detection. Since training an additive kernel SVM model is very time-consuming, which is not scalable to large-scale datasets, many efficient solutions have been developed in the past few years. However, most of the existing methods normally fail to achieve one of these three important conditions which are (1) low classification error, (2) low memory space, and (3) low training time. In order to simultaneously fulfill these three conditions, we develop the new piecewise-linear approximate measure (PLAME) for additive kernels. By incorporating PLAME with the well-known dual coordinate descent method, we theoretically show that this approach can achieve the above three conditions. Experimental results on twelve real datasets show that our approach can achieve the best trade-off between the accuracy, memory space, and training time compared with different types of state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Tsz Nam Chan and Zhe Li and Leong Hou U and Reynold Cheng},
  doi          = {10.1109/TKDE.2023.3253263},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9985-9997},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PLAME: Piecewise-linear approximate measure for additive kernel SVM},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatio-temporal dynamic graph relation learning for urban
metro flow prediction. <em>TKDE</em>, <em>35</em>(10), 9973–9984. (<a
href="https://doi.org/10.1109/TKDE.2023.3269771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban metro flow prediction is of great value for metro operation scheduling, passenger flow management and personal travel planning. However, the problem is challenging. First, different metro stations, e.g. transfer stations and non-transfer stations have unique traffic patterns. Second, it is difficult to model complex spatio-temporal dynamic relation of metro stations. To address these challenges, we develop a spatio-temporal dynamic graph relational learning model (STDGRL) to predict urban metro station flow. First, we propose a spatio-temporal node embedding representation module to capture the traffic patterns of different stations. Second, we employ a dynamic graph relationship learning module to learn dynamic spatial relationships between metro stations without a predefined graph adjacency matrix. Finally, we provide a transformer-based long-term relationship prediction module for long-term metro flow prediction. Extensive experiments are conducted based on metro data in four cities, China, with experimental results demonstrating the advantages of our method compared over 14 baselines for urban metro flow prediction.},
  archive      = {J_TKDE},
  author       = {Peng Xie and Minbo Ma and Tianrui Li and Shenggong Ji and Shengdong Du and Zeng Yu and Junbo Zhang},
  doi          = {10.1109/TKDE.2023.3269771},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9973-9984},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatio-temporal dynamic graph relation learning for urban metro flow prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated graph neural network search under federated
learning framework. <em>TKDE</em>, <em>35</em>(10), 9959–9972. (<a
href="https://doi.org/10.1109/TKDE.2023.3250264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {G raph N eural N etwork (GNN) has achieved great success in the field of graph data processing and analysis, but the design of GNN architecture is difficult and time-consuming. To reduce the development cost of GNNs, recently, some GNN N eural A rchitecture S earch (GNN NAS) techniques are presented for the automatic design of GNN architectures. These techniques bring great convenience to the use of GNN, but cannot be applied to the federated learning scenarios. They only consider the single-source graph dataset, while failing to deal with the distributed and private graph datasets, which limits their applications. To address this shortcoming, in this paper we propose FL-AGNNS, an efficient GNN NAS algorithm which enables distributed agents to cooperatively design powerful GNN models while keeping personal information on local devices. FL-AGNNS designs a novel federated evolutionary optimization strategy. This strategy can fully consider the GNN architectures favored by each client, thus recommend GNN architectures that perform well in multiple datasets. In additions, FL-AGNNS applies the GNN super-network, a weight sharing strategy, to speed up the evaluation of GNN models during the search phase. Extensive experimental results show that FL-AGNNS can recommend better GNN models in short time under the federated learning framework, surpassing the state-of-the-arts GNN models.},
  archive      = {J_TKDE},
  author       = {Chunnan Wang and Bozhou Chen and Geng Li and Hongzhi Wang},
  doi          = {10.1109/TKDE.2023.3250264},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9959-9972},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Automated graph neural network search under federated learning framework},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event extraction with dynamic prefix tuning and relevance
retrieval. <em>TKDE</em>, <em>35</em>(10), 9946–9958. (<a
href="https://doi.org/10.1109/TKDE.2023.3266495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider event extraction in a generative manner with template-based conditional generation. Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have several significant challenges, including using suboptimal prompts, static event type information, and the overwhelming number of irrelevant event types. In this article, we propose a generative template-based method with dynamic prefixes and a relevance retrieval framework for event extraction ( GREE ) by first integrating context information with type-specific prefixes to learn a context-specific prefix for each context, and then retrieving the relevant event types with an adaptive threshold. Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE. Additionally, our model is proven to be portable to new types of events effectively.},
  archive      = {J_TKDE},
  author       = {Heyan Huang and Xiao Liu and Ge Shi and Qian Liu},
  doi          = {10.1109/TKDE.2023.3266495},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9946-9958},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Event extraction with dynamic prefix tuning and relevance retrieval},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Value-wise ConvNet for transformer models: An infinite
time-aware recommender system. <em>TKDE</em>, <em>35</em>(10),
9932–9945. (<a href="https://doi.org/10.1109/TKDE.2022.3219231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the most suitable individual to answer a question using brief content has important usages, including the community of question answering systems and online recommender frameworks. However, one must tackle challenges: Disregarding the indispensable noise in short text contents, authors usually answer the input query with mismatched words that can negatively influence the textual relevance. Moreover, many vocabularies imply various alterations. Finally, not every expert is eager to answer an input query given the time constraint, named the reluctance dilemma. To overcome the challenges, we devise a novel embedding approach that constructs context-aware vectors. We then extract the knowledge domains out of the online contextual content. While we track user textual-temporal behavioral patterns via an infinite continuous-time module, we recommend a set of experts pertinent to the given query and willingly provide the response during the expected time. Experimental results on two real-world datasets of StackOverflow and Yahoo show that our online time-sensitive value-wise transformer can achieve higher effectiveness and efficiency versus other trending rivals in online expert recommendation systems. In addition, we empirically experience that Fourier transformers can automatically infer multi-aspect base signals and overpass manual discrete-time models in obtaining time-specific user profiles.},
  archive      = {J_TKDE},
  author       = {Mohsen Saaki and Saeid Hosseini and Sana Rahmani and Mohammad Reza Kangavari and Wen Hua and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2022.3219231},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9932-9945},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Value-wise ConvNet for transformer models: An infinite time-aware recommender system},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Popularity bias is not always evil: Disentangling benign
and harmful bias for recommendation. <em>TKDE</em>, <em>35</em>(10),
9920–9931. (<a href="https://doi.org/10.1109/TKDE.2022.3218994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender system usually suffers from severe popularity bias — the collected interaction data usually exhibits quite imbalanced or even long-tailed distribution over items. Such skewed distribution may result from the users’ conformity to the group, which deviates from reflecting users’ true preference. Existing efforts for tackling this issue mainly focus on completely eliminating popularity bias. However, we argue that not all popularity bias is evil. Popularity bias not only results from conformity but also item quality , which is usually ignored by existing methods. Some items exhibit higher popularity as they have intrinsic better property. Blindly removing the popularity bias would lose such important signal, and further deteriorate model performance. To sufficiently exploit such important information for recommendation, it is essential to disentangle the benign popularity bias caused by item quality from the harmful popularity bias caused by conformity. Although important, it is quite challenging as we lack an explicit signal to differentiate the two factors of popularity bias. In this paper, we propose to leverage temporal information as the two factors exhibit quite different patterns along the time: item quality revealing item inherent property is stable and static while conformity that depends on items’ recent clicks is highly time-sensitive. Correspondingly, we further propose a novel Ti me-aware D is E ntangled framework ( TIDE ), where a click is generated from three components namely the static item quality, the dynamic conformity effect, as well as the user-item matching score returned by any recommendation model. Lastly, we conduct interventional inference so that the recommendation can benefit from the benign popularity bias while circumvent the harmful one. Extensive experiments on four real-world datasets demonstrated the effectiveness of TIDE.},
  archive      = {J_TKDE},
  author       = {Zihao Zhao and Jiawei Chen and Sheng Zhou and Xiangnan He and Xuezhi Cao and Fuzheng Zhang and Wei Wu},
  doi          = {10.1109/TKDE.2022.3218994},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9920-9931},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Popularity bias is not always evil: Disentangling benign and harmful bias for recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TAG: Joint triple-hierarchical attention and GCN for
review-based social recommender system. <em>TKDE</em>, <em>35</em>(10),
9904–9919. (<a href="https://doi.org/10.1109/TKDE.2022.3194952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems across many Internet services have become a critical part of online businesses, as consumers would refer to them before making decisions. However, the lack of explicit ratings for items on many services makes it challenging to capture user preferences and item characteristics. Both academia and the industry have drawn attention to rating predications as a fundamental problem in recommendation systems. With the emergence of social networks, social recommender systems have been proposed to utilize the relationship between users and items to alleviate the data sparsity problem for rating predictions. However, they either concentrate on the opinion mining for each user and item, or consider the connections between users only. In this paper, we present an effective framework, Triple-hierarchical Attention Graph-based social rating prediction (TAG), to exploit the social relationships between users, the user-item interest relationships, the correlation relationships between items, and reviews for rating predictions. In order to consider opinions from reviews and these complex relationships, we first employ two triple-hierarchical attention to extract user and item features from reviews. We then design an inductive GNN, which generates effective embedding for users and items. Experiments over Yelp show that TAG outperforms state-of-the-art methods across RMSE, MAE, and NDCG metrics.},
  archive      = {J_TKDE},
  author       = {Pengpeng Qiao and Zhiwei Zhang and Zhetao Li and Yuanxing Zhang and Kaigui Bian and Yanzhou Li and Guoren Wang},
  doi          = {10.1109/TKDE.2022.3194952},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9904-9919},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TAG: Joint triple-hierarchical attention and GCN for review-based social recommender system},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge graph-enhanced sampling for conversational
recommendation system. <em>TKDE</em>, <em>35</em>(10), 9890–9903. (<a
href="https://doi.org/10.1109/TKDE.2022.3185154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional recommendation systems mainly use offline user data to train offline models, and then recommend items for online users, thus suffering from the unreliable estimation of user preferences based on sparse and noisy historical data. Conversational Recommendation System(CRS) uses the interactive form of the dialogue systems to solve the intrinsic problems of traditional recommendation systems. However, due to the lack of contextual information modeling, the existing CRS models are unable to deal with the exploitation and exploration(E&amp;E) problem well, resulting in the heavy burden on users. To address the aforementioned issue, this work proposes a contextual information enhancement model tailored for CRS, called Knowledge Graph-enhanced Sampling(KGenSam). KGenSam integrates the dynamic graph of user interaction data with the external knowledge into one heterogeneous Knowledge Graph(KG) as the contextual information environment. Then, two samplers are designed to enhance knowledge by sampling fuzzy samples with high uncertainty for obtaining user preferences and reliable negative samples for updating recommender to achieve efficient acquisition of user preferences and model updating, and thus provide a powerful solution for CRS to deal with E&amp;E problem. Experimental results on two real-world datasets demonstrate the superiority of KGenSam with significant improvements over state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Mengyuan Zhao and Xiaowen Huang and Lixi Zhu and Jitao Sang and Jian Yu},
  doi          = {10.1109/TKDE.2022.3185154},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9890-9903},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Knowledge graph-enhanced sampling for conversational recommendation system},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generative adversarial reward learning for generalized
behavior tendency inference. <em>TKDE</em>, <em>35</em>(10), 9878–9889.
(<a href="https://doi.org/10.1109/TKDE.2022.3186920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in reinforcement learning have inspired increasing interest in learning user modeling adaptively through dynamic interactions, e.g., in reinforcement learning based recommender systems. In most reinforcement learning applications, reward functions provide the critical guideline for optimization. However, current reinforcement learning-based methods rely on manually-defined reward functions, which cannot adapt to dynamic, noisy environments. Moreover, they generally use task-specific reward functions that sacrifice generalization ability. We propose a generative inverse reinforcement learning for user behavioral preference modeling to address the above issues. Instead of using predefined reward functions, our model can automatically learn the rewards from user&#39;s actions based on discriminative actor-critic network and Wasserstein GAN. Our model provides a general approach to characterizing and explaining underlying behavioral tendencies. Our experiments show our method outperforms state-of-the-art methods in several scenarios, namely traffic signal control, online recommender systems, and scanpath prediction.},
  archive      = {J_TKDE},
  author       = {Xiaocong Chen and Lina Yao and Xianzhi Wang and Aixin Sun and Quan Z. Sheng},
  doi          = {10.1109/TKDE.2022.3186920},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9878-9889},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Generative adversarial reward learning for generalized behavior tendency inference},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intent disentanglement and feature self-supervision for
novel recommendation. <em>TKDE</em>, <em>35</em>(10), 9864–9877. (<a
href="https://doi.org/10.1109/TKDE.2022.3175536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One key property in recommender systems is the long-tail distribution in user-item interactions where most items only have few user feedback. Improving the recommendation of tail items can promote novelty and bring positive effects to both users and providers, and thus is a desirable property of recommender systems. Current novel recommendation methods over-emphasize the importance of tail items without differentiating the degree of users’ intent on popularity and often incur a sharp decline of accuracy. Moreover, none of existing studies has ever taken the extreme case of tail items, i.e., cold-start items without any interaction, into consideration. In this work, we first disclose the mechanism that drives a user&#39;s interaction towards popular or niche items by disentangling her intent into conformity influence (popularity) and personal interests (preference). We then present a unified end-to-end framework to simultaneously optimize accuracy and novelty targets based on the disentangled intent of popularity and that of preference. We further develop a new paradigm for novel recommendation of cold-start items which exploits the self-supervised learning technique to model the correlation between collaborative features and content features. We conduct extensive experiments on three real-world datasets. The results demonstrate that our proposed model yields significant improvements over the state-of-the-art baselines in terms of the trade-off between accuracy and novelty.},
  archive      = {J_TKDE},
  author       = {Tieyun Qian and Yile Liang and Qing Li and Xuan Ma and Ke Sun and Zhiyong Peng},
  doi          = {10.1109/TKDE.2022.3175536},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9864-9877},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Intent disentanglement and feature self-supervision for novel recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MetaKG: Meta-learning on knowledge graph for cold-start
recommendation. <em>TKDE</em>, <em>35</em>(10), 9850–9863. (<a
href="https://doi.org/10.1109/TKDE.2022.3168775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A knowledge graph (KG) consists of a set of interconnected typed entities and their attributes. Recently, KGs are popularly used as the auxiliary information to enable more accurate, explainable, and diverse user preference recommendations. Specifically, existing KG-based recommendation methods target modeling high-order relations/dependencies from long connectivity user-item interactions hidden in KG. However, most of them ignore the cold-start problems (i.e., user cold-start and item cold-start) of recommendation analytics, which restricts their performance in scenarios when involving new users or new items. Inspired by the success of meta-learning on scarce training samples, we propose a novel meta-learning based framework called MetaKG, which encompasses a collaborative-aware meta learner and a knowledge-aware meta learner, to capture meta users’ preference and entities’ knowledge for cold-start recommendations. The collaborative-aware meta learner aims to locally aggregate user preferences for each preference learning task. In contrast, the knowledge-aware meta learner is to globally generalize knowledge representation across different user preference learning tasks. Guided by two meta learners, MetaKG can effectively capture the high-order collaborative relations and semantic representations, which could be easily adapted to cold-start scenarios. Besides, we devise a novel adaptive task scheduler which can adaptively select the informative tasks for meta learning in order to prevent the model from being corrupted by noisy tasks. Extensive experiments on various cold-start scenarios using three real datasets demonstrate that our presented MetaKG outperforms all the existing state-of-the-art competitors in terms of effectiveness, efficiency, and scalability.},
  archive      = {J_TKDE},
  author       = {Yuntao Du and Xinjun Zhu and Lu Chen and Ziquan Fang and Yunjun Gao},
  doi          = {10.1109/TKDE.2022.3168775},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9850-9863},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MetaKG: Meta-learning on knowledge graph for cold-start recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Causal disentanglement for semantic-aware intent learning
in recommendation. <em>TKDE</em>, <em>35</em>(10), 9836–9849. (<a
href="https://doi.org/10.1109/TKDE.2022.3159802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional recommendation models trained on observational interaction data have generated large impacts in a wide range of applications, it faces bias problems that cover users’ true intent and thus deteriorate the recommendation effectiveness. Existing methods track this problem as eliminating bias for the robust recommendation, e.g., by re-weighting training samples or learning disentangled representations. The disentangled representation methods as the state-of-the-art eliminate bias by revealing cause-effect of the bias generation. However, how to design the semantic-aware and unbiased representations for users’ true intents is largely unexplored. To bridge the gap, we are the first to propose an unbiased and semantic-aware disentanglement learning called CaDSI ( Ca usal D isentanglement for S emantics-Aware I ntent Learning) from a causal perspective. Particularly, CaDSI explicitly models the causal relations underlying recommendation task, and thus produces semantic-aware representations via disentangling users’ true intents aware of specific item context. Moreover, the causal intervention mechanism is designed to eliminate confounding bias stemming from context information, which further aligns the semantic-aware representation with users’ true intent. Extensive experiments and case studies both validate the robustness and interpretability of our proposed model.},
  archive      = {J_TKDE},
  author       = {Xiangmeng Wang and Qian Li and Dianer Yu and Peng Cui and Zhichao Wang and Guandong Xu},
  doi          = {10.1109/TKDE.2022.3159802},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9836-9849},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Causal disentanglement for semantic-aware intent learning in recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from atypical behavior: Temporary interest aware
recommendation based on reinforcement learning. <em>TKDE</em>,
<em>35</em>(10), 9824–9835. (<a
href="https://doi.org/10.1109/TKDE.2022.3144292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional robust recommendation methods view atypical user-item interactions as noise and aim to reduce their impact with some kind of noise filtering technique, which often suffers from two challenges. First, in real world, atypical interactions may signal users’ temporary interest different from their general preference. Therefore, simply filtering out the atypical interactions as noise may be inappropriate and degrade the personalization of recommendations. Second, it is hard to acquire the temporary interest since there are no explicit supervision signals to indicate whether an interaction is atypical or not. To address this challenges, we propose a novel model called Temporary Interest Aware Recommendation (TIARec), which can distinguish atypical interactions from normal ones without supervision and capture the temporary interest as well as the general preference of users. Particularly, we propose a reinforcement learning framework containing a recommender agent and an auxiliary classifier agent, which are jointly trained with the objective of maximizing the cumulative return of the recommendations made by the recommender agent. During the joint training process, the classifier agent can judge whether the interaction with an item recommended by the recommender agent is atypical, and the knowledge about learning temporary interest from atypical interactions can be transferred to the recommender agent, which makes the recommender agent able to alone make recommendations that balance the general preference and temporary interest of users. At last, the experiments conducted on real world datasets verify the effectiveness of TIARec.},
  archive      = {J_TKDE},
  author       = {Ziwen Du and Ning Yang and Zhonghua Yu and Philip S. Yu},
  doi          = {10.1109/TKDE.2022.3144292},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9824-9835},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning from atypical behavior: Temporary interest aware recommendation based on reinforcement learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combinatorial optimization meets reinforcement learning:
Effective taxi order dispatching at large-scale. <em>TKDE</em>,
<em>35</em>(10), 9812–9823. (<a
href="https://doi.org/10.1109/TKDE.2021.3127077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ride hailing has become prevailing. Central in ride hailing platforms is taxi order dispatching which involves recommending a suitable driver for each order. Previous works use pure combinatorial optimization solutions for taxi dispatching, which suffer in practice due to complex dynamics of demand and supply and temporal dependency among dispatching decisions. Recent studies try to adopt data-driven method into combinatorial optimization hoping knowledge from history data would help overcome these challenges. Among these attempts, adoption of reinforcement learning shows great promise but current adoptions are a unidirectional integration which restricts the potential performance gains. In this work, we propose L earning T o D ispatch(LTD), a systematic solution that allows synergic integration of reinforcement learning and combinatorial optimization for large-scale taxi order dispatching. We demonstrate the necessity of online learning and taxi scheduling for reinforcement learning to work in synergy with combinatorial optimization, and devise corresponding algorithms. We also devise many tricks for more efficient calculation of the bipartite matching. Experiments show our methods can improve $36.4\%$ and $42.0\%$ on utility and efficiency at most, respectively. Especially, it achieves state-of-the-art performance in terms of utility.},
  archive      = {J_TKDE},
  author       = {Yongxin Tong and Dingyuan Shi and Yi Xu and Weifeng Lv and Zhiwei Qin and Xiaocheng Tang},
  doi          = {10.1109/TKDE.2021.3127077},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9812-9823},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Combinatorial optimization meets reinforcement learning: Effective taxi order dispatching at large-scale},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Guest editorial special issue on online recommendation
using AI and big data techniques. <em>TKDE</em>, <em>35</em>(10),
9809–9811. (<a href="https://doi.org/10.1109/TKDE.2023.3310728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of online service platforms has greatly influenced the way users conduct daily activities. In response to the requirements of frequent online activities, recommendation has become one of the best ways for the organizations, governments and individuals to understand their users and promote their services. Effective recommendation of online items has become critical in domains such as e-commerce and online media. Driven by the business success, academic research in this field has been active for many years. However, there are still many research challenges, such as context discovery, sequential user behavior influence, explainability and user interaction of system, big service data management. Especially, the highly dynamic online network data make these challenges even critical. Due to the high pressing interest and challenges in this area, this special issue is devoted to this topic, and focuses on the new solutions using AI and Big Data techniques.},
  archive      = {J_TKDE},
  author       = {Lei Chen and Xiangmin Zhou and Xiaochun Yang and Timos Sellis},
  doi          = {10.1109/TKDE.2023.3310728},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {10},
  pages        = {9809-9811},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Guest editorial special issue on online recommendation using AI and big data techniques},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A methodology to quickly perform opinion mining and build
supervised datasets using social networks mechanics. <em>TKDE</em>,
<em>35</em>(9), 9797–9808. (<a
href="https://doi.org/10.1109/TKDE.2023.3250822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social Networking Sites (SNS) offer a full set of possibilities to perform opinion studies such as polling or market analysis. Normally, artificial intelligence techniques are applied, and they often require supervised datasets. The process of building these is complex, time-consuming and expensive. In this paper, we propose to assist the labelling task by taking advantage of social network mechanics. In order to do that, we introduce the co-retweet relation to build a graph that allows us to propagate user labels to their similarity neighbourhood. Therefore, it is possible to iteratively build supervised datasets with significant less human effort and with higher accuracy than other weak-supervision techniques. We tested our proposal with 3 datasets labelled by an expert committee, and results shows that it outperforms other weak-supervision techniques. This methodology may be adapted to other social networks and topics, and it is relevant for applications like informed decision-making (e.g., content moderation), specially when interpretability is required.},
  archive      = {J_TKDE},
  author       = {Manuel Francisco and Juan Luis Castro},
  doi          = {10.1109/TKDE.2023.3250822},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9797-9808},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A methodology to quickly perform opinion mining and build supervised datasets using social networks mechanics},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Worker-churn-based task assignment with context-LSTM in
spatial crowdsourcing. <em>TKDE</em>, <em>35</em>(9), 9783–9796. (<a
href="https://doi.org/10.1109/TKDE.2023.3249828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasiveness of GPS-enabled devices and wireless communication technologies flourish the market of Spatial Crowdsourcing (SC), which consists of location-based tasks and requires workers to be at specific locations physically to complete them. In this work, we study the problem of worker-churn-based task assignment in SC, where tasks are assigned by considering workers’ churn. In particular, we aim to maximize the total rewards of task assignments based on the worker churn prediction. To solve the problem, we propose a two-phase framework, which consists of a worker churn prediction and a task assignment phase. In the first phase, we use an LSTM-based model to extract latent feelings of workers based on historical data and then estimate idle time intervals of workers. In the assignment phase, we design an efficient greedy algorithm and a Kuhn-Munkras-based algorithm that can achieve the optimal task assignment. To improve the accuracy of the idle time interval estimation for workers, we adopt a context-dependent LSTM model, which involves interactions between inputs and their context. We further optimize the original task assignment framework by proposing a travel distance optimization strategy to reduce the overall travel distance. Extensive experiments offer insight into the effectiveness and efficiency of the proposed solutions.},
  archive      = {J_TKDE},
  author       = {Yan Zhao and Tinghao Lai and Ziwei Wang and Kaixuan Chen and Huan Li and Kai Zheng},
  doi          = {10.1109/TKDE.2023.3249828},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9783-9796},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Worker-churn-based task assignment with context-LSTM in spatial crowdsourcing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning large neighborhood search for vehicle routing in
airport ground handling. <em>TKDE</em>, <em>35</em>(9), 9769–9782. (<a
href="https://doi.org/10.1109/TKDE.2023.3249799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dispatching vehicle fleets to serve flights is a key task in airport ground handling (AGH). Due to the notable growth of flights, it is challenging to simultaneously schedule multiple types of operations (services) for a large number of flights, where each type of operation is performed by one specific vehicle fleet. To tackle this issue, we first represent the operation scheduling as a complex vehicle routing problem and formulate it as a mixed integer linear programming (MILP) model. Then given the graph representation of the MILP model, we propose a learning assisted large neighborhood search (LNS) method using data generated based on real scenarios, where we integrate imitation learning and graph convolutional network (GCN) to learn a destroy operator to automatically select variables, and employ an off-the-shelf solver as the repair operator to reoptimize the selected variables. Experimental results based on a real airport show that the proposed method allows for handling up to 200 flights with 10 types of operations simultaneously, and outperforms state-of-the-art methods. Moreover, the learned method performs consistently accompanying different solvers, and generalizes well on larger instances, verifying the versatility and scalability of our method.},
  archive      = {J_TKDE},
  author       = {Jianan Zhou and Yaoxin Wu and Zhiguang Cao and Wen Song and Jie Zhang and Zhenghua Chen},
  doi          = {10.1109/TKDE.2023.3249799},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9769-9782},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning large neighborhood search for vehicle routing in airport ground handling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ultra data-oriented parallel fractional hot-deck imputation
with efficient linearized variance estimation. <em>TKDE</em>,
<em>35</em>(9), 9754–9768. (<a
href="https://doi.org/10.1109/TKDE.2023.3249567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel fractional hot-deck imputation (P-FHDI (Yang et al. 2020)) is a general-purpose, assumption-free tool for handling item nonresponse in big incomplete data by combining the theory of FHDI and parallel computing. FHDI cures multivariate missing data by filling each missing unit with multiple observed values (thus, hot-deck) without resorting to distributional assumptions. P-FHDI can tackle big incomplete data with millions of instances (big- $n$ ) or 10,000 variables (big- $p$ ). However, handling ultra incomplete data (i.e., concurrently big- $n$ and big- $p$ ) with tremendous instances and high dimensionality has posed problems to P-FHDI due to excessive memory requirement and execution time. To tackle the aforementioned challenges, we propose the ultra data-oriented P-FHDI (named UP-FHDI) capable of curing ultra incomplete data. In addition to the parallel Jackknife method, this paper enables a computationally efficient ultra data-oriented variance estimation using parallel linearization techniques. Results confirm that UP-FHDI can tackle an ultra dataset with one million instances and 10,000 variables. This paper illustrates the special parallel algorithms of UP-FHDI and confirms its positive impact on the subsequent deep learning performance.},
  archive      = {J_TKDE},
  author       = {Yicheng Yang and Yonghyun Kwon and Jae Kwang Kim and In Ho Cho},
  doi          = {10.1109/TKDE.2023.3249567},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9754-9768},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Ultra data-oriented parallel fractional hot-deck imputation with efficient linearized variance estimation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient spectral clustering algorithm based on
granular-ball. <em>TKDE</em>, <em>35</em>(9), 9743–9753. (<a
href="https://doi.org/10.1109/TKDE.2023.3249475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem that the traditional spectral clustering algorithm is time-consuming and resource consuming when applied to large-scale data, resulting in poor clustering effect or even unable to cluster, this paper proposes a spectral clustering algorithm based on granular-ball(GBSC). The algorithm changes the construction method of the similarity matrix. Based on granular-ball, the size of the similarity matrix is greatly reduced, and the construction of the similarity matrix is more reasonable. Experimental results show that the proposed algorithm achieves better speedup ratio, less memory consumption and stronger anti noise performance while achieving similar clustering results to the traditional spectral clustering algorithm. Suppose the number of granular-balls is $m$ , $n$ is the number of points in the dataset, and $m&amp;lt; &amp;lt; n$ , the time complexity of GBSC is $O(m^{3})$ . It is proved that GBSC has good adaptability to large-scale datasets. All codes have been released at https://github.com/xjnine/GBSC .},
  archive      = {J_TKDE},
  author       = {Jiang Xie and Weiyu Kong and Shuyin Xia and Guoyin Wang and Xinbo Gao},
  doi          = {10.1109/TKDE.2023.3249475},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9743-9753},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An efficient spectral clustering algorithm based on granular-ball},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling privacy-preserving and efficient authenticated
graph queries on blockchain-assisted clouds. <em>TKDE</em>,
<em>35</em>(9), 9728–9742. (<a
href="https://doi.org/10.1109/TKDE.2023.3249279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior research has introduced a new scenario of blockchain-assisted clouds where the data owner outsources original data to cloud servers and stores some metadata on the blockchain. Despite some research on key-value query and range query in this hybrid-storage scenario, other more complicated data types are not yet supported. In this article, we conduct pioneering research on authenticated queries for graph data, which is a popular data type such as the knowledge graph data, on the blockchain-assisted cloud. The primary challenge is how to design an authenticated data structure (ADS) that supports authenticated queries and can be easily maintained by the blockchain. To this end, we propose a novel ADS, named PAGB, based on the RSA accumulator and completeness set. It can also prevent the original data from being revealed to the public through blockchain or irrelevant queries. We further optimize our design to be more efficient in terms of communication and computation. The effectiveness and efficiency of PAGB are verified through theoretical analysis and extensive experiments.},
  archive      = {J_TKDE},
  author       = {Haotian Wu and Zecheng Li and Rui Song and Bin Xiao},
  doi          = {10.1109/TKDE.2023.3249279},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9728-9742},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enabling privacy-preserving and efficient authenticated graph queries on blockchain-assisted clouds},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep tabular data modeling with dual-route
structure-adaptive graph networks. <em>TKDE</em>, <em>35</em>(9),
9715–9727. (<a href="https://doi.org/10.1109/TKDE.2023.3249186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the inherent spatial or sequential structures underlying the data like images and texts, deep architectures such as convolutional neural networks (CNNs) and the Transformer have been recognized as the preeminent approaches in image processing and language modeling. In the real world, there are a large number of tabular data without any explicit structures, which breaks the inductive bias of most neural networks like CNNs. Although multi-layer perceptrons (MLPs) obtain empirical success on tabular data, they cannot well explain the underlying relationship between multiple variables. Compared with other fields, research on deep models toward tabular data has received relatively less scrutiny. To bridge this gap, we propose Dual-Route Structure-Adaptive Graph Networks (DRSA-Net) to model the nonlinearity in tabular feature vectors without any prior. DRSA-Net adaptively learns a sparse graph structure between variables and then characterizes interactions between them from the view of dual-route message passing. We demonstrate that DRSA-Net could easily degenerate into the typical MLPs and factorization machines (FMs). Extensive experiments on recommendations, images (no spatial information after preprocessing), and some benchmark machine learning datasets show that DRSA-Net achieves comparable or superior performance with many classic algorithms and recently proposed deep models.},
  archive      = {J_TKDE},
  author       = {Qinghua Zheng and Zhen Peng and Zhuohang Dang and Linchao Zhu and Ziqi Liu and Zhiqiang Zhang and Jun Zhou},
  doi          = {10.1109/TKDE.2023.3249186},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9715-9727},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep tabular data modeling with dual-route structure-adaptive graph networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective and efficient lexicographical order dependency
discovery. <em>TKDE</em>, <em>35</em>(9), 9700–9714. (<a
href="https://doi.org/10.1109/TKDE.2023.3248780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lexicographical order dependencies state relationships of order between lists of attributes. They naturally model the order-by clauses in SQL queries, and are proven useful in query optimizations concerning sorting. Despite their importance, order dependencies on a dataset are typically unknown and are too costly, if not impossible, to design or discover manually. Techniques for automatic order dependency discovery are recently studied. It is challenging for order dependency discovery to scale well, since it is by nature factorial in the number $m$ of attributes and quadratic in the number $n$ of tuples. In this article, we adopt a strategy that decouples the impact of $m$ from that of $n$ , and that still finds all minimal and valid lexicographical order dependencies. We present carefully designed data structures, a host of algorithms and optimizations, and an enhanced strategy combined with multithreaded parallelism, for an efficient implementation. Using a host of real-life and synthetic datasets, we experimentally verify our approach is up to orders of magnitude faster than the state-of-the-art methods, and can deliver better results with an improved definition of minimal attribute lists.},
  archive      = {J_TKDE},
  author       = {Jixuan Chen and Yifeng Jin and Yihan Li and Zijing Tan and Weidong Yang and Shuai Ma},
  doi          = {10.1109/TKDE.2023.3248780},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9700-9714},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Effective and efficient lexicographical order dependency discovery},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical interdisciplinary topic detection model for
research proposal classification. <em>TKDE</em>, <em>35</em>(9),
9685–9699. (<a href="https://doi.org/10.1109/TKDE.2023.3248608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The peer merit review of research proposals has been the major mechanism to decide grant awards. However, research proposals have become increasingly interdisciplinary. It has been a longstanding challenge to assign interdisciplinary proposals to appropriate reviewers so proposals are fairly evaluated. One of the critical steps in reviewer assignment is to generate accurate interdisciplinary topic labels for proposal-reviewer matching. Existing systems mainly collect topic labels manually generated by principle investigators. However, such human-reported labels can be non-accurate, incomplete, labor intensive, and time costly. What role can AI play in developing a fair and precise proposal reviewer assignment system? In this study, we collaborate with the National Science Foundation of China to address the task of automated interdisciplinary topic path detection. For this purpose, we develop a deep Hierarchical Interdisciplinary Research Proposal Classification Network (HIRPCN). Specifically, we first propose a hierarchical transformer to extract the textual semantic information of proposals. We then design an interdisciplinary graph and leverage GNNs to learn representations of each discipline in order to extract interdisciplinary knowledge. After extracting the semantic and interdisciplinary knowledge, we design a level-wise prediction component to fuse the two types of knowledge representations and detect interdisciplinary topic paths for each proposal. We conduct extensive experiments and expert evaluations on three real-world datasets to demonstrate the effectiveness of our proposed model.},
  archive      = {J_TKDE},
  author       = {Meng Xiao and Ziyue Qiao and Yanjie Fu and Hao Dong and Yi Du and Pengyang Wang and Hui Xiong and Yuanchun Zhou},
  doi          = {10.1109/TKDE.2023.3248608},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9685-9699},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical interdisciplinary topic detection model for research proposal classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multi-view scholar clustering with dynamic interest
tracking. <em>TKDE</em>, <em>35</em>(9), 9671–9684. (<a
href="https://doi.org/10.1109/TKDE.2023.3248221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scholar clustering has garnered increasing attention due to the explosive growth of scholar data. Although researchers have proposed many algorithms to cluster scholars, they typically focus on clustering scholars from the intrinsic view (scholars’ contents). These algorithms may lead to inaccurate and biased clustering results because they ignore the extrinsic view (scholar&#39;s specialty) and the changeability of scholars’ interest in each view. In this paper, we propose a multi-view scholar clustering topic model (MSCT), which integrates complementary information from both intrinsic and extrinsic views while considering dynamic scholar interests. Specifically, MSCT involves two novel schemes. The first one is multi-view integration , where MSCT collaboratively tracks scholars’ time-varying topic distribution from two views: intrinsic view and extrinsic view . The former exploits the details of different academic degrees in the title and information in the abstract; the latter leverages the specialty of different categories in the corresponding research field and research discipline. The second one is dynamic interest tracking , which dynamically models each scholar&#39;s interest distribution in terms of the current scholar texts and previously estimated distribution through a newly designed collapsed Gibbs sampling algorithm. Experimental results demonstrate that MSCT can significantly outperform state-of-the-art algorithms.},
  archive      = {J_TKDE},
  author       = {Ang Li and Yawen Li and Yingxia Shao and Bingyan Liu},
  doi          = {10.1109/TKDE.2023.3248221},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9671-9684},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view scholar clustering with dynamic interest tracking},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-aware dynamic graph embedding for asynchronous
structural evolution. <em>TKDE</em>, <em>35</em>(9), 9656–9670. (<a
href="https://doi.org/10.1109/TKDE.2023.3246059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic graphs refer to graphs whose structure dynamically changes over time. Despite the benefits of learning vertex representations (i.e., embeddings) for dynamic graphs, existing works merely view a dynamic graph as a sequence of changes within the vertex connections, neglecting the crucial asynchronous nature of such dynamics where the evolution of each local structure starts at different times and lasts for various durations. To maintain asynchronous structural evolutions within the graph, we innovatively formulate dynamic graphs as temporal edge sequences associated with joining time of vertices (ToV) and timespan of edges (ToE). Then, a time-aware Transformer is proposed to embed vertices’ dynamic connections and ToEs into the learned vertex representations. Meanwhile, we treat each edge sequence as a whole and embed its ToV of the first vertex to further encode the time-sensitive information. Extensive evaluations on several datasets show that our approach outperforms the state-of-the-art in a wide range of graph mining tasks. At the same time, it is very efficient and scalable for embedding large-scale dynamic graphs.},
  archive      = {J_TKDE},
  author       = {Yu Yang and Hongzhi Yin and Jiannong Cao and Tong Chen and Quoc Viet Hung Nguyen and Xiaofang Zhou and Lei Chen},
  doi          = {10.1109/TKDE.2023.3246059},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9656-9670},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Time-aware dynamic graph embedding for asynchronous structural evolution},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A trend of AI conference convergence in similarity: An
empirical study through trans-temporal heterogeneous graph.
<em>TKDE</em>, <em>35</em>(9), 9642–9655. (<a
href="https://doi.org/10.1109/TKDE.2023.3243602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Publishing the research works on academic publications is an important part of the scientific process. Since the development of computer science research is very fast, researchers tend to publish the research works in a fast way, such as conferences whose review processes are faster than the journals. In the past decades, one conference usually focuses on a specific research field and the topic or method overlap between conferences is low. We have noticed that, in recent years, some topics or methods which were once studied in a small number of specific research fields have become popular in many other fields. Naturally, we come up with two research questions: (1) Do the conferences indeed become similar? and (2) How do conferences become similar? In this paper, we first use a trans-temporal heterogeneous graph network to model academic conferences in recent 20 years. Due to the large number of conferences, we categorize these conferences into 6 research fields for brevity. Then, we first quantitatively and qualitatively assess “Do the research fields become similar?” and then focus on exploring “How do research fields become similar?”. From the result, we find the reason for the research fields in computer science become similar is that AI becomes pervasive and researchers tend to apply the machine learning methods to different application fields. Since the methods become universal between different research fields, researchers should pay more attention to advanced information in other fields to motivate more interdisciplinary works. To assist the researchers to explore related interdisciplinary advanced information, it is crucial to measure the cross-field impact of papers using the citation information and recommend the paper which has a high cross-field impact on the related researchers. As for the newly published papers which do not have any citations, we also propose a cross-field impact prediction model to recommend the cutting-edge research works to related researchers accurately. Experiments conducted on real-world datasets verify the effectiveness of the proposed method.},
  archive      = {J_TKDE},
  author       = {Shen Gao and Haotong Zhang and Xiuying Chen and Chongyang Tao and Dongyan Zhao and Rui Yan},
  doi          = {10.1109/TKDE.2023.3243602},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9642-9655},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A trend of AI conference convergence in similarity: An empirical study through trans-temporal heterogeneous graph},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zone-enhanced spatio-temporal representation learning for
urban POI recommendation. <em>TKDE</em>, <em>35</em>(9), 9628–9641. (<a
href="https://doi.org/10.1109/TKDE.2023.3243239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Points-of-interest (POIs) recommendation plays a vital role in location-based social networks (LBSNs) by introducing unexplored POIs to consumers and has drawn extensive attention from academia and industry. Existing POI recommender systems usually learn fixed latent vectors to represent both consumers and POIs from historical check-ins and make recommendations under the spatio-temporal constraints. However, we argue that the existing works still suffer from the challenges of explaining consumers’ complicated check-in actions. To this end, we first explore the interpretability of recommendations from the POI aspect, i.e., for a specific POI, its function usually changes over time, so representing a POI with a single fixed latent vector is not sufficient to describe the dynamic nature of POIs. Besides, check-in actions to a POI are also affected by the zone where it is located. In other words, the zone&#39;s embedding learned from POI distributions, road segments, and historical check-ins could be jointly utilized to enhance POI embeddings. Along this line, we propose a T ime-z o ne-space P OI embedding model ( ToP ), which integrates multi-knowledge graphs and topic model to introduce not only spatio-temporal effects but also sentiment constraints into POI embeddings for strengthening interpretability of recommendation. Specifically, ToP learns multiple latent vectors for a POI in a different period with spatial constraints via knowledge graph learning. To add sentiment constraints, ToP jointly combines these vectors with the zone&#39;s representations learned by topic models to make explainable recommendations. ToP considers the time, space, and sentiment of POI in a unified embedding framework, which benefits the POI recommendations. Extensive experiments on real-world Changchun city datasets demonstrate that ToP achieves state-of-the-art performance in terms of common metrics and provides more insights for consumers’ POI check-in actions.},
  archive      = {J_TKDE},
  author       = {En Wang and Yuanbo Xu and Yongjian Yang and Yiheng Jiang and Fukang Yang and Jie Wu},
  doi          = {10.1109/TKDE.2023.3243239},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9628-9641},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Zone-enhanced spatio-temporal representation learning for urban POI recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards lightweight and automated representation learning
system for networks. <em>TKDE</em>, <em>35</em>(9), 9613–9627. (<a
href="https://doi.org/10.1109/TKDE.2023.3243169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose LightNE 2.0 , a cost-effective, scalable, automated, and high-quality network embedding system that scales to graphs with hundreds of billions of edges on a single machine. In contrast to the mainstream belief that distributed architecture and GPUs are needed for large-scale network embedding with good quality, we prove that we can achieve higher quality, better scalability, lower cost, and faster runtime with shared-memory, CPU-only architecture. LightNE 2.0 combines two theoretically grounded embedding methods NetSMF and ProNE. We introduce the following techniques to network embedding for the first time: (1) a newly proposed downsampling method to reduce the sample complexity of NetSMF while preserving its theoretical advantages; (2) a high-performance parallel graph processing stack GBBS to achieve high memory efficiency and scalability; (3) sparse parallel hash table to aggregate and maintain the matrix sparsifier in memory; (4) a fast randomized singular value decomposition (SVD) enhanced by power iteration and fast orthonormalization to improve vanilla randomized SVD in terms of both efficiency and effectiveness; (5) Intel MKL for proposed fast randomized SVD and spectral propagation; and (6) a fast and lightweight AutoML library FLAML for automated hyperparameter tuning. Experimental results show that LightNE 2.0 can be up to 84× faster than GraphVite, 30× faster than PBG and 9× faster than NetSMF while delivering better performance. LightNE 2.0 can embed very large graph with 1.7 billion nodes and 124 billion edges in half an hour on a CPU server, while other baselines cannot handle very large graphs of this scale.},
  archive      = {J_TKDE},
  author       = {Yuyang Xie and Jiezhong Qiu and Laxman Dhulipala and Wenjian Yu and Jie Tang and Richard Peng and Chi Wang},
  doi          = {10.1109/TKDE.2023.3243169},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9613-9627},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards lightweight and automated representation learning system for networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Imbalanced clustering with theoretical learning bounds.
<em>TKDE</em>, <em>35</em>(9), 9598–9612. (<a
href="https://doi.org/10.1109/TKDE.2023.3242306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced clustering, where the number of samples varies in different clusters, has arisen from many real data mining applications. It has gained increasing attention. Nevertheless, due to its unsupervised nature, imbalanced clustering is more challenging than its supervised counterpart, i.e., imbalanced classification. Furthermore, existing imbalanced clustering methods are empirically designed and they often lack solid theoretical guarantees, e.g., the excess risk estimation. To solve these important but rarely studied problems, we first propose a novel $k$ -Means algorithm for imbalanced clustering problem with Adaptive Cluster Weight (MACW), together with its excess clustering risk bound analysis. Inspired by this theoretical result, we further propose an improved algorithm called Imbalanced Clustering with Theoretical Learning Bounds (ICTLB). It refines the weights and encourages the optimal trade-off among per-cluster weights by optimizing the excess clustering risk bound. A theoretically-principled justification of ICTLB is provided for verification. Comprehensive experiments on many imbalanced datasets verify the effectiveness of ICTLB in solving cluster imbalanced problems.},
  archive      = {J_TKDE},
  author       = {Jing Zhang and Hong Tao and Chenping Hou},
  doi          = {10.1109/TKDE.2023.3242306},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9598-9612},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Imbalanced clustering with theoretical learning bounds},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial index structures for modern storage devices: A
survey. <em>TKDE</em>, <em>35</em>(9), 9578–9597. (<a
href="https://doi.org/10.1109/TKDE.2023.3242207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To optimize the processing of spatial queries, there is an increasing interest in combining spatial index structures with modern storage devices like flash-based Solid State Drives, PCM, and 3D Xpoint. These devices have several advantages compared to classical Hard Disk Drives, such as lower power consumption, and faster reads and writes. However, modern storage devices have changed the paradigm of data management because of their intrinsic characteristics, such as asymmetric read and write costs. Intending to exploit the benefits of modern storage devices, the development of spatial index structures for these devices has been an emerging research topic with recent and constant advances in the literature. This includes the adaptation of existing spatial index structures like the R-tree, or even the design of innovative structures. In this article, we present a comprehensive survey that highlights the key ideas, compares the main characteristics, and discusses the advantages and disadvantages of spatial index structures for modern storage devices. Further, we study how experimental evaluations have been conducted to empirically compare these structures. Finally, we discuss challenges and identify potential future trends when indexing spatial data in this era of modern storage devices.},
  archive      = {J_TKDE},
  author       = {Anderson Chaves Carniel and Cristina Dutra Aguiar},
  doi          = {10.1109/TKDE.2023.3242207},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9578-9597},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatial index structures for modern storage devices: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-party sequential data publishing under differential
privacy. <em>TKDE</em>, <em>35</em>(9), 9562–9577. (<a
href="https://doi.org/10.1109/TKDE.2023.3241661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a set of local sequential datasets held by multiple parties, we study the problem of publishing a synthetic dataset that preserves approximate sequentiality information of the integrated dataset while satisfying differential privacy for each local dataset. The existing solutions for publishing differentially private sequential data in the centralized setting mostly adopt tree-based approaches. Such approaches rely on different tree structures that encode sequential data&#39;s statistical information. The construction of a tree structure is normally done by recursively splitting nodes whose noisy scores (e.g., entropy or count) are larger than a given threshold. However, extending similar ideas to the multi-party setting is challenging. First, the comparison between noisy scores and a given threshold needs to be done in a distributed manner without letting the parties know the noisy scores, while satisfying differential privacy for each local dataset. Second, in the multi-party setting the large number of node splitting decisions incurs prohibitive computation costs. In addressing the above challenges, we present DPST , a distributed prediction suffix tree construction solution. In DPST, we first introduce a novel node splitting decision method that calculates the comparison result under encryption with substantially improved efficiency. Then we present a novel batch-based tree construction approach to reduce computation costs. In order to achieve high parallel performance without incurring any extra communication cost, we introduce the conjunction and slide methods to ensure that each batch contains a stable number of carefully arranged decision tasks . To further reduce communication and computation costs, we propose a prefix-based pre-pruning method to reduce the number of nodes that need to be judged whether to split by an interactive protocol. Extensive experiments on real datasets demonstrate that our DPST solution offers desirable data utility with low computation and communication costs.},
  archive      = {J_TKDE},
  author       = {Peng Tang and Rui Chen and Sen Su and Shanqing Guo and Lei Ju and Gaoyuan Liu},
  doi          = {10.1109/TKDE.2023.3241661},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9562-9577},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-party sequential data publishing under differential privacy},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A distributed privacy-preserving learning dynamics in
general social networks. <em>TKDE</em>, <em>35</em>(9), 9547–9561. (<a
href="https://doi.org/10.1109/TKDE.2023.3241442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study a distributed privacy-preserving learning problem in social networks with general topology. The agents can communicate with each other over the network, which may result in privacy disclosure, since the trustworthiness of the agents cannot be guaranteed. Given a set of options which yield unknown stochastic rewards, each agent is required to learn the best one, aiming at maximizing the resulting expected average cumulative reward. To serve the above goal, we propose a four-staged distributed algorithm which efficiently exploits the collaboration among the agents while preserving the local privacy for each of them. In particular, our algorithm proceeds iteratively, and in every round, each agent i) randomly perturbs its adoption for the privacy-preserving purpose, ii) disseminates the perturbed adoption over the social network in a nearly uniform manner through random walking, iii) selects an option by referring to the perturbed suggestions received from its peers, and iv) decides whether or not to adopt the selected option as preference according to its latest reward feedback. Through solid theoretical analysis, we quantify the trade-off among the number of agents (or communication overhead), privacy preserving and learning utility. We also perform extensive simulations to verify the efficacy of our proposed social learning algorithm.},
  archive      = {J_TKDE},
  author       = {Youming Tao and Shuzhen Chen and Feng Li and Dongxiao Yu and Jiguo Yu and Hao Sheng},
  doi          = {10.1109/TKDE.2023.3241442},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9547-9561},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A distributed privacy-preserving learning dynamics in general social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). MCoR-miner: Maximal co-occurrence nonoverlapping sequential
rule mining. <em>TKDE</em>, <em>35</em>(9), 9531–9546. (<a
href="https://doi.org/10.1109/TKDE.2023.3241213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of sequential pattern mining (SPM) is to discover potentially useful information from a given sequence. Although various SPM methods have been investigated, most of these focus on mining all of the patterns. However, users sometimes want to mine patterns with the same specific prefix pattern, called co-occurrence pattern. Since sequential rule mining can make better use of the results of SPM, and obtain better recommendation performance, this paper addresses the issue of maximal co-occurrence nonoverlapping sequential rule (MCoR) mining and proposes the MCoR-Miner algorithm. To improve the efficiency of support calculation, MCoR-Miner employs depth-first search and backtracking strategies equipped with an indexing mechanism to avoid the use of sequential searching. To obviate useless support calculations for some sequences, MCoR-Miner adopts a filtering strategy to prune the sequences without the prefix pattern. To reduce the number of candidate patterns, MCoR-Miner applies the frequent item and binomial enumeration tree strategies. To avoid searching for the maximal rules through brute force, MCoR-Miner uses a screening strategy. To validate the performance of MCoR-Miner, eleven competitive algorithms were conducted on eight sequences. Our experimental results showed that MCoR-Miner outperformed other competitive algorithms, and yielded better recommendation performance than frequent co-occurrence pattern mining. All algorithms and datasets can be downloaded from https://github.com/wuc567/Pattern-Mining/tree/master/MCoR-Miner .},
  archive      = {J_TKDE},
  author       = {Yan Li and Chang Zhang and Jie Li and Wei Song and Zhenlian Qi and Youxi Wu and Xindong Wu},
  doi          = {10.1109/TKDE.2023.3241213},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9531-9546},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MCoR-miner: Maximal co-occurrence nonoverlapping sequential rule mining},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TFS-ABS: Traceable and forward-secure attribute-based
signature scheme with constant-size. <em>TKDE</em>, <em>35</em>(9),
9514–9530. (<a href="https://doi.org/10.1109/TKDE.2023.3241198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-based signature (ABS) is a versatile and useful cryptogrammic technology. In an ABS scheme, every signer is distributed a signing secret key in term of her/his attributes, and endorses a message in relation to some signing policy fulfilled by the signer&#39;s attributes. The verifier checks that the signature is indeed endorsed by the signer whose attributes match with the signing policy. However, existing ABS schemes suffer from the issue of abusing signature and key exposure. To address the above issues, we provide a traceable and forward-secure attribute-based signature (TFS-ABS) scheme with constant-size supporting flexible threshold predicates. Furthermore, we prove that the presented TFS-ABS scheme is existential unforgeability against selective predicate attack under the standard model. We reduce the security for the provided scheme to $q$ -Diffie-Hellman exponentiation assumption. The designed scheme can be used to alleviate the damage induced by key exposure and traces the real identity of signer by attribute authority (AA) when the signer occurs abusing behavior. Furthermore, the signature size in presented scheme keeps constant and is independent of the number of attributes. Experimental evaluations exhibit that the presented TFS-ABS scheme is efficient in term of the communication and computation overhead.},
  archive      = {J_TKDE},
  author       = {Zhaozhe Kang and Jiguo Li and Jian Shen and Jinguang Han and Yuting Zuo and Yichen Zhang},
  doi          = {10.1109/TKDE.2023.3241198},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9514-9530},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TFS-ABS: Traceable and forward-secure attribute-based signature scheme with constant-size},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sequential addressing subsampling method for massive data
analysis under memory constraint. <em>TKDE</em>, <em>35</em>(9),
9502–9513. (<a href="https://doi.org/10.1109/TKDE.2023.3241075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of massive data in recent years brings challenges to automatic statistical inference. This is particularly true if the data are too numerous to be read into memory as a whole. Accordingly, new sampling techniques are needed to sample data from a hard drive. In this paper, we propose a sequential addressing subsampling (SAS) method that can sample data directly from the hard drive. The newly proposed SAS method is time saving in terms of addressing cost compared to that of the random addressing subsampling (RAS) method. Estimators (e.g., the sample mean) based on the SAS subsamples are constructed, and their properties are studied. We conduct a series of simulation studies to verify the finite sample performance of the proposed SAS estimators. The time cost is also compared between the SAS and RAS methods. An analysis of the airline data is presented for illustration purpose.},
  archive      = {J_TKDE},
  author       = {Rui Pan and Yingqiu Zhu and Baishan Guo and Xuening Zhu and Hansheng Wang},
  doi          = {10.1109/TKDE.2023.3241075},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9502-9513},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A sequential addressing subsampling method for massive data analysis under memory constraint},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selective imputation for multivariate time series datasets
with missing values. <em>TKDE</em>, <em>35</em>(9), 9490–9501. (<a
href="https://doi.org/10.1109/TKDE.2023.3240858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series often contain missing values for reasons such as failures in data collection mechanisms. Since these missing values can complicate the analysis of time series data, imputation techniques are typically used to deal with this issue. However, the quality of the imputation directly affects the performance of downstream tasks. In this paper, we propose a selective imputation method that identifies a subset of timesteps with missing values to impute in a multivariate time series dataset. This selection, which will result in shorter and simpler time series, is based on both reducing the uncertainty of the imputations and representing the original time series as good as possible. In particular, the method uses multi-objective optimization techniques to select the optimal set of points, and in this selection process, we leverage the beneficial properties of the Multi-task Gaussian Process (MGP). The method is applied to different datasets to analyze the quality of the imputations and the performance obtained in downstream tasks, such as classification or anomaly detection. The results show that much shorter and simpler time series are able to maintain or even improve both the quality of the imputations and the performance of the downstream tasks.},
  archive      = {J_TKDE},
  author       = {Ane Blázquez-García and Kristoffer Wickstrøm and Shujian Yu and Karl Øyvind Mikalsen and Ahcène Boubekki and Angel Conde and Usue Mori and Robert Jenssen and Jose A. Lozano},
  doi          = {10.1109/TKDE.2023.3240858},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9490-9501},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Selective imputation for multivariate time series datasets with missing values},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards hard few-shot relation classification.
<em>TKDE</em>, <em>35</em>(9), 9476–9489. (<a
href="https://doi.org/10.1109/TKDE.2023.3240851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot relation classification (FSRC) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a task, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing models still perform suboptimally when handling hard FSRC tasks with similar categories that confuse the model to distinguish correctly. We argue this is largely due to two reasons, 1) ignoring pivotal and discriminate information that is crucial to distinguish confusing classes, and 2) training indiscriminately via randomly sampled tasks of varying difficulty. In this article, we introduce a novel prototypical network approach with contrastive learning that learns more informative and discriminative representations by exploiting relation label information. We further design two strategies that increase the difficulty of training tasks and allow the model to adaptively learn to focus on hard tasks. By doing so, our model can better represent subtle inter-relation variance and grow up through task difficulty. Extensive experiments on three standard benchmarks demonstrate the effectiveness of our method.},
  archive      = {J_TKDE},
  author       = {Jiale Han and Bo Cheng and Zhiguo Wan and Wei Lu},
  doi          = {10.1109/TKDE.2023.3240851},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9476-9489},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards hard few-shot relation classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based non-sampling for knowledge graph enhanced
recommendation. <em>TKDE</em>, <em>35</em>(9), 9462–9475. (<a
href="https://doi.org/10.1109/TKDE.2023.3240832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) enhanced recommendation, which aims to solve the cold start and explainability in recommender systems, has attracted considerable research interest recently. Existing recommender systems usually focus on implicit feedback such as purchase history without negative feedback. Most of them apply the negative sampling strategy to deal with the implicit feedback data, which may ignore the latent positive user-item interaction. Some other works adopt the non-sampling strategy that treats all non-observed interactions as negative samples and assigns a weight for each negative sample to represent the probability that this sample is a positive sample. However, they use a simple and intuitive weight assignment strategy and cannot catch the latent relationship from all interaction data. To address these problems, we consider graph structure information of both user-item interaction and knowledge graph, and propose a Graph-based Non-Sampling strategy to achieve efficient performance in Knowledge graph enhanced Recommendation (GNSKR). GNSKR utilizes node centrality to significantly improve recommendation performance with low computation cost. Meanwhile, we combine knowledge graph embedding and recommendation task with a local aggregation block, which efficiently catches the high-order connection information in KG enhanced recommendation. Experiments on three datasets show that GNSKR embraces the state-of-the-art with competitive efficiency.},
  archive      = {J_TKDE},
  author       = {Shuang Liang and Jie Shao and Jiasheng Zhang and Bin Cui},
  doi          = {10.1109/TKDE.2023.3240832},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9462-9475},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph-based non-sampling for knowledge graph enhanced recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HGNAS++: Efficient architecture search for heterogeneous
graph neural networks. <em>TKDE</em>, <em>35</em>(9), 9448–9461. (<a
href="https://doi.org/10.1109/TKDE.2023.3239842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graphs are commonly used to describe networked data with multiple types of nodes and edges. Heterogeneous Graph Neural Networks (HGNNs) are powerful tools for analyzing heterogeneous graphs. However, designing neural architectures of HGNNs requires extensive domain knowledge and time-consuming manual work. Recently, neural architecture search algorithms have become popular in automatically designing neural architectures for homogeneous graph neural networks. In this paper, we present a Heterogeneous Graph Neural Architecture Search algorithm (HGNAS for short) which allows the automatic design of heterogeneous graph neural architectures. Specifically, HGNAS first designs a new search space based on existing popular HGNNs. Then, HGNAS uses a policy network as the controller to sample and find the best neural architecture from the designed search space by maximizing the expected accuracy of the selected architectures on a given validation dataset. Moreover, we design a new method HGNAS++ to improve the efficiency of HGNAS by training the RNN controller within a generative adversarial learning framework. The basic idea of HGNAS++ is to embed a pairwise ranker into the reinforcement learning based architecture search algorithm. The pairwise ranker can be taken as a discriminator which selects more accurate architectures between pairs of candidate architectures. Then, the RNN controller can be updated more efficiently by only using a relatively small number of candidate architectures selected by the pairwise ranker. Experiments on real-world heterogeneous graph datasets show that HGNAS is capable of designing novel HGNNs that beat the best human-invented HGNNs. On the benchmark datasets, HGNAS++ improves HGNAS in terms of evaluation cost, with a reduction of 50\% of the evaluated candidate architectures and a decrease of 24\% in search time on average. As a byproduct, HGNAS++ can find sparse yet powerful neural architectures for HGNNs.},
  archive      = {J_TKDE},
  author       = {Yang Gao and Peng Zhang and Chuan Zhou and Hong Yang and Zhao Li and Yue Hu and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3239842},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9448-9461},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HGNAS++: Efficient architecture search for heterogeneous graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepAltTrip: Top-k alternative itineraries for trip
recommendation. <em>TKDE</em>, <em>35</em>(9), 9433–9447. (<a
href="https://doi.org/10.1109/TKDE.2023.3239595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trip itinerary recommendation finds an ordered sequence of Points-of-Interest (POIs) from a large number of candidate POIs in a city. In this paper, we propose a deep learning-based framework, called DeepAltTrip, that learns to recommend top- $k$ alternative itineraries for given source and destination POIs. These alternative itineraries would be not only popular given the historical routes adopted by past users but also dissimilar (or diverse) to each other. The DeepAltTrip consists of two major components: (i) Itinerary Net (ITRNet) which estimates the likelihood of POIs on an itinerary by using graph autoencoders and two (forward and backward) LSTMs; and (ii) a route generation procedure to generate $k$ diverse itineraries passing through relevant POIs obtained using ITRNet. For the route generation step, we propose a novel sampling algorithm that can seamlessly handle a wide variety of user-defined constraints. To the best of our knowledge, this is the first work that learns from historical trips to provide a set of alternative itineraries to the users. Extensive experiments conducted on eight popular real-world datasets show the effectiveness and efficacy of our approach over state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Syed Md. Mukit Rashid and Mohammed Eunus Ali and Muhammad Aamir Cheema},
  doi          = {10.1109/TKDE.2023.3239595},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9433-9447},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DeepAltTrip: Top-K alternative itineraries for trip recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximal clique search in weighted graphs. <em>TKDE</em>,
<em>35</em>(9), 9421–9432. (<a
href="https://doi.org/10.1109/TKDE.2023.3239409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching for $k$ -cliques in graphs has been an important problem in graph analysis due to its large number of applications. Previously, finding $k$ -cliques in weighted graphs aimed at finding cliques with the largest sum of weight (with no distinction between the edge or the vertex weights), usually called the sum model. However, the algorithms under the sum model may result in solutions consisting of low-weight vertices or edges (outliers). To address this issue, we propose a new model named maximal $(S, C, K)$ -clique in weighted graphs and study the problem of maximal ( $S, C, K$ )-clique search (MCS). We first propose an enumeration-based algorithm MCSE, which checks every $k$ -clique to identify the maximal ( $S, C, K$ )-clique. To improve the efficiency, we further propose two improved algorithms MCSP and MCSC. Instead of checking every possible $k$ -clique, MCSP focuses on ( $S, C$ ) values that cannot be dominated and obtains the maximal $(S, C, K)$ -cliques directly based on these values. MCSC is devised by further optimizing MCSP based on some key observations on maximal cliques and cliques’ nesting property. We also propose two index structures, BCS-Index and ICS-Index, to achieve optimal query. The former stores all maximal $(S, C, K)$ -cliques, while the latter uses the clique&#39;s nesting property to reduce the space cost of index construction. Extensive experiments conducted on six real graphs demonstrate the efficiency and effectiveness of our proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Dongxiao Yu and Lifang Zhang and Qi Luo and Xiuzhen Cheng and Zhipeng Cai},
  doi          = {10.1109/TKDE.2023.3239409},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9421-9432},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Maximal clique search in weighted graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling reviews for few-shot recommendation via enhanced
prototypical network. <em>TKDE</em>, <em>35</em>(9), 9407–9420. (<a
href="https://doi.org/10.1109/TKDE.2023.3239169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although some existing models are proposed to exploit reviews for improving performance for recommender systems, few of them can handle the following issues led by the insufficient review data: (i) The regular training process does not exactly fit the scenario of preference prediction with few historical behaviors. (ii) Extracting informative and sufficient semantic features from limited review texts is a challenging work. To alleviate these issues, this paper proposes an enhanced prototypical network, FS-EPN, that leverages reviews for recommendation under the few-shot setting. FS-EPN consists of an attentional prototypical network being the basic architecture, a sentiment encoder and a memory collector cooperating to capture the extra sentimental and collaborative information from both user and item perspectives for semantic information supplement. We train FS-EPN under the meta-learning framework, which models the training process in the episodic manner to mimic the few-shot test environment. Extensive experiments conducted on six publicly available datasets demonstrate the superior capability of FS-EPN over several state-of-the-art models in few-shot recommendation.},
  archive      = {J_TKDE},
  author       = {Tingting Liang and Congying Xia and Haoran Xu and Ziqiang Zhao and Yuyu Yin and Liang Chen and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3239169},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9407-9420},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling reviews for few-shot recommendation via enhanced prototypical network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised graph completion for incomplete multi-view
clustering. <em>TKDE</em>, <em>35</em>(9), 9394–9406. (<a
href="https://doi.org/10.1109/TKDE.2023.3238416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering (IMVC) is challenging, as it requires adequately exploring complementary and consistency information under the incompleteness of data. Most existing approaches attempt to overcome the incompleteness at instance-level. In this work, we develop a new approach to facilitate IMVC from a new perspective. Specifically, we transfer the issue of missing instances to a similarity graph completion problem for incomplete views, and propose a self-supervised multi-view graph completion algorithm to infer the associated missing entries. Further, by incorporating constrained feature learning, the inferred graph can be naturally leveraged in representation learning. We theoretically show that our feature learning process performs an Auto-Regressive filter function by encoding the learned similarity graph, which could yield discriminative representation for a clustering task. Extensive experiments demonstrate the effectiveness of the proposed method in comparison with state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Cheng Liu and Si Wu and Rui Li and Dazhi Jiang and Hau-San Wong},
  doi          = {10.1109/TKDE.2023.3238416},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9394-9406},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Self-supervised graph completion for incomplete multi-view clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-quality temporal link prediction for weighted dynamic
graphs via inductive embedding aggregation. <em>TKDE</em>,
<em>35</em>(9), 9378–9393. (<a
href="https://doi.org/10.1109/TKDE.2023.3238360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal link prediction (TLP) is an inference task on dynamic graphs that predicts future topology using historical graph snapshots. Existing TLP methods are usually designed for unweighted graphs with fixed node sets. Some of them cannot be generalized to the prediction of weighted graphs with non-fixed node sets. Although several methods can still be used to predict weighted graphs, they can only derive low-quality prediction snapshots sensitive to large edge weights but fail to distinguish small and zero weights in adjacency matrices. In this study, we consider the challenging high-quality TLP on weighted dynamic graphs and propose a novel inductive dynamic embedding aggregation (IDEA) method, inspired by the high-resolution video prediction. IDEA combines conventional error minimization objectives with a scale difference minimization objective, which can generate high-quality weighted prediction snapshots, distinguishing differences among large, small, and zero weights in adjacency matrices. Since IDEA adopts an inductive dynamic embedding scheme with an attentive node aligning unit and adaptive embedding aggregation module, it can also tackle the TLP on weighted graphs even with non-fixed node sets. Experiments on datasets of various scenarios validate that IDEA can derive high-quality prediction results for weighted dynamic graphs and tackle the variation of node sets.},
  archive      = {J_TKDE},
  author       = {Meng Qin and Chaorui Zhang and Bo Bai and Gong Zhang and Dit-Yan Yeung},
  doi          = {10.1109/TKDE.2023.3238360},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9378-9393},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {High-quality temporal link prediction for weighted dynamic graphs via inductive embedding aggregation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In pursuit of beauty: Aesthetic-aware and context-adaptive
photo selection in crowdsensing. <em>TKDE</em>, <em>35</em>(9),
9364–9377. (<a href="https://doi.org/10.1109/TKDE.2023.3237969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive view of the mobile crowd bridges various real-world scenes and people&#39;s perceptions with the gathering of distributed crowdsensing photos. To elaborate informative visuals for viewers, existing techniques introduce photo selection as an essential step in crowdsensing. Yet, the aesthetic preference of viewers, at the very heart of their experiences under various crowdsensing contexts (e.g., travel planning), is seldom considered and hardly guaranteed. We propose CrowdPicker, a novel photo selection framework with adaptive aesthetic awareness for crowdsensing. With the observations on aesthetic uncertainty and bias in different crowdsensing contexts, we exploit a joint effort of mobile crowdsourcing and domain adaptation to actively learn contextual knowledge for dynamically tailoring the aesthetic predictor. Concretely, an aesthetic utility measure is invented based on the probabilistic balance formalization to quantify the benefit of photos in improving the adaptation performance. We prove the NP-hardness of sampling the best-utility photos for crowdsourcing annotation and present a (1-1/e) approximate solution. Furthermore, a two-stage distillation-based adaptation architecture is designed based on fusing contextual and common aesthetic preferences. Extensive experiments on three datasets and four raw models demonstrate the performance superiority of CrowdPicker over four photo selection baselines and four typical sampling strategies. Cross-dataset evaluation illustrates the impacts of aesthetic bias on selection.},
  archive      = {J_TKDE},
  author       = {Tongqing Zhou and Zhiping Cai and Fang Liu and Jinshu Su},
  doi          = {10.1109/TKDE.2023.3237969},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9364-9377},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {In pursuit of beauty: Aesthetic-aware and context-adaptive photo selection in crowdsensing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast gumbel-max sketch and its applications. <em>TKDE</em>,
<em>35</em>(9), 9350–9363. (<a
href="https://doi.org/10.1109/TKDE.2023.3237857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The well-known Gumbel-Max Trick for sampling elements from a categorical distribution (or more generally a non-negative vector) and its variants have been widely used in areas such as machine learning and information retrieval. To sample a random element $i$ in proportion to its positive weight $v_{i}$ , the Gumbel-Max Trick first computes a Gumbel random variable $g_{i}$ for each positive weight element $i$ , and then samples the element $i$ with the largest value of $g_{i}+\ln v_{i}$ . Recently, applications including similarity estimation and weighted cardinality estimation require to generate $k$ independent Gumbel-Max variables from high dimensional vectors. However, it is computationally expensive for a large $k$ (e.g., hundreds or even thousands) when using the traditional Gumbel-Max Trick. To solve this problem, we propose a novel algorithm, FastGM , which reduces the time complexity from $O(kn^+)$ to $O(k \ln k + n^+)$ , where $n^+$ is the number of positive elements in the vector of interest. FastGM stops the procedure of Gumbel random variables computing for many elements, especially for those with small weights. We perform experiments on a variety of real-world datasets and the experimental results demonstrate that FastGM is orders of magnitude faster than state-of-the-art methods without sacrificing accuracy or incurring additional expenses.},
  archive      = {J_TKDE},
  author       = {Yuanming Zhang and Pinghui Wang and Yiyan Qi and Kuankuan Cheng and Junzhou Zhao and Guangjian Tian and Xiaohong Guan},
  doi          = {10.1109/TKDE.2023.3237857},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9350-9363},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast gumbel-max sketch and its applications},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards multi-user, secure, and verifiable <span
class="math inline"><em>k</em></span>NN query in cloud database.
<em>TKDE</em>, <em>35</em>(9), 9333–9349. (<a
href="https://doi.org/10.1109/TKDE.2023.3237879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the boom in cloud computing, data outsourcing in location-based services is proliferating and has attracted increasing interest from research communities and commercial applications. Nevertheless, since the cloud server is probably both untrusted and malicious, concerns about data security and result integrity have become on the rise sharply. In addition, in the single-user situation assumed by most existing works, query users can capture query content from each other even though the queries are encrypted, which may incur the leakage of query privacy. Unfortunately, there exists little work that can commendably assure data security and result integrity in the multi-user setting. To this end, in this article, we study the problem of multi-user, secure, and verifiable $k$ nearest neighbor query ( MSV $k$k NN ). To support MSV $k$ NN, we first propose a novel unified structure, called verifiable and secure index (VSI). Based on this, we devise a series of secure protocols to facilitate query processing and develop a compact verification strategy. Given an MSV $k$ NN query, our proposed solution can not merely answer the query efficiently while can guarantee: 1) preserving data privacy , query privacy , result privacy , and access patterns privacy ; 2) authenticating the correctness and completeness of the results; 3) supporting multi-user with different keys. Finally, the formal security analysis and complexity analysis are theoretically proven and the performance and feasibility of our proposed approach are empirically evaluated and demonstrated.},
  archive      = {J_TKDE},
  author       = {Ningning Cui and Kang Qian and Taotao Cai and Jianxin Li and Xiaochun Yang and Jie Cui and Hong Zhong},
  doi          = {10.1109/TKDE.2023.3237879},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9333-9349},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards multi-user, secure, and verifiable $k$NN query in cloud database},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Incremental learning based on granular ball rough sets for
classification in dynamic mixed-type decision system. <em>TKDE</em>,
<em>35</em>(9), 9319–9332. (<a
href="https://doi.org/10.1109/TKDE.2023.3237833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Granular computing, a new paradigm for solving large-scale and complex problems, has made significant progresses in knowledge discovery. Granular ball computing (GBC) is a novel granular computing method, which can rapidly generate scalable and robust information granules, that is, granular balls. However, a comprehensive index for measuring the performance of a granular ball does not exist. Furthermore, GBC lacks a mechanism to deal with dynamic decision systems. Therefore, in this study, the quality index of a granular ball is first formulated. Next, with this index, a novel granular ball rough sets model (GBRS) based on GBC is proposed. GBRS is more conducive to learning knowledge from uncertain datasets and more suited to incremental learning than the latest granular ball neighborhood rough sets model based on GBC. Subsequently, an incremental mechanism is introduced into GBRS, and two incremental learning models are developed for objects increasing in stream patterns and batch patterns, respectively. In the incremental learning process, three patterns of granular balls, that is, update, fusion, and split, were well studied when a set of objects was added to the decision system. Finally, to verify the effectiveness and efficiency, we apply GBRS and these two incremental learning models into classification tasks. Compared with four current state-of-the-art classification methods based on granular computing and four classical classifiers in machine learning, the proposed classifiers in this paper achieve higher classification accuracy as well as better efficiency on benchmark datasets.},
  archive      = {J_TKDE},
  author       = {Qinghua Zhang and Chengying Wu and Shuyin Xia and Fan Zhao and Man Gao and Yunlong Cheng and Guoyin Wang},
  doi          = {10.1109/TKDE.2023.3237833},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9319-9332},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incremental learning based on granular ball rough sets for classification in dynamic mixed-type decision system},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Symbolic minimization on relational data. <em>TKDE</em>,
<em>35</em>(9), 9307–9318. (<a
href="https://doi.org/10.1109/TKDE.2022.3222827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current wave of AI is heavily driven by data, especially for cognitive capabilities. Minimization of data semantics not only reveals core information but also becomes a guide in a wide range of domains. However, scalability is theoretically weak in pure semantic methodologies. In order to cooperate with large DBs, expressiveness is over-sacrificed in existing techniques. Thus, the quality of discovered patterns and redundancies are far from satisfactory. In this article, we formalize symbolic minimization on relational DBs and prove its NP-Completeness. A lossless technique is proposed by inducing generic first-order Horn rules that infer a subset of records from the others. More importantly, we further improve the scalability via effective caching and pruning without sacrificing the expressiveness of first-order Horn rules. A concrete system is implemented and comprehensively evaluated. Experiments show that our technique removes up to 70\% contents and outperforms the state-of-the-art on minimization and scalability. The optimizations reduce up to 96\% memory consumption and accelerate the performance by two orders. Our technique shows the practicality of pure semantic approaches in database mining.},
  archive      = {J_TKDE},
  author       = {Ruoyu Wang and Daniel Sun and Raymond K. Wong},
  doi          = {10.1109/TKDE.2022.3222827},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9307-9318},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Symbolic minimization on relational data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AI assistants: A framework for semi-automated data
wrangling. <em>TKDE</em>, <em>35</em>(9), 9295–9306. (<a
href="https://doi.org/10.1109/TKDE.2022.3222538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data wrangling tasks such as obtaining and linking data from various sources, transforming data formats, and correcting erroneous records, can constitute up to 80\% of typical data engineering work. Despite the rise of machine learning and artificial intelligence, data wrangling remains a tedious and manual task. We introduce AI assistants , a class of semi-automatic interactive tools to streamline data wrangling. An AI assistant guides the analyst through a specific data wrangling task by recommending a suitable data transformation that respects the constraints obtained through interaction with the analyst. We formally define the structure of AI assistants and describe how existing tools that treat data cleaning as an optimization problem fit the definition. We implement AI assistants for four common data wrangling tasks and make AI assistants easily accessible to data analysts in an open-source notebook environment for data science, by leveraging the common structure they follow. We evaluate our AI assistants both quantitatively and qualitatively through three example scenarios. We show that the unified and interactive design makes it easy to perform tasks that would be difficult to do manually or with a fully automatic tool.},
  archive      = {J_TKDE},
  author       = {Tomas Petricek and Gerrit J. J. van den Burg and Alfredo Nazábal and Taha Ceritli and Ernesto Jiménez-Ruiz and Christopher K. I. Williams},
  doi          = {10.1109/TKDE.2022.3222538},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9295-9306},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AI assistants: A framework for semi-automated data wrangling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GRRS: Accurate and efficient neighborhood rough set for
feature selection. <em>TKDE</em>, <em>35</em>(9), 9281–9294. (<a
href="https://doi.org/10.1109/TKDE.2022.3222447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important preprocessing step in data mining and pattern recognition. The neighborhood rough set (NRS) model is a widely-used rough set model for feature selection on continuous data. All currently known NRS models are defined on a distance metric — mostly the euclidean distance metric — which invalidates the NRS models in scenarios wherein the euclidean distance is ineffective, for example, while considering differing attribute weights. We first introduce the concept of space division of granular-rectangular, and then construct the neighborhood radius in our method by describing the relationship between child and parent spaces, which avoids the use of a distance metric and reduces the search space for the neighborhood radius. This greatly improves both the accuracy and efficiency of NRS. In addition, the upper and lower approximations of the granular-rectangular rough sets (GRRSs) comprise equivalence classes; this results in better performance of GRRS in knowledge representation compared with the traditional NRS. Experimental results on public benchmark datasets reveal that our method, GRRS, achieves higher accuracy than ten popular and state-of-the-art feature-selection methods, including two NRS algorithms. Moreover, GRRS outperforms the established NRS algorithms regarding efficiency, including the state-of-the-art NRS algorithm, GBNRS. All code has been released as an open libary called GRRS: https://github.com/syxiaa/GRRS .},
  archive      = {J_TKDE},
  author       = {Shuyin Xia and Shulin Wu and Xinxing Chen and Guoyin Wang and Xinbo Gao and Qinghua Zhang and Elisabeth Giem and Zizhong Chen},
  doi          = {10.1109/TKDE.2022.3222447},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9281-9294},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GRRS: Accurate and efficient neighborhood rough set for feature selection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and secure skyline queries over vertical data
federation. <em>TKDE</em>, <em>35</em>(9), 9269–9280. (<a
href="https://doi.org/10.1109/TKDE.2022.3222415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skyline is a primitive operation in multi-objective decision applications and there is a growing demand to support such operations over a data federation, where the entire dataset is separately held by multiple data providers (a.k.a., silos). Data federations notably increase the amount of data available for data-intensive applications such as commercial recommendation and location based services. Yet they also challenge the conventional implementation of skyline queries because the raw data cannot be shared within the federation and the secure computation cross silos can be two or three orders of magnitude slower than plaintext computation. These constraints render existing solutions inefficient on data federation. In this work, we propose a novel local dominance based framework for efficient skyline queries over a vertical data federation. We decompose the skyline query into plaintext local dominance computations and secure result aggregations, which can perform as many computations in plaintext as possible without compromising security. We further propose a dedicate private set intersection based algorithm to accelerate the query processing. Extensive evaluations on both synthetic and real-world datasets show that compared with general-purpose secure multi-party computation techniques, our solutions reduce the time cost by up to 35.4× and communication cost by two orders of magnitude respectively.},
  archive      = {J_TKDE},
  author       = {Yuanyuan Zhang and Yexuan Shi and Zimu Zhou and Chunbo Xue and Yi Xu and Ke Xu and Junping Du},
  doi          = {10.1109/TKDE.2022.3222415},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9269-9280},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient and secure skyline queries over vertical data federation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mixed-order relation-aware recurrent neural networks for
spatio-temporal forecasting. <em>TKDE</em>, <em>35</em>(9), 9254–9268.
(<a href="https://doi.org/10.1109/TKDE.2022.3222373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal forecasting has a wide range of applications in smart city efforts, such as traffic forecasting and air quality prediction. Graph Convolutional Recurrent Neural Networks (GCRNN) are the state-of-the-art methods for this problem, which learn temporal dependencies by RNNs and exploit pairwise node proximity to model spatial dependencies. However, the spatial relations in real data are not simply pairwise but sometimes in a higher order among multiple nodes. Moreover, spatio-temporal sequences deriving from nature are often regulated by known or unknown physical laws. GCRNNs rarely take into account the underlying physics in real-world systems, which may result in degenerated performance. To address these issues, we devise a general model called Mixed-Order Relation-Aware RNN (MixRNN+) for spatio-temporal forecasting. Specifically, our MixRNN+ captures the complex mixed-order spatial relations of nodes through a newly proposed building block called Mixer, and simultaneously addressing the underlying physics by the integration of a new residual update strategy. Experimental results on three forecasting tasks in smart city applications (including traffic speed, taxi flow, and air quality prediction) demonstrate the superiority of our model against the state-of-the-art methods. We have also deployed a cloud-based system using our method as the bedrock model to show its practicality.},
  archive      = {J_TKDE},
  author       = {Yuxuan Liang and Kun Ouyang and Yiwei Wang and Zheyi Pan and Yifang Yin and Hongyang Chen and Junbo Zhang and Yu Zheng and David S. Rosenblum and Roger Zimmermann},
  doi          = {10.1109/TKDE.2022.3222373},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9254-9268},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Mixed-order relation-aware recurrent neural networks for spatio-temporal forecasting},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spectral adversarial training for robust graph neural
network. <em>TKDE</em>, <em>35</em>(9), 9240–9253. (<a
href="https://doi.org/10.1109/TKDE.2022.3222207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies demonstrate that Graph Neural Networks (GNNs) are vulnerable to slight but adversarially designed perturbations, known as adversarial examples . To address this issue, robust training methods against adversarial examples have received considerable attention in the literature. Adversarial Training (AT) is a successful approach to learning a robust model using adversarially perturbed training samples. Existing AT methods on GNNs typically construct adversarial perturbations in terms of graph structures or node features. However, they are less effective and fraught with challenges on graph data due to the discreteness of graph structure and the relationships between connected examples. In this work, we seek to address these challenges and propose S pectral A dversarial T raining (SAT), a simple yet effective adversarial training approach for GNNs. SAT first adopts a low-rank approximation of the graph structure based on spectral decomposition, and then constructs adversarial perturbations in the spectral domain rather than directly manipulating the original graph structure. To investigate its effectiveness, we employ SAT on three widely used GNNs. Experimental results on four public graph datasets demonstrate that SAT significantly improves the robustness of GNNs against adversarial attacks without sacrificing classification accuracy and training efficiency.},
  archive      = {J_TKDE},
  author       = {Jintang Li and Jiaying Peng and Liang Chen and Zibin Zheng and Tingting Liang and Qing Ling},
  doi          = {10.1109/TKDE.2022.3222207},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9240-9253},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spectral adversarial training for robust graph neural network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). OAG: Linking entities across large-scale heterogeneous
knowledge graphs. <em>TKDE</em>, <em>35</em>(9), 9225–9239. (<a
href="https://doi.org/10.1109/TKDE.2022.3222168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different knowledge graphs for the same domain are often uniquely housed on the Web. Effectively linking entities from different graphs is critical for building an open and comprehensive knowledge graph. However, linking entities across different sources has thus far faced various challenges, including the increasingly large-scale volume of the data, the heterogeneity of the graphs, and the ambiguity of real-world entities. To address them, we propose a unified framework LinKG. Specifically, we decouple the problem into different linking tasks based on the unique properties of each type of entity. To link word sequence based entities, we propose an LSTM-based method to capture word dependencies. To link entities of large scale, we utilize the hashing technique and convolutional neural networks for scalable and accurate linking. To link ambiguous entities, we propose heterogeneous graph attention networks to leverage heterogeneous structural information. Finally, to validate the design choices of different LinKG modules, we characterize the relationships between different tasks based on the single-domain and multi-domain transfer models. Extensive experiments demonstrate the effectiveness of LinKG with an overall F1-score of 95.15\%, based on which we deploy and release the Open Academic Graph (OAG)—the largest publicly available heterogeneous academic graph to date.},
  archive      = {J_TKDE},
  author       = {Fanjin Zhang and Xiao Liu and Jie Tang and Yuxiao Dong and Peiran Yao and Jie Zhang and Xiaotao Gu and Yan Wang and Evgeny Kharlamov and Bin Shao and Rui Li and Kuansan Wang},
  doi          = {10.1109/TKDE.2022.3222168},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9225-9239},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {OAG: Linking entities across large-scale heterogeneous knowledge graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable kernel <span
class="math inline"><em>k</em></span>-means with randomized sketching:
From theory to algorithm. <em>TKDE</em>, <em>35</em>(9), 9210–9224. (<a
href="https://doi.org/10.1109/TKDE.2022.3222146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel $k$ -means is a fundamental unsupervised learning in data mining. Its computational requirements are typically at least quadratic in the number of data, which are prohibitive for large-scale scenarios. To address these issues, we propose a novel randomized sketching approach SKK based on the circulant matrix. SKK projects the kernel matrix left and right according to the proposed sketch matrices to obtain a smaller one and accelerates the matrix-matrix product by the fast Fourier transform based on the circulant matrix, which can greatly reduce the computational requirements of the approximate kernel $k$ -means estimator with the same generalization bound as the exact kernel $k$ -means in the statistical setting. In particular, theoretical analysis shows that taking the sketch dimension of $\sqrt{n}$ is sufficient for SKK to achieve the optimal excess risk bound with only a fraction of computations, where $n$ is the number of data. The extensive experiments verify our theoretical analysis, and SKK achieves the state-of-the-art performances on 12 real-world datasets. To the best of our knowledge, in randomized sketching, this is the first time that unsupervised learning makes such a significant breakthrough.},
  archive      = {J_TKDE},
  author       = {Rong Yin and Yong Liu and Weiping Wang and Dan Meng},
  doi          = {10.1109/TKDE.2022.3222146},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9210-9224},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Scalable kernel $k$-means with randomized sketching: From theory to algorithm},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locating multiple equivalent feature subsets in feature
selection for imbalanced classification. <em>TKDE</em>, <em>35</em>(9),
9195–9209. (<a href="https://doi.org/10.1109/TKDE.2022.3222047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection can be used to solve imbalanced classification problems encountered in big data projects. There often exist multiple feature subsets achieving the same accuracy. These subsets tend to exhibit different acquisition difficulty and reliability, thus offering decision-makers with multiple choices if they can be well-identified. This work formulates feature selection as a Multimodal Multiobjective Problem (MMOP), where a point on Pareto front in objective space has multiple equivalent feature subsets in decision space. To seek more equivalent feature subsets, this work proposes a new multiobjective fireworks algorithm. It extends a latest single-objective fireworks algorithm to a multiobjective version such that it becomes suitable for solving MMOP. An adaptive strategy and special archive guidance are newly designed to improve its performance. A weighted extreme learning machine is chosen to classify datasets and return classification accuracy due to its fast learning speed. Experimental results show that the proposed algorithm outperforms its compared ones on 15 imbalanced classification datasets including 5 low-dimensional, 5 high-dimensional feature selection problems and 5 large-scale problems with larger imbalanced ratio, and its runtime is the least among them. Also, fault diagnosis in self-organizing cellular networks, as an important imbalance classification problem, is performed by the proposed algorithm and the results show that it can perform fault diagnosis well.},
  archive      = {J_TKDE},
  author       = {Shoufei Han and Kun Zhu and MengChu Zhou and Hesham Alhumade and Abdullah Abusorrah},
  doi          = {10.1109/TKDE.2022.3222047},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9195-9209},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Locating multiple equivalent feature subsets in feature selection for imbalanced classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Data augmented sequential recommendation based on
counterfactual thinking. <em>TKDE</em>, <em>35</em>(9), 9181–9194. (<a
href="https://doi.org/10.1109/TKDE.2022.3222070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation has recently attracted increasing attention from the industry and academic communities. While previous models have achieved remarkable successes, an important problem may still hinder their performances, that is, the sparsity of the real-world data. In this paper, we propose a novel counterfactual data augmentation framework to alleviate the problem of data sparsity. In specific, our framework contains a sampler model and an anchor model. The sampler model aims to generate high-quality user behavior sequences, while the anchor model is trained based on the original and new generated samples, and leveraged to provide the final recommendation list. To implement the sampler model, we first design four types of heuristic methods based on either random or frequency-based strategies. And then, to improve the quality of the generated sequences, we propose two learning-based samplers by discovering the decision boundaries or increasing the sample informativeness. At last, we build an RL based model to automatically determine where to edit the history behaviors and how many items should be replaced. Considering that the sampler model can be imperfect, we, at last, analyze the influence of the noisy information contained in the generated sequences on the anchor model in theory, and design a simple but effective method to better serve the anchor model. We conduct extensive experiments to demonstrate the effectiveness of our model.},
  archive      = {J_TKDE},
  author       = {Xu Chen and Zhenlei Wang and Hongteng Xu and Jingsen Zhang and Yongfeng Zhang and Wayne Xin Zhao and Ji-Rong Wen},
  doi          = {10.1109/TKDE.2022.3222070},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9181-9194},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Data augmented sequential recommendation based on counterfactual thinking},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate time series forecasting with dynamic graph
neural ODEs. <em>TKDE</em>, <em>35</em>(9), 9168–9180. (<a
href="https://doi.org/10.1109/TKDE.2022.3221989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series forecasting has long received significant attention in real-world applications, such as energy consumption and traffic prediction. While recent methods demonstrate good forecasting abilities, they have three fundamental limitations. (i). Discrete neural architectures: Interlacing individually parameterized spatial and temporal blocks to encode rich underlying patterns leads to discontinuous latent state trajectories and higher forecasting numerical errors. (ii). High complexity: Discrete approaches complicate models with dedicated designs and redundant parameters, leading to higher computational and memory overheads. (iii). Reliance on graph priors: Relying on predefined static graph structures limits their effectiveness and practicability in real-world applications. In this paper, we address all the above limitations by proposing a continuous model to forecast M ultivariate T ime series with dynamic G raph neural O rdinary D ifferential E quations ( MTGODE ). Specifically, we first abstract multivariate time series into dynamic graphs with time-evolving node features and unknown graph structures. Then, we design and solve a neural ODE to complement missing graph topologies and unify both spatial and temporal message passing, allowing deeper graph propagation and fine-grained temporal information aggregation to characterize stable and precise latent spatial-temporal dynamics. Our experiments demonstrate the superiorities of MTGODE from various perspectives on five time series benchmark datasets.},
  archive      = {J_TKDE},
  author       = {Ming Jin and Yu Zheng and Yuan-Fang Li and Siheng Chen and Bin Yang and Shirui Pan},
  doi          = {10.1109/TKDE.2022.3221989},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9168-9180},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multivariate time series forecasting with dynamic graph neural ODEs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Latent structure mining with contrastive modality fusion
for multimedia recommendation. <em>TKDE</em>, <em>35</em>(9), 9154–9167.
(<a href="https://doi.org/10.1109/TKDE.2022.3221949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimedia contents are of predominance in the modern Web era. Recent years have witnessed growing research interests in multimedia recommendation, which aims to predict whether a user will interact with an item with multimodal contents. Most previous studies focus on modeling user-item interactions with multimodal features included as side information. However, this scheme is not well-designed for multimedia recommendation. First, only collaborative item-item relationships are implicitly modeled through high-order item-user-item co-occurrences. Considering that items are associated with rich contents in multiple modalities, we argue that the latent semantic item-item structures underlying these multimodal contents could be beneficial for learning better item representations and assist the recommender models to comprehensively discover candidate items. Second, although previous studies consider multiple modalities, their ways of fusing multiple modalities by linear combination or concatenation is insufficient to fully capture content information of items and item relationships. To address these deficiencies, we propose a latent structure MI ning with C ont R astive m O dality fusion model, which we term MICRO for brevity. To be specific, we devise a novel modality-aware structure learning module, which learns item-item relationships for each modality. Based on the learned modality-aware latent item relationships, we perform graph convolutions to explicitly inject item affinities into modality-aware item representations. Additionally, we design a novel multimodal contrastive framework to facilitate item-level multimodal fusion by mining both modality-shared and modality-specific information. Finally, the item representations are plugged into existing collaborative filtering methods to make accurate recommendation. Extensive experiments on three real-world datasets demonstrate the superiority of our method over state-of-arts and rationalize the design choice of our work.},
  archive      = {J_TKDE},
  author       = {Jinghao Zhang and Yanqiao Zhu and Qiang Liu and Mengqi Zhang and Shu Wu and Liang Wang},
  doi          = {10.1109/TKDE.2022.3221949},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9154-9167},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Latent structure mining with contrastive modality fusion for multimedia recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ROLE: Rotated lorentzian graph embedding model for
asymmetric proximity. <em>TKDE</em>, <em>35</em>(9), 9140–9153. (<a
href="https://doi.org/10.1109/TKDE.2022.3221929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding, which aims to learn low-dimensional node representations to preserve original graph structures, has attracted extensive research interests. However, most existing graph embedding models represent nodes in Euclidean spaces, which cannot effectively preserve complex patterns, e.g., hierarchical structures. Very recently, several hyperbolic embedding models have been proposed to preserve the hierarchical information in negative curvature spaces. Nevertheless, existing hyperbolic models fail to model the asymmetric proximity between nodes. To address this, we investigate a new asymmetric hyperbolic network representation problem, which targets at jointly preserving the hierarchical structures and asymmetric proximity for general directed graphs. We solve this problem by proposing a novel Ro tated L orentzian E mbedding (ROLE) model, which yields two main benefits. First, our model can effectively capture both implicit and explicit hierarchical structures that come from the network topology and category information of nodes, respectively. Second, it can model the asymmetric proximity using rotation transformations. Specifically, we represent each node with a Lorentzian embedding vector, and learn two rotation matrices to reflect the direction of edges. We conduct extensive experiments on four real-world directed graph datasets. Empirical results demonstrate that the proposed approach consistently outperforms various state-of-the-art embedding models. In particular, ROLE achieves HR@1 scores up to 19.8\% higher and NDCG@5 scores up to 11.3\% higher than the best baselines on the task of node recommendation.},
  archive      = {J_TKDE},
  author       = {Shanshan Feng and Lisi Chen and Kaiqi Zhao and Wei Wei and Xuemeng Song and Shuo Shang and Panos Kalnis and Ling Shao},
  doi          = {10.1109/TKDE.2022.3221929},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9140-9153},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ROLE: Rotated lorentzian graph embedding model for asymmetric proximity},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure learning via meta-hyperedge for dynamic rumor
detection. <em>TKDE</em>, <em>35</em>(9), 9128–9139. (<a
href="https://doi.org/10.1109/TKDE.2022.3221438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social networks have greatly facilitated our lives but have also propagated the spreading of rumours. Traditional works mostly find rumors from content, but content can be strategically manipulated to evade such detection, making these methods brittle. To improve the accuracy and robustness of rumor detection, we propose to integrate and exploit the content, propagation structure, and temporal relations because information in the networks always spreads dynamically with significant structures. In this paper, we propose a novel rumor detection framework in online temporal networks via structure learning. Specifically, to exploit the propagation structure, we propose a novel hyperedge walking strategy on a meta-hyperedge graph to learn the representations of sub-structures in the networks. Then a hyperedge expansion method is proposed to generate more global structural features. The expanded hyperedges are more hierarchical, making the learned structural embeddings more expressive. To make full use of content, we design a hypergraph learning model using hyperedge expansion to fuse node content with structural features and generate comprehensive representations for the entire graph. To exploit temporal relations, we design a masked temporal attention unit for learning the evolving patterns of the network. Extensive evaluations with six state-of-the-art baselines on two real-world datasets demonstrate the superiority of our solution.},
  archive      = {J_TKDE},
  author       = {Xiangguo Sun and Hongzhi Yin and Bo Liu and Qing Meng and Jiuxin Cao and Alexander Zhou and Hongxu Chen},
  doi          = {10.1109/TKDE.2022.3221438},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9128-9139},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Structure learning via meta-hyperedge for dynamic rumor detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyperbolic neural collaborative recommender. <em>TKDE</em>,
<em>35</em>(9), 9114–9127. (<a
href="https://doi.org/10.1109/TKDE.2022.3221386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning techniques have yielded immense success on recommender systems. However, one weakness of most deep methods is that, users/items mutual semantic relationships, which are latent in the user-item interactions, are not distilled out explicitly. Moreover, most methods have been primarily focused on representation learning in euclidean geometry. Since recent studies have shown that the bipartite graph structure has the non-euclidean latent anatomy, euclidean embeddings may suffer from a certain degree of distortion. In this work, we present H yperbolic N eural C ollaborative R ecommender (HNCR), a deep hyperbolic representation learning method that exploits mutual semantic relationships among users/items for collaborative filtering tasks. HNCR first introduces a neighbor construction strategy to build user and item semantic neighborhoods. Then HNCR develops a framework based on deep learning and hyperbolic geometry to integrate constructed neighborhoods into recommendation. To evaluate our method, we conduct experiments on the four datasets. Experimental results show the superiority of HNCR compared with its euclidean counterpart and state-of-the-art recommendation baselines. The results also indicate that hyperbolic representations can reflect meaningful data insights.},
  archive      = {J_TKDE},
  author       = {Anchen Li and Bo Yang and Huan Huo and Hongxu Chen and Guandong Xu and Zhen Wang},
  doi          = {10.1109/TKDE.2022.3221386},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9114-9127},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hyperbolic neural collaborative recommender},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <span
class="math inline"><em>k</em><em>t</em></span>-safety: Graph release
via <span class="math inline"><em>k</em></span>-anonymity and <span
class="math inline"><em>t</em></span>-closeness. <em>TKDE</em>,
<em>35</em>(9), 9102–9113. (<a
href="https://doi.org/10.1109/TKDE.2022.3221333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a wide spectrum of real-world applications, it is very important to analyze and mine graph data such as social networks, communication networks, citation networks, and so on. However, the release of such graph data often raises privacy issue, and the graph privacy preservation has recently drawn much attention from the database community. While prior works on graph privacy preservation mainly focused on protecting the privacy of either the graph structure only or vertex attributes only, in this paper, we propose a novel mechanism for graph privacy preservation by considering attacks from both graph structures and vertex attributes, which transforms the original graph to a so-called $kt$ -safe graph, via $k$ -anonymity and $t$ -closeness. We prove that the generation of a $kt$ -safe graph is NP-hard, therefore, we propose a feasible framework for effectively and efficiently anonymizing a graph with low anonymization cost. In particular, we design a cost-model-based graph partitioning approach to enable our proposed divide-and-conquer strategy for the graph anonymization, and propose effective optimization techniques such as pruning method and a tree synopsis to improve the anonymization efficiency over large-scale graphs. Extensive experiments have been conducted to verify the efficiency and effectiveness of our proposed $kt$ -safe graph generation approach on both real and synthetic data sets.},
  archive      = {J_TKDE},
  author       = {Weilong Ren and Kambiz Ghazinour and Xiang Lian},
  doi          = {10.1109/TKDE.2022.3221333},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9102-9113},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {$kt$-safety: Graph release via $k$-anonymity and $t$-closeness},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). DMGAN: Dynamic multi-hop graph attention network for
traffic forecasting. <em>TKDE</em>, <em>35</em>(9), 9088–9101. (<a
href="https://doi.org/10.1109/TKDE.2022.3221316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the intelligent transportation system, traffic forecasting, which is generally characterized as a graph spatial-temporal prediction task, plays a crucial role. It is challenging to generate reliable forecast results due to the complexity of traffic topological information and the inherent uncertainty of road traffic circumstances. Existing works generally focus on modeling spatial dependency on static graph structures, but ignore dynamic relations between road segments and cannot extract long-range traffic dependencies in spatial-temporal domains. To bridge the above gaps, we present a novel framework, called Dynamic Multi-Hop Graph Attention Network (DMGAN). Specifically, we leverage dynamic graph modeling to capture time-varying relations across road sections and introduce the multi-hop operation in each message propagation layer to extract long-range spatial dependency. Meanwhile, we develop a fusion-attention module, preserving both local and global hidden layer outputs of the encoder, to capture both long- and short-term temporal dependencies jointly. In this way, our method can fully model complex time-varying traffic topology information and capture the internal patterns of traffic series by integrating dynamic graph structure and temporal attention component. DGMAN achieves state-of-the-art performance in three metrics, as demonstrated by experimental findings on four real-world public traffic datasets, METR-LA, PEMS-BAY, PEMS03, and PEMS07. This code and data are available at https://github.com/EEHITer/2022-TKDE-DMGAN-Pytorch/tree/main for reproducibility and further studies.},
  archive      = {J_TKDE},
  author       = {Rui Li and Fan Zhang and Tong Li and Ning Zhang and Tingting Zhang},
  doi          = {10.1109/TKDE.2022.3221316},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9088-9101},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DMGAN: Dynamic multi-hop graph attention network for traffic forecasting},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traffic flow prediction based on spatiotemporal potential
energy fields. <em>TKDE</em>, <em>35</em>(9), 9073–9087. (<a
href="https://doi.org/10.1109/TKDE.2022.3221183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is a fundamental problem in spatiotemporal data mining. Most of the existing studies focuses on designing statistical models to fit historical traffic data, which are purely data-driven approaches and fail to reveal the underlying mechanisms of urban traffic. To address this issue, we propose the spatiotemporal potential energy field model (ST-PEF+), which applies the field theory for human mobility to interpret the underlying mechanisms of urban traffic, and introduces the theory into data-driven deep learning models. ST-PEF+ consists of a PEF extraction module and a data-driven module. Inspired by the field theory for human mobility, the PEF extraction module adopts an algorithm to decompose the grid-based traffic flow graph into several polytree-based potential energy fields (PEFs), where traffic flows from high potential locations to low potential locations, just as water is driven by the gravity field. We also provide a theoretical analysis to ensure that the polytree decomposition algorithm can decompose any traffic flow graph. In the data-driven module, ST-PEF+ learns a spatiotemporal deep learning model to predict the dynamics of PEFs. The model adopts correlation-adaptive neural network structures, which consists of a temporal component for temporal correlations and a spatial component for spatial correlations. The temporal component employs a GRU and DCN combined structure to capture both short-term autocorrelation and long-term repeating patterns of PEFs. The spatial component extends the GAT using weighted directed attention to model the asymmetric spatial structure in PEFs. The prediction results of traffic flow are finally derived from PEFs that are predicted by the spatiotemporal deep learning model. We conduct extensive evaluations on three real-world traffic datasets. The results show that our model outperforms the state-of-the-art baselines. In addition, case studies confirm that the PEFs learned in our framework can reveal the underlying mechanisms of urban traffic, thus improving the model interpretability.},
  archive      = {J_TKDE},
  author       = {Jingyuan Wang and Jiahao Ji and Zhe Jiang and Leilei Sun},
  doi          = {10.1109/TKDE.2022.3221183},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9073-9087},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Traffic flow prediction based on spatiotemporal potential energy fields},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HoppingSketch: More accurate temporal membership query and
frequency query. <em>TKDE</em>, <em>35</em>(9), 9067–9072. (<a
href="https://doi.org/10.1109/TKDE.2022.3221111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, research on temporal membership queries is indispensable. Generally, temporal membership queries exist in two modalities: fixed windows and sliding windows, the latter having obvious advantages. The first sketch that implements temporal membership queries is the persistent Bloom filter (PBF). PBF has two shortcomings: it does not support sliding windows nor frequency queries. Here, we propose HoppingSketch to promote the original PBF. It is the first sketch that implements temporal membership queries for sliding windows. HoppingSketch is a general and efficient data stream processing framework, able to implement different tasks thanks to different atomic sketches. When the atomic sketches are Bloom filters and we apply them to PBF, HoppingSketch can achieve significantly higher temporal membership query accuracy than the original PBF. When the atomic sketches are sketches of Count-Min, Conservative Update, and Count, HoppingSketch can achieve more accurate frequency query than by applying PBF on the corresponding sketches. Our experimental results demonstrate the advantages of HoppingSketch compared with the state-of-the-art.},
  archive      = {J_TKDE},
  author       = {Zhuochen Fan and Yubo Zhang and Siyuan Dong and Yi Zhou and Fangyi Liu and Tong Yang and Steve Uhlig and Bin Cui},
  doi          = {10.1109/TKDE.2022.3221111},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9067-9072},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HoppingSketch: More accurate temporal membership query and frequency query},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning bi-typed multi-relational heterogeneous graph via
dual hierarchical attention networks. <em>TKDE</em>, <em>35</em>(9),
9054–9066. (<a href="https://doi.org/10.1109/TKDE.2022.3221099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bi-typed multi-relational heterogeneous graph (BMHG) is one of the most common graphs in practice, for example, academic networks, e-commerce user behavior graph and enterprise knowledge graph. It is a critical and challenge problem on how to learn the numerical representation for each node to characterize subtle structures. However, most previous studies treat all node relations in BMHG as the same class of relation without distinguishing the different characteristics between the intra-type relations and inter-type relations of the bi-typed nodes, causing the loss of significant structure information. To address this issue, we propose a novel D ual H ierarchical A ttention N etworks (DHAN) based on the bi-typed multi-relational heterogeneous graphs to learn comprehensive node representations with the intra-type and inter-type attention-based encoder under a hierarchical mechanism. Specifically, the former encoder aggregates information from the same type of nodes, while the latter aggregates node representations from its different types of neighbors. Moreover, to sufficiently model node multi-relational information in BMHG, we adopt a newly proposed hierarchical mechanism. By doing so, the proposed dual hierarchical attention operations enable our model to fully capture the complex structures of the bi-typed multi-relational heterogeneous graphs. Experimental results on various tasks against the state-of-the-arts sufficiently confirm the capability of DHAN in learning node representations on the BMHGs.},
  archive      = {J_TKDE},
  author       = {Yu Zhao and Shaopeng Wei and Huaming Du and Xingyan Chen and Qing Li and Fuzhen Zhuang and Ji Liu and Gang Kou},
  doi          = {10.1109/TKDE.2022.3221099},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9054-9066},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning bi-typed multi-relational heterogeneous graph via dual hierarchical attention networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking graph auto-encoder models for attributed graph
clustering. <em>TKDE</em>, <em>35</em>(9), 9037–9053. (<a
href="https://doi.org/10.1109/TKDE.2022.3220948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recent graph clustering methods have resorted to Graph Auto-Encoders (GAEs) to perform joint clustering and embedding learning. However, two critical issues have been overlooked. First, the accumulative error, inflicted by learning from noisy clustering assignments, degrades the effectiveness of the clustering model. This problem is called Feature Randomness. Second, reconstructing the adjacency matrix sets the model to learn irrelevant similarities for the clustering task. This problem is called Feature Drift. Furthermore, the theoretical relation between the aforementioned problems has not yet been investigated. We study these issues from two aspects: (1) there is a trade-off between Feature Randomness and Feature Drift when clustering and reconstruction are performed at the same level, and (2) the problem of Feature Drift is more pronounced for GAE models, compared with vanilla auto-encoder models. Thus, we reformulate the GAE-based clustering methodology. Our solution is two-fold. First, we propose a sampling operator $\Xi$ that triggers a protection mechanism against Feature Randomness. Second, we propose an operator $\Upsilon$ that triggers a correction mechanism against Feature Drift by gradually transforming the reconstructed graph into a clustering-oriented one. As principal advantages, our solution grants a considerable improvement in clustering effectiveness and can be easily tailored to GAE models.},
  archive      = {J_TKDE},
  author       = {Nairouz Mrabah and Mohamed Bouguessa and Mohamed Fawzi Touati and Riadh Ksantini},
  doi          = {10.1109/TKDE.2022.3220948},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9037-9053},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Rethinking graph auto-encoder models for attributed graph clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Region embedding with intra and inter-view contrastive
learning. <em>TKDE</em>, <em>35</em>(9), 9031–9036. (<a
href="https://doi.org/10.1109/TKDE.2022.3220874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised region representation learning aims to extract dense and effective features from unlabeled urban data. While some efforts have been made for solving this problem based on multiple views, existing methods are still insufficient in extracting representations in a view and/or incorporating representations from different views. Motivated by the success of contrastive learning for representation learning, we propose to leverage it for multi-view region representation learning and design a model called ReMVC (Region Embedding with Multi-View Contrastive Learning) by following two guidelines: $i$ ) comparing a region with others within each view for effective representation extraction and $ii$ ) comparing a region with itself across different views for cross-view information sharing. We design the intra-view contrastive learning module which helps to learn distinguished region embeddings and the inter-view contrastive learning module which serves as a soft co-regularizer to constrain the embedding parameters and transfer knowledge across multi-views. We exploit the learned region embeddings in two downstream tasks named land usage clustering and region popularity prediction. Extensive experiments demonstrate that our model achieves impressive improvements compared with seven state-of-the-art baseline methods, and the margins are over 30\% in the land usage clustering task.},
  archive      = {J_TKDE},
  author       = {Liang Zhang and Cheng Long and Gao Cong},
  doi          = {10.1109/TKDE.2022.3220874},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9031-9036},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Region embedding with intra and inter-view contrastive learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Dual feature interaction-based graph convolutional network.
<em>TKDE</em>, <em>35</em>(9), 9019–9030. (<a
href="https://doi.org/10.1109/TKDE.2022.3220789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are widely used to model various practical applications. In recent years, graph convolution networks (GCNs) have attracted increasing attention due to the extension of convolution operation from traditional grid data to graph one. However, the representation ability of current GCNs is undoubtedly limited because existing work fails to consider feature interactions. Toward this end, we propose a Dual Feature Interaction-based GCN. Specifically, it models feature interaction in the aspects of 1) node features where we use Newton&#39;s identity to extract different-order cross features implicit in the original features and design an attention mechanism to fuse them; and 2) graph convolution where we capture the pairwise interactions among nodes in the neighborhood to expand a weighted sum operation. We evaluate the proposed model with graph data from different fields, and the experimental results on semi-supervised node classification and link prediction demonstrate the effectiveness of the proposed GCN. The data and source codes of this work are available at https://github.com/ZZY-GraphMiningLab/DFI-GCN .},
  archive      = {J_TKDE},
  author       = {Zhongying Zhao and Zhan Yang and Chao Li and Qingtian Zeng and Weili Guan and MengChu Zhou},
  doi          = {10.1109/TKDE.2022.3220789},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {9019-9030},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual feature interaction-based graph convolutional network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Deep learning for approximate nearest neighbour search: A
survey and future directions. <em>TKDE</em>, <em>35</em>(9), 8997–9018.
(<a href="https://doi.org/10.1109/TKDE.2022.3220683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate nearest neighbour search (ANNS) in high-dimensional space is an essential and fundamental operation in many applications from many domains such as multimedia database, information retrieval and computer vision. With the rapidly growing volume of data and the dramatically increasing demands of users, traditional heuristic-based ANNS solutions have been facing great challenges in terms of both efficiency and accuracy. Inspired by the recent successes of deep learning in many fields, substantial efforts have been devoted to applying deep learning techniques to ANNS for learning to index and learning to search, resulting in numerous algorithms that achieve state-of-the-art performance compared with conventional methods. In this survey paper, we comprehensively review the different types of deep learning-based ANNS methods according to two learning paradigms: learning to index and learning to search . We provide a comprehensive overview and analysis of these methods in a systematic manner. Based on the overview, we point out that end-to-end learning will be a new and promising research direction for deep learning-based ANNS, i.e., applying deep learning techniques to jointly learn the indexing and searching together, such that the underlying knowledge learned from data can directly contribute to the final searching performance. Finally, we conduct experiments and provide general performance analyses for the representative deep learning-based ANNS algorithms.},
  archive      = {J_TKDE},
  author       = {Mingjie Li and Yuan-Gen Wang and Peng Zhang and Hanpin Wang and Lisheng Fan and Enxia Li and Wei Wang},
  doi          = {10.1109/TKDE.2022.3220683},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8997-9018},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep learning for approximate nearest neighbour search: A survey and future directions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive multi-modal knowledge graph representation
learning. <em>TKDE</em>, <em>35</em>(9), 8983–8996. (<a
href="https://doi.org/10.1109/TKDE.2022.3220625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning of knowledge graphs (KGs) aims to embed both entities and relations as vectors in a continuous low-dimensional space, which has facilitated various applications such as link prediction and entity retrieval. Most existing KG embedding methods focus on modeling the structured fact triples independently and ignore the multi-type relations among triples as well as the variety of data types (e.g., texts and images) associated with entities in KGs, and thus fail to capture the complex and multi-modal information that is inherently inside the entity-relation triples. In this paper, we propose a novel approach for knowledge graph embedding named Contrastive Multi-modal Graph Neural Network (CMGNN), which can encapsulate comprehensive features from multi-modal content descriptions of entities and high-order connectivity structures. Specifically, CMGNN first learns entity embeddings from multi-modal content and then contrasts encodings from multi-relational local neighbors and high-order connectivities to obtain latent representations of entities and relations simultaneously. Experimental results demonstrate that CMGNN can effectively model the multi-modalities and multi-type structures in KGs, and significantly outperforms existing state-of-the-art methods on benchmark datasets for the tasks of link prediction and entity classification.},
  archive      = {J_TKDE},
  author       = {Quan Fang and Xiaowei Zhang and Jun Hu and Xian Wu and Changsheng Xu},
  doi          = {10.1109/TKDE.2022.3220625},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8983-8996},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Contrastive multi-modal knowledge graph representation learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <span class="math inline">W<sup>2</sup></span>Parking: A
data-driven win-win contract parking sharing mechanism under both supply
and demand uncertainties. <em>TKDE</em>, <em>35</em>(9), 8968–8982. (<a
href="https://doi.org/10.1109/TKDE.2022.3220495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of the number of private vehicles, searching for accessible parking spaces becomes intractable for drivers, especially during high-demand hours. In recent years, we are witnessing a number of sharing economy services. Contract parking sharing, as an innovative sharing economy mode, has the potential to alleviate the difficult parking issue and make full use of the urban parking resources. However, the uncertainties of both drivers’ parking demand and owners’ sharing supply make it challenging to achieve efficient sharing. Thanks to IoT technology, many current parking lots now record vehicles’ fine-grained parking data for billing purposes. Leveraging these fine-grained parking data, we exploit available contract parking spaces to share them with drivers that have temporary parking demand. Specifically, we propose $\mathrm{W^{2}}$ Parking, a w in- w in contract parking sharing system, which includes two key components: (i) an idle time prediction model to estimate available periods of parking spaces and (ii) a parking sharing model to schedule temporary users to have access to these available parking spaces under both demand and supply uncertainties using dynamic programming combined with a 2-approximation algorithm with performance-bound guarantees. we evaluate our system on seven-month real-world parking data from 368 parking lots with 14,704 parking spaces. Extensive experimental results show that our $\mathrm{W^{2}}$ Parking achieves more than 90\% of accuracy in parking time prediction, and the utilization rate of contract parking spaces is improved by 35\%.},
  archive      = {J_TKDE},
  author       = {Shuai Wang and Xin Zhu and Guang Wang and Desheng Zhang and Lai Tu and Tian He},
  doi          = {10.1109/TKDE.2022.3220495},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8968-8982},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {$\mathrm{W}^{2}$Parking: A data-driven win-win contract parking sharing mechanism under both supply and demand uncertainties},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SecSkyline: Fast privacy-preserving skyline queries over
encrypted cloud databases. <em>TKDE</em>, <em>35</em>(9), 8955–8967. (<a
href="https://doi.org/10.1109/TKDE.2022.3220595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The well-known benefits of cloud computing have spurred the popularity of database service outsourcing, where one can resort to the cloud to conveniently store and query databases. Coming with such popular trend is the threat to data privacy, as the cloud gains access to the databases and queries which may contain sensitive information, like medical or financial data. A large body of work has been presented for querying encrypted databases, which has been mostly focused on secure keyword search. In this paper, we instead focus on the support for secure skyline query processing over encrypted outsourced databases, where little work has been done. Skyline query is an advanced kind of database query which is important for multi-criteria decision-making systems and applications. We propose SecSkyline, a new system framework building on lightweight cryptography for fast privacy-preserving skyline queries. SecSkyline ambitiously provides strong protection for not only the content confidentiality of the outsourced database, the query, and the result, but also for data patterns that may incur indirect data leakages, such as dominance relationships among data points and search access patterns. Extensive experiments demonstrate that SecSkyline is substantially superior to the state-of-the-art in query latency, with up to $813\times$ improvement.},
  archive      = {J_TKDE},
  author       = {Yifeng Zheng and Weibo Wang and Songlei Wang and Xiaohua Jia and Hejiao Huang and Cong Wang},
  doi          = {10.1109/TKDE.2022.3220595},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8955-8967},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SecSkyline: Fast privacy-preserving skyline queries over encrypted cloud databases},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on deep semi-supervised learning. <em>TKDE</em>,
<em>35</em>(9), 8934–8954. (<a
href="https://doi.org/10.1109/TKDE.2022.3220219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep semi-supervised learning is a fast-growing field with a range of practical applications. This paper provides a comprehensive survey on both fundamentals and recent advances in deep semi-supervised learning methods from perspectives of model design and unsupervised loss functions. We first present a taxonomy for deep semi-supervised learning that categorizes existing methods, including deep generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Then we provide a comprehensive review of 60 representative methods and offer a detailed comparison of these methods in terms of the type of losses, architecture differences, and test performance results. In addition to the progress in the past few years, we further discuss some shortcomings of existing methods and provide some tentative heuristic solutions for solving these open problems.},
  archive      = {J_TKDE},
  author       = {Xiangli Yang and Zixing Song and Irwin King and Zenglin Xu},
  doi          = {10.1109/TKDE.2022.3220219},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8934-8954},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on deep semi-supervised learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel core maintenance of dynamic graphs. <em>TKDE</em>,
<em>35</em>(9), 8919–8933. (<a
href="https://doi.org/10.1109/TKDE.2022.3219096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A $k$ -core is the special cohesive subgraph where each vertex has at least $k$ degree. It is widely used in graph mining applications such as community detection, visualization, and clique discovery. Because dynamic graphs frequently evolve, obtaining their $k$ -cores via decomposition is inefficient. Instead, previous studies proposed various methods for updating $k$ -cores based on inserted (removed) edges. Unfortunately, the parallelism of existing approaches is limited due to their theoretical constraints. To further improve the parallelism of maintenance algorithms, we refine the $k$ -core maintenance theorem and propose two effective parallel methods to update $k$ -cores for insertion and removal cases. Experimental results show that our methods outperform the state-of-the-art algorithms on real-world graphs by one order of magnitude.},
  archive      = {J_TKDE},
  author       = {Wen Bai and Yuncheng Jiang and Yong Tang and Yayang Li},
  doi          = {10.1109/TKDE.2022.3219096},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8919-8933},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Parallel core maintenance of dynamic graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Find another me across the world - large-scale semantic
trajectory analysis using spark. <em>TKDE</em>, <em>35</em>(9),
8905–8918. (<a href="https://doi.org/10.1109/TKDE.2022.3218930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today&#39;s society, location-based services are widely used which collect a huge amount of human trajectories. Analyzing semantic meanings of these trajectories can benefit numerous real-world applications, such as product advertisement, friend recommendation, and social behavior analysis. However, existing works on semantic trajectories are mostly centralized approaches that are not able to keep up with the rapidly growing trajectory collections. In this paper, we propose a novel large-scale semantic trajectory analysis algorithm in Apache Spark. We design a new hash function along with efficient distributed algorithms that can quickly compute semantic trajectory similarities and identify communities of people with similar behavior across the world. The experimental results show that our approach is more than 30 times faster than centralized approaches without sacrificing any accuracy like other parallel approaches.},
  archive      = {J_TKDE},
  author       = {Chaoquan Cai and Dan Lin},
  doi          = {10.1109/TKDE.2022.3218930},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8905-8918},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Find another me across the world - large-scale semantic trajectory analysis using spark},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pre-trained semantic embeddings for POI categories based on
multiple contexts. <em>TKDE</em>, <em>35</em>(9), 8893–8904. (<a
href="https://doi.org/10.1109/TKDE.2022.3218851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decade has witnessed the increasingly created point-of-interest (POI) data, which are utilized to express the semantics of places. To understand the POI semantics, current methods usually embed POI categories into a latent space via certain trajectory sequential models, while neglecting the underlining spatial information. It is noteworthy that the complex spatial relationships among POI categories contain substantial information that benefits meaningful semantic embeddings for various POI categories. Inspired by this, we present a unified POI Category Embedding Method (CatEM for short), which jointly encodes the sequential transitions and spatial relations of POI categories as well as the adaptive semantic neighbors of each POI category. The merits of CatEM lie in that: (1) it considers the pairwise spatial similarities between categories and represents categories with larger similarity values with adjacent embeddings in the latent space, and (2) it adaptively locates neighbor categories with similar semantics in the embedding space to improve the adaptivity of POI category embedding. The proposed pre-trained POI category embeddings are justified by three downstream tasks. Extensive experiments demonstrate the superiority of our proposed model, as compared to several cutting-edge baselines.},
  archive      = {J_TKDE},
  author       = {Junxiang Bing and Meng Chen and Min Yang and Weiming Huang and Yongshun Gong and Liqiang Nie},
  doi          = {10.1109/TKDE.2022.3218851},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8893-8904},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Pre-trained semantic embeddings for POI categories based on multiple contexts},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). OERL: Enhanced representation learning via open knowledge
graphs. <em>TKDE</em>, <em>35</em>(9), 8880–8892. (<a
href="https://doi.org/10.1109/TKDE.2022.3218850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparseness and incompleteness of knowledge graphs (KGs) trigger considerable interest in enhancing the representation learning with external corpora. However, the difficulty of aligning entities and relations with external corpora leads to inferior performance improvement. Open knowledge graphs (OKGs) consist of entity-mentions and relation-mentions that are represented by noncanonicalized freeform phrases, which generally do not rely on the specification of ontology schema. The roughness of the nonontological construction method leads to a specific characteristic of OKGs: diversity, where multiple entity-mentions (or relation-mentions) have the same meaning but different expressions. The diversity of OKGs can provide potential textual and structural features for the representation learning of KGs. We speculate that leveraging OKGs to enhance the representation learning of KGs can be more effective than using pure text or pure structure corpora. In this paper, we propose a new OERL , O pen knowledge graph E nhanced R epresentation L earning of KGs. OERL automatically extracts textual and structural connections between KGs and OKGs, models and transfers refined profitable features to enhance the representation learning of KGs. The strong performance improvement and exhaustive experimental analysis prove the superiority of OERL over state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Qian Li and Daling Wang and Shi Feng and Kaisong Song and Yifei Zhang and Ge Yu},
  doi          = {10.1109/TKDE.2022.3218850},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8880-8892},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {OERL: Enhanced representation learning via open knowledge graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implicit event argument extraction with argument-argument
relational knowledge. <em>TKDE</em>, <em>35</em>(9), 8865–8879. (<a
href="https://doi.org/10.1109/TKDE.2022.3218830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a challenging sub-task of event argument extraction, implicit event argument extraction seeks to identify document-level arguments that play direct or implicit roles in a given event. Prior work mainly focuses on capturing direct relations between arguments and the event trigger; however, the lack of reasoning ability imposes limitations to the extraction of implicit arguments. In this work, we propose an A rgument-argument R elation-enhanced E vent A rgument extraction (AREA) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues, and utilizes such argument-argument dependencies to guide the reasoning process. To bridge the distribution gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we introduce a conventional knowledge distillation strategy to drive a final model that can work without extra inputs by mimicking the behaviour of a well-informed teacher model. In addition, considering that conventional knowledge distillation methods transfer knowledge individually, we integrate it with a novel relational knowledge distillation mechanism to explicitly capture the structural mutual argument-argument relation. Moreover, since the training process is not compatible with the real situation, a curriculum learning method is further introduced to make the training process smoother. Experimental results demonstrate that the learning framework obtains state-of-the-art performance on the RAMS and Wikievents datasets. Ablation study and further discussion also show it could handle long-range dependency and implicit argument problems effectively.},
  archive      = {J_TKDE},
  author       = {Kaiwen Wei and Xian Sun and Zequn Zhang and Li Jin and Jingyuan Zhang and Jianwei Lv and Zhi Guo},
  doi          = {10.1109/TKDE.2022.3218830},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8865-8879},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Implicit event argument extraction with argument-argument relational knowledge},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A diversified attention model for interpretable multiple
clusterings. <em>TKDE</em>, <em>35</em>(9), 8852–8864. (<a
href="https://doi.org/10.1109/TKDE.2022.3218693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple clusterings can explore the same set of data from different perspectives by discovering different and meaningful clusterings. However, most, if not all, of the existing approaches overwhelmingly focus on the diversity between clustering subspaces, and pay much less attention on the salience of the subspaces. As a consequence, the quality of the produced clusterings is an understudied aspect of the problem. Furthermore, existing methods cannot explain the unique internal subspace structure of each clustering, and cannot incorporate multi-facet knowledge to generate different clusterings. In this paper, we propose a solution named iMClusts ( i nterpretable M ultiple Clust ering s by diversified attention). iMClusts makes use of the expressive representational power of deep autoencoders and multi-head attention to generate multiple salient embedding matrices, and multiple clusterings therein. In addition, it leverages multi-facet knowledge and enhances the diversity between heads to boost the quality and diversity of multiple clusterings. Experimental results on benchmark datasets show that iMClusts can generate multiple clusterings with quality, interpretability, and diversity.},
  archive      = {J_TKDE},
  author       = {Liangrui Ren and Guoxian Yu and Jun Wang and Lei Liu and Carlotta Domeniconi and Xiangliang Zhang},
  doi          = {10.1109/TKDE.2022.3218693},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8852-8864},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A diversified attention model for interpretable multiple clusterings},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Work together: Correlation-identity reconstruction hashing
for unsupervised cross-modal retrieval. <em>TKDE</em>, <em>35</em>(9),
8838–8851. (<a href="https://doi.org/10.1109/TKDE.2022.3218656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised cross-modal hashing has attracted considerable attention to support large-scale cross-modal retrieval. Although promising progresses have been made so far, existing methods still suffer from limited capability on excavating and preserving the intrinsic multi-modal semantics. In this paper, we propose a Correlation-Identity Reconstruction Hashing (CIRH) method to alleviate this challenging problem. We develop a new unsupervised deep cross-modal hash learning framework to model and preserve the heterogeneous multi-modal correlation semantics into both hash codes and functions, and simultaneously, we involve both the hash codes and functions with the descriptive identity semantics. Specifically, we construct a multi-modal collaborated graph to model the heterogeneous multi-modal correlations, and jointly perform the intra-modal and cross-modal semantic aggregation on homogeneous and heterogeneous graph networks to generate a multi-modal complementary representation with correlation reconstruction. Furthermore, an identity semantic reconstruction process is designed to involve the generated representation with identity semantics by reconstructing the input modality representations. Finally, we propose a correlation-identity consistent hash function learning strategy to transfer the modelled multi-modal semantics into the neural networks of modality-specific deep hash functions. Experiments demonstrate the superior performance of the proposed method on both retrieval accuracy and efficiency. We provide our source codes and experimental datasets at https://github.com/XizeWu/CIRH},
  archive      = {J_TKDE},
  author       = {Lei Zhu and Xize Wu and Jingjing Li and Zheng Zhang and Weili Guan and Heng Tao Shen},
  doi          = {10.1109/TKDE.2022.3218656},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8838-8851},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Work together: Correlation-identity reconstruction hashing for unsupervised cross-modal retrieval},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local search for efficient causal effect estimation.
<em>TKDE</em>, <em>35</em>(9), 8823–8837. (<a
href="https://doi.org/10.1109/TKDE.2022.3218131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal effect estimation from observational data is a challenging problem, especially with high dimensional data and in the presence of unobserved variables. The available data-driven methods for tackling the problem either provide an estimation of the bounds of a causal effect (i.e., nonunique estimation) or have low efficiency. The major hurdle for achieving high efficiency while trying to obtain unique and unbiased causal effect estimation is how to find a proper adjustment set for confounding control in a fast way, given the huge covariate space and considering unobserved variables. In this paper, we approach the problem as a local search task for finding valid adjustment sets in data. We establish the theorems to support the local search for adjustment sets, and we show that unique and unbiased estimation can be achieved from observational data even when there exist unobserved variables. We then propose a data-driven algorithm that is fast and consistent under mild assumptions. We also make use of a frequent pattern mining method to further speed up the search of minimal adjustment sets for causal effect estimation. Experiments conducted on extensive synthetic and real-world datasets demonstrate that the proposed algorithm outperforms the state-of-the-art criteria/estimators in both accuracy and time-efficiency.},
  archive      = {J_TKDE},
  author       = {Debo Cheng and Jiuyong Li and Lin Liu and Jiji Zhang and Jixue Liu and Thuc Duy Le},
  doi          = {10.1109/TKDE.2022.3218131},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8823-8837},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Local search for efficient causal effect estimation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sylvester equation induced collaborative representation
learning for recommendation. <em>TKDE</em>, <em>35</em>(9), 8811–8822.
(<a href="https://doi.org/10.1109/TKDE.2022.3217090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an actual recommendation system, it generally involves a variety of heterogeneous interactive relationships, such as the typical user-user (U2U), item-item (I2I), and user-item (U2I) interaction relationships. With the application of graph neural networks (GNNs) in embedding various interactive relations, recommendation technology has made gratifying progress in recent years, which benefits lot from its powerful ability in relation modeling. However, most of the existing GNN-based methods fail to collaboratively explore the above heterogeneous multiple interactive relationships, including the internal correlations among multiple relationships and the intrinsic association behind different relationships. As a consequence, the user&#39;s personalized preference for the items to be recommended will not be well captured. In this paper, we propose a S ylvester equation induced C ollaborative R epresentation L earning framework (S-CRL) for recommendation system by utilizing the heterogeneous multiple interactive relationships. In particular, we ingeniously define a novel Sylvester equation to associate tactfully the multiple heterogeneous relations together. From the perspective of rating propagation, such Sylvester equation is shown theoretically to be the optimal solution of a local structure sensitive rating propagation function. Additionally, to seek more expressive embeddings about user and item, a layer-wise attention is introduced to aggregate the multi-hop information from U2U and I2I graphs, respectively, so as to promote the aggregation with the corresponding embeddings from the U2I interaction graph. Extensive experiments on three real-world datasets verify that our model achieves more favorable performance over currently representative methods.},
  archive      = {J_TKDE},
  author       = {Xingyuan Li and Zhenfeng Zhu and Shuai Zheng and Zhizhe Liu and Youru Li and Xiaobo Guo and Deqiang Kong and Yao Zhao},
  doi          = {10.1109/TKDE.2022.3217090},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8811-8822},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Sylvester equation induced collaborative representation learning for recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concept-level model interpretation from the causal aspect.
<em>TKDE</em>, <em>35</em>(9), 8799–8810. (<a
href="https://doi.org/10.1109/TKDE.2022.3209997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing growth of data and the ability of learning with them, machine learning models are adopted in various domains. However, few of machine learning models are able to reason their prediction, which limits their further applications in real-world tasks. With the potential to address this dilemma, model interpretation has become an important research topic because of the ability to provide the underlying reasons for model predictions at the feature level or concept level. Model interpretation at the concept level focuses on exploring the roles of concepts in model prediction, which enables more compact and understandable interpretations. Concept-level model interpretation requires the identification of the concepts that contribute to model prediction and the exploration of the rules underneath these concepts. To achieve the two objectives, we propose a Concept-level Model Interpretation framework (CMIC) from the perspective of causality. CMIC can automatically detect concepts in data and discover the causal relation between the detected concepts and the model&#39;s predicted labels. Furthermore, CMIC ranks the contributions of concepts by their causal effect on the model prediction, reflecting the detected concepts’ importance. We evaluate the proposed CMIC framework on both synthetic and real-world datasets to demonstrate the quality of the provided interpretation.},
  archive      = {J_TKDE},
  author       = {Liuyi Yao and Yaliang Li and Sheng Li and Jinduo Liu and Mengdi Huai and Aidong Zhang and Jing Gao},
  doi          = {10.1109/TKDE.2022.3209997},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8799-8810},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Concept-level model interpretation from the causal aspect},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). H-diffu: Hyperbolic representations for information
diffusion prediction. <em>TKDE</em>, <em>35</em>(9), 8784–8798. (<a
href="https://doi.org/10.1109/TKDE.2022.3209067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of online social networks, a great deal of online user action data has been generated. Such data has enabled the study of information diffusion prediction, which is a fundamental problem for understanding the propagation of information on social media platforms. In diffusion prediction models, there are two standard components, i.e., a social graph and information diffusion cascades. We observe that both components exhibit latent hierarchical structures. However, most existing models are designed based on euclidean spaces, and hence cannot effectively capture complex patterns, especially hierarchical structures. Therefore, we investigate a novel research problem to learn hyperbolic representations for information diffusion prediction. To reflect the different characteristics of social graphs and diffusion cascades, we encode them into two latent hyperbolic spaces with different trainable curvatures. In addition, to model influence dependencies, we propose a co-attention mechanism to capture the processes of diffusion cascades using positional embeddings. Given a set of activated seed users, we jointly exploit diffusion cascades and social links to predict which users will be influenced. We conduct extensive experiments on four real-world datasets. Empirical results demonstrate that the proposed H-Diffu model significantly outperforms several state-of-the-art diffusion prediction frameworks.},
  archive      = {J_TKDE},
  author       = {Shanshan Feng and Kaiqi Zhao and Lanting Fang and Kaiyu Feng and Wei Wei and Xutao Li and Ling Shao},
  doi          = {10.1109/TKDE.2022.3209067},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8784-8798},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {H-diffu: Hyperbolic representations for information diffusion prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistically significant pattern mining with ordinal
utility. <em>TKDE</em>, <em>35</em>(9), 8770–8783. (<a
href="https://doi.org/10.1109/TKDE.2022.3208626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistically significant pattern mining (SSPM), which evaluates each pattern via a hypothesis test, is an essential and challenging data mining task for knowledge discovery. We introduce a preference relation between patterns and aim to discover the most preferred patterns under the constraint of statistical significance, which has never been considered in existing SSPM problems. We propose an iterative multiple testing procedure that can alternately reject a hypothesis and safely ignore the less useful hypotheses than the rejected one. By filtering out patterns with low utility, we can avoid the significance budget consumption of rejecting useless (uninteresting) patterns and focus the significance budget on more useful patterns, leading to more useful discoveries. We show that the proposed method can control the familywise error rate (FWER) under certain assumptions, which can be satisfied by a realistic problem class in SSPM. We also show that the proposed method always discovers equally or more useful patterns than Tarone-Bonferroni and Subfamily-wise Multiple Testing (SMT). Finally, we conducted several experiments with both synthetic and real-world data to evaluate the performance of our method. The proposed method discovered many more useful patterns in the experiments with real-world datasets than the existing method for all five conducted tasks.},
  archive      = {J_TKDE},
  author       = {Thien Q. Tran and Kazuto Fukuchi and Youhei Akimoto and Jun Sakuma},
  doi          = {10.1109/TKDE.2022.3208626},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8770-8783},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Statistically significant pattern mining with ordinal utility},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noun compound interpretation with relation classification
and paraphrasing. <em>TKDE</em>, <em>35</em>(9), 8757–8769. (<a
href="https://doi.org/10.1109/TKDE.2022.3208617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noun compounds are abundant in various languages and their interpretations have been applied in a wide range of NLP tasks. However, most existing work only uses relation classification- or paraphrasing-based methods to model this problem, failing in coverage or accuracy. We argue that the above two approaches are complementary to each other for the noun compound interpretation. In this paper, we propose a two-phase strategy to solve this task. The first phase is to perform the relation classification sub-task with a novel multi-view representation learning model. When noun compounds are predicted as the non-semantic relation, i.e., NA, or the confidence scores are below the threshold, the second phase, namely paraphrasing, will be triggered to interpret noun compounds with a contrastive slot filling method. To evaluate the effectiveness of our methods, we construct the largest Chinese dataset for noun compound interpretation in the life service domain. The experimental results on our constructed and public datasets prove the effectiveness of our solution. Furthermore, the online A/B testing on Meituan APP suggests that the Query View Click-Through Rate increases by 0.91\% when noun compounds are used to enrich semantic information of items with the help of their interpretations on the platform.},
  archive      = {J_TKDE},
  author       = {Jingping Liu and Juntao Liu and Lihan Chen and Jiaqing Liang and Yanghua Xiao and Huimin Xu and Fubao Zhang and Zongyu Wang and Rui Xie},
  doi          = {10.1109/TKDE.2022.3208617},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8757-8769},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Noun compound interpretation with relation classification and paraphrasing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HSDN: A high-order structural semantic disentangled neural
network. <em>TKDE</em>, <em>35</em>(9), 8742–8756. (<a
href="https://doi.org/10.1109/TKDE.2022.3208604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph disentangling is a new promising direction that can help us to discover the latent patterns in the data and understand the behaviors of a graph learning model. Despite the many efforts in disentangling representation learning, few works focus on disentangling the latent factors behind a graph. Most current foci are mainly on studying node-level semantics in the graphs. Compared with node-level, the structure-level view can provide a new interpretable and in-depth insight into graph data. The study of structure-level relations enables us to reveal the high-order structural semantics in the data. To explore the complex high-order structural semantics in the data, we propose the High-order Structural Semantic Disentangled Neural Network (HSDN) to model the graph structure units and disentangle structural semantics. It&#39;s the first attempt to hypergraph disentangled networks. Unlike prior methods that disentangle factor graphs based on pair-wise relations only, we introduce hyperedges on pair-wise graphs to model structure units and disentangle the complex high-order structural semantics between different structures. Extensive experiments demonstrate that HSDN achieves state-of-the-art performances in terms of both disentangling and downstream tasks.},
  archive      = {J_TKDE},
  author       = {Bingde Hu and Xingen Wang and Zunlei Feng and Jie Song and Ji Zhao and Mingli Song and Xinyu Wang},
  doi          = {10.1109/TKDE.2022.3208604},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8742-8756},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HSDN: A high-order structural semantic disentangled neural network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023f). Model inversion attacks against graph neural networks.
<em>TKDE</em>, <em>35</em>(9), 8729–8741. (<a
href="https://doi.org/10.1109/TKDE.2022.3207915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data mining tasks rely on graphs to model relational structures among individuals (nodes). Since relational data are often sensitive, there is an urgent need to evaluate the privacy risks in graph data. One famous privacy attack against data analysis models is the model inversion attack, which aims to infer sensitive data in the training dataset and leads to great privacy concerns. Despite its success in grid-like domains, directly applying model inversion attacks on non-grid domains such as graph leads to poor attack performance. This is mainly due to the failure to consider the unique properties of graphs. To bridge this gap, we conduct a systematic study on model inversion attacks against Graph Neural Networks (GNNs), one of the state-of-the-art graph analysis tools in this paper. First, in the white-box setting where the attacker has full access to the target GNN model, we present GraphMI to infer the private training graph data. Specifically in GraphMI, a projected gradient module is proposed to tackle the discreteness of graph edges and preserve the sparsity and smoothness of graph features; a graph auto-encoder module is used to efficiently exploit graph topology, node attributes, and target model parameters for edge inference; a random sampling module can finally sample discrete edges. Furthermore, in the hard-label black-box setting where the attacker can only query the GNN API and receive the classification results, we propose two methods based on gradient estimation and reinforcement learning (RL-GraphMI). With the proposed methods, we study the connection between model inversion risk and edge influence and show that edges with greater influence are more likely to be recovered. Extensive experiments over several public datasets demonstrate the effectiveness of our methods. We also evaluate our attacks under two defenses: one is the well-designed differential private training, and the other is graph preprocessing. Our experimental results show that such defenses are not sufficiently effective and call for more advanced defenses against privacy attacks.},
  archive      = {J_TKDE},
  author       = {Zaixi Zhang and Qi Liu and Zhenya Huang and Hao Wang and Chee-Kong Lee and Enhong Chen},
  doi          = {10.1109/TKDE.2022.3207915},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8729-8741},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Model inversion attacks against graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HmcNet: A general approach for hierarchical multi-label
classification. <em>TKDE</em>, <em>35</em>(9), 8713–8728. (<a
href="https://doi.org/10.1109/TKDE.2022.3207511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical multi-label classification (HMC) deals with the problem of assigning each entity to multiple classes with a taxonomic structure (e.g., tree). Within this structure, classes at different levels tend to have dependencies under the hierarchy constraints. However, most prior studies for HMC tasks tend to ignore the class dependencies within the hierarchy. Moreover, most existing methods generate incoherent predictions and do not satisfy the hierarchy constraint. To this end, based on previously developed HARNN, we propose a general framework, HmcNet, for introducing explicit and implicit class hierarchy constraints to generate coherent predictions. We develop an efficient Prune-based Coherent Prediction (PCP) strategy for the optimal paths selection, which produces coherent predictions in a principled way. HmcNet can be well explained from two perspectives. First, it develops the Hierarchical Attention-based Memory (HAM) unit with implicit class hierarchy constraints to capture class dependencies more intuitively; Second, it subsumes explicit class hierarchy constraints during training and inference phases and generates coherent predictions in a consistent manner. Finally, extensive experimental results on six real-world datasets demonstrate the effectiveness and interpretability of the HmcNet frameworks. To facilitate future research, our code has been made publicly available.},
  archive      = {J_TKDE},
  author       = {Wei Huang and Enhong Chen and Qi Liu and Hui Xiong and Zhenya Huang and Shiwei Tong and Dan Zhang},
  doi          = {10.1109/TKDE.2022.3207511},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8713-8728},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HmcNet: A general approach for hierarchical multi-label classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Least-mean-squares coresets for infinite streams.
<em>TKDE</em>, <em>35</em>(9), 8699–8712. (<a
href="https://doi.org/10.1109/TKDE.2022.3180808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a stream of $d$ -dimensional rows (points in $\mathbb {R}^{d}$ ) arriving sequentially. An $\epsilon$ -coreset is a positively weighted subset that approximates their sum of squared distances to any linear subspace of $\mathbb {R}^{d}$ , up to a $1 \pm \epsilon$ factor. Unlike other data summarizations, such a coreset: (1) can be used to minimize faster any optimization function that uses this sum, such as regularized or constrained regression, (2) preserves input sparsity; (3) easily interpretable; (4) avoids numerical errors; (5) applies to problems with constraints on the input, such as subspaces that are spanned by few input points. Our main result is the first algorithm that returns such an $\epsilon$ -coreset using finite and constant memory during the streaming, i.e., independent of $n$ , the number of rows seen so far. The coreset consists of $O(d \log ^{2}\;d / \epsilon ^{2})$ weighted rows, which is nearly optimal according to existing lower bounds of $\Omega (d / \epsilon ^{2})$ . We support our findings with experiments on the Wikipedia dataset benchmarked against state-of-the-art algorithms.},
  archive      = {J_TKDE},
  author       = {Vladimir Braverman and Dan Feldman and Harry Lang and Daniela Rus and Adiel Statman},
  doi          = {10.1109/TKDE.2022.3180808},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {9},
  pages        = {8699-8712},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Least-mean-squares coresets for infinite streams},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zorro: Valid, sparse, and stable explanations in graph
neural networks. <em>TKDE</em>, <em>35</em>(8), 8687–8698. (<a
href="https://doi.org/10.1109/TKDE.2022.3201170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ever-increasing popularity and applications of graph neural networks, several proposals have been made to explain and understand the decisions of a graph neural network. Explanations for graph neural networks differ in principle from other input settings. It is important to attribute the decision to input features and other related instances connected by the graph structure. We find that the previous explanation generation approaches that maximize the mutual information between the label distribution produced by the model and the explanation to be restrictive. Specifically, existing approaches do not enforce explanations to be valid, sparse, or robust to input perturbations. In this paper, we lay down some of the fundamental principles that an explanation method for graph neural networks should follow and introduce a metric RDT-Fidelity as a measure of the explanation&#39;s effectiveness. We propose a novel approach Zorro based on the principles from rate-distortion theory that uses a simple combinatorial procedure to optimize for RDT-Fidelity. Extensive experiments on real and synthetic datasets reveal that Zorro produces sparser, stable, and more faithful explanations than existing graph neural network explanation approaches.},
  archive      = {J_TKDE},
  author       = {Thorben Funke and Megha Khosla and Mandeep Rathee and Avishek Anand},
  doi          = {10.1109/TKDE.2022.3201170},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8687-8698},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Zorro: Valid, sparse, and stable explanations in graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Willingness maximization for ego network data extraction in
multiple online social networks. <em>TKDE</em>, <em>35</em>(8),
8672–8686. (<a href="https://doi.org/10.1109/TKDE.2022.3207150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Egocentric network (ego network) data are very important for evaluating algorithms and machine learning approaches in Online Social Networks (OSNs). Nevertheless, obtaining the ego network data from OSNs is not a trivial task. Conventional manual approaches are time-consuming, and sometimes the ego network data are quite incomplete because only a small number of users would agree to provide their data. This is because there are two important factors that should be considered simultaneously for this data acquisition task: i) users’ willingness to provide their data, and ii) the structure of the ego network. However, addressing the above two factors to obtain the more complete ego network data has not received much research attention. Therefore, in this paper, we address this issue by proposing a family of new research problems. The first proposed problem, named Willingness Maximization for Ego Network Extraction in Online Social Networks (WMEgo) , identifies a set of ego networks from a single OSN, such that the willingness of the users to provide their data is maximized. We prove that WMEgo is NP-hard and propose a $\frac{1}{2}(1-\frac{1}{e})$ -approximation algorithm, named Ego Network Identification with Maximum Willingness (EIMW) . Furthermore, we extend the idea of WMEgo to multiple social networks and formulate a new research problem, named Willingness Maximization on Multiple Social Networks for Ego Network Extraction (WM$^{2}$2Ego) , which is able to effectively obtain ego network data from multiple social networks simultaneously . We propose a $\frac{1}{2}$ -approximation algorithm, named Maximum Expansion for UNified EXpenses (MUNEX) for a special case of WM $^{2}$ Ego and then design a constant-ratio approximation algorithm to the general WM $^{2}$ Ego problem, named Maximum Expansion with Expense Examination (M3E) . We conduct two evaluation studies with 672 and 1,052 volunteers to validate the proposed WMEgo and WM $^{2}$ Ego problems, respectively, and show that they are able to obtain much more complete ego network data compared to other baselines. We also perform extensive experiments on multiple real datasets to demonstrate that the proposed approaches significantly outperform the other baselines.},
  archive      = {J_TKDE},
  author       = {Bay-Yuan Hsu and Lo-Yao Yeh and Ming-Yi Chang and Chih-Ya Shen},
  doi          = {10.1109/TKDE.2022.3207150},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8672-8686},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Willingness maximization for ego network data extraction in multiple online social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trust-aware multi-task knowledge graph for recommendation.
<em>TKDE</em>, <em>35</em>(8), 8658–8671. (<a
href="https://doi.org/10.1109/TKDE.2022.3221160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sparsity and cold start problems are common in recommender systems. Adding some side information, such as knowledge graph and users’ trust relationship, is an effective method to alleviate these problems. However, few work jointly explore the fine-grained implicit relationships between the external heterogeneous graphs to enhance the recommendation accuracy. To address this issue, in this paper, we propose a new method named Trust-aware Multi-task Knowledge Graph (TMKG), which uses multi-task learning to integrate two kinds of side information of trust graph and knowledge graph in an end-to-end manner. First, we mine the intra-graph and inter-graph high-order connections through the node propagation and aggregation, and optimize the embedding of nodes through the implicit relationships obtained. Furthermore, through the shared cross unit, the connection relationships between each layer is mined, and the high-order interaction of nodes of different layers is obtained. We conduct extensive experiments on real-world datasets and prove that our model has the superior performance compared with the state-of-the-art models.},
  archive      = {J_TKDE},
  author       = {Yan Zhou and Jie Guo and Bin Song and Chen Chen and Jianglong Chang and Fei Richard Yu},
  doi          = {10.1109/TKDE.2022.3221160},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8658-8671},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Trust-aware multi-task knowledge graph for recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tradeoff options for bipartite graph partitioning.
<em>TKDE</em>, <em>35</em>(8), 8644–8657. (<a
href="https://doi.org/10.1109/TKDE.2022.3208902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web connectivity graphs and similar linked data such as inverted indexes are important components of the information access systems provided by social media and web search services. The Bipartite Graph Partitioning mechanism of Dhulipala et al. [KDD 2016] relabels the vertices of large sparse graphs, seeking to enhance compressibility and thus reduce the storage space occupied by these costly structures. Here we develop a range of algorithmic and heuristic refinements to Bipartite Graph Partitioning ( ${{\sf BP}}$ ) that lead to faster computation of space-reducing vertex orderings whilst continuing to apply the same broad algorithmic paradigm. Using a range of web graph and information retrieval system index data as test cases, we demonstrate an implementation that executes up to approximately four times faster than the baseline implementation we commenced with, while holding compressibility approximately constant. We have also improved the asymptotic execution time of ${{\sf BP}}$ by replacing a sorting step by a customized median-finding step.},
  archive      = {J_TKDE},
  author       = {Joel Mackenzie and Matthias Petri and Alistair Moffat},
  doi          = {10.1109/TKDE.2022.3208902},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8644-8657},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Tradeoff options for bipartite graph partitioning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). TENSILE: A tensor granularity dynamic GPU memory scheduling
method toward multiple dynamic workloads system. <em>TKDE</em>,
<em>35</em>(8), 8630–8643. (<a
href="https://doi.org/10.1109/TKDE.2022.3221426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning has been an area of intense research. However, as a kind of computing-intensive task, deep learning highly relies on the scale of GPU memory, which is usually prohibitive and scarce. Although some extensive works have been proposed for dynamic GPU memory management, they are hard to apply to systems with multiple dynamic workloads, such as in-database machine learning systems. In this paper, we demonstrated TENSILE, a method of managing GPU memory in tensor granularity to reduce the GPU memory peak, considering the multiple dynamic workloads. TENSILE tackled the cold-starting and across-iteration scheduling problem existing in previous works. We implemented TENSILE on a deep learning framework built by ourselves and evaluated its performance. The experiment results show that TENSILE can save more GPU memory with less extra overhead than prior works in single and multiple dynamic workloads scenarios.},
  archive      = {J_TKDE},
  author       = {Kaixin Zhang and Hongzhi Wang and Han Hu and Songling Zou and Jiye Qiu and Tongxin Li and Zhishun Wang},
  doi          = {10.1109/TKDE.2022.3221426},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8630-8643},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TENSILE: A tensor granularity dynamic GPU memory scheduling method toward multiple dynamic workloads system},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task variance regularized multi-task learning.
<em>TKDE</em>, <em>35</em>(8), 8615–8629. (<a
href="https://doi.org/10.1109/TKDE.2022.3207049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task Learning (MTL), which involves the simultaneous learning of multiple tasks, can achieve better performance than learning each task independently. It has achieved great success in various applications, ranging from Computer Vision (CV) to Natural Language Processing (NLP). In MTL, the losses of the including tasks are jointly optimized. However, it is common for these tasks to be competing. When the tasks are competing, minimizing the losses of some tasks increases the losses of others, which accordingly increases the task variance (variance between the task-specific loss); furthermore, it induces under-fitting in some tasks and over-fitting in others, which degenerates the generalization performance of an MTL model. To address this issue, it is necessary to control the task variance; thus, task variance regularization is a natural choice. While intuitive, task variance regularization remains unexplored in MTL. Accordingly, to fill this gap, we study the generalization error bound of MTL through the lens of task variance and propose the task variance matters the generalization performance of MTL. Furthermore, this paper investigates how the task variance might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit. The proposed method, dubbed BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting BanditMTL both in CV and NLP applications is found to achieve state-of-the-art performance. The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals.},
  archive      = {J_TKDE},
  author       = {Yuren Mao and Zekai Wang and Weiwei Liu and Xuemin Lin and Wenbin Hu},
  doi          = {10.1109/TKDE.2022.3207049},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8615-8629},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Task variance regularized multi-task learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). System log parsing: A survey. <em>TKDE</em>, <em>35</em>(8),
8596–8614. (<a href="https://doi.org/10.1109/TKDE.2022.3222417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern information and communication systems have become increasingly challenging to manage. The ubiquitous system logs contain plentiful information and are thus widely exploited as an alternative source for system management. As log files usually encompass large amounts of raw data, manually analyzing them is laborious and error-prone. Consequently, many research endeavors have been devoted to automatic log analysis. However, these works typically expect structured input and struggle with the heterogeneous nature of raw system logs. Log parsing closes this gap by converting the unstructured system logs to structured records. Many parsers were proposed during the last decades to accommodate various log analysis applications. However, due to the ample solution space and lack of systematic evaluation, it is not easy for practitioners to find ready-made solutions that fit their needs. This paper aims to provide a comprehensive survey on log parsing. We begin with an exhaustive taxonomy of existing log parsers. Then we empirically analyze the critical performance and operational features for 17 open-source solutions both quantitatively and qualitatively, and whenever applicable discuss the merits of alternative approaches. We also elaborate on future challenges and discuss the relevant research directions. We envision this survey as a helpful resource for system administrators and domain experts to choose the most desirable open-source solution or implement new ones based on application-specific requirements.},
  archive      = {J_TKDE},
  author       = {Tianzhu Zhang and Han Qiu and Gabriele Castellano and Myriana Rifai and Chung Shue Chen and Fabio Pianese},
  doi          = {10.1109/TKDE.2022.3222417},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8596-8614},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {System log parsing: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structured sparse non-negative matrix factorization with
<span class="math inline"><em>ℓ</em><sub>2, 0</sub></span>ℓ2,0-norm.
<em>TKDE</em>, <em>35</em>(8), 8584–8595. (<a
href="https://doi.org/10.1109/TKDE.2022.3206881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative matrix factorization (NMF) is a powerful tool for dimensionality reduction and clustering. However, the interpretation of the clustering result from NMF is difficult, especially for the high-dimensional biological data without effective feature selection. To address this problem, we introduce a row-sparse NMF with $\ell _{2,0}$ -norm constraint (NMF_ $\ell _{20}$ ), where the basis matrix $\boldsymbol{W}$ is constrained by using the $\ell _{2,0}$ -norm constraint such that $\boldsymbol{W}$ has a row-sparsity pattern with feature selection. However, it is a challenge to solve the model, because the $\ell _{2,0}$ -norm constraint is a non-convex and non-smooth function. Fortunately, we prove that the $\ell _{2,0}$ -norm constraint satisfies the Kurdyka-Łojasiewicz property. Based on this finding, we present a proximal alternating linearized minimization algorithm and its monotone accelerated version to solve the NMF_ $\ell _{20}$ model. In addition, we further present a orthogonal NMF with $\ell _{2,0}$ -norm constraint (ONMF_ $\ell _{20}$ ) to enhance the clustering performance by using a non-negative orthogonal constraint. The ONMF_ $\ell _{20}$ model is solved by transforming into a series of constrained and penalized matrix factorization problems. The convergence and guarantees for these proposed algorithms are proved and the computational complexity is well evaluated. The results on numerical and scRNA-seq datasets demonstrate the efficiency of our methods in comparison with existing methods.},
  archive      = {J_TKDE},
  author       = {Wenwen Min and Taosheng Xu and Xiang Wan and Tsung-Hui Chang},
  doi          = {10.1109/TKDE.2022.3206881},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8584-8595},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Structured sparse non-negative matrix factorization with $\ell _{2,0}$ℓ2,0-norm},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure diversity and mean hitting time for random walks
on stochastic uniform growth tree networks. <em>TKDE</em>,
<em>35</em>(8), 8572–8583. (<a
href="https://doi.org/10.1109/TKDE.2022.3206210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a principled framework using Vertex-based and Edge-based uniform generation mechanisms to build stochastic uniform growth tree networks that have a wide range of applications in various fields including physics, engineering, chemistry, ect., and then uncover the associated structural features analytically. When considering vertex-degree distribution, there exist three different classes of forms in the thermodynamic limit, i.e., exponential distribution, power-law distribution along with multiple-point distribution. At meantime, three distinct structural shapes are observed in the study of fractal phenomena, that is, fractal feature, critical phenomenon and non-fractal property. In addition, we obtain the analytical solution to fractal dimension for fractal structure from the probability point of view. More importantly, some well-known models, for instance, Vicsek fractal and T-graph, fall into our framework. Next, we precisely consider two families of stochastic uniform growth tree networks generated through the proposed framework. Specifically, we derive the analytic solution to mean hitting time $\langle \mathcal {H}\rangle$ for measuring efficiency of delivering information on networks in a random-walk-based manner, and find that the introduction of randomness certainly enriches the scaling exponent of quantity $\langle \mathcal {H}\rangle$ . Finally, we conduct extensive experiments, which suggests that computer simulations are in good agreement with theoretical analysis.},
  archive      = {J_TKDE},
  author       = {Fei Ma and Ping Wang and Xudong Luo and Renbo Zhu},
  doi          = {10.1109/TKDE.2022.3206210},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8572-8583},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Structure diversity and mean hitting time for random walks on stochastic uniform growth tree networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Stock movement prediction based on bi-typed
hybrid-relational market knowledge graph via dual attention networks.
<em>TKDE</em>, <em>35</em>(8), 8559–8571. (<a
href="https://doi.org/10.1109/TKDE.2022.3220520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock Movement Prediction (SMP) aims at predicting listed companies’ stock future price trend, which is a challenging task due to the volatile nature of financial markets. Recent financial studies show that the momentum spillover effect plays a significant role in stock fluctuation. However, previous studies typically only learn the simple connection information among related companies, which inevitably fail to model complex relations of listed companies in real financial market. To address this issue, we first construct a more comprehensive Market Knowledge Graph (MKG) which contains bi-typed entities including listed companies and their associated executives, and hybrid-relations including the explicit relations and implicit relations. Afterward, we propose DanSmp , a novel Dual Attention Networks to learn the momentum spillover signals based upon the constructed MKG for stock prediction. The empirical experiments on our constructed datasets against nine SOTA baselines demonstrate that the proposed DanSmp is capable of improving stock prediction with the constructed MKG.},
  archive      = {J_TKDE},
  author       = {Yu Zhao and Huaming Du and Ying Liu and Shaopeng Wei and Xingyan Chen and Fuzhen Zhuang and Qing Li and Gang Kou},
  doi          = {10.1109/TKDE.2022.3220520},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8559-8571},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Stock movement prediction based on bi-typed hybrid-relational market knowledge graph via dual attention networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised entity alignment via relation-based adaptive
neighborhood matching. <em>TKDE</em>, <em>35</em>(8), 8545–8558. (<a
href="https://doi.org/10.1109/TKDE.2022.3222811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many recent studies of Entity Alignment (EA) use Graph Neural Networks (GNNs) to aggregate the neighborhood features of entities and achieve better performance. However, aligned entities in real Knowledge Graphs (KGs) usually have non-isomorphic neighborhood structures due to the different data sources of KGs. Therefore, it is insufficient to simply compare the global direct neighborhood of aligned entities, which may also become a variable for the EA judgment. In this paper, we propose a Relation-based Adaptive Neighborhood Matching method ( RANM ), which matches larger range and higher confidence neighborhoods for aligned entities based on relation matching instead of alignment seeds. RANM first uses alignment seeds to construct the best relation matching set, and then performs local direct neighborhood matching and feature aggregation on the candidate alignments. To obtain high-quality entity embeddings, we design a variant attention mechanism based on heterogeneous graphs, which considers the heterogeneity of relations in KGs. We also adopt a bi-directional iterative co-training to further improve the performance. Extensive experiments on three well-known datasets show our method significantly outperforms 14 state-of-the-art methods, and is 3.01-11.5\% higher than the best-performing baselines in Hits@1. RANM also shows high performance on the long-tailed entities and the dataset with less alignment seeds.},
  archive      = {J_TKDE},
  author       = {Weishan Cai and Wenjun Ma and Lina Wei and Yuncheng Jiang},
  doi          = {10.1109/TKDE.2022.3222811},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8545-8558},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised entity alignment via relation-based adaptive neighborhood matching},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised EEG clustering with multiple constraints.
<em>TKDE</em>, <em>35</em>(8), 8529–8544. (<a
href="https://doi.org/10.1109/TKDE.2022.3206330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG)-based applications in Brain-Computer Interfaces (BCIs, or Human-Machine Interfaces, HMIs), diagnosis of neurological disease, rehabilitation, etc , rely on supervised techniques such as EEG classification that requires given class labels or markers. Incomplete or incorrectly labeled or unlabeled EEG data are increasing with the ever-expanding amount of EEG data generated by such applications and the ambiguities these generate degrade the performance of supervised techniques. To address the challenging task of clustering EEG data with limited priori knowledge, we introduce a semi-supervised graph embedding EEG clustering approach termed ConsEEGc with multiple constraints, i.e., label-transformed connectivity constraints that constrains the connection or disconnection among EEG data, compactness-and-scatter constraint that constrains the intra-cluster compactness and inter-cluster scatter of EEG clusters, and fairness constraint that constrains the fair ratio of elements between EEG clusters, to make best use of limited priori knowledge of EEG data and to achieve better EEG clustering results. ConsEEGc is conducted with an optimization objective function that integrates pseudo label learning, least-square error minimization and multiple constraints, and it can quickly converge to local optima. The experiments demonstrate that ConsEEGc can efficiently yield good clustering results on various types of real-world EEG datasets, compared to state-of-the-art standard unsupervised and semi-supervised EEG/time series clustering algorithms.},
  archive      = {J_TKDE},
  author       = {Chenglong Dai and Jia Wu and Jessica J. M. Monaghan and Guanghui Li and Hao Peng and Stefanie I. Becker and David McAlpine},
  doi          = {10.1109/TKDE.2022.3206330},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8529-8544},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised EEG clustering with multiple constraints},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Searching personalized <span
class="math inline"><em>k</em></span>k-wing in bipartite graphs.
<em>TKDE</em>, <em>35</em>(8), 8515–8528. (<a
href="https://doi.org/10.1109/TKDE.2022.3199592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are extensive studies focusing on the application scenario that all the bipartite cohesive subgraphs need to be discovered in a bipartite graph. However, we observe that, for some applications, one is interested in finding bipartite cohesive subgraphs containing a specific vertex. In this paper, we study a new query-dependent bipartite cohesive subgraph search problem based on $k$ -wing model, named as personalized $k$ -wing search problem. We study the $k$ -wing equivalence relationship to summarize the edges of a bipartite graph $G$ into groups. Therefore, all the edges of $G$ are segregated into different groups, i.e., $k$k-wing equivalence class , forming an efficient and wing number conserving index called EquiWing-Graph . Further, we propose a more compact index, EquiWing-Tree , which is achieved by using our proposed $k$k-butterfly loose approach and discovered hierarchy properties. These indices are used to expedite the personalized $k$ -wing search with a non-repetitive access to $G$ , which leads to linear algorithms for searching the personalized $k$ -wing. Moreover, we conduct a thorough study on the maintenance of the proposed indices for evolving bipartite graphs. We discover novel properties that help us localize the scope of the maintenance at a low cost. By exploiting the discoveries, we propose novel algorithms for maintaining the two indices, which substantially reduces the cost of maintenance. We perform extensive experimental studies in real-world graphs to validate the efficiency and effectiveness of EquiWing-Graph and EquiWing-Tree compared to the baseline.},
  archive      = {J_TKDE},
  author       = {Aman Abidi and Lu Chen and Rui Zhou and Chengfei Liu},
  doi          = {10.1109/TKDE.2022.3199592},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8515-8528},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Searching personalized $k$k-wing in bipartite graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable fuzzy clustering with anchor graph. <em>TKDE</em>,
<em>35</em>(8), 8503–8514. (<a
href="https://doi.org/10.1109/TKDE.2022.3200685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy clustering algorithms have been widely used to reveal the possible hidden structure of data. However, with the increasing of data amount, large scale data has brought genuine challenges for fuzzy clustering. Most fuzzy clustering algorithms suffer from the long time-consumption problem since a large amount of distance calculations are involved to update the solution per iteration. To address this problem, we introduce the popular anchor graph technique into fuzzy clustering and propose a scalable fuzzy clustering algorithm referred to as Scalable Fuzzy Clustering with Anchor Graph (SFCAG). The main characteristic of SFCAG is that it addresses the scalability issue plaguing fuzzy clustering from two perspectives: anchor graph construction and membership matrix learning. Specifically, we select a small number of anchors and construct a sparse anchor graph, which is beneficial to reduce the computational complexity. We then formulate a trace ratio model, which is parameter-free, to learn the membership matrix of anchors to speed up the clustering procedure. In addition, the proposed method enjoys linear time complexity with the data size. Extensive experiments performed on both synthetic and real world datasets demonstrate the superiority (both effectiveness and scalability) of the proposed method over some representative large scale clustering methods.},
  archive      = {J_TKDE},
  author       = {Chaodie Liu and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2022.3200685},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8503-8514},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Scalable fuzzy clustering with anchor graph},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust sparse weighted classification for crowdsourcing.
<em>TKDE</em>, <em>35</em>(8), 8490–8502. (<a
href="https://doi.org/10.1109/TKDE.2022.3201955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected from nature is usually unlabeled, and it is difficult to be used directly. This issue is well addressed by crowdsourcing, which provides a reasonable way for effectively using these unlabeled data. Generally, workers in crowdsourcing tasks are not professionals, so it is hard to obtain high-quality labels. To address this issue, a robust sparse weighted classification algorithm is proposed, which try to adjust the samples that are not correctly classified in the original lables as much as possible. Specifically, we evalute the ability of different workers(indicator weight matrix) to accurately label different samples by fitting the real data matrix to its weighted reconstruction matrix. And then, $ l_{2,1}$ -norm and worker labeling ability similarity matrix are added, and negative effects of some bad workers are eliminated through the row sparsity property of $ l_{2,1}$ -norm. Finally, the optimal indicator weight matrix is obtained by optimizing the two matrices in the objective function simultaneously. Therefore, the obtained optimal indicator weight matrix takes the similarity of worker labeling ability into consideration, and infers all the predicted labels. The results on synthetic and real data sets demonstrate that our algorithm is superior to other state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Hao Yu and Chengyuan Zhang and Jiaye Li and Shichao Zhang},
  doi          = {10.1109/TKDE.2022.3201955},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8490-8502},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust sparse weighted classification for crowdsourcing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Road-aware indexing for trajectory range queries.
<em>TKDE</em>, <em>35</em>(8), 8476–8489. (<a
href="https://doi.org/10.1109/TKDE.2022.3220822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answering spatio-temporal range queries (RQs) on trajectory databases, i.e., finding all trajectories that intersect given ranges, is crucial in many real-world applications. Various kinds of indexes have been proposed to accelerate RQs. However, existing indexes typically use euclidean distance to prune irrelevant regions without concerning the underlying road network information. Nevertheless, as vehicle trajectories are generated on road network edges, the road network could be seen as meta knowledge of trajectories and be used to index and query trajectories. To this end, we propose RP-Tree, a r oad network-aware p artition tree to support efficient RQs. The basic idea is partitioning a road network graph into hierarchical subgraphs and generate a balanced tree structure, where each tree node maintains its associated trajectories. We compactly index the spatio-temporal information of trajectories on the corresponding road network edges. Then, we design efficient search algorithms to support RQs by pruning irrelevant trajectories through subgraph range borders associated with RP-Tree nodes. Last but not least, we scale RP-Tree to very large datasets by devising approximate algorithms with bounded confidence at an interactive speed. Experimental results on three real-world datasets from Porto, Chengdu, and Beijing show that our method outperform baselines by 1 to 2 orders of magnitude.},
  archive      = {J_TKDE},
  author       = {Yong Wang and Kaiyu Li and Guoliang Li and Nan Tang},
  doi          = {10.1109/TKDE.2022.3220822},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8476-8489},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Road-aware indexing for trajectory range queries},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting embedding-based entity alignment: A robust and
adaptive method. <em>TKDE</em>, <em>35</em>(8), 8461–8475. (<a
href="https://doi.org/10.1109/TKDE.2022.3200981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity alignment—the discovery of identical entities across different knowledge graphs (KGs)—is a critical task in data fusion. In this paper, we revisit existing entity alignment methods in practical and challenging scenarios. Our empirical studies show that current work has a low level of robustness to long-tail entities and the lack of entity names or relation triples. We aim to develop a robust and adaptive entity alignment method, and the availability of relations, attributes, or names is not required. Our method consists of an attribute encoder and a relation encoder, representing an entity by aggregating its attributes or relational neighbors using the attention mechanisms that can highlight the useful attributes and relations in end-to-end learning. To let the encoders complement each other and produce a coherent representation space, we propose adaptive embedding fusion via a gating mechanism. We consider four evaluation settings, i.e., the conventional setting with both relation and attribute triples, as well as three challenging settings without attributes, without relations, without both relations and names, respectively. Results show that our method can achieve state-of-the-art performance. Even in the most challenging setting without relations and names, our method can still achieve promising results while existing methods fail.},
  archive      = {J_TKDE},
  author       = {Zequn Sun and Wei Hu and Chengming Wang and Yuxin Wang and Yuzhong Qu},
  doi          = {10.1109/TKDE.2022.3200981},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8461-8475},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Revisiting embedding-based entity alignment: A robust and adaptive method},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Repulsion-GNNs: Use repulsion to supplement aggregation.
<em>TKDE</em>, <em>35</em>(8), 8448–8460. (<a
href="https://doi.org/10.1109/TKDE.2022.3218825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have achieved prominent performance in the node classification task, by constructing an aggregation process to integrate node features and graph topology. The aggregation process makes the features of the connected nodes similar, which helps to classify. However, this will also cause nodes that are connected but belong to different classes to be more confusing. In this paper, we propose Repulsion-GNNs, in which a repulsion process is introduced to supplement the aggregation process. First, the nodes are divided into hyper nodes based on a basic node classification model. Then, the repulsion for each node can be obtained according to these hyper nodes. Finally, the node embeddings are obtained by combining the aggregation and repulsion. Many existing GNNs can be combined with the repulsion without adding any learnable parameter. Extensive experiments on benchmark datasets for node classification demonstrate that the repulsion can boost the performance of many GNNs, such as GCN, GAT, SAGE, and GCNII.},
  archive      = {J_TKDE},
  author       = {Jian Gao and Jianshe Wu},
  doi          = {10.1109/TKDE.2022.3218825},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8448-8460},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Repulsion-GNNs: Use repulsion to supplement aggregation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Refined commonsense knowledge from large-scale web contents.
<em>TKDE</em>, <em>35</em>(8), 8431–8447. (<a
href="https://doi.org/10.1109/TKDE.2022.3206505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commonsense knowledge (CSK) about concepts and their properties is helpful for AI applications. Prior works, such as ConceptNet, have compiled large CSK collections. However, they are restricted in their expressiveness to subject-predicate-object (SPO) triples with simple concepts for S and strings for P and O. This paper presents a method called Ascent++ to automatically build a large-scale knowledge base (KB) of CSK assertions, with refined expressiveness and both better precision and recall than prior works. Ascent++ goes beyond SPO triples by capturing composite concepts with subgroups and aspects, and by refining assertions with semantic facets. The latter is essential to express the temporal and spatial validity of assertions and further qualifiers. Furthermore, Ascent++ combines open information extraction (OpenIE) with judicious cleaning and ranking by typicality and saliency scores. For high coverage, our method taps into the large-scale crawl C4 with broad web contents. The evaluation with human judgments shows the superior quality of the Ascent++ KB, and an extrinsic evaluation for QA-support tasks underlines the benefits of Ascent++ . A web interface, data, and code can be accessed at https://ascentpp.mpi-inf.mpg.de/ .},
  archive      = {J_TKDE},
  author       = {Tuan-Phong Nguyen and Simon Razniewski and Julien Romero and Gerhard Weikum},
  doi          = {10.1109/TKDE.2022.3206505},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8431-8447},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Refined commonsense knowledge from large-scale web contents},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random walk on multiple networks. <em>TKDE</em>,
<em>35</em>(8), 8417–8430. (<a
href="https://doi.org/10.1109/TKDE.2022.3221668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random Walk is a basic algorithm to explore the structure of networks, which can be used in many tasks, such as local community detection and network embedding. Existing random walk methods are based on single networks that contain limited information. In contrast, real data often contain entities with different types or/and from different sources, which are comprehensive and can be better modeled by multiple networks. To take the advantage of rich information in multiple networks and make better inferences on entities, in this study, we propose random walk on multiple networks, $\mathsf {RWM}$ . $\mathsf {RWM}$ is flexible and supports both multiplex networks and general multiple networks, which may form many-to-many node mappings between networks. $\mathsf {RWM}$ sends a random walker on each network to obtain the local proximity (i.e., node visiting probabilities) w.r.t. the starting nodes. Walkers with similar visiting probabilities reinforce each other. We theoretically analyze the convergence properties of $\mathsf {RWM}$ . Two approximation methods with theoretical performance guarantees are proposed for efficient computation. We apply $\mathsf {RWM}$ in link prediction, network embedding, and local community detection. Comprehensive experiments conducted on both synthetic and real-world datasets demonstrate the effectiveness and efficiency of $\mathsf {RWM}$ .},
  archive      = {J_TKDE},
  author       = {Dongsheng Luo and Yuchen Bian and Yaowei Yan and Xiong Yu and Jun Huan and Xiao Liu and Xiang Zhang},
  doi          = {10.1109/TKDE.2022.3221668},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8417-8430},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Random walk on multiple networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Projective ranking-based GNN evasion attacks. <em>TKDE</em>,
<em>35</em>(8), 8402–8416. (<a
href="https://doi.org/10.1109/TKDE.2022.3219209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) offer promising learning methods for graph-related tasks. However, GNNs are at risk of adversarial attacks. Two primary limitations of the current evasion attack methods are highlighted: (1) The current GradArgmax ignores the “long-term” benefit of the perturbation. It is faced with zero-gradient and invalid benefit estimates in certain situations. (2) In reinforcement learning-based attack methods, the learned attack strategies might not be transferable when the attack budget changes. To this end, we first formulate the perturbation space and propose an evaluation framework and the projective ranking method. We aim to learn a powerful attack strategy then adapt it as little as possible to generate adversarial samples under dynamic budget settings. In our method, based on mutual information, we rank and assess the attack benefits of each perturbation for an effective attack strategy. By projecting the strategy, our method dramatically minimizes the cost of learning a new attack strategy when the attack budget changes. In the comparative assessment with GradArgmax and RL-S2V , the results show our method owns high attack performance and effective transferability. The visualization of our method also reveals various attack patterns in the generation of adversarial samples.},
  archive      = {J_TKDE},
  author       = {He Zhang and Xingliang Yuan and Chuan Zhou and Shirui Pan},
  doi          = {10.1109/TKDE.2022.3219209},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8402-8416},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Projective ranking-based GNN evasion attacks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Profit optimization in spatial crowdsourcing: Effectiveness
and efficiency. <em>TKDE</em>, <em>35</em>(8), 8386–8401. (<a
href="https://doi.org/10.1109/TKDE.2022.3220360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Spatial crowdsourcing, mobile users perform spatio-temporal tasks that involve travel to specified locations. Spatial crowdsourcing (SC) is enabled by SC platforms that support mobile worker recruitment and retention, as well as task assignment, which is essential to maximize profits that are accrued from serving task requests. Specifically, how to best achieve task assignment in a cost-effective manner while contending with spatio-temporal constraints is a key challenge in SC. To address this challenge, we formalize and study a novel Profit-driven Task Assignment problem. We first establish a task reward pricing model that takes into account the temporal constraints (i.e., expected completion time and deadline) of tasks. Then we adopt an optimal algorithm based on tree decomposition to achieve an optimal task assignment and propose greedy algorithms based on Random Tuning Optimization to improve the computational efficiency. To balance effectiveness and efficiency, we also provide a heuristic task assignment algorithm based on Ant Colony Optimization that assigns tasks by simulating behavior of ant colonies foraging for food. Finally, we conduct extensive experiments using real and synthetic data, offering detailed insight into effectiveness and efficiency of the proposed methods.},
  archive      = {J_TKDE},
  author       = {Yan Zhao and Kai Zheng and Yunchuan Li and Jinfu Xia and Bin Yang and Torben Bach Pedersen and Rui Mao and Christian S. Jensen and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2022.3220360},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8386-8401},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Profit optimization in spatial crowdsourcing: Effectiveness and efficiency},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <span class="math inline">PF-HIN</span>: Pre-training for
heterogeneous information networks. <em>TKDE</em>, <em>35</em>(8),
8372–8385. (<a href="https://doi.org/10.1109/TKDE.2022.3206597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network representation learning we learn how to represent heterogeneous information networks in a low-dimensional space so as to facilitate effective search, classification, and prediction solutions. Previous network representation learning methods typically require sufficient task-specific labeled data to address domain-specific problems. The trained model usually cannot be transferred to out-of-domain datasets. We propose a self-supervised pre-training and fine-tuning framework, $\mathsf{PF\text{-}HIN}$ , to capture the features of a heterogeneous information network. Unlike traditional network representation learning models that have to train the entire model all over again for every downstream task and dataset, $\mathsf{PF\text{-}HIN}$ only needs to fine-tune the model and a small number of extra task-specific parameters, thus improving model efficiency and effectiveness. During pre-training, we first transform the neighborhood of a given node into a sequence. $\mathsf{PF\text{-}HIN}$ is pre-trained based on two self-supervised tasks, masked node modeling and adjacent node prediction. We adopt deep bi-directional transformer encoders to train the model, and leverage factorized embedding parameterization and cross-layer parameter sharing to reduce the parameters. In the fine-tuning stage, we choose four benchmark downstream tasks, i.e., link prediction, similarity search, node classification, and node clustering. $\mathsf{PF\text{-}HIN}$ outperforms state-of-the-art alternatives on each of these tasks, on four datasets.},
  archive      = {J_TKDE},
  author       = {Yang Fang and Xiang Zhao and Yifan Chen and Weidong Xiao and Maarten de Rijke},
  doi          = {10.1109/TKDE.2022.3206597},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8372-8385},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {$\mathsf{PF\text{-}HIN}$: Pre-training for heterogeneous information networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Personalized dynamic counter ad-blocking using deep
learning. <em>TKDE</em>, <em>35</em>(8), 8358–8371. (<a
href="https://doi.org/10.1109/TKDE.2022.3201058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast increase in ad-blocker usage has resulted in significant revenue loss for online publishers. To mitigate this, many publishers implement the Wall strategy, where an adblock user is asked to whitelist the intended webpage. If the user refuses, the result is a loss-loss situation: the user is denied access to content, and the publisher cannot receive revenue. An alternative strategy, called AAX, is to show only acceptable ads to users. However, acceptable ads generate less revenue than regular ads. This article proposes personalized counter ad-blocking that dynamically chooses a counter ad-blocking strategy for individual users. To implement it, we propose a novel deep learning-based whitelist prediction model. Adblock users predicted to whitelist a page receive the Wall strategy; the others receive the AAX strategy. The proposed Deep Ad-Block Whitelist Network (DAWN) for whitelist prediction captures page characteristics, user interests in pages and their sensitivity to ads, reflected in historic behavior, using a deep learning mechanism. Furthermore, DAWN leverages multi-task learning on whitelist prediction and dwell-time prediction to boost performance. DAWN&#39;s effectiveness is validated on a real-world dataset provided by Forbes Media. The experimental results demonstrate the advantages of the proposed counter ad-blocking policy over existing policies on revenue generation and user engagement.},
  archive      = {J_TKDE},
  author       = {Shuai Zhao and Michael K. Chen and Cristian Borcea and Yi Chen},
  doi          = {10.1109/TKDE.2022.3201058},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8358-8371},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Personalized dynamic counter ad-blocking using deep learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Outlining and filling: Hierarchical query graph generation
for answering complex questions over knowledge graphs. <em>TKDE</em>,
<em>35</em>(8), 8343–8357. (<a
href="https://doi.org/10.1109/TKDE.2022.3207477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query graph construction aims to construct the correct executable SPARQL on the KG to answer natural language questions. Although recent methods have achieved good results using neural network-based query graph ranking, they suffer from three new challenges when handling more complex questions: 1) complicated SPARQL syntax, 2) huge search space, and 3) locally ambiguous query graphs. In this paper, we provide a new solution. As a preparation, we extend the query graph by treating each SPARQL clause as a subgraph consisting of vertices and edges and define a unified graph grammar called AQG to describe the structure of query graphs. Based on these concepts, we propose a novel end-to-end model that performs hierarchical autoregressive decoding to generate query graphs. The high-level decoding generates an AQG as a constraint to prune the search space and reduce the locally ambiguous query graph. The bottom-level decoding accomplishes the query graph construction by selecting appropriate instances from the preprepared candidates to fill the slots in the AQG. The experimental results show that our method greatly improves the SOTA performance on complex KGQA benchmarks. Equipped with pre-trained models, the performance of our method is further improved, achieving SOTA for all three datasets used.},
  archive      = {J_TKDE},
  author       = {Yongrui Chen and Huiying Li and Guilin Qi and Tianxing Wu and Tenggou Wang},
  doi          = {10.1109/TKDE.2022.3207477},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8343-8357},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Outlining and filling: Hierarchical query graph generation for answering complex questions over knowledge graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric and online change detection in multivariate
datastreams using QuantTree. <em>TKDE</em>, <em>35</em>(8), 8328–8342.
(<a href="https://doi.org/10.1109/TKDE.2022.3201635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of online change detection in multivariate datastreams, and we introduce QuantTree Exponentially Weighted Moving Average (QT-EWMA), a nonparametric change-detection algorithm that can control the expected time before a false alarm, yielding a desired Average Run Length (ARL $_{0}$ ). Controlling false alarms is crucial in many applications and is rarely guaranteed by online change-detection algorithms that can monitor multivariate datastreams without knowing the data distribution. Like many change-detection algorithms, QT-EWMA builds a model of the data distribution, in our case a QuantTree histogram, from a stationary training set. To monitor datastreams even when the training set is extremely small, we propose QT-EWMA-update, which incrementally updates the QuantTree histogram during monitoring, always keeping the ARL $_{0}$ under control. Our experiments, performed on synthetic and real-world datastreams, demonstrate that QT-EWMA and QT-EWMA-update control the ARL $_{0}$ and the false alarm rate better than state-of-the-art methods operating in similar conditions, achieving lower or comparable detection delays.},
  archive      = {J_TKDE},
  author       = {Luca Frittoli and Diego Carrera and Giacomo Boracchi},
  doi          = {10.1109/TKDE.2022.3201635},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8328-8342},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Nonparametric and online change detection in multivariate datastreams using QuantTree},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeuralCD: A general framework for cognitive diagnosis.
<em>TKDE</em>, <em>35</em>(8), 8312–8327. (<a
href="https://doi.org/10.1109/TKDE.2022.3201037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive diagnosis is widely applicable in the scenarios where users’ cognitive states need to be assessed, such as games and clinical measurement. Especially in intelligent education, which has become increasingly popular recent decades, cognitive diagnosis serves as a fundamental module for discovering the proficiency level of students on specific knowledge concepts. Existing approaches usually mine linear interactions of student exercising process by manually designed function (e.g., logistic function). However, the cognitive interactions between students and exercises is a complex process, and excessive simplifications would lead to under fitting and thus get inaccurate diagnostic results. Besides, the manually designed interaction functions are relatively inflexible and limits their extensibility. This consequently causes lack of consideration about useful non-numerical information in the cognitive process besides response logs. In this article, we propose a general Neural Cognitive Diagnosis (NeuralCD) framework as well as several implemented models (a basic implementation NeuralCDM and three extensions), where we project students and exercises to factor vectors and incorporates neural networks to learn the complex exercising interactions. To ensure the interpretability of diagnostic results, which is essential for cognitive diagnosis, we apply an monotonicity assumption to our NeuralCD framework. Moreover, NeuralCD is a general framework and has good extensibility. We show the generality of NeuralCD through proving how it can cover some traditional models. Then, we demonstrate the extensibility of NeuralCD, which benefits future developments. On one hand, we demonstrate content-based extensions where we provide examples of exploring the rich contents of exercise texts (CNCD-Q and CNCD-F). On the other hand, we demonstrate a knowledge-association based extension to show that NeuralCD is flexible for structural adjustments so as to solve specific problems. For instance, we improve the diagnostic results on uncovered knowledge concepts of a student by extending NeuralCD with the knowledge associations consideration (KaNCD). Extensive experimental results on real-world datasets show the effectiveness of NeuralCD framework with both accuracy and interpretability.},
  archive      = {J_TKDE},
  author       = {Fei Wang and Qi Liu and Enhong Chen and Zhenya Huang and Yu Yin and Shijin Wang and Yu Su},
  doi          = {10.1109/TKDE.2022.3201037},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8312-8327},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {NeuralCD: A general framework for cognitive diagnosis},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nessy: A neuro-symbolic system for label noise reduction.
<em>TKDE</em>, <em>35</em>(8), 8300–8311. (<a
href="https://doi.org/10.1109/TKDE.2022.3199570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy labels represent one of the key issues in supervised machine learning. Existing work for label noise reduction mainly takes a probabilistic approach that infers true labels from data distributions in low-level feature spaces. Such an approach is not only limited by its capability to learn high-quality data representations, but also by the low predictive power of data distributions in inferring true classes. To address those problems, we introduce Nessy, a neuro-symbolic system that integrates deep probabilistic modeling and symbolic knowledge for label noise reduction. Our deep probabilistic model infers the true classes of data instances with noisy labels by exploiting data distributions in an underlying latent feature representation space. For data instances where inference is not reliable enough, Nessy extracts symbolic rules and ranks them according to several utility metrics. Top-ranking rules are injected into the deep probabilistic model via expectation regularization, i.e., via a posterior regularization term constraining the class distribution in the objective function. In a real deployment over multiple relation extraction tasks, we demonstrate that Nessy is able to significantly improve the state of the art, by 7\% accuracy and 10.7\% AUC on average.},
  archive      = {J_TKDE},
  author       = {Alisa Smirnova and Jie Yang and Dingqi Yang and Philippe Cudre-Mauroux},
  doi          = {10.1109/TKDE.2022.3199570},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8300-8311},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Nessy: A neuro-symbolic system for label noise reduction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mutual information-guided GA for bayesian network structure
learning. <em>TKDE</em>, <em>35</em>(8), 8282–8299. (<a
href="https://doi.org/10.1109/TKDE.2022.3219862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian network structure learning (BNSL) from data is an NP-hard problem. Genetic algorithms are powerful for solving combinatorial optimization problems, but the lack of effective guidance results in slow convergence and low accuracy regarding BNSL. To address this problem, we propose a mutual information (MI) guided genetic algorithm (MIGA) for BNSL in this paper, which uses MI to effectively search BN structures. In the initialization phase of MIGA, the population is generated by adding additional constraints based on MI to reach a higher score without losing diversity. By employing normalized MI and defining the population support, the potential dominance in the population can be identified and then used to design a novel crossover operator in order to preserve the dominant genes with a higher probability. Moreover, with the guidance of MI for removing loops from the structures, infeasible solutions can be handled in a straightforward and practical way. The proposed MIGA is evaluated on eleven well-known benchmark datasets and compared with four GA-based methods and four other state-of-the-art BNSL algorithms. Experimental results show that MIGA outperforms the compared algorithms in convergence and learning accuracy.},
  archive      = {J_TKDE},
  author       = {Kefei Yan and Wei Fang and Hengyang Lu and Xin Zhang and Jun Sun and Xiaojun Wu},
  doi          = {10.1109/TKDE.2022.3219862},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8282-8299},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Mutual information-guided GA for bayesian network structure learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view subspace clustering by joint measuring of
consistency and diversity. <em>TKDE</em>, <em>35</em>(8), 8270–8281. (<a
href="https://doi.org/10.1109/TKDE.2022.3199587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-view subspace clustering, it is significant to find a common latent space in which the multi-view datasets are located. A number of multi-view subspace clustering methods have been proposed to explore the common latent subspace and achieved promising performance. However, previous multi-view subspace clustering algorithms seldom consider the multi-view consistency and multi-view diversity, let alone take them into consideration simultaneously. In this paper, we propose a novel multi-view subspace clustering by joint measuring the consistency and diversity, which is able to exploit these two complementary criteria seamlessly into a holistic design of clustering algorithms. The proposed model first searches a pure graph for each view by detecting the intrinsic consistent and diverse parts. A consensus graph is then obtained by fusing the multiple pure graphs. Moreover, the consensus graph is structurized to contain exactly $c$ connected components where $c$ is the number of clusters. In this way, the final clustering result can be obtained directly since each connected component precisely corresponds to an individual cluster. Extensive experimental studies on various datasets manifest that our model achieves comparable performance than the other state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Shudong Huang and Yixi Liu and Ivor W. Tsang and Zenglin Xu and Jiancheng Lv},
  doi          = {10.1109/TKDE.2022.3199587},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8270-8281},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view subspace clustering by joint measuring of consistency and diversity},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective cluster ensemble based on filter refinement
scheme. <em>TKDE</em>, <em>35</em>(8), 8257–8269. (<a
href="https://doi.org/10.1109/TKDE.2022.3207141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster ensemble improves the robustness and stability of clustering performances by utilizing multiple solutions. Although traditional cluster ensemble methods have achieved promising performances, they are not adaptive enough to cope with data sets that have multiple levels of complexities. Besides, these methods may contain noisy and redundancy members which have negative effects. To mitigate the above issues, in this paper, we propose a multi-objective filter refinement scheme (MOFRS). First, we perform various clustering methods on different representations of data to generate diverse solutions. Second, we propose a solution filter to select a proper method and reduce the number of initial partitions for a given data set. Third, four stability indices are designed to split instances into stable and unstable groups. Fourth, objective functions based on diversity and quality are utilized to quantify the goodness of base clustering solutions. Finally, we design an improvement oriented multi-objective evolutionary algorithm to optimize these objective functions. Extensive experimental results conducted on 27 real-world data sets show that MOFRS outperforms most cluster ensemble selection methods, and achieves statistically significant improvements, compared with full ensemble methods.},
  archive      = {J_TKDE},
  author       = {Dan Dai and Zhiwen Yu and Weijie Huang and Yang Hu and C. L. Philip Chen},
  doi          = {10.1109/TKDE.2022.3207141},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8257-8269},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-objective cluster ensemble based on filter refinement scheme},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi global information assisted streaming session-based
recommendation system. <em>TKDE</em>, <em>35</em>(8), 8245–8256. (<a
href="https://doi.org/10.1109/TKDE.2022.3199373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming Session-Based Recommendation (SSBR) is a challenging problem as user preferences in sessions are continually drifting with sessions generated chronologically. In recent years, some SSBR models have been proposed to address this problem by reservoir technique and Graph Neural Networks (GNN) which help to preserve a representative sketch of the historical data and extract item transition information in sessions. However, there are two critical problems in existing methods: (1) most existing methods only focus on the local session information without exploiting the information of other sessions and users; (2) GNN models in existing SSBR methods are unable to capture the importance of different user features. To address the problems mentioned above, we propose a novel architecture named G lobal I tem and U ser embedding A ssisted G raph N eural N etwork ( GIUA-GNN ) for combining the global user and item information in an attentional manner with local session information for the recommendation. We also propose a novel architecture of graph neural network which utilizes the attention mechanism for better extracting the importance of different features of user embeddings named B i-directed A ttentional G raph C onvolutional N etwork ( BA-GCN ). Extensive experiments on three different sizes of real-world datasets have been conducted to demonstrate the superiority of our model on metrics MRR and Recall.},
  archive      = {J_TKDE},
  author       = {Zhizhuo Yin and Kai Han and Pengzi Wang and Haibing Hu},
  doi          = {10.1109/TKDE.2022.3199373},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8245-8256},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi global information assisted streaming session-based recommendation system},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MSTDB: A hybrid storage-empowered scalable semantic
blockchain database. <em>TKDE</em>, <em>35</em>(8), 8228–8244. (<a
href="https://doi.org/10.1109/TKDE.2022.3220522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain has been regarded as a trusted carrier for distributed data storage. With large volumes of valuable data stored on blockchain, data query has become a major requirement. However, the existing blockchains do not provide efficient query functionality because of their deep-rooted chain structure. Blockchain database is a new direction that constructs index on top of blockchain to provide rich query functionalities. The existing works are either insecure because the query process separates from the blockchain consensus, or inscalable because all the data needs to be stored in the block. In this paper, we propose a novel semantic blockchain database called MSTDB. We design a hybrid on/off chain blockchain storage architecture in which the majority of blockchain storage is offloaded to the off-chain storage and a novel index structure named Merkle Semantic Trie (MST) is designed to be a secure and semantic bridge between on- and off-chain. Based on MST, MSTDB provides a variety of semantic query functions including multi-keyword query, range query, Top-K query, and cross-chain query. To improve the performance further, we design some index compression and query preprocessing techniques for MSTDB. Extensive experiments demonstrate the effectiveness and efficiency of our blockchain database.},
  archive      = {J_TKDE},
  author       = {Enyuan Zhou and Zicong Hong and Yang Xiao and Dongxiao Zhao and Qingqi Pei and Song Guo and Rajendra Akerkar},
  doi          = {10.1109/TKDE.2022.3220522},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8228-8244},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MSTDB: A hybrid storage-empowered scalable semantic blockchain database},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monitoring student progress for learning process-consistent
knowledge tracing. <em>TKDE</em>, <em>35</em>(8), 8213–8227. (<a
href="https://doi.org/10.1109/TKDE.2022.3221985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) is the task of tracing students’ evolving knowledge state during learning, which has improved the learning efficiency. To facilitate KT&#39;s development, most existing methods pursue high accuracy of student performance prediction but neglect the consistency between students’ dynamic knowledge state with their learning process. Moreover, they focus on learning outcomes at a single learning interaction, while student progress at continuous learning interactions is more instructive. In this paper, we explore a new paradigm for the KT task and propose a novel model named Learning Process-consistent Knowledge Tracing (LPKT), which captures the evolution of students’ knowledge state through monitoring their learning progress. Specifically, we utilize both the positive effect of the learning gain and the negative effect of forgetting in learning to calculate student progress in continuous learning interactions. Then, considering that the rate of progress is student-specific, we extend LPKT to LPKT-S by explicitly distinguishing the individual progress rate of each student. Extensive experimental results on three public datasets demonstrate that LPKT and LPKT-S could obtain more appropriate knowledge states in line with the learning process. Moreover, LPKT and LPKT-S outperform state-of-the-art KT methods on student performance prediction. Our work indicates a promising future research direction for KT, which is highly interpretable and accurate.},
  archive      = {J_TKDE},
  author       = {Shuanghong Shen and Enhong Chen and Qi Liu and Zhenya Huang and Wei Huang and Yu Yin and Yu Su and Shijin Wang},
  doi          = {10.1109/TKDE.2022.3221985},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8213-8227},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Monitoring student progress for learning process-consistent knowledge tracing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MetaCAR: Cross-domain meta-augmentation for content-aware
recommendation. <em>TKDE</em>, <em>35</em>(8), 8199–8212. (<a
href="https://doi.org/10.1109/TKDE.2022.3209005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cold-start has become critical for recommendations, especially for sparse user-item interactions. Recent approaches based on meta-learning succeed in alleviating the issue, owing to the fact that these methods have strong generalization, so they can fast adapt to new tasks under cold-start settings. However, these meta-learning-based recommendation models learned with single and spase ratings are easily falling into the meta-overfitting, since the one and only rating $r_{ui}$ to a specific item $i$ cannot reflect a user&#39;s diverse interests under various circumstances(e.g., time, mood, age, etc), i.e., if $r_{ui}$ equals to 1 in the historical dataset, but $r_{ui}$ could be 0 in some circumstance. In meta-learning, tasks with these single ratings are called Non-Mutually-Exclusive(Non-ME) tasks, and tasks with diverse ratings are called Mutually-Exclusive(ME) tasks. Fortunately, a meta-augmentation technique is proposed to relief the meta-overfitting for meta-learning methods by transferring Non-ME tasks into ME tasks by adding noises to labels without changing inputs. Motivated by the meta-augmentation method, in this paper, we propose a cross-domain meta-augmentation technique for content-aware recommendation systems (MetaCAR) to construct ME tasks in the recommendation scenario. Our proposed method consists of two stages: meta-augmentation and meta-learning. In the meta-augmentation stage, we first conduct domain adaptation by a dual conditional variational autoencoder (CVAE) with a multi-view information bottleneck constraint, and then apply the learned CVAE to generate ratings for users in the target domain. In the meta-learning stage, we introduce both the true and generated ratings to construct ME tasks that enables the meta-learning recommendations to avoid meta-overfitting. Experiments evaluated in real-world datasets show the significant superiority of MetaCAR for coping with the cold-start user issue over competing baselines including cross-domain, content-aware, and meta-learning-based recommendations.},
  archive      = {J_TKDE},
  author       = {Hui Xu and Changyu Li and Yan Zhang and Lixin Duan and Ivor W. Tsang and Jie Shao},
  doi          = {10.1109/TKDE.2022.3209005},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8199-8212},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MetaCAR: Cross-domain meta-augmentation for content-aware recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local differentially private fuzzy counting in stream data
using probabilistic data structures. <em>TKDE</em>, <em>35</em>(8),
8185–8198. (<a href="https://doi.org/10.1109/TKDE.2022.3198478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving estimation of counts of items in streaming data finds applications in several real-world scenarios including word auto-correction and traffic management applications. Recent works of RAPPOR Erlingsson et al. (2014) and Apple&#39;s count-mean sketch (CMS) algorithm D. P. T. Apple, (2017) propose privacy preserving mechanisms for count estimation in large volumes of data using probabilistic data structures like counting Bloom filter and CMS. However, these existing methods fall short in providing a sound solution for real-time streaming data applications. Since the size of the data structure in these methods is not adaptive to the volume of the streaming data, the utility (accuracy of the count estimate) can suffer over time due to increased false positive rates. Further, the lookup operation needs to be highly efficient to answer count estimate queries in real-time. More importantly, the local Differential privacy mechanisms used in these approaches to provide privacy guarantees come at a large cost to utility (impacting the accuracy of count estimation). In this work, we propose a novel (local) Differentially private mechanism that provides high utility for the streaming data count estimation problem with similar or even lower privacy budgets while providing: a) fuzzy counting to report counts of related or similar items (for instance to account for typing errors and data variations), and b) improved querying efficiency to reduce the response time for real-time querying of counts. Our algorithm uses a combination of two probabilistic data structures Cuckoo filter and Bloom filter. We provide formal proofs for privacy and utility guarantees and present extensive experimental evaluation of our algorithm using real and synthetic English words datasets for both the exact and fuzzy counting scenarios. Our privacy preserving mechanism substantially outperforms the prior work in terms of lower querying time, significantly higher utility (accuracy of count estimation) under similar or lower privacy guarantees, at the cost of communication overhead.},
  archive      = {J_TKDE},
  author       = {Dinusha Vatsalan and Raghav Bhaskar and Mohamed Ali Kaafar},
  doi          = {10.1109/TKDE.2022.3198478},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8185-8198},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Local differentially private fuzzy counting in stream data using probabilistic data structures},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LEMON: Explainable entity matching. <em>TKDE</em>,
<em>35</em>(8), 8171–8184. (<a
href="https://doi.org/10.1109/TKDE.2022.3200644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art entity matching (EM) methods are hard to interpret, and there is significant value in bringing explainable AI to EM. Unfortunately, most popular explainability methods do not work well out of the box for EM and need adaptation. In this paper, we identify three challenges of applying local post hoc feature attribution methods to entity matching: cross-record interaction effects, non-match explanations, and variation in sensitivity. We propose our novel model-agnostic and schema-flexible method LEMON that addresses all three challenges by (i) producing dual explanations to avoid cross-record interaction effects, (ii) introducing the novel concept of attribution potential to explain how two records could have matched, and (iii) automatically choosing explanation granularity to match the sensitivity of the matcher and record pair in question. Experiments on public datasets demonstrate that the proposed method is more faithful to the matcher and does a better job of helping users understand the decision boundary of the matcher than previous work. Furthermore, user studies show that the rate at which human subjects can construct counterfactual examples after seeing an explanation from our proposed method increases from 54\% to 64\% for matches and from 15\% to 49\% for non-matches compared to explanations from a standard adaptation of LIME.},
  archive      = {J_TKDE},
  author       = {Nils Barlaug},
  doi          = {10.1109/TKDE.2022.3200644},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8171-8184},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LEMON: Explainable entity matching},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Learning linear and nonlinear low-rank structure in
multi-task learning. <em>TKDE</em>, <em>35</em>(8), 8157–8170. (<a
href="https://doi.org/10.1109/TKDE.2022.3203904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the trace norm can discover low-rank structures in a matrix, it has been widely used in multi-task learning to recover the low-rank structure contained in the parameter matrix. Recently, with the emerging of big complex datasets and the popularity of deep learning techniques, tensor trace norms have been used for deep multi-task models. However, existing tensor trace norms exhibit some limitations. For example, they cannot discover all the low-rank structures in a tensor, they require users to manually specify the importance of each component in the corresponding tensor trace norm, and they only capture the linear low-rank structure. To solve the first issue, in this paper, we propose a Generalized Tensor Trace Norm (GTTN). The GTTN is defined as a convex combination of matrix trace norms of all possible tensor flattenings and hence it can discover all the possible low-rank structures. For the second issue, in the induced objective function with the GTTN, we propose four strategies to learn combination coefficients in the GTTN. Furthermore, we propose the Nonlinear GTTN (NGTTN) to capture nonlinear low-rank structure among all the tasks. Experiments on benchmark datasets demonstrate the effectiveness of the proposed GTTN and NGTTN.},
  archive      = {J_TKDE},
  author       = {Yi Zhang and Yu Zhang and Wei Wang},
  doi          = {10.1109/TKDE.2022.3203904},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8157-8170},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning linear and nonlinear low-rank structure in multi-task learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KDCRec: Knowledge distillation for counterfactual
recommendation via uniform data. <em>TKDE</em>, <em>35</em>(8),
8143–8156. (<a href="https://doi.org/10.1109/TKDE.2022.3199585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bias problems in recommender systems are an important challenge. In this paper, we focus on solving the bias problems via uniform data. Previous works have shown that simple modeling with a uniform data can alleviate the bias problems and improve the performance. However, the uniform data is usually few and expensive to collect in a real product. In order to use the valuable uniform data more effectively, we propose a novel and general knowledge distillation framework for counterfactual recommendation with four specific methods, including label-based distillation, feature-based distillation, sample-based distillation and model structure-based distillation. Moreover, we discuss the relation between the proposed framework and the previous works. We then conduct extensive experiments on both public and product datasets to verify the effectiveness of the proposed four methods. In addition, we explore and analyze the performance trends of the proposed methods on some key factors, and the changes in the distribution of the recommendation lists. Finally, we emphasize that counterfactual modeling with uniform data is a rich research area, and list some interesting and promising research topics worthy of further exploration. Note that the source codes are available at https://github.com/dgliu/TKDE_KDCRec .},
  archive      = {J_TKDE},
  author       = {Dugang Liu and Pengxiang Cheng and Zinan Lin and Jinwei Luo and Zhenhua Dong and Xiuqiang He and Weike Pan and Zhong Ming},
  doi          = {10.1109/TKDE.2022.3199585},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8143-8156},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {KDCRec: Knowledge distillation for counterfactual recommendation via uniform data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incomplete multiview clustering using normalizing alignment
strategy with graph regularization. <em>TKDE</em>, <em>35</em>(8),
8126–8142. (<a href="https://doi.org/10.1109/TKDE.2022.3202561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix factorization has demonstrated promising performance in the incomplete multiview clustering (IMC) tasks. However, many algorithms require feature normalization operations to ensure the stability of model results, so either the convergence is unstable, or the objective function cannot fit the data well. Addressing these issues, we propose a novel IMC algorithm using a normalizing alignment strategy (IMCNAS) based on nonnegative matrix factorization. Specifically, the columns of the basis matrices are constrained into unit vector space, which integrates the feature normalization and the optimizing process, and makes the model converge fast and stable. On the other hand, this enables the model to fit the data better and produce more reasonable factorization results. Further, we develop a novel pairwise co-regularization to align incomplete multiple views more directly, without introducing a common consensus matrix like traditional centroid-based co-regularization. Graph regularization is also incorporated in the proposed model to utilize the geometrical information of data. We implement IMCNAS with a centroid-based regularization and a pairwise co-regularization respectively, and leads to two variants, i.e., IMCNAS-1 and IMCNAS-2. Both variants are optimized with multiplicative updating rules. Extensive experiments conducted on various real-world datasets comparing several state-of-the-art IMC methods verified the effectiveness of the proposed methods. The source code is available at: https://github.com/GuoshengCui/IMCNAS .},
  archive      = {J_TKDE},
  author       = {Guosheng Cui and Ruxin Wang and Dan Wu and Ye Li},
  doi          = {10.1109/TKDE.2022.3202561},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8126-8142},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incomplete multiview clustering using normalizing alignment strategy with graph regularization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HyperISO: Efficiently searching subgraph containment in
hypergraphs. <em>TKDE</em>, <em>35</em>(8), 8112–8125. (<a
href="https://doi.org/10.1109/TKDE.2022.3203856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching subgraph containment, also called subgraph matching in hypergraphs, is to enumerate all the embeddings of a data hypergraph with a given query hypergraph, which plays an important role in the analysis of hypergraph-modeled applications. However, existing subgraph matching frameworks mainly focus on pairwise graphs and the existing techniques can not efficiently be applied to search subgraph containment at low costs. Therefore, this paper proposes HyperISO to efficiently search subgraph containment that consists of three parts: 1) new filtering techniques driven by exploring the properties and connections of hyperedges to reduce unpromising products for the sake of low matching costs, 2) a novel ordering strategy that is able to generate an optimized matching process by considering both the sizes of hyperedge candidates and the unmatched vertices of the hyperedges, and 3) a dual enumeration algorithm to list both the vertex and hyperedge mappings. Extensive experiments on both real and synthetic data show that HyperISO outperforms the best among the sophisticated subgraph matching frameworks and meanwhile verify the efficiency of HyperISO in various types of hypergraphs.},
  archive      = {J_TKDE},
  author       = {Lingling Zhang and Zhiwei Zhang and Guoren Wang and Ye Yuan and Shuai Zhao and Jianliang Xu},
  doi          = {10.1109/TKDE.2022.3203856},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8112-8125},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HyperISO: Efficiently searching subgraph containment in hypergraphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human-in-the-loop rule discovery for micropost event
detection. <em>TKDE</em>, <em>35</em>(8), 8100–8111. (<a
href="https://doi.org/10.1109/TKDE.2022.3208345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platforms such as Twitter are increasingly being used for real-world event detection. Recent work often leverages event-related keywords for training machine learning based event detection models. These approaches make strong assumptions on the distribution of the relevant microposts containing the keyword – referred to as the expectation – and use it as a posterior regularization parameter during model training. Such approaches are, however, limited by the informativeness of the keywords and by the accuracy of the expectation estimation for keywords. In this work, we introduce a human-in-the-loop approach to jointly discover informative rules for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both rule-specific expectation and the disagreement between the crowd and the model in order to discover new rules that are most beneficial for model training. To identify such rules, we introduce a hybrid human-machine workflow that engages human workers in rule discovery through an interactive hypothesis creation and testing interface and leverages automatic methods for suggesting useful rules for human verification. We empirically demonstrate the merits of our approach, on multiple real-world datasets and show that our approach improves the state of the art by a margin of 25.63\% in terms of AUC.},
  archive      = {J_TKDE},
  author       = {Akansha Bhardwaj and Jie Yang and Philippe Cudré-Mauroux},
  doi          = {10.1109/TKDE.2022.3208345},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8100-8111},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Human-in-the-loop rule discovery for micropost event detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-dimensional multi-label data stream classification with
concept drifting detection. <em>TKDE</em>, <em>35</em>(8), 8085–8099.
(<a href="https://doi.org/10.1109/TKDE.2022.3200068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label data streams such as Web texts and images have been popular on the Web. These data present the characteristics of multiple label, high dimensionality, high volume, high velocity and especial concept drift etc. Thus, multi-label data stream classification is a very challenging and significant task especially in the handling of high-dimensional data with concept drifts. However, this challenge has received little attention from the research community. Therefore, we propose the max-relevance and min-redundancy based algorithm adaptation approach for the efficient and effective classification on multi-label data streams with high-dimensional attributes and concept drifts. In order to reduce the impact from the high-dimensional data with noisy attributes, we first refine the minimal-redundancy-maximal-relevance criterion based on mutual information to select qualified features in multi-label data streams. Second, we propose the data distribution based concept drifting detection approach to distinguish concept drifts hidden in data streams. Finally, we build an incremental ensemble classification model for efficiently classifying multi-label data streams. Extensive studies show that our approach can get optimal subsets of features while maintaining a good performance in the multi-label classification, as compared to several state-of-the-art multi-label feature selection algorithms using two efficient multi-label classification methods as base classifiers. Meanwhile, our approach is superior to three well-known multi-label data stream classification approaches in the effectiveness and efficiency.},
  archive      = {J_TKDE},
  author       = {Peipei Li and Haixiang Zhang and Xuegang Hu and Xindong Wu},
  doi          = {10.1109/TKDE.2022.3200068},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8085-8099},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {High-dimensional multi-label data stream classification with concept drifting detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph neural networks for missing value classification in a
task-driven metric space. <em>TKDE</em>, <em>35</em>(8), 8073–8084. (<a
href="https://doi.org/10.1109/TKDE.2022.3198689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete instances with various missing values in real-world scenes have brought challenges to the classification tasks. Many existing methods impute the incomplete instances based on their neighbouring instances before classification. However, the construction of such neighbourhood relationships in the original data space may become unreliable since missing values could disturb the traditional distance measurement. Moreover, these methods decouple the imputation from the classification, which makes it difficult for the former to learn from the supervised information, resulting in sub-optimal performance. To this end, this paper proposes graph neural networks for missing values classification (GNN4MV), which directly classify the incomplete instances based on their neighbourhood relationships constructed in a novel task-driven metric space. Specifically, the supervised information is taken as additional guidance in a task-driven metric space to reduce the impact of the missing values for neighbourhood relationship construction. Furthermore, a novel neighbourhood graph convolutional network is proposed in GNN4MV, which enables direct classification of the incomplete instances without imputation by utilizing the graph topology in the constructed neighbourhood relationships. Experiments on real-world datasets demonstrate the robustness and effectiveness of the proposed algorithm.},
  archive      = {J_TKDE},
  author       = {Buliao Huang and Yunhui Zhu and Muhammad Usman and Xiren Zhou and Huanhuan Chen},
  doi          = {10.1109/TKDE.2022.3198689},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8073-8084},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph neural networks for missing value classification in a task-driven metric space},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizing to unseen domains: A survey on domain
generalization. <em>TKDE</em>, <em>35</em>(8), 8052–8072. (<a
href="https://doi.org/10.1109/TKDE.2022.3178128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.},
  archive      = {J_TKDE},
  author       = {Jindong Wang and Cuiling Lan and Chang Liu and Yidong Ouyang and Tao Qin and Wang Lu and Yiqiang Chen and Wenjun Zeng and Philip S. Yu},
  doi          = {10.1109/TKDE.2022.3178128},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8052-8072},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Generalizing to unseen domains: A survey on domain generalization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GCCAD: Graph contrastive coding for anomaly detection.
<em>TKDE</em>, <em>35</em>(8), 8037–8051. (<a
href="https://doi.org/10.1109/TKDE.2022.3200459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based anomaly detection has been widely used for detecting malicious activities in real-world applications. Existing attempts to address this problem have thus far focused on structural feature engineering or learning in the binary classification regime. In this work, we propose to leverage graph contrastive learning and present the supervised GCCAD model for contrasting abnormal nodes with normal ones in terms of their distances to the global context (e.g., the average of all nodes). To handle scenarios with scarce labels, we further enable GCCAD as a self-supervised framework by designing a graph corrupting strategy for generating synthetic node labels. To achieve the contrastive objective, we design a graph neural network encoder that can infer and further remove suspicious links during message passing, as well as learn the global context of the input graph. We conduct extensive experiments on four public datasets, demonstrating that 1) GCCAD significantly and consistently outperforms various advanced baselines and 2) its self-supervised version without fine-tuning can achieve comparable performance with its fully supervised version.},
  archive      = {J_TKDE},
  author       = {Bo Chen and Jing Zhang and Xiaokang Zhang and Yuxiao Dong and Jian Song and Peng Zhang and Kaibo Xu and Evgeny Kharlamov and Jie Tang},
  doi          = {10.1109/TKDE.2022.3200459},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8037-8051},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GCCAD: Graph contrastive coding for anomaly detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Free energy node embedding via generalized skip-gram with
negative sampling. <em>TKDE</em>, <em>35</em>(8), 8024–8036. (<a
href="https://doi.org/10.1109/TKDE.2022.3206175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A widely established set of unsupervised node embedding methods can be interpreted as consisting of two distinctive steps: i) the definition of a similarity matrix based on the graph of interest followed by ii) an explicit or implicit factorization of such matrix. Inspired by this viewpoint, we propose improvements in both steps of the framework. On the one hand, we propose to encode node similarities based on the free energy distance, which interpolates between the shortest path and the commute time distances, thus, providing an additional degree of flexibility. On the other hand, we propose a matrix factorization method based on a loss function that generalizes that of the skip-gram model with negative sampling to arbitrary similarity matrices. Compared with factorizations based on the widely used $\ell _{2}$ loss, the proposed method can better preserve node pairs associated with higher similarity scores. Moreover, it can be easily implemented using advanced automatic differentiation toolkits and computed efficiently by leveraging GPU resources. Node clustering, node classification, and link prediction experiments on real-world datasets demonstrate the effectiveness of incorporating free-energy-based similarities as well as the proposed matrix factorization compared with state-of-the-art alternatives.},
  archive      = {J_TKDE},
  author       = {Yu Zhu and Ananthram Swami and Santiago Segarra},
  doi          = {10.1109/TKDE.2022.3206175},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8024-8036},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Free energy node embedding via generalized skip-gram with negative sampling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting fine-grained urban flows via spatio-temporal
contrastive self-supervision. <em>TKDE</em>, <em>35</em>(8), 8008–8023.
(<a href="https://doi.org/10.1109/TKDE.2022.3200734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a critical task of the urban traffic services, fine-grained urban flow inference (FUFI) benefits in many fields including intelligent transportation management, urban planning, public safety. FUFI is a technique that focuses on inferring fine-grained urban flows depending solely on observed coarse-grained data. However, existing methods always require massive learnable parameters and the complex network structures. To reduce these defects, we formulate a contrastive self-supervision method to predict fine-grained urban flows taking into account all correlated spatial and temporal contrastive patterns. Through several well-designed self-supervised tasks, uncomplicated networks have a strong ability to capture high-level representations from flow data. Then, a fine-tuning network combining with three pre-training encoder networks is proposed. We conduct experiments to evaluate our model and compare with other state-of-the-art methods by using two real-world datasets. All the empirical results not only show the superiority of our model against other comparative models, but also demonstrate its effectiveness in the resource-limited environment.},
  archive      = {J_TKDE},
  author       = {Hao Qu and Yongshun Gong and Meng Chen and Junbo Zhang and Yu Zheng and Yilong Yin},
  doi          = {10.1109/TKDE.2022.3200734},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {8008-8023},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Forecasting fine-grained urban flows via spatio-temporal contrastive self-supervision},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Finding the maximum <span
class="math inline"><em>k</em></span>-balanced biclique on weighted
bipartite graphs. <em>TKDE</em>, <em>35</em>(8), 7994–8007. (<a
href="https://doi.org/10.1109/TKDE.2022.3206351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bipartite graphs are widely used to capture the relationships between two types of entities. In bipartite graph analysis, finding the maximum balanced biclique (MBB) is an important problem with numerous applications. A biclique is balanced if its two disjoint vertex sets are of equal size. However, in real-world scenarios, each vertex is associated with a weight to denote its properties, such as influence, i.e., weighted bipartite graph. For weighted bipartite graphs, the previous studies for MBB are no longer applicable due to the ignorance of weight. To fill the gap, in this paper, we propose a reasonable definition of “balance” by restricting the weight difference between two sides of a biclique within $k$ . Given a weighted bipartite graph $G$ and a constraint $k$ , we aim to find the maximum $k$ -balanced biclique (Max $k$ BB) with the maximum weight. To address the problem, we first propose an approach based on biclique enumeration on single side of $G$ following the Branch-and-Bound framework. To improve the performance, we further devise three optimization strategies to prune invalid search branches. Moreover, we utilize graph reduction strategy to reduce the redundant search space. Extensive experiments are conducted on 12 real bipartite datasets to demonstrate the efficiency, effectiveness and scalability of our proposed algorithms. The experimental results show that our algorithms can address MBB detection problem efficiently, and the case study demonstrates the effectiveness of our model compared with MBB model.},
  archive      = {J_TKDE},
  author       = {Yiwei Zhao and Zi Chen and Chen Chen and Xiaoyang Wang and Xuemin Lin and Wenjie Zhang},
  doi          = {10.1109/TKDE.2022.3206351},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7994-8007},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Finding the maximum $k$-balanced biclique on weighted bipartite graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding high-quality item attributes for recommendation.
<em>TKDE</em>, <em>35</em>(8), 7980–7993. (<a
href="https://doi.org/10.1109/TKDE.2022.3209008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparse interactions between users and items on the web have aggravated the difficulty of their representations in recommender systems. Existing approaches leverage item attributes (e.g., item categories and tags) to alleviate the data sparsity problem, so as to enhance the performance and interpretability of recommendation. However, directly using all attributes of items cannot avoid the negative impacts of low-quality attributes, where manually labeling the quality of attributes is time-consuming. To this end, we propose HQRec to jointly measure the quality of attributes automatically and perform recommendation accurately. Specifically, we first analyze the different qualities among item attributes, and propose to leverage item categories to select high-quality tags via category-guided quality measurement and direction-aware optimization in an unsupervised fashion. Then, we propose to capture the complex relations among users and items based on the high-quality attributes, where a novel quality-aware embedding fusion and quality-aware embedding propagation mechanism for users and items is devised. Extensive experiments on four real-world benchmark datasets show drastic performance gains brought by our proposed HQRec framework, which constantly achieves an average of 14.73\% improvement over the state-of-the-art baselines in terms of Recall and NDCG metrics. Insightful case studies also show that our automatic quality measurements are highly accurate and interpretable.},
  archive      = {J_TKDE},
  author       = {Xiaolin Zheng and Yanchao Tan and Yan Wang and Xiangyu Wei and Shengjia Zhang and Chaochao Chen and Longfei Li and Carl Yang},
  doi          = {10.1109/TKDE.2022.3209008},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7980-7993},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Finding high-quality item attributes for recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Faster TKD: Towards lightweight decomposition for
large-scale tensors with randomized block sampling. <em>TKDE</em>,
<em>35</em>(8), 7966–7979. (<a
href="https://doi.org/10.1109/TKDE.2022.3218846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Tucker Decomposition (TKD) is able to provide the low-dimensional and informative representations of real-world large-scale tensorial data, which are necessary to extract potential features and enhance the original data. However, computing such decomposition directly for a dense tensor is usually computationally elusive, due to the repetitive operations of computing large-scale tensor-matrix product. Instead of direct decomposition, this paper proposes an efficient algorithm for seeking the Faster TKD of the large-scale tensor, which is a lightweight decomposition approach based on the technique of randomized sampling. The proposed algorithm first converts the original large-scale tensor into a small-scale subtensor via full-mode sampling operation, and then the core tensor of TKD can be computed directly based on the subtensor with low complexity. Finally, an approximate TKD of the original large-scale tensor can be obtained after sequentially computing approximate full-mode factor matrices. A theoretical error analysis is provided to show that the approximation error approximates zero with high probability, and the proposed algorithm is verified based on real tensorial data of $23821.24GB$ .},
  archive      = {J_TKDE},
  author       = {Xiaofeng Jiang and Xiaodong Wang and Jian Yang and Shuangwu Chen and Xi Qin},
  doi          = {10.1109/TKDE.2022.3218846},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7966-7979},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Faster TKD: Towards lightweight decomposition for large-scale tensors with randomized block sampling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Exploring privileged features for relation extraction with
contrastive student-teacher learning. <em>TKDE</em>, <em>35</em>(8),
7953–7965. (<a href="https://doi.org/10.1109/TKDE.2022.3161584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has been made by joint entity and relation extraction methods, which directly generate the relation triplets and mitigate the issue of overlapping relations. However, previous models generate the entity-relation triplets solely from input sentences. Such information is insufficient to support the modeling of interactive information between entities and relations. In this paper, we define the features that provide mutual supports for entity and relation detection but can only be accessed at training time as privileged features for relation extraction, and devise two teacher models to exploit privileged entity and relation features, respectively. Meanwhile, we propose a novel contrastive s tudent- t eacher learning framework for joint extraction of e ntities and r elations (STER), where a student network is encouraged to amalgamate privileged knowledge from two expert teacher networks that additionally utilize the privileged features, based on contrastive learning. Experiment results on three benchmark datasets (i.e., ADE, SciERC and CoNLL04) demonstrate that STER has robust superiority over competitors and sets state-of-the-art. For reproducibility, we release the data and source code at: https://github.com/siat-nlp/STER .},
  archive      = {J_TKDE},
  author       = {Xiaoyan Zhao and Min Yang and Qiang Qu and Ruifeng Xu and Jieke Li},
  doi          = {10.1109/TKDE.2022.3161584},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7953-7965},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Exploring privileged features for relation extraction with contrastive student-teacher learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Evaluation of reachability queries based on recursive DAG
decomposition. <em>TKDE</em>, <em>35</em>(8), 7935–7952. (<a
href="https://doi.org/10.1109/TKDE.2022.3220191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let G ( V , E ) be a digraph (directed graph) with n nodes and e arcs. Digraph G* = ( V , E* ) is the reflexive, transitive closure if v → u ∈ E* iff there is a path from v to u in G . Efficient storage of G* is important for supporting reachability queries which are not only common in graph databases, but also serve as fundamental operations used in many graph algorithms. A lot of strategies have been proposed based on the graph labeling, by which each node is assigned with certain labels such that the reachability of any two nodes through a path can be determined by their labels. Among them are interval labeling, chain decomposition, 2-hop labeling, and path-trees, as well as partial index based methods. However, due to the very large size of many real world graphs, the computational cost and size of labels using existing methods would prove too expensive to be practical. In this paper, we propose a new approach to deduct and decompose a graph into a series of spanning trees and transform a query q to a series of subqueries each evaluated against a spanning tree. Using the so-called tree labeling, each subquery needs only O(1) time. More importantly, the number of such subqueries is $\ll$ n . Thus, q can be evaluated very efficiently. We demonstrate both analytically and empirically the efficiency and effectiveness of our method. While the query time of our method is orders of magnitude better than almost all the existing strategies, its indexing time and index sizes are comparable to them.},
  archive      = {J_TKDE},
  author       = {Yangjun Chen and Yibin Chen and Yifeng Zhang},
  doi          = {10.1109/TKDE.2022.3220191},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7935-7952},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Evaluation of reachability queries based on recursive DAG decomposition},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient maximum edge-weighted biclique search on large
bipartite graphs. <em>TKDE</em>, <em>35</em>(8), 7921–7934. (<a
href="https://doi.org/10.1109/TKDE.2022.3220901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a bipartite graph, the maximum edge biclique problem ( ${{\sf MEB}}$ ) aims to find a biclique with the largest number of edges. ${{\sf MEB}}$ is a fundamental problem with many real applications, such as community analysis, E-commerce services and bioinformatics. However, in some scenarios, the weight of an edge reflects valuable and important information on the relationship between two entities. Motivated by this, in this paper, we investigate the problem of maximum edge-weighted biclique search ( ${{\sf MEWB}}$ ), which finds a biclique with the largest total weight of edges in a weighted bipartite graph. ${{\sf MEWB}}$ has many real applications, including item recommendation, fraud detection, gene clustering, etc. Although we show that ${{\sf MEWB}}$ can be resolved by adapting the search algorithm designed for ${{\sf MEB}}$ , the performance of this method is yet unsatisfactory. To improve the computation efficiency, two optimizations in terms of upper bound and search order are proposed. For the upper bound, we consider the degree distribution for vertices in the candidate set, and thus have a chance to discard a few edges to tighten the upper bound. For search order, we theoretically show that a vertex order generating the most similar search depth on vertices can achieve the least time cost for ${{\sf MEWB}}$ . Guided by this fact, we propose the global summation vertex order. To further accelerate the computation, we extend our approach to a parallel environment, and develop a heuristic approach to deal with large-scale graphs by slightly sacrificing the answer quality. Extensive performance studies conducted on real datasets demonstrate that our proposals can significantly outperform the baseline method by up to two orders of magnitude. Besides, our heuristic approach gives the optimal result on 8 out of 10 real datasets, while achieving more than an order of magnitude of speed-up.},
  archive      = {J_TKDE},
  author       = {Jianhua Wang and Jianye Yang and Chengyuan Zhang and Xuemin Lin},
  doi          = {10.1109/TKDE.2022.3220901},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7921-7934},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient maximum edge-weighted biclique search on large bipartite graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective clustering via structured graph learning.
<em>TKDE</em>, <em>35</em>(8), 7909–7920. (<a
href="https://doi.org/10.1109/TKDE.2022.3222411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an affinity graph of data samples, graph-based clustering aims to partition these samples into disjoint groups based on the affinities, and most previous works are based on spectral clustering. However, two problems among spectral-based methods heavily affect the clustering performance. First, the randomness of post-processing procedures, such as $K$ -means, affects the stability of clustering. Second, the separated stages of spectral-based methods, including graph construction, spectral embedding learning, and clustering decision, lead to mismatched problems. In this paper, we explore a structured graph learning (SGL) framework that aims to fuse these stages to improve clustering stability. Specifically, SGL adaptively learns a structured affinity graph that contains exact $k$ connected components. Each connected component corresponds to a cluster so clustering assignments can be directly obtained according to the connectivity of the learned graph. In this way, SGL avoids the randomness brought by reliance on traditional post-processing procedures. Meanwhile, the graph construction and structured graph learning procedures happen simultaneously, which alleviates the mismatched problem effectively. Moreover, we propose an efficient algorithm to solve the involved optimization problems and discuss the connections between this work and previous works. Numerical experiments on several synthetic and real datasets demonstrate the effectiveness of our methods.},
  archive      = {J_TKDE},
  author       = {Danyang Wu and Feiping Nie and Jitao Lu and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2022.3222411},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7909-7920},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Effective clustering via structured graph learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distribution knowledge embedding for graph pooling.
<em>TKDE</em>, <em>35</em>(8), 7898–7908. (<a
href="https://doi.org/10.1109/TKDE.2022.3208063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-level representation learning is the pivotal step for downstream tasks that operate on the whole graph. The most common approach to this problem is graph pooling, where node features are typically averaged or summed to obtain the graph representations. However, pooling operations like averaging or summing inevitably cause severe information missing, which may severely downgrade the final performance. In this article, we argue what is crucial to graph-level downstream tasks includes not only the topological structure but also the distribution from which nodes are sampled. Therefore, powered by existing Graph Neural Networks (GNN), we propose a new plug-and-play pooling module, termed as Distribution Knowledge Embedding (DKEPool), where graphs are viewed as distributions on top of GNNs and the pooling goal is to summarize the entire distribution information instead of retaining a certain feature vector by simple predefined pooling operations. A DKEPool network de facto disassembles representation learning into two stages, structure learning and distribution learning . Structure learning follows a recursive neighborhood aggregation scheme to update node features where structure information is obtained. Distribution learning, on the other hand, omits node interconnections and focuses more on the distribution depicted by all the nodes. Extensive experiments on graph classification tasks demonstrate that the proposed DKEPool significantly and consistently outperforms the state-of-the-art methods. The code is avaliable at https://github.com/chenchkx/dkepool},
  archive      = {J_TKDE},
  author       = {Kaixuan Chen and Jie Song and Shunyu Liu and Na Yu and Zunlei Feng and Gengshi Han and Mingli Song},
  doi          = {10.1109/TKDE.2022.3208063},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7898-7908},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Distribution knowledge embedding for graph pooling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disentangling geographical effect for point-of-interest
recommendation. <em>TKDE</em>, <em>35</em>(8), 7883–7897. (<a
href="https://doi.org/10.1109/TKDE.2022.3221873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point-of-Interest (POI) recommendation has drawn a lot of attention in both academia and industry. It utilizes user check-in data, aiming at recommending unvisited POIs to users. To address the data-sparsity problem, geographical information of POIs is often incorporated into recommender systems. However, most of the existing approaches model geographical impact in an implicit way, in which geographical information is encoded as auxiliary vectors for learning unified representations of users and POIs. Following this paradigm, the embedding of POIs can not reflect geographical similarity directly; thus, an explicit modeling approach is needed as geography is of great importance in POI recommendation. To address challenges in disentangling geographical effect, we proposed a disentangled representation learning method named DIG (short for D isentangled embedding of user I nterest and POIs’ G eographical information). Aiming at decoupling the geographical factor and the user interest factor thoroughly, we first proposed a geo-constrained negative sampling strategy, which helps to find reliable negative samples for the two factors. Second, a geo-enhanced soft-weighted loss function was proposed to quantify the trade-off between the two factors in loss computation. Extensive experiments have been conducted on two real-world datasets, and results have demonstrated the significant improvement of DIG at $3.92\% \sim 20.32\%$ on recall, and $2.53\%\sim 11.48\%$ on hit ratio, compared with other state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Yingrong Qin and Chen Gao and Yue Wang and Shuangqing Wei and Depeng Jin and Jian Yuan and Lin Zhang},
  doi          = {10.1109/TKDE.2022.3221873},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7883-7897},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Disentangling geographical effect for point-of-interest recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disentangled graph neural networks for session-based
recommendation. <em>TKDE</em>, <em>35</em>(8), 7870–7882. (<a
href="https://doi.org/10.1109/TKDE.2022.3208782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation (SBR) has drawn increasingly research attention in recent years, due to its great practical value by only exploiting the limited user behavior history in the current session. The key of SBR is to accurately infer the anonymous user purpose in a session which is typically represented as session embedding, and then match it with the item embeddings for the next item prediction. Existing methods typically learn the session embedding at the item level, namely, aggregating the embeddings of items with or without assigned attention weights to items. However, they ignore the fact that a user&#39;s intent on adopting an item is driven by certain factors of the item (e.g., the leading actors of an movie). In other words, they have not explored finer-granularity interests of users at the factor level to generate the session embedding, leading to sub-optimal performance. To address the problem, we propose a novel method called Disentangled Graph Neural Network (Disen-GNN) to capture the session purpose with the consideration of factor-level attention on each item. Specifically, we first employ the disentangled learning technique to cast item embeddings into the embeddings of multiple factors, and then use the gated graph neural network (GGNN) to learn the embedding factor-wisely based on the item adjacent similarity matrix computed for each factor. Moreover, the distance correlation is adopted to enhance the independence between each pair of factors. After representing each item with independent factors, an attention mechanism is designed to learn user intent to different factors of each item in the session. The session embedding is then generated by aggregating the item embeddings with attention weights of each item&#39;s factors. To this end, our model takes user intents at the factor level into account to infer the user purpose in a session. Extensive experiments on three benchmark datasets demonstrate the superiority of our method over existing methods.},
  archive      = {J_TKDE},
  author       = {Ansong Li and Zhiyong Cheng and Fan Liu and Zan Gao and Weili Guan and Yuxin Peng},
  doi          = {10.1109/TKDE.2022.3208782},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7870-7882},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Disentangled graph neural networks for session-based recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disentangled graph contrastive learning with independence
promotion. <em>TKDE</em>, <em>35</em>(8), 7856–7869. (<a
href="https://doi.org/10.1109/TKDE.2022.3206875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning for graph neural networks has attracted considerable attention and shows notable successes in graph representation learning. However, the formation of a real-world graph typically arises from highly complex interactions of many latent factors. The existing self-supervised learning methods for GNNs are inherently holistic and neglect the entanglement of the latent factors, resulting in suboptimal learned representations for downstream tasks and difficult to be interpreted. Learning disentangled graph representations with self-supervised learning poses great challenges and remains largely ignored by the existing literature. In this paper, we introduce Independence Promoted Disentangled Graph Contrastive Learning ( IDGCL ) method, which can learn disentangled graph-level representations with self-supervision. In particular, we first identify the latent factors of the input graph and derive its factorized representations. Then we propose a factor-wise discrimination objective in a contrastive learning manner, which can force the factorized representations to independently reflect the expressive information from different latent factors. To further promote the independence between the representations, we employ the Hilbert-Schmidt Independence Criterion to eliminate the dependence among different representations, which is effectively integrated into the self-supervised framework as a regularizer. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of our method against several state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Haoyang Li and Ziwei Zhang and Xin Wang and Wenwu Zhu},
  doi          = {10.1109/TKDE.2022.3206875},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7856-7869},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Disentangled graph contrastive learning with independence promotion},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep graph memory networks for forgetting-robust knowledge
tracing. <em>TKDE</em>, <em>35</em>(8), 7844–7855. (<a
href="https://doi.org/10.1109/TKDE.2022.3206447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracing a student&#39;s knowledge is vital for tailoring the learning experience. Recent knowledge tracing methods tend to respond to these challenges by modelling knowledge state dynamics across learning concepts. However, they still suffer from several inherent challenges including: modelling forgetting behaviors and identifying relationships among latent concepts. To address these challenges, in this paper, we propose a novel knowledge tracing model, namely Deep Graph Memory Network (DGMN). In this model, we incorporate a forget gating mechanism into an attention memory structure in order to capture forgetting behaviors dynamically during the knowledge tracing process. Particularly, this forget gating mechanism is built upon attention forgetting features over latent concepts considering their mutual dependencies. Further, this model has the capability of learning relationships between latent concepts from a dynamic latent concept graph in light of a student&#39;s evolving knowledge states. A comprehensive experimental evaluation has been conducted using four well-established benchmark datasets. The results show that DGMN consistently outperforms the state-of-the-art KT models over all the datasets. The effectiveness of modelling forgetting behaviors and learning latent concept graphs has also been analyzed in our experiments.},
  archive      = {J_TKDE},
  author       = {Ghodai Abdelrahman and Qing Wang},
  doi          = {10.1109/TKDE.2022.3206447},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7844-7855},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep graph memory networks for forgetting-robust knowledge tracing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain meta-learner for cold-start recommendation.
<em>TKDE</em>, <em>35</em>(8), 7829–7843. (<a
href="https://doi.org/10.1109/TKDE.2022.3208005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cold-start problem is a major factor that limits the effectiveness of recommendation systems. Having too few available interaction records brings a series of challenges when predicting user preferences. At present, there are two main kinds of strategies for solving this problem from different perspectives. One is cross-domain recommendation (CDR), which introduces additional information by domain knowledge propagation with transfer learning. However, CDR methods follow traditional training processes in machine learning and cannot solve this typical few-shot problem from the perspective of optimization. The other type of methods that has recently emerged is based on meta-learning. Most of these approaches focus only on generating a meta-model to perform better on new tasks and ignore improvements based on cross-domain information. Therefore, it is necessary to design a novel approach to solve this problem with both domain knowledge and meta-optimization. To achieve this goal, a novel cross-domain meta-learner for cold-start recommendation (MetaCDR) is proposed. In MetaCDR, we design a domain knowledge meta-transfer module to connect different domain networks. In addition, we introduce a pretraining strategy to ensure its efficiency. The experimental results show that MetaCDR performs significantly better than state-of-the-art models in a variety of scenarios.},
  archive      = {J_TKDE},
  author       = {Renchu Guan and Haoyu Pang and Fausto Giunchiglia and Yanchun Liang and Xiaoyue Feng},
  doi          = {10.1109/TKDE.2022.3208005},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7829-7843},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-domain meta-learner for cold-start recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous geo-social group monitoring in dynamic LBSNs.
<em>TKDE</em>, <em>35</em>(8), 7815–7828. (<a
href="https://doi.org/10.1109/TKDE.2022.3218844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geo-social group queries, which return a social cohesive user group with a spatial constraint, have receive significant research interests due to their promising applications for group-based activity planning and scheduling in location-based social networks (LBSNs). However, existing studies on geo-social group queries mostly assume the users are stationary whereas in realistic LBSN application scenarios all users may continuously move over time. Thus, in this paper, we investigate the problem of continuous geo-social groups monitoring ( CGSGM ) over moving users. A challenge in answering CGSGM queries over moving users is how to efficiently update geo-social groups when users are continuously moving. To address the CGSGM problem, we first propose a baseline algorithm, namely Baseline-BB , which recomputes the new geo-social groups from scratch at each time instance by utilizing a branch and bound (BB) strategy. To improve the inefficiency of BB, we explore a new strategy, called common neighbor or neighbor expanding (CNNE), which expands the common neighbors of edges or the neighbors of users in intermediate groups to quickly produce the valid group combinations. Accordingly, another baseline algorithm, namely Baseline-CNNE , is proposed. As these baseline algorithms do not maintain intermediate results to facilitate further query processing, we develop an incremental algorithm, called incremental monitoring algorithm (IMA) , which maintains the support, common neighbors and the neighbors of current users when exploring possible user groups for further updates and query processing. Since IMA requires many times of truss decomposition when processing mutiple-users updates, we propose an improved incremental algorithm, called improved incremental monitoring algorithm (IIMA) , which performs truss decompostion only once. Moreover, we design algorithms for handling the social changes that result in insertion/deletion of some edges in the social network. Owing to the challenge in setting, an appropriate monitoring distance, we further study the top $N$ CGSGM problem, which finds top $N$ result groups at each time instance. Finally, we conduct extensive experiments using four real datasets to validate our ideas and evaluate the proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Huaijie Zhu and Wei Liu and Jian Yin and Libin Zheng and Xin Huang and Jianliang Xu and Wang-Chien Lee},
  doi          = {10.1109/TKDE.2022.3218844},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7815-7828},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Continuous geo-social group monitoring in dynamic LBSNs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-evolution of viral processes and structural stability in
signed social networks. <em>TKDE</em>, <em>35</em>(8), 7809–7814. (<a
href="https://doi.org/10.1109/TKDE.2022.3207123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction and control of spreading processes in social networks (SNs) are closely tied to the underlying connectivity patterns. Contrary to most existing efforts that exclusively focus on positive social user interactions, the impact of contagion processes on the temporal evolution of signed SNs (SSNs) with distinctive friendly (positive) and hostile (negative) relationships yet, remains largely unexplored. In this paper, we study the interplay between social link polarity and propagation of viral phenomena coupled with user alertness. In particular, we propose a novel energy model built on Heider&#39;s balance theory that relates the stochastic susceptible-alert-infected-susceptible epidemic dynamical model with the structural balance of SSNs to substantiate the trade-off between social tension and epidemic spread. Moreover, the role of hostile social links in the formation of disjoint friendly clusters of alerted and infected users is analyzed. Using three real-world SSN datasets, we further present a time-efficient algorithm to expedite the energy computation in our Monte-Carlo simulation method and show compelling insights on the effectiveness and rationality of user awareness and initial network settings in reaching structurally balanced local and global network energy states.},
  archive      = {J_TKDE},
  author       = {Temirlan Kalimzhanov and Amir Haji Ali Khamseh’i and Aresh Dadlani and Muthukrishnan Senthil Kumar and Ahmad Khonsari},
  doi          = {10.1109/TKDE.2022.3207123},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7809-7814},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Co-evolution of viral processes and structural stability in signed social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Authorized update in multi-user homomorphic encrypted cloud
database. <em>TKDE</em>, <em>35</em>(8), 7796–7808. (<a
href="https://doi.org/10.1109/TKDE.2022.3221148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud Computing is a promising solution in distributed internet computing for IT and scientific research. However, data storage and computing in the cloud domain raise added questions in terms of security. Data encrypted with traditional encryption schemes may confirm confidentiality, but computation in cloud domain becomes infeasible. This paper focuses on designing an encrypted database considering homomorphic encryption (HE) as an underlying scheme so that query execution is carried out on encrypted version of the database without any need of intermediate decryption. Existing encrypted databases are either based on partial HE or deterministic HE to achieve practical performance; hence, they are either limited in terms of types of query execution or prone to known attacks. To mitigate these issues, we explore the practical challenges of a fully homomorphic encryption (FHE)-based database design. FHE theoretically promises to perform arbitrary operations on encrypted data. However, realizing any algorithm in homomorphic domain requires a circuit-based representation of that specific algorithm, which is a non-trivial task. In this work, we explore the practical challenges of FHE database design, mostly in the case of multi-user organizational scenarios, and propose a scheme for secure modification or conditional update of the encrypted database. Moreover, when an organization outsources a database to the cloud, a single FHE key is used to encrypt all columns of the database. However, all users(or employees) of the same organization should not have equal read and write access permission to the whole database. In this work, we propose an architecture to apply Attribute-Based Access Control (ABAC) on FHE databases with minimum overhead in terms of performance and storage. We propose required changes in registration, login, and user revocation phases for our scheme to perform conditional SQL query processing on FHE encrypted database (EDB). Our proposed framework is capable of performing end-to-end encrypted conditional UPDATE with suitable access control within 17 minutes on a multi-core processor platform for 769 rows and 9 columns of database size with 16-bit size of data. To the best of our knowledge, our proposed technique is the first one in literature to support arbitrary secure encrypted SQL query execution with suitable access control.},
  archive      = {J_TKDE},
  author       = {Tanusree Parbat and Ayantika Chatterjee},
  doi          = {10.1109/TKDE.2022.3221148},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7796-7808},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Authorized update in multi-user homomorphic encrypted cloud database},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention over self-attention: Intention-aware re-ranking
with dynamic transformer encoders for recommendation. <em>TKDE</em>,
<em>35</em>(8), 7782–7795. (<a
href="https://doi.org/10.1109/TKDE.2022.3208633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Re-ranking models refine item recommendation lists generated by the prior global ranking model, which have demonstrated their effectiveness in improving the recommendation quality. However, most existing re-ranking solutions only learn from implicit feedback with a shared prediction model, which regrettably ignore inter-item relationships under diverse user intentions. In this paper, we propose a novel Intention-awa r e Re-r a nking Model with Dynam i c Tran s former E ncoder (RAISE), aiming to perform user-specific prediction for each individual user based on her intentions. Specifically, we first propose to mine latent user intentions from text reviews with an intention discovering module (IDM). By differentiating the importance of review information with a co-attention network, the latent user intention can be explicitly modeled for each user-item pair. We then introduce a dynamic transformer encoder (DTE) to capture user-specific inter-item relationships among item candidates by seamlessly accommodating the learned latent user intentions via IDM. As such, one can not only achieve more personalized recommendations but also obtain corresponding explanations by constructing RAISE upon existing recommendation engines. Empirical study on four public datasets shows the superiority of our proposed RAISE, with up to 13.95\%, 9.60\%, and 13.03\% relative improvements evaluated by Precision@5, MAP@5, and NDCG@5 respectively.},
  archive      = {J_TKDE},
  author       = {Zhuoyi Lin and Sheng Zang and Rundong Wang and Zhu Sun and J. Senthilnath and Chi Xu and Chee Keong Kwoh},
  doi          = {10.1109/TKDE.2022.3208633},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7782-7795},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Attention over self-attention: Intention-aware re-ranking with dynamic transformer encoders for recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Aspect-guided syntax graph learning for explainable
recommendation. <em>TKDE</em>, <em>35</em>(8), 7768–7781. (<a
href="https://doi.org/10.1109/TKDE.2022.3221847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable recommendation systems provide explanations for recommendation results to improve their transparency and persuasiveness. The existing explainable recommendation methods generate textual explanations without explicitly considering the user&#39;s preferences on different aspects of the item. In this paper, we propose a novel explanation generation framework, namely A spect-guided E xplanation generation with S yntax G raph ( AESG ), for explainable recommendation. Specifically, AESG employs a review-based syntax graph to provide a unified view of the user/item details. An aspect-guided graph pooling operator is proposed to extract the aspect-relevant information from the review-based syntax graphs to model the user&#39;s preferences on an item at the aspect level. Then, an aspect-guided explanation decoder is developed to generate aspects and aspect-relevant explanations based on the attention mechanism. The experimental results on three real datasets indicate that AESG outperforms state-of-the-art explanation generation methods in both single-aspect and multi-aspect explanation generation tasks, and also achieves comparable or even better preference prediction accuracy than strong baseline methods.},
  archive      = {J_TKDE},
  author       = {Yidan Hu and Yong Liu and Chunyan Miao and Gongqi Lin and Yuan Miao},
  doi          = {10.1109/TKDE.2022.3221847},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7768-7781},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Aspect-guided syntax graph learning for explainable recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Analyzing preference data with local privacy: Optimal
utility and enhanced robustness. <em>TKDE</em>, <em>35</em>(8),
7753–7767. (<a href="https://doi.org/10.1109/TKDE.2022.3207486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online service providers benefit from collecting and analyzing preference data from users, including both implicit preference data (e.g., watched videos of a user) and explicit preference data (e.g., ranking data over candidates). However, it brings ethical and legal issues of data privacy at the same time. In this paper, we study the problem of aggregating individual&#39;s preference data in the local differential privacy (LDP) setting. One naive approach is to add Laplace random noises, which however suffers from low statistical utility and is fragile to LDP-specific poisoning attacks. Therefore, we propose a novel mechanism to improve the utility and the robustness simultaneously: the additive mechanism . The additive mechanism randomly outputs a subset of candidates with a probability proportional to their total scores. For preference data with Borda rule over $d$ items, its mean squared error bound is optimized from $O(\frac{d^{5}}{n\epsilon ^{2}})$ to $O(\frac{d^{4}}{n\epsilon ^{2}})$ , and its maximum poisoning risk bound is reduced from $+\infty$ to $O(\frac{d^{2}}{n\epsilon })$ . We also theoretically investigate minimax lower bounds of $\epsilon$ -LDP preference data aggregation, and prove the error rate of $O(\frac{d^{4}}{n\epsilon ^{2}})$ is optimal for the Borda rule. Experimental results validate that our proposed approaches averagely reduce estimation error by $50\%$ and are more robust to adversarial poisoning attacks.},
  archive      = {J_TKDE},
  author       = {Shaowei Wang and Xuandi Luo and Yuqiu Qian and Jiachun Du and Wenqing Lin and Wei Yang},
  doi          = {10.1109/TKDE.2022.3207486},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7753-7767},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Analyzing preference data with local privacy: Optimal utility and enhanced robustness},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An inverse-free block-SOR method with randomly sampling for
temporal multiplex PageRank problems. <em>TKDE</em>, <em>35</em>(8),
7736–7752. (<a href="https://doi.org/10.1109/TKDE.2022.3197920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranking nodes in a multiplex network is one of the most pressing and challenging tasks in network analysis. Generalizing centrality measures to multiplex networks is an active area of research. The Multiplex PageRank model is an extension of Google&#39;s PageRank, which introduces a new centrality measure to extend the usual PageRank to multiplex networks. In this work, we focus on the Multiplex PageRank problem. First, based on the special structure of the Multiplex PageRank problem, we propose an inverse-free block-SOR method. Second, with the help of randomly sampling, we propose a new strategy for estimating the optimal relaxation parameter. Specifically, the multiplex network is frequently updated in real world applications, and we have to deal with temporal Multiplex PageRank problems including the incremental and the decremental Multiplex PageRank problems. To the best of our knowledge, however, there are few efficient algorithms for solving these type of problems. To fill-in this gap, the third contribution of this work is to propose both incremental and decremental algorithms for solving the temporal Multiplex PageRank problems. Comprehensive numerical experiments are performed to illustrate the numerical behavior of the proposed algorithms, and show the effectiveness of our new strategies.},
  archive      = {J_TKDE},
  author       = {Gang Wu and Keke Peng},
  doi          = {10.1109/TKDE.2022.3197920},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7736-7752},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An inverse-free block-SOR method with randomly sampling for temporal multiplex PageRank problems},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient and accurate rough set for feature selection,
classification, and knowledge representation. <em>TKDE</em>,
<em>35</em>(8), 7724–7735. (<a
href="https://doi.org/10.1109/TKDE.2022.3220200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a strong data-mining method based on a rough set, which can simultaneously realize feature selection, classification, and knowledge representation. Although a rough set, a popular method for feature selection, has good interpretability, it is not sufficiently efficient and accurate to deal with large-scale datasets with high dimensions, which prevents it from being immediately applied to real-world scenarios. To address the efficiency issue of a rough set, we discover the stability of the local redundancy (SLR) of attributes and propose a theorem to prove it rigorously. Based on SLR, only the parts of objects in the boundary region are partitioned when calculating outer significance, which further improves the efficiency of the rough set. With regard to the accuracy issue, we show that overfitting may lead to ineffectiveness of the rough set, especially when processing noise attributes. We then propose relative importance, a robust measurement for an attribute, to alleviate such overfitting issues. In this paper, we propose a novel rough-set framework that significantly improves the efficiency and accuracy of existing rough-set methods. We further develop our rough set framework by proposing a “rough concept tree” for knowledge representation and classification. Experimental results on public benchmark datasets show that our proposed framework achieves higher accuracy than seven state-of-the-art feature-selection methods. All the codes are available at https://github.com/syxiaa/powerroughset .},
  archive      = {J_TKDE},
  author       = {Shuyin Xia and Xinyu Bai and Guoyin Wang and Yunlong Cheng and Deyu Meng and Xinbo Gao and Yujia Zhai and Elisabeth Giem},
  doi          = {10.1109/TKDE.2022.3220200},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7724-7735},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An efficient and accurate rough set for feature selection, classification, and knowledge representation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aligning spatially constrained graphs. <em>TKDE</em>,
<em>35</em>(8), 7712–7723. (<a
href="https://doi.org/10.1109/TKDE.2022.3206823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on the problem of aligning graphs that have a spatial basis. In such graphs, which we refer to as rigid graphs , nodes have preferred positions relative to their graph neighbors. Rigid graphs can be used to abstract objects in diverse applications such as large biomolecules, where edges corresponding to chemical bonds have preferred lengths, functional connectomes of the human brain, where edges corresponding to co-firing regions of the brain have preferred anatomical distances, and mobile device/ sensor communication logs, where edges corresponding to point-to-point communications across devices have distance constraints. Effective analysis of such graphs must account for edge lengths in addition to topological features. For instance, when identifying conserved patterns through graph alignment, it is important for matched edges to have correlated lengths, in addition to topological similarity. In this paper, we formulate the problem of rigid graph alignment and present a method for solving it. Our formulation of rigid graph alignment simultaneously aligns the topology of the input graphs, as well as the geometric structure represented by the edge lengths, which is solved using a block coordinate descent technique. Using detailed experiments on real and synthetic datasets, we demonstrate a number of important desirable features of our method: (i) it significantly outperforms topological and structural aligners on a wide range of problems; (ii) it scales to problems in important real-world applications; and (iii) it has excellent stability properties, in view of noise and missing data in typical applications.},
  archive      = {J_TKDE},
  author       = {Vikram Ravindra and Huda Nassar and David F. Gleich and Ananth Grama},
  doi          = {10.1109/TKDE.2022.3206823},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7712-7723},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Aligning spatially constrained graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial attack and defense on graph data: A survey.
<em>TKDE</em>, <em>35</em>(8), 7693–7711. (<a
href="https://doi.org/10.1109/TKDE.2022.3201243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. Though there are several works about adversarial attack and defense strategies on domains such as images and natural language processing, it is still difficult to directly transfer the learned knowledge to graph data due to its representation structure. Given the importance of graph analysis, an increasing number of studies over the past few years have attempted to analyze the robustness of machine learning models on graph data. Nevertheless, existing research considering adversarial behaviors on graph data often focuses on specific types of attacks with certain assumptions. In addition, each work proposes its own mathematical formulation, which makes the comparison among different methods difficult. Therefore, this review is intended to provide an overall landscape of more than 100 papers on adversarial attack and defense strategies for graph data, and establish a unified formulation encompassing most graph adversarial learning models. Moreover, we also compare different graph attacks and defenses along with their contributions and limitations, as well as summarize the evaluation metrics, datasets and future trends. We hope this survey can help fill the gap in the literature and facilitate further development of this promising new field 1 .},
  archive      = {J_TKDE},
  author       = {Lichao Sun and Yingtong Dou and Carl Yang and Kai Zhang and Ji Wang and Philip S. Yu and Lifang He and Bo Li},
  doi          = {10.1109/TKDE.2022.3201243},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7693-7711},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adversarial attack and defense on graph data: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A time impulse neural network framework for solving the
minimum path pair problems of the time-varying network. <em>TKDE</em>,
<em>35</em>(8), 7681–7692. (<a
href="https://doi.org/10.1109/TKDE.2022.3217394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum path pair (MPP) query problem is to find the optimal meeting point of two minimum paths for two users in a network, where each user&#39;s minimum path has its own departure and destination nodes. However, the MPP query problem on a time-varying network that generally exists in the world remains open. In this study, we investigate the time-varying minimum path pair (TMPP) query problem, where the arcs of the network are time dependent. We first model the TMPP and then propose a time impulse neural network (TINN) to solve the TMPP. In the design of the TINN, the entire network topology is considered as the architecture of the neural network, and each node is viewed as a neuron. The core of an impulse-based neuron consists of six parts: input, impulse receiver, time window selector, neuron storage, impulse sender, and output. Unlike the traditional neural network, the whole neural network does not require a training process but is implemented through an impulse mechanism. The TINN consists of two stages; the first stage is to find the two minimum paths of two users, while the second stage is to calculate the distance between these two minimum paths. The underlying idea of the TINN is to find the minimum path using an impulse mechanism. The minimum path relies on the earliest time impulse stemming from the depart node that arrives at the destination node. With this mechanism, both the minimum path of the network and the distance between two paths can be addressed. Furthermore, theoretical analysis demonstrates the correctness and provides the complexity of the TINN. Experiments of the TINN are carried out based on the well-known New York City Map. A comparative study illustrates the effectiveness of the TINN.},
  archive      = {J_TKDE},
  author       = {Wei Huang and Yongqing Wang and Liehuang Zhu},
  doi          = {10.1109/TKDE.2022.3217394},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7681-7692},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A time impulse neural network framework for solving the minimum path pair problems of the time-varying network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-view multi-task learning framework for multi-variate
time series forecasting. <em>TKDE</em>, <em>35</em>(8), 7665–7680. (<a
href="https://doi.org/10.1109/TKDE.2022.3218803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-variate time series (MTS) data is a ubiquitous class of data abstraction in the real world. Any instance of MTS is generated from a hybrid dynamical system and their specific dynamics are usually unknown. The hybrid nature of such a dynamical system is a result of complex external attributes, such as geographic location and time of day, each of which can be categorized into either spatial attributes or temporal attributes. Therefore, there are two fundamental views which can be used to analyze MTS data, namely the spatial view and the temporal view. Moreover, from each of these two views, we can partition the set of data samples of MTS into disjoint forecasting tasks in accordance with their associated attribute values. Then, samples of the same task will manifest similar forthcoming pattern, which is less sophisticated to be predicted in comparison with the original single-view setting. Considering this insight, we propose a novel multi-view multi-task (MVMT) learning framework for MTS forecasting. Instead of being explicitly presented in most scenarios, MVMT information is deeply concealed in the MTS data, which severely hinders the model from capturing it naturally. To this end, we develop two kinds of basic operations, namely task-wise affine transformation and task-wise normalization, respectively. Applying these two operations with prior knowledge on the spatial and temporal view allows the model to adaptively extract MVMT information while predicting. Extensive experiments on three datasets are conducted to illustrate that canonical architectures can be greatly enhanced by the MVMT learning framework in terms of both effectiveness and efficiency. In addition, we design rich case studies to reveal the properties of representations produced at different phases in the entire prediction procedure.},
  archive      = {J_TKDE},
  author       = {Jinliang Deng and Xiusi Chen and Renhe Jiang and Xuan Song and Ivor W. Tsang},
  doi          = {10.1109/TKDE.2022.3218803},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7665-7680},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A multi-view multi-task learning framework for multi-variate time series forecasting},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A measurement-driven analysis and prediction of content
propagation in the device-to-device social networks. <em>TKDE</em>,
<em>35</em>(8), 7651–7664. (<a
href="https://doi.org/10.1109/TKDE.2022.3219399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the 5G era, data traffic has been growing rapidly. A small number of popular data files may dominate the network traffic and lead to heavy network congestion. Device-to-Device (D2D) communication can be used for caching and offloading significant data traffic. D2D social networks are instantiated paradigms of D2D communication. Existing studies maximize the performances of caching and offloading in D2D social networks by predicting potential content propagation paths. However, predicting such paths still faces many challenges, such as limitation of user spatial-temporal features, fragility of D2D social networks, and uncertainty of participants. As a solution, we first measure users’ multi-dimensional features and content propagation paths to explore the distributions of D2D activities. Then we propose a D2D-LSTM model to predict complete content propagation paths hierarchically and design a prototype-user model for new participants. Experimental results demonstrate the state-of-the-art performances of D2D-LSTM. D2D-LSTM achieves at most 95\% and at least 84.6\% average precision in predicting terminal prototype-user class. Tree generation tests show that the generated trees have at most 64\% and at least 17\% similarity with ground-truth trees.},
  archive      = {J_TKDE},
  author       = {Heng Zhang and Shaoyuan Huang and Xin Wang and Jianxin Li and Xiaofei Wang and Victor C. M. Leung},
  doi          = {10.1109/TKDE.2022.3219399},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7651-7664},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A measurement-driven analysis and prediction of content propagation in the device-to-device social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A light causal feature selection approach to
high-dimensional data. <em>TKDE</em>, <em>35</em>(8), 7639–7650. (<a
href="https://doi.org/10.1109/TKDE.2022.3218786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal feature selection has received increasing attention in recent years. However, the state-of-the-art causal feature selection algorithms use the conditional independence tests, which require enumerating conditioning sets, leading to an exponential increase in computational complexity along with an increase in feature space. To address this problem, in this paper, we theoretically analyze the unique performance of causal features in mutual information, and propose a novel C ausal F eature S election algorithm using M utual I nformation, called CFS-MI. Specifically, CFS-MI separately instantiates the pairwise comparison of mutual information in two stages to reduce computational complexity, and thus improves the efficiency on high-dimensional data. Extensive experiments on 5 benchmark Bayesian networks and 16 real-world datasets validate that CFS-MI has comparable accuracy compared to 7 state-of-the-art causal feature selection algorithms, while presenting more superior computational efficiency.},
  archive      = {J_TKDE},
  author       = {Zhaolong Ling and Ying Li and Yiwen Zhang and Kui Yu and Peng Zhou and Bo Li and Xindong Wu},
  doi          = {10.1109/TKDE.2022.3218786},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7639-7650},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A light causal feature selection approach to high-dimensional data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A learned index for exact similarity search in metric
spaces. <em>TKDE</em>, <em>35</em>(8), 7624–7638. (<a
href="https://doi.org/10.1109/TKDE.2022.3206441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indexing is an effective way to support efficient query processing in large databases. Recently the concept of learned index , which replaces or complements traditional index structures with machine learning models, has been actively explored to reduce storage and search costs. However, accurate and efficient similarity query processing in high-dimensional metric spaces remains to be an open challenge. In this paper, we propose a novel indexing approach called LIMS that uses data clustering, pivot-based data transformation techniques and learned indexes to support efficient similarity query processing in metric spaces. In LIMS, the underlying data is partitioned into clusters such that each cluster follows a relatively uniform data distribution. Data redistribution is achieved by utilizing a small number of pivots for each cluster. Similar data are mapped into compact regions and the mapped values are totally ordinal. Machine learning models are developed to approximate the position of each data record on disk. Efficient algorithms are designed for processing range queries and nearest neighbor queries based on LIMS, and for index maintenance with dynamic updates. Extensive experiments on real-world and synthetic datasets demonstrate the superiority of LIMS compared with traditional indexes and state-of-the-art learned indexes.},
  archive      = {J_TKDE},
  author       = {Yao Tian and Tingyun Yan and Xi Zhao and Kai Huang and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2022.3206441},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7624-7638},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A learned index for exact similarity search in metric spaces},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A complex weighted discounting multisource information
fusion with its application in pattern classification. <em>TKDE</em>,
<em>35</em>(8), 7609–7623. (<a
href="https://doi.org/10.1109/TKDE.2022.3206871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex evidence theory (CET) is an effective method for uncertainty reasoning in knowledge-based systems with good interpretability that has recently attracted much attention. However, approaches to improve the performance of uncertainty reasoning in CET-based expert systems remains an open issue. One key to performance improvement is the adequate management of conflict from multisource information. In this paper, a generalized correlation coefficient, namely, the complex evidential correlation coefficient (CECC), is proposed for the complex mass functions or complex basic belief assignments (CBBAs) in CET. On this basis, a complex conflict coefficient is proposed to measure the conflict between CBBAs; when CBBAs turn into classic BBAs, the complex correlation and conflict coefficients will degrade into traditional coefficients. The complex conflict coefficient satisfies nonnegativity, symmetry, boundedness, extreme consistency, and insensitivity to refinement properties, which are desirable for conflict measurement. Several numerical examples validate through comparisons the superiority of the complex conflict coefficient. In this context, a weighted discounting multisource information fusion algorithm, which is called the CECC-WDMSIF, is designed based on the CECC to improve the performance of CET-based expert systems. By applying the CECC-WDMSIF method to the pattern classification of diverse real-world datasets, it is demonstrated that the proposed CECC-WDMSIF outperforms well-known related approaches with higher classification accuracy and robustness.},
  archive      = {J_TKDE},
  author       = {Fuyuan Xiao and Zehong Cao and Chin-Teng Lin},
  doi          = {10.1109/TKDE.2022.3206871},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {8},
  pages        = {7609-7623},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A complex weighted discounting multisource information fusion with its application in pattern classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When behavior analysis meets social network alignment.
<em>TKDE</em>, <em>35</em>(7), 7590–7607. (<a
href="https://doi.org/10.1109/TKDE.2022.3197985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, aligning users among different social networks has received significant attention. However, most of the existing studies do not consider users’ behavior information during the aligning procedure and thus still suffer from poor learning performance. In fact, we observe that social network alignment and user behavior analysis can benefit from each other. Motivated by such an observation, we propose to jointly study the social network alignment and user behavior analysis problem in this paper. We design a novel framework named BANANA-RGB. In this framework, to capture users’ multi-scale behavior information in each social network, we train a variant of the hierarchical periodic memory network with personalized memorization. To leverage behavior analysis for social network alignment, we design a tensor fusion network-based alignment component to improve the performance. To further leverage social network alignment for behavior analysis, we design a gating-based cross-network behavior fusion component to integrate users’ behavior information in different social networks based on the alignment result. We iteratively train the above two components to make the two tasks benefit from each other. Extensive experiments on real-world datasets demonstrate that our proposed approach outperforms the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Zhongbao Zhang and Fuxin Ren and Jiawei Zhang and Sen Su and Yang Yan and Qian Wei and Li Sun and Guozhen Zhu and Congying Guo},
  doi          = {10.1109/TKDE.2022.3197985},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7590-7607},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {When behavior analysis meets social network alignment},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). What is event knowledge graph: A survey. <em>TKDE</em>,
<em>35</em>(7), 7569–7589. (<a
href="https://doi.org/10.1109/TKDE.2022.3180362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Besides entity-centric knowledge, usually organized as Knowledge Graph (KG), events are also an essential kind of knowledge in the world, which trigger the spring up of event-centric knowledge representation form like Event KG (EKG). It plays an increasingly important role in many downstream applications, such as search, question-answering, recommendation, financial quantitative investments, and text generation. This paper provides a comprehensive survey of EKG from history, ontology, instance, and application views. Specifically, to characterize EKG thoroughly, we focus on its history, definition, schema induction, acquisition, related representative graphs/systems, and applications. The development processes and trends are studied therein. We further summarize prospective directions to facilitate future research on EKG.},
  archive      = {J_TKDE},
  author       = {Saiping Guan and Xueqi Cheng and Long Bai and Fujun Zhang and Zixuan Li and Yutao Zeng and Xiaolong Jin and Jiafeng Guo},
  doi          = {10.1109/TKDE.2022.3180362},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7569-7589},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {What is event knowledge graph: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-aware context-gated graph attention network for
clinical risk prediction. <em>TKDE</em>, <em>35</em>(7), 7557–7568. (<a
href="https://doi.org/10.1109/TKDE.2022.3181780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical risk prediction based on Electronic Health Records (EHR) can assist doctors in better judgment and can make sense of early diagnosis. However, the prediction performance heavily relies on effective representations from multi-dimensional time-series EHR data. Existing solutions usually focus on temporal features or inherent relations between clinical event variables or extract both information in two separate phases. This usually leads to insufficient patient feature information and results in poor prediction performance. Moreover, existing methods based on Heterogeneous Graph Neural Network usually require manual selection of proper Meta-Paths. To solve these problems, we propose the Time-aware Context-Gated Graph Attention Network (T-ContextGGAN). Specifically, we design a GNN based module with Time-aware Meta-Paths and self-attention mechanism to extract both temporal semantic information and inherent relations of EHR data simultaneously and perform automatic Meta-Path selection. To evaluate the proposed model, we extract the first 48 hour EHR data in the first Intensive Care Unit (ICU) admission of three different tasks from two open-source datasets and model various clinical variables on the proposed EHRGraph. Extensive experimental results show the proposed model can effectively extract informative features, and outperform existing state-of-art models in terms of various prediction measures. Our code is available in https://github.com/OwlCitizen/TContext-GGAN .},
  archive      = {J_TKDE},
  author       = {Yuyang Xu and Haochao Ying and Siyi Qian and Fuzhen Zhuang and Xiao Zhang and Deqing Wang and Jian Wu and Hui Xiong},
  doi          = {10.1109/TKDE.2022.3181780},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7557-7568},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Time-aware context-gated graph attention network for clinical risk prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TBNet: A two-stream boundary-aware network for generic image
manipulation localization. <em>TKDE</em>, <em>35</em>(7), 7541–7556. (<a
href="https://doi.org/10.1109/TKDE.2022.3187091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding tampered regions in images is a common research topic in machine learning and computer vision. Although many image manipulation location algorithms have been proposed, most of them only focus on RGB images with different color spaces, and the frequency information that contains the potential tampering clues is often ignored. Moreover, among the manipulation operations, splicing and copy-move are two frequently used methods, but as their characteristics are quite different, specific methods have been individually designed for detecting the operations of either splicing or copy-move, and it is very difficult to widely apply these methods in practice. To solve these issues, in this work, a novel end-to-end two-stream boundary-aware network (abbreviated as TBNet) is proposed for generic image manipulation localization where the RGB stream, the frequency stream, and the boundary artifact location are explored in a unified framework. Specifically, we first design an adaptive frequency selection module (AFS) to adaptively select the appropriate frequency to mine inconsistent statistics and eliminate the interference of redundant statistics. Then, an adaptive cross-attention fusion module (ACF) is proposed to adaptively fuse the RGB feature and the frequency feature. Finally, the boundary artifact location network (BAL) is designed to locate the boundary artifacts for which the parameters are jointly updated by the outputs of the ACF, and its results are further fed into the decoder. Thus, the parameters of the RGB stream, the frequency stream, and the boundary artifact location network are jointly optimized, and their latent complementary relationships are fully mined. The results of the extensive experiments performed on six public benchmarks of the image manipulation localization task, namely, CASIA1.0, COVER, Carvalho, In-The-Wild, NIST-16, and IMD-2020, demonstrate that the proposed TBNet can substantially outperform state-of-the-art generic image manipulation localization methods in terms of MCC, F1, and AUC while maintaining robustness with respect to various attacks. Compared with DeepLabV3+ on the CASIA1.0, COVER, Carvalho, In-The-Wild, and NIST-16 datasets, the improvements in MCC/F1 reach 11\%/11.1\%, 8.2\%/10.3\%, 10.2\%/11.6\%, 8.9\%/6.2\%, and 13.3\%/16.0\%, respectively. Moreover, when IMD2020 is utilized, its AUC improvement can achieve 14.7\%.},
  archive      = {J_TKDE},
  author       = {Zan Gao and Chao Sun and Zhiyong Cheng and Weili Guan and Anan Liu and Meng Wang},
  doi          = {10.1109/TKDE.2022.3187091},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7541-7556},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TBNet: A two-stream boundary-aware network for generic image manipulation localization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STP-TrellisNets+: Spatial-temporal parallel TrellisNets for
multi-step metro station passenger flow prediction. <em>TKDE</em>,
<em>35</em>(7), 7526–7540. (<a
href="https://doi.org/10.1109/TKDE.2022.3187690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The drastic increase of metro passengers in recent years inevitably causes the overcrowdedness in the metro systems. Accurately predicting passenger flows at metro stations is critical for efficient metro system management, which helps alleviate such overcrowdedness. Compared to the prevalent next-step prediction, multi-step passenger flow prediction could prominently increase the prediction duration and reveal finer-grained passenger flow variations, which better helps metro system management. Thus, in this paper, we address the problem of multi-step metro station passenger (MSP) flow prediction . In light of MSP flows’ unique spatial-temporal characteristics, we propose STP-TrellisNets+ , which for the first time augments the newly-emerged temporal convolutional framework TrellisNet for multi-step MSP flow prediction. The temporal module of STP-TrellisNets+ (named CP-TrellisNetsED ) employs a Closeness TrellisNet followed by a Periodicity TrellisNets-based Encoder-Decoder (P-TrellisNetsED) to jointly capture the short- and long-term temporal correlation of MSP flows. In parallel to CP-TrellisNetsED, its spatial module (named GC-TrellisNetsED ) adopts a novel transfer flow-based metric to characterize the spatial correlation among MSP flows, and implements another TrellisNetsED on multiple diffusion graph convolutional networks (DGCNs) in time-series order to capture the dynamics of such spatial correlation. Extensive experiments with two large-scale real-world automated fare collection datasets demonstrate that STP-TrellisNets+ outperforms the state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Junjie Ou and Jiahui Sun and Yichen Zhu and Haiming Jin and Yijuan Liu and Fan Zhang and Jianqiang Huang and Xinbing Wang},
  doi          = {10.1109/TKDE.2022.3187690},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7526-7540},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {STP-TrellisNets+: Spatial-temporal parallel TrellisNets for multi-step metro station passenger flow prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). ST-ExpertNet: A deep expert framework for traffic
prediction. <em>TKDE</em>, <em>35</em>(7), 7512–7525. (<a
href="https://doi.org/10.1109/TKDE.2022.3196936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, forecasting the crowd flows has become an important research topic, and plentiful technologies have achieved good performances. As we all know, the flow at a citywide level is in a mixed state with several basic patterns (e.g., commuting, working, and commercial) caused by the city area functional distributions (e.g., developed commercial areas, educational areas and parks). However, existing technologies have been criticized for their lack of considering the differences in the flow patterns among regions since they want to build only one comprehensive model to learn the mixed flow tensors. Recognizing this limitation, we present a new perspective on flow prediction and propose an explainable framework named ST-ExpertNet, which can adopt every spatial-temporal model and train a set of functional experts devoted to specific flow patterns. Technically, we train a bunch of experts based on the Mixture of Experts (MoE), which guides each expert to specialize in different kinds of flow patterns in sample spaces by using the gating network. We define several criteria, including comprehensiveness, sparsity, and preciseness, to construct the experts for better interpretability and performances. We conduct experiments on a wide range of real-world taxi and bike datasets in Beijing and NYC. The visualizations of the expert&#39;s intermediate results demonstrate that our ST-ExpertNet successfully disentangles the city&#39;s mixed flow tensors along with the city layout, e.g., the urban ring road structure. Different network architectures, such as ST-ResNet, ConvLSTM, and CNN, have been adopted into our ST-ExpertNet framework for experiments and the results demonstrates the superiority of our framework in both interpretability and performances.},
  archive      = {J_TKDE},
  author       = {Hongjun Wang and Jiyuan Chen and Zipei Fan and Zhiwen Zhang and Zekun Cai and Xuan Song},
  doi          = {10.1109/TKDE.2022.3196936},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7512-7525},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ST-ExpertNet: A deep expert framework for traffic prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical tests and association measures for business
processes. <em>TKDE</em>, <em>35</em>(7), 7497–7511. (<a
href="https://doi.org/10.1109/TKDE.2022.3197840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through the application of process mining, organisations can improve their business processes by leveraging data recorded as a result of the performance of these processes. Over the past two decades, the field of process mining evolved considerably, offering a rich collection of analysis techniques with different objectives and characteristics. Despite the advances in this field, a solid statistical foundation is still lacking. Such a foundation would allow analysis outcomes to be found or judged using the notion of statistical significance, thus providing a more objective way to assess these outcomes. This article contributes several statistical tests and association measures that treat process behaviour as a variable. The sensitivity of these tests to their parameters is evaluated and their applicability is illustrated through the use of real-life event logs. The presented tests and measures constitute a key contribution to a statistical foundation for process mining.},
  archive      = {J_TKDE},
  author       = {Sander J.J. Leemans and James M. McGree and Artem Polyvyanyy and Arthur H.M. ter Hofstede},
  doi          = {10.1109/TKDE.2022.3197840},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7497-7511},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Statistical tests and association measures for business processes},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Sliced sparse gradient induced multi-view subspace
clustering via tensorial arctangent rank minimization. <em>TKDE</em>,
<em>35</em>(7), 7483–7496. (<a
href="https://doi.org/10.1109/TKDE.2022.3185126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering method tries to improve the performance of clustering by using the information existing in different views. The tensorial representation is more suitable to capture the high order correlations across different views while keep local geometrical structure in specific view. In this paper, we propose a sliced sparse gradient induced multi-view subspace clustering method via tensorial arctangent rank minimization, named SSG-TAR method. First, a tensorial arctangent rank (TAR) is defined, which is a tighter surrogate of the tensor rank and more effective to explore the consistency among multiple views. Second, a sliced sparse gradient regularization (SSG) is first proposed to enhance the discrimination between clusters and better capture the complementary information in view-specific feature space. Finally, we unify these two terms together and establish an efficient algorithm to optimize the proposed model. Furthermore, the constructed sequence was proved to converge to the stationary KKT point. We have carried out extensive experiments on ten datasets across different types and sizes to verify the performance of our model. The experimental results show that our method have achieved the state-of-the-art performance.},
  archive      = {J_TKDE},
  author       = {Xiaoli Sun and Rui Zhu and Ming Yang and Xiujun Zhang and Yuanyan Tang},
  doi          = {10.1109/TKDE.2022.3185126},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7483-7496},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Sliced sparse gradient induced multi-view subspace clustering via tensorial arctangent rank minimization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised discriminative feature learning for deep
multi-view clustering. <em>TKDE</em>, <em>35</em>(7), 7470–7482. (<a
href="https://doi.org/10.1109/TKDE.2022.3193569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering is an important research topic due to its capability to utilize complementary information from multiple views. However, there are few methods to consider the negative impact caused by certain views with unclear clustering structures, resulting in poor multi-view clustering performance. To address this drawback, we propose s elf-supervised discriminative feature learning for d eep m ulti- v iew c lustering (SDMVC). Concretely, deep autoencoders are applied to learn embedded features for each view independently. To leverage the multi-view complementary information, we concatenate all views’ embedded features to form the global features, which can overcome the negative impact of some views’ unclear clustering structures. In a self-supervised manner, pseudo-labels are obtained to build a unified target distribution to perform multi-view discriminative feature learning. During this process, global discriminative information can be mined to supervise all views to learn more discriminative features, which in turn are used to update the target distribution. Besides, this unified target distribution can make SDMVC learn consistent cluster assignments, which accomplishes the clustering consistency of multiple views while preserving their features’ diversity. Experiments on various types of multi-view datasets show that SDMVC outperforms 14 competitors including classic and state-of-the-art methods. The code is available at https://github.com/SubmissionsIn/SDMVC .},
  archive      = {J_TKDE},
  author       = {Jie Xu and Yazhou Ren and Huayi Tang and Zhimeng Yang and Lili Pan and Yang Yang and Xiaorong Pu and Philip S. Yu and Lifang He},
  doi          = {10.1109/TKDE.2022.3193569},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7470-7482},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Self-supervised discriminative feature learning for deep multi-view clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selecting the best part from multiple laplacian autoencoders
for multi-view subspace clustering. <em>TKDE</em>, <em>35</em>(7),
7457–7469. (<a href="https://doi.org/10.1109/TKDE.2022.3178145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-view subspace clustering attracts much attention in recent years. Most methods follow the framework of fusing the affinity graph learned in each view. In this framework, both the fusion strategy and built graph of each view are very important. In this paper, we propose novel methods for multi-view subspace clustering to address these two aspects. On the one hand, we adopt the autoencoders with Laplacian regularization to construct the affinity graph in each view. Compared with previous work employing the autoencoders, the Laplacian term in our method can guide the learned latent representation favoring affinity extraction. Besides, we also discuss the reasons for adding Laplacian regularization. On the other hand, we propose a novel fusion strategy distinguished from the related literature. If the affinity graph of some view is not extracted well, the performance of previous fusion strategies will be seriously affected. Since our strategy can choose the best part from each affinity graph, it can overcome this limitation to some extent. Extensive experimental results on multiple benchmark data sets confirm the effectiveness of our method.},
  archive      = {J_TKDE},
  author       = {Kewei Tang and Kaiqiang Xu and Wei Jiang and Zhixun Su and Xiyan Sun and Xiaonan Luo},
  doi          = {10.1109/TKDE.2022.3178145},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7457-7469},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Selecting the best part from multiple laplacian autoencoders for multi-view subspace clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Secure and high-quality watermarking algorithms for
relational database based on semantic. <em>TKDE</em>, <em>35</em>(7),
7440–7456. (<a href="https://doi.org/10.1109/TKDE.2022.3194191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relational databases are widely applied in various industries, such as government departments, medical institutions and enterprises, for data storage and relationship management. It is convenience to data maintenance, however, the data in it maybe vulnerable to be forged or tampered with. Therefore, protecting the copyright of relational databases is a critical issue. Fortunately, watermarking technology can be used to prove copyright ownership. Since the digital watermarking technology can solve this problem effectively, based on semantic, we propose two watermarking approaches with high security and strong robustness for numeric and non-numeric data of relational databases in this paper. On the one hand, we propose a reversible numeric watermarking approach. It performs the replacement operation at the semantic level and retain the statistical characteristics of the data. On the other hand, based on word segmentation and word embedding, the non-numerical attributes of the relational database with natural language are chosen to embed watermark. It is noteworthy that the proposed mechanism can be applied to both Chinese and English with minimum distortion. Additionally, we also propose the virtual splitting of attribute column and the principle of modification minimum to guarantee the capacity of watermark and reduce the data modification rate. Additionally, the BCH (31,16,7) error control code is added to the binary watermark string to improve the detection rate of watermark. Based on the above innovations, the security of watermarking algorithm is improved successfully by double encryption (chaos encryption and hash encryption) with two keys (the user&#39;s private key and the attribute column key). The simulation results demonstrate that the proposed two algorithms in this paper have stronger robustness on defensing malicious attacks than previous works.},
  archive      = {J_TKDE},
  author       = {Wenling Li and Ning Li and Jianen Yan and Zhaoxin Zhang and Ping Yu and Gang Long},
  doi          = {10.1109/TKDE.2022.3194191},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7440-7456},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Secure and high-quality watermarking algorithms for relational database based on semantic},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RMT-net: Reject-aware multi-task network for modeling
missing-not-at-random data in financial credit scoring. <em>TKDE</em>,
<em>35</em>(7), 7427–7439. (<a
href="https://doi.org/10.1109/TKDE.2022.3179025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In financial credit scoring, loan applications may be approved or rejected. We can only observe default/non-default labels for approved samples but have no observations for rejected samples, which leads to missing-not-at-random selection bias. Machine learning models trained on such biased data are inevitably unreliable. In this work, we find that the default/non-default classification task and the rejection/approval classification task are highly correlated, according to both real-world data study and theoretical analysis. Consequently, the learning of default/non-default can benefit from rejection/approval. Accordingly, we for the first time propose to model the biased credit scoring data with Multi-Task Learning (MTL). Specifically, we propose a novel Reject-aware Multi-Task Network (RMT-Net), which learns the task weights that control the information sharing from the rejection/approval task to the default/non-default task by a gating network based on rejection probabilities. RMT-Net leverages the relation between the two tasks that the larger the rejection probability, the more the default/non-default task needs to learn from the rejection/approval task. Furthermore, we extend RMT-Net to RMT-Net++ for modeling scenarios with multiple rejection/approval strategies. Extensive experiments are conducted on several datasets, and strongly verifies the effectiveness of RMT-Net on both approved and rejected samples. In addition, RMT-Net++ further improves RMT-Net’s performances.},
  archive      = {J_TKDE},
  author       = {Qiang Liu and Yingtao Luo and Shu Wu and Zhen Zhang and Xiangnan Yue and Hong Jin and Liang Wang},
  doi          = {10.1109/TKDE.2022.3179025},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7427-7439},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RMT-net: Reject-aware multi-task network for modeling missing-not-at-random data in financial credit scoring},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rel-CNN: Learning relationship features in time series for
classification. <em>TKDE</em>, <em>35</em>(7), 7412–7426. (<a
href="https://doi.org/10.1109/TKDE.2022.3186963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification has ubiquitous applications in the real world. Owing to its importance, many time series classification techniques have been proposed over years. Among them, neural network based methods have attracted significant research attention due to their ability to automatically extract latent and discriminative features from data. In this paper, we explore relationship features , which provide valuable global information for time series analytics, and propose a general neural network architecture, namely Rel-CNN , to learn both global and local subsequence features for time series classification. Moreover, we provide two detailed model designs, Relationship Feature based Convolution Filtering and Latent Relationship Feature based Convolution Filtering , and address technical issues due to excessive parameters to learn in these models. We evaluate our models and baselines on time series classification, with extensive experiments on the widely-used 85 uni-variate “bake-off” datasets and 8 multi-variate UEA datasets. Experimental results show that our Rel-CNN models are superior to the representative time series classifiers, in terms of average accuracy, average Macro-f1 and ranking metrics. In addition, an ensemble version of Rel-CNN also outperforms the state-of-the-art ensemble classifiers in terms of average rank, average accuracy and average Macro-f1 on the bake-off datasets.},
  archive      = {J_TKDE},
  author       = {Fang He and Tao-Yang Fu and Wang-Chien Lee},
  doi          = {10.1109/TKDE.2022.3186963},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7412-7426},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Rel-CNN: Learning relationship features in time series for classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning-enhanced shared-account cross-domain
sequential recommendation. <em>TKDE</em>, <em>35</em>(7), 7397–7411. (<a
href="https://doi.org/10.1109/TKDE.2022.3185101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared-account Cross-domain Sequential Recommendation (SCSR) is an emerging yet challenging task that simultaneously considers the shared-account and cross-domain characteristics in the sequential recommendation. Existing works on Shared-account Cross-domain Sequential Recommendation (SCSR) are mainly based on Recurrent Neural Network (RNN) and Graph Neural Network (GNN) but they ignore the fact that although multiple users share a single account, it is mainly occupied by one user at a time. This observation motivates us to learn a more accurate user-specific account representation by attentively focusing on its recent behaviors. Furthermore, though existing works endow lower weights to irrelevant interactions, they may still dilute the domain information and impede the cross-domain recommendation. To address the above issues, we propose a reinforcement learning-based solution, namely RL-ISN, which consists of a basic cross-domain recommender and a reinforcement learning-based domain filter. Specifically, to model the account representation in the shared-account scenario, the basic recommender first clusters users’ mixed behaviors as latent users, and then leverages an attention model over them to conduct user identification. To reduce the impact of irrelevant domain information, we formulate the domain filter as a hierarchical reinforcement learning task, where a high-level task is utilized to decide whether to revise the whole transferred sequence or not, and if it does, a low-level task is further performed to determine whether to remove each interaction within it or not. To evaluate the performance of our solution, we conduct extensive experiments on two real-world datasets, and the experimental results demonstrate the superiority of our RL-ISN method compared with the state-of-the-art recommendation methods.},
  archive      = {J_TKDE},
  author       = {Lei Guo and Jinyu Zhang and Tong Chen and Xinhua Wang and Hongzhi Yin},
  doi          = {10.1109/TKDE.2022.3185101},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7397-7411},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reinforcement learning-enhanced shared-account cross-domain sequential recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Reachable distance function for KNN classification.
<em>TKDE</em>, <em>35</em>(7), 7382–7396. (<a
href="https://doi.org/10.1109/TKDE.2022.3185149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance function is a main metrics of measuring the affinity between two data points in machine learning. Extant distance functions often provide unreachable distance values in real applications. This can lead to incorrect measure of the affinity between data points. This paper proposes a reachable distance function for KNN classification. The reachable distance function is not a geometric direct-line distance between two data points. It gives a consideration to the class attribute of a training dataset when measuring the affinity between data points. Concretely speaking, the reachable distance between data points includes their class center distance and real distance. Its shape looks like “Z,” and we also call it a Z distance function. In this way, the affinity between data points in the same class is always stronger than that in different classes. Or, the intraclass data points are always closer than those interclass data points. We evaluated the reachable distance with experiments, and demonstrated that the proposed distance function achieved better performance in KNN classification.},
  archive      = {J_TKDE},
  author       = {Shichao Zhang and Jiaye Li and Yangding Li},
  doi          = {10.1109/TKDE.2022.3185149},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7382-7396},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reachable distance function for KNN classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rank-r discrete matrix factorization for anchor graph
clustering. <em>TKDE</em>, <em>35</em>(7), 7371–7381. (<a
href="https://doi.org/10.1109/TKDE.2022.3198800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering many graph clustering methods are with quadratic or cubic time complexity and need post-processing to obtain the discrete solution. Combining with the anchor graph, we study a novel graph clustering model called Rank- $r$ Discrete Matrix Factorization (DMF-RR), which is linear time complexity, and motivated by nonnegative matrix factorization (NMF). Instead of constraining the factor matrices of NMF to be nonnegative as many existed methods, we constrain them to indicator matrices. Thus, DMF-RR can obtain the discrete solution by directly solving the original problem without post-processing. Furthermore, considering the greater similarity between samples of the same category, an anchor graph is constructed as an input to capture essential clustering structure by utilizing the duality information between samples and anchors. Subsequently, an efficient and simple algorithm is proposed due to the nature of indicator matrices. Extensive experiments performed on synthetic and real-world datasets demonstrate the superiority of DMF-RR.},
  archive      = {J_TKDE},
  author       = {Jingjing Xue and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2022.3198800},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7371-7381},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Rank-r discrete matrix factorization for anchor graph clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prototypical concept representation. <em>TKDE</em>,
<em>35</em>(7), 7357–7370. (<a
href="https://doi.org/10.1109/TKDE.2022.3180886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concepts are building blocks of human thinking. For machines, concept understanding has also been increasingly important, which makes concept representation a fundamental problem in artificial intelligence. While many concepts have their instances, the massive amount of information carried by instances has long been ignored in current concept representation, which limits the usage of these concepts in applications. In this paper, inspired by prototype theory in cognitive science, we propose prototypical concept representation for machines, which represents each concept with a distributed prototype derived from representations of its instances. For prototypical representation learning, we further introduce a novel model named Prototypical Siamese Network (PSN). PSN is trained under the supervision of isA determination, one of the most important concept-related applications. Results of extensive experiments demonstrate that, our method achieves state-of-the-art performance, thus validating the effectiveness of prototypical concept representation.},
  archive      = {J_TKDE},
  author       = {Xintao Wang and Jiaqing Liang and Yanghua Xiao and Wei Wang},
  doi          = {10.1109/TKDE.2022.3180886},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7357-7370},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Prototypical concept representation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving analytics on decentralized social graphs:
The case of eigendecomposition. <em>TKDE</em>, <em>35</em>(7),
7341–7356. (<a href="https://doi.org/10.1109/TKDE.2022.3185079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analytics over social graphs allows to extract valuable knowledge and insights for many fields like community detection, fraud detection, and interest mining. In practice, decentralized social graphs frequently arise, where the social graph is not available to a single entity and is decentralized among a large number of users, each holding only a limited local view about the whole graph. Collecting the local views for analytics of decentralized social graphs raises critical privacy concerns, as they encode private information about the social interactions among individuals. In this paper, we design, implement, and evaluate PrivGED, a new system aimed at privacy-preserving analytics over decentralized social graphs. PrivGED focuses on the support for eigendecomposition, one popular and fundamental graph analytics task producing eigenvalues/eigenvectors over the adjacency matrix of a social graph and benefits various practical applications. PrivGED is built from a delicate synergy of insights on graph analytics, lightweight cryptography, and differential privacy, allowing users to securely contribute their local views on a decentralized social graph for a cloud-based eigendecomposition analytics service while gaining strong privacy protection. Extensive experiments over real-world social graph datasets demonstrate that PrivGED achieves accuracy comparable to the plaintext domain, with practically affordable performance superior to prior art.},
  archive      = {J_TKDE},
  author       = {Songlei Wang and Yifeng Zheng and Xiaohua Jia and Xun Yi},
  doi          = {10.1109/TKDE.2022.3185079},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7341-7356},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Privacy-preserving analytics on decentralized social graphs: The case of eigendecomposition},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). OOD-GNN: Out-of-distribution generalized graph neural
network. <em>TKDE</em>, <em>35</em>(7), 7328–7340. (<a
href="https://doi.org/10.1109/TKDE.2022.3193725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have achieved impressive performance when testing and training graph data come from identical distribution. However, existing GNNs lack out-of-distribution generalization abilities so that their performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this problem, in this work, we propose an out-of-distribution generalized graph neural network ( OOD-GNN ) for achieving satisfactory performance on unseen testing graphs that have different distributions with training graphs. Our proposed OOD-GNN employs a novel nonlinear graph representation decorrelation method utilizing random Fourier features, which encourages the model to eliminate the statistical dependence between relevant and irrelevant graph representations through iteratively optimizing the sample graph weights and graph encoder. We further present a global weight estimator to learn weights for training graphs such that variables in graph representations are forced to be independent. The learned weights help the graph encoder to get rid of spurious correlations and, in turn, concentrate more on the true connection between learned discriminative graph representations and their ground-truth labels. We conduct extensive experiments to validate the out-of-distribution generalization abilities on two synthetic and 12 real-world datasets with distribution shifts. The results demonstrate that our proposed OOD-GNN significantly outperforms state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Haoyang Li and Xin Wang and Ziwei Zhang and Wenwu Zhu},
  doi          = {10.1109/TKDE.2022.3193725},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7328-7340},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {OOD-GNN: Out-of-distribution generalized graph neural network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the evolutionary of bloom filter false positives - an
information theoretical approach to optimizing bloom filter parameters.
<em>TKDE</em>, <em>35</em>(7), 7316–7327. (<a
href="https://doi.org/10.1109/TKDE.2022.3200045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fundamental issue of how to calculate the false positive probability of widely used Bloom Filters (BF), from which the conventional wisdom is to derive the optimal value of $k$ , remains elusive. Since Bloom gave the false positive formula in 1970, in 2008, Bose et al. pointed out that Bloom&#39;s formula is flawed; and in 2010, Christensen et al. pointed out that Bose&#39;s formula is also flawed and gave another formula. Although Christensen&#39;s formula is perfectly accurate, it is time-consuming and impossible to calculate the optimal value of $k$ . Based on the following observation: for a BF with $m$ bits and $n$ elements, if and only if its entropy is the largest, its false positive probability is the smallest, we propose the first approach to calculating the optimal $k$ without any false positive formula. Furthermore, we propose a new and more accurate upper bound for the false positive probability. When the size of a Bloom Filter becomes infinitely large, our upper bound turns equal to the lower bound, which becomes Bloom&#39;s formula and deepens our understanding towards it. Besides, we derive the bounds of correct rate of Counting Bloom Filters (CBFs) by applying our proposed formulas about BFs to them.},
  archive      = {J_TKDE},
  author       = {Zhuochen Fan and Gang Wen and Zhipeng Huang and Yang Zhou and Qiaobin Fu and Tong Yang and Alex X. Liu and Bin Cui},
  doi          = {10.1109/TKDE.2022.3200045},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7316-7327},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On the evolutionary of bloom filter false positives - an information theoretical approach to optimizing bloom filter parameters},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network embedding with dual generation tasks. <em>TKDE</em>,
<em>35</em>(7), 7303–7315. (<a
href="https://doi.org/10.1109/TKDE.2022.3187851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of Network Embedding (NE) for content-rich networks. NE models aim to learn efficient low-dimensional dense vectors for network vertices which are crucial to many network analysis tasks. The core problem of content-rich network embedding is to learn and integrate the semantic information conveyed by network structure and node content. In this paper, we propose a general end-to-end model, D ual GE nerative N etwork E mbedding (DGENE), to leverage the complementary information of network structure and content. In this model, each vertex is regarded as an object with two modalities: node identity and textual content. Then we formulate two dual generation tasks, Node Identification (NI) which recognizes nodes’ identities given their contents, and Content Generation (CG) which generates textual contents given the nodes’ identities. We develop specific Content2Node and Node2Content models for the two tasks. Under the DGENE framework, the two dual models are learned by sharing and integrating intermediate layers. Extensive experimental results show that our model yields a significant performance gain compared to the state-of-the-art NE methods. Moreover, our model has an interesting and useful byproduct, that is, a component of our model can generate texts and nodes, which is potentially useful for many tasks.},
  archive      = {J_TKDE},
  author       = {Na Li and Jie Liu and Zhicheng He and Chunhai Zhang and Jiaying Xie},
  doi          = {10.1109/TKDE.2022.3187851},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7303-7315},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Network embedding with dual generation tasks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view attribute weighted naive bayes. <em>TKDE</em>,
<em>35</em>(7), 7291–7302. (<a
href="https://doi.org/10.1109/TKDE.2022.3177634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Naive Bayes (NB) continues to be one of the top 10 data mining algorithms due to its simplicity, efficiency and efficacy. Numerous enhancements have been proposed to weaken its attribute conditional independence assumption. However, all of them only focus on the raw attribute view, which is hard to reflect all the data characteristics in real-world applications. To portray data characteristics more comprehensively, in this study, we construct two label views from the raw attributes and propose a novel model called multi-view attribute weighted naive Bayes (MAWNB). In MAWNB, we first build multiple super-parent one-dependence estimators (SPODEs) as well as random trees (RTs), then we utilize each of them to classify each training instance in turn and use all their predicted class labels to construct two label views. Next, to avoid attribute redundancy, we optimize the weight of each attribute value for each class by minimizing the negative conditional log-likelihood (CLL) in each view. Finally, the estimated class-membership probabilities by three views are fused to predict the class label for each test instance. Extensive experiments show that MAWNB significantly outperforms NB and all the other existing state-of-the-art competitors.},
  archive      = {J_TKDE},
  author       = {Huan Zhang and Liangxiao Jiang and Wenjun Zhang and Chaoqun Li},
  doi          = {10.1109/TKDE.2022.3177634},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7291-7302},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view attribute weighted naive bayes},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple kernel subspace learning for clustering and
classification. <em>TKDE</em>, <em>35</em>(7), 7278–7290. (<a
href="https://doi.org/10.1109/TKDE.2022.3200723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the face of high-dimensional and complex data, effective subspace can preserve specific statistical properties and provide an appropriate representation of data, which generally facilitates the underlying tasks such as clustering or classification. Meanwhile, multiple kernel learning is a technique to combine multiple kernels from different feature spaces effectively. Thus, by incorporating multiple kernels into the process of subspace learning, different feature spaces can be projected into a unified subspace. This article proposes the Multiple Kernel Subspace Learning (MKSL) for embedding the original space into a unified subspace. Multiple kernels of different feature spaces are combined by MKSL in the process of learning, which can extend the suitability for various applications. Moreover, to generate the optimal combination kernel of subspace learning, we propose a two-step iteration strategy to learn the appropriate kernel weights and transformation matrix of projecting simultaneously. Furthermore, our proposed formulation of MKSL can introduce different prior knowledge such as class information and neighborhood relationships. Thus it is competent to the unsupervised learning, semi-supervised learning, and supervised learning. Extensive experiments are conducted on diverse datasets, and the performances are comprehensively evaluated on different tasks. The experimental results indicate that the proposed algorithm is outstanding in unsupervised clustering task and effective in supervised and semi-supervised classification tasks.},
  archive      = {J_TKDE},
  author       = {Ziqiu Chi and Zhe Wang and Bolu Wang and Zhongli Fang and Zonghai Zhu and Dongdong Li and Wenli Du},
  doi          = {10.1109/TKDE.2022.3200723},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7278-7290},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multiple kernel subspace learning for clustering and classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multi-granularity regularized re-balancing for class
incremental learning. <em>TKDE</em>, <em>35</em>(7), 7263–7277. (<a
href="https://doi.org/10.1109/TKDE.2022.3188335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models suffer from catastrophic forgetting when learning new tasks incrementally. Incremental learning has been proposed to retain the knowledge of old classes while learning to identify new classes. A typical approach is to use a few exemplars to avoid forgetting old knowledge. In such a scenario, data imbalance between old and new classes is a key issue that leads to performance degradation of the model. Several strategies have been designed to rectify the bias towards the new classes due to data imbalance. However, they heavily rely on the assumptions of the bias relation between old and new classes. Therefore, they are not suitable for complex real-world applications. In this study, we propose an assumption-agnostic method, Multi-Granularity Regularized re-Balancing (MGRB), to address this problem. Re-balancing methods are used to alleviate the influence of data imbalance; however, we empirically discover that they would under-fit new classes. To this end, we further design a novel multi-granularity regularization term that enables the model to consider the correlations of classes in addition to re-balancing the data. A class hierarchy is first constructed by ontology or grouping semantically or visually similar classes. The multi-granularity regularization then transforms the one-hot label vector into a continuous label distribution, which reflects the relations between the target class and other classes based on the constructed class hierarchy. Thus, the model can learn the inter-class relational information, which helps enhance the learning of both old and new classes. Experimental results on both public datasets and a real-world fault diagnosis dataset verify the effectiveness of the proposed method. Code is available at https://github.com/lilyht/CIL-MGRB .},
  archive      = {J_TKDE},
  author       = {Huitong Chen and Yu Wang and Qinghua Hu},
  doi          = {10.1109/TKDE.2022.3188335},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7263-7277},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-granularity regularized re-balancing for class incremental learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-grained semantics-aware graph neural networks.
<em>TKDE</em>, <em>35</em>(7), 7251–7262. (<a
href="https://doi.org/10.1109/TKDE.2022.3195004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are powerful techniques in representation learning for graphs and have been increasingly deployed in a multitude of different applications that involve node- and graph-wise tasks. Most existing studies solve either the node-wise task or the graph-wise task independently while they are inherently correlated. This work proposes a unified model, AdamGNN, to interactively learn node and graph representations in a mutual-optimisation manner. Compared with existing GNN models and graph pooling methods, AdamGNN enhances the node representation with the learned multi-grained semantics and avoids losing node features and graph structure information during pooling. Specifically, a differentiable pooling operator is proposed to adaptively generate a multi-grained structure that involves meso- and macro-level semantic information in the graph. We also devise the unpooling operator and the flyback aggregator in AdamGNN to better leverage the multi-grained semantics to enhance node representations. The updated node representations can further adjust the graph representation in the next iteration. Experiments on 14 real-world graph datasets show that AdamGNN can significantly outperform 17 competing models on both node- and graph-wise tasks. The ablation studies confirm the effectiveness of AdamGNN&#39;s components, and the last empirical analysis further reveals the ingenious ability of AdamGNN in capturing long-range interactions.},
  archive      = {J_TKDE},
  author       = {Zhiqiang Zhong and Cheng-Te Li and Jun Pang},
  doi          = {10.1109/TKDE.2022.3195004},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7251-7262},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-grained semantics-aware graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multi-faceted knowledge-driven pre-training for product
representation learning. <em>TKDE</em>, <em>35</em>(7), 7239–7250. (<a
href="https://doi.org/10.1109/TKDE.2022.3200921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a key component of e-commerce computing, product representation learning (PRL) provides benefits for a variety of applications, including product matching, search, and categorization. The existing PRL approaches have poor language understanding ability due to their inability to capture contextualized semantics. In addition, the learned representations by existing methods are not easily transferable to new products. Inspired by the recent advance of pre-trained language models (PLMs), we make the attempt to adapt PLMs for PRL to mitigate the above issues. In this article, we develop KINDLE, a K nowledge-dr I ven pre-traini N g framework for pro D uct representation LE arning, which can preserve the contextual semantics and multi-faceted product knowledge robustly and flexibly . Specifically, we first extend traditional one-stage pre-training to a two-stage pre-training framework, and exploit a deliberate knowledge encoder to ensure a smooth knowledge fusion into PLM. In addition, we propose a multi-objective heterogeneous embedding method to represent thousands of knowledge elements. This helps KINDLE calibrate knowledge noise and sparsity automatically by replacing isolated classes as training targets in knowledge acquisition tasks. Furthermore, an input-aware gating network is proposed to select the most relevant knowledge for different downstream tasks. Finally, extensive experiments have demonstrated the advantages of KINDLE over the state-of-the-art baselines across three downstream tasks.},
  archive      = {J_TKDE},
  author       = {Denghui Zhang and Yanchi Liu and Zixuan Yuan and Yanjie Fu and Haifeng Chen and Hui Xiong},
  doi          = {10.1109/TKDE.2022.3200921},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7239-7250},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-faceted knowledge-driven pre-training for product representation learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling multiple views via implicitly preserving global
consistency and local complementarity. <em>TKDE</em>, <em>35</em>(7),
7220–7238. (<a href="https://doi.org/10.1109/TKDE.2022.3198746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While self-supervised learning techniques are often used to mine hidden knowledge from unlabeled data via modeling multiple views, it is unclear how to perform effective representation learning in a complex and inconsistent context. To this end, we propose a new multi-view self-supervised learning method, namely consistency and complementarity network (CoCoNet), to comprehensively learn global inter-view consistent and local cross-view complementarity-preserving representations from multiple views. To capture crucial common knowledge which is implicitly shared among views, CoCoNet employs a global consistency module that aligns the probabilistic distribution of views by utilizing an efficient discrepancy metric based on the generalized sliced Wasserstein distance. To incorporate cross-view complementary information, CoCoNet proposes a heuristic complementarity-aware contrastive learning approach, which extracts a complementarity-factor jointing cross-view discriminative knowledge and uses it as the contrast to guide the learning of view-specific encoders. Theoretically, the superiority of CoCoNet is verified by our information-theoretical-based analyses. Empirically, our thorough experimental results show that CoCoNet outperforms the state-of-the-art self-supervised methods by a significant margin, for instance, CoCoNet beats the best benchmark method by an average margin of 1.1\% on ImageNet.},
  archive      = {J_TKDE},
  author       = {Jiangmeng Li and Wenwen Qiang and Changwen Zheng and Bing Su and Farid Razzak and Ji-Rong Wen and Hui Xiong},
  doi          = {10.1109/TKDE.2022.3198746},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7220-7238},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling multiple views via implicitly preserving global consistency and local complementarity},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and detecting communities in node attributed
networks. <em>TKDE</em>, <em>35</em>(7), 7206–7219. (<a
href="https://doi.org/10.1109/TKDE.2022.3197612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental structure in real-world networks, in addition to graph topology, communities can also be reflected by abundant node attributes. In attributed community detection, probabilistic generative models (PGMs) have become the mainstream method due to their principled characterization and competitive performances. Here, we propose a novel PGM without imposing any distributional assumptions on attributes, which is superior to the existing PGMs that require attributes to be categorical or Gaussian distributed. Based on the block model of graph structure, our model incorporates the attribute by describing its effect on node popularity. To characterize the effect quantitatively, we analyze the community detectability for our model and then establish the requirements of the node popularity term. This leads to a new scheme for the crucial model selection problem in choosing and solving attributed community detection models. With the model determined, an efficient algorithm is developed to estimate the parameters and to infer the communities. The proposed method is validated from two aspects. First, the effectiveness of our algorithm is theoretically guaranteed by the detectability condition. Second, extensive experiments indicate that our method not only outperforms the competing approaches on the employed datasets, but also shows better applicability to networks with various node attributes.},
  archive      = {J_TKDE},
  author       = {Ren Ren and Jinliang Shao and Adrian N. Bishop and Wei Xing Zheng},
  doi          = {10.1109/TKDE.2022.3197612},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7206-7219},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling and detecting communities in node attributed networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). MHNF: Multi-hop heterogeneous neighborhood information
fusion graph representation learning. <em>TKDE</em>, <em>35</em>(7),
7192–7205. (<a href="https://doi.org/10.1109/TKDE.2022.3186158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attention mechanism enables graph neural networks (GNNs) to learn the attention weights between the target node and its one-hop neighbors, thereby improving the performance further. However, most existing GNNs are oriented toward homogeneous graphs, and in which each layer can only aggregate the information of one-hop neighbors. Stacking multilayer networks introduces considerable noise and easily leads to over smoothing. We propose here a multihop heterogeneous neighborhood information fusion graph representation learning method (MHNF). Specifically, we propose a hybrid metapath autonomous extraction model to efficiently extract multihop hybrid neighbors. Then, we formulate a hop-level heterogeneous information aggregation model, which selectively aggregates different-hop neighborhood information within the same hybrid metapath. Finally, a hierarchical semantic attention fusion model (HSAF) is constructed, which can efficiently integrate different-hop and different-path neighborhood information. In this fashion, this paper solves the problem of aggregating multihop neighborhood information and learning hybrid metapaths for target tasks. This mitigates the limitation of manually specifying metapaths. In addition, HSAF can extract the internal node information of the metapaths and better integrate the semantic information present at different levels. Experimental results on real datasets show that MHNF achieves the best or competitive performance against state-of-the-art baselines with only a fraction of 1/10 $\sim$ 1/100 parameters and computational budgets. Our code is publicly available at https://github.com/PHD-lanyu/MHNF .},
  archive      = {J_TKDE},
  author       = {Yundong Sun and Dongjie Zhu and Haiwen Du and Zhaoshuo Tian},
  doi          = {10.1109/TKDE.2022.3186158},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7192-7205},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MHNF: Multi-hop heterogeneous neighborhood information fusion graph representation learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memory-guided multi-view multi-domain fake news detection.
<em>TKDE</em>, <em>35</em>(7), 7178–7191. (<a
href="https://doi.org/10.1109/TKDE.2022.3185151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide spread of fake news is increasingly threatening both individuals and society. Great efforts have been made for automatic fake news detection on a single domain (e.g., politics). However, correlations exist commonly across multiple news domains, and thus it is promising to simultaneously detect fake news of multiple domains. Based on our analysis, we pose two challenges in multi-domain fake news detection: 1) domain shift , caused by the discrepancy among domains in terms of words, emotions, styles, etc. 2) domain labeling incompleteness , stemming from the real-world categorization that only outputs one single domain label, regardless of topic diversity of a news piece. In this paper, we propose a Memory-guided Multi-view Multi-domain Fake News Detection Framework (M $^{3}$ FEND) to address these two challenges. We model news pieces from a multi-view perspective, including semantics, emotion, and style. Specifically, we propose a Domain Memory Bank to enrich domain information which could discover potential domain labels based on seen news pieces and model domain characteristics. Then, with enriched domain information as input, a Domain Adapter could adaptively aggregate discriminative information from multiple views for news in various domains. Extensive offline experiments on English and Chinese datasets demonstrate the effectiveness of M $^{3}$ FEND, and online tests verify its superiority in practice. Our code is available at https://github.com/ICTMCG/M3FEND .},
  archive      = {J_TKDE},
  author       = {Yongchun Zhu and Qiang Sheng and Juan Cao and Qiong Nan and Kai Shu and Minghui Wu and Jindong Wang and Fuzhen Zhuang},
  doi          = {10.1109/TKDE.2022.3185151},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7178-7191},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Memory-guided multi-view multi-domain fake news detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MBA-STNet: Bayes-enhanced discriminative multi-task learning
for flow prediction. <em>TKDE</em>, <em>35</em>(7), 7164–7177. (<a
href="https://doi.org/10.1109/TKDE.2022.3179781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd flow prediction, which aims to predict the in/out flows of different areas of a city, plays a critically important role in many real-world applications including intelligent transportation systems and public safety. The challenges of this problem lie in both the dynamic mobility patterns of crowds and the complex spatial-temporal correlations. Meanwhile, crowd flow is highly correlated to and affected by the Origin-Destination (OD) locations of the flow trajectories, which is largely ignored by existing works. In this paper, we study the novel problem of predicting the crowd flow and flow OD simultaneously, and propose a Multi-task Bayes-enhanced Adversarial Spatial Temporal Network entitled MBA-STNet to effectively address it. MBA-STNet adopts a shared-private framework which contains private spatial-temporal encoders, a shared spatial-temporal encoder, and decoders to learn the task-specific features and shared features. To effectively extract discriminative shared features, an adversarial loss on shared feature extraction is incorporated to reduce information redundancy. A Bayesian heterogeneous Spatio-temporal Attention Network is designed to learn the complex spatio-temporal correlations and alleviate the problem of data uncertainty. We also design an attentive temporal queue to capture the complex temporal dependency automatically without the help of domain knowledge. Extensive evaluations are conducted over the bike and taxicab trip datasets in New York. The results demonstrate that the proposed MBA-STNet is superior to state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Hao Miao and Jiaxing Shen and Jiannong Cao and Jiangnan Xia and Senzhang Wang},
  doi          = {10.1109/TKDE.2022.3179781},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7164-7177},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MBA-STNet: Bayes-enhanced discriminative multi-task learning for flow prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local feature selection for large-scale data sets with
limited labels. <em>TKDE</em>, <em>35</em>(7), 7152–7163. (<a
href="https://doi.org/10.1109/TKDE.2022.3181208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing large-scale data sets with limited labels has always been a difficult task in data mining. Facing this difficulty, two local feature selection algorithms, LARD and LRSD, have been proposed based on dependency degree, which can process partially labeled data sets and greatly improve the computational efficiency. However, it is very difficult for these algorithms to calculate large-scale data with millions of samples on a typical personal computer. Although the related family method is a more efficient approach than dependency degree, it cannot be used for partially labeled large-scale data. As a result, a local feature selection method based on related family is proposed to accelerate data processing in the paper. Experiments show that the proposed algorithm can run 405 times faster than LARD on partially labeled data sets and maintain high classification accuracy. In addition, this new algorithm can effectively process partially labeled large-scale data sets with 5,000,000 samples or 20,000 features on a typical personal computer.},
  archive      = {J_TKDE},
  author       = {Tian Yang and Yanfang Deng and Bin Yu and Yuhua Qian and Jianhua Dai},
  doi          = {10.1109/TKDE.2022.3181208},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7152-7163},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Local feature selection for large-scale data sets with limited labels},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Link prediction with contextualized self-supervision.
<em>TKDE</em>, <em>35</em>(7), 7138–7151. (<a
href="https://doi.org/10.1109/TKDE.2022.3200390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction aims to infer the link existence between pairs of nodes in networks/graphs. Despite their wide application, the success of traditional link prediction algorithms is hindered by three major challenges— link sparsity , node attribute noise and dynamic changes —that are faced by many real-world networks. To address these challenges, we propose a C ontextualized S elf- S upervised L earning (CSSL) framework that fully exploits structural context prediction for link prediction. The proposed CSSL framework learns a link encoder to infer the link existence probability from paired node embeddings, which are constructed via a transformation on node attributes. To generate informative node embeddings for link prediction, structural context prediction is leveraged as a self-supervised learning task to boost the link prediction performance. Two types of structural context are investigated, i.e., context nodes collected from random walks versus context subgraphs. The CSSL framework can be trained in an end-to-end manner, with the learning of model parameters supervised by both the link prediction and self-supervised learning tasks. The proposed CSSL is a generic and flexible framework in the sense that it can handle both attributed and non-attributed networks, and operate under both transductive and inductive link prediction settings. Extensive experiments and ablation studies on seven real-world benchmark networks demonstrate the superior performance of the proposed self-supervision based link prediction algorithm over state-of-the-art baselines, on different types of networks under both transductive and inductive settings. The proposed CSSL also yields competitive performance in terms of its robustness to node attribute noise and scalability over large-scale networks.},
  archive      = {J_TKDE},
  author       = {Daokun Zhang and Jie Yin and Philip S. Yu},
  doi          = {10.1109/TKDE.2022.3200390},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7138-7151},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Link prediction with contextualized self-supervision},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning generative RNN-ODE for collaborative time-series
and event sequence forecasting. <em>TKDE</em>, <em>35</em>(7),
7118–7137. (<a href="https://doi.org/10.1109/TKDE.2022.3185115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series and event sequences are widely collected data types in real-world applications. Modeling and forecasting of such temporal data play an important role in an informed decision-making process. A major limitation of previous methods is that they either focus on time-series or events, rather than the combination of the two worlds. In fact, the two types of data often provide complementary information, emphasizing the necessity of jointly modeling the both. In this paper, we propose the RNN-ODE collaborative model for joint modeling and forecasting of heterogeneous time-series and event sequence data, which combines several useful techniques from both Bayesian and deep learning for its interpretability. Specifically, we devise a tailored encoder to combine the advances in deep temporal point processes models and variational recurrent neural networks. To predict the probability of event occurrence over an arbitrary continuous-time horizon, we base our model on the mathematical foundation of Neural Ordinary Differential Equations (NODE). Extensive experimental results on simulations and real data sets show that compared with existing methods, our integrated approach can achieve more competitive forecasting performance of both time-series and event sequences.},
  archive      = {J_TKDE},
  author       = {Longyuan Li and Junchi Yan and Yunhao Zhang and Jihai Zhang and Jie Bao and Yaohui Jin and Xiaokang Yang},
  doi          = {10.1109/TKDE.2022.3185115},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7118-7137},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning generative RNN-ODE for collaborative time-series and event sequence forecasting},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning fair graph neural networks with limited and private
sensitive attribute information. <em>TKDE</em>, <em>35</em>(7),
7103–7117. (<a href="https://doi.org/10.1109/TKDE.2022.3197554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make biased predictions w.r.t protected sensitive attributes, e.g., skin color and gender. This is because machine learning algorithms including GNNs are trained to reflect the distribution of the training data which often contains historical bias towards sensitive attributes. In addition, we empirically show that the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism of GNNs. As a result, the applications of GNNs in high-stake domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on independently and identically distributed (i.i.d) data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Generally, learning fair models require abundant sensitive attributes to regularize the model. However, for many graphs such as social networks, users are reluctant to share sensitive attributes. Thus, only limited sensitive attributes are available for fair GNN training in practice. Moreover, directly collecting and applying the sensitive attributes in fair model training may cause privacy issues, because the sensitive information can be leaked in data breach or attacks on the trained model. Therefore, we study a novel and important problem of learning fair GNNs with limited number of private sensitive attributes, i.e., sensitive attributes that are processed with a privacy-preserving mechanism. In an attempt to address these problems, FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node classification accuracy by leveraging graph structures and limited sensitive information. To further preserve the privacy, private sensitive attributes with privacy guarantee are obtained by injecting noise based on local differential privacy. And We further extend FairGNN to NT-FairGNN to handle the limited and private sensitive attributes to simultaneously achieve fairness and preserve privacy. Theoretical analysis and extensive experiments on real-world datasets demonstrate the effectiveness of FairGNN and NT-FairGNN in achieving fair and high-accurate classification.},
  archive      = {J_TKDE},
  author       = {Enyan Dai and Suhang Wang},
  doi          = {10.1109/TKDE.2022.3197554},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7103-7117},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning fair graph neural networks with limited and private sensitive attribute information},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning entity linking features for emerging entities.
<em>TKDE</em>, <em>35</em>(7), 7088–7102. (<a
href="https://doi.org/10.1109/TKDE.2022.3197707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity linking (EL) is the process of linking entity mentions appearing in text with their corresponding entities in a knowledge base. EL features of entities (e.g., prior probability, relatedness score, and entity embedding) are usually estimated based on Wikipedia. However, for newly emerging entities (EEs) which have just been discovered in news, they may still not be included in Wikipedia yet. As a consequence, it is unable to obtain required EL features for those EEs from Wikipedia and EL models will always fail to link ambiguous mentions with those EEs correctly as the absence of their EL features. To deal with this problem, in this paper we focus on a new task of learning EL features for emerging entities in a general way. We propose a novel approach called STAMO to learn high-quality EL features for EEs automatically, which needs just a small number of labeled documents for each EE collected from the Web, as it could further leverage the knowledge hidden in the unlabeled data. STAMO is mainly based on self-training, which makes it flexibly integrated with any EL feature or EL model, but also makes it easily suffer from the error reinforcement problem caused by the mislabeled data. Instead of some common self-training strategies that try to throw the mislabeled data away explicitly, we regard self-training as a multiple optimization process with respect to the EL features of EEs, and propose both intra-slot and inter-slot optimizations to alleviate the error reinforcement problem implicitly. We construct two EL datasets involving selected EEs to evaluate the quality of obtained EL features for EEs, and the experimental results show that our approach significantly outperforms other baseline methods of learning EL features.},
  archive      = {J_TKDE},
  author       = {Chenwei Ran and Wei Shen and Jianbo Gao and Yuhan Li and Jianyong Wang and Yantao Jia},
  doi          = {10.1109/TKDE.2022.3197707},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7088-7102},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning entity linking features for emerging entities},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent representation guided multi-view clustering.
<em>TKDE</em>, <em>35</em>(7), 7082–7087. (<a
href="https://doi.org/10.1109/TKDE.2022.3192686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering aims to reveal the correlation between different input modalities in an unsupervised way. Similarity between data samples can be described by a similarity graph, which governs the quality of multi-view clustering. However, existing multi-view graph learning methods mainly construct similarity graph based on raw features, which are unreliable as real-world datasets usually contain noises, outliers, or even redundant information. In this paper, we formulate a novel model to simultaneously learn a robust structured similarity graph and perform multi-view clustering. The similarity graph is adaptively learned based on a latent representation that is invulnerable to noises and outliers. Furthermore, the similarity graph is enforced to contain a clear structure, i.e., the number of connected components of the target graph is exactly equal to the ground-truth class number. Consequently, the label to each data sample can be directly assigned without any postprocessing. As a result, our model aims at accomplishing three subtasks: latent representation extraction, similarity graph learning, and cluster label allocation, in a unified framework. These three subtasks are seamlessly integrated and can be mutually boosted by each other towards the overall optimal solution. An efficient alternation algorithm is proposed to solve the optimization problem. Experimental results on several benchmark datasets illustrate the effectiveness of the proposed model.},
  archive      = {J_TKDE},
  author       = {Shudong Huang and Ivor W. Tsang and Zenglin Xu and Jiancheng Lv},
  doi          = {10.1109/TKDE.2022.3192686},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7082-7087},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Latent representation guided multi-view clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KG-MTL: Knowledge graph enhanced multi-task learning for
molecular interaction. <em>TKDE</em>, <em>35</em>(7), 7068–7081. (<a
href="https://doi.org/10.1109/TKDE.2022.3188154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular interaction prediction is essential in various applications including drug discovery and material science. The problem becomes quite challenging when the interaction is represented by unmapped relationships in molecular networks, namely molecular interaction, because it easily suffers from (i) insufficient labeled data with many false-positive samples, and (ii) ignoring a large number of biological entities with rich information in the knowledge graph. Most of the existing methods cannot properly exploit the information of knowledge graph and molecule graph simultaneously. In this paper, we propose a large-scale K nowledge G raph enhanced M ulti- T ask L earning model, namely KG-MTL, which extracts the features from both knowledge graph and molecular graph in a synergistic way. Moreover, we design an effective Shared Unit that helps the model to jointly preserve the semantic relations of drug entity and the neighbor structures of the compound in both knowledge graph and molecular graph. Extensive experiments on four real-world datasets demonstrate that our proposed KG-MTL outperforms the state-of-the-art methods on two representative molecular interaction prediction tasks: drug-target interaction prediction and compound-protein interaction prediction. The source code of KG-MTL is available at https://github.com/xzenglab/KG-MTL .},
  archive      = {J_TKDE},
  author       = {Tengfei Ma and Xuan Lin and Bosheng Song and Philip S. Yu and Xiangxiang Zeng},
  doi          = {10.1109/TKDE.2022.3188154},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7068-7081},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {KG-MTL: Knowledge graph enhanced multi-task learning for molecular interaction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Job and employee embeddings: A joint deep learning approach.
<em>TKDE</em>, <em>35</em>(7), 7056–7067. (<a
href="https://doi.org/10.1109/TKDE.2022.3180593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accumulated massive job and employee data at various platforms such as LinkedIn and Glassdoor are very valuable for many online applications such as job/employee search and recommendations. In order to exploit these data, an interesting and practical problem is how to learn effective job and employee representations, which could be further utilized for many computing tasks such as searching for similar jobs and employees. Yet this problem is very challenging because these user-generated job and employee data are semi-structured and created without standards, which makes them very messy, sparse, and difficult to model. Developing novel and advanced methods to learn job and employee representations has become an urgent need. To this end, in this paper, we develop a novel neural network model for job and employee embeddings. Our proposed approach consists of three components to model career data from three levels of granularity: job content, job context, and job sequence. We fine-tune a transformer model to learn the semantics of massive text in job content, build a shallow neural network to accumulate contextual information in job sequences, and develop an RNN encoder-decoder model to learn representations of employees’ career paths. To evaluate the proposed method, we conduct two experimental tasks: job similarity and employee similarity searches. The experimental results with a real-world dataset demonstrate the superiority of the developed approach.},
  archive      = {J_TKDE},
  author       = {Hao Liu and Yong Ge},
  doi          = {10.1109/TKDE.2022.3180593},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7056-7067},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Job and employee embeddings: A joint deep learning approach},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Iteratively re-weighted method for sparsity-inducing norms.
<em>TKDE</em>, <em>35</em>(7), 7045–7055. (<a
href="https://doi.org/10.1109/TKDE.2022.3179554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among a big body of recently developed algorithms for machine learning and data mining, a class of models using non-convex/non-smooth sparsity-inducing norms achieves promising results on many challenging tasks. An important problem faced with such models is to find an effective solution for the objective function with one or multiple intractable terms. Although a large number of optimization approaches have been developed, most of them are tailored to a specific model. Besides, these approaches generally introduce some additional parameters and no longer guarantee convergence. In this work, we first revisit some representative non-convex/non-smooth machine learning models, and then unity them into a generic formulation. Theoretically, we develop a simple yet efficient optimization framework, namely Iteratively Re-Weighted method (IRW), to solve such a class of models and provide the corresponding convergence analysis. Particularly, we validate our proposed method on two challenging machine learning tasks: multi-task regression and feature selection. Source codes are available at: https://github.com/KDD-Code/Sparse.git .},
  archive      = {J_TKDE},
  author       = {Feiping Nie and Zhanxuan Hu and Xiaoqian Wang and Xuelong Li and Heng Huang},
  doi          = {10.1109/TKDE.2022.3179554},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7045-7055},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Iteratively re-weighted method for sparsity-inducing norms},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental graph computation: Anchored vertex tracking in
dynamic social networks. <em>TKDE</em>, <em>35</em>(7), 7030–7044. (<a
href="https://doi.org/10.1109/TKDE.2022.3199494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User engagement has recently received significant attention in understanding the decay and expansion of communities in many online social networking platforms. When a user chooses to leave a social networking platform, it may cause a cascading dropping out among her friends. In many scenarios, it would be a good idea to persuade critical users to stay active in the network and prevent such a cascade because critical users can have significant influence on user engagement of the whole network. Many user engagement studies have been conducted to find a set of critical (anchored) users in the static social network. However, social networks are highly dynamic and their structures are continuously evolving. In order to fully utilize the power of anchored users in evolving networks, existing studies have to mine multiple sets of anchored users at different times, which incurs an expensive computational cost. To better understand user engagement in evolving network, we target a new research problem called Anchored Vertex Tracking (AVT) in this article, aiming to track the anchored users at each timestamp of evolving networks. Nonetheless, it is nontrivial to handle the AVT problem which we have proved to be NP-hard. To address the challenge, we develop a greedy algorithm inspired by the previous anchored $k$ -core study in the static networks. Furthermore, we design an incremental algorithm to efficiently solve the AVT problem by utilizing the smoothness of the network structure&#39;s evolution. The extensive experiments conducted on real and synthetic datasets demonstrate the performance of our proposed algorithms and the effectiveness in solving the AVT problem.},
  archive      = {J_TKDE},
  author       = {Taotao Cai and Shuiqiao Yang and Jianxin Li and Quan Z. Sheng and Jian Yang and Xin Wang and Wei Emma Zhang and Longxiang Gao},
  doi          = {10.1109/TKDE.2022.3199494},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7030-7044},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incremental graph computation: Anchored vertex tracking in dynamic social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypergraph representation for detecting 3D objects from
noisy point clouds. <em>TKDE</em>, <em>35</em>(7), 7016–7029. (<a
href="https://doi.org/10.1109/TKDE.2022.3179608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to detect 3D objects from noise point clouds by Graph Neural Networks (GNNs), though graph-based methods have shown promising results in 3D classifications. Since strong robustness against noise is offered by hypergraph, a relative paradigm named HyperGraph Construction-Compression-Conversion (HG3C) is proposed for detecting 3D objects from noise point clouds. Our method presents the capacity of reducing graph redundancy and capturing the variances from multiple features, by pre-encoding the graph, to improve the graph representations in point clouds. A fused graph neural network is further designed to predict the shape and category of the target in converted graphs. The experiments, on both the KITTI and Nuscene, show that the proposed approach achieves leading accuracy. Our results demonstrate the potential of using the hypergraph transformation to extract and compress point cloud information from noisy point clouds.},
  archive      = {J_TKDE},
  author       = {Ping Jiang and Xiaoheng Deng and Leilei Wang and Zailiang Chen and Shichao Zhang},
  doi          = {10.1109/TKDE.2022.3179608},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7016-7029},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hypergraph representation for detecting 3D objects from noisy point clouds},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyperbolic embedding of attributed and directed networks.
<em>TKDE</em>, <em>35</em>(7), 7003–7015. (<a
href="https://doi.org/10.1109/TKDE.2022.3188426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding – finding a low dimensional representation of the nodes with attributes in a hierarchical, directed network remains a challenging problem in the machine learning community. An emerging approach is to embed complex networks – networks of real-world systems – into hyperbolic space due to the fact that hyperbolic space can better naturally represent such a network&#39;s hierarchical structure. Existing hyperbolic embedding approaches, however, cannot handle the embedding of attributed directed networks to an arbitrary embedding dimension. To fill this gap, we introduce HEADNet, for Hyperbolic Embedding of Attributed Directed Networks, an algorithm based on extending previous works for embedding directed attributed networks to Gaussian distributions in hyperbolic space of arbitrary dimension. Through experimentation on a variety of both synthetic and real-world networks, we show that HEADNet can achieve competitive performance on common downstream machine learning tasks, including predicting directed links for previously unseen nodes. HEADNet provides an inductive hyperbolic embedding method for directed attributed networks, which opens the door to hyperbolic manifold learning on a wider range of real-world networks. The source code is freely available at https://github.com/DavidMcDonald1993/HEADNET .},
  archive      = {J_TKDE},
  author       = {David McDonald and Shan He},
  doi          = {10.1109/TKDE.2022.3188426},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {7003-7015},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hyperbolic embedding of attributed and directed networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). HATR-i: Hierarchical adaptive temporal relational
interaction for stock trend prediction. <em>TKDE</em>, <em>35</em>(7),
6988–7002. (<a href="https://doi.org/10.1109/TKDE.2022.3188320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock trend prediction is a hot issue in the Fintech field. Effective stock profiling is challenging due to highly non-stationary dynamics and complex interplays. Existing methods usually regard each stock independently or detect simplistic homogeneous structures. Practically, stock correlation originates from diverse aspects and underlying relationship signals are implicit in comprehensive graphs. Besides, RNNs are extensively used to simulate stock volatility while inadequate in capturing fine-granular patterns across local time snippets. To this end, in this paper we propose HATR-I, a Hierarchical Adaptive Temporal-Relational Interaction model to characterize and predict stock evolutions. Specifically, we grasp short- and long-term transition regularities of stock dynamics based on cascaded dilated convolutions and gating paths. By formulating different views of domain adjacency graphs into a unified multiplex network with edge attributes, we inject node- and semantic-level dual attention to refine the propagation of inter-stock collaborative information. Particularly, the stock pair matching is proceeding along each time-stage rather than until final compressed representations, meanwhile significant feature points and scales are identified considering the effect of time attenuation. Finally, we deduce latent shared clusters as global regularization to optimize the stock representations. Experiments on three real-world stock market datasets demonstrate the effectiveness of our proposed model.},
  archive      = {J_TKDE},
  author       = {Heyuan Wang and Tengjiao Wang and Shun Li and Shijie Guan},
  doi          = {10.1109/TKDE.2022.3188320},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6988-7002},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HATR-I: Hierarchical adaptive temporal relational interaction for stock trend prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GraphNAS++: Distributed architecture search for graph neural
networks. <em>TKDE</em>, <em>35</em>(7), 6973–6987. (<a
href="https://doi.org/10.1109/TKDE.2022.3178153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are popularly used to analyze non-euclidean graph data. Despite their successes, the design of graph neural networks requires heavy manual work and rich domain knowledge. Recently, neural architecture search algorithms are widely used to automatically design neural architectures for CNNs and RNNs. Inspired by the success of neural architecture search algorithms, we present a graph neural architecture search algorithm GraphNAS that enables automatic design of the best graph neural architecture based on reinforcement learning. Specifically, GraphNAS uses a recurrent network as the controller to generate variable-length strings that describe the architectures of graph neural networks, and trains the recurrent network with policy gradient to maximize the expected accuracy of the generated architectures on a validation data set. Moreover, based on GraphNAS, we design a new GraphNAS++ model using distributed neural architecture search. Compared with GraphNAS that generates and evaluates only one candidate architecture at each iteration, GraphNAS++ generates a mini-batch of candidate architectures and evaluates them in a distributed computing environment until convergence. Experiments on real-world graph datasets demonstrate that GraphNAS can design a novel network architecture that rivals the best human-invented architecture in terms of accuracy. Moreover, GraphNAS++ can speed up the design process at least five times by using the distributed training framework with GPUs.},
  archive      = {J_TKDE},
  author       = {Yang Gao and Peng Zhang and Hong Yang and Chuan Zhou and Yue Hu and Zhihong Tian and Zhao Li and Jingren Zhou},
  doi          = {10.1109/TKDE.2022.3178153},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6973-6987},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GraphNAS++: Distributed architecture search for graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GraphLIME: Local interpretable model explanations for graph
neural networks. <em>TKDE</em>, <em>35</em>(7), 6968–6972. (<a
href="https://doi.org/10.1109/TKDE.2022.3187455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph neural networks (GNN) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. However, explaining the effectiveness of GNN models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose GraphLIME, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. Through experiments on two real-world datasets, the explanations of GraphLIME are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.},
  archive      = {J_TKDE},
  author       = {Qiang Huang and Makoto Yamada and Yuan Tian and Dinesh Singh and Yi Chang},
  doi          = {10.1109/TKDE.2022.3187455},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6968-6972},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GraphLIME: Local interpretable model explanations for graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geo-ellipse-indistinguishability: Community-aware location
privacy protection for directional distribution. <em>TKDE</em>,
<em>35</em>(7), 6957–6967. (<a
href="https://doi.org/10.1109/TKDE.2022.3192360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directional distribution analysis has long served as a fundamental functionality in abstracting dispersion and orientation of spatial datasets. Spatial datasets that describe sensitive information of individuals such as health status and home addresses must be used and shared cautiously to protect individuals’ privacy. There is an inherent tension between the need of accurate directional distribution result and the requirement of individuals’ location privacy. Plenty of excellent location privacy protection approaches such as geo-indistinguishability can provide strong protection for locations but considerably at the expense of statistical quality of subsequent directional distribution analysis. In this paper, to protect individual location data for directional distribution, we define the geographic feature of community with covariance matrix and then propose a geo-ellipse-indistinguishability privacy notion incorporating this covariance matrix. As an instantiation of metric differential privacy, geo-ellipse-indistinguishability guarantees pairwise inputs cannot be distinguishable with the level proportional to privacy budget and Mahalanobis distance between them, given a randomized output. We also present elliptical privacy mechanisms to achieve this privacy definition on the basis of gamma distribution and multivariate normal distribution. We finally evaluate the empirical utility of the proposed mechanism in New York home addresses database. Our experiments demonstrate that under the same privacy level, our proposed elliptical approach can achieve significantly higher directional distribution utility than circular noise function based method.},
  archive      = {J_TKDE},
  author       = {Ying Zhao and Dong Yuan and Jia Tina Du and Jinjun Chen},
  doi          = {10.1109/TKDE.2022.3192360},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6957-6967},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Geo-ellipse-indistinguishability: Community-aware location privacy protection for directional distribution},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized divergence-based decision making method with an
application to pattern classification. <em>TKDE</em>, <em>35</em>(7),
6941–6956. (<a href="https://doi.org/10.1109/TKDE.2022.3177896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In decision-making systems, how to address uncertainty plays an important role for the improvement of system performance in uncertainty reasoning. Dempster–Shafer evidence (DSE) theory is an effective method to address uncertainty in decision-making problems by means of basic belief assignments (BBAs) and Dempster&#39;s combination rule. In the DSE theory, divergence measure between BBAs, which is beneficial for conflict information management in decision making, remains an open issue. In this paper, several generalized evidential divergences (EDs) are proposed and studied to measure the difference and discrepancy between BBAs in DSE theory, which have more universal applicability in decision theory. On this basis, a uniform $\mathcal {BJS}$ divergence-based decision-making algorithm is devised to improve the decision level. Furthermore, the extensions of weighted $\mathcal {BJS}$ to decision-making algorithms are discussed by considering not only subjective weights but also objective weights. Notably, this is the first work to propose the weighted $\mathcal {BJS}$ divergence in DSE theory providing a promising way to analyze decision-making problems from different perspectives. Besides, experiments demonstrate the effectiveness and superiority of the proposed methods. Finally, the proposed $\mathcal {BJS}$ -based decision-making algorithm is applied to pattern classification. The results validate that the proposed decision-making algorithm is beneficial for diverse real-world datasets and outperforms several well-known related works and demonstrates higher classification accuracy as well as robustness.},
  archive      = {J_TKDE},
  author       = {Fuyuan Xiao and Junhao Wen and Witold Pedrycz},
  doi          = {10.1109/TKDE.2022.3177896},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6941-6956},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Generalized divergence-based decision making method with an application to pattern classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Fast flexible bipartite graph model for co-clustering.
<em>TKDE</em>, <em>35</em>(7), 6930–6940. (<a
href="https://doi.org/10.1109/TKDE.2022.3194275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-clustering methods make use of the correlation between samples and attributes to explore the co-occurrence structure in data. These methods have played a significant role in gene expression analysis, image segmentation, and document clustering. In bipartite graph partition-based co-clustering methods, the relationship between samples and attributes is described by constructing a diagonal symmetric bipartite graph matrix, which is clustered by the philosophy of spectral clustering. However, this not only has high time complexity but also the same number of row and column clusters. In fact, the number of categories of rows and columns often changes in the real world. To address these problems, this paper proposes a novel fast flexible bipartite graph model for the co-clustering method (FBGPC) that directly uses the original matrix to construct the bipartite graph. Then, it uses the inflation operation to partition the bipartite graph in order to learn the co-occurrence structure of the original data matrix based on the inherent relationship between bipartite graph partitioning and co-clustering. Finally, hierarchical clustering is used to obtain the clustering results according to the set relationship of the co-occurrence structure. Extensive empirical results show the effectiveness of our proposed model and verify the faster performance, generality, and flexibility of our model.},
  archive      = {J_TKDE},
  author       = {Wei Chen and Hongjun Wang and Zhiguo Long and Tianrui Li},
  doi          = {10.1109/TKDE.2022.3194275},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6930-6940},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast flexible bipartite graph model for co-clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explicit message-passing heterogeneous graph neural network.
<em>TKDE</em>, <em>35</em>(7), 6916–6929. (<a
href="https://doi.org/10.1109/TKDE.2022.3185128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural network (GNN) has shown its prominent performance in representation learning of graphs but it has not been fully considered for heterogeneous graphs which contain more complex structures and rich semantics. The rich semantic information of heterogeneous graph can be usually revealed by meta-paths. Therefore, most of the existing GNN models designed for heterogeneous graphs utilize the meta-path based neighborhood sampler to divide a heterogeneous graph into multiple homogeneous subgraphs according to various meta-paths so that the homogeneous GNN can be applied to investigate heterogeneous graphs. Nevertheless, the way of embedding semantic information of meta-paths into multiple homogeneous graphs is implicit and ineffective, which cannot accurately capture the semantics of heterogeneous graphs. In this paper, we propose a novel semi-supervised GNN model named E xplicit M essage- P assing Heterogeneous Graph Neural Network (EMP), which executes the process of explicit message-passing along the meta-paths. Besides, we also propose a split method for meta-paths and consider mutual effect between various meta-paths in advance in the proposed model, so that the semantic information of the whole set of meta-paths can be captured accurately. Extensive experiments conducted on three real-world datasets demonstrate the superiority of the proposed model.},
  archive      = {J_TKDE},
  author       = {Lei Xu and Zhen-Yu He and Kai Wang and Chang-Dong Wang and Shu-Qiang Huang},
  doi          = {10.1109/TKDE.2022.3185128},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6916-6929},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Explicit message-passing heterogeneous graph neural network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable discrete collaborative filtering. <em>TKDE</em>,
<em>35</em>(7), 6901–6915. (<a
href="https://doi.org/10.1109/TKDE.2022.3185093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using hashing to learn the binary codes of users and items significantly improves the efficiency and reduces the space consumption of the recommender system. However, existing hashing-based recommender systems remain black boxes without any explainable outputs that illustrate why the system recommends the items. In this paper, we present a new end-to-end discrete recommendation framework based on the multi-task learning to simultaneously perform explainable and efficient recommendation. Toward this goal, an Explainable Discrete Collaborative Filtering (EDCF) method is proposed to preserve the user-item interaction features and semantic text features into binary hash codes by adaptively exploiting the correlations between the preference prediction task and the explanation generation task. At the online recommendation stage, EDCF makes efficient top-K recommendation by calculating the Hamming distances between the feature hash codes, and simultaneously generates natural language explanations for recommendation results through the explanation generation module. To obtain the hash codes directly from the end-to-end neural network, we introduce an attentive TextCNN and an Adaptive Tanh layer in the preference prediction task. For explanation generation, Long Short-Term Memory is employed to generate the explanations for recommendation results from the binary hash codes of user and item. Experiments demonstrate the superiority of the proposed method.},
  archive      = {J_TKDE},
  author       = {Lei Zhu and Yang Xu and Jingjing Li and Weili Guan and Zhiyong Cheng},
  doi          = {10.1109/TKDE.2022.3185093},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6901-6915},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Explainable discrete collaborative filtering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient multi-view k-means clustering with multiple anchor
graphs. <em>TKDE</em>, <em>35</em>(7), 6887–6900. (<a
href="https://doi.org/10.1109/TKDE.2022.3185683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering has attracted a lot of attention due to its ability to integrate information from distinct views, but how to improve efficiency is still a hot research topic. Anchor graph-based methods and k-means-based methods are two current popular efficient methods, however, both have limitations. Clustering on the derived anchor graph takes a while for anchor graph-based methods, and the efficiency of k-means-based methods drops significantly when the data dimension is large. To emphasize these issues, we developed an efficient multi-view k-means clustering method with multiple anchor graphs (EMKMC). It first constructs anchor graphs for each view and then integrates these anchor graphs using an improved k-means strategy to obtain sample categories without any extra post-processing. Since EMKMC combines the high-efficiency portions of anchor graph-based methods and k-means-based methods, its efficiency is substantially higher than current fast methods, especially when dealing with large-scale high-dimensional multi-view data. Extensive experiments demonstrate that, compared to other state-of-the-art methods, EMKMC can boost clustering efficiency by several to thousands of times while maintaining comparable or even exceeding clustering effectiveness.},
  archive      = {J_TKDE},
  author       = {Ben Yang and Xuetao Zhang and Zhongheng Li and Feiping Nie and Fei Wang},
  doi          = {10.1109/TKDE.2022.3185683},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6887-6900},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient multi-view K-means clustering with multiple anchor graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Edge-cloud polarization and collaboration: A comprehensive
survey for AI. <em>TKDE</em>, <em>35</em>(7), 6866–6886. (<a
href="https://doi.org/10.1109/TKDE.2022.3178211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influenced by the great success of deep learning via cloud computing and the rapid development of edge chips, research in artificial intelligence (AI) has shifted to both of the computing paradigms, i.e., cloud computing and edge computing. In recent years, we have witnessed significant progress in developing more advanced AI models on cloud servers that surpass traditional deep learning models owing to model innovations (e.g., Transformers, Pretrained families), explosion of training data and soaring computing capabilities. However, edge computing, especially edge and cloud collaborative computing, are still in its infancy to announce their success due to the resource-constrained IoT scenarios with very limited algorithms deployed. In this survey, we conduct a systematic review for both cloud and edge AI. Specifically, we are the first to set up the collaborative learning mechanism for cloud and edge modeling with a thorough review of the architectures that enable such mechanism. We also discuss potentials and practical experiences of some on-going advanced edge AI topics including pretraining models, graph neural networks and reinforcement learning. Finally, we discuss the promising directions and challenges in this field.},
  archive      = {J_TKDE},
  author       = {Jiangchao Yao and Shengyu Zhang and Yang Yao and Feng Wang and Jianxin Ma and Jianwei Zhang and Yunfei Chu and Luo Ji and Kunyang Jia and Tao Shen and Anpeng Wu and Fengda Zhang and Ziqi Tan and Kun Kuang and Chao Wu and Fei Wu and Jingren Zhou and Hongxia Yang},
  doi          = {10.1109/TKDE.2022.3178211},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6866-6886},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Edge-cloud polarization and collaboration: A comprehensive survey for AI},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distantly-supervised long-tailed relation extraction using
constraint graphs. <em>TKDE</em>, <em>35</em>(7), 6852–6865. (<a
href="https://doi.org/10.1109/TKDE.2022.3177226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label noise and long-tailed distributions are two major challenges in distantly supervised relation extraction. Recent studies have shown great progress on denoising, but paid little attention to the problem of long-tailed relations. In this paper, we introduce a constraint graph to model the dependencies between relation labels. On top of that, we further propose a novel constraint graph-based relation extraction framework(CGRE) to handle the two challenges simultaneously. CGRE employs graph convolution networks to propagate information from data-rich relation nodes to data-poor relation nodes, and thus boosts the representation learning of long-tailed relations. To further improve the noise immunity, a constraint-aware attention module is designed in CGRE to integrate the constraint information. Extensive experimental results indicate that CGRE achieves significant improvements over the previous methods for both denoising and long-tailed relation extraction.},
  archive      = {J_TKDE},
  author       = {Tianming Liang and Yang Liu and Xiaoyan Liu and Hao Zhang and Gaurav Sharma and Maozu Guo},
  doi          = {10.1109/TKDE.2022.3177226},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6852-6865},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Distantly-supervised long-tailed relation extraction using constraint graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovery of cross joins. <em>TKDE</em>, <em>35</em>(7),
6839–6851. (<a href="https://doi.org/10.1109/TKDE.2022.3192842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cross join between two attribute sets holds on a relation whenever its projection onto the union of the attribute sets is the cross join between its projections on the first and second attribute set. Hence, the cross join is a fundamental operator on database relations. For example, it can rewrite the division operator into a simple projection, or measure the independence of tuple values between two attribute sets during cardinality estimation. It is therefore surprising that we present the first research on the discovery problem of cross joins. We show that the problem of deciding whether there is a cross join that holds on a given relation is not only NP-complete but W[3]-complete in its arguably most natural parameter, namely its arity. We establish the first algorithms that discover all cross joins that hold on a given relation. We illustrate in experiments with benchmark data that our algorithms perform well within the limits established by our hardness results. Our treatment of cross joins and the design of our algorithms enables us to extend our findings to the discovery of cross joins that meet a given approximation ratio. Our experiments quantify the trade-off between discovery time and targeted ratio.},
  archive      = {J_TKDE},
  author       = {Miika Hannula and Zhuoxing Zhang and Bor-Kuan Song and Sebastian Link},
  doi          = {10.1109/TKDE.2022.3192842},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6839-6851},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discovery of cross joins},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development of fully convolutional neural networks based on
discretization in time series classification. <em>TKDE</em>,
<em>35</em>(7), 6827–6838. (<a
href="https://doi.org/10.1109/TKDE.2022.3177724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Series Classification (TSC) is a crucial area in machine learning. Although applications of Deep Neural Networks (DNNs) in this area have led to relatively good results, classifying this kind of data is a major challenge. This issue is due to the nature of time-series as it involves high data volume, unfavorable elements such as noise, inconsistency, and missing values. Shallow approaches solve these challenges using temporal discretization. Despite offering significant advantages, deep learning (DL) models do not apply temporal discretization in an end-to-end manner. This paper develops three end-to-end DL models, namely FCN-DISC, LSTM-FCN-DISC, and ALSTM-FCN-DISC, to integrate the benefits of temporal discretization and deep network architecture. The proposed models attempt to select the values of the input time-series that play a more effective role in model training by embedding temporal discretization in deep network architecture. These models use two loss functions to construct a discretized time series and optimize network weights. Thus, a new loss function for discretization was also introduced in addition to the cross-entropy. Experiments on univariate TSC datasets demonstrate that the proposed models, in most cases, outperform the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Marzieh Hajizadeh Tahan and Mohammad Ghasemzadeh and Shahrokh Asadi},
  doi          = {10.1109/TKDE.2022.3177724},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6827-6838},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Development of fully convolutional neural networks based on discretization in time series classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep generative networks coupled with evidential reasoning
for dynamic user preferences using short texts. <em>TKDE</em>,
<em>35</em>(7), 6811–6826. (<a
href="https://doi.org/10.1109/TKDE.2022.3188497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seeking an efficient solution for the problem of dynamic user preferences on social networks is challenging because the input data are short texts and user preferences usually change over time. This work proposes a novel framework that tackles these challenges based on deep neural networks and the Dempster-Shafer theory of evidence. The framework consists of three primary phases: (1) learning the hidden space of user texts; (2) word generation and mass inference; and (3) mass combination and keyword extraction. In the first phase, user texts are grouped into small batches according to timestamps. Each batch is used for separately training two types of neural networks, the Variational Autoencoder (VAE) and the Generative Adversarial Network (GAN). In the second phase, the generators in the trained VAE and GAN work independently as two experts to generate bunches of tokens for modeling user preferences. Each bunch is considered as one piece of evidence, and is transformed into the so-called mass function in Dempster-Shafer theory by maximum a posterior estimation. In the final phase, Dempster&#39;s rule of combination is utilized for fusing the two independent pieces of evidence into an overall mass. This mass is used for extracting top keywords to form the user preferences within a specific time span. The experiments on short text datasets verified that the proposed method outperforms baseline models on many evaluation metrics. Additionally, the output of the proposed framework could be used for visualization, which is useful in many practical applications.},
  archive      = {J_TKDE},
  author       = {Duc-Vinh Vo and Trung-Tin Tran and Kiyoaki Shirai and Van-Nam Huynh},
  doi          = {10.1109/TKDE.2022.3188497},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6811-6826},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep generative networks coupled with evidential reasoning for dynamic user preferences using short texts},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep cross-modal proxy hashing. <em>TKDE</em>,
<em>35</em>(7), 6798–6810. (<a
href="https://doi.org/10.1109/TKDE.2022.3187023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high retrieval efficiency and low storage cost for cross-modal search tasks, cross-modal hashing methods have attracted considerable attention from the researchers. For the supervised cross-modal hashing methods, how to make the learned hash codes sufficiently preserve semantic information contained in the label of datapoints is the key to further enhance the retrieval performance. Hence, almost all supervised cross-modal hashing methods usually depend on defining similarities between datapoints with the label information to guide the hashing model learning fully or partly. However, the defined similarity between datapoints can only capture the label information of datapoints partially and misses abundant semantic information, which then hinders the further improvement of retrieval performance. Thus, in this paper, different from previous works, we propose a novel cross-modal hashing method without defining the similarity between datapoints, called Deep Cross-modal Proxy Hashing (DCPH). Specifically, DCPH first trains a proxy hashing network to transform each category information of a dataset into a semantic discriminative hash code, called proxy hash code. Each proxy hash code can preserve the semantic information of its corresponding category well. Next, without defining the similarity between datapoints to supervise the training process of the modality-specific hashing networks, we propose a novel margin-dynamic-softmax loss to directly utilize the proxy hashing codes as supervised information. Finally, by minimizing the novel margin-dynamic-softmax loss , the modality-specific hashing networks can be trained to generate hash codes that can simultaneously preserve the cross-modal similarity and abundant semantic information well. Extensive experiments on three benchmark datasets show that the proposed method outperforms the state-of-the-art baselines in the cross-modal retrieval tasks.},
  archive      = {J_TKDE},
  author       = {Rong-Cheng Tu and Xian-Ling Mao and Rong-Xin Tu and Binbin Bian and Chengfei Cai and Hongfa Wang and Wei Wei and Heyan Huang},
  doi          = {10.1109/TKDE.2022.3187023},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6798-6810},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep cross-modal proxy hashing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DDRM: A continual frequency estimation mechanism with local
differential privacy. <em>TKDE</em>, <em>35</em>(7), 6784–6797. (<a
href="https://doi.org/10.1109/TKDE.2022.3177721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications rely on continual data collection to provide real-time information services, e.g., real-time road traffic forecasts. However, the collection of original data brings risks to user privacy. Recently, local differential privacy (LDP) has emerged as a private data collection framework for mass population. However, for continual data collection, existing LDP schemes, e.g., those employing the memoization technique, are known to have privacy leakage on data change points over time. In this paper, we propose a new scheme with stronger privacy guarantee for continual frequency estimation under LDP, namely, D ynamic D ifference R eport M echanism (DDRM). In DDRM, we introduce difference trees to capture the data changes over time, which well addresses possible privacy leakage on data change points. As for the utility enhancement, DDRM exploits the common case of no data change in time series and thereby suppresses the consumption of privacy budget in such cases. Meanwhile, an optimal privacy budget allocation scheme is proposed to encourage users to report more data for better estimation accuracy. By both theoretical analysis and experimental evaluations, we show DDRM achieves highly accurate frequency estimation in real time.},
  archive      = {J_TKDE},
  author       = {Qiao Xue and Qingqing Ye and Haibo Hu and Youwen Zhu and Jian Wang},
  doi          = {10.1109/TKDE.2022.3177721},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6784-6797},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DDRM: A continual frequency estimation mechanism with local differential privacy},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). ConPhrase: Enhancing context-aware phrase mining from text
corpora. <em>TKDE</em>, <em>35</em>(7), 6767–6783. (<a
href="https://doi.org/10.1109/TKDE.2022.3193126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phrase mining is an essential step when transforming unstructured text into structured information, in which the aim is to extract high-quality phrases from given corpora automatically. Existing statistics-based methods have achieved state-of-the-art performance on this task. However, such methods often rely heavily on statistical signals to extract quality phrases, ignoring the effect of contextual information . In this paper, we propose a novel context-aware method, called ConPhrase, for quality phrase mining under distantly supervised settings. Specifically, ConPhrase formulates phrase mining as a sequence labeling problem by considering local contextual information, and also incorporates distant supervision methods to automatically generate labeled data. It comprises two modules designed to tackle global information scarcity and noisy data filtration: 1) a topic-aware phrase recognition network that incorporates domain-related topic information into word representation learning to identify quality phrases effectively; 2) an instance selection network that focuses on choosing correct sentences with reinforcement learning for improving the prediction performance of the phrase recognition network. Moreover, we also propose an extended variant of ConPhrase, called ConPhrase+, that further enhances phrase recognition by utilizing document-level contextual information across sentences within the entire document. Experimental results show that contextual information is indispensable for phrase mining and our context-aware methods perform significantly better than state-of-the-art approaches on three publicly available datasets.},
  archive      = {J_TKDE},
  author       = {Xue Zhang and Qinghua Li and Cuiping Li and Hong Chen},
  doi          = {10.1109/TKDE.2022.3193126},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6767-6783},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ConPhrase: Enhancing context-aware phrase mining from text corpora},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collecting preference rankings under local differential
privacy. <em>TKDE</em>, <em>35</em>(7), 6752–6766. (<a
href="https://doi.org/10.1109/TKDE.2022.3186907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the deep penetration of the Internet and mobile devices, preference rankings are being collected on a massive scale by diverse data collectors for various business demands. However, users’ preference rankings in many applications are highly sensitive. Without proper privacy protection mechanisms, it either puts individual privacy in jeopardy or hampers business opportunities due to users’ unwillingness to share their true rankings. In this paper, we initiate the study of collecting preference rankings under local differential privacy. The key technical challenge comes from the fact that the number of possible rankings could be large in practical settings, leading to excessive injected noise. To solve this problem, we present a novel approach SAFARI, whose main idea is to collect a set of distributions over small domains which are carefully chosen based on the riffle independent (RI) model to approximate the overall distribution of users’ rankings, and then generate a synthetic ranking dataset from the obtained distributions. By working on small domains instead of a large domain, SAFARI can significantly reduce the magnitude of added noise. In SAFARI, we design two transformation rules, namely Rule I and Rule II, to instruct users to transform their data to provide the information about the distributions of the small domains. In particular, we propose a method called LADE to precisely estimate the required distributions used for the structure learning of RI model. We also propose a new LDP method called SAFA for frequency estimation over multiple attributes that have small domains. We formally prove that SAFARI guarantees $\varepsilon$ -local differential privacy. Extensive experiments on real datasets confirm the effectiveness of SAFARI.},
  archive      = {J_TKDE},
  author       = {Xiang Cheng and Jianyu Yang and Yufei Wang and Rui Chen and Sen Su and Yuejia Li},
  doi          = {10.1109/TKDE.2022.3186907},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6752-6766},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Collecting preference rankings under local differential privacy},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collecting geospatial data under local differential privacy
with improving frequency estimation. <em>TKDE</em>, <em>35</em>(7),
6739–6751. (<a href="https://doi.org/10.1109/TKDE.2022.3181049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geospatial data provides a lot of benefits for personalized services. However, since the geospatial data contains sensitive information about personal activities, collecting the raw data has a potential risk of leaking private information from the collectors. Recently, local differential privacy (LDP), which protects the privacy of users without trusting the collector, has been adopted to preserve privacy in many real applications. In this paper, we investigate the problem of collecting the locations of individual users under LDP, and propose a perturbation mechanism designed carefully to minimize the expected error of perturbed locations according to the privacy budget and the data domain. The frequency distribution of perturbed locations inevitably has a large error. To tackle the problem, we also propose a postprocessing algorithm to estimate the original frequency distribution of collected data by using convex optimization. By experiments with various real datasets, we show the effectiveness of the proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Daeyoung Hong and Woohwan Jung and Kyuseok Shim},
  doi          = {10.1109/TKDE.2022.3181049},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6739-6751},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Collecting geospatial data under local differential privacy with improving frequency estimation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Classification-labeled continuousization and multi-domain
spatio-temporal fusion for fine-grained urban crime prediction.
<em>TKDE</em>, <em>35</em>(7), 6725–6738. (<a
href="https://doi.org/10.1109/TKDE.2022.3180726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained urban crime prediction is of great significance to urban management and public safety. Previous crime prediction work has been done at a relatively coarse time granularity, which may suffer from two issues for fine-grained crime prediction. 1) The zero-inflation problem associated with fine-grained granularity. Crime occurrence is sparse, and when the time granularity becomes finer, it leads to a more sparse prediction label for this problem resulting in the zero inflation problem. 2) Insufficient amount of information involved in crime datasets. When the spatio-temporal granularity becomes smaller, more information from related fields needs to be introduced to extract spatio-temporal features to assist the analysis. To address the first issue, we introduce a classification-labeled continuousization strategy and a weighted loss function for sparse classification problem, making the model more likely to focus on non-zero elements in zero-inflated datasets. For the second issue, we propose a novel deep learning based model, termed attention-based spatio-temporal multi-domain fusion network, which fuses features from multiple datasets in related domains. We evaluate our method on six real-world datasets collected in New York City and experiments on our model show the advantages beyond many competitive baselines.},
  archive      = {J_TKDE},
  author       = {Shuai Zhao and Ruiqiang Liu and Bo Cheng and Daxing Zhao},
  doi          = {10.1109/TKDE.2022.3180726},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6725-6738},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Classification-labeled continuousization and multi-domain spatio-temporal fusion for fine-grained urban crime prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bloom filter with noisy coding framework for multi-set
membership testing. <em>TKDE</em>, <em>35</em>(7), 6710–6724. (<a
href="https://doi.org/10.1109/TKDE.2022.3199646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is on designing a compact data structure for multi-set membership testing that allows fast set querying. Multi-set membership testing is a fundamental operation for computing systems. Most existing schemes for multi-set membership testing are built upon Bloom filter and fall short in either storage space cost or query speed. To address this issue, we propose Noisy Bloom Filter (NBF), Error Corrected Noisy Bloom Filter (NBF-E), and Data-driven Noisy Bloom Filter (NBF-D) in this paper. We optimize their misclassification and false positive rates by theoretical analysis and present criteria for selection between NBF, NBF-E, and NBF-D. The key novelty of the three schemes is to store set ID information in a compact but noisy way that allows fast recording and querying and use a denoising method for querying. Especially, NBF-E incorporates asymmetric error-correcting coding techniques into NBF, and NBF-D encodes set ID basedt membership testing. on their cardinality. To evaluate NBF, NBF-E, and NBF-D in comparison with the prior art, we conducted experiments using real-world network traces. The results show that NBF, NBF-E, and NBF-D significantly advance the state-of-the-art on multi-se},
  archive      = {J_TKDE},
  author       = {Haipeng Dai and Jun Yu and Meng Li and Wei Wang and Alex X. Liu and Jinghao Ma and Lianyong Qi and Guihai Chen},
  doi          = {10.1109/TKDE.2022.3199646},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6710-6724},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Bloom filter with noisy coding framework for multi-set membership testing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BGNN-XML: Bilateral graph neural networks for extreme
multi-label text classification. <em>TKDE</em>, <em>35</em>(7),
6698–6709. (<a href="https://doi.org/10.1109/TKDE.2022.3193657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme multi-label text classification (XMTC) aims to tag a text instance with the most relevant subset of labels from an extremely large label set. XMTC has attracted much recent attention due to massive label sets yielded by modern applications, such as news annotation and product recommendation. The main challenges of XMTC are the data scalability and sparsity , thereby leading to two issues: i) the intractability to scale to the extreme label setting, ii) the presence of long-tailed label distribution, implying that a large fraction of labels have few positive training instances. To overcome these problems, we propose BGNN-XML, a scalable graph neural network framework tailored for XMTC problems. Specifically, we exploit label correlations via excavating their co-occurrence patterns and build a label graph based on the correlation matrix. We then conduct the attributed graph clustering by performing graph convolution with a low-pass graph filter to jointly model label dependencies and label features, which induces semantic label clusters. We further propose a bilateral-branch graph isomorphism network to decouple representation learning and classifier learning for better modeling tail labels . Experimental results on multiple benchmark datasets demonstrate that BGNN-XML significantly outperforms state-of-the-art baselines while maintaining comparable prediction efficiency and model size.},
  archive      = {J_TKDE},
  author       = {Daoming Zong and Shiliang Sun},
  doi          = {10.1109/TKDE.2022.3193657},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6698-6709},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BGNN-XML: Bilateral graph neural networks for extreme multi-label text classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond low-pass filtering: Graph convolutional networks with
automatic filtering. <em>TKDE</em>, <em>35</em>(7), 6687–6697. (<a
href="https://doi.org/10.1109/TKDE.2022.3186016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks are becoming indispensable for deep learning from graph-structured data. Most of the existing graph convolutional networks share two big shortcomings. First, they are essentially low-pass filters, thus the potentially useful middle and high frequency band of graph signals are ignored. Second, the bandwidth of existing graph convolutional filters is fixed. Parameters of a graph convolutional filter only transform the graph inputs without changing the curvature of a graph convolutional filter function. In reality, we are uncertain about whether we should retain or cut off the frequency at a certain point unless we have expert domain knowledge. In this paper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture the full spectrum of graph signals and automatically update the bandwidth of graph convolutional filters. While it is based on graph spectral theory, our AutoGCN is also localized in space and has a spatial form. Experimental results show that AutoGCN achieves significant improvement over baseline methods which only work as low-pass filters.},
  archive      = {J_TKDE},
  author       = {Zonghan Wu and Shirui Pan and Guodong Long and Jing Jiang and Chengqi Zhang},
  doi          = {10.1109/TKDE.2022.3186016},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6687-6697},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Beyond low-pass filtering: Graph convolutional networks with automatic filtering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoSrh: An embedding dimensionality search framework for
tabular data prediction. <em>TKDE</em>, <em>35</em>(7), 6673–6686. (<a
href="https://doi.org/10.1109/TKDE.2022.3186387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction over tabular data is often a crucial task in many real-life applications. Recent advances in deep learning give rise to various deep models for tabular data prediction. A common and essential step in these models is to vectorize raw input features in tabular data into dense embeddings. Choosing a suitable dimension for each feature is challenging yet necessary to improve model&#39;s performance and reduce memory cost of model parameters. Existing solutions to embedding dimensionality search always choose dimensions from a restricted candidate set. This restriction improves the search efficiency but would produce suboptimal embedding dimensions that hurt model&#39;s predictive performance. In this paper, we develop AutoSrh, a flexible embedding dimensionality search framework that can select varying dimensions for different features through differentiable optimization. The key idea of AutoSrh is to relax the search space to be continuous and optimize the selection of embedding dimensions via gradient descent. After optimization, AutoSrh performs embedding pruning to derive the mixed embedding dimensions and retrains the model to further improve the performance. Extensive experiments on five real-world tabular datasets demonstrate that AutoSrh can achieve better predictive performance than the existing approaches with 1.1 $\sim$ 1.6x lower training time cost and reserve model&#39;s predictive performance while reducing 50 $\sim$ 95\% embedding parameters.},
  archive      = {J_TKDE},
  author       = {Shuming Kong and Weiyu Cheng and Yanyan Shen and Linpeng Huang},
  doi          = {10.1109/TKDE.2022.3186387},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6673-6686},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AutoSrh: An embedding dimensionality search framework for tabular data prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An investigation of SMOTE based methods for imbalanced
datasets with data complexity analysis. <em>TKDE</em>, <em>35</em>(7),
6651–6672. (<a href="https://doi.org/10.1109/TKDE.2022.3179381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many binary class datasets in real-life applications are affected by class imbalance problem. Data complexities like noise examples, class overlap and small disjuncts problems are observed to play a key role in producing poor classification performance. These complexities tend to exist in tandem with class imbalance problem. Synthetic Minority Oversampling Technique (SMOTE) is a well-known method to re-balance the number of examples in imbalanced datasets. However, this technique cannot effectively tackle data complexities and it also has the capability of magnifying the degree of complexities. Also, the performance of the SMOTE is still not satisfactory. Therefore, various SMOTE variants have been proposed to overcome the downsides of SMOTE either by combining SMOTE with other algorithms or modifying the existing SMOTE algorithm. This paper aims to comparatively review the algorithms applied in SMOTE variants and investigate which data complexities are being addressed in what variants. Series of experiments are conducted on 24 binary class imbalanced datasets to observe the changes in the data complexity measures after SMOTE variants were applied in these datasets. The evaluation metrics like G-Mean and F1-Score are also analyzed to investigate the difference in classification performance between SMOTE variants.},
  archive      = {J_TKDE},
  author       = {Nur Athirah Azhar and Muhammad Syafiq Mohd Pozi and Aniza Mohamed Din and Adam Jatowt},
  doi          = {10.1109/TKDE.2022.3179381},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6651-6672},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An investigation of SMOTE based methods for imbalanced datasets with data complexity analysis},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An experimental survey of missing data imputation
algorithms. <em>TKDE</em>, <em>35</em>(7), 6630–6650. (<a
href="https://doi.org/10.1109/TKDE.2022.3186498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the ubiquity of missing data, data imputation has received extensive attention in the past decades. It is a well-recognized problem impacting almost all fields of scientific study. Existing imputation algorithms differ in problem settings, model selection, and data evaluation. There is a lack of systematic comparison study among imputation algorithms. In this paper, we survey this interesting and evolving research topic by broadly reviewing and experimentally comparing the state-of-the-art missing data imputation algorithms. We analyze and categorize 19 imputation algorithms. Extensive experiments over 15 real-world benchmark datasets are conducted under various settings of data types, missing mechanisms, missing rates, dataset/model parameters, as well as the post-imputation prediction task. We shed light on a series of constructive insights on imputation algorithms to tackle imputation problem in real-life scenarios. Moreover, we put forward promising future directions for data imputation problem.},
  archive      = {J_TKDE},
  author       = {Xiaoye Miao and Yangyang Wu and Lu Chen and Yunjun Gao and Jianwei Yin},
  doi          = {10.1109/TKDE.2022.3186498},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6630-6650},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An experimental survey of missing data imputation algorithms},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive generalized multi-view canonical correlation
analysis for incrementally update multiblock data. <em>TKDE</em>,
<em>35</em>(7), 6616–6629. (<a
href="https://doi.org/10.1109/TKDE.2022.3185399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major problems in real-life multiblock dynamic data analysis is that all the available modalities may not be relevant. Some of them may provide noisy or even inconsistent information with respect to other modalities. So, it is necessary to evaluate the quality of a new modality before considering it for feature extraction. In this regard, the paper introduces a new multiset canonical correlation analysis (MCCA), termed as incremental MCCA (IMCCA). When a new modality is available for the analysis, the IMCCA generates the new canonical variables from that of the earlier modalities, without repeating the same procedure with the original data augmented by the new modality. The proposed IMCCA deals with the “curse of dimensionality” problem associated with multidimensional data sets, by using the ridge regression optimization technique. Using the proposed IMCCA model, a new feature extraction algorithm is introduced, which considers a new modality for the analysis if it has relevant and significant information with respect to existing modalities. The proposed algorithm starts with the two most relevant modalities, and the remaining modalities are added sequentially according to their relevance. The optimum regularization parameters for the proposed algorithm are estimated based on the supervised information of sample categories. The effectiveness of the proposed algorithm, along with a comparison with state-of-the-art multimodal data integration methods, is established on several real-life multiblock data sets.},
  archive      = {J_TKDE},
  author       = {Ankita Mandal and Pradipta Maji},
  doi          = {10.1109/TKDE.2022.3185399},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6616-6629},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive generalized multi-view canonical correlation analysis for incrementally update multiblock data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A survey on dropout methods and experimental verification
in recommendation. <em>TKDE</em>, <em>35</em>(7), 6595–6615. (<a
href="https://doi.org/10.1109/TKDE.2022.3187013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overfitting is a common problem in machine learning, which means the model too closely fits the training data while performing poorly in the test data. Among various methods of coping with overfitting, dropout is one of the representative ways. From randomly dropping neurons to dropping neural structures, dropout has achieved great success in improving model performances. Although various dropout methods have been designed and widely applied in past years, their effectiveness, application scenarios, and contributions have not been comprehensively summarized and empirically compared by far. It is the right time to make a comprehensive survey. In this paper, we systematically review previous dropout methods and classify them into three major categories according to the stage where dropout operation is performed. Specifically, more than seventy dropout methods published in top AI conferences or journals (e.g., TKDE, KDD, TheWebConf, SIGIR) are involved. The designed taxonomy is easy to understand and capable of including new dropout methods. Then, we further discuss their application scenarios, connections, and contributions. To verify the effectiveness of distinct dropout methods, extensive experiments are conducted on recommendation scenarios with abundant heterogeneous information. Finally, we propose some open problems and potential research directions about dropout that worth to be further explored.},
  archive      = {J_TKDE},
  author       = {Yangkun Li and Weizhi Ma and Chong Chen and Min Zhang and Yiqun Liu and Shaoping Ma and Yuekui Yang},
  doi          = {10.1109/TKDE.2022.3187013},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6595-6615},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on dropout methods and experimental verification in recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of context-aware recommender systems: From an
evaluation perspective. <em>TKDE</em>, <em>35</em>(7), 6575–6594. (<a
href="https://doi.org/10.1109/TKDE.2022.3187434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, context-aware recommender systems (CARSs), which incorporate contextual information to achieve better recommendations, become a hot topic in the domain of recommender systems. Many context-aware recommendation methods have been proposed in the past decades. Some literatures provide survey of the research on CARSs. However, they mainly focus on context-aware recommendation methods and overlook the evaluation of them. The evaluation methods, evaluation properties and datasets of CARSs are different from those of traditional recommender systems where contexts are not considered. In this paper, we provide a review for evaluation of CARSs. We will introduce the basic concepts of CARSs, propose a new dataset partition method for each category of CARSs according to our classification of CARSs, summarize the evaluation method. Then we summarize the evaluation properties that CARSs pays attention to, which are different from the NCARSs. In addition, we also summarize the datasets specifically for CARSs, its applicable CARSs type and its evaluation dimensions and metrics. Based on our review, we draw some conclusions from evaluation perspective and point out future research directions.},
  archive      = {J_TKDE},
  author       = {Xiangwu Meng and Yulu Du and Yujie Zhang and Xiaofeng Han},
  doi          = {10.1109/TKDE.2022.3187434},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6575-6594},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey of context-aware recommender systems: From an evaluation perspective},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid spiking neurons embedded LSTM network for
multivariate time series learning under concept-drift environment.
<em>TKDE</em>, <em>35</em>(7), 6561–6574. (<a
href="https://doi.org/10.1109/TKDE.2022.3178176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complicated temporal patterns can provide important information for accurate time series forecasting. Existing long short-term memory (LSTM) model with attention mechanism have achieved significant performance. However, the exponential decay of long-term memory of LSTM has not be resolved yet in these efforts, remaining a longstanding open problem in recurrent nature. This problem exhibits a bottleneck which restricts the performance of existing studies. Recently, spiking neural networks (SNNs) have shown high efficiency in capturing temporal patterns via the surrogate gradient (SG) method to resolve this issue. However, the concept-drift environment makes it impossible to pre-set the variance into the standard SG method due to time-varying data distribution. In this paper, we propose a novel adaptive and hybrid spiking (AHS) module embedded LSTM, collaborating with two attention mechanisms (called HSN-LSTM) to resolve above-mentioned problems. First, the AHS module is analyzed theoretically can remain long-term memory. Moreover, our smooth SG method avoids pre-setting of variance, which is not sensitive in the above scenarios. Besides, we use the negative log-likelihood function to adjust the attention score for alleviating the negative impact from the concept-drift. Experiment results show the HSN-LSTM outperformed the state-of-the-art models on several multivariate time series datasets.},
  archive      = {J_TKDE},
  author       = {Wendong Zheng and Putian Zhao and Gang Chen and Huihui Zhou and Yonghong Tian},
  doi          = {10.1109/TKDE.2022.3178176},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6561-6574},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A hybrid spiking neurons embedded LSTM network for multivariate time series learning under concept-drift environment},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A graph and attentive multi-path convolutional network for
traffic prediction. <em>TKDE</em>, <em>35</em>(7), 6548–6560. (<a
href="https://doi.org/10.1109/TKDE.2022.3179646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is an important and yet highly challenging problem due to the complexity and constantly changing nature of traffic systems. To address the challenges, we propose a graph and attentive multi-path convolutional network (GAMCN) model to predict traffic conditions such as traffic speed across a given road network into the future. Our model focuses on the spatial and temporal factors that impact traffic conditions. To model the spatial factors, we propose a variant of the graph convolutional network (GCN) named LPGCN to embed road network graph vertices into a latent space, where vertices with correlated traffic conditions are close to each other. To model the temporal factors, we use a multi-path convolutional neural network (CNN) to learn the joint impact of different combinations of past traffic conditions on the future traffic conditions. Such a joint impact is further modulated by an attention generated from an embedding of the prediction time, which encodes the periodic patterns of traffic conditions. We evaluate our model on real-world road networks and traffic data. The experimental results show that our model outperforms state-of-art traffic prediction models by up to 18.9\% in terms of prediction errors and 23.4\% in terms of prediction efficiency.},
  archive      = {J_TKDE},
  author       = {Jianzhong Qi and Zhuowei Zhao and Egemen Tanin and Tingru Cui and Neema Nassir and Majid Sarvi},
  doi          = {10.1109/TKDE.2022.3179646},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6548-6560},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A graph and attentive multi-path convolutional network for traffic prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A data-driven approach for scheduling bus services subject
to demand constraints. <em>TKDE</em>, <em>35</em>(7), 6534–6547. (<a
href="https://doi.org/10.1109/TKDE.2022.3188243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passenger satisfaction is extremely important for the success of a public transportation system. Many studies have shown that passenger satisfaction strongly depends on the time they have to wait at the bus stop (waiting time) to get on a bus. To be specific, user satisfaction drops faster as the waiting time increases. Therefore, service providers want to provide a bus to the waiting passengers within a threshold to keep them satisfied. It is a two-pronged problem: (a) to satisfy more passengers the transport planner may increase the frequency of the buses, and (b) in turn, the increased frequency may impact the service operational costs. To address it, we propose PASS and COST as the two variants that satisfy different optimization criteria mentioned above. The optimization goal of PASS is the number of satisfied passengers while the optimization goal of COST is the number of passengers served per unit of driving time. Consequently, PASS utilizes resources to the maximum to satisfy the highest number of passengers, while COST optimizes for both passenger satisfaction and operational costs. Accordingly, we propose two algorithms to solve PASS and COST respectively and evaluate their performance based on real passenger demand data-set.},
  archive      = {J_TKDE},
  author       = {Janaka Chathuranga Brahmanage and Thivya Kandappu and Baihua Zheng},
  doi          = {10.1109/TKDE.2022.3188243},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6534-6547},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A data-driven approach for scheduling bus services subject to demand constraints},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A collaborative alignment framework of transferable
knowledge extraction for unsupervised domain adaptation. <em>TKDE</em>,
<em>35</em>(7), 6518–6533. (<a
href="https://doi.org/10.1109/TKDE.2022.3185233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims to utilize knowledge from a label-rich source domain to understand a similar yet distinct unlabeled target domain. Notably, global distribution statistics across domains and local semantic characteristics across samples, are two essential factors of data analysis that should be fully explored. Most existing UDA approaches either harness only one of them or fail to closely associate them for efficient adaptation. In this work, we propose a unified framework, called Collaborative Alignment Framework (CAF), which simultaneously reduces the global domain discrepancy and preserves the local semantic consistency for cross-domain knowledge transfer in a collaborative manner. Specifically, for domain-oriented alignment, we utilize adversarial training or minimize the Wasserstein distance between the two distributions to learn domain-level invariant representations. For semantic-oriented matching, we capture the semantic discrepancy between the predictions of two diverse task-specific classifiers and enhance the features of target data to be near the support of the source data class-wisely, which promotes semantic consistency across domains effectively. These two adaptation processes can be deeply intertwined in CAF via collaborative training, thus CAF can learn domain-invariant and semantic-consistent feature representations. Extensive experiments on four popular benchmarks, including DomainNet, VisDA-2017, Office-31, and ImageCLEF, demonstrate the proposed methods significantly outperform the existing methods, especially on the large-scale dataset. The code is available at https://github.com/BIT-DA/CAF .},
  archive      = {J_TKDE},
  author       = {Binhui Xie and Shuang Li and Fangrui Lv and Chi Harold Liu and Guoren Wang and Dapeng Wu},
  doi          = {10.1109/TKDE.2022.3185233},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {7},
  pages        = {6518-6533},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A collaborative alignment framework of transferable knowledge extraction for unsupervised domain adaptation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subgraph-aware few-shot inductive link prediction via
meta-learning. <em>TKDE</em>, <em>35</em>(6), 6512–6517. (<a
href="https://doi.org/10.1109/TKDE.2022.3177212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction for knowledge graphs aims to predict missing connections between entities. Prevailing methods are limited to a transductive setting and hard to process unseen entities. The recently proposed subgraph-based models provide alternatives to predict links from the subgraph structure surrounding a candidate triplet. However, these methods require abundant known facts of training triplets and perform poorly on relationships that only have a few triplets. In this paper, we propose Meta-iKG, a novel subgraph-based meta-learner for few-shot inductive relation reasoning. Meta-iKG utilizes local subgraphs to transfer subgraph-specific information and to rapidly learn transferable patterns via meta-gradients. In this way, we find the model can quickly adapt to few-shot relationships using only a handful of known facts with inductive settings. Moreover, we introduce a large-shot relation updating procedure to ensure that our model can generalize well to both few-shot and large-shot relations. We evaluate Meta-iKG on inductive benchmarks sampled from the NELL and Freebase, and the results show that Meta-iKG outperforms the currently state-of-the-art methods in both few-shot scenarios and standard inductive settings.},
  archive      = {J_TKDE},
  author       = {Shuangjia Zheng and Sijie Mai and Ya Sun and Haifeng Hu and Yuedong Yang},
  doi          = {10.1109/TKDE.2022.3177212},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6512-6517},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Subgraph-aware few-shot inductive link prediction via meta-learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incomplete multi-view clustering with sample-level
auto-weighted graph fusion. <em>TKDE</em>, <em>35</em>(6), 6504–6511.
(<a href="https://doi.org/10.1109/TKDE.2022.3171911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering (IMC) has received considerable attention due to its flexibility in fusing the multi-view information when the view samples are partly missing. However, existing methods seldom consider the affection of the missing samples to the contributions of the views. In this paper, we propose a novel graph fusion based IMC model (SAGF_IMC) to handle this problem. Instead of directly weighting the whole view, SAGF_IMC learns the sample-level auto weight, which allows considering both the contributions of different views and the affection of the missing samples. An effective iterative algorithm is developed, together with its convergence analysis. Experiments are provided to demonstrate that SAGF_IMC is superior to the related state-of-the-art methods by using several real-world datasets.},
  archive      = {J_TKDE},
  author       = {Naiyao Liang and Zuyuan Yang and Shengli Xie},
  doi          = {10.1109/TKDE.2022.3171911},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6504-6511},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incomplete multi-view clustering with sample-level auto-weighted graph fusion},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). What is market talking about? Market-oriented prospect
analysis for entrepreneur fundraising. <em>TKDE</em>, <em>35</em>(6),
6489–6503. (<a href="https://doi.org/10.1109/TKDE.2022.3174336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, innovation and entrepreneurship have become buzz words. With entrepreneurial projects emerging in large numbers every day, the common intention of all investors, i.e., putting every penny into good entrepreneurial projects, is becoming more difficult. In reality, traditional research with empirical results is not practical for analyzing these newly launched projects of small and micro enterprises before production and sale. Actually, the future market prospect is an important criterion for evaluating entrepreneurial projects. However, this direction has not been well explored due to the limitations of scenarios and technical challenges especially for these small and micro enterprises. In this paper, we construct an interesting study of exploiting the market prospect from the sales markets (i.e., E-commerce) to help evaluate newly-posted campaigns in crowdfunding. Specifically, we propose a novel Market-oriented Prospect Analysis with Transferring Attention (MoPa-A) model which contains two learning modules, i.e., HostTask Learning and GuestTask Learning connected and enhanced by transferring attention. The former is designed for funding performance modeling with heterogeneous features of crowdfunding campaigns, and the latter is to represent and transfer the latent semantics of market prospect for target campaigns from campaigns’ comments with the help of relevant products in sales market. The model design of MoPa-A brings some new insights on flexible knowledge transfer for different or cross domains. With the real-world datasets collected from Indiegogo and Amazon, we construct extensive experiments and the results demonstrate the effectiveness of our MoPa-A model.},
  archive      = {J_TKDE},
  author       = {Hongke Zhao and Yihang Cheng and Xi Zhang and Hengshu Zhu and Qi Liu and Hui Xiong and Wei Zhang},
  doi          = {10.1109/TKDE.2022.3174336},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6489-6503},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {What is market talking about? market-oriented prospect analysis for entrepreneur fundraising},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Weakly-supervised enhanced semantic-aware hashing for
cross-modal retrieval. <em>TKDE</em>, <em>35</em>(6), 6475–6488. (<a
href="https://doi.org/10.1109/TKDE.2022.3172216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to its query and storage efficiency, hash learning has sparked much interest for Cross-Modal Retrieval (CMR) task. Previous literatures have proved the superiority of supervised Cross-Modal Hashing (CMH) methods over unsupervised ones. Nevertheless, most existing supervised CMH methods still suffer from some limitations: 1) it is assumed that the observed labels of training data are complete and accurate, which may be impractical due to the missing and wrong class assignments in real applications, and 2) the semantic information is not fully excavated, especially for the semantic correlations among labels. To address these issues, this paper proposes a Weakly-supervised enhAnced Semantic-aware Hashing (WASH) method which simultaneously estimates the label noises and performs enhanced semantic-aware hash learning. WASH employs the low-rank and sparse decomposition to alleviate the label noises, and a high-level semantic factor as well as a semantic correlation matrix is obtained by low-rank factorization on the noise-reduced labels. The low-rank semantic factors and multi-modal features are jointly factorized into a common subspace to reduce the heterogeneity gaps, so as to enhance the semantic awareness of shared representation. In this way, the hash codes can be obtained by binarizing the shared representation with pairwise semantic similarity preserved. Experiments on several benchmark datasets verify the effectiveness of the proposed method in comparison with the state-of-the-art CMH approaches.},
  archive      = {J_TKDE},
  author       = {Chao Zhang and Huaxiong Li and Yang Gao and Chunlin Chen},
  doi          = {10.1109/TKDE.2022.3172216},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6475-6488},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Weakly-supervised enhanced semantic-aware hashing for cross-modal retrieval},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WATCH: Two-stage discrete cross-media hashing.
<em>TKDE</em>, <em>35</em>(6), 6461–6474. (<a
href="https://doi.org/10.1109/TKDE.2022.3159131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the explosive growth of multimedia data in recent years, cross-media hashing (CMH) approaches have recently received increasing attention. To learn the hash codes, most existing supervised CMH algorithms employ the strict binary label information, which has tiny gaps between the incorrect labels (0) and the true labels (1), increasing the risk of classification error. Besides, most existing CMH approaches are one-stage algorithms, in which the hash functions and binary codes can be learned synchronously, complicating the optimization. To avoid NP-hard optimization, which is the consequence of the discrete constraints of the objective function, many approaches utilize a relaxation strategy. However, this optimisation trick may cause large quantization errors. To address these issues, we present a novel t W o-st A ge discre T e C ross-media H ashing method based on a smooth matrix factorization and label relaxation, named WATCH. The proposed WATCH controls the margins adaptively by the novel label relaxation strategy. This innovation reduces the quantization error significantly. Besides, WATCH is a two-stage model. In stage 1, we employ a discrete smooth matrix factorization model. Then, the hash codes can be generated discretely, reducing the large quantization loss greatly. In stage 2, we adopt an effective hash function learning strategy, which produces a more effective hash function. Comprehensive experiments on three popular datasets demonstrate that WATCH achieves significantly better performance than several state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Donglin Zhang and Xiao-Jun Wu and Tianyang Xu and Josef Kittler},
  doi          = {10.1109/TKDE.2022.3159131},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6461-6474},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {WATCH: Two-stage discrete cross-media hashing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unified one-step multi-view spectral clustering.
<em>TKDE</em>, <em>35</em>(6), 6449–6460. (<a
href="https://doi.org/10.1109/TKDE.2022.3172687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view spectral clustering, which exploits the complementary information among graphs of diverse views to obtain superior clustering results, has attracted intensive attention recently. However, most existing multi-view spectral clustering methods obtain the clustering partitions in a two-step scheme, i.e., spectral embedding and subsequent $k$ -means. This two-step scheme inevitably seeks sub-optimal clustering results due to the information loss during the two-steps processes. Besides, existing multi-view spectral clustering methods do not jointly utilize the information of graphs and embedding matrices, which also degrades final clustering results. To solve these issues, we propose a unified one-step multi-view spectral clustering method, which integrates the spectral embedding and $k$ -means into a unified framework to obtain discrete clustering labels with a one-step strategy. Under the observation that the inner product of the embedding matrix is a low-rank approximation of the graph, we combine graphs and embedding matrices of different views to obtain a unified graph. Then, we directly capture the discrete clustering indicator matrix from the unified graph. Furthermore, we design an effective optimization algorithm to solve the resultant problem. Finally, a set of experiments on various datasets are conducted to verify the effectiveness of the proposed method. The demo code of this work is publicly available at r gb]0,0,1 https://github.com/guanyuezhen/UOMvSC .},
  archive      = {J_TKDE},
  author       = {Chang Tang and Zhenglai Li and Jun Wang and Xinwang Liu and Wei Zhang and En Zhu},
  doi          = {10.1109/TKDE.2022.3172687},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6449-6460},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unified one-step multi-view spectral clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Tensor kalman filter and its applications. <em>TKDE</em>,
<em>35</em>(6), 6435–6448. (<a
href="https://doi.org/10.1109/TKDE.2022.3169129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kalman filter is one of the most important estimation algorithms, which estimates certain unknown variables given the measurements observed over time subject to a dynamic system, for many applications in science and engineering including environmental science, ecometrics, robotics, financial analysis, data mining, etc. With the recent emergence of the Big Data era, it is often necessary to characterize multiple relationships among various kinds of signals/data in tensor form. The conventional Kalman filter paradigm is based on the low-dimensional state-space representation, which is restricted by the state-transition, observation-model, process-noise covariance, and observation-noise covariance matrices. However, we often need to express some or all of them in terms of tensors in practice. Very lately, the aforementioned Kalman filter in tensor form was tackled using tensor decomposition but the exact estimator has never been established so far. In this work, we propose a new generalized Kalman filter framework consisting of state, state-transition model, observation-model, process-noise covariance, and observation-noise covariance tensors of arbitrary orders by applying the Sherman–Morrison–Woodbury identity and block tensor inverse, which we call “Tensor Kalman Filter” (TKF). Our proposed new approach can produce the exact Kalman filter estimator without any need of tensor decomposition (approximation). The pertinent computational- and memory-complexity studies are also provided in this paper. Finally, numerical experiments are conducted to evaluate the prediction performance of the proposed new TKF over biomedical signal data in comparison with other existing high-dimensional prediction methods.},
  archive      = {J_TKDE},
  author       = {Shih Yu Chang and Hsiao-Chun Wu},
  doi          = {10.1109/TKDE.2022.3169129},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6435-6448},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Tensor kalman filter and its applications},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Suggesting assess queries for interactive analysis of
multidimensional data. <em>TKDE</em>, <em>35</em>(6), 6421–6434. (<a
href="https://doi.org/10.1109/TKDE.2022.3171516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessment is the process of comparing the actual to the expected behavior of a business phenomenon and judging the outcome of the comparison. The ${{\sf assess}}$ querying operator has been recently proposed to support assessment based on the results of a query on a data cube. This operator requires (i) the specification of an OLAP query to determine a target cube; (ii) the specification of a reference cube of comparison (benchmark), which represents the expected performance; (iii) the specification of how to perform the comparison, and (iv) a labeling function that classifies the result of this comparison. Despite the adoption of a SQL-like syntax that hides the complexity of the assessment process, writing a complete assess statement is not easy. In this paper we focus on making the user experience more comfortable by letting the system suggest suitable completions for partially-specified statements. To this end we propose two interaction modes: progressive refinement and auto-completion, both starting from an assess statement partially declared by the user. These two modes are evaluated both in terms of scalability and user experience, with the support of two experiments made with real users.},
  archive      = {J_TKDE},
  author       = {Matteo Francia and Matteo Golfarelli and Patrick Marcel and Stefano Rizzi and Panos Vassiliadis},
  doi          = {10.1109/TKDE.2022.3171516},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6421-6434},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Suggesting assess queries for interactive analysis of multidimensional data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stable subgraph isomorphism search in temporal networks.
<em>TKDE</em>, <em>35</em>(6), 6405–6420. (<a
href="https://doi.org/10.1109/TKDE.2022.3175800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a new problem of seeking stable subgraph isomorphisms for a query graph in a temporal graph. To solve our problem, we first develop a pruning-based search algorithm using several new pruning tricks to prune the unpromising matching results during the search procedure. To further improve the efficiency, we propose a novel index structure called ${{\mathsf {BCCIndex}}}$ , based on an idea of bi-connected component decomposition of the query graph, which can efficiently support the stable subgraph isomorphism search. Equipped with the ${{\mathsf {BCCIndex}}}$ , we present an efficient query processing algorithm based on a carefully designed tree join technique. We conduct extensive experiments to evaluate our algorithms on four large real-life datasets, and the results demonstrate the efficiency and effectiveness of our algorithms.},
  archive      = {J_TKDE},
  author       = {Qi Zhang and Rong-Hua Li and Hongchao Qin and Guoren Wang and Zhiwei Zhang and Ye Yuan},
  doi          = {10.1109/TKDE.2022.3175800},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6405-6420},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Stable subgraph isomorphism search in temporal networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stable prediction with leveraging seed variable.
<em>TKDE</em>, <em>35</em>(6), 6392–6404. (<a
href="https://doi.org/10.1109/TKDE.2022.3169333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the problem of stable prediction across unknown test data, where the test distribution might be different from the training one and is always agnostic when model training. In such a case, previous machine learning methods might exploit subtly spurious correlations induced by non-causal variables in training data for prediction. Those spurious correlations can vary across datasets, leading to instability of prediction across unknown test data. To address this problem, we propose an algorithm based on conditional independence tests to screen out non-causal features and reduce spurious correlations by leveraging a seed variable. We show, both theoretically and with empirical experiments, that our algorithm can precisely screen out the isolated non-causal variables, which have no causal relationship with other variables, and remove the spurious correlations induced by them, increasing the stability of prediction across unknown test data. Extensive experiments on both synthetic and real-world datasets demonstrate that our algorithm outperforms state-of-the-art methods for stable prediction across unknown test data.},
  archive      = {J_TKDE},
  author       = {Kun Kuang and Haotian Wang and Yue Liu and Ruoxuan Xiong and Runze Wu and Weiming Lu and Yueting Zhuang and Fei Wu and Peng Cui and Bo Li},
  doi          = {10.1109/TKDE.2022.3169333},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6392-6404},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Stable prediction with leveraging seed variable},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial distribution-based imbalanced undersampling.
<em>TKDE</em>, <em>35</em>(6), 6376–6391. (<a
href="https://doi.org/10.1109/TKDE.2022.3161537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Undersampling is one of the most popular techniques for dealing with class-imbalance problems. Various undersampling methods have emerged over the past few decades. Each of them exhibits the superiority in some scenarios. However, selecting representative majority-class samples such that the structures of the selected groups are maintained according to the underlying imbalanced distribution remains a challenge. For this purpose, this paper proposes Spatial Distribution-based UnderSampling (SDUS) for imbalanced learning. SDUS uses a supervised constructive process to learn majority-class local patterns in terms of sphere neighborhoods (SPN). Two sample selection strategies, specifically, a top-down strategy and a bottom-up strategy, are proposed for maintaining the distribution pattern of original data in selecting majority-class sample subsets from different perspectives. SDUS introduces an ensemble technique that improves learning performance by utilizing the diversity caused by the randomness of the local-pattern learning process. Numerical experiments on 38 typical datasets from KEEL repository and 13 state-of-the-art comparison methods demonstrate the effectiveness of SDUS in maintaining the underlying distribution characteristics for imbalanced undersampling. The implementation of the proposed SDUS in programming language Python is available at https://github.com/ytyancp/SDUS .},
  archive      = {J_TKDE},
  author       = {Yuanting Yan and Yuanwei Zhu and Ruiqing Liu and Yiwen Zhang and Yanping Zhang and Ling Zhang},
  doi          = {10.1109/TKDE.2022.3161537},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6376-6391},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatial distribution-based imbalanced undersampling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Sparse and flexible projections for unsupervised feature
selection. <em>TKDE</em>, <em>35</em>(6), 6362–6375. (<a
href="https://doi.org/10.1109/TKDE.2022.3167996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, unsupervised feature selection methods have become increasingly popular. Nevertheless, most of the existing unsupervised feature selection methods suffer from two major problems that lead to suboptimal solutions. Many methods impose a hard linear projection constraint on original data, which is overly strict in nature and not suitable for dealing with data sampled from nonlinear manifolds. Second, most existing methods use $\ell _{2,p}$ -norm ( $0&amp;lt; p\leq 1$ ) regularization on projection matrix to obtain row sparse matrix and then calculate scores of each feature, which would introduce extra parameter with a slight possibility to directly obtain indexes of discriminative features. To solve the above problems, we propose two novel unsupervised feature selection methods called SF $^{2}$ S and SF $^{2}$ SOG, which can simultaneously learn optimal flexible projections and obtain an orthogonal sparse projection to directly select discriminative features by applying $\ell _{2,0}$ -norm constraint. Moreover, we propose to explore the local structure of flexible embedding through preserving the manifold structure of original data and adaptively constructing an optimal graph in subspace. Third, the novel iterative optimization algorithms are presented to solve objective functions guaranteeing convergence theoretically. Various evaluation experiments on synthetic and real-world datasets demonstrate the effectiveness and superiority of our proposed methods.},
  archive      = {J_TKDE},
  author       = {Rong Wang and Canyu Zhang and Jintang Bian and Zheng Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TKDE.2022.3167996},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6362-6375},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Sparse and flexible projections for unsupervised feature selection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Snippet policy network for multi-class varied-length ECG
early classification. <em>TKDE</em>, <em>35</em>(6), 6349–6361. (<a
href="https://doi.org/10.1109/TKDE.2022.3160706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arrhythmia detection from ECG is an important research subject in the prevention and diagnosis of cardiovascular diseases. The prevailing studies formulate arrhythmia detection from ECG as a time series classification problem. Meanwhile, early detection of arrhythmia presents a real-world demand for early prevention and diagnosis. In this paper, we address a problem of cardiovascular diseases early classification, which is a varied-length and long-length time series early classification problem as well. For solving this problem, we propose a deep reinforcement learning-based framework, namely Snippet Policy Network (SPN), consisting of four modules, snippet generator, backbone network, controlling agent, and discriminator. Comparing to the existing approaches, the proposed framework features flexible input length, solves the dual-optimization solution of the earliness and accuracy goals. Experimental results demonstrate that SPN achieves an excellent performance of over 80\% in terms of accuracy. Compared to the state-of-the-art methods, at least 7\% improvement on different metrics, including the precision, recall, F1-score, and harmonic mean, is delivered by the proposed SPN. To the best of our knowledge, this is the first work focusing on solving the cardiovascular early classification problem based on varied-length ECG data. Based on these excellent features from SPN, it offers a good exemplification for addressing all kinds of varied-length time series early classification problems.},
  archive      = {J_TKDE},
  author       = {Yu Huang and Gary G. Yen and Vincent S. Tseng},
  doi          = {10.1109/TKDE.2022.3160706},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6349-6361},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Snippet policy network for multi-class varied-length ECG early classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SNDProb: A probabilistic approach for streaming novelty
detection. <em>TKDE</em>, <em>35</em>(6), 6335–6348. (<a
href="https://doi.org/10.1109/TKDE.2022.3169229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A probabilistic framework for streaming novelty detection is proposed and illustrated with a mixture of Gaussian distributions that models the set of classes. Instances are predicted based on the probability of belonging to each of the classes. Those for which the model cannot provide confident predictions are introduced into a fixed-sized buffer. When the buffer is full, an Expectation Maximization (EM) algorithm is run to search for new emerging classes in the buffer, and update the current model. The EM algorithm has to deal with an scenario where both probability distributions and instances are available. To overcome this issue, the probability distributions (classes) are weighted. The weights are inferred using a meta-regression model which has been pretrained and supplied with the proposed algorithm. Experiments have been run using synthetic datasets to have a close control over the class arrival strategies, the shape, and the overlapping degree between classes. It is shown that when the assumptions of the probabilistic model are fulfilled, the proposed method outperforms literature non-parametric approaches. Furthermore it obtains competitive results in the case of non-Gaussian classes. The experiments reveal, for the first time, the high sensitivity of the novelty detection algorithms to the class arrival strategies.},
  archive      = {J_TKDE},
  author       = {Ander Carreño and Iñaki Inza and Jose A. Lozano},
  doi          = {10.1109/TKDE.2022.3169229},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6335-6348},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SNDProb: A probabilistic approach for streaming novelty detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-source personalized PageRanks with workload
robustness. <em>TKDE</em>, <em>35</em>(6), 6320–6334. (<a
href="https://doi.org/10.1109/TKDE.2022.3175814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a source node $s$ and a target node $t$ in a graph $G$ , the Personalized PageRank (PPR) from $s$ to $t$ is the probability of a random walk starting from $s$ terminates at $t$ . PPR is a classic measure of the relevance between two nodes in a graph. It has been applied in numerous real-world systems. However, existing techniques for PPR queries are not robust to dynamic real-world graphs, which typically have different evolving speeds. Their performance is significantly degraded either at a lower graph evolving rate (e.g., much more queries than updates) or a higher rate. To address the above deficiencies, we propose Agenda to efficiently process, with strong approximation guarantees, the single-source PPR (SSPPR) queries on dynamically evolving graphs with various evolving speeds. Compared with previous methods, Agenda has significantly better workload robustness, while ensuring the same result accuracy. Agenda also has theoretically-guaranteed small query and update costs. Experiments on up to billion-edge scale graphs show that Agenda significantly outperforms state-of-the-art methods for various query/update workloads, while maintaining better or comparable approximation accuracies.},
  archive      = {J_TKDE},
  author       = {Dingheng Mo and Siqiang Luo},
  doi          = {10.1109/TKDE.2022.3175814},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6320-6334},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Single-source personalized PageRanks with workload robustness},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SemCKD: Semantic calibration for cross-layer knowledge
distillation. <em>TKDE</em>, <em>35</em>(6), 6305–6319. (<a
href="https://doi.org/10.1109/TKDE.2022.3171571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a technique to enhance the generalization ability of a student model by exploiting outputs from a teacher model. Recently, feature-map based variants explore knowledge transfer between manually assigned teacher-student pairs in intermediate layers for further improvement. However, layer semantics may vary in different neural networks, resulting in performance degeneration due to negative regularization from semantic mismatch in manual layer associations. To address this issue, we propose semantic calibration for cross-layer knowledge distillation (SemCKD), which automatically assigns proper target layers of the teacher model for each student layer with an attention mechanism. With a learned attention distribution, each student layer distills knowledge contained in multiple teacher layers rather than a specific intermediate layer for appropriate cross-layer supervision. We further provide theoretical analysis of the association weights and conduct extensive experiments to demonstrate the effectiveness of our approach. On average, SemCKD improves the student Top-1 classification accuracy by 4.27\% across twelve different teacher-student model combinations on CIFAR-100. Code is available at https://github.com/DefangChen/SemCKD .},
  archive      = {J_TKDE},
  author       = {Can Wang and Defang Chen and Jian-Ping Mei and Yuan Zhang and Yan Feng and Chun Chen},
  doi          = {10.1109/TKDE.2022.3171571},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6305-6319},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SemCKD: Semantic calibration for cross-layer knowledge distillation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). RLCharge: Imitative multi-agent spatiotemporal
reinforcement learning for electric vehicle charging station
recommendation. <em>TKDE</em>, <em>35</em>(6), 6290–6304. (<a
href="https://doi.org/10.1109/TKDE.2022.3178819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric Vehicle (EV) has become a preferable choice in the modern transportation system due to its environmental and energy sustainability. However, in many large cities, EV drivers often fail to find the proper spots for charging, because of the limited charging infrastructures and the spatiotemporally unbalanced charging demands. Indeed, the recent emergence of deep reinforcement learning provides great potential to improve the charging experience from various aspects over a long-term horizon. In this paper, we propose an Imitative Multi-Agent Spatio-Temporal Reinforcement Learning ( RlCharge ) framework for intelligently recommending public accessible charging stations by jointly considering various long-term spatio-temporal factors. Specifically, by regarding each charging station as an individual agent, we formulate the problem as a multi-objective multi-agent reinforcement learning task. We first develop a multi-agent actor-critic framework with centralized training decentralized execution. Particularly, we propose a tailor-designed centralized attentive critic to coordinate the recommendation between geo-distributed agents, and introduce a delayed access strategy to exploit the knowledge of future charging competition during centralized training. Moreover, to handle the partial observability problem during decentralized execution in the large-scale multi-agent system, we propose the spatio-temporal heterogeneous graph convolution module, including (1) a dynamic graph convolution block to generate real-time representations for observable forthcoming EVs, and (2) a spatial graph convolution block to share the agent observations by message propagation between spatially adjacent agents. After that, to effectively optimize multiple divergent learning objectives, we extend the centralized attentive critic to multi-critics, and develop a dynamic gradient re-weighting strategy to adaptively guide the optimization direction. In addition, we propose an adaptive imitation learning scheme to further accelerate and stabilize the policy convergence. Finally, extensive experiments on two real-world datasets demonstrate that RlCharge achieves the best comprehensive performance compared with ten baseline approaches.},
  archive      = {J_TKDE},
  author       = {Weijia Zhang and Hao Liu and Hui Xiong and Tong Xu and Fan Wang and Haoran Xin and Hua Wu},
  doi          = {10.1109/TKDE.2022.3178819},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6290-6304},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RLCharge: Imitative multi-agent spatiotemporal reinforcement learning for electric vehicle charging station recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regulating systemic crises: Stemming the contagion risk in
networked-loans through deep graph learning. <em>TKDE</em>,
<em>35</em>(6), 6278–6289. (<a
href="https://doi.org/10.1109/TKDE.2022.3162339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In networked-loans, guarantor enterprises have a legal duty to repay debt to the commercial bank when the guaranteed borrower enterprise defaults (fail to repay). During an economic recession, the risk of defaults may spread like wildfire – the loan network structure could amplify both reach and impact; thus leading to a large-scale corporation defaults even systemic financial crises. The Central Bank urges advanced regulation technology to recognize and act on the contagion risk in order to avoid the “gray rhino”. Therefore, we present a novel approach to help the regulators quantify the systemic risk and provide stemming clues. In particular, we report a state-of-the-art graph neural network architecture (iConReg) for detecting and isolating of contagion risk in China’s national-wide networked-loans. The overall accuracy of our model reaches over 91\% of AUC (Area under the ROC Curve), which considerably outperforms the compared benchmark methods. By isolating the top 1\% of predicted high-risky nodes in the contagion chains, iConReg reports a significant shrink (averaged 25.8\%) of loan default rates. Moreover, we conduct extensive case and user studies to evaluate the effectiveness of our proposed method and the result also demonstrates its superior performance. Our presented approach opens up a new direction of using deep graph learning techniques to regulate the contagion risk of networked-loans, which enables the authorities to design more prompt prevention measures against systemic financial crises.},
  archive      = {J_TKDE},
  author       = {Dawei Cheng and Zhibin Niu and Jie Li and Changjun Jiang},
  doi          = {10.1109/TKDE.2022.3162339},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6278-6289},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Regulating systemic crises: Stemming the contagion risk in networked-loans through deep graph learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Region or global? A principle for negative sampling in
graph-based recommendation. <em>TKDE</em>, <em>35</em>(6), 6264–6277.
(<a href="https://doi.org/10.1109/TKDE.2022.3155155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based recommendation systems are blossoming recently, which models user-item interactions as a user-item graph and utilizes graph neural networks (GNNs) to learn the embeddings for users and items. A fundamental challenge of graph-based recommendation is that there only exists observed positive user-item pairs in the user-item graph. Negative sampling is a vital technique to solve the one-class problem and is widely used in many recommendation methods. However, the previous works only focus on the design of negative sampling distribution but ignore the sampled region for negative sampling. In this work, we propose the Three-Region Principle to guide negative sampling, which suggests that we should negatively sample more items at an intermediate region and less adjacent and distant items. In light of this principle, we present the RecNS method, which is a general negative sampling method designed with two sampling strategies: positive-assisted sampling and exposure-augmented sampling. Instead of sampling existing negative items from graph data, we merge these two strategies in embedding space to generate negative item embeddings. Extensive experiments demonstrate that RecNS method significantly outperforms all negative sampling baselines, e.g., $10.47\%$ for PinSage, $6.02\%$ for NGCF, and $8.20\%$ for LightGCN in terms of Recall@20 on the Alibaba dataset.},
  archive      = {J_TKDE},
  author       = {Zhen Yang and Ming Ding and Xu Zou and Jie Tang and Bin Xu and Chang Zhou and Hongxia Yang},
  doi          = {10.1109/TKDE.2022.3155155},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6264-6277},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Region or global? a principle for negative sampling in graph-based recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rearranging “indivisible” blocks for community detection.
<em>TKDE</em>, <em>35</em>(6), 6252–6263. (<a
href="https://doi.org/10.1109/TKDE.2022.3173271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unattributed social networks are more complicated, and it tends not to determine the best division by over-optimizing a theoretical measure for unsupervised algorithms. Nowadays, communities strongly overlap due to the fact that people strongly interact, which makes community detection even more challenging. The paper develops a new algorithm by rearranging ‘indivisible’ blocks (RaidB). In RaidB, we first initialize ‘indivisible’ blocks by disjoint $k$ -clique blocks in a network, and then these blocks are rearranged by moving nodes from one block to another based on maximizing modularity to uncover non-overlapping communities. For identifying overlapping communities, the above blocks are further rearranged, i.e., each block is subdivided and expanded to determine sub-blocks by introducing a dynamic linear threshold (DLT) model for influence interpenetration, and we finally determine a division from these sub-blocks with the minimum size that can cover the network. We compare RaidB with the existing state of the art methods for non-overlapping and overlapping community detection. The results show that RaidB tends to achieve better performance especially on sparse networks with unobvious communities and networks with strongly overlapping communities.},
  archive      = {J_TKDE},
  author       = {Peng Gang Sun and Xunlian Wu and Yining Quan and Qiguang Miao},
  doi          = {10.1109/TKDE.2022.3173271},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6252-6263},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Rearranging ‘indivisible’ blocks for community detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Reachability queries with label and substructure
constraints on knowledge graphs. <em>TKDE</em>, <em>35</em>(6),
6238–6251. (<a href="https://doi.org/10.1109/TKDE.2022.3161375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since knowledge graphs (KGs) describe and model the relationships between entities and concepts in the real world, reasoning on KGs often correspond to the r eachability queries with l abel and s ubstructure c onstraints (LSCR queries). Specifically, for a search path $p$ , LSCR queries not only require that the labels of the edges passed by $p$ are in a certain label set, but also claim that a vertex in $p$ could satisfy a certain substructure constraint. They are much more complex than existing label-constraint reachability (LCR) queries. LSCR queries on KGs can be addressed by two natural ways (EA-1) an online search algorithm and (EA-2) a combined search strategy, to the best of our knowledge. This paper presents two optimized algorithms for EA-1 and EA-2, but the optimized algorithms are still inefficient, since their efficiencies are highly dominated by their search directions as analyzed in this paper. Motivated by that, this paper presents an efficient in formed s earch strategy on K Gs, named INSK, with a lightweight index, named local index. An extensive experimental evaluation, on both synthetic and real KGs, illustrates that our INSK can efficiently process LSCR queries on KGs.},
  archive      = {J_TKDE},
  author       = {Xiaolong Wan and Hongzhi Wang},
  doi          = {10.1109/TKDE.2022.3161375},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6238-6251},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reachability queries with label and substructure constraints on knowledge graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy data diffusion modeling and preserving in online
social network. <em>TKDE</em>, <em>35</em>(6), 6224–6237. (<a
href="https://doi.org/10.1109/TKDE.2022.3176948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ubiquity of social media such as Twitter, Facebook and Instagram, privacy leakage has become a urgent problem for social media managers. Hence, studying how the privacy information diffuses through social media has attracted much attention. As a prerequisite, modeling privacy information diffusion is important research. Current approaches for modeling information diffusion are not available for privacy information since they did not consider the propagation features of privacy information in social media. This paper discusses the problem of modeling privacy information in social media and its challenges. We first analyse the information diffusion paths in the basic parameters of complex network and the high-order structures. We find that the privacy information is different in propagation features and the size of star structures. Second, a new information diffusion model is illustrated to simulate the diffusion process of information in social media by considering the following three parameters: 1) the probability of users receiving this message, 2) the probability that users have a tendency to forward this message and 3) the interest the users hold for this message. Finally, a block mechanism is designed to congest the diffusion of privacy information in social media. Our block mechanism considers not only the affection of congesting privacy propagation but also the user experience in social media.},
  archive      = {J_TKDE},
  author       = {Xiangyu Hu and Tianqing Zhu and Xuemeng Zhai and Hengming Wang and Wanlei Zhou and Wei Zhao},
  doi          = {10.1109/TKDE.2022.3176948},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6224-6237},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Privacy data diffusion modeling and preserving in online social network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PRESTO: Fast and effective group closeness maximization.
<em>TKDE</em>, <em>35</em>(6), 6209–6223. (<a
href="https://doi.org/10.1109/TKDE.2022.3178925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a graph and an integer $k$ , the goal of group closeness maximization is to find, among all possible sets of $k$ vertices (called seed sets ), a set that has the highest group closeness centrality. Existing techniques for this NP-hard problem strive to quickly find a seed set with a high , but not necessarily the highest centrality. We propose PRESTO , a new solution that can efficiently provide both approximate and exact answers to the group closeness maximization problem. PRESTO continuously calculates the centrality of different seed sets until a time limit is reached or it identifies a seed set with the highest possible centrality. It prioritizes seed sets to quickly find ones that are highly central and thus can be used as accurate approximate answers to the problem. Furthermore, PRESTO can proactively discard large groups of seed sets that cannot have the highest centrality, thereby drastically speeding up the discovery of approximate and exact answers. In our evaluations, compared to other state-of-the-art solutions, PRESTO finds seed sets that have up to 39\% higher centrality.},
  archive      = {J_TKDE},
  author       = {Baibhav Rajbhandari and Paul Olsen and Jeremy Birnbaum and Jeong-Hyon Hwang},
  doi          = {10.1109/TKDE.2022.3178925},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6209-6223},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PRESTO: Fast and effective group closeness maximization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Polestar++: An intelligent routing engine for national-wide
public transportation. <em>TKDE</em>, <em>35</em>(6), 6194–6208. (<a
href="https://doi.org/10.1109/TKDE.2022.3153711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public transportation plays a critical role in people&#39;s daily life. It has been proven that public transportation is more environmentally sustainable, efficient, and economical than any other forms of travel. However, due to the increasing expansion of transportation networks and more complex travel situations, people are having difficulties in efficiently finding the most preferred route from one place to another through public transportation systems for both intra-city and inter-city trips. To this end, in this paper, we present ${\mathsf {Polestar\tt {++}}}$ , a data-driven engine for intelligent and efficient public transportation routing. Specifically, we first propose a novel hierarchical public transportation graph (HPTG) to model both intra-city and inter-city public transportation in terms of various travel costs, such as time or distance. Then, we introduce a general route search algorithm coupled with an efficient station binding method for efficient route candidate generation. After that, we propose a two-pass route candidate ranking module to capture user preferences under dynamic travel situations. In particular, we propose two re-ranking models to decide the proper order of public routes: (1) a light-weight and explainable gradient boosting decision tree (GBDT) based model that integrates features from various urban data sources, and (2) a wide and deep learning (WDL) based model that automatically captures high order feature interactions from both inter-city and intra-city routes. Finally, experiments on two real-world data sets demonstrate the advantages of ${\mathsf {Polestar\tt {++}}}$ in terms of both efficiency and effectiveness. Indeed, in early 2019, ${\mathsf {Polestar\tt {++}}}$ has been deployed on Baidu Maps, one of the world&#39;s largest map services. To date, ${\mathsf {Polestar\tt {++}}}$ is servicing over 330 cities, answers over a hundred millions of queries each day, and achieves substantial improvement of user click ratio.},
  archive      = {J_TKDE},
  author       = {Hao Liu and Ying Li and Yanjie Fu and Huaibo Mei and Hui Xiong},
  doi          = {10.1109/TKDE.2022.3153711},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6194-6208},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Polestar++: An intelligent routing engine for national-wide public transportation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Permutation-equivariant and proximity-aware graph neural
networks with stochastic message passing. <em>TKDE</em>, <em>35</em>(6),
6182–6193. (<a href="https://doi.org/10.1109/TKDE.2022.3154391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are emerging machine learning models on graphs. Permutation-equivariance and proximity-awareness are two important properties highly desirable for GNNs. Both properties are needed to tackle some challenging graph problems, such as finding communities and leaders. In this paper, we first analytically show that the existing GNNs, mostly based on the message-passing mechanism, cannot simultaneously preserve the two properties. Then, we propose Stochastic Message Passing (SMP) model, a general and simple GNN to maintain both proximity-awareness and permutation-equivariance. In order to preserve node proximities, we augment the existing GNNs with stochastic node representations. We theoretically prove that the mechanism can enable GNNs to preserve node proximities, and at the same time, maintain permutation-equivariance with certain parametrization. We report extensive experimental results on ten datasets and demonstrate the effectiveness and efficiency of SMP for various typical graph mining tasks, including graph reconstruction, node classification, and link prediction.},
  archive      = {J_TKDE},
  author       = {Ziwei Zhang and Chenhao Niu and Peng Cui and Jian Pei and Bo Zhang and Wenwu Zhu},
  doi          = {10.1109/TKDE.2022.3154391},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6182-6193},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Permutation-equivariant and proximity-aware graph neural networks with stochastic message passing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Oversampling with reliably expanding minority class regions
for imbalanced data learning. <em>TKDE</em>, <em>35</em>(6), 6167–6181.
(<a href="https://doi.org/10.1109/TKDE.2022.3171706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a simple interpolation Oversampling method with the purpose of Reliably Expanding the Minority class regions (OREM). OREM first finds the candidate minority region around each original minority sample, then exploits this region to further identify those clean subregions without distributing any majority sample. The synthetic samples are only allowed to generate in the clean subregions, so that the regions of the minority class can be broadened reliably. Given that the learning from multiclass imbalanced data is more challenging as compared to two-class scenarios, we also extend OREM to handle multiclass imbalance problems by leveraging an iteration procedure of generating synthetic samples, consequently leading to a multiclass oversampling algorithm OREM-M. The key peculiarity of OREM-M is to reduce the class overlapping not only between the synthetic minority and original samples, but also from the synthetic samples of different minority classes. In this way, OREM-M ensures that the data of each class after oversampling can be modeled well. In addition, we embed OREM into boosting framework to develop a new ensemble method OREMBoost addressing class imbalance problems. Extensive experiments demonstrate the effectiveness of the proposed OREM, OREM-M, and OREMBoost.},
  archive      = {J_TKDE},
  author       = {Tuanfei Zhu and Xinwang Liu and En Zhu},
  doi          = {10.1109/TKDE.2022.3171706},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6167-6181},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Oversampling with reliably expanding minority class regions for imbalanced data learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neulft: A novel approach to nonlinear canonical polyadic
decomposition on high-dimensional incomplete tensors. <em>TKDE</em>,
<em>35</em>(6), 6148–6166. (<a
href="https://doi.org/10.1109/TKDE.2022.3176466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A H igh- D imensional and I ncomplete (HDI) tensor is frequently encountered in a big data-related application concerning the complex dynamic interactions among numerous entities. Traditional tensor factorization-based models cannot handle an HDI tensor efficiently, while existing latent factorization of tensors models are all linear models unable to model an HDI tensor&#39;s nonlinearity. Motivated by this critical discovery, this paper proposes a Neural Latent Factorization of Tensors model, which provides a novel approach to nonlinear Canonical Polyadic decomposition on an HDI tensor. It is implemented with three-fold interesting ideas: a) adopting the density-oriented modeling principle to build rank-one tensor series with high computational efficiency and affordable storage cost; b) treating each rank-one tensor as a hidden neuron to achieve an efficient neural network structure; and c) developing an a daptive b ackward p ropagation (ABP) learning scheme for efficient model training. Experimental results on six HDI tensors from a real system demonstrate that compared with state-of-the-art models, the proposed model achieves significant performance gain in both convergence rate and accuracy. Hence, it is of great significance in performing challenging HDI tensor analysis.},
  archive      = {J_TKDE},
  author       = {Xin Luo and Hao Wu and Zechao Li},
  doi          = {10.1109/TKDE.2022.3176466},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6148-6166},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Neulft: A novel approach to nonlinear canonical polyadic decomposition on high-dimensional incomplete tensors},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network change detection based on random walk in latent
space. <em>TKDE</em>, <em>35</em>(6), 6136–6147. (<a
href="https://doi.org/10.1109/TKDE.2022.3167062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of network changes over time is based on identifying deviations of the network structure. The challenge mainly lies in designing a good summary or descriptor of the network structure for facilitating the measure of deviations. In particular, a network may have a huge number of nodes and edges. Moreover, there can exist complicated dependences among edges, e.g., the existence of some edges may be because of others. Therefore, it is non-trivial to measure the contribution of each node and each edge to the deviation of the entire network structure. Existing descriptors are designed to have factors less than the number of nodes and edges. They also model edge dependences, but can only achieve partial modeling. In this paper, we propose a novel type of descriptor. We first obtain node coordinates or positions in a latent space where nodes connected by edges have close positions by network embedding. Node positions are low-dimensional. More importantly, node positions can fully model edge dependences. We then design the descriptor based on random walk on the node positions. We conducted extensive experiments on synthetic datasets and three real-world datasets to demonstrate the effectiveness of our proposed change detection framework with the descriptor.},
  archive      = {J_TKDE},
  author       = {Chuan-Hao Lin and Linchuan Xu and Kenji Yamanishi},
  doi          = {10.1109/TKDE.2022.3167062},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6136-6147},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Network change detection based on random walk in latent space},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neighborhood information-based method for multivariate
association mining. <em>TKDE</em>, <em>35</em>(6), 6126–6135. (<a
href="https://doi.org/10.1109/TKDE.2022.3178090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current data is multivariable, exploring and identifying valuable information in these datasets has far-reaching impacts. In particular, discovering meaningful hidden association patterns in multivariate plays an important role. Plenty of measures for multivariate association have been proposed, yet it is still an open research challenge for effectively capturing association patterns among three or more variables, especially the scenario without any prior knowledge about those relationships. To do so, we desire a distribution-free, association type-independent and non-parametrical measure. For practical applications, such a measure should comparable , interpretable , scalable , intuitive , reliability , and robust . However, no exiting measures fulfill all of these desiderata. In this paper, taking advantage of the neighborhood information of a sample, we propose MNA, a maximal neighborhood multivariate association measure that satisfies all the above criteria. Extensive experiments on synthetic and real data show it outperforms state-of-the-art multivariate association measures.},
  archive      = {J_TKDE},
  author       = {Honghong Cheng and Yuhua Qian and Yingjie Guo and Keyin Zheng and Qingfu Zhang},
  doi          = {10.1109/TKDE.2022.3178090},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6126-6135},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Neighborhood information-based method for multivariate association mining},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple kernel representation learning on networks.
<em>TKDE</em>, <em>35</em>(6), 6113–6125. (<a
href="https://doi.org/10.1109/TKDE.2022.3172048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning representations of nodes in a low dimensional space is a crucial task with numerous interesting applications in network analysis, including link prediction, node classification, and visualization. Two popular approaches for this problem are matrix factorization and random walk -based models. In this paper, we aim to bring together the best of both worlds, towards learning node representations. In particular, we propose a weighted matrix factorization model that encodes random walk-based information about nodes of the network. The benefit of this novel formulation is that it enables us to utilize kernel functions without realizing the exact proximity matrix so that it enhances the expressiveness of existing matrix decomposition methods with kernels and alleviates their computational complexities. We extend the approach with a multiple kernel learning formulation that provides the flexibility of learning the kernel as the linear combination of a dictionary of kernels in data-driven fashion. We perform an empirical evaluation on real-world networks, showing that the proposed model outperforms baseline node embedding algorithms in downstream machine learning tasks.},
  archive      = {J_TKDE},
  author       = {Abdulkadir Çelikkanat and Yanning Shen and Fragkiskos D. Malliaros},
  doi          = {10.1109/TKDE.2022.3172048},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6113-6125},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multiple kernel representation learning on networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-behavior sequential recommendation with temporal graph
transformer. <em>TKDE</em>, <em>35</em>(6), 6099–6112. (<a
href="https://doi.org/10.1109/TKDE.2022.3175094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling time-evolving preferences of users with their sequential item interactions, has attracted increasing attention in many online applications. Hence, sequential recommender systems have been developed to learn the dynamic user interests from the historical interactions for suggesting items. However, the interaction pattern encoding functions in most existing sequential recommender systems have focused on single type of user-item interactions. In many real-life online platforms, user-item interactive behaviors are often multi-typed (e.g., click, add-to-favorite, purchase) with complex cross-type behavior inter-dependencies. Learning from informative representations of users and items based on their multi-typed interaction data, is of great importance to accurately characterize the time-evolving user preference. In this work, we tackle the dynamic user-item relation learning with the awareness of multi-behavior interactive patterns. Towards this end, we propose a new T emporal G raph T ransformer (TGT) recommendation framework to jointly capture dynamic short-term and long-range user-item interactive patterns, by exploring the evolving correlations across different types of behaviors. The new TGT method endows the sequential recommendation architecture to distill dedicated knowledge for type-specific behavior relational context and the implicit behavior dependencies. Experiments on the real-world datasets indicate that our method TGT consistently outperforms various state-of-the-art recommendation methods. Our model implementation codes are available at https://github.com/akaxlh/TGT .},
  archive      = {J_TKDE},
  author       = {Lianghao Xia and Chao Huang and Yong Xu and Jian Pei},
  doi          = {10.1109/TKDE.2022.3175094},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6099-6112},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-behavior sequential recommendation with temporal graph transformer},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). MuL-GRN: Multi-level graph relation network for few-shot
node classification. <em>TKDE</em>, <em>35</em>(6), 6085–6098. (<a
href="https://doi.org/10.1109/TKDE.2022.3176880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) that acquires new knowledge with little supervision, attracts much attention due to expensive cost of data annotation. Various meta-learning methods have made a great progress for few-shot problem in image and text data. In reality, data samples are not independent but rich in link relations. Large amounts of data exists in the form of graph structure such as citation, social, and biological networks. However, FSL study on graph data is still in its infancy because of the obstacle on extracting meta-knowledge from a meta node classification task. Current research simply combines the FSL methods experienced in computer vision with node representation models together, ignoring the effect of rich links among support and query nodes in few-shot meta-task. For this issue, we propose a novel Multi-Level Graph Relation Network (MuL-GRN) for the challenging few-shot node classification. MuL-GRN extracts node embeddings through the popular graph neural networks (GNNs). And it includes a relation learning module to mine the deep node relations from three views, namely node-level, global subgraph-level, and local subgraph-level relations. For any two nodes, the node-level relation is computed on their node embeddings, global subgraph-level relation is measured on their subgraph embeddings, and the local subgraph-level relation is mined according to the pairwise node comparison information in their subgraphs. The three-view relation vectors are fused together with an interesting relation fusion module, which measures the importance of relation vector for the current few-shot classification task automatically. Extensive experiments on five real datasets show that MuL-GRN significantly outperforms existing state-of-the-art methods by a large margin.},
  archive      = {J_TKDE},
  author       = {Lingling Zhang and Shaowei Wang and Jun Liu and Xiaojun Chang and Qika Lin and Yaqiang Wu and Qinghua Zheng},
  doi          = {10.1109/TKDE.2022.3176880},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6085-6098},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MuL-GRN: Multi-level graph relation network for few-shot node classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining statistically significant communities from weighted
networks. <em>TKDE</em>, <em>35</em>(6), 6073–6084. (<a
href="https://doi.org/10.1109/TKDE.2022.3176816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most important issues in data mining and network science, the community detection problem has been extensively investigated during the past decades. Despite of the success achieved by existing methods, how to directly access the statistical significance of an individual community in a weighted network remains unsolved. To address this issue, we present a new method to calculate the analytical p -value of an individual community in weighted networks. The proposed analytical p -value is able to assess the statistical significance that one target community appears in a random weighted graph in a straightforward manner. To verify the effectiveness of the proposed p -value in community evaluation, it is utilized as the objective function in a local search procedure to derive a new community detection algorithm. Experimental results show that the new algorithm is able to achieve comparable performance to those state-of-the-art algorithms for identifying communities from weighted networks. The source codes of our method are available at: https://github.com/chenwenfang/MSSC .},
  archive      = {J_TKDE},
  author       = {Zengyou He and Wenfang Chen and Xiaoqi Wei and Yan Liu},
  doi          = {10.1109/TKDE.2022.3176816},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6073-6084},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Mining statistically significant communities from weighted networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning robust deep state space for unsupervised anomaly
detection in contaminated time-series. <em>TKDE</em>, <em>35</em>(6),
6058–6072. (<a href="https://doi.org/10.1109/TKDE.2022.3171562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalies are ubiquitous in real-world time-series data which call for effective and timely detection, especially in an unsupervised setting for labeling cost saving. In this paper, we develop an unsupervised density reconstruction model for multi-dimensional time-series anomaly detection. In particular, it directly handles an important realistic setting that the detection is achieved towards raw time-series contaminated with noise for training, in contrast to most existing anomaly detection works that assume the training data is in general clean i.e., not contaminated with anomaly. It extends recent advancements in deep generative models and state space models to achieve robust anomaly detection. Our approach comprises of a novel state space based generative model, a filtering based inference model, together with a carefully-designated emission model based on robust statistics theory. Extensive experimental results are conducted to show that our approach can adapt to complex patterns even given severely contaminated training data. We also develop visualization techniques to help better understand the behavior of the anomaly detection models. Empirical results show that our method outperforms state-of-the-arts on both synthetic and real-world datasets.},
  archive      = {J_TKDE},
  author       = {Longyuan Li and Junchi Yan and Qingsong Wen and Yaohui Jin and Xiaokang Yang},
  doi          = {10.1109/TKDE.2022.3171562},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6058-6072},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning robust deep state space for unsupervised anomaly detection in contaminated time-series},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from ideography and labels: A schema-aware
radical-guided associative model for chinese text classification.
<em>TKDE</em>, <em>35</em>(6), 6043–6057. (<a
href="https://doi.org/10.1109/TKDE.2022.3171690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reading psychology believes text comprehension to involve a complex psychological construction process, with the reader mind being a dynamic associative system that stores an abundance of schemata. For Chinese text, in particular, the unique ideographic writing system allows its lansign to trigger semantic association and schema recalling without the need of phonetics. In contrast to previous research efforts on text classification problems, in this paper we present an interdisciplinary modeling approach that draws inspirations from the cognitive principles of ideography, schema theory and deep learning to study Chinese text classification. Specifically, we first propose a Radical-guided Associative Model (RAM) for preliminary cognitive imitation, which comprises two coupled spaces, namely the Literal Space and Associative Space. Then, taking consideration of the schemata acquired from the mind of a reader which plays an important role in influencing text-dependent information revision, we extend RAM with a systematic Schema-aware Radical-guided Associative Model (SRAM) that embeds label semantics as essential text-independent human knowledge for real-world abstraction. In SRAM, the Schema Space is introduced and a Schema Attention module is proposed with a novel loss paradigm that includes the linkage and interaction between text-dependent prior concepts and text-independent label schemata. Extensive experiments on three real-world datasets demonstrate the effectiveness and rationality of our proposed method.},
  archive      = {J_TKDE},
  author       = {Hanqing Tao and Guanqi Zhu and Enhong Chen and Shiwei Tong and Kun Zhang and Tong Xu and Qi Liu and Yew-Soon Ong},
  doi          = {10.1109/TKDE.2022.3171690},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6043-6057},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning from ideography and labels: A schema-aware radical-guided associative model for chinese text classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning adaptive node embeddings across graphs.
<em>TKDE</em>, <em>35</em>(6), 6028–6042. (<a
href="https://doi.org/10.1109/TKDE.2022.3160211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, learning embeddings of nodes in graphs has attracted increasing research attention. There are two main kinds of graph embedding methods, i.e., transductive embedding methods and inductive embedding methods. The former focuses on directly optimizing the embedding vectors, and the latter tries to learn a mapping function for the given nodes and features. However, little work has focused on applying the learned model from one graph to another, which is a pervasive idea in Computer Vision or Natural Language Processing. Although some of the graph neural networks (GNNs) present a similar motivation, none of them considers both the structure bias and the feature bias between graphs. In this paper, we present a novel graph embedding problem called Adaptive Task (AT), and propose a unified framework for the adaptive task, which introduces two types of alignment to learn adaptive node embeddings across graphs. Then, based on the proposed framework, a novel Graph Adaptive Embedding network (GraphAE) is designed to address the adaptive task. Furthermore, we extend GraphAE to a multi-graph version to consider a more complex adaptive situation. The extensive experimental results demonstrate that our model significantly outperforms the state-of-the-art methods, and also show that our framework can make a great improvement over a number of existing GNNs.},
  archive      = {J_TKDE},
  author       = {Gaoyang Guo and Chaokun Wang and Bencheng Yan and Yunkai Lou and Hao Feng and Junchao Zhu and Jun Chen and Fei He and Philip S. Yu},
  doi          = {10.1109/TKDE.2022.3160211},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6028-6042},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning adaptive node embeddings across graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LAAT: Locally aligned ant technique for discovering multiple
faint low dimensional structures of varying density. <em>TKDE</em>,
<em>35</em>(6), 6014–6027. (<a
href="https://doi.org/10.1109/TKDE.2022.3177368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction and clustering are often used as preliminary steps for many complex machine learning tasks. The presence of noise and outliers can deteriorate the performance of such preprocessing and therefore impair the subsequent analysis tremendously. In manifold learning, several studies indicate solutions for removing background noise or noise close to the structure when the density is substantially higher than that exhibited by the noise. However, in many applications, including astronomical datasets, the density varies alongside manifolds that are buried in a noisy background. We propose a novel method to extract manifolds in the presence of noise based on the idea of Ant colony optimization. In contrast to the existing random walk solutions, our technique captures points that are locally aligned with major directions of the manifold. Moreover, we empirically show that the biologically inspired formulation of ant pheromone reinforces this behavior enabling it to recover multiple manifolds embedded in extremely noisy data clouds. The algorithm performance in comparison to state-of-the-art approaches for noise reduction in manifold detection and clustering is demonstrated, on several synthetic and real datasets, including an N-body simulation of a cosmological volume.},
  archive      = {J_TKDE},
  author       = {Abolfazl Taghribi and Kerstin Bunte and Rory Smith and Jihye Shin and Michele Mastropietro and Reynier F. Peletier and Peter Tiňo},
  doi          = {10.1109/TKDE.2022.3177368},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6014-6027},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LAAT: Locally aligned ant technique for discovering multiple faint low dimensional structures of varying density},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intra-category aware hierarchical supervised document
hashing. <em>TKDE</em>, <em>35</em>(6), 6003–6013. (<a
href="https://doi.org/10.1109/TKDE.2022.3161807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document hashing is a powerful paradigm for document retrieval, which maps high-dimensional documents to compact hashing codes with preserving the similarity of original data. While fairly successful, the existing document hashing methods do not consider the relevance relationship among different documents from a category and the hierarchical relationship among categories. Intuitively, the intra-category relevance connects related concepts among different documents, which can supplement the omitted information for each document; meanwhile the hierarchical categories can help to identify whether mistakes occur in leaf categories or parent categories, which can be used to reduce the mistakes occurring in parent categories that are often more serious. Inspired by above intuitions, we propose a novel I ntra-category aware H ierarchical supervised D ocument H ashing, called IHDH. Specifically, IHDH is a binary autoencoder architecture equipped with two novel components: intra-category component and hierarchy component. The intra-category component exploits the difference among latent semantic representations of different documents from a category to supplement the omitted information for each document. The hierarchy component utilizes the hierarchical structure to transform the probabilities of leaf categories into the probabilities of parent categories by union operation, and then gives a further parent-level penalty to reduce the mistakes occurring in parent categories. Extensive experiments over three benchmark datasets show that IHDH significantly outperforms the state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Jia-Nan Guo and Xian-Ling Mao and Wei Wei and Heyan Huang},
  doi          = {10.1109/TKDE.2022.3161807},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {6003-6013},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Intra-category aware hierarchical supervised document hashing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incomplete gamma integrals for deep cascade prediction using
content, network, and exogenous signals. <em>TKDE</em>, <em>35</em>(6),
5991–6002. (<a href="https://doi.org/10.1109/TKDE.2022.3174206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The behavior of information cascades (such as retweets) has been modeled extensively. While point process-based generative models have long been in use for estimating cascade growths, deep learning has greatly enhanced the integration of diverse features and signals. We observe two significant temporal signals in cascade data that have not been reported or exploited to our knowledge. First, the popularity of the cascade root is known to influence cascade size strongly; but we find that the effect falls off rapidly with time. Second, we find a measurable positive correlation between the novelty of the root content (with respect to a streaming external corpus) and the relative size of the resulting cascade. Responding to these observations, we propose GammaCas , a new cascade growth model as a parametric function of time, which combines deep influence signals from content (e.g., tweet text), network features (e.g., followers of the root user), and exogenous event sources (e.g., online news). Specifically, our model processes these signals through a customized recurrent network, whose states then provide the parameters of the cascade rate function, which is integrated over time to predict the cascade size. The network parameters are trained end-to-end using observed cascades. GammaCas outperforms seven recent and diverse baselines significantly on a large-scale dataset of retweet cascades coupled with time-aligned online news — it beats the best baseline with 18.98\% increase in terms of Kendall&#39;s $\tau$ correlation and a reduction of 19.2 in Mean Absolute Percentage Error. Extensive ablation and case studies unearth interesting insights regarding retweet cascade dynamics.},
  archive      = {J_TKDE},
  author       = {Subhabrata Dutta and Shravika Mittal and Dipankar Das and Soumen Chakrabarti and Tanmoy Chakraborty},
  doi          = {10.1109/TKDE.2022.3174206},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5991-6002},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incomplete gamma integrals for deep cascade prediction using content, network, and exogenous signals},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HS-GCN: Hamming spatial graph convolutional networks for
recommendation. <em>TKDE</em>, <em>35</em>(6), 5977–5990. (<a
href="https://doi.org/10.1109/TKDE.2022.3158317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An efficient solution to the large-scale recommender system is to represent users and items as binary hash codes in the Hamming space. Towards this end, existing methods tend to code users by modeling their Hamming similarities with the items they historically interact with, which are termed as the first-order similarities in this work. Despite their efficiency, these methods suffer from the suboptimal representative capacity, since they forgo the correlation established by connecting multiple first-order similarities, i.e., the relation among the indirect instances, which could be defined as the high-order similarity. To tackle this drawback, we propose to model both the first- and the high-order similarities in the Hamming space through the user-item bipartite graph. Therefore, we develop a novel learning to hash framework, namely Hamming Spatial Graph Convolutional Networks (HS-GCN), which explicitly models the Hamming similarity and embeds it into the codes of users and items. Extensive experiments on three public benchmark datasets demonstrate that our proposed model significantly outperforms several state-of-the-art hashing models, and obtains performance comparable with the real-valued recommendation models.},
  archive      = {J_TKDE},
  author       = {Han Liu and Yinwei Wei and Jianhua Yin and Liqiang Nie},
  doi          = {10.1109/TKDE.2022.3158317},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5977-5990},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HS-GCN: Hamming spatial graph convolutional networks for recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical feature selection based on label distribution
learning. <em>TKDE</em>, <em>35</em>(6), 5964–5976. (<a
href="https://doi.org/10.1109/TKDE.2022.3177246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical classification learning, which organizes data categories into a hierarchical structure, is an effective approach for large-scale classification tasks. The high dimensionality of data feature space, represented in hierarchical class structures, is one of the main research challenges. In addition, the class hierarchy often introduces imbalanced class distributions and causes overfitting. In this paper, we propose a feature selection method based on label distribution learning to address the above challenges. The crux is to alleviate the class imbalance problem and learn a discriminative feature subset for hierarchical classification process. Due to correlation between different class categories in the hierarchical tree structure, sibling categories can provide additional supervisory information for each learning sub tasks, which, in turn, alleviates the problem of under-sampling of minority categories. Therefore, we transform hierarchical labels to a hierarchical label distribution to represent this correlation. After that, a discriminative feature subset is selected recursively, by the common features and label-specific feature constraints, to ensure that downstream classification tasks can achieve the best performance. Experiments and comparisons, using seven well-established feature selection algorithms on six real data sets with different degrees of imbalance, demonstrate the superiority of the proposed method.},
  archive      = {J_TKDE},
  author       = {Yaojin Lin and Haoyang Liu and Hong Zhao and Qinghua Hu and Xingquan Zhu and Xindong Wu},
  doi          = {10.1109/TKDE.2022.3177246},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5964-5976},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical feature selection based on label distribution learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hide and mine in strings: Hardness, algorithms, and
experiments. <em>TKDE</em>, <em>35</em>(6), 5948–5963. (<a
href="https://doi.org/10.1109/TKDE.2022.3158063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sanitization and frequent pattern mining are two well-studied topics in data mining. Data sanitization is the process of disguising (hiding) confidential information in a given dataset. Typically, this process incurs some utility loss that should be minimized. Frequent pattern mining is the process of obtaining all patterns occurring frequently enough in a given dataset. Our work initiates a study on the fundamental relation between data sanitization and frequent pattern mining in the context of sequential (string) data. Current methods for string sanitization hide confidential patterns. This, however, may lead to spurious patterns that harm the utility of frequent pattern mining. The main computational problem is to minimize this harm. Our contribution here is as follows. First, we present several hardness results, for different variants of this problem, essentially showing that these variants cannot be solved or even be approximated in polynomial time. Second, we propose integer linear programming formulations for these variants and algorithms to solve them, which work in polynomial time under realistic assumptions on the input parameters. We also complement the integer linear programming algorithms with a greedy heuristic. Third, we present an extensive experimental study, using both synthetic and real-world datasets, that demonstrates the effectiveness and efficiency of our methods. Beyond sanitization, the process of missing value replacement may also lead to spurious patterns. Interestingly, our results apply in this context as well. We show that, unlike popular approaches, our methods can fill missing values in genomic sequences, while preserving the accuracy of frequent pattern mining.},
  archive      = {J_TKDE},
  author       = {Giulia Bernardini and Alessio Conte and Garance Gourdel and Roberto Grossi and Grigorios Loukides and Nadia Pisanti and Solon P. Pissis and Giulia Punzi and Leen Stougie and Michelle Sweering},
  doi          = {10.1109/TKDE.2022.3158063},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5948-5963},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hide and mine in strings: Hardness, algorithms, and experiments},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous graph representation learning with relation
awareness. <em>TKDE</em>, <em>35</em>(6), 5935–5947. (<a
href="https://doi.org/10.1109/TKDE.2022.3160208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning on heterogeneous graphs aims to obtain meaningful node representations to facilitate various downstream tasks, such as node classification and link prediction. Existing heterogeneous graph learning methods are primarily developed by following the propagation mechanism of node representations. There are few efforts on studying the role of relations for improving the learning of more fine-grained node representations. Indeed, it is important to collaboratively learn the semantic representations of relations and discern node representations with respect to different relation types. To this end, in this paper, we propose a R elation-aware H eterogeneous G raph N eural N etwork, namely R-HGNN, to learn node representations on heterogeneous graphs at a fine-grained level by considering relation-aware characteristics. Specifically, a dedicated graph convolution component is first designed to learn unique node representations from each relation-specific graph separately. Then, a cross-relation message passing module is developed to improve the interactions of node representations across different relations. Also, the relation representations are learned in a layer-wise manner to capture relation semantics, which are used to guide the node representation learning process. Moreover, a semantic fusing module is presented to aggregate relation-aware node representations into a compact representation with the learned relation representations. Finally, we conduct extensive experiments on a variety of graph learning tasks, and experimental results demonstrate that our approach consistently outperforms existing methods among all the tasks.},
  archive      = {J_TKDE},
  author       = {Le Yu and Leilei Sun and Bowen Du and Chuanren Liu and Weifeng Lv and Hui Xiong},
  doi          = {10.1109/TKDE.2022.3160208},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5935-5947},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Heterogeneous graph representation learning with relation awareness},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph vulnerability and robustness: A survey. <em>TKDE</em>,
<em>35</em>(6), 5915–5934. (<a
href="https://doi.org/10.1109/TKDE.2022.3163672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of network robustness is a critical tool in the characterization and sense making of complex interconnected systems such as infrastructure, communication and social networks. While significant research has been conducted in these areas, gaps in the surveying literature still exist. Answers to key questions are currently scattered across multiple scientific fields and numerous papers. In this survey, we distill key findings across numerous domains and provide researchers crucial access to important information by— (1) summarizing and comparing recent and classical graph robustness measures; (2) exploring which robustness measures are most applicable to different categories of networks (e.g., social, infrastructure); (3) reviewing common network attack strategies, and summarizing which attacks are most effective across different network topologies; and (4) extensive discussion on selecting defense techniques to mitigate attacks across a variety of networks. This survey guides researchers and practitioners in navigating the expansive field of network robustness, while summarizing answers to key questions. We conclude by highlighting current research directions and open problems.},
  archive      = {J_TKDE},
  author       = {Scott Freitas and Diyi Yang and Srijan Kumar and Hanghang Tong and Duen Horng Chau},
  doi          = {10.1109/TKDE.2022.3163672},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5915-5934},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph vulnerability and robustness: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph stream sketch: Summarizing graph streams with high
speed and accuracy. <em>TKDE</em>, <em>35</em>(6), 5901–5914. (<a
href="https://doi.org/10.1109/TKDE.2022.3174570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph stream is a continuous sequence of data items, in which each item indicates an edge, including its two endpoints and edge weight. It forms a dynamic graph that changes with every item. Graph streams play important roles in cyber security, social networks, cloud troubleshooting systems and more. Due to the vast volume and high update speed of graph streams, traditional data structures for graph storage such as the adjacency matrix and the adjacency list are no longer sufficient. However, prior art of graph stream summarization either supports limited kinds of queries or suffers from poor accuracy of query results. In this paper, we propose a novel G raph S tream S ketch (GSS for short) to summarize the graph streams, which has linear space cost $O(|E|)$ (E is the edge set of the graph) and high update speed, and supports most kinds of queries over graph streams with controllable errors. Experimental results show that our solution is up to 142 times faster than the adjacency list when processing updates in graph streams, and its memory consumption is as small as $30\%$ of the adjacency list. Though error is introduced as a trade off in our solution, both theoretical analysis and experiment results confirm that such error is small and controllable. The relative error is below $10^{-2}$ in edge weight query, and the precision is above $90\%$ is 1-hop precursor/successor queries.},
  archive      = {J_TKDE},
  author       = {Xiangyang Gou and Lei Zou and Chenxingyu Zhao and Tong Yang},
  doi          = {10.1109/TKDE.2022.3174570},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5901-5914},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph stream sketch: Summarizing graph streams with high speed and accuracy},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph self-supervised learning: A survey. <em>TKDE</em>,
<em>35</em>(6), 5879–5900. (<a
href="https://doi.org/10.1109/TKDE.2022.3172903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning on graphs has attracted significant interests recently. However, most of the works have focused on (semi-) supervised learning, resulting in shortcomings including heavy label reliance, poor generalization, and weak robustness. To address these issues, self-supervised learning (SSL), which extracts informative knowledge through well-designed pretext tasks without relying on manual labels, has become a promising and trending learning paradigm for graph data. Different from SSL on other domains like computer vision and natural language processing, SSL on graphs has an exclusive background, design ideas, and taxonomies. Under the umbrella of graph self-supervised learning , we present a timely and comprehensive review of the existing approaches which employ SSL techniques for graph data. We construct a unified framework that mathematically formalizes the paradigm of graph SSL. According to the objectives of pretext tasks, we divide these approaches into four categories: generation-based, auxiliary property-based, contrast-based, and hybrid approaches. We further describe the applications of graph SSL across various research fields and summarize the commonly used datasets, evaluation benchmark, performance comparison and open-source codes of graph SSL. Finally, we discuss the remaining challenges and potential future directions in this research field.},
  archive      = {J_TKDE},
  author       = {Yixin Liu and Ming Jin and Shirui Pan and Chuan Zhou and Yu Zheng and Feng Xia and Philip S. Yu},
  doi          = {10.1109/TKDE.2022.3172903},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5879-5900},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph self-supervised learning: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GATrust: A multi-aspect graph attention network model for
trust assessment in OSNs. <em>TKDE</em>, <em>35</em>(6), 5865–5878. (<a
href="https://doi.org/10.1109/TKDE.2022.3174044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social trust assessment that characterizes a pairwise trustworthiness relationship can spur diversified applications. Extensive efforts have been put in exploration, but mainly focusing on applying graph convolutional network to establish a social trust evaluation model, overlooking user feature factors related to context-aware information on social trust prediction. In this article, we aim to design a new trust assessment framework GATrust which integrates multi-aspect properties of users, including user context-specific information, network topological structure information, and locally-generated social trust relationships. GATrust can assigns different attention coefficients to multi-aspect properties of users in online social networks, for improving the prediction accuracy of social trust evaluation. The framework can then learn multiple latent factors of each trustor-trustee pair to establish a social trust evaluation model, by fusing graph attention network and graph convolution network. We conduct extensive experiments on two popular real-world datasets and the results exhibit that our proposed framework can improve the precision of social trust prediction, outperforming the state-of-the-art in the literature by 4.3\% and 5.5\% on both two datasets, respectively.},
  archive      = {J_TKDE},
  author       = {Nan Jiang and Jie Wen and Jin Li and Ximeng Liu and Di Jin},
  doi          = {10.1109/TKDE.2022.3174044},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5865-5878},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GATrust: A multi-aspect graph attention network model for trust assessment in OSNs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Fine-grained urban flow inference with incomplete data.
<em>TKDE</em>, <em>35</em>(6), 5851–5864. (<a
href="https://doi.org/10.1109/TKDE.2022.3154163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained urban flow inference, which aims to infer the fine-grained urban flows of a city given the coarse-grained urban flow observations, is critically important to various smart city related applications such as urban planning and public safety. Previous works assume that the urban flow monitoring sensors are evenly distributed in space for data collection and thus the observed urban flows are complete. However, in real-world scenarios, sensors are usually unevenly deployed in space. For example, the traffic cameras are mostly deployed at the crossroads and central areas of a city, but less likely to be deployed in suburb. The data scarcity issue poses great challenges to existing methods for accurately inferring the fine-grained urban flows, because they require all urban flow observations to be available. In this paper, we make the first attempt to infer fine-grained urban flows based on the incomplete coarse-grained urban flow observations, and propose a Multi-Task urban flow Completion and Super-Resolution network (MT-CSR for short) to simultaneously complete the coarse-grained urban flows and infer the fine-grained flows. Specifically, MT-CSR consists of the data completion network (CMPNet for short) and data super-resolution network (SRNet for short). CmpNet is composed of a local spatial information based data completion module LocCmp and an auxiliary information based data completion module AuxCmp to consider both the local geographical and global semantic correlations for urban flow data completion. SRNet is designed to capture the complex associations between fine- and coarse-grained urban flows and upsample the coarse-grained data by stacking the designed super-resolution blocks. To gain an accurate inference, two parts are jointly conducted under a multi-task learning framework, and trained in an end-to-end manner using a two-stage training strategy. Extensive experiments on four large real-world datasets validate the effectiveness and efficiency of our method compared with the state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Jiyue Li and Senzhang Wang and Jiaqiang Zhang and Hao Miao and Junbo Zhang and Philip S. Yu},
  doi          = {10.1109/TKDE.2022.3154163},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5851-5864},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fine-grained urban flow inference with incomplete data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable tensorized neural ordinary differential
equations for arbitrary-step time series prediction. <em>TKDE</em>,
<em>35</em>(6), 5837–5850. (<a
href="https://doi.org/10.1109/TKDE.2022.3167536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a continuous neural network architecture, referred to as Explainable Tensorized Neural - Ordinary Differential Equations (ETN-ODE) network for multi-step time series prediction at arbitrary time points. Unlike existing approaches which mainly handle univariate time series for multi-step prediction, or multivariate time series for single-step predictions, ETN-ODE is capable of handling multivariate time series with arbitrary-step predictions. An additional benefit is its tandem attention mechanism, with respect to temporal and variable attention, which enable it to greatly facilitate data interpretability. Specifically, the proposed model combines an explainable tensorized gated recurrent unit with ordinary differential equations, with the derivatives of the latent states parameterized through a neural network. We quantitatively and qualitatively demonstrate the effectiveness and interpretability of ETN-ODE on one arbitrary-step prediction task and five standard multi-step prediction tasks. Extensive experiments show that the proposed method achieves very accurate predictions at arbitrary time points while attaining very competitive performance against the baseline methods in standard multi-step time series prediction.},
  archive      = {J_TKDE},
  author       = {Penglei Gao and Xi Yang and Rui Zhang and Kaizhu Huang and John Y. Goulermas},
  doi          = {10.1109/TKDE.2022.3167536},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5837-5850},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Explainable tensorized neural ordinary differential equations for arbitrary-step time series prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EGraph: Efficient concurrent GPU-based dynamic graph
processing. <em>TKDE</em>, <em>35</em>(6), 5823–5836. (<a
href="https://doi.org/10.1109/TKDE.2022.3171588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications of the analysis of dynamic graph, many Timing iterative Graph Processing (TGP) jobs usually need to be generated for the processing of the corresponding snapshots of the dynamic graph to obtain the results at different points of time. For high throughput of such applications, it is expected to run the TGP jobs on the GPU concurrently. Although many GPU-based systems have been recently developed, for out-of-GPU-memory dynamic graph processing, this concurrent way suffers from significant data access overhead due to a large volume of data transfer between CPU and GPU and the interference between these concurrently running jobs, which eventually incurs low GPU utilization ratio. In this work, we observed that the TGP jobs have strong temporal and spatial similarity when they access different snapshots for their own processing as most parts of the snapshots are the same and only a few parts are changing with time. It creates ideal opportunities for efficient concurrent execution of the TGP jobs by dramatically reducing CPU-GPU graph data transfer cost. Based on this observation, we develop the first GPU-based dynamic graph processing system EGraph , which can be integrated into the existing out-of-GPU-memory static graph processing systems to enable them to efficiently support concurrent execution of TGP jobs on dynamic graphs with the help of GPU accelerators. Different from the existing approaches, we propose in EGraph an effective Loading-Processing-Switching ( LPS ) execution model. It is able to effectively reduce the overhead of CPU-GPU data transfer and ensures a higher GPU utilization ratio for efficient execution of the TGP jobs by fully utilizing the data access similarity between the TGP jobs. Experimental results show that the existing GPU-accelerated systems achieve performance improvements of 2.3-3.5 times after being integrated with EGraph.},
  archive      = {J_TKDE},
  author       = {Yu Zhang and Yuxuan Liang and Jin Zhao and Fubing Mao and Lin Gu and Xiaofei Liao and Hai Jin and Haikun Liu and Song Guo and Yangqing Zeng and Hang Hu and Chen Li and Ji Zhang and Biao Wang},
  doi          = {10.1109/TKDE.2022.3171588},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5823-5836},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {EGraph: Efficient concurrent GPU-based dynamic graph processing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient subhypergraph matching based on hyperedge
features. <em>TKDE</em>, <em>35</em>(6), 5808–5822. (<a
href="https://doi.org/10.1109/TKDE.2022.3160393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraphs consist of vertices and hyperedges that can connect multiple vertices. Since hypergraphs can effectively simulate complex intergroup relationships between entities, they have a wide range of applications such as computer vision and bioinformatics. In this paper, we study the subhypergraph matching problem, which is one of the most challenging problems in the processing of the hypergraphs. We aim to extract all subhypergraph isomorphism embeddings of a query hypergraph $q$ in a large data hypergraph $D$ . The existing methods on subgraph matching are designed for the ordinary graphs, which typically achieve the goal by three phases, i.e., filtering candidate vertices, refining candidate sets, and then enumeration final results in some matching order. However, such a design cannot be trivially extended to efficiently handle hypergraphs due to the inherent difference between ordinary graphs and hypergraphs. This motivates us to enhance the performance by exploiting hyperedge features, such as the typical intersections and inclusion relations between hyperedges. In our work, we present an efficient subhypergraph matching solution with two novel techniques, maximum hyperedge candidate filtering and co-occurrence matrix candidate refinement strategy. Maximum hyperedge candidate filtering is a filtering method based on hyperedge features, which can provide powerful pruning capability. Co-occurrence matrix candidate refinement strategy considers the high-order relationship between vertices in the hypergraph and provides an effective candidate refinement scheme to further reduce the overall search space. In order to find more effective matching order, we design a new enumeration strategy, which calculates the pseudo-isomorphic mapping set and then performs hyperedge verification. On real and synthetic data sets, we conduct extensive experiments to show our method outperforms existing methods by up to 2 orders of magnitude.},
  archive      = {J_TKDE},
  author       = {Yuhang Su and Yu Gu and Zhigang Wang and Ying Zhang and Jianbin Qin and Ge Yu},
  doi          = {10.1109/TKDE.2022.3160393},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5808-5822},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient subhypergraph matching based on hyperedge features},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient range and kNN twin subsequence search in time
series. <em>TKDE</em>, <em>35</em>(6), 5794–5807. (<a
href="https://doi.org/10.1109/TKDE.2022.3167257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing time series data is crucial for many applications. In particular, subsequence search refers to finding subsequences within an input time series $T$ that are similar to a query sequence $Q$ . Existing subsequence search approaches typically employ Euclidean distance or Dynamic Time Warping as similarity measures and address range queries. In this paper, we focus on Chebyshev distance, which is the largest difference between each individual pair of points across the entire length of two compared subsequences. We call such similar pairs twins . We first show how existing time series indices can be extended to perform twin subsequence search. Then, we introduce TS-Index, a novel index tailored to the computation of twin subsequence search queries. Moreover, given that specifying a distance threshold is often not straightforward, we show how TS-Index can also be used to evaluate $k$ NN queries. Our extensive experimental evaluation compares these approaches using real time series datasets. The results demonstrate that TS-Index can retrieve twin subsequences faster than all other methods under various conditions.},
  archive      = {J_TKDE},
  author       = {Georgios Chatzigeorgakidis and Dimitrios Skoutas and Kostas Patroumpas and Themis Palpanas and Spiros Athanasiou and Spiros Skiadopoulos},
  doi          = {10.1109/TKDE.2022.3167257},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5794-5807},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient range and kNN twin subsequence search in time series},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient hierarchical storage management empowered by
reinforcement learning. <em>TKDE</em>, <em>35</em>(6), 5780–5793. (<a
href="https://doi.org/10.1109/TKDE.2022.3176753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of big data and cloud computing, data management has become increasingly challenging. Over the years, a number of frameworks for data management have become available. Most of them are highly efficient, but ultimately create data silos. It becomes difficult to move and work coherently with data as new requirements emerge. A possible solution is to use an intelligent hierarchical (multi-tier) storage system (HSS). A HSS is a meta solution that consists of different storage frameworks organized as a jointly constructed storage pool. A built-in data migration policy that determines the optimal placement of the datasets in the hierarchy is essential. Placement decisions is a non-trivial task since it should be made according to the characteristics of the dataset, the tier status in a hierarchy, and access patterns. This paper presents an open-source hierarchical storage framework with a dynamic migration policy based on reinforcement learning (RL). We present a mathematical model, a software architecture, and implementations based on both simulations and a live cloud-based environment. We compare the proposed RL-based strategy to a baseline of three rule-based policies, showing that the RL-based policy achieves significantly higher efficiency and optimal data distribution in different scenarios.},
  archive      = {J_TKDE},
  author       = {Tianru Zhang and Andreas Hellander and Salman Toor},
  doi          = {10.1109/TKDE.2022.3176753},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5780-5793},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient hierarchical storage management empowered by reinforcement learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Efficient adaptive matching for real-time city express
delivery. <em>TKDE</em>, <em>35</em>(6), 5767–5779. (<a
href="https://doi.org/10.1109/TKDE.2022.3162220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {City express delivery services (a.k.a. last-mile delivery) have become more prominent in recent years. Many logistics giants, such as Amazon, JD, and Cainiao, have deployed intelligent express delivery systems to deal with the growing demand for parcel delivery. Existing works adopt queuing or batch processing approaches to assign parcels to couriers. However, these approaches do not fully consider the distribution of parcels and couriers, leading to poor quality of task assignment. In this paper, we investigate a problem of delivery matching based on revenue maximization in real-time city express delivery services. Given a set of couriers and a stream of parcel collection tasks, our problem aims to assign each collection task to a suitable courier to maximize the overall revenue of the platform. The problem is shown to be NP-hard. To tackle the problem efficiently, we present a time-aware batch matching algorithm to offer high-quality courier-task matching in each sliding window. We further theoretically analyze the matching approximation bound. In addition, we propose an efficient deep reinforcement learning based approach to adaptively determine the sliding window size for better matching results. Finally, extensive experiments demonstrate that our proposed algorithms can achieve desirable effectiveness and efficiency under a wide range of parameter settings.},
  archive      = {J_TKDE},
  author       = {Yafei Li and Qingshun Wu and Xin Huang and Jianliang Xu and Wanru Gao and Mingliang Xu},
  doi          = {10.1109/TKDE.2022.3162220},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5767-5779},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient adaptive matching for real-time city express delivery},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distribution agnostic symbolic representations for time
series dimensionality reduction and online anomaly detection.
<em>TKDE</em>, <em>35</em>(6), 5752–5766. (<a
href="https://doi.org/10.1109/TKDE.2022.3174630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the importance of the lower bounding distances and the attractiveness of symbolic representations, the family of symbolic aggregate approximations (SAX) has been used extensively for encoding time series data. However, typical SAX-based methods rely on two restrictive assumptions; the Gaussian distribution and equiprobable symbols. This paper proposes two novel data-driven SAX-based symbolic representations, distinguished by their discretization steps. The first representation, oriented for general data compaction and indexing scenarios, is based on the combination of kernel density estimation and Lloyd-Max quantization to minimize the information loss and mean squared error in the discretization step. The second method, oriented for high-level mining tasks, employs the Mean-Shift clustering method and is shown to enhance anomaly detection in the lower-dimensional space. Besides, we verify on a theoretical basis a previously observed phenomenon of the intrinsic process that results in a lower than the expected variance of the intermediate piecewise aggregate approximation. This phenomenon causes an additional information loss but can be avoided with a simple modification. The proposed representations possess all the attractive properties of the conventional SAX method. Furthermore, experimental evaluation on real-world datasets demonstrates their superiority compared to the traditional SAX and an alternative data-driven SAX variant.},
  archive      = {J_TKDE},
  author       = {Konstantinos Bountrogiannis and George Tzagkarakis and Panagiotis Tsakalides},
  doi          = {10.1109/TKDE.2022.3174630},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5752-5766},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Distribution agnostic symbolic representations for time series dimensionality reduction and online anomaly detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disentangled modeling of social homophily and influence for
social recommendation. <em>TKDE</em>, <em>35</em>(6), 5738–5751. (<a
href="https://doi.org/10.1109/TKDE.2022.3185388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social recommendation leverages social information to alleviate data sparsity and cold-start issues of collaborative filtering (CF) methods. Most existing works model user interests following the assumption of social homophily based on social-relation data. The explicit modeling of social influence , which also largely affects user behaviors, has not been well explored. Considering user behaviors may be driven by social factors in today&#39;s information services (e.g., purchasing products shared by close friends on social e-commerce applications), these methods will be suboptimal. In this work, we propose a method modeling both social homophily-aware user interests and social influence as two essential effects on user behaviors for social recommendation, named as DISGCN (short for DIS entangled modeling of Social homophily and influence with G raph C onvolutional N etwork). Specifically, we devise a disentangled embedding layer to encode these two effects. Furthermore, two tailored graph convolutional layers are developed to disentangle them refinedly, leveraging the high-order embedding propagation in social-network graph from two aspects. Technically, first, the operation of attentive embedding propagation is adopted for capturing personalized social homophily-aware interests, and second, the item-gate-based embedding propagation is proposed for capturing item-specific social influence. In addition, to ensure the disentanglement of social influence, we propose a contrastive learning framework that endows corresponding embeddings with explicit semantics. Extensive experiments on two real-world datasets demonstrate the effectiveness of our proposed model. Further studies also verify the rationality and necessity of our designs. We have released the datasets and codes at this link: https://github.com/tsinghua-fib-lab/DISGCN .},
  archive      = {J_TKDE},
  author       = {Nian Li and Chen Gao and Depeng Jin and Qingmin Liao},
  doi          = {10.1109/TKDE.2022.3185388},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5738-5751},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Disentangled modeling of social homophily and influence for social recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentially private demand side management for
incentivized dynamic pricing in smart grid. <em>TKDE</em>,
<em>35</em>(6), 5724–5737. (<a
href="https://doi.org/10.1109/TKDE.2022.3157472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to efficiently provide demand side management (DSM) in smart grid, carrying out pricing on the basis of real-time energy usage is considered to be the most vital tool because it is directly linked with the finances associated with smart meters. Hence, every smart meter user wants to pay the minimum possible amount along with getting maximum benefits. In this context, usage based dynamic pricing strategies of DSM plays their role and provide users with specific incentives that help shaping their load curve according to the forecasted load. However, these reported real-time values can leak privacy of smart meter users, which can lead to serious consequences such as spying, etc. Moreover, most dynamic pricing algorithms charge all users equally irrespective of their contribution in causing peak factor. Therefore, in this paper, we propose a modified usage based dynamic pricing mechanism that only charges the users responsible for causing peak factor. We further integrate the concept of differential privacy to protect the privacy of real-time smart metering data. To calculate accurate billing, we also propose a noise adjustment method. Finally, we propose D emand R esponse enhancing D ifferential P ricing (DRDP) strategy that effectively enhances demand response along with providing dynamic pricing to smart meter users. We also carry out theoretical analysis for differential privacy guarantees and for cooperative state probability to analyze behavior of cooperative smart meters. The performance evaluation of DRDP strategy at various privacy parameters show that the proposed strategy outperforms previous mechanisms in terms of dynamic pricing and privacy preservation. 1 1.A preliminary version has been published by 2020 IEEE International Conference on Communications (ICC 2020), June, 2020, Dublin, Ireland entitled Differentially Private Dynamic Pricing for Efficient Demand Response in Smart Grid.},
  archive      = {J_TKDE},
  author       = {Muneeb Ul Hassan and Mubashir Husain Rehmani and Jia Tina Du and Jinjun Chen},
  doi          = {10.1109/TKDE.2022.3157472},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5724-5737},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Differentially private demand side management for incentivized dynamic pricing in smart grid},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep kernel network embedding. <em>TKDE</em>,
<em>35</em>(6), 5710–5723. (<a
href="https://doi.org/10.1109/TKDE.2022.3153053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the problem of network embedding (NE), whose aim is to learn a low-dimensional representation for each node in networks. We shed a new light to solve the sparsity problem where most of nodes including the new arrival nodes have little knowledge with respect to the network. A novel paradigm is proposed to integrate the multiple heterogeneous information from the subgraphs covering the target node instead of only the target node. Particularly, a probabiltiy distribution in subgraph space is contructed for each node, which is more effective to express the distinctive feature over the vertex domain compared to the traditional shallow representations. We boost NE performance by defining the convolution operation over the subgraph distributions that are efficient to evaluate and learn. Our method expliots the advantages of kernel method and deep learning such that the context semantics of subgraph distributions of nodes with dense links is transferred to the sparse nodes effectively via sharing model parameters. Experiments on four real-world network datasets demonstrate that our approach significantly outperforms state-of-the-art methods, especially on the representation learning for the nodes newly joining in the network.},
  archive      = {J_TKDE},
  author       = {Bo Zhang and Xiaoming Zhang and Feiran Huang and Ming Lu and Shuai Ma},
  doi          = {10.1109/TKDE.2022.3153053},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5710-5723},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep kernel network embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CSKG4APT: A cybersecurity knowledge graph for advanced
persistent threat organization attribution. <em>TKDE</em>,
<em>35</em>(6), 5695–5709. (<a
href="https://doi.org/10.1109/TKDE.2022.3175719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-source cyber threat intelligence (OSCTI) is becoming more influential in obtaining current network security information. Most studies on cyber threat intelligence (CTI) focus on automating the extraction of threat entities from public sources that describe attack events. The cybersecurity knowledge graph aims to change the expression of threat knowledge so that security researchers can accurately and efﬁciently obtain various types of threat information for preliminary intelligent decisions. The attribution technology can not only assist security analysts in detecting advanced persistent threats, but can also identify the same threat from different attack events. Therefore, it is important to trace the attack threat actor. In this study, we used the knowledge graph technology, considered the latest research on cyber threat attack attribution, and thoroughly examined key related technologies and theories in the process of constructing and applying the advanced persistent threat (APT) knowledge graph from OSCTI. We designed a cybersecurity platform named CSKG4APT based on a knowledge graph. Inspired by the theory of ontology, we constructed CSKG4APT as an APT knowledge graph model based on real APT attack scenarios. We then designed an APT threat knowledge extraction algorithm for completing and updating the knowledge graph using deep learning and expert knowledge. Finally, we proposed a practical APT attack attribution method with attribution and countermeasures. CSKG4APT is not a passive defense method in traditional network confrontation but one that integrates a large amount of fragmented intelligence and can actively adjust its defense strategy. It lays the foundation for further dominance in network attack and defense.},
  archive      = {J_TKDE},
  author       = {Yitong Ren and Yanjun Xiao and Yinghai Zhou and Zhiyong Zhang and Zhihong Tian},
  doi          = {10.1109/TKDE.2022.3175719},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5695-5709},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CSKG4APT: A cybersecurity knowledge graph for advanced persistent threat organization attribution},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous <span
class="math inline"><em>k</em></span>-regret minimization queries: A
dynamic coreset approach. <em>TKDE</em>, <em>35</em>(6), 5680–5694. (<a
href="https://doi.org/10.1109/TKDE.2022.3166835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a small set of representative tuples from a large database is an important functionality for supporting multi-criteria decision making. Top- $k$ queries and skyline queries are two widely studied queries to fulfill this task. However, both of them have some limitations: a top- $k$ query requires the user to provide her utility functions for finding the $k$ tuples with the highest scores as the result; a skyline query does not need any user-specified utility function but cannot control the result size. To overcome their drawbacks, the $k$ -regret minimization query was proposed and received much attention recently, since it does not require any user-specified utility function and returns a fixed-size result set. Specifically, it selects a set $R$ of tuples with a pre-defined size $r$ from a database $D$ such that the maximum $k$-regret ratio , which captures how well the top-ranked tuple in $R$ represents the top- $k$ tuples in $D$ for any possible utility function, is minimized. Although there have been many methods for $k$ -regret minimization query processing, most of them are designed for static databases without tuple insertions and deletions. The only known algorithm to process continuous $k$ -regret minimization queries (C $k$ RMQ) in dynamic databases suffers from suboptimal approximation and high time complexity. In this paper, we propose a novel dynamic coreset-based approach, called DynCore , for C $k$ RMQ processing. It achieves the same (asymptotically optimal) upper bound on the maximum $k$ -regret ratio as the best-known static algorithm. Meanwhile, its time complexity is sublinear to the database size, which is significantly lower than that of the existing dynamic algorithm. The efficiency and effectiveness of DynCore is confirmed by experimental results on real-world and synthetic datasets.},
  archive      = {J_TKDE},
  author       = {Jiping Zheng and Wei Ma and Yanhao Wang and Xiaoyang Wang},
  doi          = {10.1109/TKDE.2022.3166835},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5680-5694},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Continuous $k$-regret minimization queries: A dynamic coreset approach},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consensus-clustering-based automatic distribution matching
for cross-domain image steganalysis. <em>TKDE</em>, <em>35</em>(6),
5665–5679. (<a href="https://doi.org/10.1109/TKDE.2022.3155924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image steganalysis is a technique to detect whether an image contains hidden information. Although the existing cross-domain steganalysis methods have been presented to narrow the distribution gap between different domains, it is still challenging to effectively capture the transferable steganalysis representations under the condition of severe distribution shifts. To address this issue, we propose a novel consensus-clustering-based automatic distribution matching scheme, called CADM, which can automatically and accurately match inconsistent distributions in cross-domain steganalysis scenarios. First, the original steganalysis features are clustered by the spatially constrained fuzzy $c$ -means (SCFCM) algorithm with controllable parameters to fully perceive and mine inherent structural relationships. Subsequently, the cluster consensus knowledge is derived from the perspective of intra-domain and inter-domain to facilitate the clustering and the matching. In this way, the representations of weak stego signals can be augmented by identifying cluster centers that can be combined across domains. Ultimately, the cycle-consistent optimization and adaptation is achieved by gradually adjusting the learning strength of well-aligned and poorly-aligned samples to promote the positive transfer of overlapped clusters and prevent the negative transfer of outlier clusters. Furthermore, extensive experiments on various benchmark databases for cross-domain steganalysis demonstrate the superiority of CADM over the current state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Ju Jia and Meng Luo and Siqi Ma and Lina Wang and Yang Liu},
  doi          = {10.1109/TKDE.2022.3155924},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5665-5679},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Consensus-clustering-based automatic distribution matching for cross-domain image steganalysis},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Breaking the expression bottleneck of graph neural networks.
<em>TKDE</em>, <em>35</em>(6), 5652–5664. (<a
href="https://doi.org/10.1109/TKDE.2022.3168070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressiveness of graph neural networks (GNNs), showing that the neighborhood aggregation GNNs were at most as powerful as 1-WL test in distinguishing graph structures. There were also improvements proposed in analogy to $k$ -WL test ( $k&amp;gt;1$ ). However, the aggregations in these GNNs are far from injective as required by the WL test, and suffer from weak distinguishing strength, making it become the expression bottleneck. In this paper, we improve the expressiveness by exploring powerful aggregations. We reformulate an aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements on this matrix for building more powerful and even injective aggregations. We also show the necessity of applying nonlinear units ahead of aggregations, which is different from most existing GNNs. Based on our theoretical analysis, we develop ExpandingConv. Experimental results show that our model significantly boosts performance, especially for large and densely connected graphs.},
  archive      = {J_TKDE},
  author       = {Mingqi Yang and Renjian Wang and Yanming Shen and Heng Qi and Baocai Yin},
  doi          = {10.1109/TKDE.2022.3168070},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5652-5664},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Breaking the expression bottleneck of graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Biomedical knowledge graph embedding with capsule network
for multi-label drug-drug interaction prediction. <em>TKDE</em>,
<em>35</em>(6), 5640–5651. (<a
href="https://doi.org/10.1109/TKDE.2022.3154792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug-drug interaction (DDI) plays an important role in drug development and administration. Identifying potential DDI effectively is critical for public health since it can avoid adverse drug effects to a certain extent. Most of existing network-based computation models regard the DDI prediction as a binary classification problem and generate negative DDI samples randomly, but the binary classification is not in line with the real problem since there are dozens of types of DDI and randomly generating negative samples may introduce false-negative samples since the non-observed facts can be either false or just missing. To address the above limitations, we propose a new framework called KG2ECapsule that explicitly models the multi-relational DDI data based on biomedical knowledge graphs in an end-to-end fashion. It first generates high-quality negative samples based on the average number of tail entities and head entities for each relation to reduce false-negative samples to some extent. KG2ECapsule then refines the representations of entities by recursively propagating the embeddings from the attention-based receptive fields of entities. Moreover, KG2ECapsule conducts non-linear transformation to enrich the representations of entities under specified relational space based on capsule network and scores the triplets of drug-relation-drug. Empirical results on three biomedical knowledge graphs of different scales show that KG2ECapsule outperforms the state-of-the-art methods consistently in multi-label DDI prediction task and further studies verify the efficacy of both probability-based sampling strategy and non-linear transformation for modeling multi-relational data.},
  archive      = {J_TKDE},
  author       = {Xiaorui Su and Zhuhong You and Deshuang Huang and Lei Wang and Leon Wong and Boya Ji and Bowei Zhao},
  doi          = {10.1109/TKDE.2022.3154792},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5640-5651},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Biomedical knowledge graph embedding with capsule network for multi-label drug-drug interaction prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). <span
class="math inline"><code>A</code><code>u</code><code>t</code><code>o</code><code>V</code><code>i</code><code>e</code><code>w</code></span>:
An autonomous materialized view management system with encoder-reducer.
<em>TKDE</em>, <em>35</em>(6), 5626–5639. (<a
href="https://doi.org/10.1109/TKDE.2022.3163195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Materialized views (MVs) can significantly optimize the query processing in databases. However, it is hard to generate MVs for ordinary users because it relies on background knowledge, and existing methods rely on DBAs to generate and maintain MVs. However, DBAs cannot handle large-scale databases, especially cloud databases that have millions of database instances and support millions of users. Thus it calls for an autonomous MV management system. In this paper, we propose an autonomous materialized view management system, $\mathtt {AutoView}$ . It analyzes query workloads, estimates the costs and benefits of materializing queries as views, and selects MVs to maximize the benefit within a space budget. We propose a serialization and encoding method that can encode the features of both queries and views into vectors. Then we design a sequence-to-sequence model, Encoder-Reducer, to estimate MVs’ cost/benefit by taking the encoding vectors as input. Next, we propose a deep reinforcement learning model to select high-quality MVs, which enriches the state representation with Encoder-Reducer’s output. Experimental results show that our method outperforms existing studies in terms of MV selection quality.},
  archive      = {J_TKDE},
  author       = {Yue Han and Guoliang Li and Haitao Yuan and Ji Sun},
  doi          = {10.1109/TKDE.2022.3163195},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5626-5639},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {$\mathtt {AutoView}$: An autonomous materialized view management system with encoder-reducer},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An NVM SSD-based high performance query processing
framework for search engines. <em>TKDE</em>, <em>35</em>(6), 5612–5625.
(<a href="https://doi.org/10.1109/TKDE.2022.3160557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commercial search engines generally maintain hundreds of thousands of machines equipped with large sized DRAM in order to process huge volume of user queries with fast responsiveness, which incurs high hardware cost since DRAM is very expensive. Recently, NVM Optane SSD has been considered as a promising underlying storage device due to its price advantage over DRAM and speed advantage over traditional slow block devices. However, to achieve a comparable efficiency performance with in-memory index, applying NVM to both latency and I/O bandwidth critical applications such as search engines still face non-trivial challenges, because NVM has much lower I/O speed and bandwidth compared to DRAM. In this paper, we propose an NVM SSD-optimized query processing framework, aiming to address both the latency and bandwidth issues of using NVM in search engines. First, we propose a pipelined query processing methodology which significantly reduces the I/O waiting time by fine-grained overlapping of the computation and I/O operations. Second, we propose a cache-aware query reordering algorithm which enables queries sharing more data to be processed adjacently so that the I/O traffic is minimized. Third, we propose a data prefetching mechanism which reduces the extra thread waiting time due to data sharing and improves bandwidth utilization. Moreover, we propose intra-query parallel mechanisms for long-tail queries, including query subtask scheduling, heap concurrent access strategy, query parallelism prediction and adaptive pipelining. Extensive experimental studies show that our framework significantly outperforms the state-of-the-art baselines, which obtains comparable processing latency and throughput with DRAM while using much less space in both inter-query and intra-query parallel scenarios.},
  archive      = {J_TKDE},
  author       = {Xinyu Liu and Yu Pan and Yusen Li and Gang Wang and Xiaoguang Liu},
  doi          = {10.1109/TKDE.2022.3160557},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5612-5625},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An NVM SSD-based high performance query processing framework for search engines},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aligning dynamic social networks: An optimization over
dynamic graph autoencoder. <em>TKDE</em>, <em>35</em>(6), 5597–5611. (<a
href="https://doi.org/10.1109/TKDE.2022.3152502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social network alignment, aligning different social networks on their common users, is receiving increasing attention from both academic and industry. Most of the existing studies consider the social network to be static and neglect its inherent dynamics. In fact, the dynamics of social networks contain the discriminative pattern of an individual, which can be leveraged to facilitate social network alignment. Hence, we for the first time propose to study the problem of aligning dynamic social networks. Towards this end, we propose a novel Dynamic Graph autoencoder based dynamic social network Alignment approach, referred to as DGA , unfolding the fruitful dynamics of social networks for user alignment. However, it faces challenges in both modeling and optimization: (1) To model the intra-network dynamics, we design a novel dynamic graph autoencoder to learn user embeddings with complex network dynamics. (2) To model the inter-network alignment, we design a unified optimization framework over proposed dynamic graph autoencoders, constructing a common subspace for user alignment across different networks. (3) To address this optimization problem, we design an effective alternating algorithm with solid theoretical guarantees. We conduct extensive experiments on real-world datasets and show that the proposed approach substantially outperforms the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Li Sun and Zhongbao Zhang and Feiyang Wang and Pengxin Ji and Jian Wen and Sen Su and Philip S. Yu},
  doi          = {10.1109/TKDE.2022.3152502},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5597-5611},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Aligning dynamic social networks: An optimization over dynamic graph autoencoder},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey and experimental study on privacy-preserving
trajectory data publishing. <em>TKDE</em>, <em>35</em>(6), 5577–5596.
(<a href="https://doi.org/10.1109/TKDE.2022.3174204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory data has become ubiquitous nowadays, which can benefit various real-world applications such as traffic management and location-based services. However, trajectories may disclose highly sensitive information of an individual including mobility patterns, personal profiles and gazetteers, social relationships, etc, making it indispensable to consider privacy protection when releasing trajectory data. Ensuring privacy on trajectories demands more than hiding single locations, since trajectories are intrinsically sparse and high-dimensional, and require to protect multi-scale correlations. To this end, extensive research has been conducted to design effective techniques for privacy-preserving trajectory data publishing. Furthermore, protecting privacy requires carefully balance two metrics: privacy and utility. In other words, it needs to protect as much privacy as possible and meanwhile guarantee the usefulness of the released trajectories for data analysis. In this survey, we provide a comprehensive study and a systematic summarization of existing protection models, privacy and utility metrics for trajectories developed in the literature. We also conduct extensive experiments on two real-life public trajectory datasets to evaluate the performance of several representative privacy protection models, demonstrate the trade-off between privacy and utility, and guide the choice of the right privacy model for trajectory publishing given certain privacy and utility desiderata.},
  archive      = {J_TKDE},
  author       = {Fengmei Jin and Wen Hua and Matteo Francia and Pingfu Chao and Maria E Orlowska and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2022.3174204},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5577-5596},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey and experimental study on privacy-preserving trajectory data publishing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sparse reconstructive evidential k-nearest neighbor
classifier for high-dimensional data. <em>TKDE</em>, <em>35</em>(6),
5563–5576. (<a href="https://doi.org/10.1109/TKDE.2022.3157346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Evidential K -Nearest Neighbor (EK-NN) classification rule provides a global treatment of uncertainty and imprecision in class labels, and has been widely used in pattern recognition. Nevertheless, EK-NN still suffers from the fixed presupposition of hyper-parameter K without prior knowledge, due to the different spatial distribution of neighbors of each pattern in Euclidean space. More concretely, neighbors of some patterns may provide confusing information and then derive wrong classification results. To address this issue, we propose a sparse reconstructive evidential K -NN (SEK-NN) classifier, appropriately determining an individual K for each pattern and mapping the correlations between patterns from Euclidean space to a sparse reconstructed space. To match with this sparse reconstructed space, SEK-NN supersedes the Euclidean distance by correlation coefficients to measure the dissimilarities between patterns. When handling high-dimensional data, a parallel version of SEK-NN is implemented under the Apache Spark to speed up the parameter estimation. We respectively test SEK-NN and parallel SEK-NN over 19 middle dimensional datasets, 1 middle volume and 4 high-dimensional datasets that are up to 100 thousand of dimensions. Experimental results show that SEK-NN has great prediction performance and parallel SEK-NN is able to appropriately tackle high-dimensional datasets.},
  archive      = {J_TKDE},
  author       = {Chaoyu Gong and Zhi-Gang Su and Pei-Hong Wang and Qian Wang and Yang You},
  doi          = {10.1109/TKDE.2022.3157346},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5563-5576},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A sparse reconstructive evidential K-nearest neighbor classifier for high-dimensional data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust oversampling approach for class imbalance problem
with small disjuncts. <em>TKDE</em>, <em>35</em>(6), 5550–5562. (<a
href="https://doi.org/10.1109/TKDE.2022.3161291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is one of the important challenges for machine learning because of it’s learning to bias toward the majority classes. The oversampling method is a fundamental imbalance-learning technique with many real-world applications. However, when the small disjuncts problem occurs, how to effectively avoiding the negative oversampling results rather than using clusters previously, remains a challenging task. Thus, this study introduces a disjuncts-robust oversampling (DROS) method. The novel method shows that the data filling of new synthetic samples to the minority class areas in data space can be thought of as the searchlight illuminating with light cones to the restricted areas in real life. In the first step, DROS computes a series of light-cone structures that is first started from the inner minority class area, then passes through the boundary minority class area, last is stopped by the majority class area. In the second step, DROS generates new synthetic samples in those light-cone structures. Experiments considering both real-world and 2D emulational datasets demonstrate that our method outperforms the current state-of-the-art oversampling methods and suggest that our method is able to deal with the small disjuncts.},
  archive      = {J_TKDE},
  author       = {Yi Sun and Lijun Cai and Bo Liao and Wen Zhu and Junlin Xu},
  doi          = {10.1109/TKDE.2022.3161291},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5550-5562},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A robust oversampling approach for class imbalance problem with small disjuncts},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A knowledge-enriched ensemble method for word embedding and
multi-sense embedding. <em>TKDE</em>, <em>35</em>(6), 5534–5549. (<a
href="https://doi.org/10.1109/TKDE.2022.3159539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representing words as embeddings has been proven to be successful in improving the performance in many natural language processing tasks. Different from the traditional methods that learn the embeddings from large text corpora, ensemble methods have been proposed to leverage the merits of pre-trained word embeddings as well as external semantic sources. In this paper, we propose a knowledge-enriched ensemble method to combine information from both knowledge graphs and pre-trained word embeddings. Specifically, we propose an attention network to retrofit the semantic information in the lexical knowledge graph into the pre-trained word embeddings. In addition, we further extend our method to contextual word embeddings and multi-sense embeddings. Extensive experiments demonstrate that the proposed word embeddings outperform the state-of-the-art models in word analogy, word similarity and several downstream tasks. The proposed word sense embeddings outperform the state-of-the-art models in word similarity and word sense induction tasks.},
  archive      = {J_TKDE},
  author       = {Lanting Fang and Yong Luo and Kaiyu Feng and Kaiqi Zhao and Aiqun Hu},
  doi          = {10.1109/TKDE.2022.3159539},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5534-5549},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A knowledge-enriched ensemble method for word embedding and multi-sense embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A joint maximum likelihood estimation framework for truth
discovery: A unified perspective. <em>TKDE</em>, <em>35</em>(6),
5521–5533. (<a href="https://doi.org/10.1109/TKDE.2022.3173911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Truth discovery algorithms have been widely applied to identify the true claims from the conflicting information provided by multiple sources. In general, they conduct an iterative procedure to estimate source reliability degrees as weights and infer the true claims via weighted voting. However, there is little prior work that provides theoretical analysis on the convergence of truth discovery methods. In this paper, we formulated the truth discovery task as a joint maximum likelihood estimation (JMLE) problem for unknown source reliability and truth claims. Within this framework, we proposed a Unified Truth Discovery (UTD) algorithm to get the numerical solution to JMLE for truth and source reliability. With mild conditions, we proved the consistency of the JMLE and the convergence of the proposed UTD algorithm. In addition, our proposed UTD algorithm turns out to include many existing truth discovery algorithms as special cases. This guarantees that our theoretical results can be applied to these truth discovery algorithms. We further conduct extensive experiments on synthetic data sets as well as five real-world data sets, and results from these numerical analysis support the theoretical results of the proposed UTD algorithm and the other state-of-the-art truth discovery algorithms.},
  archive      = {J_TKDE},
  author       = {Houping Xiao and Shiyu Wang},
  doi          = {10.1109/TKDE.2022.3173911},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5521-5533},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A joint maximum likelihood estimation framework for truth discovery: A unified perspective},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A high-dimensional outlier detection approach based on local
coulomb force. <em>TKDE</em>, <em>35</em>(6), 5506–5520. (<a
href="https://doi.org/10.1109/TKDE.2022.3172167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional outlier detections are inadequate for high-dimensional data analysis due to the interference of distance tending to be concentrated (“curse of dimensionality”). Inspired by the Coulomb’s law, we propose a new high-dimensional data similarity measure vector, which consists of outlier Coulomb force and outlier Coulomb resultant force. Outlier Coulomb force not only effectively gauges similarity measures among data objects, but also fully reflects differences among dimensions of data objects by vector projection in each dimension. More importantly, Coulomb resultant force can effectively measure deviations of data objects from a data center, making detection results interpretable. We introduce a new neighborhood outlier factor, which drives the development of a high-dimensional outlier detection algorithm. In our approach, attribute values with a high deviation degree is treated as interpretable information of outlier data. Finally, we implement and evaluate our algorithm using the UCI and synthetic datasets. Our experimental results show that the algorithm effectively alleviates the interference of “Curse of Dimensionality”. The findings confirm that high-dimensional outlier data originated by the algorithm are interpretable.},
  archive      = {J_TKDE},
  author       = {Pengyun Zhu and Chaowei Zhang and Xiaofeng Li and Jifu Zhang and Xiao Qin},
  doi          = {10.1109/TKDE.2022.3172167},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5506-5520},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A high-dimensional outlier detection approach based on local coulomb force},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general cardinality estimation framework for subgraph
matching in property graphs. <em>TKDE</em>, <em>35</em>(6), 5485–5505.
(<a href="https://doi.org/10.1109/TKDE.2022.3161328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework for cardinality estimation of query patterns over property graph databases. This framework makes it possible to analyze, compare and combine different cardinality estimation approaches. It consists of three phases: obtaining a set of estimates for some subqueries, extending this set and finally combining the set into a single cardinality estimate for the query. We show that (parts of) many existing cardinality estimation approaches can be used as techniques in one of the phases from our framework. The phases are loosely coupled, making it possible to combine (parts of) current cardinality estimation approaches. We created a graph version of the Join Order Benchmark to perform experiments with different combinations of techniques. The results showed that query patterns without property constraints can be accurately estimated using synopses for small patterns. Accurate estimation of query patterns with property constraints require new estimation techniques to be developed that capture correlations between the property constraints and the topology in graph databases.},
  archive      = {J_TKDE},
  author       = {Wilco van Leeuwen and George Fletcher and Nikolay Yakovets},
  doi          = {10.1109/TKDE.2022.3161328},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5485-5505},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A general cardinality estimation framework for subgraph matching in property graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast local balanced label diffusion algorithm for
community detection in social networks. <em>TKDE</em>, <em>35</em>(6),
5472–5484. (<a href="https://doi.org/10.1109/TKDE.2022.3162161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection in large-scale networks is one of the main challenges in social networks analysis. Proposing a fast and accurate algorithm with low time complexity is vital for large-scale networks. In this paper, a fast community detection algorithm based on local balanced label diffusion (LBLD) is proposed. The LBLD algorithm starts with assigning node importance score to each node using a new local similarity measure. After that, top 5\% important nodes are selected as initial rough cores to expand communities. In the first step, two neighbor nodes with highest similarity than others receive a same label. In the second step, based on the selected rough cores, the proposed algorithm diffuses labels in a balanced approach from both core and border nodes to expand communities. Next, a label selection step is performed to ensure that each node is surrendered by the most appropriate label. Finally, by utilizing a fast merge step, final communities are discovered. Besides, the proposed method not only has a fast convergence speed, but also provides stable and accurate results. Moreover, there is no randomness as well as adjustable parameter in the LBLD algorithm. Performed experiments on real-world and synthetic networks show the superiority of the LBLD method compared with examined algorithms.},
  archive      = {J_TKDE},
  author       = {Hamid Roghani and Asgarali Bouyer},
  doi          = {10.1109/TKDE.2022.3162161},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5472-5484},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A fast local balanced label diffusion algorithm for community detection in social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast and more accurate seed-and-extension density-based
clustering algorithm. <em>TKDE</em>, <em>35</em>(6), 5458–5471. (<a
href="https://doi.org/10.1109/TKDE.2022.3161117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms have been widely studied in many scientific areas, such as data mining, knowledge discovery, bioinformatics and machine learning. A density-based clustering algorithm, called density peaks (DP), which was proposed by Rodriguez and Laio, outperforms almost all other approaches. Although the DP algorithm performs well in many cases, there is still room for improvement in the precision of its output clusters as well as the quality of the selected centers. In this study, we propose a more accurate clustering algorithm, seed-and-extension-based density peaks (SDP). SDP selects the centers that hold the features of their clusters while building a spanning forest, and meanwhile, constructs the output clusters in a seed-and-extension manner. Experiment results demonstrate the effectiveness of SDP, especially when dealing with clusters with relatively high densities. Precisely, we show that SDP is more accurate than the DP algorithm as well as other state-of-the-art clustering approaches concerning the quality of both output clusters and cluster centers while maintaining similar running time of the DP algorithm, particularly for a variety of time-series data. Moreover, SDP outperforms DP in the dynamic model in which data point insertion and deletion are allowed. From a practical perspective, the proposed SDP algorithm is obviously helpful to many application problems.},
  archive      = {J_TKDE},
  author       = {Ming-Hao Tung and Yi-Ping Phoebe Chen and Chen-Yu Liu and Chung-Shou Liao},
  doi          = {10.1109/TKDE.2022.3161117},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5458-5471},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A fast and more accurate seed-and-extension density-based clustering algorithm},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A correlation graph based approach for personalized and
compatible web APIs recommendation in mobile APP development.
<em>TKDE</em>, <em>35</em>(6), 5444–5457. (<a
href="https://doi.org/10.1109/TKDE.2022.3168611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using Web APIs registered in service sharing communities for mobile APP development can not only reduce development period and cost, but also fully reuse state-of-the-art research outcomes in broad domain so as to ensure up-to-date APP development and applications. However, the big volume of available APIs in Web communities as well as their differences make it difficult for APIs selection considering compatibility, preferred partial APIs and expected APIs functions which are often of high variety. Accordingly, how to recommend a set of functional-satisfactory and compatibility-optimal APIs based on the APP developer&#39;s multiple function expectation and pre-chosen partial APIs is on demand as a significant challenge for successful APP development. To address this challenge, we first construct a Web APIs correlation graph that incorporates functional descriptions and compatibility information of Web APIs, and then propose a correlation graph-based approach for personalized and compatible Web APIs recommendation in mobile APP development. Finally, through extensive experiments on a real dataset crawled from Web APIs websites, we prove the feasibility of our proposed recommendation approach.},
  archive      = {J_TKDE},
  author       = {Lianyong Qi and Wenmin Lin and Xuyun Zhang and Wanchun Dou and Xiaolong Xu and Jinjun Chen},
  doi          = {10.1109/TKDE.2022.3168611},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {6},
  pages        = {5444-5457},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A correlation graph based approach for personalized and compatible web APIs recommendation in mobile APP development},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A similarity-based framework for classification task.
<em>TKDE</em>, <em>35</em>(5), 5438–5443. (<a
href="https://doi.org/10.1109/TKDE.2022.3151979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity-based method gives rise to a new class of methods for multi-label learning and also achieves promising performance. In this paper, we generalize this method, resulting in a new framework for classification task. Specifically, we unite similarity-based learning and generalized linear models to achieve the best of both worlds. This allows us to capture interdependencies between classes and prevent from impairing performance of noisy classes. Each learned parameter of the model can reveal the contribution of one class to another, providing interpretability to some extent. Experiment results show the effectiveness of the proposed approach on multi-class and multi-label datasets.},
  archive      = {J_TKDE},
  author       = {Zhongchen Ma and Songcan Chen},
  doi          = {10.1109/TKDE.2022.3151979},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5438-5443},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A similarity-based framework for classification task},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot hashing via asymmetric ratio similarity matrix.
<em>TKDE</em>, <em>35</em>(5), 5426–5437. (<a
href="https://doi.org/10.1109/TKDE.2022.3150790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot hashing targets to learn the hash codes of images in unseen classes based on the limited training data provided by seen classes. In zero-shot hashing, transferring the supervised knowledge, such as attributes and semantic relations, from seen classes to unseen ones is a widely employed method, where the performance is always subject to the ability to capture these supervised knowledge (which is always difficult to obtain). Therefore, in this study, we propose a new methodology for zero-shot hashing via an asymmetric ratio similarity matrix (ASZH), which only needs to calculate the semantic similarity among seen classes for hash learning. Specifically, we use an asymmetric ratio matrix in the similarity calculation to further explore the influence of similarity, where the values of positive weights for similar samples are not equivalent to those of negative ones for dissimilar samples. Additionally, a theoretical analysis regarding the utilization of an asymmetric ratio matrix is provided in this study. The experiments on three large benchmark datasets indicate that the proposed method achieves excellent performance than several state-of-the-art hashing methods.},
  archive      = {J_TKDE},
  author       = {Yang Shi and Xiushan Nie and Xingbo Liu and Lu Yang and Yilong Yin},
  doi          = {10.1109/TKDE.2022.3150790},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5426-5437},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Zero-shot hashing via asymmetric ratio similarity matrix},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Who’s next: Rising star prediction via diffusion of user
interest in social networks. <em>TKDE</em>, <em>35</em>(5), 5413–5425.
(<a href="https://doi.org/10.1109/TKDE.2022.3151835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding items with potential to increase sales is of great importance in online market. In this paper, we propose to study this novel and practical problem: rising star prediction. We call these potential items Rising Star , which implies their ability to rise from low-turnover items to best-sellers in the future. Rising stars can be used to help with unfair recommendation in e-commerce platform, balance supply and demand to benefit the retailers and allocate marketing resources rationally. Although the study of rising star can bring great benefits, it also poses challenges to us. The sales trend of rising star fluctuates sharply in the short-term and exhibits more contingency caused by some external events (e.g., COVID-19 caused increasing purchase of the face mask) than other items, which cannot be solved by existing sales prediction methods. To address above challenges, in this paper, we observe that the presence of rising stars is closely correlated with the early diffusion of user interest in social networks, which is validated in the case of Taocode (an intermediary that diffuses user interest in Taobao). Thus, we propose a novel framework, RiseNet, to incorporate the user interest diffusion process with the item dynamic features to effectively predict rising stars. Specifically, we adopt a coupled mechanism to capture the dynamic interplay between items and user interest, and a special designed GNN based framework to quantify user interest. Our experimental results on large-scale real-world datasets provided by Taobao demonstrate the effectiveness of our proposed framework.},
  archive      = {J_TKDE},
  author       = {Xuan Yang and Yang Yang and Jintao Su and Yifei Sun and Shen Fan and Zhongyao Wang and Jun Zhang and Jingmin Chen},
  doi          = {10.1109/TKDE.2022.3151835},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5413-5425},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Who&#39;s next: Rising star prediction via diffusion of user interest in social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). What you like, what i am: Online dating recommendation via
matching individual preferences with features. <em>TKDE</em>,
<em>35</em>(5), 5400–5412. (<a
href="https://doi.org/10.1109/TKDE.2022.3148485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dating recommendation becomes a critical task since the rapidly development of the online dating sites and it is beneficial for users to find their ideal relationships from a large number of registered members. Different users usually have different tastes when choosing their dating partners. Therefore, it is necessary to distinguish the user’s personal features and preferences in dating recommendation methods. However, present approaches don’t capture enough user preferences from social graph and attribute data. They also ignore user attributes, which is the complementary and consistent side information of user social graphs. In this paper, we propose a Matching Individual Preferences with Features (MIPF) model to recommend dating partners jointly using user attributes and social graphs. Our specific design in the model is that user preferences and user features are completely different. User features are users’ personal characteristics, while user preferences indicate which kind of other users they are seeking. We aim to model user features and preferences to identify what the user is and what the user likes. We also distinguish user preferences into explicit preference and implicit preferences. The implicit preferences are mined from social graphs, while the explicit preferences are captured from the social links. Additionally, convolutional neural networks are used to extract the latent non-linear information in user attributes. Experiments on real-world online dating datasets demonstrate our MIPF model is superior to existing methods.},
  archive      = {J_TKDE},
  author       = {Xuanzhi Zheng and Guoshuai Zhao and Li Zhu and Jihua Zhu and Xueming Qian},
  doi          = {10.1109/TKDE.2022.3148485},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5400-5412},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {What you like, what i am: Online dating recommendation via matching individual preferences with features},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Verifiable fuzzy multi-keyword search over encrypted data
with adaptive security. <em>TKDE</em>, <em>35</em>(5), 5386–5399. (<a
href="https://doi.org/10.1109/TKDE.2022.3152033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure the security of outsourced data without affecting data availability, one can use Symmetric Searchable Encryption (SSE) to achieve search over encrypted data. Considering that query users may search with misspelled words, the fuzzy search should be supported. However, conventional privacy-preserving fuzzy multi-keyword search schemes are incapable of achieving the result verification and adaptive security. To solve the above challenging issues, in this paper we propose a V erifiable F uzzy multi-keyword S earch scheme with A daptive security (VFSA). VFSA first employs the locality sensitive hashing to hash the misspelled and correct keywords to the same positions, then designs a twin Bloom filter for each document to store and mask all keywords contained in the document, next constructs an index tree based on the graph-based keyword partition algorithm to achieve adaptive sublinear retrieval, finally combines the Merkle hash tree structure with the adapted multiset accumulator to check the correctness and completeness of search results. Our formal security analysis shows that VFSA is secure under the IND-CKA2 model and achieves query authentication. Our empirical experiments using the real-world dataset demonstrate the practicality of VFSA.},
  archive      = {J_TKDE},
  author       = {Qiuyun Tong and Yinbin Miao and Jian Weng and Ximeng Liu and Kim-Kwang Raymond Choo and Robert H. Deng},
  doi          = {10.1109/TKDE.2022.3152033},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5386-5399},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Verifiable fuzzy multi-keyword search over encrypted data with adaptive security},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational bandwidth auto-encoder for hybrid recommender
systems. <em>TKDE</em>, <em>35</em>(5), 5371–5385. (<a
href="https://doi.org/10.1109/TKDE.2022.3155408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid recommendations have recently attracted a lot of attention where user features are utilized as auxiliary information to address the sparsity problem caused by insufficient user-item interactions. However, extracted user features generally contain rich multimodal information, and most of them are irrelevant to the recommendation purpose. Therefore, excessive reliance on these features will make the model overfit on noise and difficult to generalize. In this article, we propose a variational bandwidth auto-encoder (VBAE) for recommendations, aiming to address the sparsity and noise problems simultaneously. VBAE first encodes user collaborative and feature information into Gaussian latent variables via deep neural networks to capture non-linear user similarities. Moreover, by considering the fusion of collaborative and feature variables as a virtual communication channel from an information-theoretic perspective, we introduce a user-dependent channel to dynamically control the information allowed to be accessed from the feature embeddings. A quantum-inspired uncertainty measurement of the hidden rating embeddings is proposed accordingly to infer the channel bandwidth by disentangling the uncertainty information in the ratings from the semantic information. Through this mechanism, VBAE incorporates adequate auxiliary information from user features if collaborative information is insufficient, while avoiding excessive reliance on noisy user features to improve its generalization ability to new users. Extensive experiments conducted on three real-world datasets demonstrate the effectiveness of the proposed method. Codes and datasets are released at https://github.com/yaochenzhu/VBAE .},
  archive      = {J_TKDE},
  author       = {Yaochen Zhu and Zhenzhong Chen},
  doi          = {10.1109/TKDE.2022.3155408},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5371-5385},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Variational bandwidth auto-encoder for hybrid recommender systems},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traffic anomaly prediction based on joint static-dynamic
spatio-temporal evolutionary learning. <em>TKDE</em>, <em>35</em>(5),
5356–5370. (<a href="https://doi.org/10.1109/TKDE.2022.3150272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic anomaly prediction offers an opportunity to save the wounded at the right location in time. However, the complex process of traffic anomaly is affected by both various static factors and dynamic interactions. The recent evolving representation learning provides a new possibility to understand this complicated process, but with challenges of imbalanced data distribution and heterogeneity of features. To tackle these problems, this paper proposes a spatio-temporal evolution model named SNIPER for learning intricate feature interactions to predict traffic anomalies. Specifically, we design spatio-temporal encoders to transform spatio-temporal information into vector space indicating their natural relationship. Then, we propose a temporally dynamical evolving embedding method to pay more attention to rare traffic anomalies and develop an effective attention-based multiple graph convolutional network to formulate the spatially mutual influence from three different perspectives. The FC-LSTM is adopted to aggregate the heterogeneous features considering the spatio-temporal influences. Finally, a loss function is designed to overcome the ’over-smoothing’ and solve the imbalanced data problem. Extensive experiments show that SNIPER averagely outperforms state-of-the-arts by 3.9\%, 0.9\%, 1.9\% and 1.6\% on Chicago datasets, and 2.4\%, 0.6\%, 2.6\% and 1.3\% on New York City datasets in metrics of AUC-PR, AUC-ROC, F1 score, and accuracy, respectively.},
  archive      = {J_TKDE},
  author       = {Xiaoming Liu and Zhanwei Zhang and Lingjuan Lyu and Zhaohan Zhang and Shuai Xiao and Chao Shen and Philip S. Yu},
  doi          = {10.1109/TKDE.2022.3150272},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5356-5370},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Traffic anomaly prediction based on joint static-dynamic spatio-temporal evolutionary learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards automatic job description generation with
capability-aware neural networks. <em>TKDE</em>, <em>35</em>(5),
5341–5355. (<a href="https://doi.org/10.1109/TKDE.2022.3145396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A job description shows the responsibilities of the job position and the skill requirements for the job. An effective job description will help employers to identify the right talents for the job, and give a clear understanding to candidates of what their duties and qualifications for a particular position would be. However, due to the variation in experiences, it is always a challenge for both hiring managers and recruiters to decide what capabilities the job requires and prioritize them accordingly on the job description. Also, tedious and expensive human efforts are usually required to prepare a job description. Therefore, in this paper, we investigate how to automate the process to generate job descriptions with less human intervention. To this end, we propose an end-to-end capability-aware neural job description generation framework, namely Cajon, to facilitate the writing of job description. Specifically, we first propose a novel capability-aware neural topic model to distill the various capability information from the larger-scale recruitment data. Also, an encoder-decoder recurrent neural network is designed for enabling the job description generation. In particular, the capability-aware attention mechanism and copy mechanism are proposed to guide the generation process to ensure the generated job descriptions can comprehensively cover relevant and representative capability requirements for the job. Moreover, we propose a capability-aware policy gradient training algorithm to further enhance the rationality of the generated job description. Finally, extensive experiments on real-world recruitment data clearly show our Cajon framework can help to generate more effective job descriptions in an interpretable way. In particular, our Cajon framework has been deployed in Baidu as an intelligent tool for talent recruitment.},
  archive      = {J_TKDE},
  author       = {Chuan Qin and Kaichun Yao and Hengshu Zhu and Tong Xu and Dazhong Shen and Enhong Chen and Hui Xiong},
  doi          = {10.1109/TKDE.2022.3145396},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5341-5355},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards automatic job description generation with capability-aware neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Top-k socio-spatial co-engaged location selection for social
users. <em>TKDE</em>, <em>35</em>(5), 5325–5340. (<a
href="https://doi.org/10.1109/TKDE.2022.3151095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of location-based social networks, users can tag their daily activities in different locations through check-ins. These check-in locations signify user preferences for various socio-spatial activities and can be used to improve the quality of services in some applications such as recommendation systems, advertising, and group formation. To support such applications, in this paper, we formulate a new problem of identifying top-k S ocio- S patial co-engaged L ocation Selection ( SSLS ) for users in a social graph, that selects the best set of $k$ locations from a large number of location candidates relating to the user and her friends. The selected locations should be (i) spatially and socially relevant to the user and her friends, and (ii) diversified both spatially and socially to maximize the coverage of friends in the socio-spatial space. This problem has been proved as NP-hard. To address such a challenging problem, we first develop an Exact solution by designing some pruning strategies based on derived bounds on diversity. To make the solution scalable for large datasets, we also develop an approximate solution by deriving relaxed bounds and advanced termination rules to filter out insignificant intermediate results. To further accelerate the efficiency, we present one fast exact approach and a meta-heuristic approximate approach by avoiding the repeated computation of diversity at the running time. Finally, we have performed extensive experiments to evaluate the performance of our proposed algorithms against three adapted existing methods using four large real-world datasets.},
  archive      = {J_TKDE},
  author       = {Nur Al Hasan Haldar and Jianxin Li and Mohammed Eunus Ali and Taotao Cai and Yunliang Chen and Timos Sellis and Mark Reynolds},
  doi          = {10.1109/TKDE.2022.3151095},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5325-5340},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Top-k socio-spatial co-engaged location selection for social users},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Telecom fraud detection via hawkes-enhanced sequence model.
<em>TKDE</em>, <em>35</em>(5), 5311–5324. (<a
href="https://doi.org/10.1109/TKDE.2022.3150803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting frauds from a massive amount of user behavioral data is often regarded as finding a needle in a haystack. While tremendous efforts have been devoted to fraud detection from behavioral sequences, existing studies rarely consider behavioral targets and companions and their interactions simultaneously in a sequence model. In this paper, we suggest extracting source and target neighbor sequences from the temporal bipartite network of user behaviors, and disclose the interesting correlation mode and repetition mode hidden inside the two types of sequences as important clues for fraudsters distinguishment. We then propose a novel Hawkes-enhanced sequence model (HESM) by integrating the Hawkes process into LSTM for historical influence learning. A historical attention mechanism is also proposed to enhance the strength of the long-term historical influence in response to the repetition mode. Moreover, in order to collectively model both types of neighbor sequences for capturing the correlation mode, we propose a correlation gate to control the information flow in sequences. We conduct extensive experiments on real-world datasets and demonstrate that HESM outperforms competitive baseline methods consistently in telecom fraud detection. Particularly, the abilities of HESM in historical influence leaning and sequence correlation learning have been explored visually and intensively.},
  archive      = {J_TKDE},
  author       = {Yan Jiang and Guannan Liu and Junjie Wu and Hao Lin},
  doi          = {10.1109/TKDE.2022.3150803},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5311-5324},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Telecom fraud detection via hawkes-enhanced sequence model},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Streaming graph embeddings via incremental neighborhood
sketching. <em>TKDE</em>, <em>35</em>(5), 5296–5310. (<a
href="https://doi.org/10.1109/TKDE.2022.3149999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embeddings have become a key paradigm to learn node representations and facilitate downstream graph analysis tasks. Many real-world scenarios such as online social networks and communication networks involve streaming graphs, where edges connecting nodes are continuously received in a streaming manner, making the underlying graph structures evolve over time. Such a streaming graph raises great challenges for graph embedding techniques not only in capturing the structural dynamics of the graph, but also in efficiently accommodating high-speed edge streams. Against this background, we propose SGSketch, a highly-efficient streaming graph embedding technique via incremental neighborhood sketching. SGSketch cannot only generate high-quality node embeddings from a streaming graph by gradually forgetting outdated streaming edges, but also efficiently update the generated node embeddings via an incremental embedding updating mechanism. Our extensive evaluation compares SGSketch against a sizable collection of state-of-the-art techniques using both synthetic and real-world streaming graphs. The results show that SGSketch achieves superior performance on different graph analysis tasks, showing 31.9\% and 21.9\% improvement on average over the best-performing static and dynamic graph embedding baselines, respectively. Moreover, SGSketch is significantly more efficient in both embedding learning and incremental embedding updating processes, showing 54x-1813x and 118x-1955x speedup over the baseline techniques, respectively.},
  archive      = {J_TKDE},
  author       = {Dingqi Yang and Bingqing Qu and Jie Yang and Liang Wang and Philippe Cudre-Mauroux},
  doi          = {10.1109/TKDE.2022.3149999},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5296-5310},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Streaming graph embeddings via incremental neighborhood sketching},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shortening passengers’ travel time: A dynamic metro train
scheduling approach using deep reinforcement learning. <em>TKDE</em>,
<em>35</em>(5), 5282–5295. (<a
href="https://doi.org/10.1109/TKDE.2022.3153385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban metros have become the foremost public transit to modern cities, carrying millions of daily rides. As travel efficiency matters to the work productivity of the city, shortening passengers’ travel time for metros is therefore a pressing need, which can bring substantial economic benefits. In this paper, we study a fine-grained, safe, and energy-efficient strategy to improve the efficiency of metro systems by dynamically scheduling dwell time for trains. However, developing such a strategy is very challenging because of three aspects: 1) The objective of optimizing the average travel time of passengers is complex, as it needs to properly balance passengers’ waiting time at platforms and journey time on trains, as well as considering long-term impacts on the whole metro system; 2) Capturing dynamic spatio-temporal (ST) correlations of incoming passengers for metro stations is difficult; and 3) For each train, the dwell time scheduling is affected by other trains on the same metro line, which is not easy to measure. To tackle these challenges, we propose a novel deep neural network, entitled AutoDwell. Specifically, AutoDwell optimizes the long-term rewards of dwell time settings in terms of passengers’ waiting time at platforms and journey time on trains by a reinforcement learning framework. Next, AutoDwell employs gated recurrent units and graph attention networks to extract the ST correlations of the passenger flows among metro stations. In addition, attention mechanisms are leveraged in AutoDwell for capturing the interactions between the trains on the same metro line. Extensive experiments on two real-world datasets collected from Beijing and Hangzhou, China, demonstrate the superior performance of AutoDwell over several baselines, capable of saving passengers’ overall travel time. In particular, the model can shorten the waiting time by at least 9\%, which can boost passengers’ experience significantly.},
  archive      = {J_TKDE},
  author       = {Zhaoyuan Wang and Zheyi Pan and Shun Chen and Shenggong Ji and Xiuwen Yi and Junbo Zhang and Jingyuan Wang and Zhiguo Gong and Tianrui Li and Yu Zheng},
  doi          = {10.1109/TKDE.2022.3153385},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5282-5295},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Shortening passengers’ travel time: A dynamic metro train scheduling approach using deep reinforcement learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Short text topic learning using heterogeneous information
network. <em>TKDE</em>, <em>35</em>(5), 5269–5281. (<a
href="https://doi.org/10.1109/TKDE.2022.3147766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of short texts on users’ interests and preferences, learning discriminative and coherent latent topics from short texts is a critical and significative work, since many practical applications, such as e-commerce and recommendations, require semantic understandings that short texts convey explicitly and implicitly. However, existing short text topic learning methods face the challenge of fully capturing semantically related co-occurrence phrases. Therefore, this paper proposes a novel H eterogeneous I nformation N etwork-based Sho rt T ext T opic learning approach (HIN-ShoTT) in terms of parts of speech, without depending on any auxiliary information. Specifically, HIN-ShoTT can be decomposed into three phases: ${{i}}$ ) seeking semantic relations among words with different parts of speech, where HIN-ShoTT models multiple explicit and implicit semantic relations among words based on a Heterogeneous Information Network (HIN) in terms of parts of speech; ${{ii}}$ ) extracting co-occurrence phrases and filtering noises, where HIN-ShoTT defines parts-of-speech meta structures to guide co-occurrence phrase extraction and a self-adapting threshold filtering module is proposed for discarding noises; and ${{iii}}$ ) inferring topics, where HIN-ShoTT directly models the generative process of co-occurrence phrases to make topic learning effective with the abundant corpus-level information. Our experimental results on three real-world datasets not only show that HIN-ShoTT performs well, but also demonstrate that it is feasible to incorporate HIN into short text topic learning for accuracy improvement.},
  archive      = {J_TKDE},
  author       = {Qingren Wang and Chengcheng Zhu and Yiwen Zhang and Hong Zhong and Jinqin Zhong and Victor S. Sheng},
  doi          = {10.1109/TKDE.2022.3147766},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5269-5281},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Short text topic learning using heterogeneous information network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023h). Semi-supervised learning via bipartite graph construction
with adaptive neighbors. <em>TKDE</em>, <em>35</em>(5), 5257–5268. (<a
href="https://doi.org/10.1109/TKDE.2022.3151315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based semi-supervised learning, which further utilizes graph structure behind samples for boosting semi-supervised learning, gains convincing results in several machine learning tasks. Nevertheless, existing graph-based methods have shortcomings from two aspects. On the one hand, many of them concentrate on improving label propagation over the constructed graph through time-saving methods, e.g., path searching, without giving insights on constructing a proper graph accommodated to samples. On the other hand, some models are only devoted to constructing the appropriate graph resulting in a two-stage procedure, which may incur a suboptimal scenario. In this paper, we develop a joint learning method that considers both bipartite graph construction and label propagation simultaneously. With this configuration, the constructed graph is constantly adjusted by the smoothness term in the objective as the algorithm proceeds. The time complexity of our method gets significant improvement compared with traditional graph-based methods, and the experimental results on one synthetic dataset and several real-world benchmarks demonstrate the effectiveness and scalability of our proposed method.},
  archive      = {J_TKDE},
  author       = {Zhen Wang and Long Zhang and Rong Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TKDE.2022.3151315},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5257-5268},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised learning via bipartite graph construction with adaptive neighbors},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised clustering under a “compact-cluster”
assumption. <em>TKDE</em>, <em>35</em>(5), 5244–5256. (<a
href="https://doi.org/10.1109/TKDE.2022.3145347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised clustering (SSC) aims to improve clustering performance with the support of prior knowledge (i.e., side information). Compared with pairwise constraints, the partial labeling information is more natural to characterize the data distribution in a high level. However, the natural gap between the class information and the clustering is not adequately taken into account in exiting SSC methods when utilizing partial labeling information to guide the clustering procedure. In order to address this problem, we present a “compact-cluster” assumption for SSC to utilize the partial labeling information via a cluster-splitting technique. Based on this assumption, a general framework, CSSC, is proposed to supervise the traditional clustering with an objective function which is defined by incorporating an item to measure the compact degree of clusters. Furthermore, we provide two effective solutions for Kmeans and spectral clustering within the CSSC framework and derive the corresponding algorithms to seek the optimum number of clusters and their centroids. Corresponding theoretical analyses demonstrate the feasibility and effectivity of the proposed method. Finally, the extensive experiments on eight real-world datasets demonstrate the superiority of our method over other state-of-the-art SSC methods.},
  archive      = {J_TKDE},
  author       = {Zhen Jiang and Yongzhao Zhan and Qirong Mao and Yang Du},
  doi          = {10.1109/TKDE.2022.3145347},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5244-5256},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised clustering under a “Compact-cluster” assumption},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised air quality forecasting via self-supervised
hierarchical graph neural network. <em>TKDE</em>, <em>35</em>(5),
5230–5243. (<a href="https://doi.org/10.1109/TKDE.2022.3149815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting air quality in fine spatiotemporal granularity is of great importance for air pollution control and urban sustainability. However, existing studies are either focused on predicting station-wise future air quality, or inferring current air quality for unmonitored regions. How to accurately forecast future air quality for these unmonitored regions in a fine granularity remains an unexplored problem. In this paper, we propose the Self-Supervised Hierarchical Graph Neural Network (SSH-GNN), for fine-grained air quality forecasting in a semi-supervised way. Specifically, to augment spatially sparse air quality observations, SSH-GNN first approximates the city-wide air quality distribution based on historical readings and various urban contextual factors (e.g., weather conditions and traffic flows). Then, we propose a hierarchical recurrent graph neural network to make city-wide predictions, which encodes the spatial hierarchy of urban regions for long-range spatiotemporal correlation modeling. Moreover, by leveraging spatiotemporal self-supervision strategies, SSH-GNN exploits both universal topological and contextual patterns to further enhance the forecasting effectiveness. Extensive experiments on two real-world datasets show that SSH-GNN significantly outperforms the state-of-the-art algorithms.},
  archive      = {J_TKDE},
  author       = {Jindong Han and Hao Liu and Haoyi Xiong and Jing Yang},
  doi          = {10.1109/TKDE.2022.3149815},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5230-5243},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised air quality forecasting via self-supervised hierarchical graph neural network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAGES: Scalable attributed graph embedding with sampling for
unsupervised learning. <em>TKDE</em>, <em>35</em>(5), 5216–5229. (<a
href="https://doi.org/10.1109/TKDE.2022.3148272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised graph embedding method generates node embeddings to preserve structural and content features in a graph without human labeling burden. However, most unsupervised graph representation learning methods suffer issues like poor scalability or limited utilization of content/structural relationships, especially on attributed graphs. In this paper, we propose SAGES, a graph sampling based autoencoder framework, which can promote both the performance and scalability of unsupervised learning on attributed graphs. Specifically, we propose a graph sampler that considers both the node connections and node attributes, thus nodes having a high influence on each other will be sampled in the same subgraph. After that, an unbiased Graph Autoencoder (GAE) with structure-level, content-level, and community-level reconstruction loss is built on the properly-sampled subgraphs in each epoch. The time and space complexity analysis is carried out to show the scalability of SAGES. We conducted experiments on three medium-size attributed graphs and three large attributed graphs. Experimental results illustrate that SAGES achieves the competitive performance in unsupervised attributed graph learning on a variety of node classification benchmarks and node clustering benchmarks.},
  archive      = {J_TKDE},
  author       = {Jialin Wang and Xiaoru Qu and Jinze Bai and Zhao Li and Ji Zhang and Jun Gao},
  doi          = {10.1109/TKDE.2022.3148272},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5216-5229},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SAGES: Scalable attributed graph embedding with sampling for unsupervised learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust clustering model based on attention mechanism and
graph convolutional network. <em>TKDE</em>, <em>35</em>(5), 5203–5215.
(<a href="https://doi.org/10.1109/TKDE.2022.3150300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GCN-based clustering schemes cannot interactively fuse feature information of nodes and topological structure information of graphs, leading to insufficient accuracy of clustering results. Moreover, the deep clustering model based on graph structure is vulnerable to the attack of adversarial samples leading to the reduced robustness of the model. To solve the above two problems, this paper proposes a robust clustering model based on attention mechanism and graph convolutional network (GCN), named AG-cluster. This model firstly uses graph attention network and GCN to learn the feature information of nodes and the topological structure information of graphs, respectively. Then the representation results of the above two learning modules are interactively fused by the interlayer transfer operator. Finally, the model is trained end-to-end using a self-supervised training module to optimize the clustering results of the model. In particular, an efficient graph purification defense mechanism (GPDM) is designed to resist adversarial attacks on graph data to improve the robustness of the model. Experimental results show that AG-cluster outperforms the other four benchmark methods, specifically, AG-cluster improves 7.6\% in Accuracy and 11.5\% in NMI compared to the best benchmark method. Besides, the new model still shows higher robustness and stronger transferability under multiple attacks.},
  archive      = {J_TKDE},
  author       = {Hui Xia and Shushu Shao and Chunqiang Hu and Rui Zhang and Tie Qiu and Fu Xiao},
  doi          = {10.1109/TKDE.2022.3150300},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5203-5215},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust clustering model based on attention mechanism and graph convolutional network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reliable keyword query interpretation on summary graphs.
<em>TKDE</em>, <em>35</em>(5), 5187–5202. (<a
href="https://doi.org/10.1109/TKDE.2022.3144001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic gap between keyword queries and search intents behind them motivates intensive studies on keyword query interpretation, which aims to interpret a keyword query to structured queries (a.k.a. patterns) representing most possibly relevant search intents. However, there still lacks of study on an important issue: how to guarantee the patterns are “reliable”, which means the structured queries can be evaluated as really existing results. In this paper, we regard the reliability as a new metric for ranking patterns, and present a keyword query interpretation approach to find both reliable and relevant pattern trees on an arbitrary summary graph of underlying data. Specifically, we first propose a reliability estimation model to measure how possibly a pattern tree can be evaluated as a nonempty result set by statistics under reasonable assumptions. Second, we develop constrained top- $k$ search algorithms that guarantee to return the optimal pattern trees for a specific keyword query. Moreover, to improve the efficiency of online search, we also design elaborate indexes, search heuristics and pruning strategies. Lastly, we perform comprehensive experiments on two real-world datasets, DBpedia and Yago, with both QALD-9 queries and random queries. The observations indicate our approach improves the accuracy and overall quality of top- $k$ results significantly.},
  archive      = {J_TKDE},
  author       = {Ming Zhong and Yingyi Zheng and Guotong Xue and Mengchi Liu},
  doi          = {10.1109/TKDE.2022.3144001},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5187-5202},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reliable keyword query interpretation on summary graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ProbSky: Efficient computation of probabilistic skyline
queries over distributed data. <em>TKDE</em>, <em>35</em>(5), 5173–5186.
(<a href="https://doi.org/10.1109/TKDE.2022.3151740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skyline queries have been widely used in various application domains including multi-criteria decision making, search pruning, and personalized recommendation systems. Given multiple criteria, skyline queries prune the search space of a large collection of multi-dimensional objects to a small set by returning objects that are not dominated by or superior to others. As an extension of the traditional skyline queries, probabilistic skyline queries aim to cope with uncertain datasets. This paper presents a novel MapReduce-based framework, ProbSky, in support of fast parallel distributed evaluation of probabilistic skyline queries on large high-dimensional data. ProbSky is adept at efficiently evaluating exact $p$ -skyline queries on large uncertain data without compromising the quality of query results. From the theoretical point of view, we formally prove two pruning lemmas integrated with ProbSky to strengthen the early pruning capacity. ProbSky builds on top of three optimization techniques: dominant instance pruning, slab-based partitioning, and reference point-based acceleration. These extensive experiments driven by both real and synthetic datasets, reveal that compared to the state-of-the-art methods ProbSky speeds up the evaluation of the exact $p$ -skyline queries on large high-dimensional data by at least one order of magnitude in most cases. Our experimental results also validate that by balancing the memory consumption and execution time among machines, ProbSky is adroit at curbing the bottleneck effect that causes severe system performance deterioration.},
  archive      = {J_TKDE},
  author       = {Ai-Te Kuo and Haiquan Chen and Liang Tang and Wei-Shinn Ku and Xiao Qin},
  doi          = {10.1109/TKDE.2022.3151740},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5173-5186},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ProbSky: Efficient computation of probabilistic skyline queries over distributed data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving boolean range query with temporal access
control in mobile computing. <em>TKDE</em>, <em>35</em>(5), 5159–5172.
(<a href="https://doi.org/10.1109/TKDE.2022.3152168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasingly popular GPS-equipped mobile devices (e.g., smartphones, tablets, laptops), massive spatio-textual data has been outsourced to cloud servers for storage and analysis such as spatial keyword search. However, existing privacy-preserving spatial keyword query schemes only support coarse-grained non-temporal access control in single user sharing scenario, which does not scale well in time-related scenes such as message valid period. To solve the above issues, we propose P rivacy-preserving B oolean R ange Q uery with T emporal access control in mobile computing (PBRQ-T). Specifically, we first achieve PBRQ with linear search complexity using the adapted Gray code, Bloom filter, and Katz-Sahai-Waters encryption. Then, we provide fine-grained and temporal access control in PBRQ based on the forward/backward derivation function and attribute-based encryption, where PBRQ is executed only when the spatio-textual data is accessible. Finally, an enhanced PBRQ-T (i.e., PBRQ-T $^+$ ) with faster-than-linear search complexity is proposed by constructing a Quadtree index structure. Our formal security analysis shows that data privacy and index privacy can be guaranteed during the query process. Our extensive experiments using a real-world dataset demonstrate the efficiency and feasibility of our schemes.},
  archive      = {J_TKDE},
  author       = {Qiuyun Tong and Xinghua Li and Yinbin Miao and Ximeng Liu and Jian Weng and Robert H. Deng},
  doi          = {10.1109/TKDE.2022.3152168},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5159-5172},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Privacy-preserving boolean range query with temporal access control in mobile computing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Point-set kernel clustering. <em>TKDE</em>, <em>35</em>(5),
5147–5158. (<a href="https://doi.org/10.1109/TKDE.2022.3144914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring similarity between two objects is the core operation in existing clustering algorithms in grouping similar objects into clusters. This paper introduces a new similarity measure called point-set kernel which computes the similarity between an object and a set of objects. The proposed clustering procedure utilizes this new measure to characterize every cluster grown from a seed object. We show that the new clustering procedure is both effective and efficient that enables it to deal with large scale datasets. In contrast, existing clustering algorithms are either efficient or effective. In comparison with the state-of-the-art density-peak clustering and scalable kernel k-means clustering, we show that the proposed algorithm is more effective and runs orders of magnitude faster when applying to datasets of millions of data points, on a commonly used computing machine.},
  archive      = {J_TKDE},
  author       = {Kai Ming Ting and Jonathan R. Wells and Ye Zhu},
  doi          = {10.1109/TKDE.2022.3144914},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5147-5158},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Point-set kernel clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel non-negative matrix tri-factorization for text data
co-clustering. <em>TKDE</em>, <em>35</em>(5), 5132–5146. (<a
href="https://doi.org/10.1109/TKDE.2022.3145489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a novel paradigm for data mining and dimensionality reduction, Non-negative Matrix Tri-Factorization (NMTF) has attracted much attention due to its notable performance and elegant mathematical derivation, and it has been applied to a plethora of real-world applications, such as text data co-clustering. However, the existing NMTF-based methods usually involve intensive matrix multiplications, which exhibits a major limitation of high computational complexity. With the explosion at both the size and the feature dimension of texts, there is a growing need to develop a parallel and scalable NMTF-based algorithm for text data co-clustering. To this end, we first show in this paper how to theoretically derive the original optimization problem of NMTF by introducing the Lagrangian multipliers. Then, we propose to solve the Lagrange dual objective function in parallel through an efficient distributed implementation. Extensive experiments on five benchmark corpora validate the effectiveness, efficiency, and scalability of our distributed parallel update algorithm for an NMTF-based text data co-clustering method.},
  archive      = {J_TKDE},
  author       = {Yufu Chen and Zhiqi Lei and Yanghui Rao and Haoran Xie and Fu Lee Wang and Jian Yin and Qing Li},
  doi          = {10.1109/TKDE.2022.3145489},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5132-5146},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Parallel non-negative matrix tri-factorization for text data co-clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network representation lightening from hashing to
quantization. <em>TKDE</em>, <em>35</em>(5), 5119–5131. (<a
href="https://doi.org/10.1109/TKDE.2022.3151474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information network embedding is an important way to enable efficient graph analytics. However, it still faces with computational challenges in problems such as link prediction and node recommendation, particularly with the increasing scale of networks. Both hashing and quantization are promising approaches for accelerating these problems by orders of magnitude. In the preliminary work, we have proposed to learn binary codes for information networks, but graph analytics may suffer from large accuracy degradation. To reduce information loss while achieving memory and search efficiency, we further propose to learn quantized codes for information networks. In particular, each node is represented by compositing multiple latent vectors, each of which is optimally selected from a distinct set. Since (generalized) matrix factorization unifies several well-known embedding methods with high-order proximity preserved, we propose a N etwork R epresentation L ightening framework based on M atrix F actorization (NRL-MF) to learn binary and quantized codes. We also propose an alternating optimization algorithm for efficient parameter learning, even for the generalized matrix factorization case. We finally evaluate NRL-MF on four real-world information network datasets with respect to the tasks of node classification and node recommendation. The results show that NRL-MF significantly outperforms competing baselines in both tasks, and that quantized representations indeed incur much smaller information loss than binarized codes.},
  archive      = {J_TKDE},
  author       = {Defu Lian and Zhihao Zhu and Kai Zheng and Yong Ge and Xing Xie and Enhong Chen},
  doi          = {10.1109/TKDE.2022.3151474},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5119-5131},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Network representation lightening from hashing to quantization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-level attention map network for multimodal sentiment
analysis. <em>TKDE</em>, <em>35</em>(5), 5105–5118. (<a
href="https://doi.org/10.1109/TKDE.2022.3155290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA) is a very challenging task due to its complex and complementary interactions between multiple modalities, which can be widely applied into areas of product marketing, public opinion monitoring, and so on. However, previous works directly utilized the features extracted from multimodal data, in which the noise reduction within and among multiple modalities has been largely ignored before multimodal fusion. This paper proposes a multi-level attention map network (MAMN) to filter noise before multimodal fusion and capture the consistent and heterogeneous correlations among multi-granularity features for multimodal sentiment analysis. Architecturally, MAMN is comprised of three modules: multi-granularity feature extraction module, multi-level attention map generation module, and attention map fusion module. The first module is designed to sufficiently extract multi-granularity features from multimodal data. The second module is constructed to filter noise and enhance the representation ability for multi-granularity features before multimodal fusion. And the third module is built to extensibly mine the interactions among multi-level attention maps by the proposed extensible co-attention fusion method. Extensive experimental results on three public datasets show the proposed model is significantly superior to the state-of-the-art methods, and demonstrate its effectiveness on two tasks of document-based and aspect-based MSA tasks.},
  archive      = {J_TKDE},
  author       = {Xiaojun Xue and Chunxia Zhang and Zhendong Niu and Xindong Wu},
  doi          = {10.1109/TKDE.2022.3155290},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5105-5118},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-level attention map network for multimodal sentiment analysis},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modality-invariant asymmetric networks for cross-modal
hashing. <em>TKDE</em>, <em>35</em>(5), 5091–5104. (<a
href="https://doi.org/10.1109/TKDE.2022.3144352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing has garnered considerable attention and gained great success in many cross-media similarity search applications due to its prominent computational efficiency and low storage overhead. However, it still remains challenging how to effectively take multilevel advantages of semantics on the entire database to jointly bridge the semantic and heterogeneity gaps across different modalities. In this paper, we propose a novel Modality-Invariant Asymmetric Networks (MIAN) architecture, which explores the asymmetric intra- and inter-modal similarity preservation under a probabilistic modality alignment framework. Specifically, an intra-modal asymmetric network is conceived to capture the query-vs-all internal pairwise similarities for each modality in a probabilistic asymmetric learning manner. Moreover, an inter-modal asymmetric network is deployed to fully harness the cross-modal semantic similarities supported by the maximum inner product search formula between two distinct hash embeddings. Particularly, the pairwise, piecewise and transformed semantics are jointly considered into one unified semantic-preserving hash codes learning scheme. Furthermore, we construct a modality alignment network to distill the redundancy-free visual features and maximize the conditional bottleneck information between different modalities. Such a network could close the heterogeneity and domain shift across different modalities and enable it to yield discriminative modality-invariant hash codes. Extensive experiments evidence that our MIAN approach can outperform the state-of-the-art cross-modal hashing methods.},
  archive      = {J_TKDE},
  author       = {Zheng Zhang and Haoyang Luo and Lei Zhu and Guangming Lu and Heng Tao Shen},
  doi          = {10.1109/TKDE.2022.3144352},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5091-5104},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modality-invariant asymmetric networks for cross-modal hashing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rank tensor based proximity learning for multi-view
clustering. <em>TKDE</em>, <em>35</em>(5), 5076–5090. (<a
href="https://doi.org/10.1109/TKDE.2022.3151861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-oriented multi-view clustering methods have achieved impressive performances by employing relationships and complex structures hidden in multi-view data. However, most of them still suffer from the following two common problems. (1) They target at studying a common representation or pairwise correlations between views, neglecting the comprehensiveness and deeper higher-order correlations among multiple views. (2) The prior knowledge of view-specific representation can not be taken into account to obtain the consensus indicator graph in a unified graph construction and clustering framework. To deal with these problems, we propose a novel Low-rank Tensor Based Proximity Learning (LTBPL) approach for multi-view clustering, where multiple low-rank probability affinity matrices and consensus indicator graph reflecting the final performances are jointly studied in a unified framework. Specifically, multiple affinity representations are stacked in a low-rank constrained tensor to recover their comprehensiveness and higher-order correlations. Meanwhile, view-specific representation carrying different adaptive confidences is jointly linked with the consensus indicator graph. Extensive experiments on nine real-world datasets indicate the superiority of LTBPL compared with the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Man-Sheng Chen and Chang-Dong Wang and Jian-Huang Lai},
  doi          = {10.1109/TKDE.2022.3151861},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5076-5090},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Low-rank tensor based proximity learning for multi-view clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rank linear embedding for robust clustering.
<em>TKDE</em>, <em>35</em>(5), 5060–5075. (<a
href="https://doi.org/10.1109/TKDE.2022.3144294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of k -means clustering is often degenerate when dealing with high-dimensional and noisy scenarios. In this study, an end-to-end robust clustering method with low-rank linear embedding techniques (RCLR) is presented in conjunction with k -means. Sparse coefficients and a space projection matrix can be simultaneously learned. The global structures and local neighborhood properties are well captured in the learning procedures. Both the processes of clustering and dimensionality reduction are realized at the same time. The notions of clustering, dimensionality reduction, low-rank representation, and local property preservation are seamlessly integrated into a unified model. The limitation of error accumulation encountered in the previous two-stage clustering framework involving low-rank representation can be alleviated. This is the first attempt to introduce both the global and local geometrical structures into k -means directly, as well ${L_{2,1}}$ -norm is used as a basic metric instead of the conventional $F$ -norm to further improve the robustness and interpretation of the model. The superiority of the proposed RCLR method is demonstrated by extensive experiments completed on various well-known benchmark datasets.},
  archive      = {J_TKDE},
  author       = {Jie Zhou and Witold Pedrycz and Jun Wan and Can Gao and Zhihui Lai and Xiaodong Yue},
  doi          = {10.1109/TKDE.2022.3144294},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5060-5075},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Low-rank linear embedding for robust clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). LOKI: A practical data poisoning attack framework against
next item recommendations. <em>TKDE</em>, <em>35</em>(5), 5047–5059. (<a
href="https://doi.org/10.1109/TKDE.2022.3181270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the openness of the online platform, recommendation systems are vulnerable to data poisoning attacks, where malicious samples are injected into the training set of the recommendation system to manipulate its recommendation results. Existing attack approaches are either based on heuristic rules or designed against specific recommendation approaches. The former suffers unsatisfactory performance, while the latter requires strong knowledge of the target system. In this paper, we propose a practical poisoning attack approach named LOKI against blackbox recommendation systems. The proposed LOKI utilizes the reinforcement learning algorithm to train the attack agent, which can be used to generate user behavior samples for data poisoning. In real-world recommendation systems, the cost of retraining recommendation models is high, and the interaction frequency between users and a recommendation system is restricted. Thus, we propose to let the agent interact with a recommender simulator instead of the target recommendation system and leverage the transferability of the generated adversarial samples to poison the target system. We also use the influence function to efficiently estimate the influence of injected samples on recommendation results, without re-training the models. Extensive experiments on multiple datasets against four representative recommendation models show that the proposed LOKI outperformances existing method. We also discuss the characteristics of vulnerable users/items, and evaluate whether anomaly detection methods can be used to mitigate the impact of data poisoning attacks.},
  archive      = {J_TKDE},
  author       = {Hengtong Zhang and Yaliang Li and Bolin Ding and Jing Gao},
  doi          = {10.1109/TKDE.2022.3181270},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5047-5059},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LOKI: A practical data poisoning attack framework against next item recommendations},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023f). Local-to-global deep clustering on approximate uniform
manifold. <em>TKDE</em>, <em>35</em>(5), 5035–5046. (<a
href="https://doi.org/10.1109/TKDE.2022.3144952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering usually treats the clustering assignments as supervisory signals to learn a more compact representation with deep neural networks, under the guidance of clustering-oriented losses. Nevertheless, we observe that, without reliable supervision, such losses for global clustering would destroy the locally geometric structure underlying data. In this paper, we propose a local-to-global deep clustering method based on approximate uniform manifold (LGC-AUM) to address this issue in a two-stage fashion. In the local stage, an intra-manifold preservation loss is proposed to preserve intra-manifold structures locally on basis of approximate uniform manifold, and an inter-manifold discrimination loss is for global inter-manifold structure. Thus, this stage serves to learn more discriminative structure-preserving features by reducing the correlations between different manifolds, which paves the way for the final clustering. Build off the learned features, the second stage explores a clustering loss based on approximate uniform manifold to establish stable network training for effective clustering with two auxiliary distributions. Experiments on five benchmark datasets verify the efficacy of our LGC-AUM as compared to several well-behaved clustering counterparts.},
  archive      = {J_TKDE},
  author       = {Tuo Wang and Xiang Zhang and Long Lan and Zhigang Luo},
  doi          = {10.1109/TKDE.2022.3144952},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5035-5046},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Local-to-global deep clustering on approximate uniform manifold},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning regularized noise contrastive estimation for robust
network embedding. <em>TKDE</em>, <em>35</em>(5), 5017–5034. (<a
href="https://doi.org/10.1109/TKDE.2022.3148284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skip-gram models are popular in large-scale network embedding for their cost-effectiveness. The objectives of many skip-gram based methods relate to the word2vec model which closely relates to Noise Contrastive Estimation (NCE). Among existing embedding methods, the differences mostly lie in how the node neighborhood is modeled e.g., by different ways of random walk, which leads to different learning strategies. Orthogonal to these efforts, we take a unified view that the NCE based methods commonly involve two basic NCE components in the learning objective. This perspective allows a natural generalization of the objectives by taking different forms of scoring function in the NCE components. We theoretically analyze how the vanilla NCE-based objectives suffer from the slow convergence speed and challenge in first-/second-order proximity preservation. We also prove the fundamental difficulty for NCE methods to capture non-linearity of complex networks. To mitigate such issues, we devise a general distance-based term added to the used NCE term, inspired by its physical meaning. The distance functions include Wasserstein- $k$ distance and Laplacian/Gaussian kernel functions, with relatively little additional time overhead. The effectiveness of our approach is verified both by prototype examples as well as real-world datasets, for the task of node classification and network reconstruction.},
  archive      = {J_TKDE},
  author       = {Hao Xiong and Junchi Yan and Zengfeng Huang},
  doi          = {10.1109/TKDE.2022.3148284},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5017-5034},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning regularized noise contrastive estimation for robust network embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning holistic interactions in LBSNs with high-order,
dynamic, and multi-role contexts. <em>TKDE</em>, <em>35</em>(5),
5002–5016. (<a href="https://doi.org/10.1109/TKDE.2022.3150792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location-based social networks (LBSNs) have emerged over the past few years. Their exponential network effects depend on the fact that each user can share her daily digital footprints with different communities, in different places, and at different times (for example in the form of check-in activities). Unlike other types of social networks, activities in an LBSN can potentially be performed by several users in a collaborative way. Existing studies of representation learning for LBSNs often consider them as regular graphs and ignore these high-order, dynamic, and multi-role contexts, since their holistic interactions are quite difficult to capture. In this paper, we propose a model in which these holistic interactions can be learned and transferred into node embeddings derived from a hypergraph representation and a persona decomposition process. More specifically, the model learns from friendship edges, check-in hyperedges, and node personas at the same time, and devises multiple presentations for each user that reflects their multiple roles in a social context. The embedding learning process also exploits useful patterns such as user co-location and sequential effects through a carefully designed point-of-interest splitting step. Extensive experiments on real and synthetic datasets show that our model outperforms alternative state-of-the-art embedding methods on friendship and location prediction tasks by an average margin of 45.7\% and 29.46\%, respectively. We also demonstrate the robustness of our model against adversarial conditions such as structural noise, attribute noise, and hyperparameter sensitivity.},
  archive      = {J_TKDE},
  author       = {Huynh Thanh Trung and Tong Van Vinh and Nguyen Thanh Tam and Jun Jo and Hongzhi Yin and Nguyen Quoc Viet Hung},
  doi          = {10.1109/TKDE.2022.3150792},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {5002-5016},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning holistic interactions in LBSNs with high-order, dynamic, and multi-role contexts},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning decomposed representations for treatment effect
estimation. <em>TKDE</em>, <em>35</em>(5), 4989–5001. (<a
href="https://doi.org/10.1109/TKDE.2022.3150807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational studies, confounder separation and balancing are the fundamental problems of treatment effect estimation. Most of the previous methods focused on addressing the problem of confounder balancing by treating all observed pre-treatment variables as confounders, ignoring confounder separation. In general, not all the observed pre-treatment variables are confounders that refer to the common causes of the treatment and the outcome, some variables only contribute to the treatment (i.e., instrumental variables) and some only contribute to the outcome (i.e., adjustment variables). Balancing those non-confounders, including instrumental variables and adjustment variables, would generate additional bias for treatment effect estimation. By modeling the different causal relations among observed pre-treatment variables, treatment variables and outcome variables, we propose a synergistic learning framework to i) separate confounders by learning decomposed representations of both confounders and non-confounders, ii) balance confounder with sample re-weighting technique, and simultaneously iii) estimate the treatment effect in observational studies via counterfactual inference. Empirical results on synthetic and real-world datasets demonstrate that the proposed method can precisely decompose confounders and achieve a more precise estimation of treatment effect than baselines.},
  archive      = {J_TKDE},
  author       = {Anpeng Wu and Junkun Yuan and Kun Kuang and Bo Li and Runze Wu and Qiang Zhu and Yueting Zhuang and Fei Wu},
  doi          = {10.1109/TKDE.2022.3150807},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4989-5001},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning decomposed representations for treatment effect estimation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge graph quality management: A comprehensive survey.
<em>TKDE</em>, <em>35</em>(5), 4969–4988. (<a
href="https://doi.org/10.1109/TKDE.2022.3150080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a powerful expression of human knowledge in a structural form, knowledge graph (KG) has drawn great attention from both the academia and the industry and a large number of construction and application technologies have been proposed. Large-scale knowledge graphs such as DBpedia, YAGO and Wikidata are published and widely used in various tasks. However, most of them are far from perfect and have many quality issues. For example, they may contain inaccurate or outdated entries and do not cover enough facts, which limits their credibility and further utility. Data quality has a long research history in the field of traditional relational data and recently attracts more knowledge graph experts. In this paper, we provide a systematic and comprehensive review of the quality management on knowledge graphs, covering overall research topics about not only quality issues, dimentions and metrics, but also quality management processes from quality assessment and error detection, to error correction and KG completion. We categorize existing works in terms of target goals and used methods for better understanding. In the end, we discuss some key issues and possible directions on knowledge graph quality management for further research.},
  archive      = {J_TKDE},
  author       = {Bingcong Xue and Lei Zou},
  doi          = {10.1109/TKDE.2022.3150080},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4969-4988},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Knowledge graph quality management: A comprehensive survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge enhanced graph neural networks for explainable
recommendation. <em>TKDE</em>, <em>35</em>(5), 4954–4968. (<a
href="https://doi.org/10.1109/TKDE.2022.3142260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, explainable recommendation has attracted increasing attentions, which can make the recommender system more transparent and improve user satisfactions by recommending products with useful explanations. However, existing methods trend to trade-off between the recommendation accuracy and the interpretability of recommendation results. In this manuscript, we propose Knowledge Enhanced Graph Neural Networks (KEGNN) for explainable recommendation. Semantic knowledge from the external knowledge base is leveraged into representation learning of three sides, respectively user, items and user-item interactions, and the knowledge enhanced semantic embedding are exploited to initialize the user/item entities and user-item relations of one constructed user behavior graph. We design a graph neural networks based user behavior learning and reasoning model to perform both semantic and relational knowledge propagation and reasoning over the user behavior graph for comprehensive understanding of user behaviors. On the top of comprehensive representations of users/items and user-item interactions, hierarchical neural collaborative filtering layers are developed for precise rating prediction, and one generation-mode and copy-mode combined generator is devised for human-like semantic explanation generation by integrating the copy mechanism into gated recurrent neural networks. Quantitative and qualitative results demonstrate the superiority of KEGNN over the state-of-art methods, and the explainability and interpretability of our method.},
  archive      = {J_TKDE},
  author       = {Ziyu Lyu and Yue Wu and Junjie Lai and Min Yang and Chengming Li and Wei Zhou},
  doi          = {10.1109/TKDE.2022.3142260},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4954-4968},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Knowledge enhanced graph neural networks for explainable recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint reason generation and rating prediction for
explainable recommendation. <em>TKDE</em>, <em>35</em>(5), 4940–4953.
(<a href="https://doi.org/10.1109/TKDE.2022.3146178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recommendation systems focus on predicting rating or finding aspect information in reviews to understand user preferences and item properties. However, these methods ignore the effectiveness and persuasiveness of recommendation results. Consequently, explainable recommendation, namely providing recommendation results with recommendation reasons at the same time, has attracted increasing attention of researchers due to its ability in fostering transparency and trust. It is lucky that some E-commerce websites provide a kind of new interaction box called Tips and users can express their comments on items with a simple sentence. This brings us an opportunity to realize explainable recommendation. Under the supervision of two explicit feedbacks, namely rating and textual tips, we can implement a multi-task learning model which can provide recommendation results and generate recommendation reasons at the same time. In this paper, we propose an E ncoder-Decoder and M ulti-Layer Perception (MLP) based E xplainable R ecommendation model named EMER to simultaneously implement reason generation and rating prediction. Item’s title contains significant product-related information and plays an important role in grabbing user’s attention, so we fuse it in our model to generate recommendation reasons. Numerous experiments on benchmark datasets demonstrate that our model is superior to the state-of-the-art models.},
  archive      = {J_TKDE},
  author       = {Jihua Zhu and Yujiao He and Guoshuai Zhao and Xuxiao Bu and Xueming Qian},
  doi          = {10.1109/TKDE.2022.3146178},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4940-4953},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Joint reason generation and rating prediction for explainable recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-grounded goal planning for conversational
recommendation. <em>TKDE</em>, <em>35</em>(5), 4923–4939. (<a
href="https://doi.org/10.1109/TKDE.2022.3147210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommendation casts the recommendation problem as a dialog-based interactive task, which could acquire user interest more efficiently and effectively by allowing users to express what they like. In this work, we move a step towards a new conversational recommendation task that is more suitable for real-world applications. In this task, the recommender will proactively and naturally lead a dialog from non-recommendation content (i.e., chitchat or question answering) to approach an item being of interest to users, and allow users to ask questions about the item for better support of user decisions. The challenge of this task lies in how to effectively control the dialog flow to complete the recommendation while appropriately responding to user utterances. To address this challenge, we first construct a Chinese recommendation dialog dataset with 10k dialogs and 156k utterances at Baidu ( DuRecDial ). We then propose a two-stage Multi-Goal driven Conversation Generation framework ( MGCG ) with a graph-grounded goal planning module and a goal-guided responding module. The goal planning module leverages the information of global graph structure information and local goal-sequence information to effectively control the dialog flow step by step. The goal-guided responding module can produce an in-depth dialog about each goal by fully exploiting hierarchical goal information for response retrieval or generation. Results on DuRecDial demonstrate that compared with the state of the art models, MGCG can lead the dialog more proactively and naturally, and complete the recommendation task more effectively, confirming the benefits of hierarchical goal information to conversational recommendation.},
  archive      = {J_TKDE},
  author       = {Zeming Liu and Ding Zhou and Hao Liu and Haifeng Wang and Zheng-Yu Niu and Hua Wu and Wanxiang Che and Ting Liu and Hui Xiong},
  doi          = {10.1109/TKDE.2022.3147210},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4923-4939},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph-grounded goal planning for conversational recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph transfer learning via adversarial domain adaptation
with graph convolution. <em>TKDE</em>, <em>35</em>(5), 4908–4922. (<a
href="https://doi.org/10.1109/TKDE.2022.3144250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of cross-network node classification to overcome the insufficiency of labeled data in a single network. It aims to leverage the label information in a partially labeled source network to assist node classification in a completely unlabeled or partially labeled target network. Existing methods for single network learning cannot solve this problem due to the domain shift across networks. Some multi-network learning methods heavily rely on the existence of cross-network connections, thus are inapplicable for this problem. To tackle this problem, we propose a novel graph transfer learning framework AdaGCN by leveraging the techniques of adversarial domain adaptation and graph convolution. It consists of two components: a semi-supervised learning component and an adversarial domain adaptation component. The former aims to learn class discriminative node representations with given label information of the source and target networks, while the latter contributes to mitigating the distribution divergence between the source and target domains to facilitate knowledge transfer. Extensive empirical evaluations on real-world datasets show that AdaGCN can successfully transfer class information with a low label rate on the source network and a substantial divergence between the source and target domains.},
  archive      = {J_TKDE},
  author       = {Quanyu Dai and Xiao-Ming Wu and Jiaren Xiao and Xiao Shen and Dan Wang},
  doi          = {10.1109/TKDE.2022.3144250},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4908-4922},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph transfer learning via adversarial domain adaptation with graph convolution},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph substructure assembling network with soft sequence and
context attention. <em>TKDE</em>, <em>35</em>(5), 4894–4907. (<a
href="https://doi.org/10.1109/TKDE.2022.3148299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a surge of researchers’ interest in building predictive models over graphs. However, the overwhelming complexity of graph space often makes it challenging to extract interpretable and discriminative structural features for graph classification. In this work, we propose a new graph neural network model called Substructure Assembling Network (SAN) to learn graph representations for classification. The key innovation is a unified Substructure Assembling Unit (SAU), which is a variant of Recurrent Neural Network (RNN) designed to hierarchically assemble useful pieces of graph components so as to fabricate discriminative substructures. A key challenge is that SAUs need to process the neighbors of a node sequentially while no natural order is defined therein. SAN tries to make the model insensitive to neighborhood orders by randomly shuffling neighborhood sequences in training. However, this could suffer high variance, especially when the neighborhood size is large. Hence, we further propose to equip SAN with a novel module named Soft Sequence with Context Attention (SSCA). SAN-SSCA employs the proposed context attention technique to learn the best “soft” permutation of the neighbors w.r.t. classification. It helps the model achieve higher accuracy as well as lower variance. Experiments confirm the effectiveness of SAN-SSCA.},
  archive      = {J_TKDE},
  author       = {Yaming Yang and Ziyu Guan and Wei Zhao and Weigang Lu and Bo Zong},
  doi          = {10.1109/TKDE.2022.3148299},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4894-4907},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph substructure assembling network with soft sequence and context attention},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph representation learning beyond node and homophily.
<em>TKDE</em>, <em>35</em>(5), 4880–4893. (<a
href="https://doi.org/10.1109/TKDE.2022.3146270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised graph representation learning aims to distill various graph information into a downstream task-agnostic dense vector embedding. However, existing graph representation learning approaches are designed mainly under the node homophily assumption: connected nodes tend to have similar labels and optimize performance on node-centric downstream tasks. Their design is apparently against the task-agnostic principle and generally suffers poor performance in tasks, e.g., edge classification, that demands feature signals beyond the node-view and homophily assumption. To condense different feature signals into the embeddings, this paper proposes PairE, a novel unsupervised graph embedding method using two paired nodes as the basic unit of embedding to retain the high-frequency signals between nodes to support node-related and edge-related tasks. Accordingly, a multi-self-supervised autoencoder is designed to fulfill two pretext tasks: one retains the high-frequency signal better, and another enhances the representation of commonality. Our extensive experiments on a diversity of benchmark datasets clearly show that PairE outperforms the unsupervised state-of-the-art baselines, with up to 101.1\% relative improvement on the edge classification tasks that rely on both the high and low-frequency signals in the pair and up to 82.5\% relative performance gain on the node classification tasks.},
  archive      = {J_TKDE},
  author       = {You Li and Bei Lin and Binli Luo and Ning Gui},
  doi          = {10.1109/TKDE.2022.3146270},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4880-4893},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph representation learning beyond node and homophily},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Graph neural point process for temporal interaction
prediction. <em>TKDE</em>, <em>35</em>(5), 4867–4879. (<a
href="https://doi.org/10.1109/TKDE.2022.3149927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal graphs are ubiquitous data structures in many scenarios, including social networks, user-item interaction networks, etc. In this paper, we focus on predicting the exact time of future interactions between node pairs on a temporal graph. This problem can support interesting applications including time-sensitive items recommendation, congestion prediction on road networks, etc. We present the Graph Neural Point Process (GNPP) to tackle this problem. GNPP relies on the graph neural message passing and the temporal point process framework. Most previous graph neural models devised for temporal graphs either utilize the chronological order information or rely on specific point process models, ignoring the exact timestamps and complicated temporal patterns. In GNPP, we adapt a time encoding scheme to map real-valued timestamps to a high-dimensional vector space so that the temporal information can be modeled precisely. Further, GNPP considers the structural information of graphs by conducting message passing aggregation on the constructed line graph. The obtained representation defines a neural conditional intensity function that models events’ generation mechanisms for predicting interactions’ time between node pairs. We evaluate this model on several synthetic and real-world temporal graphs where it outperforms recently proposed neural point process models and graph neural models devised for temporal graphs. We further conduct ablation comparisons and visual analyses to shed some light on the learned model and understand the functionality of important components comprehensively.},
  archive      = {J_TKDE},
  author       = {Wenwen Xia and Yuchen Li and Shenghong Li},
  doi          = {10.1109/TKDE.2022.3149927},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4867-4879},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph neural point process for temporal interaction prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Granularity-aware area prototypical network with bimargin
loss for few shot relation classification. <em>TKDE</em>,
<em>35</em>(5), 4852–4866. (<a
href="https://doi.org/10.1109/TKDE.2022.3147455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation Classification is one of the most important tasks in text mining. Previous methods either require large-scale manually-annotated data or rely on distant supervision approaches which suffer from the long-tail problem. To reduce the expensive manually-annotating cost and solve the long-tail problem, prototypical networks are widely used in few-shot RC tasks. Despite their remarkable performance, current prototypical networks ignore the different granularities of relations, which degrades the classification performance dramatically. Moreover, the optimization of current prototypical networks simply relies on the cross-entropy loss, which cannot consider the intra-relation compactness and the dispersion among relations in a semantic space. It is not robust enough for the current prototypical network in real-world and complicated scenarios. In this paper, we propose an area prototypical network with a granularity-aware measurement, aiming to consider the different granularities of relations. Each relation is represented as an area whose width can reflect the granularity level of relation. Moreover, to improve the robustness, bimargin loss is designed to force area prototypical network to improve the intra-relation compactness and inter-relation dispersion for the feature representation in a semantic space. Extensive experiments on two public datasets are conducted and evaluate the effectiveness of our proposed model.},
  archive      = {J_TKDE},
  author       = {Haopeng Ren and Yi Cai and Raymond Y.K. Lau and Ho-fung Leung and Qing Li},
  doi          = {10.1109/TKDE.2022.3147455},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4852-4866},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Granularity-aware area prototypical network with bimargin loss for few shot relation classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric algebra based embeddings for static and temporal
knowledge graph completion. <em>TKDE</em>, <em>35</em>(5), 4838–4851.
(<a href="https://doi.org/10.1109/TKDE.2022.3151435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, Knowledge Graph Embeddings (KGEs) have shown promising performance on link prediction tasks by mapping the entities and relations from a Knowledge Graph (KG) into a geometric space and thus have gained increasing attentions. In addition, many recent Knowledge Graphs involve evolving data, e.g., the fact ( Obama , PresidentOf , USA ) is valid only from 2009 to 2017. This introduces important challenges for knowledge representation learning since such temporal KGs change over time. In this work, we strive to move beyond the complex or hypercomplex space for KGE and propose a novel geometric algebra based embedding approach, GeomE, which uses multivector representations and the geometric product to model entities and relations. GeomE subsumes several state-of-the-art KGE models and is able to model diverse relations patterns. On top of this, we extend GeomE to TGeomE for temporal KGE, which performs 4th-order tensor factorization of a temporal KG and devises a new linear temporal regularization for time representation learning. Moreover, we study the effect of time granularity on the performance of TGeomE models. Experimental results show that our proposed models achieve the state-of-the-art performances on link prediction over four commonly-used static KG datasets and four well-established temporal KG datasets across various metrics.},
  archive      = {J_TKDE},
  author       = {Chengjin Xu and Mojtaba Nayyeri and Yung-Yu Chen and Jens Lehmann},
  doi          = {10.1109/TKDE.2022.3151435},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4838-4851},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Geometric algebra based embeddings for static and temporal knowledge graph completion},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating characteristic summaries for entity descriptions.
<em>TKDE</em>, <em>35</em>(5), 4825–4837. (<a
href="https://doi.org/10.1109/TKDE.2022.3144391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-structured data describing entities and their properties has become a notable component of the Web. With the increasing size of data graphs, an entity is often associated with too many property values to be entirely shown to the user, thereby requiring a compact but characteristic summary to present its most distinguishing features. This paper aims to automatically generate such characteristic entity summaries for human users. To achieve it, we exploit the informativeness of property values by analyzing the data graph using information theory. To improve the utility of information carried by a summary, we learn it from a text corpus. To reduce the information redundancy of a summary, we perform logical reasoning and measure similarity with statistical support. We formalize the entity summarization problem considering these factors as combinatorial optimization problems to solve. Experiments based on a real data graph and hand-crafted gold standards show that our approach improves on two state-of-the-art approaches in F-measure by 20.63\%–38.79\%.},
  archive      = {J_TKDE},
  author       = {Gong Cheng and Qingxia Liu and Yuzhong Qu},
  doi          = {10.1109/TKDE.2022.3144391},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4825-4837},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Generating characteristic summaries for entity descriptions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GDSRec: Graph-based decentralized collaborative filtering
for social recommendation. <em>TKDE</em>, <em>35</em>(5), 4813–4824. (<a
href="https://doi.org/10.1109/TKDE.2022.3153284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating recommendations based on user-item interactions and user-user social relations is a common use case in web-based systems. These connections can be naturally represented as graph-structured data and thus utilizing graph neural networks (GNNs) for social recommendation has become a promising research direction. However, existing graph-based methods fails to consider the bias offsets of users (items). For example, a low rating from a fastidious user may not imply a negative attitude toward this item because the user tends to assign low ratings in common cases. Such statistics should be considered into the graph modeling procedure. While some past work considers the biases, we argue that these proposed methods only treat them as scalars and can not capture the complete bias information hidden in data. Besides, social connections between users should also be differentiable so that users with similar item preference would have more influence on each other. To this end, we propose Graph-Based Decentralized Collaborative Filtering for Social Recommendation (GDSRec). GDSRec treats the biases as vectors and fuses them into the process of learning user and item representations. The statistical bias offsets are captured by decentralized neighborhood aggregation while the social connection strength is defined according to the preference similarity and then incorporated into the model design. We conduct extensive experiments on two benchmark datasets to verify the effectiveness of the proposed model. Experimental results show that the proposed GDSRec achieves superior performance compared with state-of-the-art related baselines. Our implementations are available in https://github.com/MEICRS/GDSRec .},
  archive      = {J_TKDE},
  author       = {Jiajia Chen and Xin Xin and Xianfeng Liang and Xiangnan He and Jun Liu},
  doi          = {10.1109/TKDE.2022.3153284},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4813-4824},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GDSRec: Graph-based decentralized collaborative filtering for social recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine-grained entity typing with a type taxonomy: A
systematic review. <em>TKDE</em>, <em>35</em>(5), 4794–4812. (<a
href="https://doi.org/10.1109/TKDE.2022.3148980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained entity typing (FGET) is an important natural language processing (NLP) task. It is to assign fine-grained semantic types of a type taxonomy (e.g., Person / artist / actor ) to entity mentions. Fine-grained entity semantic types have been successfully applied in many natural language processing applications, such as relation extraction, entity linking and question answering. The key challenge for FGET is how to deal with label noises that disperse in corpora since the corpora are normally automatically annotated. Various type taxonomies, typing methods and representation learning approaches for FGET have been proposed and developed in the past two decades. This paper systematically categorizes and reviews these various typing methods and representation learning approaches to provide a reference for future studies on FGET. We also present a comprehensive review of type taxonomies, resources, applications for FGET and methods for automatically generating FGET training corpora. Furthermore, we identify the current trends in FGET research and discuss future research directions for FGET. To the best of our knowledge, this is the first comprehensive review of FGET.},
  archive      = {J_TKDE},
  author       = {Ruili Wang and Feng Hou and Steven F. Cahan and Li Chen and Xiaoyun Jia and Wanting Ji},
  doi          = {10.1109/TKDE.2022.3148980},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4794-4812},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fine-grained entity typing with a type taxonomy: A systematic review},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast unsupervised feature selection with bipartite graph and
<span class="math inline"><em>ℓ</em><sub>2, 0</sub></span>ℓ2,0-norm
constraint. <em>TKDE</em>, <em>35</em>(5), 4781–4793. (<a
href="https://doi.org/10.1109/TKDE.2022.3146403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since obtaining data labels is a time-consuming and laborious task, unsupervised feature selection has become a popular feature selection technique. However, the current unsupervised feature selection methods are facing three challenges: (1) they rely on a fixed similarity matrix derived from the original data, which will affect their performance; (2) due to the limitation of sparsity, they can only obtain sub-optimal solutions; (3) they have high computational complexity and cannot handle large-scale data. To solve this dilemma, we propose a fast unsupervised feature selection algorithm with bipartite graph and $\ell _{2,0}$ -norm constraint (BGCFS). We use the original data and the selected anchors to construct an adaptive bipartite graph in the subspace, and apply the $\ell _{2,0}$ -norm constraint to the projection matrix for feature selection. In this way, we can update the adaptive bipartite graph and the projection matrix simultaneously, and we can get the feature subset directly, without sorting the features. In addition, we propose an iterative algorithm that can solve the proposed problem globally to obtain a closed-form solution, and we provide a strict proof of convergence for it. Experiments on eight real data sets with different scales show that our method can select more valuable feature subsets more quickly.},
  archive      = {J_TKDE},
  author       = {Hong Chen and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2022.3146403},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4781-4793},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast unsupervised feature selection with bipartite graph and $\ell _{2,0}$ℓ2,0-norm constraint},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast LDP-MST: An efficient density-peak-based clustering
method for large-size datasets. <em>TKDE</em>, <em>35</em>(5),
4767–4780. (<a href="https://doi.org/10.1109/TKDE.2022.3150403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a new density-peak-based clustering method, called clustering with local density peaks-based minimum spanning tree (LDP-MST), was proposed, which has several attractive merits, e.g., being able to detect arbitrarily shaped clusters and not very sensitive to noise and parameters. Nevertheless, we also found the limitation of LDP-MST in efficiency. Specifically, LDP-MST has $O(N\log N+M^{2})$ time, where $N$ denotes the dataset size and $M$ is an intermediate variable denoting the number of local density peaks. As our experimental results reveal, when processing large-size datasets, the value of $M$ could be very large and consequently those steps of LDP-MST involving $O(M^{2})$ time term would be time-consuming. And in the worst case, the value of $M$ could be very close to that of $N$ , which means that the time complexity of LDP-MST could be $O(N^{2})$ in the worst case of $M$ . In this study, we use more efficient algorithms to implement those steps of LDP-MST that involve the $O(M^{2})$ time term such that the proposed method, Fast LDP-MST, has $O(N\log N)$ time complexity even if $M\approx N$ . Our experiments demonstrate that Fast LDP-MST is overall more efficient than LDP-MST on large-size datasets, without sacrificing the merits of LDP-MST in effectiveness, robustness, and user-friendliness.},
  archive      = {J_TKDE},
  author       = {Teng Qiu and Yong-Jie Li},
  doi          = {10.1109/TKDE.2022.3150403},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4767-4780},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast LDP-MST: An efficient density-peak-based clustering method for large-size datasets},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic prototype network based on sample adaptation for
few-shot malware detection. <em>TKDE</em>, <em>35</em>(5), 4754–4766.
(<a href="https://doi.org/10.1109/TKDE.2022.3142820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous increase and spread of malware have caused immeasurable losses to social enterprises and even the country, especially unknown malware. Most existing methods use predefined class samples to train models, which cannot handle unknown malware detection. In this paper, we formalize unknown malware detection as a Few-Shot Learning problem. However, the existing model cannot dynamically adjust the model parameters according to the samples and does not deeply consider the influence of the correlation between samples, so it achieves sub-optimal performance. We propose a Dynamic Prototype Network based on Sample Adaptation for few-shot malware detection (DPNSA). Specifically, we use dynamic convolution to realize dynamic feature extraction based on sample adaptation. Secondly, we define the class feature (prototype) as the mean of the dynamic embedding of all malware samples of each class in the support set. Then, a dual-sample dynamic activation function is proposed, which uses the correlation of the dual-sample to reduce the impact of unrelated features between samples on the metric. Finally, we use the metric-based method to calculate the distance between the query sample and the prototype to realize malware detection. Experiments show that our method outperforms the existing few-shot malware detection models and achieves significant improvement.},
  archive      = {J_TKDE},
  author       = {Yuhan Chai and Lei Du and Jing Qiu and Lihua Yin and Zhihong Tian},
  doi          = {10.1109/TKDE.2022.3142820},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4754-4766},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic prototype network based on sample adaptation for few-shot malware detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Dynamic graph neural networks for sequential
recommendation. <em>TKDE</em>, <em>35</em>(5), 4741–4753. (<a
href="https://doi.org/10.1109/TKDE.2022.3151618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling user preference from his historical sequences is one of the core problems of sequential recommendation. Existing methods in this field are widely distributed from conventional methods to deep learning methods. However, most of them only model users’ interests within their own sequences and ignore the dynamic collaborative signals among different user sequences, making it insufficient to explore users’ preferences. We take inspiration from dynamic graph neural networks to cope with this challenge, modeling the user sequence and dynamic collaborative signals into one framework. We propose a new method named Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which connects different user sequences through a dynamic graph structure, exploring the interactive behavior of users and items with time and order information. Furthermore, we design a Dynamic Graph Recommendation Network to extract user&#39;s preferences from the dynamic graph. Consequently, the next-item prediction task in sequential recommendation is converted into a link prediction between the user node and the item node in a dynamic graph. Extensive experiments on four public benchmarks show that DGSR outperforms several state-of-the-art methods. Further studies demonstrate the rationality and effectiveness of modeling user sequences through a dynamic graph.},
  archive      = {J_TKDE},
  author       = {Mengqi Zhang and Shu Wu and Xueli Yu and Qiang Liu and Liang Wang},
  doi          = {10.1109/TKDE.2022.3151618},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4741-4753},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic graph neural networks for sequential recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic classifier alignment for unsupervised multi-source
domain adaptation. <em>TKDE</em>, <em>35</em>(5), 4727–4740. (<a
href="https://doi.org/10.1109/TKDE.2022.3144423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation leverages the previously gained knowledge from a labeled source domain to tackle the task from a different but similar unlabeled target domain. Most existing methods focus on transferring knowledge from a single source domain, but the information from a single domain may be inadequate to complete the target task. Some previous studies have turned to multi-view representations to enrich the transferable information. However, they simply concatenate multi-view features, which may result in information redundancy. In this paper, we propose a dynamic classifier alignment (DCA) method for multi-source domain adaptation, which aligns classifiers driven from multi-view features via a sample-wise automatic way. As proposed, both the importance of each view and the contribution of each source domain are investigated. To determine the important degrees of multiple views, an importance learning function is built by generating an auxiliary classifier. To learn the source combination parameters, a domain discriminator is developed to estimate the probability of a sample belonging to multiple source domains. Meanwhile, a self-training strategy is proposed to enhance the cross-domain ability of source classifiers with the assistance of pseudo target labels. Experiments on real-world visual datasets show the superiority of the proposed DCA.},
  archive      = {J_TKDE},
  author       = {Keqiuyin Li and Jie Lu and Hua Zuo and Guangquan Zhang},
  doi          = {10.1109/TKDE.2022.3144423},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4727-4740},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic classifier alignment for unsupervised multi-source domain adaptation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Durable subgraph matching on temporal graphs. <em>TKDE</em>,
<em>35</em>(5), 4713–4726. (<a
href="https://doi.org/10.1109/TKDE.2022.3148995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Durable subgraph matching on a temporal graph finds all subgraphs in the temporal graph that not only match the given query graph but also have duration longer than a user-specified duration threshold. The state-of-the-art algorithm (Semertzidis and Pitoura, 2016, 2019) for solving this problem requires lots of memory when the input temporal graph is large. In this paper, a new algorithm is proposed to solve this problem. Many effective techniques are developed to improve the performance of this algorithm, including the DFS-based query decomposition method, the TD-tree index structure, the sort-based vertex matching order, and the time instance set compaction method. The experimental results show that the proposed algorithm is an order of magnitude faster and requires significantly less memory than the state-of-the-art algorithm.},
  archive      = {J_TKDE},
  author       = {Faming Li and Zhaonian Zou and Jianzhong Li},
  doi          = {10.1109/TKDE.2022.3148995},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4713-4726},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Durable subgraph matching on temporal graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distilled neural networks for efficient learning to rank.
<em>TKDE</em>, <em>35</em>(5), 4695–4712. (<a
href="https://doi.org/10.1109/TKDE.2022.3152585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies in Learning to Rank have shown the possibility to effectively distill a neural network from an ensemble of regression trees. This result leads neural networks to become a natural competitor of tree-based ensembles on the ranking task. Nevertheless, ensembles of regression trees outperform neural models both in terms of efficiency and effectiveness, particularly when scoring on CPU. In this paper, we propose an approach for speeding up neural scoring time by applying a combination of Distillation, Pruning and Fast Matrix multiplication. We employ knowledge distillation to learn shallow neural networks from an ensemble of regression trees. Then, we exploit an efficiency-oriented pruning technique that performs a sparsification of the most computationally-intensive layers of the neural network that is then scored with optimized sparse matrix multiplication. Moreover, by studying both dense and sparse high performance matrix multiplication, we develop a scoring time prediction model which helps in devising neural network architectures that match the desired efficiency requirements. Comprehensive experiments on two public learning-to-rank datasets show that neural networks produced with our novel approach are competitive at any point of the effectiveness-efficiency trade-off when compared with tree-based ensembles, providing up to 4x scoring time speed-up without affecting the ranking quality.},
  archive      = {J_TKDE},
  author       = {Franco Maria Nardini and Cosimo Rulli and Salvatore Trani and Rossano Venturini},
  doi          = {10.1109/TKDE.2022.3152585},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4695-4712},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Distilled neural networks for efficient learning to rank},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diffusion pixelation: A game diffusion model of rumor &amp;
anti-rumor inspired by image restoration. <em>TKDE</em>, <em>35</em>(5),
4682–4694. (<a href="https://doi.org/10.1109/TKDE.2022.3144310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is inspired by the current image restoration technology. If we regard the users participating in the rumor as image pixels, similar to social networks, the recovery of pixel data is affected by the pixels themselves and neighbor pixels, then the prediction of user behavior in the rumor diffusion can be regarded as the process of image restoration for pixel-blurred user behavior images. We first propose a Diffusion2pixel algorithm that transforms the user relationship network of topic diffusion into image pixel matrix. To cope with the diversity and complexity of the diffusion feature space, the user relationship network is reduced to a low-rank dense vectorization by representation learning before being pixelated by cutting and diffusion. Second, considering the competitive relationship between rumor and anti-rumor, transition matrix of rumor mutual influences is established by evolutionary game theory. A mutual influence model of rumor and anti-rumor is then proposed. Finally, we combine the transition matrix of rumor mutual influence into a simple prediction method Graph-CNN of rumor and anti-rumor topic diffusion based on dynamic iteration mechanism. Experiments confirmed the proposed model can effectively predict the group diffusion trends of rumor, and reflects the competitive relationship between rumor and anti-rumor.},
  archive      = {J_TKDE},
  author       = {Yunpeng Xiao and Zhen Huang and Qian Li and Xingyu Lu and Tun Li},
  doi          = {10.1109/TKDE.2022.3144310},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4682-4694},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Diffusion pixelation: A game diffusion model of rumor &amp; anti-rumor inspired by image restoration},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DataOps-4G: On supporting generalists in data quality
discovery. <em>TKDE</em>, <em>35</em>(5), 4668–4681. (<a
href="https://doi.org/10.1109/TKDE.2022.3151605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data preparation has become a necessary but labor and resource-intensive step to perform data analytics. To date, such activities still require considerable manual effort from experts. In this paper, we focus on a specific data preparation activity, namely data quality discovery. We explore different settings in which data workers undertake data quality discovery tasks and the implications of those settings for the efficiency and effectiveness of data workers. To this end, we propose DataOps-4G, a data quality discovery platform for generalists that allows users to interact with data without the need to write code. We wrap up pre-defined code snippets that implement useful functionalities to explore data quality and bundle the code into so-called DataOps. Then, we conduct a lab-based user study to evaluate our DataOps-4G platform from two perspectives: (i) effectiveness , the accuracy of the outcomes achieved by participants; and (ii) efficiency , their effort and strategies in task completion. Our experimental results uncover how effectiveness and efficiency can be affected by their task completion patterns and strategies. This opens up the possibility of popularizing data quality discovery processes by employing non-experts (e.g., from crowdsourcing platforms) and consequently allowing experts to focus on more complex activities (e.g., building machine learning models).},
  archive      = {J_TKDE},
  author       = {Shaochen Yu and Tianwa Chen and Lei Han and Gianluca Demartini and Shazia Sadiq},
  doi          = {10.1109/TKDE.2022.3151605},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4668-4681},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DataOps-4G: On supporting generalists in data quality discovery},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data management for machine learning: A survey.
<em>TKDE</em>, <em>35</em>(5), 4646–4667. (<a
href="https://doi.org/10.1109/TKDE.2022.3148237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) has widespread applications and has revolutionized many industries, but suffers from several challenges. First, sufficient high-quality training data is inevitable for producing a well-performed model, but the data is always human expensive to acquire. Second, a large amount of training data and complicated model structures lead to the inefficiency of training and inference. Third, given an ML task, one always needs to train lots of models, which are hard to manage in real applications. Fortunately, database techniques can benefit ML by addressing the above three challenges. In this paper, we review existing studies from the following three aspects along with the pipeline highly related to ML. (1) Data preparation (Pre-ML): it focuses on preparing high-quality training data that can improve the performance of the ML model, where we review data discovery, data cleaning and data labeling. (2) Model training &amp; inference (In-ML): researchers in ML community focus on improving the model performance during training, while in this survey we mainly study how to accelerate the entire training process, also including feature selection and model selection. (3) Model management (Post-ML): in this part, we survey how to store, query, deploy and debug the models after training. Finally, we provide research challenges and future directions.},
  archive      = {J_TKDE},
  author       = {Chengliang Chai and Jiayi Wang and Yuyu Luo and Zeping Niu and Guoliang Li},
  doi          = {10.1109/TKDE.2022.3148237},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4646-4667},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Data management for machine learning: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CSCAD: Correlation structure-based collective anomaly
detection in complex system. <em>TKDE</em>, <em>35</em>(5), 4634–4645.
(<a href="https://doi.org/10.1109/TKDE.2022.3154166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting anomalies in large complex systems is a critical and challenging task. The difficulties arise from several aspects. First, collecting ground truth labels or prior knowledge for anomalies is hard in real-world systems, which often lead to limited or no anomaly labels in the dataset. Second, anomalies in large systems usually occur in a collective manner due to the underlying dependency structure among devices or sensors. Lastly, real-time anomaly detection for high-dimensional data requires efficient algorithms that are capable of handling different types of data (i.e. continuous and discrete). We propose a correlation structure-based collective anomaly detection (CSCAD) model for high-dimensional anomaly detection problem in large systems, which is also generalizable to semi-supervised or supervised settings. Our framework utilize graph convolutional network combining a variational autoencoder to jointly exploit the feature space correlation and reconstruction deficiency of samples to perform anomaly detection. We propose an extended mutual information (EMI) metric to mine the internal correlation structure among different data features, which enhances the data reconstruction capability of CSCAD. The reconstruction loss and latent standard deviation vector of a sample obtained from reconstruction network can be perceived as two natural anomalous degree measures. An anomaly discriminating network can then be trained using low anomalous degree samples as positive samples, and high anomalous degree samples as negative samples. Experimental results on five public datasets demonstrate that our approach consistently outperforms all the competing baselines.},
  archive      = {J_TKDE},
  author       = {Huiling Qin and Xianyuan Zhan and Yu Zheng},
  doi          = {10.1109/TKDE.2022.3154166},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4634-4645},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CSCAD: Correlation structure-based collective anomaly detection in complex system},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain knowledge graph chiasmal embedding for
multi-domain item-item recommendation. <em>TKDE</em>, <em>35</em>(5),
4621–4633. (<a href="https://doi.org/10.1109/TKDE.2022.3151986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender system can provide users with the required information accurately and efficiently, playing a very important role in improving users’ life experience. Although knowledge graph-based recommender system can solve the sparsity and cold start problems faced by traditional recommender system, it cannot handle the cross-domain cold start problem and cannot provide multi-domain recommendations. Therefore, this paper focuses on multi-domain item-item (I2I) recommendation based on cross-domain knowledge graph embedding by analyzing the association between items of the same domain and the interaction between items of diverse domains with the aid of knowledge graph that contains rich information. First, a cross-domain knowledge graph chiasmal embedding approach is proposed to efficiently interact all items in multiple domains. To help achieve both homo-domain embedding and hetero-domain embedding of items, a binding rule is put forward. Second, a multi-domain I2I recommendation method is presented to efficiently recommend items in multiple domains, which is a recommendation method based on link prediction of knowledge graph. Finally, the proposed methods are compared and analyzed with some benchmark methods using two datasets. The experimental results show that the proposed methods achieve better link prediction results and multi-domain recommendation results.},
  archive      = {J_TKDE},
  author       = {Jia Liu and Wei Huang and Tianrui Li and Shenggong Ji and Junbo Zhang},
  doi          = {10.1109/TKDE.2022.3151986},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4621-4633},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-domain knowledge graph chiasmal embedding for multi-domain item-item recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Connecting embeddings based on multiplex relational graph
attention networks for knowledge graph entity typing. <em>TKDE</em>,
<em>35</em>(5), 4608–4620. (<a
href="https://doi.org/10.1109/TKDE.2022.3142056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph entity typing (KGET) aims to infer missing entity typing instances in KGs, which is a significant subtask of KG completion. Despite of its progress, however, we observe that it still faces two non-trivial challenges: (i) most existing KGET methods extract features by encoding the existing entity typing tuples, while underutilizing or even ignoring rich relational knowledge. (ii) they typically treat each entity typing tuple in KGs independently, and thus inevitably fail to take account of the inherent and valuable neighborhood information surrounding a tuple. To address these challenges, we build a novel Heterogeneous Relational Graph (HRG), and propose a Multiplex Relational Graph Attention Networks (MRGAT) to learn on HRG, and then utilize a Connecting Embeddings model (ConnectE) to make entity type inference. Specifically, the overall framework contains three significant components. First, to effectively integrate the heterogeneous structural information including the entity typing tuples and entity relation triples in KGs, we construct a heterogeneous relational graph that consists of three semantic subgraphs. Second, we employ MRGAT to learn embeddings on HRG. In MRGAT, each subgraph of HRG is fed to its corresponding model that is capable of capturing neighborhood information by aggregating the surrounding nodes’ features. Finally, given the learned embeddings, we make entity type prediction by the connecting embeddings method ConnectE. Experimental results demonstrate the effectiveness of our proposed model against various state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Yu Zhao and Han Zhou and Anxiang Zhang and Ruobing Xie and Qing Li and Fuzhen Zhuang},
  doi          = {10.1109/TKDE.2022.3142056},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4608-4620},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Connecting embeddings based on multiplex relational graph attention networks for knowledge graph entity typing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Condensed representations of association rules in n-ary
relations. <em>TKDE</em>, <em>35</em>(5), 4598–4607. (<a
href="https://doi.org/10.1109/TKDE.2022.3153709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of association rules mining has given rise to a rich literature, especially in classic binary bidimensional data. In particular, the representation of the set of rules without loss of information is well understood. This is not the case in multidimensional binary data. This paper shows that the knowledge of $n-1$ components of every closed $n$ -sets of a multidimensional Boolean tensor, as well as the cardinality of the remaining dimension, is enough to allow for the derivation of the confidence of every multidimensional association rule. This generalises well-known results in the bidimensional case. This paper provides experimental comparisons between the numbers of closed $n$ -sets and frequent associations.},
  archive      = {J_TKDE},
  author       = {Alexandre Bazin and Nicolas Gros and Aurélie Bertaux and Christophe Nicolle},
  doi          = {10.1109/TKDE.2022.3153709},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4598-4607},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Condensed representations of association rules in n-ary relations},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concept drift detection delay index. <em>TKDE</em>,
<em>35</em>(5), 4585–4597. (<a
href="https://doi.org/10.1109/TKDE.2022.3153349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data streams may encounter data distribution changes, which can significantly impair the accuracy of models. Concept drift detection tracks data distribution changes and signals when to update models. Many drift detection methods apply thresholds to distinguish between drift or non-drift streams and to claim their method outperforms others with non-aligned drift thresholds. We consider that selecting a proper drift threshold could be more important than developing a new drift detection algorithm, and different drift detection algorithms may end up with very similar performance with aligned drift thresholds. To better understand this process, we propose a novel threshold selection algorithm to align the drift thresholds of a set of algorithms so that they are all at the same sensitivity level. Based on comprehensive experiment evaluations, we observed that several state-of-the-art drift detection algorithms could achieve similar results by aligning their thresholds, providing a novel insight to explain how drift detection algorithms contribute to data stream learning. We noticed that a higher detection sensitivity improves accuracy for data streams with frequent distribution change. The evaluation results are showing that drift thresholds should not be fixed during stream learning. Rather, they should adjust dynamically based on the prevailing conditions of the data stream.},
  archive      = {J_TKDE},
  author       = {Anjin Liu and Jie Lu and Yiliao Song and Junyu Xuan and Guangquan Zhang},
  doi          = {10.1109/TKDE.2022.3153349},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4585-4597},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Concept drift detection delay index},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Common neighbors matter: Fast random walk sampling with
common neighbor awareness. <em>TKDE</em>, <em>35</em>(5), 4570–4584. (<a
href="https://doi.org/10.1109/TKDE.2022.3150427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random walk is widely applied to sample large-scale graphs due to its simplicity of implementation and solid theoretical foundations of bias analysis. However, its computational efficiency is heavily limited by the slow convergence rate (a.k.a. long burn-in period). To address this issue, we propose a common neighbor aware random walk framework called CNARW, which leverages weighted walking by differentiating the next-hop candidate nodes to speed up the convergence. Specifically, CNARW takes into consideration the common neighbors between previously visited nodes and next-hop candidate nodes in each walking step. Based on CNARW, we further develop two efficient “unbiased sampling” schemes, and we also design two variant algorithms which can reduce sampling cost and speed up the convergence. Experimental results on real-world network datasets show that our approach converges remarkably faster than the state-of-the-art random walk sampling algorithms; and to achieve the same estimation accuracy, our approach reduces the query cost significantly. Last, we use two case studies to demonstrate the effectiveness of our sampling framework in solving large-scale graph analysis tasks.},
  archive      = {J_TKDE},
  author       = {Rui Wang and Yongkun Li and Shuai Lin and WeiJie Wu and Hong Xie and Yinlong Xu and John C.S. Lui},
  doi          = {10.1109/TKDE.2022.3150427},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4570-4584},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Common neighbors matter: Fast random walk sampling with common neighbor awareness},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CenGCN: Centralized convolutional networks with vertex
imbalance for scale-free graphs. <em>TKDE</em>, <em>35</em>(5),
4555–4569. (<a href="https://doi.org/10.1109/TKDE.2022.3149888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have achieved impressive performance in a wide variety of areas, attracting considerable attention. The core step of GCNs is the information-passing framework that considers all information from neighbors to the central vertex to be equally important. Such equal importance, however, is inadequate for scale-free networks, where hub vertices propagate more dominant information due to vertex imbalance. In this paper, we propose a novel centrality-based framework named CenGCN to address the inequality of information. This framework first quantifies the similarity between hub vertices and their neighbors by label propagation with hub vertices. Based on this similarity and centrality indices, the framework transforms the graph by increasing or decreasing the weights of edges connecting hub vertices and adding self-connections to vertices. In each non-output layer of the GCN, this framework uses a hub attention mechanism to assign new weights to connected non-hub vertices based on their common information with hub vertices. We present two variants CenGCN_D and CenGCN_E, based on degree centrality and eigenvector centrality, respectively. We also conduct comprehensive experiments, including vertex classification, link prediction, vertex clustering, and network visualization. The results demonstrate that the two variants significantly outperform state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Feng Xia and Lei Wang and Tao Tang and Xin Chen and Xiangjie Kong and Giles Oatley and Irwin King},
  doi          = {10.1109/TKDE.2022.3149888},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4555-4569},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CenGCN: Centralized convolutional networks with vertex imbalance for scale-free graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CCGL: Contrastive cascade graph learning. <em>TKDE</em>,
<em>35</em>(5), 4539–4554. (<a
href="https://doi.org/10.1109/TKDE.2022.3151829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised learning, while prevalent for information cascade modeling, often requires abundant labeled data in training, and the trained model is not easy to generalize across tasks and datasets. It often learns task-specific representations, which can easily result in overfitting for downstream tasks. Recently, self-supervised learning is designed to alleviate these two fundamental issues in linguistic and visual tasks. However, its direct applicability for information cascade modeling, especially graph cascade related tasks, remains underexplored. In this work, we present Contrastive Cascade Graph Learning ( CCGL ), a novel framework for information cascade graph learning in a contrastive , self-supervised , and task-agnostic way. In particular, CCGL first designs an effective data augmentation strategy to capture variation and uncertainty by simulating the information diffusion in graphs. Second, it learns a generic model for graph cascade tasks via self-supervised contrastive pre-training using both unlabeled and labeled data. Third, CCGL learns a task-specific cascade model via fine-tuning using labeled data. Finally, to make the model transferable across datasets and cascade applications, CCGL further enhances the model via distillation using a teacher-student architecture. We demonstrate that CCGL significantly outperforms its supervised and semi-supervised counterparts for several downstream tasks.},
  archive      = {J_TKDE},
  author       = {Xovee Xu and Fan Zhou and Kunpeng Zhang and Siyuan Liu},
  doi          = {10.1109/TKDE.2022.3151829},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4539-4554},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CCGL: Contrastive cascade graph learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Category alignment adversarial learning for cross-modal
retrieval. <em>TKDE</em>, <em>35</em>(5), 4527–4538. (<a
href="https://doi.org/10.1109/TKDE.2022.3153962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval aims to retrieve one semantically similar media from multiple media types based on queries entered by another type of media. An intuitive idea is to map different media data into a common space and then directly measure content similarity between different types of data. In this paper, we present a novel method, called Category Alignment Adversarial Learning (CAAL) for cross-modal retrieval. It aims to find a common representation space supervised by category information, in which the samples from different modalities can be compared directly. Specifically, CAAL first employs two parallel encoders to generate common representations for image and text features respectively. Furthermore, we employ two parallel GANs with category information to generate fake image and text features which next will be utilized with already generated embedding to reconstruct the common representation. At last, two joint discriminators are utilized to reduce the gap between the mapping of the first stage and the embedding of the second stage. Comprehensive experimental results on four widely-used benchmark datasets demonstrate the superior performance of our proposed method compared with the state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Shiyuan He and Weiyang Wang and Zheng Wang and Xing Xu and Yang Yang and Xiaoming Wang and Heng Tao Shen},
  doi          = {10.1109/TKDE.2022.3153962},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4527-4538},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Category alignment adversarial learning for cross-modal retrieval},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond the limits of predictability in human mobility
prediction: Context-transition predictability. <em>TKDE</em>,
<em>35</em>(5), 4514–4526. (<a
href="https://doi.org/10.1109/TKDE.2022.3148300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban human mobility prediction is forecasting how people move in cities. It is crucial for many smart city applications including route optimization, preparing for dramatic shifts in modes of transportation, or mitigating the epidemic spread of viruses such as COVID-19. Previous research propose the maximum predictability to derive the theoretical limits of accuracy that any predictive algorithm could achieve on predicting urban human mobility. However, existing maximum predictability only considers the sequential patterns of human movements and neglects the contextual information such as the time or the types of places that people visit, which plays an important role in predicting one&#39;s next location. In this paper, we propose new theoretical limits of predictability, namely Context-Transition Predictability, which not only captures the sequential patterns of human mobility, but also considers the contextual information of human behavior. We compare our Context-Transition Predictability with other kinds of predictability and find that it is larger than these existing ones. We also show that our proposed Context-Transition Predictability provides us a better guidance on which predictive algorithm to be used for forecasting the next location when considering the contextual information. Source code is at https://github.com/zcfinal/ContextTransitionPredictability .},
  archive      = {J_TKDE},
  author       = {Chao Zhang and Kai Zhao and Meng Chen},
  doi          = {10.1109/TKDE.2022.3148300},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4514-4526},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Beyond the limits of predictability in human mobility prediction: Context-transition predictability},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial attack framework on graph embedding models with
limited knowledge. <em>TKDE</em>, <em>35</em>(5), 4499–4513. (<a
href="https://doi.org/10.1109/TKDE.2022.3153060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the success of the graph embedding model in both academic and industrial areas, the robustness of graph embeddings against adversarial attacks inevitably becomes a crucial problem in graph learning. Existing works usually perform the attack in a white-box fashion: they need to access the predictions/labels to construct their adversarial losses. However, the inaccessibility of predictions/labels makes the white-box attack impractical for a real graph learning system. This paper promotes current frameworks in a more general and flexible sense – we consider the ability of various types of graph embedding models to remain resilient against black-box driven attacks. We investigate the theoretical connection between graph signal processing and graph embedding models, and formulate the graph embedding model as a general graph signal process with a corresponding graph filter. Therefore, we design a generalized adversarial attack framework: GF-Attack . Without accessing any labels and model predictions, GF-Attack can perform the attack directly on the graph filter in a black-box fashion. We further prove that GF-Attack can perform an effective attack without assumption on the number of layers/window-size of graph embedding models. To validate the generalization of GF-Attack , we construct GF-Attack on five popular graph embedding models. Extensive experiments validate the effectiveness of GF-Attack on several benchmark datasets.},
  archive      = {J_TKDE},
  author       = {Heng Chang and Yu Rong and Tingyang Xu and Wenbing Huang and Honglei Zhang and Peng Cui and Xin Wang and Wenwu Zhu and Junzhou Huang},
  doi          = {10.1109/TKDE.2022.3153060},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4499-4513},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adversarial attack framework on graph embedding models with limited knowledge},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating graph similarity search via efficient GED
computation. <em>TKDE</em>, <em>35</em>(5), 4485–4498. (<a
href="https://doi.org/10.1109/TKDE.2022.3153523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing the graph edit distance (GED) between graphs is the core operation in graph similarity search. Recent studies suggest that the existing index structures are ineffective in reducing the overall processing time of graph similarity search, and that directly verifying the GED between the query graph and every data graph in the database is still the best option. The state-of-the-art algorithm for GED verification is the recently proposed ${{{\mathsf {AStar}}} \text{-}{\mathtt {LSa}} }$ . However, ${{{\mathsf {AStar}}} \text{-}{\mathtt {LSa}} }$ may consume an extremely large amount of main memory or even run out-of-memory, when the graphs become larger and/or the GED threshold becomes larger. In this paper, we aim to improve the efficiency of GED verification and simultaneously lower the main memory consumption. To achieve that, we propose a new estimation for the lower bounds of partial mappings between graphs. We formally prove that our new lower bound is tighter than the one used in ${{{\mathsf {AStar}}} \text{-}{\mathtt {LSa}} }$ . Moreover, we also propose efficient algorithms to compute the lower bounds, as well as optimization techniques to improve the efficiency. Empirical studies on real datasets demonstrate that our newly proposed algorithm ${{{\mathsf {AStar}}} \text{-}{\mathtt {BMao}} }$ runs faster, and at the same time consumes much less main memory, than ${{{\mathsf {AStar}}} \text{-}{\mathtt {LSa}} }$ .},
  archive      = {J_TKDE},
  author       = {Lijun Chang and Xing Feng and Kai Yao and Lu Qin and Wenjie Zhang},
  doi          = {10.1109/TKDE.2022.3153523},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4485-4498},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Accelerating graph similarity search via efficient GED computation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on the high-performance computation of persistent
homology. <em>TKDE</em>, <em>35</em>(5), 4466–4484. (<a
href="https://doi.org/10.1109/TKDE.2022.3147070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent Homology is a computational method of data mining in the field of Topological Data Analysis. Large-scale data analysis with persistent homology is computationally expensive and memory intensive. The performance of persistent homology has been rigorously studied to optimize data encoding and intermediate data structures for high-performance computation. This paper provides an application-centric survey of the High-Performance Computation of Persistent Homology. Computational topology concepts are reviewed and detailed for a broad data science and engineering audience.},
  archive      = {J_TKDE},
  author       = {Nicholas O. Malott and Shangye Chen and Philip A. Wilsey},
  doi          = {10.1109/TKDE.2022.3147070},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4466-4484},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on the high-performance computation of persistent homology},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on deep reinforcement learning for data processing
and analytics. <em>TKDE</em>, <em>35</em>(5), 4446–4465. (<a
href="https://doi.org/10.1109/TKDE.2022.3155196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data processing and analytics are fundamental and pervasive. Algorithms play a vital role in data processing and analytics where many algorithm designs have incorporated heuristics and general rules from human knowledge and experience to improve their effectiveness. Recently, reinforcement learning, deep reinforcement learning (DRL) in particular, is increasingly explored and exploited in many areas because it can learn better strategies in complicated environments it is interacting with than statically designed algorithms. Motivated by this trend, we provide a comprehensive review of recent works focusing on utilizing DRL to improve data processing and analytics. First, we present an introduction to key concepts, theories, and methods in DRL. Next, we discuss DRL deployment on database systems, facilitating data processing and analytics in various aspects, including data organization, scheduling, tuning, and indexing. Then, we survey the application of DRL in data processing and analytics, ranging from data preparation, natural language processing to healthcare, fintech, etc. Finally, we discuss important open challenges and future research directions of using DRL in data processing and analytics.},
  archive      = {J_TKDE},
  author       = {Qingpeng Cai and Can Cui and Yiyuan Xiong and Wei Wang and Zhongle Xie and Meihui Zhang},
  doi          = {10.1109/TKDE.2022.3155196},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4446-4465},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on deep reinforcement learning for data processing and analytics},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on accuracy-oriented neural recommendation: From
collaborative filtering to information-rich recommendation.
<em>TKDE</em>, <em>35</em>(5), 4425–4445. (<a
href="https://doi.org/10.1109/TKDE.2022.3145690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influenced by the great success of deep learning in computer vision and language understanding, research in recommendation has shifted to inventing new recommender models based on neural networks. In recent years, we have witnessed significant progress in developing neural recommender models, which generalize and surpass traditional recommender models owing to the strong representation power of neural networks. In this survey paper, we conduct a systematic review on neural recommender models from the perspective of recommendation modeling with the accuracy goal, aiming to summarize this field to facilitate researchers and practitioners working on recommender systems. Specifically, based on the data usage during recommendation modeling, we divide the work into collaborative filtering and information-rich recommendation: 1) collaborative filtering , which leverages the key source of user-item interaction data; 2) content enriched recommendation , which additionally utilizes the side information associated with users and items, like user profile and item knowledge graph; and 3) temporal/sequential recommendation , which accounts for the contextual information associated with an interaction, such as time, location, and the past interactions. After reviewing representative work for each type, we finally discuss some promising directions in this field.},
  archive      = {J_TKDE},
  author       = {Le Wu and Xiangnan He and Xiang Wang and Kun Zhang and Meng Wang},
  doi          = {10.1109/TKDE.2022.3145690},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4425-4445},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on accuracy-oriented neural recommendation: From collaborative filtering to information-rich recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sketch framework for approximate data stream processing in
sliding windows. <em>TKDE</em>, <em>35</em>(5), 4411–4424. (<a
href="https://doi.org/10.1109/TKDE.2022.3151140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stream processing has become a hot issue in recent years. There are three fundamental stream processing tasks: membership query, frequency query, and Top-K query. While most existing solutions address these queries in fixed windows, this paper focuses on a more challenging task: answering these queries in sliding windows. While most existing solutions address different kinds of queries by using different algorithms, this paper focuses on a generic framework. In this paper, we propose a generic framework, namely the Sliding sketch, which can be applied to many existing solutions for the above three queries, and enable them to support queries in sliding windows. We apply our framework to five state-of-the-art sketches for the above three kinds of queries. Theoretical analysis and extensive experimental results show that the accuracy of existing sketches that do not support sliding windows becomes much higher than the corresponding prior art after using our framework. We released all the source code at Github.},
  archive      = {J_TKDE},
  author       = {Xiangyang Gou and Yinda Zhang and Zhoujing Hu and Long He and Ke Wang and Xilai Liu and Tong Yang and Yi Wang and Bin Cui},
  doi          = {10.1109/TKDE.2022.3151140},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4411-4424},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A sketch framework for approximate data stream processing in sliding windows},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A scalable query-aware enormous database generator for
database evaluation. <em>TKDE</em>, <em>35</em>(5), 4395–4410. (<a
href="https://doi.org/10.1109/TKDE.2022.3153651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query-aware synthetic data generation is an essential and highly challenging task, important for database management system (DBMS) testing, database application testing and application-driven benchmarking. Prior studies on query-aware data generation suffer common problems of limited parallelization, poor scalability, and excessive memory consumption, making these systems unsatisfactory to terabyte scale data generation. In order to fill the gap between the existing data generation techniques and the emerging demands of enormous query-aware test databases, we design and implement a new data generator, called Touchstone . Touchstone adopts the random sampling algorithm instantiating query parameters and the new data generation schema generating the test database, to achieve fully parallel data generation, linear scalability and austere memory consumption. It has full support of outer joins as well as non-equi-joins for application-oriented data generation. Our experimental results show that Touchstone consistently outperforms the state-of-the-art solution on TPC-H workload by a 1000× speedup without sacrificing simulation fidelity.},
  archive      = {J_TKDE},
  author       = {Qingshuai Wang and Yuming Li and Rong Zhang and Ke Shu and Zhenjie Zhang and Aoying Zhou},
  doi          = {10.1109/TKDE.2022.3153651},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4395-4410},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A scalable query-aware enormous database generator for database evaluation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review on ontology modularization techniques - a
multi-dimensional perspective. <em>TKDE</em>, <em>35</em>(5), 4376–4394.
(<a href="https://doi.org/10.1109/TKDE.2022.3152928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past two decades, the use of ontologies has grown accompanied by a diversity in ontological representations and applications to more comprehensive domains. Knowledge engineers have found it expeditious to break down large (monolithic) ontologies to work with smaller fragments. Ontology modularization is the process of extracting a fragment, or ”module”, from an ontology, based on predefined requirements. Due to both the diversity in ontological representations and motivations for modularizing, the body of research on ontology modularization techniques has become extremely large and may be intimidating to the novice ontology researcher. The objective of the paper is to present a comprehensive, albeit high-level, review of ontology modularization techniques. A systematic literature review covering January 1st 2000 to July 31st 2020 was performed to find and classify papers on ontology modularization techniques. The techniques exhibiting certain properties with respect to several features were assessed, and the modularization techniques were classified with a multi-dimensional perspective. The classifications are intended to guide one to a suitable modularization process in accordance with the requirements. The limitations of ontology modularization techniques are highlighted in the conclusion, and characteristics of a desirable framework for an ontology representation that would be best-suited for modularization are presented.},
  archive      = {J_TKDE},
  author       = {Andrew LeClair and Alicia Marinache and Haya El Ghalayini and Wendy MacCaull and Ridha Khedri},
  doi          = {10.1109/TKDE.2022.3152928},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4376-4394},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A review on ontology modularization techniques - a multi-dimensional perspective},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review on deep neural networks for ICD coding.
<em>TKDE</em>, <em>35</em>(5), 4357–4375. (<a
href="https://doi.org/10.1109/TKDE.2022.3148267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The International Classification of Diseases (ICD) is a standard for categorizing physical conditions, which has been widely used for analyzing clinical data and monitoring health issues. Manual ICD coding takes a long time and is vulnerable to errors, so researchers pay more and more attention to the application of deep neural networks in ICD automatic coding. However, there is still no comprehensive review of these studies and prospects for further research. This paper is not limited to the study of deep neural networks, but gives a formal definition of ICD coding problems, and then systematically reviews the existing literature on how to design deep neural networks to address the four major challenges of ICD coding tasks. This paper also summarizes the public data sets and future research directions, to provide guidance for the research of ICD coding in medical field.},
  archive      = {J_TKDE},
  author       = {Fei Teng and Yiming Liu and Tianrui Li and Yi Zhang and Shuangqing Li and Yue Zhao},
  doi          = {10.1109/TKDE.2022.3148267},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {5},
  pages        = {4357-4375},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A review on deep neural networks for ICD coding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Urban traffic light control via active multi-agent
communication and supply-demand modeling. <em>TKDE</em>, <em>35</em>(4),
4346–4356. (<a href="https://doi.org/10.1109/TKDE.2021.3130258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban traffic light control is an important and challenging real-world problem. By regarding intersections as agents, most of the reinforcement learning-based methods generate agents’ actions independently. They can cause action conflict and result in overflow or road resource waste in adjacent intersections. Recently, some collaborative methods have alleviated the above problems by extending the observable surroundings of agents, which can be considered inactive cross-agent communication methods. However, when agents act synchronously in these works, the perceived action value is biased, and the information exchanged is insufficient. In this work, we first propose a novel Multi-agent Communication and Action Rectification (MaCAR) framework. It enables active communication between agents by considering the impact of synchronous actions of agents. Another fundamental problem of traffic light control is the balance between traffic demand and road supply capacity. To fully describe the relation between traffic demand and road supply capacity (Supply-Demand modeling, SD), we further model and forecast the Supply-Demand relation to facilitating the effectiveness of the model’s action. The experiments show that our model outperforms state-of-the-art methods on both synthetic and real-world datasets. Combining the SD with MaCAR, SD-MaCAR can further boost the traffic light control performance even in traffic accident scenarios.},
  archive      = {J_TKDE},
  author       = {Xin Guo and Zhengxu Yu and Pengfei Wang and Zhongming Jin and Jianqiang Huang and Deng Cai and Xiaofei He and Xian-Sheng Hua},
  doi          = {10.1109/TKDE.2021.3130258},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4346-4356},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Urban traffic light control via active multi-agent communication and supply-demand modeling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two end-to-end quantum-inspired deep neural networks for
text classification. <em>TKDE</em>, <em>35</em>(4), 4335–4345. (<a
href="https://doi.org/10.1109/TKDE.2021.3130598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In linguistics, the uncertainty of context due to polysemy is widespread, which attracts much attention. Quantum-inspired complex word embedding based on Hilbert space plays an important role in natural language processing (NLP), which fully leverages the similarity between quantum states and word tokens. A word containing multiple meanings could correspond to a single quantum particle which may exist in several possible states, and a sentence could be analogous to the quantum system where particles interfere with each other. Motivated by quantum-inspired complex word embedding, interpretable complex-valued word embedding (ICWE) is proposed to design two end-to-end quantum-inspired deep neural networks (ICWE-QNN and CICWE-QNN representing convolutional complex-valued neural network based on ICWE) for binary text classification. They have the proven feasibility and effectiveness in the application of NLP and can solve the problem of text information loss in CE-Mix [1] model caused by neglecting the important linguistic features of text, since linguistic feature extraction is presented in our model with deep learning algorithms, in which gated recurrent unit (GRU) extracts the sequence information of sentences, attention mechanism makes the model focus on important words in sentences and convolutional layer captures the local features of projected matrix. The model ICWE-QNN can avoid random combination of word tokens and CICWE-QNN fully considers textual features of the projected matrix. Experiments conducted on five benchmarking classification datasets demonstrate our proposed models have higher accuracy than the compared traditional models including CaptionRep BOW, DictRep BOW and Paragram-Phrase, and they also have great performance on F1-score. Eespecially, CICWE-QNN model has higher accuracy than the quantum-inspired model CE-Mix as well for four datasets including SST, SUBJ, CR and MPQA. It is a meaningful and effictive exploration to design quantum-inspired deep neural networks to promote the performance of text classification.},
  archive      = {J_TKDE},
  author       = {Jinjing Shi and Zhenhuan Li and Wei Lai and Fangfang Li and Ronghua Shi and Yanyan Feng and Shichao Zhang},
  doi          = {10.1109/TKDE.2021.3130598},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4335-4345},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Two end-to-end quantum-inspired deep neural networks for text classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards robust knowledge graph embedding via multi-task
reinforcement learning. <em>TKDE</em>, <em>35</em>(4), 4321–4334. (<a
href="https://doi.org/10.1109/TKDE.2021.3127951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Knowledge graphs (KGs) have been playing a pivotal role in AI-related applications. Despite the large sizes, existing KGs are far from complete and comprehensive. In order to continuously enrich KGs, automatic knowledge construction and update mechanisms are usually utilized, which inevitably bring in plenty of noise. However, most existing knowledge graph embedding (KGE) methods assume that all the triple facts in KGs are correct, and project both entities and relations into a low-dimensional space without considering noise and knowledge conflicts. This will lead to low-quality and unreliable representations of KGs. To this end, in this paper, we propose a general multi-task reinforcement learning framework, which can greatly alleviate the noisy data problem. In our framework, we exploit reinforcement learning for choosing high-quality knowledge triples while filtering out the noisy ones. Also, in order to take full advantage of the correlations among semantically similar relations, the triple selection processes of similar relations are trained in a collective way with multi-task learning. Moreover, we extend popular KGE models TransE, DistMult, ConvE and RotatE with the proposed framework. Finally, the experimental validation shows that our approach is able to enhance existing KGE models and can provide more robust representations of KGs in noisy scenarios.},
  archive      = {J_TKDE},
  author       = {Zhao Zhang and Fuzhen Zhuang and Hengshu Zhu and Chao Li and Hui Xiong and Qing He and Yongjun Xu},
  doi          = {10.1109/TKDE.2021.3127951},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4321-4334},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards robust knowledge graph embedding via multi-task reinforcement learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards improving embedding based models of social network
alignment via pseudo anchors. <em>TKDE</em>, <em>35</em>(4), 4307–4320.
(<a href="https://doi.org/10.1109/TKDE.2021.3127585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social network alignment aims at aligning person identities across social networks. Embedding based models have been shown effective for the alignment where the structural proximity preserving objective is typically adopted for the model training. With the observation that “overly-close” user embeddings are unavoidable for such models causing alignment inaccuracy, we propose a novel learning framework which tries to enforce the resulting embeddings to be more widely apart among the users via the introduction of carefully implanted pseudo anchors. We further proposed a meta-learning algorithm to guide the updating of the pseudo anchor embeddings during the learning process. The proposed intervention via the use of pseudo anchors and meta-learning allows the learning framework to be applicable to a wide spectrum of network alignment methods. We have incorporated the proposed learning framework into several state-of-the-art models. Our experimental results demonstrate its efficacy where the methods with the pseudo anchors implanted can outperform their counterparts without pseudo anchors by a fairly large margin, especially when there only exist very few labeled anchors.},
  archive      = {J_TKDE},
  author       = {Zihan Yan and Li Liu and Xin Li and William K. Cheung and Youmin Zhang and Qun Liu and Guoyin Wang},
  doi          = {10.1109/TKDE.2021.3127585},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4307-4320},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards improving embedding based models of social network alignment via pseudo anchors},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time series anomaly detection with adversarial
reconstruction networks. <em>TKDE</em>, <em>35</em>(4), 4293–4306. (<a
href="https://doi.org/10.1109/TKDE.2021.3140058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data naturally exist in many domains including medical data analysis, infrastructure sensor monitoring, and motion tracking. However, a very small portion of anomalous time series can be observed, comparing to the whole data. Most existing approaches are based on the supervised classification model requiring representative labels for anomaly class(es), which is challenging in real-world problems. So can we learn how to detect anomalous time ticks in an effective yet efficient way, given mostly normal time series data? Therefore, we propose an unsupervised reconstruction model named BeatGAN which learns to detect anomalies based on normal data, or data which majority of samples are normal. BeatGAN provides a framework to adversarially learn to reconstruct, which can cooperate with both 1-d CNN and RNN. Rarely observed anomalies can result in larger reconstruction errors, which are then detected based on extreme value theory. Moreover, data augmentation with dynamic time warping regularizes reconstruction and provides robustness. In the experiments, effectiveness and sensitivity are studied in both synthetic data and various real-world time series. BeatGAN achieves better accuracy and fast inference.},
  archive      = {J_TKDE},
  author       = {Shenghua Liu and Bin Zhou and Quan Ding and Bryan Hooi and Zhengbo Zhang and Huawei Shen and Xueqi Cheng},
  doi          = {10.1109/TKDE.2021.3140058},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4293-4306},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Time series anomaly detection with adversarial reconstruction networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The infinity mirror test for graph models. <em>TKDE</em>,
<em>35</em>(4), 4281–4292. (<a
href="https://doi.org/10.1109/TKDE.2022.3140252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph models, like other machine learning models, have implicit and explicit biases built-in, which often impact performance in nontrivial ways. The model’s faithfulness is often measured by comparing the newly generated graph against the source graph using any number of graph properties. Therefore, differences in the size or topology of the generated graph indicate a loss in the model. Yet, in many systems, errors encoded in loss functions are subtle and not well understood. In the present work, we introduce the Infinity Mirror test for analyzing the robustness of graph models. This straightforward stress test works by repeatedly fitting a model to its outputs. A hypothetically perfect graph model would have no deviation from the source graph; however, a model’s implicit biases and assumptions are exaggerated by the Infinity Mirror test, exposing potential previously obscured issues. Through an analysis of thousands of experiments on synthetic and real-world graphs, we show that several conventional graph models degenerate in exciting and informative ways. We believe that the observed degenerative patterns are clues to the future development of better graph models.},
  archive      = {J_TKDE},
  author       = {Satyaki Sikdar and Daniel Gonzalez Cedre and Trenton W. Ford and Tim Weninger},
  doi          = {10.1109/TKDE.2022.3140252},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4281-4292},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {The infinity mirror test for graph models},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor-train decomposition in the presence of
interval-valued data. <em>TKDE</em>, <em>35</em>(4), 4267–4280. (<a
href="https://doi.org/10.1109/TKDE.2021.3135715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many fields of computer science, tensor decomposition techniques are increasingly being adopted as the core of many applications that rely on multi-dimensional datasets for implementing knowledge discovery tasks. Unfortunately, a major shortcoming of state-of-the-art tensor analyses is that, despite their effectiveness when the data is certain, these operations become difficult to apply, or altogether inapplicable, in the presence of uncertainty in the data, a circumstance common to many real-world scenarios. In this paper we propose a way to address this issue by extending the known Tensor-Train technique for tensor factorization in order to deal with uncertain data, here modeled as intervals. Working with interval-valued data, however, presents numerous challenges, since many algebraic operations that form the building blocks of the factorization process, as well as the properties that make these procedures useful for knowledge discovery, cannot be easily extended from their scalar counterparts, and often require some approximation (including, though it is not only the case, for keeping computational costs manageable). These challenges notwithstanding, our proposed techniques proved to be reasonably effective, and are supported by a thorough experimental validation.},
  archive      = {J_TKDE},
  author       = {Francesco Di Mauro and K Selçuk Candan and Maria Luisa Sapino},
  doi          = {10.1109/TKDE.2021.3135715},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4267-4280},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Tensor-train decomposition in the presence of interval-valued data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Similar sports play retrieval with deep reinforcement
learning. <em>TKDE</em>, <em>35</em>(4), 4253–4266. (<a
href="https://doi.org/10.1109/TKDE.2021.3136881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of commercial tracking systems, sports data is being generated at an unprecedented speed and the interest in sports play retrieval has grown dramatically as well, where a play corresponds to a fragment of a game. Existing solutions for similar play retrieval usually assume that a database of plays are materialized, which, however, is not well aligned with the practice that data is stored in units of game. In this paper, we propose to search for similar plays directly from a database of games. We tackle three challenges of the task, namely (1) how to measure the similarity between two plays, (2) how to efficiently find a similar play to a query play within a game, and (3) how to efficiently find a similar play within a database of many games. For the first challenge, we propose a deep learning approach called play2vec to learn the representations of sports plays. play2vec is robust against noise and runs in linear time. For the second challenge, we develop a suite of algorithms including two based on reinforcement learning, which use learned policies for deciding where to split a game to generate candidate plays. For the third challenge, we develop a method called ScoreSearch based on deep metric learning, which is able to prune games from being searched for better efficiency. We conduct experiments on real-world soccer match data to evaluate the techniques developed in this paper.},
  archive      = {J_TKDE},
  author       = {Zheng Wang and Cheng Long and Gao Cong},
  doi          = {10.1109/TKDE.2021.3136881},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4253-4266},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Similar sports play retrieval with deep reinforcement learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ShieldDB: An encrypted document database with padding
countermeasures. <em>TKDE</em>, <em>35</em>(4), 4236–4252. (<a
href="https://doi.org/10.1109/TKDE.2021.3126607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud storage systems have seen a growing number of clients due to the fact that more and more businesses and governments are shifting away from in-house data servers and seeking cost-effective and ease-of-access solutions. However, the security of cloud storage is underestimated in current practice, which resulted in many large-scale data breaches. To change the status quo, this paper presents the design of ShieldDB, an encrypted document database. ShieldDB adapts the searchable encryption technique to preserve the search functionality over encrypted documents without having much impact on its scalability. However, merely realising such a theoretical primitive suffers from real-world threats, where a knowledgeable adversary can exploit the leakage (aka access pattern to the database) to break the claimed protection on data confidentiality. To address this challenge in practical deployment, ShieldDB is designed with tailored padding countermeasures. Unlike prior works, we target a more realistic adversarial model, where the database gets updated continuously, and the adversary can monitor it at an (or multiple) arbitrary time interval(s). ShieldDB’s padding strategies ensure that the access pattern to the database is obfuscated all the time. We present a full-fledged implementation of ShieldDB and conduct intensive evaluations on Azure Cloud.},
  archive      = {J_TKDE},
  author       = {Viet Vo and Xingliang Yuan and Shi-Feng Sun and Joseph K. Liu and Surya Nepal and Cong Wang},
  doi          = {10.1109/TKDE.2021.3126607},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4236-4252},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ShieldDB: An encrypted document database with padding countermeasures},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning on graphs: Contrastive, generative,
or predictive. <em>TKDE</em>, <em>35</em>(4), 4216–4235. (<a
href="https://doi.org/10.1109/TKDE.2021.3131584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning on graphs has recently achieved remarkable success on a variety of tasks, while such success relies heavily on the massive and carefully labeled data. However, precise annotations are generally very expensive and time-consuming. To address this problem, self-supervised learning (SSL) is emerging as a new paradigm for extracting informative knowledge through well-designed pretext tasks without relying on manual labels. In this survey, we extend the concept of SSL, which first emerged in the fields of computer vision and natural language processing, to present a timely and comprehensive review of existing SSL techniques for graph data. Specifically, we divide existing graph SSL methods into three categories: contrastive, generative, and predictive. More importantly, unlike other surveys that only provide a high-level description of published research, we present an additional mathematical summary of existing works in a unified framework. Furthermore, to facilitate methodological development and empirical comparisons, we also summarize the commonly used datasets, evaluation metrics, downstream tasks, open-source implementations, and experimental study of various algorithms. Finally, we discuss the technical challenges and potential future directions for improving graph self-supervised learning. Latest advances in graph SSL are summarized in a GitHub repository https://github.com/LirongWu/awesome-graph-self-supervised-learning .},
  archive      = {J_TKDE},
  author       = {Lirong Wu and Haitao Lin and Cheng Tan and Zhangyang Gao and Stan Z. Li},
  doi          = {10.1109/TKDE.2021.3131584},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4216-4235},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Self-supervised learning on graphs: Contrastive, generative, or predictive},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised graph representation learning via topology
transformations. <em>TKDE</em>, <em>35</em>(4), 4202–4215. (<a
href="https://doi.org/10.1109/TKDE.2021.3133439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the Topology Transformation Equivariant Representation learning, a general paradigm of self-supervised learning for node representations of graph data to enable the wide applicability of Graph Convolutional Neural Networks (GCNNs). We formalize the proposed model from an information-theoretic perspective, by maximizing the mutual information between topology transformations and node representations before and after the transformations. We derive that maximizing such mutual information can be relaxed to minimizing the cross entropy between the applied topology transformation and its estimation from node representations. In particular, we seek to sample a subset of node pairs from the original graph and flip the edge connectivity between each pair to transform the graph topology. Then, we self-train a representation encoder to learn node representations by reconstructing the topology transformations from the feature representations of the original and transformed graphs. In experiments, we apply the proposed model to the downstream node classification, graph classification and link prediction tasks, and results show that the proposed method outperforms the state-of-the-art unsupervised approaches.},
  archive      = {J_TKDE},
  author       = {Xiang Gao and Wei Hu and Guo-Jun Qi},
  doi          = {10.1109/TKDE.2021.3133439},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4202-4215},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Self-supervised graph representation learning via topology transformations},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Region attention enhanced unsupervised cross-domain facial
emotion recognition. <em>TKDE</em>, <em>35</em>(4), 4190–4201. (<a
href="https://doi.org/10.1109/TKDE.2021.3136606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual emotion recognition from facial expressions easily suffers barrier problems of varying brightness, head pose change, various image scales when the recognition is performed in different domains. Therefore, it is required to erase such domain barriers. Considering that the human expresses their emotions always relying on the muscle motion near five sense organs of face, local features around them are typically crucial. In this paper, we propose a Region Attention eNhanced Domain Adaptation (RANDA) approach for unsupervised cross-domain facial expression recognition (FER). In RANDA, we design an unsupervised domain adaptation solution that adopts an iterative pseudo label assignment method to provide pseudo labels in the target domain, then employs adversarial learning to confuse feature representation of facial expressions in the source and target domains. Furthermore, a facial landmark guided fine-grained region attention learning module is designed to enhance significant emotion features and simultaneously weaken domain discrepancy. The proposed RANDA is adopted for cross-domain emotion recognition, and extensive evaluations are performed on multiple datasets, i.e., CK+, MMI, SFEW, RAF-DB, AffectNet. Results indicate that the RANDA outperforms the state-of-the-art approaches. It provides an effective solution for the cross-domain FER.},
  archive      = {J_TKDE},
  author       = {Yanli Ji and Yuhan Hu and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TKDE.2021.3136606},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4190-4201},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Region attention enhanced unsupervised cross-domain facial emotion recognition},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recommending learning objects through attentive
heterogeneous graph convolution and operation-aware neural network.
<em>TKDE</em>, <em>35</em>(4), 4178–4189. (<a
href="https://doi.org/10.1109/TKDE.2021.3125424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive Open Online Courses (MOOCs) have received unprecedented attention, in which learners can obtain a large number of learning objects anytime and anywhere. However, the increasing information overload on MOOCs inhibits the appropriate choice of learning objects by learners, leading to a low efficiency and high dropout rates in the learning process of this human-computer interaction scenario. E-learning recommendation systems have been studied to present learning objects directly to learners, thereby relieving such problem. However, in MOOC platforms, recommendation network structures which can selectively extract implicit feature such as heterogeneous learning preference and knowledge organization of learning objects are still not comprehensively studied. To this end, we propose a learning object recommendation model based on heterogeneous learning behavior and knowledge graph. To generate a unified representation of each entity and relation, we first propose an Attentive Composition based Graph Convolutional Network (ACGCN). By introducing an attention mechanism, information is amplified when updating the representation of the heterogeneous graph, which eliminates the impact of noise and improves the robustness of the model. Then, a Dense Feature based Operation-Aware Network (DFOAN) is utilized to capture implicit and complex learners’ interactive behaviors, and to further provide a recommendation. Experimental results using two real-world datasets revealed that our proposed model has the best precision, recall, F1, and accuracy scores compared to those of several existing models.},
  archive      = {J_TKDE},
  author       = {Yifan Zhu and Qika Lin and Hao Lu and Kaize Shi and Donglei Liu and James Chambua and Shanshan Wan and Zhendong Niu},
  doi          = {10.1109/TKDE.2021.3125424},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4178-4189},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Recommending learning objects through attentive heterogeneous graph convolution and operation-aware neural network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Publishing graphs under node differential privacy.
<em>TKDE</em>, <em>35</em>(4), 4164–4177. (<a
href="https://doi.org/10.1109/TKDE.2021.3128946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy (DP) has become the de facto standard of privacy protection. For graphs, there are two widely used definitions of differential privacy, namely, edge differential privacy (edge-DP) and node differential privacy (node-DP), and node-DP is preferred when the minimal unit of interest is a node. To preserve node-DP, one can develop different methods to answer each specific graph query, or develop a graph publishing method to answer all graph queries. However, no existing works worked on such graph publishing methods. In this work, we propose two methods for publishing graphs under node-DP. One is the node-level perturbation algorithm which modifies the input graph by randomly inserting and removing nodes. The other one is the edge-level perturbation algorithm which randomly removing edges and inserting nodes. Both methods can achieve a flexible privacy guarantee by adjusting the running parameters. We conduct extensive experiments on both real-world and synthetic graphs to show the effectiveness and efficiency of proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Xun Jian and Yue Wang and Lei Chen},
  doi          = {10.1109/TKDE.2021.3128946},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4164-4177},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Publishing graphs under node differential privacy},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Private and utility enhanced recommendations with local
differential privacy and gaussian mixture model. <em>TKDE</em>,
<em>35</em>(4), 4151–4163. (<a
href="https://doi.org/10.1109/TKDE.2021.3126577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems rely heavily on behavioural and preferential data (e.g., ratings and likes) of a user to produce accurate recommendations. However, such unethical data aggregation and analytical practices of Service Providers (SP) causes privacy concerns among users. Local differential privacy (LDP) based perturbation mechanisms address this concern by adding noise to users’ data at the user-side before sending it to the SP. The SP then uses the perturbed data to perform recommendations. Although LDP protects the privacy of users from SP, it causes a substantial decline in recommendation accuracy. We propose an LDP-based Matrix Factorization (MF) with a Gaussian Mixture Model (MoG) to address this problem. The LDP perturbation mechanism, i.e., Bounded Laplace (BLP), regulates the effect of noise by confining the perturbed ratings to a predetermined domain. We derive a sufficient condition of the scale parameter for BLP to satisfy $\varepsilon$ -LDP. We use the MoG model at the SP to estimate the noise added locally to the ratings and the MF algorithm to predict missing ratings. Our LDP based recommendation system improves the predictive accuracy without violating LDP principles. We demonstrate that our method offers a substantial increase in recommendation accuracy under a strong privacy guarantee through empirical evaluations on three real-world datasets, i.e., Movielens, Libimseti and Jester.},
  archive      = {J_TKDE},
  author       = {Jeyamohan Neera and Xiaomin Chen and Nauman Aslam and Kezhi Wang and Zhan Shu},
  doi          = {10.1109/TKDE.2021.3126577},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4151-4163},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Private and utility enhanced recommendations with local differential privacy and gaussian mixture model},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy data propagation and preservation in social media: A
real-world case study. <em>TKDE</em>, <em>35</em>(4), 4137–4150. (<a
href="https://doi.org/10.1109/TKDE.2021.3137326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media has become a ubiquitous tool for spreading news, messages, and generally allowing for communication between individuals. Hence, studying how our privacy information might also spread across social media is important research. To date, many studies have used information diffusion models to simulate and then examine how information flows through social networks. But these models are theoretical, and newsworthy information may not behave in the same way as privacy information, raising the question: Are the observed phenomena indicative of real privacy propagation? To explore this question, we assembled a dataset from Twitter comprising propagated information flows for both private and normal information. We then built a graph convolutional network to trace and classify differences in the way each type of information spreads throughout the platform. The results reveal that there are indeed key differences in the diffusion processes of the two types of information. More importantly, we design privacy-preserving methods to reduce the privacy propagation in social media.},
  archive      = {J_TKDE},
  author       = {Xiangyu Hu and Tianqing Zhu and Xuemeng Zhai and Wanlei Zhou and Wei Zhao},
  doi          = {10.1109/TKDE.2021.3137326},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4137-4150},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Privacy data propagation and preservation in social media: A real-world case study},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PartitionChain: A scalable and reliable data storage
strategy for permissioned blockchain. <em>TKDE</em>, <em>35</em>(4),
4124–4136. (<a href="https://doi.org/10.1109/TKDE.2021.3136556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain, a specific distributed database which maintains a list of data records against tampering and corruption, has aroused wide interests and become a hot topic in the real world. Nevertheless, the increasingly heavy storage consumption brought by the full-replication data storage mechanism, becomes a bottleneck to the system scalability. To address this problem, a reliable storage scheme named BFT-Store (Qi et al. 2020), integrating erasure coding with Byzantine Fault Tolerance (BFT), was proposed recently. While, three critical problems are still left open: (i) The complex re-initialization process of the blockchain when the number of nodes varies; (ii) The high computational overload of downloading data; (iii) The massive communication on the network. This paper proposes a better trade-off for blockchain storage scheme termed PartitionChain which addresses the above three problems, maintaining the merits of BFT-Store. First, our scheme allows the original nodes to merely update a single aggregate signature (e.g., 320 bits) when the number of nodes varies. Using aggregate signatures as the proof of the encoded data not only saves the storage costs but also gets rid of the trusted third party. Second, the computational complexity of retrieving data by decoding, compared to BFT-Store, is greatly reduced by about $2^{18}$ times on each node. Third, the amount of transmitted data for recovering each block is reduced from $O(n)$ (assuming $n$ is the number of nodes) to $O(1)$ , by partitioning each block into smaller pieces and applying Reed-Solomon coding to each block. Furthermore, this paper also introduces a reputation ranking system where the malicious behaviors of the nodes can be detected and marked, enabling PartitionChain to check the credits of each node termly and expel the nodes with misbehavior to the specific extent. Comparing with BFT-Store, our scheme allows blockchain system to suit dynamic network with higher efficiency and scalability.},
  archive      = {J_TKDE},
  author       = {Zhengyi Du and Xiongtao Pang and Haifeng Qian},
  doi          = {10.1109/TKDE.2021.3136556},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4124-4136},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PartitionChain: A scalable and reliable data storage strategy for permissioned blockchain},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel split-join networks for shared account cross-domain
sequential recommendations. <em>TKDE</em>, <em>35</em>(4), 4106–4123.
(<a href="https://doi.org/10.1109/TKDE.2021.3130927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation is a task in which one models and uses sequential information about user behavior for recommendation purposes. We study sequential recommendation in a context in which multiple individual users share a single account (i.e., they have a shared account) and in which user behavior is available in multiple domains (i.e., recommendations are cross-domain). These two characteristics bring new challenges on top of those of the traditional sequential recommendation task. First, we need to identify the behavior associated with different users and different user roles under the same account in order to recommend the right item to the right user role at the right time. Second, we need to identify behavior in one domain that might be helpful to improve recommendations in other domains. We study shared account cross-domain sequential recommendation and propose a p arallel s plit- j oin Net work (Parallel Split-Join Network (PSJNet)), a parallel modeling network to address the two challenges above. We use “split” to address the challenge raised by shared accounts; PSJNet learns role-specific representations and uses a gating mechanism to filter out, from mixed user behavior, information of user roles that might be useful for another domain. In addition, “join” is used to address the challenge raised by the cross-domain setting; PSJNet learns cross-domain representations by combining the information from “split” and then transforms it to another domain. We present two variants of PSJNet: PSJNet-I and PSJNet-II. PSJNet-I is a “split-by-join” framework that splits the mixed representations to get role-specific representations and joins them to obtain cross-domain representations at each timestamp simultaneously. PSJNet-II is a “split-and-join” framework that first splits role-specific representations at each timestamp, and then the representations from all timestamps and all roles are joined to obtain cross-domain representations. We concatenate the in-domain and cross-domain representations to compute a recommendation score for each item. Both PSJNet-I and PSJNet-II can simultaneously generate recommendations for two domains where user behavior in two domains is synchronously shared at each timestamp. We use two datasets to assess the effectiveness of PSJNet. The first dataset is a simulated shared account cross-domain sequential recommendation dataset obtained by randomly merging the Amazon logs from different users in the movie and book domains. The second dataset is a real-world shared account cross-domain sequential recommendation dataset built from smart TV watching logs of a commercial organization. Our experimental results demonstrate that PSJNet outperforms state-of-the-art sequential recommendation baselines in terms of MRR and Recall.},
  archive      = {J_TKDE},
  author       = {Wenchao Sun and Muyang Ma and Pengjie Ren and Yujie Lin and Zhumin Chen and Zhaochun Ren and Jun Ma and Maarten de Rijke},
  doi          = {10.1109/TKDE.2021.3130927},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4106-4123},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Parallel split-join networks for shared account cross-domain sequential recommendations},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On distributed computing continuum systems. <em>TKDE</em>,
<em>35</em>(4), 4092–4105. (<a
href="https://doi.org/10.1109/TKDE.2022.3142856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents our vision on the need of developing new managing technologies to harness distributed “computing continuum” systems. These systems are concurrently executed in multiple computing tiers: Cloud, Fog, Edge and IoT. This simple idea develops manifold challenges due to the inherent complexity inherited from the underlying infrastructures of these systems. This makes inappropriate the use of current methodologies for managing Internet distributed systems, which are based on the early systems that were based on client/server architectures and were completely specified by the application software. We present a new methodology to manage distributed “computing continuum” systems. This is based on a mathematical artifact called Markov Blanket, which sets these systems in a Markovian space, more suitable to cope with their complex characteristics. Furthermore, we develop the concept of equilibrium for these systems, providing a more flexible management framework compared with the one based on thresholds, currently in use for Internet-based distributed systems. Finally, we also link the equilibrium with the development of adaptive mechanisms. However, we are aware that developing the entire methodology requires a big effort and the use of learning techniques, therefore, we finish this article with an overview of the techniques required to develop this methodology.},
  archive      = {J_TKDE},
  author       = {Schahram Dustdar and Victor Casamayor Pujol and Praveen Kumar Donta},
  doi          = {10.1109/TKDE.2022.3142856},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4092-4105},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On distributed computing continuum systems},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view tensor graph neural networks through reinforced
aggregation. <em>TKDE</em>, <em>35</em>(4), 4077–4091. (<a
href="https://doi.org/10.1109/TKDE.2022.3142179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have yielded fruitful results in learning multi-view graph data. However, it is challenging for existing GNNs to capture the potential correlation information (PCI) among the graph structure features of multiple views. It is also challenging to adaptively identify valuable neighbors for node feature fusion in different views. To this end, we propose a novel R einforced T ensor G raph N eural N etwork (RTGNN) framework to more effectively perform multi-view graph representation learning through reinforcing inter- and intra-graph aggregation. Specifically, RTGNN first uses tensor decomposition to extract the graph structure features (GSFs) of each view in the common feature space. These GSFs contain the PCI of multiple views and alleviate fusion conflicts that may be caused by differences between view feature spaces in cross-view feature fusion. Since fusing the features of all neighbor nodes may harm the features of the center node, we filter the irrelevant neighbors to improve the performance of intra-graph aggregation in each view. Concretely, a reinforcement learning (RL)-guided scheme is developed to automatically calculate the optimal filtering threshold for each view, avoiding tedious manual updates and infeasible back propagation updates. Experimental results and analysis on five datasets show that RTGNN surpasses the best multi-view graph representation baselines and achieves the maximum 14.26\% performance improvement in terms of F1. The code link is https://github.com/RingBDStack/RTGNN .},
  archive      = {J_TKDE},
  author       = {Xusheng Zhao and Qiong Dai and Jia Wu and Hao Peng and Mingsheng Liu and Xu Bai and Jianlong Tan and Senzhang Wang and Philip S. Yu},
  doi          = {10.1109/TKDE.2022.3142179},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4077-4091},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view tensor graph neural networks through reinforced aggregation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximizing the spread of effective information in social
networks. <em>TKDE</em>, <em>35</em>(4), 4062–4076. (<a
href="https://doi.org/10.1109/TKDE.2021.3138783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization through social networks has aroused tremendous interests nowadays. However, people’s various expressions or feelings about a same idea often cause ambiguity via word of mouth. Consequently, the problem of how to maximize the spread of “effective information” still remains largely open. In this paper, we consider a practical setting where ideas can deviate from their original version to invalid forms during message passing, and make the first attempt to seek a union of users that maximizes the spread of effective influence, which is formulated as an Influence Maximization with Information Variation (IMIV) problem. To this end, we model the information as a vector, and quantify the difference of two arbitrary vectors as a distance by a matching function. We further establish a process where such distance increases with the propagation and ensure the recipient whose vector distance is less than a threshold can be effectively influenced. Due to the NP-hardness of IMIV, we greedily select users that can approximately maximize the estimation of effective propagation. Especially, for networks of small scales, we derive a condition under which all the users can be effectively influenced. Our models and theoretical findings are further consolidated through extensive experiments on real-world datasets.},
  archive      = {J_TKDE},
  author       = {Haonan Zhang and Luoyi Fu and Jiaxin Ding and Feilong Tang and Yao Xiao and Xinbing Wang and Guihai Chen and Chenghu Zhou},
  doi          = {10.1109/TKDE.2021.3138783},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4062-4076},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Maximizing the spread of effective information in social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MANE: Organizational network embedding with multiplex
attentive neural networks. <em>TKDE</em>, <em>35</em>(4), 4047–4061. (<a
href="https://doi.org/10.1109/TKDE.2022.3140866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every organization has organizational networks for exchange of ideas and information. It is believed that organizational network analysis (ONA) can help the business be more effective. While considerable research efforts have been made for visualizing and analyzing relationships in organizational networks, it lacks a holistic way to model the complex social structures and rich semantic information of these networks. Indeed, employee behaviors can occur across different communication platforms, such as email and instant messaging systems, which naturally lead to the multiplex structure of organizational social networks. Meanwhile, it is also a challenge to model the impact of semantic information, such as employee attributes and organization charts, and the collaboration relationships of employees. To this end, in this paper, we propose a Multiplex Attentive Network Embedding (MANE) approach for modeling organizational social networks in a holistic way. Specifically, we first develop a multiple attributed random walk approach to jointly model multiple networks, with the integration of external work information. Then, we preserve the network structure by maximizing the probability of predicting the central node based on the surrounding context nodes. In particular, we introduce an attention mechanism to assign a weight to each context node in the training process, according to its attributed relation and structural relation with the central node by utilizing the k-core algorithm and the shortest path algorithm. In this way, the embedding results can be kept consistent with their structural relationships. Furthermore, to solve some department-level tasks, we introduce an attentive relational transition method to learn the representation of departments in the organizational networks. Finally, we evaluate the performance of MANE with extensive experiments on real-world data for three important talent management tasks, namely employee performance prediction, employee turnover prediction and department performance prediction. We also conduct a link prediction task to validate the effectiveness of employee embedding. Experimental results clearly show the effectiveness and interpretability of MANE for organizational network analysis.},
  archive      = {J_TKDE},
  author       = {Yuyang Ye and Zheng Dong and Hengshu Zhu and Tong Xu and Xin Song and Runlong Yu and Hui Xiong},
  doi          = {10.1109/TKDE.2022.3140866},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4047-4061},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MANE: Organizational network embedding with multiplex attentive neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). M2: Mixed models with preferences, popularities and
transitions for next-basket recommendation. <em>TKDE</em>,
<em>35</em>(4), 4033–4046. (<a
href="https://doi.org/10.1109/TKDE.2022.3142773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-basket recommendation considers the problem of recommending a set of items into the next basket that users will purchase as a whole. In this paper, we develop a novel mixed model with preferences, popularities and transitions ( $\mathop {\mathtt {M^2}}\limits$ ) for the next-basket recommendation. This method models three important factors in next-basket generation process: 1) users’ general preferences, 2) items’ global popularities and 3) transition patterns among items. Unlike existing recurrent neural network-based approaches, $\mathop {\mathtt {M^2}}\limits$ does not use the complicated networks to model the transitions among items, or generate embeddings for users. Instead, it has a simple encoder-decoder based approach ( $\mathop {\mathtt {ed\text{-}Trans}}\limits$ ) to better model the transition patterns among items. We compared $\mathop {\mathtt {M^2}}\limits$ with different combinations of the factors with 5 state-of-the-art next-basket recommendation methods on 4 public benchmark datasets in recommending the first, second and third next basket. Our experimental results demonstrate that $\mathop {\mathtt {M^2}}\limits$ significantly outperforms the state-of-the-art methods on all the datasets in all the tasks, with an improvement of up to 22.1\%. In addition, our ablation study demonstrates that the $\mathop {\mathtt {ed\text{-}Trans}}\limits$ is more effective than recurrent neural networks in terms of the recommendation performance. We also have a thorough discussion on various experimental protocols and evaluation metrics for next-basket recommendation evaluation.},
  archive      = {J_TKDE},
  author       = {Bo Peng and Zhiyun Ren and Srinivasan Parthasarathy and Xia Ning},
  doi          = {10.1109/TKDE.2022.3142773},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4033-4046},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {M2: Mixed models with preferences, popularities and transitions for next-basket recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Large-scale interactive recommendation with tree-structured
reinforcement learning. <em>TKDE</em>, <em>35</em>(4), 4018–4032. (<a
href="https://doi.org/10.1109/TKDE.2021.3137310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although reinforcement learning (RL) techniques are regarded as promising solutions for interactive recommender systems (IRS), such solutions still face three main challenges, namely, i) time inefficiency when handling large discrete action space in IRS, ii) inability to deal with the cold-start scenarios in IRS, iii) data inefficiency during training the RL-based methods. To tackle these challenges, we propose a generic tree-structured RL framework taking both policy-based and value-based approaches into consideration. We propose to construct a balanced tree over representations of the items, such that picking an item is formulated as seeking a suitable path from the root to a leaf node in the balanced tree, which dramatically reduces the time complexity of item recommendation. Further, for cold-start scenarios where prior information of the items is unavailable, we initialize a random balanced tree as the starting point and then refine the tree structure based on the learned item representations. Besides, we also incorporate a user modeling component to explicitly model the environment, which can be utilized in the training phase to improve data efficiency. Extensive experiments on two real-world datasets are conducted and demonstrate that our framework can achieve superior recommendation performance and provide time and data efficiency improvement over state-of-the-art methods in both warm-start and cold-start IRS scenarios.},
  archive      = {J_TKDE},
  author       = {Haokun Chen and Chenxu Zhu and Ruiming Tang and Weinan Zhang and Xiuqiang He and Yong Yu},
  doi          = {10.1109/TKDE.2021.3137310},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4018-4032},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Large-scale interactive recommendation with tree-structured reinforcement learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). K-pleased querying. <em>TKDE</em>, <em>35</em>(4),
4003–4017. (<a href="https://doi.org/10.1109/TKDE.2021.3132992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {k-Regret Querying is a well studied problem to query a dataset $D$ for a small subset $S$ of size $k$ with the minimal regret ratio for unknown utility functions. In this paper, we point out some issues in $k$ -Regret Querying, including the assumption of non-negative dataset and the lack of shift invariance. Known algorithms for $k$ -Regret Querying are limited in scope and result quality, and are based on the assumption of non-negative data. We introduce a new problem definition called $k$ -pleased querying for dealing with the shift variance issue, and propose a strategy of random sampling of the utility functions. This strategy is based on a study of the theoretical guarantee of the sampling approach. We also introduce a dimensionality reduction strategy, an improved greedy algorithm, and a study of other utility function sampling methods. All of our solutions can handle negative data. Theoretically, we derive a guarantee on the approximation attained by our sampling algorithm. Experimental results on numerous real datasets show that our proposed method is effective even with a small number of samples and small values of $k$ .},
  archive      = {J_TKDE},
  author       = {Zitong Chen and Ada Wai-Chee Fu and Cheng Long and Yang Wu},
  doi          = {10.1109/TKDE.2021.3132992},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {4003-4017},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {K-pleased querying},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable signed link prediction with signed infomax
hyperbolic graph. <em>TKDE</em>, <em>35</em>(4), 3991–4002. (<a
href="https://doi.org/10.1109/TKDE.2021.3139035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signed link prediction in social networks aims to reveal the underlying relationships (i.e., links) among users (i.e., nodes) given their existing positive and negative interactions observed. Most of the prior efforts are devoted to learning node embeddings with graph neural networks (GNNs), which preserve the signed network topology by message-passing along edges to facilitate the downstream link prediction task. Nevertheless, the existing graph-based approaches could hardly provide human-intelligible explanations for the following three questions: (1) which neighbors to aggregate, (2) which path to propagate along, and (3) which social theory to follow in the learning process. To answer the aforementioned questions, in this paper, we investigate how to reconcile the balance and status social rules with information theory and develop a unified framework, termed as Signed Infomax Hyperbolic Graph ( SIHG ). By maximizing the mutual information between edge polarities and node embeddings, one can identify the most representative neighboring nodes that support the inference of edge sign. Different from existing GNNs that could only group features of friends in the subspace, the proposed SIHG incorporates the signed attention module, which is also capable of pushing hostile users far away from each other to preserve the geometry of antagonism. The polarity of the learned edge attention maps, in turn, provides interpretations of the social theories used in each aggregation. In order to model high-order user relations and complex hierarchies, the node embeddings are projected and measured in a hyperbolic space with a lower distortion. Extensive experiments on four signed network benchmarks demonstrate that the proposed SIHG framework significantly outperforms the state-of-the-arts in signed link prediction.},
  archive      = {J_TKDE},
  author       = {Yadan Luo and Zi Huang and Hongxu Chen and Yang Yang and Hongzhi Yin and Mahsa Baktashmotlagh},
  doi          = {10.1109/TKDE.2021.3139035},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3991-4002},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Interpretable signed link prediction with signed infomax hyperbolic graph},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyper-relations: A model for denormalization of
transactional relational databases. <em>TKDE</em>, <em>35</em>(4),
3979–3990. (<a href="https://doi.org/10.1109/TKDE.2021.3124134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A formal model of denormalized relational schemas based on outer joins of base relations is presented. The composition of joins is modeled with a hyper-tree, called hyper-schema, with vertexes representing equi-joins, and edges representing joined base relations. The base relations are not the relations defined in the conceptual schema, but their “atoms.” The semantics of hyper-relations is defined in terms of join disjunctions. Three different denormalized outer join forms are also defined, in terms of constraints imposed to hyper-schemas that gradually make the maintenance of hyper-relations simpler and more efficient. The presented model can be used as a formal underpinning for 1) a special, but important case of materialized views, 2) an implementation of the physical, yet relational schema of a highly denormalized database, or 3) different optimization techniques based on denormalization.},
  archive      = {J_TKDE},
  author       = {Dragan Milićev},
  doi          = {10.1109/TKDE.2021.3124134},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3979-3990},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hyper-relations: A model for denormalization of transactional relational databases},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Higher-order truss decomposition in graphs. <em>TKDE</em>,
<em>35</em>(4), 3966–3978. (<a
href="https://doi.org/10.1109/TKDE.2021.3137955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {$k$ -truss model is a typical cohesive subgraph model and has been received considerable attention recently. However, the $k$ -truss model only considers the direct common neighbors of an edge, which restricts its ability to reveal fine-grained structure information of the graph. Motivated by this, in this paper, we propose a new model named $(k, \tau)$ -truss that considers the higher-order neighborhood ( $\tau$ hop) information of an edge. Based on the $(k, \tau)$ -truss model, we study the higher-order truss decomposition problem which computes the $(k, \tau)$ -trusses for all possible $k$ values regarding a given $\tau$ . Higher-order truss decomposition can be used in the applications such as community detection and search, hierarchical structure analysis, and graph visualization. To address this problem, we first propose a bottom-up decomposition paradigm in the increasing order of $k$ values to compute the corresponding $(k, \tau)$ -truss. Based on the bottom-up decomposition paradigm, we further devise three optimization strategies to reduce the unnecessary computation. We evaluate our proposed algorithms on real datasets and synthetic datasets, the experimental results demonstrate the efficiency, effectiveness and scalability of our proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Zi Chen and Long Yuan and Li Han and Zhengping Qian},
  doi          = {10.1109/TKDE.2021.3137955},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3966-3978},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Higher-order truss decomposition in graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical adaptive pooling by capturing high-order
dependency for graph representation learning. <em>TKDE</em>,
<em>35</em>(4), 3952–3965. (<a
href="https://doi.org/10.1109/TKDE.2021.3133646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on ten datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 20.18\%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.42\% and 16\%.},
  archive      = {J_TKDE},
  author       = {Ning Liu and Songlei Jian and Dongsheng Li and Yiming Zhang and Zhiquan Lai and Hongzuo Xu},
  doi          = {10.1109/TKDE.2021.3133646},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3952-3965},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical adaptive pooling by capturing high-order dependency for graph representation learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HGATE: Heterogeneous graph attention auto-encoders.
<em>TKDE</em>, <em>35</em>(4), 3938–3951. (<a
href="https://doi.org/10.1109/TKDE.2021.3138788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph auto-encoder is considered a framework for unsupervised learning on graph-structured data by representing graphs in a low dimensional space. It has been proved very powerful for graph analytics. In the real world, complex relationships in various entities can be represented by heterogeneous graphs that contain more abundant semantic information than homogeneous graphs. In general, graph auto-encoders based on homogeneous graphs are not applicable to heterogeneous graphs. In addition, little work has been done to evaluate the effect of different semantics on node embedding in heterogeneous graphs for unsupervised graph representation learning. In this work, we propose a novel Heterogeneous Graph Attention Auto-Encoders (HGATE) for unsupervised representation learning on heterogeneous graph-structured data. Based on the consideration of semantic information, our architecture of HGATE reconstructs not only the edges of the heterogeneous graph but also node attributes, through stacked encoder/decoder layers. Hierarchical attention is used to learn the relevance between a node and its meta-path based neighbors, and the relevance among different meta-paths. HGATE is applicable to transductive learning as well as inductive learning. Node classification and link prediction experiments on real-world heterogeneous graph datasets demonstrate the effectiveness of HGATE for both transductive and inductive tasks.},
  archive      = {J_TKDE},
  author       = {Wei Wang and Xiaoyang Suo and Xiangyu Wei and Bin Wang and Hao Wang and Hong-Ning Dai and Xiangliang Zhang},
  doi          = {10.1109/TKDE.2021.3138788},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3938-3951},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HGATE: Heterogeneous graph attention auto-encoders},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GCN for HIN via implicit utilization of attention and
meta-paths. <em>TKDE</em>, <em>35</em>(4), 3925–3937. (<a
href="https://doi.org/10.1109/TKDE.2021.3130712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous information network (HIN) embedding, aiming to map the structure and semantic information in a HIN to distributed representations, has drawn considerable research attention. Graph neural networks for HIN embeddings typically adopt a hierarchical attention (including node-level and meta-path-level attentions) to capture the information from meta-path-based neighbors. However, this complicated attention structure often cannot achieve the function of selecting meta-paths due to severe overfitting. Moreover, when propagating information, these methods do not distinguish direct (one-hop) meta-paths from indirect (multi-hop) ones. But from the perspective of network science, direct relationships are often believed to be more essential, which can only be used to model direct information propagation. To address these limitations, we propose a novel neural network method via implicitly utilizing attention and meta-paths, which can relieve the severe overfitting brought by the current over-parameterized attention mechanisms on HIN. We first use the multi-layer graph convolutional network (GCN) framework, which performs a discriminative aggregation at each layer, along with stacking the information propagation of direct linked meta-paths layer-by-layer, realizing the function of attentions for selecting meta-paths in an indirect way. We then give an effective relaxation and improvement via introducing a new propagation operation which can be separated from aggregation. That is, we first model the whole propagation process with well-defined probabilistic diffusion dynamics, and then introduce a random graph-based constraint which allows it to reduce noise with the increase of layers. Extensive experiments demonstrate the superiority of the new approach over state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Di Jin and Zhizhi Yu and Dongxiao He and Carl Yang and Philip S. Yu and Jiawei Han},
  doi          = {10.1109/TKDE.2021.3130712},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3925-3937},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GCN for HIN via implicit utilization of attention and meta-paths},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedDSR: Daily schedule recommendation in a federated deep
reinforcement learning framework. <em>TKDE</em>, <em>35</em>(4),
3912–3924. (<a href="https://doi.org/10.1109/TKDE.2021.3130265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Daily schedule recommendation is an intelligent approach to recommend multiple suitable activity locations and activity sequences for users based on their needs in a day. In such a scenario, training the model using traditional methods requires centralized data collection from individual users, which may be prohibited by data protection acts, such as GDPR and CCPA. In this paper, we address the problem of daily schedule recommendation utilizing the deep reinforcement learning model in a federated learning framework (FedDSR). And curriculum learning is applied to guide the training process towards better local optimization and better generalization. For the uploaded local parameters, a similarity aggregation algorithm is proposed to improve the quality of the model. The experimental results show that the proposed FedDSR model is superior and effective to multiple baselines on two real datasets Geolife and Chengdu . Comparing with baselines, our method not only ensures that the parties do not need to share data and thus achieve joint modeling, but also can exceed $\sim\!\! 18\%$ under evaluation metric perimeter and improve $\sim\! 0.72\%$ under evaluation metric ADTS .},
  archive      = {J_TKDE},
  author       = {Wei Huang and Jia Liu and Tianrui Li and Tianqiang Huang and Shenggong Ji and Jihong Wan},
  doi          = {10.1109/TKDE.2021.3130265},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3912-3924},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FedDSR: Daily schedule recommendation in a federated deep reinforcement learning framework},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and accurate non-negative latent factor analysis of
high-dimensional and sparse matrices in recommender systems.
<em>TKDE</em>, <em>35</em>(4), 3897–3911. (<a
href="https://doi.org/10.1109/TKDE.2021.3125252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fast non-negative latent factor (FNLF) model for a high-dimensional and sparse (HiDS) matrix adopts a Single Latent Factor-dependent, Non-negative, Multiplicative and Momentum-incorporated Update (SLF-NM 2 U) algorithm, which enables its fast convergence. It is crucial to achieve a rigorously theoretical proof regarding its fast convergence, which has not been provided in prior research. Aiming at addressing this critical issue, this work theoretically proves that with an appropriately chosen momentum coefficient, SLF-NM 2 U enables the fast convergence of an FNLF model in both continuous and discrete time cases. Empirical analysis of HiDS matrices generated by representative industrial applications provides empirical evidences for the theoretical proof. Hence, this study represents an important milestone in the field of HiDS matrix analysis.},
  archive      = {J_TKDE},
  author       = {Xin Luo and Yue Zhou and Zhigang Liu and MengChu Zhou},
  doi          = {10.1109/TKDE.2021.3125252},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3897-3911},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast and accurate non-negative latent factor analysis of high-dimensional and sparse matrices in recommender systems},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extracting and composing robust features with broad learning
system. <em>TKDE</em>, <em>35</em>(4), 3885–3896. (<a
href="https://doi.org/10.1109/TKDE.2021.3137792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With effective performance and fast training speed, broad learning system (BLS) has been widely developed in recent years, which provides a new way for network training. However, the randomly generated feature nodes and enhancement nodes in the BLS network may have redundant and inefficient features, which will affect the subsequent classification performance. In response to the above issues, we propose a series of self-encoding networks based on BLS from the perspective of unsupervised feature extraction. These include the single hidden layer autoencoder built on the basis of BLS(BLS-AE), the stacked BLS-based autoencoder (ST-BLS), the sparse BLS-based autoencoder (SP-BLS), and the stacked sparse BLS-based autoencoder(SS-BLS). The proposed BLS-based self-encoding networks retain the advantage of efficient BLS model training, and overcome the time-consuming defect of iterative parameter optimization in traditional self-encoding networks. In addition, the higher-level abstract features of the input data can be learned through the progressive encoding and decoding process. Combining $L_1$ regularization to train the parameters can further enhance the robustness of the extracted features. Extensive comparative experiments on real-world data sets demonstrate the superiority of the proposed methods in terms of both effectiveness and efficiency.},
  archive      = {J_TKDE},
  author       = {Kaixiang Yang and Yuchen Liu and Zhiwen Yu and C. L. Philip Chen},
  doi          = {10.1109/TKDE.2021.3137792},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3885-3896},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Extracting and composing robust features with broad learning system},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring the generalizability of spatio-temporal traffic
prediction: Meta-modeling and an analytic framework. <em>TKDE</em>,
<em>35</em>(4), 3870–3884. (<a
href="https://doi.org/10.1109/TKDE.2021.3130762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Spatio-Temporal Traffic Prediction (STTP) problem is a classical problem with plenty of prior research efforts that benefit from traditional statistical learning and recent deep learning approaches. While STTP can refer to many real-world problems, most existing studies focus on quite specific applications, such as the prediction of taxi demand, ridesharing order, traffic speed, and so on. This hinders the STTP research as the approaches designed for different applications are hardly comparable, and thus how an application-driven approach can be generalized to other scenarios is unclear. To fill in this gap, this paper makes three efforts: (i) we propose an analytic framework, called STAnalytic, to qualitatively investigate STTP approaches regarding their design considerations on various spatial and temporal factors, aiming to make different application-driven approaches comparable; (ii) we design a spatio-temporal meta-model, called STMeta, which can flexibly integrate generalizable temporal and spatial knowledge identified by STAnalytic, (iii) we build an STTP benchmark platform including ten real-life datasets with five scenarios to quantitatively measure the generalizability of STTP approaches. In particular, we implement STMeta with different deep learning techniques, and STMeta demonstrates better generalizability than state-of-the-art approaches by achieving lower prediction error on average across all the datasets.},
  archive      = {J_TKDE},
  author       = {Leye Wang and Di Chai and Xuanzhe Liu and Liyue Chen and Kai Chen},
  doi          = {10.1109/TKDE.2021.3130762},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3870-3884},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Exploring the generalizability of spatio-temporal traffic prediction: Meta-modeling and an analytic framework},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023f). Event detection with dynamic word-trigger-argument graph
neural networks. <em>TKDE</em>, <em>35</em>(4), 3858–3869. (<a
href="https://doi.org/10.1109/TKDE.2021.3132956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of ACE Event Detection (ED) often encounters ambiguous and unseen trigger words. Most conventional ED systems exclusively consider the semantic or syntactic patterns as the additional evidence to resolve the problem of the ambiguous and unseen triggers, but rarely consider taking advantages of structured knowledge of the event itself. In this study, we propose Dynamic Word-Trigger-Argument Graph Neural Networks (DWTA-GNN), a novel framework that leverages event structure knowledge to facilitate the two issues simultaneously. In our approach, we utilize words, entities, and event annotations from training to construct an event background graph, which can provide sufficient information of event structure to better disambiguate polysemous triggers and identify unseen triggers. To make full use of the constructed background graph, we further design a knowledge matching module to dynamically match appropriate event structure knowledge and construct a subgraph for each incoming sentence. Besides, an event-selective graph convolution is applied to filter out the noise in the matched knowledge so as to enhance event representation. Experiments on the ACE2005 dataset show that our model achieves competitive performance and advances previous approaches on ambiguous and unseen trigger words, verifying the effectiveness of incorporating event structure knowledge for event detection.},
  archive      = {J_TKDE},
  author       = {Yilin Zhang and Ziran Li and Zhiyuan Liu and Hai-Tao Zheng and Ying Shen and Lan Zhou},
  doi          = {10.1109/TKDE.2021.3132956},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3858-3869},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Event detection with dynamic word-trigger-argument graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing potential re-finding in personalized search with
hierarchical memory networks. <em>TKDE</em>, <em>35</em>(4), 3846–3857.
(<a href="https://doi.org/10.1109/TKDE.2021.3126066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of personalized search is to tailor the document ranking list to meet user&#39;s individual needs. Previous studies showed users usually look for the information that has been searched before. This is called re-finding behavior which is widely explored in existing personalized search approaches. However, most existing methods for identifying re-finding behavior focus on simple lexical similarities between queries. In this paper, we propose a personalized framework based on hierarchical memory networks (MN) to enhance the identification of the potential re-finding behavior. Specifically, we explore the potential re-finding behaviors of users from two dimensions. (1) Granularity dimension. The framework carries out re-finding identification with external memories from word, sentence, and session levels. (2) Query intent dimension. Query-based re-finding and document-based re-finding are taken into account to cover user&#39;s different query intents. To enhance the interaction between different memory slots, we optimize the $READ$ operation of MN with two strategies that utilize the information in memory in a multi-hop way. Endowed with these memory networks, we can enhance user&#39;s potential re-finding behaviors and build a fine-grained user model dynamically. Experimental results on two datasets have a significant improvement over baselines, and the optimized $READ$ operation shows better performance.},
  archive      = {J_TKDE},
  author       = {Yujia Zhou and Zhicheng Dou and Ji-Rong Wen},
  doi          = {10.1109/TKDE.2021.3126066},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3846-3857},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing potential re-finding in personalized search with hierarchical memory networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced bayesian factorization with variant scale
partitioning for multivariate time series analysis. <em>TKDE</em>,
<em>35</em>(4), 3832–3845. (<a
href="https://doi.org/10.1109/TKDE.2021.3128770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series data (Mv-TSD) portray the evolving processes of the system(s) under examination in a “multi-view” manner. Factorization methods are salient for Mv-TSD analysis with the potentials of structural feature construction correlating various data attributes. However, research challenges remain in the derivation of factors due to highly scattered data distribution of Mv-TSD and intensive interferences/outliers embedded in the source data. The proposed Enhanced Bayesian Factorization approach ( Enhanced-BF ) addresses the challenges in three phases: (1) variant scale partitioning applies to Mv-TSD according to degree of amplitude and obtains the blocks of variant scales; (2) hierarchical Bayesian model for tensor factorization automatically derives the factors of each block with interferences suppressed; (3) Bayesian unification model merges those block factors to construct the final structural features. Enhanced-BF has been evaluated using a case study of brain data engineering with multivariate electroencephalogram (EEG). Experimental results indicate that the proposed method manifests robustness to the interferences and outperforms the counterparts in terms of operation efficiency and error when factorizing EEG tensor. Besides, Enhanced-BF excels in factorization-based analysis of ongoing autism spectrum disorder (ASD) EEG: 3 times speed-up in factorization and $87.35\%$ accuracy in ASD discrimination. The latent factors (“biomarkers”) can distinctly interpret the typical EEG characteristics of ASD subjects.},
  archive      = {J_TKDE},
  author       = {Yunbo Tang and Dan Chen and Yiping Zuo and Xiaoqiang Lu and Rajiv Ranjan and Albert Y. Zomaya and Quanming Yao and Xiaoli Li},
  doi          = {10.1109/TKDE.2021.3128770},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3832-3845},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhanced bayesian factorization with variant scale partitioning for multivariate time series analysis},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elementary subgraph features for link prediction with neural
networks. <em>TKDE</em>, <em>35</em>(4), 3822–3831. (<a
href="https://doi.org/10.1109/TKDE.2021.3132352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The enclosing subgraph of a target link has been proved to be effective for prediction of potential links. However, it is still unclear what topological features of the subgraph play the key role in determining the existence of links. To give a possible answer to this question, in this paper, we propose a neural network based learning method for link prediction with only 1-hop neighborhood information. In detail, we extract the one-hop neighborhood of a target link as the enclosing subgraph, then encode the subgraph into different types of topological features, and lastly feed these features to train a fully connected neural network for link prediction. The experimental results show that our proposed learning method with the 1-hop neighborhood features could outperform those heuristic-based methods and achieve nearly equal performance to the state-of-the-art learning-based method WLNM and SEAL. Furthermore, it is observed that these features can be concatenated with attribute vectors to greatly promote the link prediction performance in attributed graphs. This indicates that the topological pattern within an enclosing subgraph, which determines the existence of a possible link, can be aggregated by some elementary subgraph features.},
  archive      = {J_TKDE},
  author       = {Zhihong Fang and Shaolin Tan and Yaonan Wang and Jinhu Lü},
  doi          = {10.1109/TKDE.2021.3132352},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3822-3831},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Elementary subgraph features for link prediction with neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient top-k matching for publish/subscribe ride
hitching. <em>TKDE</em>, <em>35</em>(4), 3808–3821. (<a
href="https://doi.org/10.1109/TKDE.2021.3124232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continued proliferation of mobile Internet and geo-locating technologies, carpooling as a green transport mode is widely accepted and becoming tremendously popular worldwide. In this paper, we focus on a popular carpooling service called ride hitching , which is typically implemented using a publish/subscribe approach. In a ride hitching service, drivers subscribe ride orders published by riders and continuously receive matching ride orders until one is picked. The current systems (e.g., Didi Hitch) adopt a threshold-based approach to filter ride orders. That is, a new ride order will be sent to all subscribing drivers whose planned trips can match the ride order within a pre-defined detour threshold. A limitation of this approach is that it is difficult for drivers to specify a reasonable detour threshold in practice. In addressing this problem, we propose a novel type of top- $k$ subscription queries called Top-$k$k Ride Subscription (TkRS) query, which continuously returns the best $k$ ride orders that match drivers’ trip plans to them. We propose two efficient algorithms to enable the top- $k$ result maintenance. We also design a novel hybrid grid index and a two-level buffer structure to efficiently track the top- $k$ results for all TkRS queries. Finally, extensive experiments on real-life datasets suggest that our proposed algorithms are capable of achieving desirable performance in practical settings.},
  archive      = {J_TKDE},
  author       = {Yafei Li and Hongyan Gu and Rui Chen and Jianliang Xu and Shangwei Guo and Junxiao Xue and Mingliang Xu},
  doi          = {10.1109/TKDE.2021.3124232},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3808-3821},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient top-k matching for Publish/Subscribe ride hitching},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Efficient semi-external SCC computation. <em>TKDE</em>,
<em>35</em>(4), 3794–3807. (<a
href="https://doi.org/10.1109/TKDE.2021.3138994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In literature, many algorithms are proposed to find strongly connected components (SCC) for directed graphs. Specifically, a SCC of a directed graph $G$ is one of its maximal subgraphs, in which any two nodes are reachable to each other. Existing in-memory algorithms are efficient, and can find all the SCCs of $G$ in a linear time, with respect to the size of $G$ . Nevertheless, as the sizes of graphs grow rapidly in real applications, current efforts have been focused on semi-external algorithms. Existing semi-external algorithms maintain an in-memory sketch $\mathcal {A}$ of $G$ , and gradually restructure $\mathcal {A}$ with their in-memory processes (IMP) until all the SCCs can be computed based on $\mathcal {A}$ . However, the I/O and CPU costs of existing algorithms are still high when $G$ is relatively large. Thus, this paper proposes a new semi-external algorithm EP-SCC with a novel IMP EP-Reduction for finding all the SCCs of $G$ efficiently. Extensive experiments are conducted on both synthetic and real graphs, in which WDC-2014 contains 1.7 billion nodes, and eu-2015 has over 91 billion edges. Experimental results confirm that EP-SCC significantly outperforms existing semi-external SCC algorithms.},
  archive      = {J_TKDE},
  author       = {Xiaolong Wan and Hongzhi Wang},
  doi          = {10.1109/TKDE.2021.3138994},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3794-3807},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient semi-external SCC computation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient influential community search in large uncertain
graphs. <em>TKDE</em>, <em>35</em>(4), 3779–3793. (<a
href="https://doi.org/10.1109/TKDE.2021.3131611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influential community search aims to find cohesive subgraphs (communities) with considerable influence. It is a fundamental graph management operator that can play a crucial role in biological network analysis, activity organization, and other real-life applications. Existing research on influential community search is mainly focused on deterministic graphs with the assumption that influences between entities are certain. This assumption is invalid in many cases because it ignores the uncertainty which is an inherent property of influence. Against this backdrop, in this paper, we introduce an uncertain influential community model, namely $(k, \eta)$ -influential community, based on which the influential community search problem over uncertain graphs is formulated. Furthermore, we propose an online approach by integrating a peeling-pruning strategy that can progressively refine the given uncertain graph to find the $(k, \eta)$ -influential communities. To further improve the search performance, two novel indexes, ICU-Index and FICU-Index, are developed to organize the $(k, \eta)$ -influential communities at different probabilistic intervals. The indexes decompose the probabilistic interval into multiple subintervals and based on this, the $(k, \eta)$ -influential communities are divided into different groups in turn. Compared with ICU-Index, FICU-Index requires considerably less space with the introduction of two optimization strategies. These indexes help obtain results of an influential community search problem more efficiently. Extensive experiments on large real and synthetic datasets demonstrate the efficiency and effectiveness of our proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Wensheng Luo and Xu Zhou and Kenli Li and Yunjun Gao and Keqin Li},
  doi          = {10.1109/TKDE.2021.3131611},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3779-3793},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient influential community search in large uncertain graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient hashing method using 2D-2D PCA for image copy
detection. <em>TKDE</em>, <em>35</em>(4), 3765–3778. (<a
href="https://doi.org/10.1109/TKDE.2021.3131188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image copy detection is an important technology of copyright protection. This paper proposes an efficient hashing method for image copy detection using 2D-2D (two-directional two-dimensional) PCA (Principal Component Analysis). The key is the discovery of the translation invariance of 2D-2D PCA. With the property of translation invariance, a novel model of extracting rotation-invariant low-dimensional features is designed by combining PCT (Polar Coordinate Transformation) and 2D-2D PCA. The PCT can convert an input rotated image to a translation matrix. Since the 2D-2D PCA is invariant to translation, the low-dimensional features learned from the translation matrix are rotation-invariant. Moreover, vector distances of low-dimensional features are stable to common digital operations and thus hash construction with the vector distances is of robustness and compactness. Three open image datasets are exploited to conduct various experiments for validating efficiencies of the proposed method. The results demonstrate that the proposed method is much better than some representative hashing methods in the performances of classification and copy detection.},
  archive      = {J_TKDE},
  author       = {Xiaoping Liang and Zhenjun Tang and Ziqing Huang and Xianquan Zhang and Shichao Zhang},
  doi          = {10.1109/TKDE.2021.3131188},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3765-3778},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient hashing method using 2D-2D PCA for image copy detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient decomposition selection for multi-class
classification. <em>TKDE</em>, <em>35</em>(4), 3751–3764. (<a
href="https://doi.org/10.1109/TKDE.2021.3130239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing a decomposition method for multi-class classification is an important trade-off between efficiency and predictive accuracy. Trying all the decomposition methods to find the best one is too time-consuming for many applications, while choosing the wrong one may result in large loss on predictive accuracy. In this paper, we propose an automatic decomposition method selection approach called “D-Chooser”, which is lightweight and can choose the best decomposition method accurately. D-Chooser is equipped with our proposed difficulty index which consists of sub-metrics including distribution divergence, overlapping regions, unevenness degree and relative size of the solution space. The difficulty index has two intriguing properties: 1) fast to compute and 2) measuring multi-class problems comprehensively. Extensive experiments on real-world multi-class problems show that D-Chooser achieves an accuracy of 80.56\% in choosing the best decomposition method. It can choose the best method in just a few seconds, while existing approaches verify the effectiveness of a decomposition method often takes a few hours. We also provide case studies on Kaggle competitions and the results confirm that D-Chooser is able to choose a better decomposition method than the winning solutions.},
  archive      = {J_TKDE},
  author       = {Yawen Chen and Zeyi Wen and Bingsheng He and Jian Chen},
  doi          = {10.1109/TKDE.2021.3130239},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3751-3764},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient decomposition selection for multi-class classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic transformation of prior knowledge into bayesian
models for data streams. <em>TKDE</em>, <em>35</em>(4), 3742–3750. (<a
href="https://doi.org/10.1109/TKDE.2021.3139469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider how to effectively use prior knowledge when learning a Bayesian model from streaming environments where the data come endlessly and sequentially. This problem is highly important in the era of data explosion and rich sources of valuable external knowledge such as pre-trained models, ontologies, Wikipedia, etc. We show that some existing approaches can forget any knowledge very fast. We then propose a novel framework that enables to incorporate the prior knowledge of different forms into a base Bayesian model for data streams. Our framework subsumes some existing popular models for time-series/dynamic data. Extensive experiments show that our framework outperforms existing methods with a large margin. In particular, our framework can help Bayesian models generalize well on extremely short text while other methods overfit. An implementation of our framework is available at http://github.com/bachtranxuan/TPS .},
  archive      = {J_TKDE},
  author       = {Tran Xuan Bach and Nguyen Duc Anh and Ngo Van Linh and Khoat Than},
  doi          = {10.1109/TKDE.2021.3139469},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3742-3750},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic transformation of prior knowledge into bayesian models for data streams},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic set similarity join: An update log based approach.
<em>TKDE</em>, <em>35</em>(4), 3727–3741. (<a
href="https://doi.org/10.1109/TKDE.2021.3126631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The set similarity join finds all pairs of similar sets from two collections of sets. It has many real world applications, such as personalized recommendation and community mining. In this paper, we study the problem of computing the similarity join in a dynamic context, where the sets are updated dynamically. This, however, is inefficient with the state-of-the-art join methods, because they usually assume that data collections are static and have to compute the join result from scratch whenever a set is updated. To address this issue, we propose ${{\sf ALJoin}}$ , an adaptive filtering approach that computes the join result incrementally based on the update logs. We first investigate the effect of set updates on the similarity values, and on this basis we propose to build a neighborhood index for each set. The neighborhood index of a specific set consists of any other sets that can be transformed into its similar sets within a threshold number of update operations. ${{\sf ALJoin}}$ then uses this index to effectively identify both similar and dissimilar set pairs based on their update logs. To efficiently build the neighborhood index, we devise several filtering techniques and propose a “lazy-forward” method to reduce the computational cost. In addition, to improve the efficiency on varying workloads, we propose an analytical cost model, and design an online algorithm with performance guarantees to dynamically consolidate the update logs and adapt the neighborhood indexes. We evaluated our method using four real-world datasets. Experimental results show that our approach outperforms existing methods by up to $3.7\times$ .},
  archive      = {J_TKDE},
  author       = {Chengcheng Yang and Lisi Chen and Hao Wang and Shuo Shang and Rui Mao and Xiangliang Zhang},
  doi          = {10.1109/TKDE.2021.3126631},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3727-3741},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic set similarity join: An update log based approach},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Dynamic ridesharing with minimal regret: Towards an
enhanced engagement among three stakeholders. <em>TKDE</em>,
<em>35</em>(4), 3712–3726. (<a
href="https://doi.org/10.1109/TKDE.2022.3141368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dynamic ridesharing, the platform serves as the mediator by tailoring the assignment result between workers and riders with a focus on a certain objective. Existing studies generally focus on either one or two stakeholders when modelling the problem while the wellbeing of the other parties may be ignored or even undermined. For example, purely maximizing the total revenue of the ridesharing platform may cause the loss of riders and in turn lead to a low served rate, because those expensive orders will be processed in priority. In this paper, we for the first time study how to incorporate the willingness of all stakeholders (i.e., the platform, workers and riders). Given a set of workers and a set of rider requests, we aim to return the matchable worker-rider pairs in order to minimize the regret . Specifically, two types of regret are defined: (i) the served rate regret , which refers to the rate of unserved requests, catering for the reputation and profit of the platform and workers; (ii) the revenue regret , which considers the portion of revenue loss from unserved riders, catering for the focus of workers and riders in the trip schedule. We prove the NP-hardness of this problem. To tackle this problem, we first propose a dynamic programming insertion algorithm to improve the efficiency of inserting a rider request into a trip schedule of a worker. Furthermore, two kinds of heuristic algorithms are devised to match rider requests with workers effectively. Comprehensive experiments on two real-world datasets verify the effectiveness, efficiency and scalability of our solutions in dealing with different supply-demand relationships in practice.},
  archive      = {J_TKDE},
  author       = {Tingting Wang and Hui Luo and Zhifeng Bao and Lei Duan},
  doi          = {10.1109/TKDE.2022.3141368},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3712-3726},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic ridesharing with minimal regret: Towards an enhanced engagement among three stakeholders},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic auto-structuring graph neural network: A joint
learning framework for origin-destination demand prediction.
<em>TKDE</em>, <em>35</em>(4), 3699–3711. (<a
href="https://doi.org/10.1109/TKDE.2021.3135898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving the demand prediction problem is an important part of improving the efficiency and reliability of ride-hailing services. Spatial-temporal graph learning methods have shown potential in modeling the spatial-temporal dependencies of ride-hailing demand data, but most existing studies focus on region-level demand prediction with only a few researchers addressing the problem of origin-destination (OD) demand prediction. In addition, previous spatial-temporal graph learning methods employ pre-defined and rigid graph structures that do not reveal the instinct and dynamic dependencies of ride-hailing demand data. In this paper, we propose a joint learning framework called Dynamic Auto-structuring Graph Neural Network (DAGNN) to address the origin-destination demand prediction problem. We develop a Dynamic Graph Decomposition and Recombination layer (DGDR) to handle both the graph structure and the graph representation learning problems simultaneously, with graph representations learned from a group of trainable and time-aware edge-induced subgraphs. Experimental results show that our proposed model outperforms ten baseline models with two real-world ride-hailing demand datasets and is efficient in structural pattern discovery. Comparing with existing methods, the significant advantage of the proposed method is that it circumvents the difficulties in defining the underlying graph structure of the researched data.},
  archive      = {J_TKDE},
  author       = {Dapeng Zhang and Feng Xiao},
  doi          = {10.1109/TKDE.2021.3135898},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3699-3711},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic auto-structuring graph neural network: A joint learning framework for origin-destination demand prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovery of approximate lexicographical order dependencies.
<em>TKDE</em>, <em>35</em>(4), 3684–3698. (<a
href="https://doi.org/10.1109/TKDE.2021.3130227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lexicographical order dependencies (LODs) specify orders between list of attributes, and are proven useful in optimizing SQL queries with order by clauses. To discover hidden dependencies from dirty data in practice, approximate dependency discoveries are actively studied, aiming at automatically discovering dependencies that hold on data with some exceptions. In this paper we study the discovery of approximate LODs. (1) We adapt two error measures, namely $g_1$ and $g_3$ , to LODs. We prove their desirable properties, present efficient algorithms for computing the measures and related lower and upper bounds, and study the relationship between the two measures. (2) We present an efficient approximate LOD discovery algorithm that is well suited to the two error measures, with a set of pruning rules, optimization techniques and ranking functions. (3) We study techniques for estimating $g_1$ by sampling, with high accuracy and far less time. (4) We conduct extensive experiments to verify the effectiveness and scalability of our methods, using both real-life and synthetic data.},
  archive      = {J_TKDE},
  author       = {Yifeng Jin and Zijing Tan and Jixuan Chen and Shuai Ma},
  doi          = {10.1109/TKDE.2021.3130227},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3684-3698},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discovery of approximate lexicographical order dependencies},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovering and interpreting biased concepts in online
communities. <em>TKDE</em>, <em>35</em>(4), 3672–3683. (<a
href="https://doi.org/10.1109/TKDE.2021.3139680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language carries implicit human biases, functioning both as a reflection and a perpetuation of stereotypes that people carry with them. Recently, ML-based NLP methods such as word embeddings have been shown to learn such language biases with striking accuracy. This capability of word embeddings has been successfully exploited as a tool to quantify and study human biases. However, previous studies only consider a predefined set of biased concepts to attest (e.g., whether gender is more or less associated with particular jobs), or just discover biased words without helping to understand their meaning at the conceptual level. As such, these approaches can be either unable to find biased concepts that have not been defined in advance, or the biases they find are difficult to interpret and study. This could make existing approaches unsuitable to discover and interpret biases in online communities, as such communities may carry different biases than those in mainstream culture. This paper improves upon, extends, and evaluates our previous data-driven method to automatically discover and help interpret biased concepts encoded in word embeddings. We apply this approach to study the biased concepts present in the language used in online communities and experimentally show the validity and stability of our method.},
  archive      = {J_TKDE},
  author       = {Xavier Ferrer-Aran and Tom van Nuenen and Natalia Criado and Jose Such},
  doi          = {10.1109/TKDE.2021.3139680},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3672-3683},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discovering and interpreting biased concepts in online communities},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dimensionality reduction for categorical data.
<em>TKDE</em>, <em>35</em>(4), 3658–3671. (<a
href="https://doi.org/10.1109/TKDE.2021.3132373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Categorical attributes are those that can take a discrete set of values, e.g., colours. This work is about compressing vectors over categorical attributes to low-dimension discrete vectors. The current hash-based methods compressing vectors over categorical attributes to low-dimension discrete vectors do not provide any guarantee on the Hamming distances between the compressed representations. Here we present FSketch to create sketches for a sparse categorical data and an estimator to estimate the pairwise Hamming distances among the uncompressed data only from their sketches. We claim that these sketches can be used in the usual data mining tasks in place of the original data without compromising the quality of the task. For that we ensure that the sketches also are categorical, sparse, and the Hamming distance estimates are reasonably precise. Both the sketch construction and the Hamming distance estimation algorithms require just a single-pass; furthermore, changes to a data point can be incorporated into its sketch in an efficient manner. The compressibility depends upon how sparse the data is and is independent of the original dimension – making our algorithm attractive for many real-life scenarios. Our claims are backed by rigorous theoretical analysis of the properties of FSketch and supplemented by extensive comparative evaluations with related algorithms on some real-world datasets. We show that FSketch is significantly faster, and the accuracy obtained by using its sketches are among the top for the standard unsupervised tasks of $\mathrm{RMSE}$ , clustering and similarity search.},
  archive      = {J_TKDE},
  author       = {Debajyoti Bera and Rameshwar Pratap and Bhisham Dev Verma},
  doi          = {10.1109/TKDE.2021.3132373},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3658-3671},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dimensionality reduction for categorical data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeHIN: A decentralized framework for embedding large-scale
heterogeneous information networks. <em>TKDE</em>, <em>35</em>(4),
3645–3657. (<a href="https://doi.org/10.1109/TKDE.2022.3141951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling heterogeneity by extraction and exploitation of high-order information from heterogeneous information networks (HINs) has been attracting immense research attention in recent times. Such heterogeneous network embedding (HNE) methods effectively harness the heterogeneity of small-scale HINs. However, in the real world, the size of HINs grow exponentially with the continuous introduction of new nodes and different types of links, making it a billion-scale network. Learning node embeddings on such HINs creates a performance bottleneck for existing HNE methods that are commonly centralized, i.e., complete data and the model are both on a single machine. To address large-scale HNE tasks with strong efficiency and effectiveness guarantee, we present Decentralized Embedding Framework for Heterogeneous Information Network (DeHIN) in this paper. In DeHIN, we generate a distributed parallel pipeline that utilizes hypergraphs in order to infuse parallelization into the HNE task. DeHIN presents a context preserving partition mechanism that innovatively formulates a large HIN as a hypergraph, whose hyperedges connect semantically similar nodes. Our framework then adopts a decentralized strategy to efficiently partition HINs by adopting a tree-like pipeline. Then, each resulting subnetwork is assigned to a distributed worker, which employs the deep information maximization theorem to locally learn node embeddings from the partition it receives. We further devise a novel embedding alignment scheme to precisely project independently learned node embeddings from all subnetworks onto a common vector space, thus allowing for downstream tasks like link prediction and node classification. As shown from our experimental results, DeHIN significantly improves the efficiency and accuracy of existing HNE models as well as outperforms the large-scale graph embedding frameworks by efficiently scaling up to large-scale HINs.},
  archive      = {J_TKDE},
  author       = {Mubashir Imran and Hongzhi Yin and Tong Chen and Zi Huang and Kai Zheng},
  doi          = {10.1109/TKDE.2022.3141951},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3645-3657},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DeHIN: A decentralized framework for embedding large-scale heterogeneous information networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). DCT-GAN: Dilated convolutional transformer-based GAN for
time series anomaly detection. <em>TKDE</em>, <em>35</em>(4), 3632–3644.
(<a href="https://doi.org/10.1109/TKDE.2021.3130234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series anomaly detection (TSAD) is an essential problem faced in several fields, e.g., fault detection, fraud detection, and intrusion detection, etc. Although TSAD is a crucial problem in anomaly detection, few solutions in anomaly detection are suitable for it at present. Recently, some researchers use GAN-based methods such as TAnoGAN and TadGAN to solve TSAD problem. However, problems such as model collapse, low generalization capability and poor accuracy still exist. In this article, we proposed a Dilated Convolutional Transformer-based GAN (DCT-GAN) to enhance accuracy and improve generalization capability of the model. Specifically, DCT-GAN utilize several generators and a single discriminator to alleviate the mode collapse problem. Each generator consists of a dilated convolutional neural network and a Transformer block to obtain fine-grained and coarse-grained information of the time series, which is a useful component to improve generalization capability. We also use weight-based mechanism to balance these generators. Experiments verify the effectiveness of our method and each part of DCT-GAN.},
  archive      = {J_TKDE},
  author       = {Yifan Li and Xiaoyan Peng and Jia Zhang and Zhiyong Li and Ming Wen},
  doi          = {10.1109/TKDE.2021.3130234},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3632-3644},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DCT-GAN: Dilated convolutional transformer-based GAN for time series anomaly detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data level privacy preserving: A stochastic perturbation
approach based on differential privacy. <em>TKDE</em>, <em>35</em>(4),
3619–3631. (<a href="https://doi.org/10.1109/TKDE.2021.3137047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the great amount of available data, especially collecting from the ubiquitous Internet of Things (IoT), the issue of privacy leakage arises increasingly concerns recently. To preserve the privacy of IoT datasets, traditional methods usually calibrate random noises on the data values to achieve differential privacy (DP). However, the amount of the calibrating noises should be carefully designed and a heedless value will definitely degrade the availability of datasets. Thus, in this work, we propose a stochastic perturbation method to sanitize the dataset, where the perturbation is obtained from the rest samples in the same dataset. In addition, we derive the expression of the utility level based on its unique framework and prove that the proposed algorithm can achieve the $\epsilon$ -DP. To show the effectiveness of the proposed algorithm, we conduct extensive experiments on real-life datasets by various functions, such as query answers and machine learning tasks. By comparing with the state-of-the-art methods, our proposed algorithm can achieve a better performance under the same privacy level.},
  archive      = {J_TKDE},
  author       = {Chuan Ma and Long Yuan and Li Han and Ming Ding and Raghav Bhaskar and Jun Li},
  doi          = {10.1109/TKDE.2021.3137047},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3619-3631},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Data level privacy preserving: A stochastic perturbation approach based on differential privacy},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data anonymization with diversity constraints.
<em>TKDE</em>, <em>35</em>(4), 3603–3618. (<a
href="https://doi.org/10.1109/TKDE.2021.3131528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent privacy legislation has aimed to restrict and control the amount of personal data published by companies and shared with third parties. Much of this real data is not only sensitive requiring anonymization but also contains characteristic details from a variety of individuals. This diversity is desirable in many applications ranging from Web search to drug and product development. Unfortunately, data anonymization techniques have largely ignored diversity in its published result. This inadvertently propagates underlying bias in subsequent data analysis. We study the problem of finding a diverse anonymized data instance where diversity is measured via a set of diversity constraints. We formalize diversity constraints, and study their fundamental problems of satisfiability, implication, and validation. We show that determining the existence of a diverse, anonymized instance can be done in PTIME, and we present a clustering-based algorithm, along with optimizations to improve performance. We conduct extensive experiments using real and synthetic data showing the effectiveness of our techniques, and improvement over existing baselines. Our work aligns with recent trends towards responsible data science by coupling diversity with privacy-preserving data publishing.},
  archive      = {J_TKDE},
  author       = {Mostafa Milani and Yu Huang and Fei Chiang},
  doi          = {10.1109/TKDE.2021.3131528},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3603-3618},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Data anonymization with diversity constraints},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crowd-sensing enhanced parking patrol using sharing bikes’
trajectories. <em>TKDE</em>, <em>35</em>(4), 3589–3602. (<a
href="https://doi.org/10.1109/TKDE.2021.3138195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Illegal vehicle parking is a common urban problem faced by major cities in the world, as it incurs traffic jams, which lead to air pollution and traffic accidents. The government highly relies on active human efforts to detect illegal parking events. However, such an approach is extremely ineffective to cover a large city since the police have to patrol over the entire city roads. The massive and high-quality sharing bike trajectories from Mobike offer us a unique opportunity to design a ubiquitous illegal parking detection approach, as most of the illegal parking events happen at curbsides and have significant impact on the bike users. The detection result can guide the patrol schedule, i.e., send the patrol policemen to the region with higher illegal parking risks, and further improve the patrol efficiency. Inspired by this idea, three main components are employed in the proposed framework: 1) trajectory pre-processing , which filters outlier GPS points, performs map-matching, and builds trajectory indexes; 2) illegal parking detection , which models the normal trajectories, extracts features from the evaluation trajectories, and utilizes a distribution test-based method to discover the illegal parking events; and 3) patrol scheduling , which leverages the detection result as reference context, and models the scheduling task as a multi-agent reinforcement learning problem to guide the patrol police. Finally, extensive experiments are presented to validate the effectiveness of illegal parking detection, as well as the improvement of patrol efficiency.},
  archive      = {J_TKDE},
  author       = {Tianfu He and Jie Bao and Yexin Li and Hui He and Yu Zheng},
  doi          = {10.1109/TKDE.2021.3138195},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3589-3602},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Crowd-sensing enhanced parking patrol using sharing bikes’ trajectories},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Cost-effective incremental deep model: Matching model
capacity with the least sampling. <em>TKDE</em>, <em>35</em>(4),
3575–3588. (<a href="https://doi.org/10.1109/TKDE.2021.3132622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing approaches often utilize the pre-fixed structure and large number of labeled data for training complex deep models, which are difficult to implement on incremental scenarios. As a matter of fact, real-world data is always in stream form. Thereby, there exits two challenges for building incremental deep models: a) Capacity Scalability . The entire training data is not available before learning the task. It is a challenge to make the deep model structure scale with streaming data for flexible model evolution and faster convergence. b) Capacity Sustainability . The distribution streaming data usually changes in nature (concept drift), thus it is necessary to update the model while preserving previous knowledge for overcoming the catastrophic forgetting. To this end, we develop an incremental deep model (IDM), which expands the network structure according to streaming data and slows down forgetting with the adaptive fisher regularization. However, IDM ignores another significant challenge with streaming data: c) Capacity Demand . Training a deep model always needs a large amount of labeled data, whereas it is almost impossible to label all unlabeled instances in real time. The core problem is to select a small number of the most discriminative instances to label while keeping the predictive accuracy of the model. Thereby, we focus on the online semi-supervised learning scenario with abrupt changes in data distribution, and further improve IDM to a cost-effective incremental deep model (CE-IDM), which can adaptively select the most discriminative newly coming instances for query to reduce the manual labeling costs. Specifically, CE-IDM adopts a novel extensible deep network structure by using an extra attention model for hidden layers. Based on the adaptive attention weights, CE-IDM develops a novel instance selection criterion by jointly estimating unlabeled instances’ representative and informative degree to satisfy the capacity demand. With the newly labeled instances, CE-IDM can quickly update the model with adaptive depth from streaming data and enable capacity scalability. Also, we address capacity sustainability by exploiting the attention based fisher information matrix, which can slow down the forgetting in consequence. Finally, CE-IDM can deal with the three capacity challenges methioned above in a unified framework. We conduct extensive experiments on real-world data and show that CE-IDM outperforms the state-of-the-art methods with a substantial margin.},
  archive      = {J_TKDE},
  author       = {Yang Yang and Da-Wei Zhou and De-Chuan Zhan and Hui Xiong and Yuan Jiang and Jian Yang},
  doi          = {10.1109/TKDE.2021.3132622},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3575-3588},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cost-effective incremental deep model: Matching model capacity with the least sampling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive graph representations for logical formulas
embedding. <em>TKDE</em>, <em>35</em>(4), 3563–3574. (<a
href="https://doi.org/10.1109/TKDE.2021.3139333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the non-transparent computing process of deep learning has become a significant reason hindering its further development. The Neural-Symbolic (NS) system formed by integrating logic rules into neural networks has attracted increasing attention owing to its direct interpretability. Embedding symbolic logical formulas into a low-dimensional continuous space provides an effective way for the NS system. However, current studies are all constrained by the modeling ability for its syntactic structure and fail to preserve the intrinsic semantics in embeddings, which causes poor performance on downstream reasoning tasks. To this end, this paper proposes a novel method of Con trastive G raph R epresentations (ConGR) for logical formulas embedding. First, to improve the modeling ability for the syntactic structure, ConGR introduces a densely connected graph convolutional network (GCN) with an attention mechanism to process syntax parsing graphs of formulas. In this way, discriminative local and global embeddings of formulas are obtained at the syntax level. Second, the contrastive instances (positive or negative) for each anchor formula are generated by the transformation under the guidance of logical properties. To preserve semantic information, two types of contrast, global-local and global-global, are carried out to refine formula embeddings. Extensive experiments demonstrate that ConGR obtains superior performance against state-of-the-art baselines on entailment checking and premise selection datasets.},
  archive      = {J_TKDE},
  author       = {Qika Lin and Jun Liu and Lingling Zhang and Yudai Pan and Xin Hu and Fangzhi Xu and Hongwei Zeng},
  doi          = {10.1109/TKDE.2021.3139333},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3563-3574},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Contrastive graph representations for logical formulas embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-aware and time-aware attention-based model for
disease risk prediction with interpretability. <em>TKDE</em>,
<em>35</em>(4), 3551–3562. (<a
href="https://doi.org/10.1109/TKDE.2021.3130171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the huge accumulation of Electronic Health Records (EHRs), numerous deep learning based predictive models were proposed for this task. Among them, most of the existing state-of-the-art (SOTA) models were built with recurrent neural networks (RNNs). Regardless of their success, RNN-based models mainly suffer from three limitations. (i) Accuracy: the prediction accuracy of RNN-based models drops quickly as the length of EHR sequences increases. (ii) Efficiency: the recurrence property of RNN-based models makes the computation parallelization impossible, and accordingly hurts the efficiency of such models in practice. (iii) Interpretability: the outputs of RNN-based models are difficult to explain due to the unexplainable nature of deep models. In this paper, we resort to the recently advanced attention mechanism to model the dependencies between inputs and outputs, which overcomes shortages of RNN-based models in accuracy and efficiency. As for interpretability, we model the relationships with two linear mappings from the input to the output, which account for two important factors—one is for context-aware information and the other is for time-aware representation—of capturing discriminative features in learning patient’s representations. We empirically demonstrate the effectiveness of the proposed model in both accuracy and computational efficiency, meanwhile, analyze and discuss the reasonability of each explanation approach.},
  archive      = {J_TKDE},
  author       = {Xianli Zhang and Buyue Qian and Yang Li and Shilei Cao and Ian Davidson},
  doi          = {10.1109/TKDE.2021.3130171},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3551-3562},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Context-aware and time-aware attention-based model for disease risk prediction with interpretability},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CmnRec: Sequential recommendations with chunk-accelerated
memory network. <em>TKDE</em>, <em>35</em>(4), 3540–3550. (<a
href="https://doi.org/10.1109/TKDE.2022.3141102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Memory-based Neural Recommenders (MNR) have demonstrated superior predictive accuracy in the task of sequential recommendations, particularly for modeling long-term item dependencies. However, typical MNR requires complex memory access operations, i.e., both writing and reading via a controller (e.g., RNN) at every time step. Those frequent operations will dramatically increase the network training time, resulting in the difficulty in being deployed on industrial-scale recommender systems. In this paper, we present a novel general Chunk framework to accelerate MNR significantly. Specifically, our framework divides proximal information units into chunks, and performs memory access at certain time steps, whereby the number of memory operations can be greatly reduced. We investigate two ways to implement effective chunking, i.e., PEriodic Chunk (PEC) and Time-Sensitive Chunk (TSC), to preserve and recover important recurrent signals in the sequence. Since chunk-accelerated MNR models take into account more proximal information units than that from a single timestep, it can alleviate the influence of noise in the user-item interaction sequence to a large extent, and thus improve the stability of MNR. In this way, the proposed chunk mechanism can lead to not only faster training and prediction, but even slightly better results. The experimental results on three real-world datasets (weishi, ml-10M and ml-latest) show that our chunk framework notably reduces the running time (e.g., with up to 7x for training &amp; 10x for inference on ml-latest) of MNR, and meantime achieves competitive performance.},
  archive      = {J_TKDE},
  author       = {Shilin Qu and Fajie Yuan and Guibing Guo and Liguang Zhang and Wei Wei},
  doi          = {10.1109/TKDE.2022.3141102},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3540-3550},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CmnRec: Sequential recommendations with chunk-accelerated memory network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cluster-based joins for federated SPARQL queries.
<em>TKDE</em>, <em>35</em>(4), 3525–3539. (<a
href="https://doi.org/10.1109/TKDE.2021.3135507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated RDF systems allow users to retrieve data from multiple independent sources without needing to have all the data in the same triple store. The performance of these systems can be poor for large and geographically distributed RDF data where network transfer costs are high. This article introduces CBTP-OL and CBTP-Nhop, two novel join algorithms that take advantage of network topology to decrease the cost of processing Basic Graph Pattern (BGP) SPARQL queries in a geographically distributed environment. Federation members are grouped in clusters, based on the network communication cost between the members, and the bulk of the join processing is pushed to the clusters. Our CBTP-OL and CBTL-Nhop algorithms use an overlap list and, respectively, an N-hop overlap list , to efficiently compute join results from triples in different clusters. We implement our algorithms in the OpenRDF Sesame federated framework and use Apache Rya triple store instances as federation members. Experimental evaluation results show the advantages of our approach over existing techniques.},
  archive      = {J_TKDE},
  author       = {Fan Yang and Adina Crainiceanu and Zhiyuan Chen and Don Needham},
  doi          = {10.1109/TKDE.2021.3135507},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3525-3539},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cluster-based joins for federated SPARQL queries},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification in dynamic data streams with a scarcity of
labels. <em>TKDE</em>, <em>35</em>(4), 3512–3524. (<a
href="https://doi.org/10.1109/TKDE.2021.3135755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble techniques are a powerful method for recognising and reacting to changes in non-stationary data. However, most researches into dynamic classification with ensembles assume that the true class label of each incoming point is available or easily obtained. This is unrealistic in most practical applications, especially in high-velocity streams where manually labeling each point is prohibitively expensive. To address this challenge, this paper proposes an algorithm, named Clustering and One-Class Classification Ensemble Learning (COCEL), which incorporates a stream clustering algorithm and an ensemble of one-class classifiers with active learning, for classification in dynamic data streams. The method exploits the intuitive relationship between clusters and one-class classifiers to cope with a small training set (or no training set) and improve with experience, self-modifying its internal state to cope with changes in the data stream. The proposed method is evaluated on synthetic data streams exhibiting concept evolution and concept drift and a collection of high-velocity real data streams where manually labeling each incoming point is infeasible or expensive and labor intensive. Finally, a comparative evaluation with peer stream classification ensembles shows that COCEL can achieve superior or comparative accuracy while typically requiring less than 0.01\% of the stream labels.},
  archive      = {J_TKDE},
  author       = {Conor Fahy and Shengxiang Yang and Mario Gongora},
  doi          = {10.1109/TKDE.2021.3135755},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3512-3524},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Classification in dynamic data streams with a scarcity of labels},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CatGCN: Graph convolutional networks with categorical node
features. <em>TKDE</em>, <em>35</em>(4), 3500–3511. (<a
href="https://doi.org/10.1109/TKDE.2021.3133013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies on Graph Convolutional Networks (GCNs) reveal that the initial node representations (i.e., the node representations before the first-time graph convolution) largely affect the final model performance. However, when learning the initial representation for a node, most existing work linearly combines the embeddings of node features, without considering the interactions among the features (or feature embeddings). We argue that when the node features are categorical, e.g., in many real-world applications like user profiling and recommender system, feature interactions usually carry important signals for predictive analytics. Ignoring them will result in suboptimal initial node representation and thus weaken the effectiveness of the follow-up graph convolution. In this paper, we propose a new GCN model named CatGCN, which is tailored for graph learning on categorical node features. Specifically, we integrate two ways of explicit interaction modeling into the learning of initial node representation, i.e., local interaction modeling on each pair of node features and global interaction modeling on an artificial feature graph. We then refine the enhanced initial node representations with the neighborhood aggregation-based graph convolution. We train CatGCN in an end-to-end fashion and demonstrate it on the task of node classification. Extensive experiments on three tasks of user profiling (the prediction of user age, city, and purchase level) from Tencent and Alibaba datasets validate the effectiveness of CatGCN, especially the positive effect of performing feature interaction modeling before graph convolution.},
  archive      = {J_TKDE},
  author       = {Weijian Chen and Fuli Feng and Qifan Wang and Xiangnan He and Chonggang Song and Guohui Ling and Yongdong Zhang},
  doi          = {10.1109/TKDE.2021.3133013},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3500-3511},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CatGCN: Graph convolutional networks with categorical node features},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CasFlow: Exploring hierarchical structures and propagation
uncertainty for cascade prediction. <em>TKDE</em>, <em>35</em>(4),
3484–3499. (<a href="https://doi.org/10.1109/TKDE.2021.3126475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding in-network information diffusion is a fundamental problem in many applications and one of the primary challenges is to predict the information cascade size. Most of the existing models rely either on hypothesized point process (e.g., Poisson and Hawkes processes), or simply predict the information propagation via deep neural networks. However, they fail to simultaneously capture the underlying global and local structures of a cascade and the propagation uncertainty in the diffusion, which may result in unsatisfactory prediction performance. To address these, in this work we propose a novel probabilistic cascade prediction framework CasFlow : Hierarchical Cascade Normalizing Flows. CasFlow allows a non-linear information diffusion inference and models the information diffusion process by learning the latent representation of both the structural and temporal information. It is a pattern-agnostic model leveraging normalizing flows to learn the node-level and cascade-level latent factors in an unsupervised manner. In addition, CasFlow is capable of capturing both the cascade representation uncertainty and node infection uncertainty, while enabling hierarchical pattern learning of information diffusion. Extensive experiments conducted on real-world datasets demonstrate that CasFlow reduces the prediction error to 21.0\% by only observing half an hour of cascades, compared to state-of-the-art approaches, while also enabling model interpretability.},
  archive      = {J_TKDE},
  author       = {Xovee Xu and Fan Zhou and Kunpeng Zhang and Siyuan Liu and Goce Trajcevski},
  doi          = {10.1109/TKDE.2021.3126475},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3484-3499},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CasFlow: Exploring hierarchical structures and propagation uncertainty for cascade prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CaEGCN: Cross-attention fusion based enhanced graph
convolutional network for clustering. <em>TKDE</em>, <em>35</em>(4),
3471–3483. (<a href="https://doi.org/10.1109/TKDE.2021.3125020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the powerful learning ability of deep convolutional networks, deep clustering methods can extract the most discriminative information from individual data and produce more satisfactory clustering results. However, existing deep clustering methods usually ignore the relationship between the data. Fortunately, the graph convolutional network can handle such relationships, opening a new research direction for deep clustering. In this paper, we propose a cross-attention based deep clustering framework, named Cross-Attention Fusion based Enhanced Graph Convolutional Network (CaEGCN), which contains four main modules: the cross-attention fusion module which innovatively concatenates the Content Auto-encoder module (CAE) relating to the individual data and Graph Convolutional Auto-encoder module (GAE) relating to the relationship between the data in a layer-by-layer manner, and the self-supervised model that highlights the discriminative information for clustering tasks. While the cross-attention fusion module fuses two kinds of heterogeneous representation, the CAE module supplements the content information for the GAE module, which avoids the over-smoothing problem of GCN. In the GAE module, two novel loss functions are proposed that reconstruct the content and relationship between the data, respectively. Finally, the self-supervised module constrains the distributions of the middle layer representations of CAE and GAE to be consistent. Experimental results on different types of datasets prove the superiority and robustness of the proposed CaEGCN.},
  archive      = {J_TKDE},
  author       = {Guangyu Huo and Yong Zhang and Junbin Gao and Boyue Wang and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TKDE.2021.3125020},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3471-3483},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CaEGCN: Cross-attention fusion based enhanced graph convolutional network for clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anchors-based incremental embedding for growing knowledge
graphs. <em>TKDE</em>, <em>35</em>(4), 3458–3470. (<a
href="https://doi.org/10.1109/TKDE.2021.3136482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding aims to transform the entities and relations of triplets into the low-dimensional vectors. Previous methods are oriented towards the static knowledge graphs, in which all entities and relations are assumed to be known and only some unknown triplets need to be predicted. However, the real-world knowledge graphs can grow dynamically, and some new knowledge are often added. To embed the new knowledge into the space of original knowledge graph, the classic models have to perform the entire re-embedding with including the new and original knowledge. This causes heavy computational burden for embedding. To address this problem, this study proposes a new model of anchors-based incremental embedding (ABIE) to implement the dynamical embedding for the growing knowledge graph. According to ABIE, every knowledge graph has some key entities, called anchors, which can fix the embedding space of knowledge graph. When some new knowledge is added into the graph, only a few updated entities and relations are embedded into the embedding space with the help of anchors, and the entire re-embedding on the whole graph is not necessary. By this way, the computational burden of embedding caused by the growth of knowledge graph is reduced significantly.},
  archive      = {J_TKDE},
  author       = {Lijun Dong and Dongyang Zhao and Xiaoai Zhang and Xinchuan Li and Xiaojun Kang and Hong Yao},
  doi          = {10.1109/TKDE.2021.3136482},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3458-3470},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Anchors-based incremental embedding for growing knowledge graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). An effective clustering optimization method for
unsupervised linear discriminant analysis. <em>TKDE</em>,
<em>35</em>(4), 3444–3457. (<a
href="https://doi.org/10.1109/TKDE.2021.3124023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent work Unsupervised Linear Discriminant Analysis (Un-LDA) completes its clustering process during the alternating optimization by converting equivalently the objective and finally using the K-means algorithm. However, the K-means algorithm has its inherent drawbacks. It is hard for the K-means algorithm to deal well with some complex clustering cases where there are too many real clusters or non-convex clusters. In this paper, a novel clustering optimization method is presented to accomplish the clustering process in Un-LDA and the resulting method can be named Un-LDA(CD). Specifically, instead of the K-means algorithm, an elaborately designed coordinate descent algorithm is adopted to obtain the clusters after the objective function goes through a series of simple but deft equivalent conversions. Extensive experiments have demonstrated that the coordinate descent clustering solution for Un-LDA can outperform the original K-means based solution on the tested data sets especially those complex data sets with a pretty large number of real clusters.},
  archive      = {J_TKDE},
  author       = {Quan Wang and Fei Wang and Fuji Ren and Zhongheng Li and Feiping Nie},
  doi          = {10.1109/TKDE.2021.3124023},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3444-3457},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An effective clustering optimization method for unsupervised linear discriminant analysis},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective and efficient algorithm for k-means clustering
with new formulation. <em>TKDE</em>, <em>35</em>(4), 3433–3443. (<a
href="https://doi.org/10.1109/TKDE.2022.3155450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-means is one of the most simple and popular clustering algorithms, which implemented as a standard clustering method in most of machine learning researches. The goal of K-means clustering is finding a set of cluster centers and minimizing the sum of squared distances between each sample and its nearest clustering center. In this paper, we proposed a novel K-means clustering algorithm, which reformulate the classical K-Means objective function as a trace maximization problem and then replace it with a new formulation. The proposed algorithm does not need to calculate the cluster centers in each iteration and requires fewer additional intermediate variables during the optimization process. In addition, we proposed an efficient iterative re-weighted algorithm to solve the involved optimization problem and provided the corresponding convergence analysis. The proposed algorithm keeps a consistent computational complexity as Lloyd&#39;s algorithm, $\mathcal {O}(ndk)$ , but shows a faster convergence rate in experiments. Extensive experimental results on real world benchmark datasets show the effectiveness and efficiency of the proposed algorithm.},
  archive      = {J_TKDE},
  author       = {Feiping Nie and Ziheng Li and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2022.3155450},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3433-3443},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An effective and efficient algorithm for K-means clustering with new formulation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive clustering algorithm based on local-density
peaks for imbalanced data without parameters. <em>TKDE</em>,
<em>35</em>(4), 3419–3432. (<a
href="https://doi.org/10.1109/TKDE.2021.3138962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data clustering is a challenging problem in machine learning. The main difficulty is caused by the imbalance in both cluster size and data density distribution. To address this problem, we propose a novel clustering algorithm called LDPI based on local-density peaks in this study. First, an initial sub-cluster construction scheme is designed based on a 3-dimensional (3-D) decision graph that can easily detect the initial sub-cluster centers and identify the noise points. Second, a sub-cluster updating strategy is designed, which can automatically identify the false sub-cluster centers and update the initial sub-clusters. Third, a sub-cluster merging scheme is designed, which merges the updated initial sub-clusters into final clusters. Consequently, the proposed algorithm has three advantages: 1) It does not require any input parameters; 2) It can automatically determine the cluster centers and number of clusters; 3) It is suitable for imbalanced datasets and datasets with arbitrary shapes and distributions. The effectiveness of LDPI is demonstrated experimentally and the superiority of LDPI is identified by comparison with 5 state-of-the-art algorithms.},
  archive      = {J_TKDE},
  author       = {Wuning Tong and Yuping Wang and Delong Liu},
  doi          = {10.1109/TKDE.2021.3138962},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3419-3432},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An adaptive clustering algorithm based on local-density peaks for imbalanced data without parameters},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive, context-aware, and stacked attention
network-based recommendation system to capture users’ temporal
preference. <em>TKDE</em>, <em>35</em>(4), 3404–3418. (<a
href="https://doi.org/10.1109/TKDE.2022.3140387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems have become more important since the widespread use of the Internet. The large amount of information means that it is difficult for users to discover what they really need. However, users’ preferences can change over time because of the age and the impact of social networks. Recommendation systems must capture users’ preferences and recommend suitable items, which is a difficult challenge. This study proposes a novel recommendation system that adapts to the changing preferences of users. By learning and adapting to users’ changing preferences better recommendations are provided. This study uses context factors as additional information to model users’ preferences more accurately. For the proposed recommendation system, the context is based on users’ most recent interaction items. This study proposes two novel attention mechanisms for the recommendation system. The contextual item attention module captures contextual information, changing pattern in users’ preference and the importance of items. The multi-head attention module extends the diversity of users’ preferences and adapts to changing preferences. The recommendation performance is improved using additional item&#39;s temporal information to model the contextual item&#39;s representation. Experiments compare the proposed algorithm with several state-of-the-art recommendation methods using three real-world datasets. The experimental results demonstrate that the proposed context-aware recommendation model outperforms traditional methods and demonstrate the effectiveness with which contextual information is captured by the attention mechanism.},
  archive      = {J_TKDE},
  author       = {Jung-Hsien Chiang and Chung-Yao Ma and Chi-Shiang Wang and Pei-Yi Hao},
  doi          = {10.1109/TKDE.2022.3140387},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3404-3418},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An adaptive, context-aware, and stacked attention network-based recommendation system to capture users’ temporal preference},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AIM: Automatic interaction machine for click-through rate
prediction. <em>TKDE</em>, <em>35</em>(4), 3389–3403. (<a
href="https://doi.org/10.1109/TKDE.2021.3134985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature embedding learning and feature interaction modeling are two crucial components of deep models for Click-Through Rate (CTR) prediction in recommender systems. Most existing deep CTR models suffer from the following three problems. First, feature interactions are either manually designed or simply enumerated. However, not all the feature interactions are useful for the prediction task and useless feature interactions may introduce noisy signals thus causing overfitting. Second, all the feature interactions are modeled with an identical interaction function, whereas different interaction functions introduce different inductive biases to better capture various feature interaction patterns. Third, in most existing models, different features share the same embedding size. However, model size can be further optimized without sacrificing performance by differentiating embedding sizes for individual features, as the amount of information contained in each feature varies much. To address the three issues mentioned above, we propose Automatic Interaction Machine (AIM) with three core components, namely, Feature Interaction Search (FIS), Interaction Function Search (IFS) and Embedding Dimension Search (EDS), respectively. To tackle the first problem, FIS component automatically identifies different orders of essential feature interactions with useless ones pruned. Taking care of the second problem, IFS component selects appropriate interaction functions for each individual feature interaction in a learnable way. Moreover, to avoid learning conflict among different interaction functions, IFS proposes function-wise embeddings via performing multiple embeddings for each feature, where each feature embedding corresponds to one possible interaction function. However, utilizing multiple embeddings for each feature may make the model size affordably large if we keep the same embedding size as utilizing shared embedding (i.e., each feature shares the same embedding for different interaction functions). To solve this third problem, EDS automatically selects proper embedding size for each feature. Such a flexible embedding size adaptation is able to reduce the large amount of embedding parameters introduced by function-wise embeddings. Offline experiments on three large-scale datasets (two public benchmarks, one private dataset) validate that AIM can significantly improve various FM-based models. AIM has been deployed in the recommendation service of a mainstream app market, where a three-week online A/B test demonstrated the superiority of AIM, improving DeepFM model by 4.4\% in terms of CTR.},
  archive      = {J_TKDE},
  author       = {Chenxu Zhu and Bo Chen and Weinan Zhang and Jincai Lai and Ruiming Tang and Xiuqiang He and Zhenguo Li and Yong Yu},
  doi          = {10.1109/TKDE.2021.3134985},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3389-3403},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AIM: Automatic interaction machine for click-through rate prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Adversarial attacks against deep generative models on data:
A survey. <em>TKDE</em>, <em>35</em>(4), 3367–3388. (<a
href="https://doi.org/10.1109/TKDE.2021.3130903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models have gained much attention given their ability to generate data for applications as varied as healthcare to financial technology to surveillance, and many more - the most popular models being generative adversarial networks (GANs) and variational auto-encoders (VAEs). Yet, as with all machine learning models, ever is the concern over security breaches and privacy leaks and deep generative models are no exception. In fact, these models have advanced so rapidly in recent years that work on their security is still in its infancy. In an attempt to audit the current and future threats against these models, and to provide a roadmap for defense preparations in the short term, we prepared this comprehensive and specialized survey on the security and privacy preservation of GANs and VAEs. Our focus is on the inner connection between attacks and model architectures and, more specifically, on five components of deep generative models: the training data, the latent code, the generators/decoders of GANs/VAEs, the discriminators/encoders of GANs/VAEs, and the generated data. For each model, component and attack, we review the current research progress and identify the key challenges. The paper concludes with a discussion of possible future attacks and research directions in the field.},
  archive      = {J_TKDE},
  author       = {Hui Sun and Tianqing Zhu and Zhiqiu Zhang and Dawei Jin and Ping Xiong and Wanlei Zhou},
  doi          = {10.1109/TKDE.2021.3130903},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3367-3388},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adversarial attacks against deep generative models on data: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on federated learning systems: Vision, hype and
reality for data privacy and protection. <em>TKDE</em>, <em>35</em>(4),
3347–3366. (<a href="https://doi.org/10.1109/TKDE.2021.3124599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As data privacy increasingly becomes a critical societal concern, federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as PyTorch and TensorFlow that boost the development of deep learning, federated learning systems (FLSs) are equivalently important, and face challenges from various aspects such as effectiveness, efficiency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To understand the key design system components and guide future research, we introduce the definition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.},
  archive      = {J_TKDE},
  author       = {Qinbin Li and Zeyi Wen and Zhaomin Wu and Sixu Hu and Naibo Wang and Yuan Li and Xu Liu and Bingsheng He},
  doi          = {10.1109/TKDE.2021.3124599},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3347-3366},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on federated learning systems: Vision, hype and reality for data privacy and protection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A robust game-theoretical federated learning framework with
joint differential privacy. <em>TKDE</em>, <em>35</em>(4), 3333–3346.
(<a href="https://doi.org/10.1109/TKDE.2021.3140131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a promising distributed machine learning paradigm that has been playing a significant role in providing privacy-preserving learning solutions. However, alongside all its achievements, there are also limitations. First, traditional frameworks assume that all the clients are voluntary and so will want to participate in training only for improving the model’s accuracy. However, in reality, clients usually want to be adequately compensated for the data and resources they will use before participating. Second, today’s frameworks do not offer sufficient protection against malicious participants who try to skew a jointly trained model with poisoned updates. To address these concerns, we have developed a more robust federated learning scheme based on joint differential privacy. The framework provides two game-theoretic mechanisms to motivate clients to participate in training. These mechanisms are dominant-strategy truthful, individual rational, and budget-balanced. Further, the influence an adversarial client can have is quantified and restricted, and data privacy is similarly guaranteed in quantitative terms. Experiments with different training models on real-word datasets demonstrate the effectiveness of the proposed approach.},
  archive      = {J_TKDE},
  author       = {Lefeng Zhang and Tianqing Zhu and Ping Xiong and Wanlei Zhou and Philip S. Yu},
  doi          = {10.1109/TKDE.2021.3140131},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3333-3346},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A robust game-theoretical federated learning framework with joint differential privacy},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review on generative adversarial networks: Algorithms,
theory, and applications. <em>TKDE</em>, <em>35</em>(4), 3313–3332. (<a
href="https://doi.org/10.1109/TKDE.2021.3130191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) have recently become a hot research topic; however, they have been studied since 2014, and a large number of algorithms have been proposed. Nevertheless, few comprehensive studies explain the connections among different GAN variants and how they have evolved. In this paper, we attempt to provide a review of the various GAN methods from the perspectives of algorithms, theory, and applications. First, the motivations, mathematical representations, and structures of most GAN algorithms are introduced in detail, and we compare their commonalities and differences. Second, theoretical issues related to GANs are investigated. Finally, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, the medical field, and data science are discussed.},
  archive      = {J_TKDE},
  author       = {Jie Gui and Zhenan Sun and Yonggang Wen and Dacheng Tao and Jieping Ye},
  doi          = {10.1109/TKDE.2021.3130191},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3313-3332},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A review on generative adversarial networks: Algorithms, theory, and applications},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generative answer aggregation model for sentence-level
crowdsourcing tasks. <em>TKDE</em>, <em>35</em>(4), 3299–3312. (<a
href="https://doi.org/10.1109/TKDE.2022.3142821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous answer aggregation methods for sentence-level crowdsourcing tasks extract one sentence from a collection of redundant sentences. However, these extractive methods have a drawback in that they ignore the phenomenon of answer complementarity and the spammer dilemma in crowdsourcing. To alleviate this problem, in this paper, we generate new, comprehensive sentences that synthesize all redundant sentences. To achieve this goal, we design a sequence-to-sequence neural model composed of an encoder and decoder. Specifically, considering the complementarity phenomenon, the encoder synthesizes all the collected sentences into hidden states, which are then utilized by the decoder to generate the final sentence. Next, considering the spammer dilemma, we model the workers in the neural network to detect spammers. Furthermore, to train the neural model better, we construct pseudo-sentences to enrich the training data. The experimental results demonstrate that our method can efficiently aggregate redundant sentences and generate comprehensive sentences with increased quality.},
  archive      = {J_TKDE},
  author       = {Shaofei Wang and Depeng Dang},
  doi          = {10.1109/TKDE.2022.3142821},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3299-3312},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A generative answer aggregation model for sentence-level crowdsourcing tasks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general survey on attention mechanisms in deep learning.
<em>TKDE</em>, <em>35</em>(4), 3279–3298. (<a
href="https://doi.org/10.1109/TKDE.2021.3126456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention is an important mechanism that can be employed for a variety of deep learning models across many different domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The various attention mechanisms are explained by means of a framework consisting of a general attention model, uniform notation, and a comprehensive taxonomy of attention mechanisms. Furthermore, the various measures for evaluating attention models are reviewed, and methods to characterize the structure of attention models based on the proposed framework are discussed. Last, future work in the field of attention models is considered.},
  archive      = {J_TKDE},
  author       = {Gianni Brauwers and Flavius Frasincar},
  doi          = {10.1109/TKDE.2021.3126456},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3279-3298},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A general survey on attention mechanisms in deep learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep dual adversarial network for cross-domain
recommendation. <em>TKDE</em>, <em>35</em>(4), 3266–3278. (<a
href="https://doi.org/10.1109/TKDE.2021.3132953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sparsity is a common issue for most recommender systems and can severely degrade the usefulness of a system. One of the most successful solutions to this problem has been cross-domain recommender systems. These frameworks supplement the sparse data of the target domain with knowledge transferred from a source domain rich with data that is in some way related. However, there are three challenges that, if overcome, could significantly improve the quality and accuracy of cross-domain recommendation: 1) ensuring latent feature spaces of the users and items are both maximally matched; 2) taking consideration of user-item relationship and their interaction in modelling user preference; 3) enabling a two-way cross-domain recommendation that both the source and the target domains benefit from a knowledge exchange. Hence, in this paper, we propose a novel deep neural network called Dual Adversarial network for Cross-Domain Recommendation (DA-CDR). By training the shared encoders with a domain discriminator via dual adversarial learning, the latent feature spaces for both the users and items are maximally matched between the source and target domains. The domain-specific encoders are applied with an orthogonal constraint to ensure that any domain-specific features are properly extracted and work as supplement to the shared features. Allowing the two domains to collaboratively benefit from each other results in better recommendations for both domains. Extensive experiments with real-world datasets on six tasks demonstrate that DA-CDR significantly outperforms seven state-of-the-art baselines in terms of recommendation accuracy.},
  archive      = {J_TKDE},
  author       = {Qian Zhang and Wenhui Liao and Guangquan Zhang and Bo Yuan and Jie Lu},
  doi          = {10.1109/TKDE.2021.3132953},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {4},
  pages        = {3266-3278},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A deep dual adversarial network for cross-domain recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable identity-oriented speech retrieval. <em>TKDE</em>,
<em>35</em>(3), 3261–3265. (<a
href="https://doi.org/10.1109/TKDE.2021.3127520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevalence of voice devices in our daily life, speech data is accumulated at an unprecedented speed, forming an invaluable database for security surveillance and financial risk management. In these applications, a key task is given a querying speech snippet to retrieve all speech snippets that are uttered by the same speaker as the querying one, namely Identity-Oriented Speech Retrieval (IO-SR). In this paper, we propose an accuracy and scalable system for IO-SR, which seamlessly integrates speaker modeling and deep indexing techniques. Evaluations on an industrial dataset containing millions of speech snippets show that our system achieves superior performance compared with the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Chaotao Chen and Di Jiang and Jinhua Peng and Rongzhong Lian and Yawen Li and Chen Zhang and Lei Chen and Lixin Fan},
  doi          = {10.1109/TKDE.2021.3127520},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3261-3265},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Scalable identity-oriented speech retrieval},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When is early classification of time series meaningful?
<em>TKDE</em>, <em>35</em>(3), 3253–3260. (<a
href="https://doi.org/10.1109/TKDE.2021.3108580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its introduction two decades ago, there has been increasing interest in the problem of early classification of time series . This problem generalizes classic time series classification to ask if we can classify a time series subsequence with sufficient accuracy and confidence after seeing only some prefix of a target pattern. The idea is that the earlier classification would allow us to take immediate action, in a domain in which some practical interventions are possible. For example, that intervention might be sounding an alarm or applying the brakes in an automobile. In this work, we make a surprising claim. In spite of the fact that there are dozens of papers on early classification of time series, it is not clear that any of them could ever work in a real-world setting. The problem is not with the algorithms per se but with the vague and underspecified problem description. Essentially all algorithms make implicit and unwarranted assumptions about the problem that will ensure that they will be plagued by false positives and false negatives even if their results suggested that they could obtain near-perfect results. We will explain our findings with novel insights and experiments and offer recommendations to the community.},
  archive      = {J_TKDE},
  author       = {Renjie Wu and Audrey Der and Eamonn J. Keogh},
  doi          = {10.1109/TKDE.2021.3108580},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3253-3260},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {When is early classification of time series meaningful?},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). User-specific adaptive fine-tuning for cross-domain
recommendations. <em>TKDE</em>, <em>35</em>(3), 3239–3252. (<a
href="https://doi.org/10.1109/TKDE.2021.3119619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making accurate recommendations for cold-start users has been a longstanding and critical challenge for recommender systems (RS). Cross-domain recommendations (CDR) offer a solution to tackle such a cold-start problem when there is no sufficient data for the users who have rarely used the system. An effective approach in CDR is to leverage the knowledge (e.g., user representations) learned from a related but different domain and transfer it to the target domain. Fine-tuning works as an effective transfer learning technique for this objective, which adapts the parameters of a pre-trained model from the source domain to the target domain. However, current methods are mainly based on the global fine-tuning strategy: the decision of which layers of the pre-trained model to freeze or fine-tune is taken for all users in the target domain. In this paper, we argue that users in RS are personalized and should have their own fine-tuning policies for better preference transfer learning. As such, we propose a novel User-specific Adaptive Fine-tuning method (UAF), selecting which layers of the pre-trained network to fine-tune, on a per user basis. Specifically, we devise a policy network with three alternative strategies to automatically decide which layers to be fine-tuned and which layers to have their parameters frozen for each user. Extensive experiments show that the proposed UAF exhibits significantly better and more robust performance for user cold-start recommendation.},
  archive      = {J_TKDE},
  author       = {Lei Chen and Fajie Yuan and Jiaxi Yang and Xiangnan He and Chengming Li and Min Yang},
  doi          = {10.1109/TKDE.2021.3119619},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3239-3252},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {User-specific adaptive fine-tuning for cross-domain recommendations},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised optimized bipartite graph embedding.
<em>TKDE</em>, <em>35</em>(3), 3224–3238. (<a
href="https://doi.org/10.1109/TKDE.2021.3115775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding is a widely used method for dimensionality reduction due to its computational effectiveness. The quality of the graph and the efficiency of graph construction will directly affect the performance and the efficiency of the graph embedding methods. However, in the unsupervised graph embedding methods, the graph is not considered as an optimized graph since there is no label that can be used to construct this graph. In addition, the running of traditional graph embedding methods becomes very time-consuming on large-scale datasets due to the high computational cost in the step of graph construction. Aiming to solve these problems, we propose an unsupervised dimensionality reduction method based on bipartite graph, called Unsupervised Optimized Bipartite Graph Embedding (UOBGE). Representative anchors are first identified in the data. Then, we construct the bipartite graph between the projected samples and the projected anchors and the intrinsic graph connecting all the projected sample pairs with equal weights, which keep the local and global geometric structures of the data, respectively. Finally, the bipartite graph and the projection matrix are optimized simultaneously by introducing an alternating optimization procedure. Extensive experiments on several datasets demonstrate the effectiveness and efficiency of the proposed method.},
  archive      = {J_TKDE},
  author       = {Jianyong Zhu and Lihong Tao and Hui Yang and Feiping Nie},
  doi          = {10.1109/TKDE.2021.3115775},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3224-3238},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised optimized bipartite graph embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised dimensionality reduction based on fusing
multiple clustering results. <em>TKDE</em>, <em>35</em>(3), 3211–3223.
(<a href="https://doi.org/10.1109/TKDE.2021.3114204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of the classical dimensionality reduction methods can be unified into a graph-embedding-based framework. A fixed graph constructed in a high-dimensional space has been extensively employed in the graph-embedding-based dimensionality reduction methods. However, a fixed graph often cannot characterize the structure of high-dimensional data owing to the curse of dimensionality. To solve this problem, we combine graph construction and dimensionality reduction into a coherent framework. Thus, the constructed graph can be updated dynamically in dimensionality reduction. In the existing methods based on the coherent framework, graphs are usually constructed by a type of neighborhood relationship and single clustering result. This study proposes an unsupervised dimensionality reduction method guided by fusing multiple clustering results. In the proposed method, multiple clustering results are first obtained by the k-means algorithm, and then a graph is constructed using a weighted co-association matrix of fusing the clustering results to capture data distribution information. Based on the graph, we present an objective function of combining graph construction and dimensionality reduction to implement mutual guidance between them. Numerical experiments on real data sets illustrate that the proposed method achieves significant improvement over some representative and state-of-the-art unsupervised dimensionality reduction methods.},
  archive      = {J_TKDE},
  author       = {Wei Wei and Qin Yue and Kai Feng and Junbiao Cui and Jiye Liang},
  doi          = {10.1109/TKDE.2021.3114204},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3211-3223},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised dimensionality reduction based on fusing multiple clustering results},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unified and incremental SimRank: Index-free approximation
with scheduled principle. <em>TKDE</em>, <em>35</em>(3), 3195–3210. (<a
href="https://doi.org/10.1109/TKDE.2021.3111734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SimRank is a popular link-based similarity measure on graphs. It enables a variety of applications with different modes of querying (e.g., single-pair, single-source and all-pair modes). In this paper, we propose UISim, a unified and incremental framework for all SimRank modes based on a scheduled approximation principle. UISim processes queries with incremental and prioritized exploration of the entire computation space, and thus allows flexible tradeoff of time and accuracy. On the other hand, it creates and shares common “building blocks” for online computation without relying on indexes, and thus is efficient to handle both static and dynamic graphs. Our experiments on various real-world graphs show that to achieve the same accuracy, UISim runs faster than its respective state-of-the-art baselines, and scales well on larger graphs.},
  archive      = {J_TKDE},
  author       = {Fanwei Zhu and Yuan Fang and Kai Zhang and Kevin Chen-Chuan Chang and Hongtai Cao and Zhen Jiang and Minghui Wu},
  doi          = {10.1109/TKDE.2021.3111734},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3195-3210},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unified and incremental SimRank: Index-free approximation with scheduled principle},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding the long-term dynamics of mobile app usage
context via graph embedding. <em>TKDE</em>, <em>35</em>(3), 3180–3194.
(<a href="https://doi.org/10.1109/TKDE.2021.3110141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing diversity of mobile apps, users install many apps in their smartphones and often use several apps together to meet a specific requirement. Because of the evolution of user habits and app functions, the set of apps using at the same time, i.e., app usage context, may change over time, which represents the dynamic correlation of different apps and even the evolution trend of the whole app ecosystem. Therefore, understanding how an app’s usage context changes over time is very meaningful. In this paper, based on a seven-year app usage dataset, we explore the long-term app usage context dynamics and understand the underlying reasons and influence factors behind. Specifically, we build app co-occurrence graphs in different periods and learn app embeddings accordingly by leveraging graph embedding algorithm. We then measure the change of app usage context by the distance between neighboring app embeddings. As for the whole app ecosystem, we find that the change rate of app usage context undergoes up and down phrases, and varies in different app-categories. Furthermore, we explore three influence factors correlated with such dynamics. These results will be helpful for stakeholders to better understand the evolution of mobile users&#39; app usage behavior.},
  archive      = {J_TKDE},
  author       = {Yali Fan and Zhen Tu and Tong Li and Hancheng Cao and Tong Xia and Yong Li and Xiang Chen and Lin Zhang},
  doi          = {10.1109/TKDE.2021.3110141},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3180-3194},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Understanding the long-term dynamics of mobile app usage context via graph embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023f). Towards a robust deep neural network against adversarial
texts: A survey. <em>TKDE</em>, <em>35</em>(3), 3159–3179. (<a
href="https://doi.org/10.1109/TKDE.2021.3117608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved remarkable success in various tasks (e.g., image classification, speech recognition, and natural language processing (NLP)). However, researchers have demonstrated that DNN-based models are vulnerable to adversarial examples, which cause erroneous predictions by adding imperceptible perturbations into legitimate inputs. Recently, studies have revealed adversarial examples in the text domain, which could effectively evade various DNN-based text analyzers and further bring the threats of the proliferation of disinformation. In this paper, we give a comprehensive survey on the existing studies of adversarial techniques for generating adversarial texts written by both English and Chinese characters and the corresponding defense methods. More importantly, we hope that our work could inspire future studies to develop more robust DNN-based text analyzers against known and unknown adversarial techniques. We classify the existing adversarial techniques for crafting adversarial texts based on the perturbation units, helping to better understand the generation of adversarial texts and build robust models for defense. In presenting the taxonomy of adversarial attacks and defenses in the text domain, we introduce the adversarial techniques from the perspective of different NLP tasks. Finally, we discuss the existing challenges of adversarial attacks and defenses in texts and present the future research directions in this emerging and challenging field.},
  archive      = {J_TKDE},
  author       = {Wenqi Wang and Run Wang and Lina Wang and Zhibo Wang and Aoshuang Ye},
  doi          = {10.1109/TKDE.2021.3117608},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3159-3179},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards a robust deep neural network against adversarial texts: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal link prediction with motifs for social networks.
<em>TKDE</em>, <em>35</em>(3), 3145–3158. (<a
href="https://doi.org/10.1109/TKDE.2021.3108513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction has attracted considerable attention. Empiricism and the evolution mechanism based approach are the mainstream methods for link prediction. However, one drawback of such approaches is that they usually ignore the dynamic evolution mechanism of social networks, yet being dynamic is an essential characteristic of a social network that exists in every stage of the network&#39;s evolution. In this paper, we address the problem of temporal link prediction and investigate social networks from the time dimension with the purpose of dynamic evolution mechanism capturing. First, we separate a temporal network into a series of snapshots. Then, we propose a triad transition matrix prediction algorithm to learn the change of the distribution of triads among the different snapshots. The learned changes in the distribution of triads can capture the dynamic evolution of the network. With a proposed triad transition influence quantification algorithm, we propose a motifs based link prediction method for temporal link prediction. The proposed method can capture the dynamic evolution of temporal networks and is universal than existing methods. Extensive experiments on disparate real-world networks and model networks with controllable evolution demonstrate the effectiveness of the proposed method.},
  archive      = {J_TKDE},
  author       = {Zhenyu Qiu and Jia Wu and Wenbin Hu and Bo Du and Guocai Yuan and Philip S. Yu},
  doi          = {10.1109/TKDE.2021.3108513},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3145-3158},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Temporal link prediction with motifs for social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spark rough hypercuboid approach for scalable feature
selection. <em>TKDE</em>, <em>35</em>(3), 3130–3144. (<a
href="https://doi.org/10.1109/TKDE.2021.3112520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection refers to choose an optimal non-redundant feature subset with minimal degradation of learning performance and maximal avoidance of data overfitting. The appearance of large data explosion leads to the sequential execution of algorithms are extremely time-consuming, which necessitates the scalable parallelization of algorithms by efficiently exploiting the distributed computational capabilities. In this paper, we present parallel feature selection algorithms underpinned by a rough hypercuboid approach in order to scale for the growing data volumes. Metrics in terms of rough hypercuboid are highly suitable to parallel distributed processing, and fits well with the Apache Spark cluster computing paradigm. Two data parallelism strategies, namely, vertical partitioning and horizontal partitioning, are implemented respectively to decompose the data into concurrent iterative computing streams. Experimental results on representative datasets show that our algorithms significantly faster than its original sequential counterpart while guaranteeing the quality of the results. Furthermore, the proposed algorithms are perfectly capable of exploiting the distributed-memory clusters to accomplish the computation task that fails on a single node due to the memory constraints. Parallel scalability and extensibility analysis have confirmed that our parallelization extends well to process massive amount of data and can scales well with the increase of computational nodes.},
  archive      = {J_TKDE},
  author       = {Chuan Luo and Sizhao Wang and Tianrui Li and Hongmei Chen and Jiancheng Lv and Zhang Yi},
  doi          = {10.1109/TKDE.2021.3112520},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3130-3144},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spark rough hypercuboid approach for scalable feature selection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Skyline group queries in large road-social networks
revisited. <em>TKDE</em>, <em>35</em>(3), 3115–3129. (<a
href="https://doi.org/10.1109/TKDE.2021.3111868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a network with social and spatial information, cohesive group queries aim to find a group of strongly connected and closely co-located users. Most existing studies limit to finding groups with either the strongest social ties under certain spatial constraints or the minimum spatial distance under certain social constraints. It is difficult for users to decide which constraints they need to choose and how to prioritize the constraints to meet their real requirements since the social constraint and spatial constraint are different in nature. In this article, we take a new approach to consider the constraints equally and study a skyline query. Specifically, given a road-social network consisting of a road network $G_r$ and a location-based social network $G_s$ , we aim to find a set of skyline cohesive groups, in which each group cannot be dominated by any other group in terms of social cohesiveness and spatial cohesiveness. The social cohesiveness is modeled by $(k, c)$ -core/truss (a $k$ -core/truss of size $c$ ), and the spatial cohesiveness is evaluated by the travel cost to a meeting point from group members. Such skyline problem is NP-hard as we need to explore the combinations of $c$ vertices to check whether it is a qualified $(k,c)$ -core/truss. In this article, we first provide exact solutions by developing efficient pruning strategies to filter out a large number of combinations that cannot form a $(k,c)$ -core/truss, and then propose highly efficient greedy solutions based on newly designed index ${\mathsf {cd}}$ - ${\mathsf {tree}}$ / ${\mathsf {td}}$ - ${\mathsf {tree}}$ to keep the distance on the road network and social structural information simultaneously. Experimental results show that our exact methods run faster than the baseline methods by 2-4 orders of magnitude in general, and our index-based greedy methods can significantly reduce the computation cost by 1-4 orders of magnitude while the extra travel cost is less than 5\% compared to the exact method on multiple real road-social networks.},
  archive      = {J_TKDE},
  author       = {Qiyan Li and Yuanyuan Zhu and Junhao Ye and Jeffrey Xu Yu},
  doi          = {10.1109/TKDE.2021.3111868},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3115-3129},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Skyline group queries in large road-social networks revisited},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-index model tree. <em>TKDE</em>, <em>35</em>(3),
3101–3114. (<a href="https://doi.org/10.1109/TKDE.2021.3126615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel single-index model tree (SIMTree) is proposed. It adopts the recursive partitioning strategy and each data segment is modeled by a single-index model (SIM), which is a flexible extension of linear regression with non-parametric link functions. The proposed SIMTree has two major advantages: a) with only a few leaf nodes, it can achieve competitive predictive performance compared to complicated black-box models; b) SIMs fitted on each local data segment are intrinsically interpretable. However, using conventional techniques to build such a SIMTree can be extremely time-consuming. SIM estimation typically involves iterative optimization via Newton-type algorithms; such a resource-intensive estimation procedure is repeatedly used for fitting leaf node SIMs and the search of optimal splits. To make the computation burden affordable, an effective training algorithm is proposed as enabled by the efficient utilization of Stein&#39;s lemma and several accelerating strategies in the tree construction algorithm. Moreover, a new Python package simtree is developed with elegant visualization modules that can further facilitate the model interpretation. Numerical results on extensive regression datasets show that SIMTree is an accurate and interpretable machine learning model.},
  archive      = {J_TKDE},
  author       = {Agus Sudjianto and Zebin Yang and Aijun Zhang},
  doi          = {10.1109/TKDE.2021.3126615},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3101-3114},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Single-index model tree},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Signed network representation by preserving multi-order
signed proximity. <em>TKDE</em>, <em>35</em>(3), 3087–3100. (<a
href="https://doi.org/10.1109/TKDE.2021.3125148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signed network representation is a key problem for signed network data. Previous studies have shown that by preserving multi-order s igned p roximity (SP), expressive node representations can be learned. However, multi-order SP cannot be perfectly encoded using limited samples extracted from random walks, which reduces effectiveness. To perfectly encode multi-order SP, we have innovatively integrated the informativeness of infinite samples to construct high-level summaries of multi-order SP without explicit sampling. Based on these summaries, we propose a method called SPMF , in which node representations are obtained using low-rank matrix approximation. Furthermore, we theoretically investigate the rationality of SPMF by examining its relationship with a powerful representation learning architecture. In sign inference and link prediction tasks with several real-world datasets, SPMF is empirically competitive compared with state-of-the-art methods. Additionally, two tricks are designed for improving the scalability of SPMF . One trick aims to filter out less informative summaries, and another one is inspired by kernel techniques. Both tricks empirically improve scalability while preserving effective performance. The code for our methods is publicly available.},
  archive      = {J_TKDE},
  author       = {Pinghua Xu and Wenbin Hu and Jia Wu and Weiwei Liu and Yang Yang and Philip S. Yu},
  doi          = {10.1109/TKDE.2021.3125148},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3087-3100},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Signed network representation by preserving multi-order signed proximity},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequence labeling with meta-learning. <em>TKDE</em>,
<em>35</em>(3), 3072–3086. (<a
href="https://doi.org/10.1109/TKDE.2021.3118469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent neural architectures in sequence labeling have yielded state-of-the-art performance on single domain data such as newswires. However, they still suffer from (i) requiring massive amounts of training data to avoid overfitting; (ii) huge performance degradation when there is a domain shift in the data distribution between training and testing. To make a sequence labeling system more broadly useful, it is crucial to reduce its training data requirements and transfer knowledge to other domains. In this paper, we investigate the problem of domain adaptation for sequence labeling under homogeneous and heterogeneous settings. We propose MetaSeq , a novel meta-learning approach for domain adaptation in sequence labeling. Specifically, MetaSeq incorporates meta-learning and adversarial training strategies to encourage robust, general and transferable representations for sequence labeling. The key advantage of MetaSeq is that it is capable of adapting to new unseen domains with a small amount of annotated data from those domains. We extensively evaluate MetaSeq on named entity recognition, part-of-speech tagging and slot filling under homogeneous and heterogeneous settings. The experimental results show that MetaSeq achieves state-of-the-art performance against eight baselines. Impressively, MetaSeq surpasses the in-domain performance using only 16.17\% and 7\% of target domain data on average for homogeneous settings, and 34.76\%, 24\%, 22.5\% of target domain data on average for heterogeneous settings.},
  archive      = {J_TKDE},
  author       = {Jing Li and Peng Han and Xiangnan Ren and Jilin Hu and Lisi Chen and Shuo Shang},
  doi          = {10.1109/TKDE.2021.3118469},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3072-3086},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Sequence labeling with meta-learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secure your ride: Real-time matching success rate prediction
for passenger-driver pairs. <em>TKDE</em>, <em>35</em>(3), 3059–3071.
(<a href="https://doi.org/10.1109/TKDE.2021.3112739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, online ride-hailing platforms, such as Uber and Didi, have become an indispensable part of urban transportation and make our lives more convenient. After a passenger is matched up with a driver by the platform, both the passenger and the driver have the freedom to simply accept or cancel a ride with one click. Hence, accurately predicting whether a passenger-driver pair is a good match, i.e., its matching success rate (MSR), turns out to be crucial for ride-hailing platforms to devise instant strategies such as order assignment. However, since the users of ride-hailing platforms consist of two parties, decision-making needs to simultaneously account for the dynamics from both the driver and the passenger sides. This makes it more challenging than traditional online advertising tasks that predict a user&#39;s response towards an object, e.g., click-through rate prediction for advertisements. Moreover, the amount of available data is severely imbalanced across different cities, creating difficulties for training an accurate model for smaller cities with scarce data. Though a sophisticated neural network architecture can help improve the prediction accuracy under data scarcity, the overly complex design will impede the model&#39;s capacity of delivering timely predictions in a production environment. In the paper, to accurately predict the MSR of passenger-driver, we propose the M ulti- V iew model ( MV ) which comprehensively learns the interactions among the dynamic features of the passenger, driver, trip order, as well as the context. Regarding the data imbalance problem, we further design the K nowledge D istillation framework ( KD ) to supplement the model&#39;s predictive power for smaller cities using the knowledge from cities with denser data, and also generate a simple model to support efficient deployment. Finally, we conduct extensive experiments on real-world datasets from several different cities, which demonstrates the superiority of our solution.},
  archive      = {J_TKDE},
  author       = {Yuandong Wang and Hongzhi Yin and Lian Wu and Tong Chen and Chunyang Liu},
  doi          = {10.1109/TKDE.2021.3112739},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3059-3071},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Secure your ride: Real-time matching success rate prediction for passenger-driver pairs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ROVEC: Runtime optimization of vectorized expression
evaluation for column store. <em>TKDE</em>, <em>35</em>(3), 3045–3058.
(<a href="https://doi.org/10.1109/TKDE.2021.3124669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increasing demand for scalable and interactive data analytics, column stores have become the de-facto choice in many analytical databases. As a common and fundamental operation in column stores, expression evaluation has a remarkable effect on many queries. To speed up expression evaluation, vectorized techniques such as S ingle- I nstruction- M ultiple- D ata (SIMD) instructions are widely used. However, there are few works concerning dedicated optimizations for SIMD-based expression evaluation for column stores. In this paper, we propose a runtime optimization framework named ROVEC that enables effective optimizations for SIMD-based expression evaluation. The key idea is to optimize logical expression at execution time, by leveraging lightweight compression and fine-grained statistics associated with the compressed data. ROVEC removes unnecessary type casting and finds the tightest type during evaluation, which maximizes the concurrent operands in SIMD instructions. ROVEC can be applied to many expression-evaluation-intensive operators (e.g., table scan and theta join) for different data types (e.g., numeric, time and string). To validate the effectiveness of ROVEC, we integrate it into a columnar database PolarDB-C. Our evaluation results show that ROVEC improves up to $125\%$ ( $60\%$ on average) throughput of table scan and up to $50\%$ ( $30\%$ on average) latency of theta join.},
  archive      = {J_TKDE},
  author       = {Meng Li and Zheyu Miao and Di Wu and Feifei Li and Sheng Wang and Wei Cao and Zhi Qiao and Yubin Ruan and Yukun Liang and Jimmy Yang and Haipeng Dai and Guihai Chen},
  doi          = {10.1109/TKDE.2021.3124669},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3045-3058},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ROVEC: Runtime optimization of vectorized expression evaluation for column store},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust unsupervised feature selection via multi-group
adaptive graph representation. <em>TKDE</em>, <em>35</em>(3), 3030–3044.
(<a href="https://doi.org/10.1109/TKDE.2021.3124255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection can play an important role in addressing the issue of processing massive unlabelled high-dimensional data in the domain of machine learning and data mining. This paper presents a novel unsupervised feature selection method, referred to as Multi-Group Adaptive Graph Representation (MGAGR). Different from existing methods, the relationship between features is explored via the global similarity matrix, which is reconstructed by local similarities of multiple groups. Specifically, the similarity of a feature compared to other features can be represented by the linear combination of all the local similarities. The local similarity of a representative group is given a large weight to reconstruct the global similarity. Besides, an iterative algorithm is given to solve the optimization problem, in which the global similarity matrix, its corresponding reconstruction weights and the self-representation matrix are iteratively improved. Experimental results on 8 benchmark datasets demonstrates that the proposed method outperforms the state-of-the-art unsupervised feature selection methods in terms of clustering performance. The source code is available at: https://github.com/misteru/MGAGR .},
  archive      = {J_TKDE},
  author       = {Mengbo You and Aihong Yuan and Min Zou and Dongjian He and Xuelong Li},
  doi          = {10.1109/TKDE.2021.3124255},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3030-3044},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust unsupervised feature selection via multi-group adaptive graph representation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust local preserving and global aligning network for
adversarial domain adaptation. <em>TKDE</em>, <em>35</em>(3), 3014–3029.
(<a href="https://doi.org/10.1109/TKDE.2021.3112815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) requires source domain samples with clean ground truth labels during training. Accurately labeling a large number of source domain samples is time-consuming and laborious. An alternative is to utilize samples with noisy labels for training. However, training with noisy labels can greatly reduce the performance of UDA. In this paper, we address the problem that learning UDA models only with access to noisy labels and propose a novel method called robust local preserving and global aligning network (RLPGA). RLPGA improves the robustness of the label noise from two aspects. One is learning a classifier by a robust informative-theoretic-based loss function. The other is constructing two adjacency weight matrices and two negative weight matrices by the proposed local preserving module to preserve the local topology structures of input data. We conduct theoretical analysis on the robustness of the proposed RLPGA and prove that the robust informative-theoretic-based loss and the local preserving module are beneficial to reduce the empirical risk of the target domain. A series of empirical studies show the effectiveness of our proposed RLPGA.},
  archive      = {J_TKDE},
  author       = {Wenwen Qiang and Jiangmeng Li and Changwen Zheng and Bing Su and Hui Xiong},
  doi          = {10.1109/TKDE.2021.3112815},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3014-3029},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust local preserving and global aligning network for adversarial domain adaptation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust fuzzy k-means clustering with shrunk patterns
learning. <em>TKDE</em>, <em>35</em>(3), 3001–3013. (<a
href="https://doi.org/10.1109/TKDE.2021.3116257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy K-Means (FKM) clustering regards each cluster as a fuzzy set and assigns each sample to multiple clusters with a certain degree of membership. However, conventional FKM methods perform clustering on the basis of original data directly where the intrinsic structure of data may be corrupted by the noise or other factors. According, the performance of these methods would be challenged. In this paper, we present a novel fuzzy K-Means clustering model to conduct clustering tasks on the flexible manifold. Technically, we perform fuzzy clustering based on the shrunk patterns which have desired manifold structure. The shrunk patterns can be viewed as an approximation to the original data; and a penalty term is employed to measure the mismatch between them. Moreover, we integrate the learning of shrunk patterns and the learning of membership degree between shrunk patterns and clusters into a unified framework. Consider traditional projected FKM methods usually project samples into a linear subspace, which is overstrict. We further extend the proposed model for projected FKM clustering to find a suitable subspace to fit the non-linear manifold structure of data, reduce the interference of the noise and redundant features and gather homogeneous samples together simultaneously. Two alternating iterative algorithms are derived to solve these models, respectively. Extensive experimental results demonstrate the feasibility and effectiveness of our proposed clustering algorithms.},
  archive      = {J_TKDE},
  author       = {Xiaowei Zhao and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2021.3116257},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {3001-3013},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust fuzzy K-means clustering with shrunk patterns learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). QUINT: Node embedding using network hashing. <em>TKDE</em>,
<em>35</em>(3), 2987–3000. (<a
href="https://doi.org/10.1109/TKDE.2021.3111997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning using network embedding has received tremendous attention due to its efficacy to solve downstream tasks. Popular embedding methods (such as deepwalk , node2vec , LINE ) are based on a neural architecture, thus unable to scale on large networks both in terms of time and space usage. Recently, we proposed $\mathrm{BinSketch}$ , a sketching technique for compressing binary vectors to binary vectors. In this paper, we show how to extend $\mathrm{BinSketch}$ and use it for network hashing. Our proposal named QUINT is built upon $\mathrm{BinSketch}$ , and it embeds nodes of a sparse network onto a low-dimensional space using simple bit-wise operations. QUINT is the first of its kind that provides tremendous gain in terms of speed and space usage without compromising much on the accuracy of the downstream tasks. Extensive experiments are conducted to compare QUINT with seven state-of-the-art network embedding methods for two end tasks – link prediction and node classification. We observe huge performance gain for QUINT in terms of speedup (up to $7000\times$ ) and space saving (up to $800\times$ ) due to its bit-wise nature to obtain node embedding. Moreover, QUINT is a consistent top-performer for both the tasks among the baselines across all the datasets. Our empirical observations are backed by rigorous theoretical analysis to justify the effectiveness of QUINT . In particular, we prove that QUINT retains enough structural information which can be used further to approximate many topological properties of networks with high confidence.},
  archive      = {J_TKDE},
  author       = {Debajyoti Bera and Rameshwar Pratap and Bhisham Dev Verma and Biswadeep Sen and Tanmoy Chakraborty},
  doi          = {10.1109/TKDE.2021.3111997},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2987-3000},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {QUINT: Node embedding using network hashing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving multi-granular federated neural
architecture search – a general framework. <em>TKDE</em>,
<em>35</em>(3), 2975–2986. (<a
href="https://doi.org/10.1109/TKDE.2021.3116248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jointly learning from multiple datasets can help building versatile intelligent systems yet may give rise to serious concerns of data privacy and model selection. Specifically, on the one hand, these datasets can be distributed at various local clients, who may not be willing or do not ought to share data with each other. On the other hand, it is unrealistic to choose a model architecture that can well suit the disparate patterns and distributions carried by the various datasets in a priori. Whereas many works in federated learning [1] and neural architecture search [2] have been proposed to address one of the two concerns, very few have attempted the both. To close the gap, in this paper we deliver a framework, termed Multi-Granular Federated Neural Architecture Search (MGFNAS), to enable the automation of model architecture search in a federated and thus privacy-preserved setting. We argue that our MGFNAS framework is general in the sense that it does not impose any restriction on the search space or strategy, such that most existing neural architecture search techniques can be readily implemented in. The main idea of our framework is to search the optimal neural network architecture in two levels of granularity, enabling the neural-operator-based micro-level search and the cell-based macro-level search. The main challenge of implementing our framework lies in the fact that, due to the decentralized nature, the local architectures searched by multiple clients can differ drastically in order to fit their own datasets, while a general method to form the global model by aggregating the local architectures in both micro and macro levels is missing. To solve the issue, we propose a novel aggregation function, named Network Architecture Probabilistic Aggregation (NAPA). The key idea of our NAPA function is to treat the network architectures as graphs, of which the sub-graph structures being frequently appeared across multiple clients are modeled by probabilistic distributions. At each round, a global model is formed by sampling from those distributions in an exploration-exploitation fashion. Extensive experiments are carried out, and the results substantiate the viability and effectiveness of our proposed framework.},
  archive      = {J_TKDE},
  author       = {Zijie Pan and Li Hu and Weixuan Tang and Jin Li and Yi He and Zheli Liu},
  doi          = {10.1109/TKDE.2021.3116248},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2975-2986},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Privacy-preserving multi-granular federated neural architecture search – a general framework},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving collaborative filtering using fully
homomorphic encryption. <em>TKDE</em>, <em>35</em>(3), 2961–2974. (<a
href="https://doi.org/10.1109/TKDE.2021.3115776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protecting the privacy of users is one of the most important issues in recommender systems, where new items, e.g., books, movies, and friends in online social networking service/sites, are recommended to target users. To identify recommended items, encryption-based privacy-preserving collaborative filtering is widely used to generate recommendations. However, existing solutions are either slow or not scalable. To tackle this issue, in this paper, we first propose a privacy-preserving user-based CF protocol using the BGV fully homomorphic encryption scheme, which is named BGV-CF. By reducing interactions and the amount of communication traffic among users and recommendation servers, the proposed BGV-CF protocol significantly facilitates the recommendation process. Then, we propose an optimized BGV-CF (OBGV-CF) protocol where some computations are offloaded to users during the recommendation process. The security of the proposed schemes is qualitatively analyzed and quantitative analyses of the computation and communication costs are performed. In addition, provable security analysis using random oracles is provided. The BGV-CF and OBGV-CF protocols are implemented using C++, and testbeds using the MovieLens dataset are conducted. Experimental results demonstrate that the proposed BGV-CF and OBGV-CF successfully achieve their design goals.},
  archive      = {J_TKDE},
  author       = {Seiya Jumonji and Kazuya Sakai and Min-Te Sun and Wei-Shinn Ku},
  doi          = {10.1109/TKDE.2021.3115776},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2961-2974},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Privacy-preserving collaborative filtering using fully homomorphic encryption},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical high-order entropy-compressed text self-indexing.
<em>TKDE</em>, <em>35</em>(3), 2943–2960. (<a
href="https://doi.org/10.1109/TKDE.2021.3114401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed self-indexes are used widely in string processing applications, such as information retrieval, genome analysis, data mining, and web searching. The index not only indexes the data, but also encodes the data, and it is in compressed form. Moreover, the index and the data it encodes can be operated upon directly, without need to uncompress the entire index, thus saving time while maintaining small storage space. In some applications, such as in genome analysis, existing methods do not exploit the full possibilities of compressed self-indexes, and thus we seek faster and more space-efficient indexes. In this paper, we propose a practical high-order entropy-compressed self-index for efficient pattern matching in a text. We give practical implementations of compressed suffix arrays using a hybrid encoding in the representation of the neighbor function $\Phi$ . We analyze the performance in theory and practice of our recommended indexing method, called ${{\sf GeCSA}}$ . We can improve retrieval time further using an iterated version of the neighbor function. Experimental results on the tested data demonstrate that the proposed index ${{\sf GeCSA}}$ has good overall advantages in space usage and retrieval time over the state-of-the-art indexing methods, especially on the repetitive data.},
  archive      = {J_TKDE},
  author       = {Hongwei Huo and Peng Long and Jeffrey Scott Vitter},
  doi          = {10.1109/TKDE.2021.3114401},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2943-2960},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Practical high-order entropy-compressed text self-indexing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Platform-oriented event time allocation. <em>TKDE</em>,
<em>35</em>(3), 2930–2942. (<a
href="https://doi.org/10.1109/TKDE.2021.3109838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Event-based social networks (EBSNs), such as Meetup and Whova, which provide platforms for users to publish, arrange and participate in events, have become increasingly popular. A major challenge for managing EBSNs is to generate the most satisfactory event arrangement, i.e., events are scheduled at the reasonable time to attract maximum number of participants. Existing approaches usually focus on assigning a set of events organized to time intervals, but ignore the competitive relationships among different event organizers, which will lead to event time allocations unacceptable to organizers. Thus, a more intelligent EBSNs platform that allocates social events properly in a global view (i.e., the perspective of platform) is desired. In this paper, we first formally define the problem of P latform-oriented E vent T ime A llocation (PETA), which contains two parts: the prediction of event feasible time period and the event time allocation. Unfortunately, we find that the PETA problem is NP-hard due to the global conflict constraints on events. Thus, we propose a method to calculate event feasible time period based on event time prediction, and then design a greedy algorithm and two approximation algorithms to solve the PETA problem. Finally, we conduct extensive experiments on both real and synthetic datasets to test the effectiveness and efficiency of the proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Heli Sun and Ning Wang and Jingyu Jia and Jianbin Huang and Hui Xiong and Liang He and Xinwang Liu and Shan Zhang and Shaojie Qiao and Jizhong Zhao},
  doi          = {10.1109/TKDE.2021.3109838},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2930-2942},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Platform-oriented event time allocation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing random walk based statistical estimation over
graphs via bootstrapping. <em>TKDE</em>, <em>35</em>(3), 2916–2929. (<a
href="https://doi.org/10.1109/TKDE.2021.3126906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are commonly used in various applications such as online social networks (OSNs), E-commerce systems and social recommender systems. Random walk sampling is often used to conduct statistical estimation over such graphs. This paper develops an algorithmic framework to reduce the mean square error of such statistical estimation. Our algorithmic framework is inspired by that the mean square error can be decomposed into a sum of the bias and variance of the estimator. More specifically, we apply the bootstrapping technique to design a bias reduction algorithm. Our bias reduction algorithm only utilizes a small number of “valid” sub-samples, which can reduce more bias of the estimator but may increase the variance of the estimator significantly. We use multiple parallel random walks to reduce this variance such that it can be reduced to arbitrarily small by deploying a sufficient number of random walks. We provide theoretical guarantees and computational complexity analysis of our proposed bias reduction algorithms. Our algorithmic framework enables one to attain different trade-offs between the sample complexity (i.e., number of parallel random walks) and the mean square error of the statistical estimation. Also, the proposed bias reduction algorithm is generic and can be applied to optimize a large class of random walk sampling algorithms. To demonstrate the versatility of the framework, we apply it to optimize the Metropolis random walk and simple random walk sampling. Extensive experiments on four public datasets confirm the effectiveness and computational efficiency of our proposed algorithmic framework under the mean square metric and beyond.},
  archive      = {J_TKDE},
  author       = {Hong Xie and Pei Yi and Yongkun Li and John C. S. Lui},
  doi          = {10.1109/TKDE.2021.3126906},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2916-2929},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Optimizing random walk based statistical estimation over graphs via bootstrapping},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online multi-label streaming feature selection with label
correlation. <em>TKDE</em>, <em>35</em>(3), 2901–2915. (<a
href="https://doi.org/10.1109/TKDE.2021.3113514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label streaming feature selection has attracted extensive attention in diverse big data applications. However, most existing works focused on the scenarios where labels are independent, while ignoring the real scenarios that they may be interdependent and correlated with each other. This paper aims to fill this gap by developing a novel online multi-label streaming feature selection scheme by taking into account the existence of label correlation, known as (OMSFS LC ). In our design, we first calculate the correlation degree between labels to obtain the label weight. Then, we integrate the mutual information and the label weight to evaluate the correlation between features and labels. In particular, it consists of three stages: 1) online significance analysis, which can determine the significant features via the correlation degree between the newly arriving features and labels; 2) online relevance analysis, which can obtain relevant features via the mutual information; and 3) online redundancy analysis, which can filter the redundant features for removal via pairwise comparison. We implement our solution and conduct extensive experiments on benchmark datasets for performance evaluations. The experimental results exhibit that OMSFS LC significantly outperforms the state-of-the-art methods in terms of effectiveness and efficiency.},
  archive      = {J_TKDE},
  author       = {Dianlong You and Yang Wang and Jiawei Xiao and Yaojin Lin and Maosheng Pan and Zhen Chen and Limin Shen and Xindong Wu},
  doi          = {10.1109/TKDE.2021.3113514},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2901-2915},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online multi-label streaming feature selection with label correlation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the statistical significance of a community structure.
<em>TKDE</em>, <em>35</em>(3), 2887–2900. (<a
href="https://doi.org/10.1109/TKDE.2021.3125330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The community structure typically refers to the existence of a network partition in terms of a set of non-overlapping dense sub-graphs, where each sub-graph is called a community and there are few links between different communities. The detection of community structure is able to provide additional knowledge on the organization mechanism of the network and its characteristics. Despite decades of developments in community detection algorithms, how to determine whether a given community structure is true or not in a statistically sound manner still remains unresolved. In this paper, we derive an analytical upper bound on the $p$ -value of a community structure under the configuration model. To demonstrate its effectiveness on community structure validation, we further develop a community detection algorithm in which the $p$ -value upper bound is used as the objective function. Experimental results on both real networks and simulated networks show that our algorithm outperforms prior state-of-the-art community detection methods.},
  archive      = {J_TKDE},
  author       = {Zengyou He and Xiaoqi Wei and Wenfang Chen and Yan Liu},
  doi          = {10.1109/TKDE.2021.3125330},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2887-2900},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On the statistical significance of a community structure},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the fairness of time-critical influence maximization in
social networks. <em>TKDE</em>, <em>35</em>(3), 2875–2886. (<a
href="https://doi.org/10.1109/TKDE.2021.3120561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization has found applications in a wide range of real-world problems, for instance, viral marketing of products in an online social network, and propagation of valuable information such as job vacancy advertisements. While existing algorithmic techniques usually aim at maximizing the total number of people influenced, the population often comprises several socially salient groups, e.g., based on gender or race. As a result, these techniques could lead to disparity across different groups in receiving important information. Furthermore, in many applications, the spread of influence is time-critical, i.e., it is only beneficial to be influenced before a deadline. As we show in this paper, such time-criticality of information could further exacerbate the disparity of influence across groups. This disparity could have far-reaching consequences, impacting people’s prosperity and putting minority groups at a big disadvantage. In this work, we propose a notion of group fairness in time-critical influence maximization . We introduce surrogate objective functions to solve the influence maximization problem under fairness considerations. By exploiting the submodularity structure of our objectives, we provide computationally efficient algorithms with guarantees that are effective in enforcing fairness during the propagation process. Extensive experiments on synthetic and real-world datasets demonstrate the efficacy of our proposal.},
  archive      = {J_TKDE},
  author       = {Junaid Ali and Mahmoudreza Babaei and Abhijnan Chakraborty and Baharan Mirzasoleiman and Krishna P. Gummadi and Adish Singla},
  doi          = {10.1109/TKDE.2021.3120561},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2875-2886},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On the fairness of time-critical influence maximization in social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On social network de-anonymization with communities: A
maximum a posteriori perspective. <em>TKDE</em>, <em>35</em>(3),
2859–2874. (<a href="https://doi.org/10.1109/TKDE.2021.3124559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial privacy-driven issue nowadays is re-identifying anonymized social networks by mapping them to correlated cross-domain auxiliary networks. Prior works are typically based on modeling social networks as random graphs representing users and their relations, and subsequently quantify the quality of mappings through varied cost functions. However, many cost functions are empirically proposed without sufficient theoretical support. For some other works probing the theoretical bound, it remains unknown how to algorithmically meet the demand of such quantifications, i.e., to minimize the cost functions. Besides, only few prior works have discussed the de-anonymization of social networks with communities. We address those concerns in a social network modeling parameterized by community structures that can be leveraged as side information for de-anonymization. Based on the Maximum A Posteriori (MAP) estimation, our first contribution is a series of MAP-based cost functions, which, when minimized, enjoy superiority to previous ones in finding the correct mapping with the highest probability. The feasibility of the cost functions is then for the first time algorithmically characterized. We prove the general multiplicative inapproximability and thus propose two heuristics, which, respectively, enjoy an $\epsilon$ -additive approximation and a conditional optimality in carrying out successful user re-identification. Our theoretical findings are also empirically validated under classical synthetic and real-wrold social networks. Both theoretical and empirical observations manifest the importance of community in enhancing privacy inferencing.},
  archive      = {J_TKDE},
  author       = {Jiapeng Zhang and Shan Qu and Qi Li and Huquan Kang and Luoyi Fu and Haisong Zhang and Xinbing Wang and Guihai Chen},
  doi          = {10.1109/TKDE.2021.3124559},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2859-2874},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On social network de-anonymization with communities: A maximum a posteriori perspective},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multivariate correlation matrix-based deep learning model
with enhanced heuristic optimization for short-term traffic forecasting.
<em>TKDE</em>, <em>35</em>(3), 2847–2858. (<a
href="https://doi.org/10.1109/TKDE.2021.3118389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately capturing the spatial correlations of traffic network significantly benefits short-term traffic forecasting. Some existing works represent spatial correlations in a simple one-dimensional space, but they cannot represent the real spatial correlations among sensors comprehensively. The other existing works represent the spatial correlations through grid-based method, but the local correlation of constructed spatial map is too superficial to extract deep spatial features effectively. Therefore, a novel deep learning model is proposed, which aims to represent the spatial correlations more effectively through a new correlation matrix structure. In the proposed model, the correlations among sensors are calculated from multiple perspectives to construct the speed, volume, and occupancy correlation matrices respectively. Then, considering that highly correlated sensors are close in the spatial dimension, an enhanced heuristic optimization algorithm is proposed to evolve these three correlation matrices into optimal ones by reorganizing the highly correlated sensors into each other&#39;s neighborhood. Finally, the three optimal correlation matrices are combined to form a three-dimensional multivariate correlation matrix characterized by locally high correlation, which is beneficial to exploit the deep spatial features of traffic network. The experiments show that the proposed model has better accuracy and stability than other commonly used baseline models.},
  archive      = {J_TKDE},
  author       = {Shuai Zhang and Kun Zhu and Wenyu Zhang},
  doi          = {10.1109/TKDE.2021.3118389},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2847-2858},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multivariate correlation matrix-based deep learning model with enhanced heuristic optimization for short-term traffic forecasting},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple receding imputation of time series based on similar
conditions screening. <em>TKDE</em>, <em>35</em>(3), 2837–2846. (<a
href="https://doi.org/10.1109/TKDE.2021.3109115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data widely exist in the raw or processed data, implying information loss. In many cases, missing values have to be accurately imputed for further use. In this paper, an extreme case, consecutively missing data in large-length and mainly remaining data in small-length, is discussed for time series varying with operating conditions, very universal in industrial processes. Firstly, to fully utilize the information of remaining data, a similar conditions screening scheme is provided, efficient to improve imputation accuracy. Then, multiple receding imputation via Gaussian process regression (GPR) and long short term memory (LSTM) neural network are proposed, deducing generic multiple combination imputation and bidirectional imputation structures. At last, applied for data imputation of extremely missing wind power data, condition-dependent on wind speed, imputation effects of the proposed methods are carefully compared. Simulation results reveal effectiveness of these methods to impute missing data under the extreme case, laying very important foundation for data-driven applications.},
  archive      = {J_TKDE},
  author       = {Yang Hu and Ze Yang and Wenchang Hou},
  doi          = {10.1109/TKDE.2021.3109115},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2837-2846},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multiple receding imputation of time series based on similar conditions screening},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MOSE: A monotonic selectivity estimator using learned CDF.
<em>TKDE</em>, <em>35</em>(3), 2823–2836. (<a
href="https://doi.org/10.1109/TKDE.2021.3112753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of selectivity estimation is of vital importance to create good query plans. Traditional estimators such as histograms make several assumptions during estimation that can lead to huge errors. Recently the database community started exploring the usage of machine learning in selectivity estimation and won great achievements. However, due to the black box models they used, existing learning-based methods still face a variety of new challenges, including high estimation latency, large training data demanding, and occurrence of illogical results. In this work, we propose a learning-based MO notonic S electivity E stimator (MOSE) to address these challenges. We first learn a multi-dimensional C umulative D istribution F unction (CDF) of the data in a supervised method and then compute selectivity for ad hoc query predicates at rum-time. We propose a novel regularizer and an effective attribute-aware calibration method to improve the estimation accuracy. To further improve the model efficiency, we design a mutual information based attributes partition algorithm for model ensemble. With regard to the heavy cost of training data collection, we design a model-based active learning strategy to generate high-quality training data cost-effectively. We conduct extensive experiments on both real-world and synthetic datasets and the results show that MOSE outperforms the state-of-the-art methods in terms of accuracy and efficiency.},
  archive      = {J_TKDE},
  author       = {Luming Sun and Cuiping Li and Tao Ji and Hong Chen},
  doi          = {10.1109/TKDE.2021.3112753},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2823-2836},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MOSE: A monotonic selectivity estimator using learned CDF},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling the impact of person-organization fit on talent
management with structure-aware attentive neural networks.
<em>TKDE</em>, <em>35</em>(3), 2809–2822. (<a
href="https://doi.org/10.1109/TKDE.2021.3115620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person-Organization fit (P-O fit) refers to the compatibility between employees and their organizations. The study of P-O fit is important for enhancing proactive talent management. While considerable efforts have been made in this direction, it still lacks a quantitative and holistic way for measuring P-O fit and its impact on talent management. To this end, in this paper, we propose a novel data-driven neural network approach for dynamically modeling the compatibility in P-O fit and its meaningful relationships with two critical issues in talent management, namely talent turnover and job performance. Specifically, inspired by the practical management scenarios, we creatively propose a novel neural-network-based P-O fit model. We first designed three kinds of organization-aware compatibility features extraction layers for measuring P-O fit. Then, to capture the dynamic nature of P-O fit and its consequent impact, we further exploit an adapted Recurrent Neural Network with attention mechanism to model the temporal information of P-O fit. Finally, we compare our approach with a number of state-of-the-art baseline methods on real-world talent data. Experimental results clearly demonstrate the effectiveness in terms of turnover and job performance prediction. Moreover, we show some interesting indicators of talent management through the visualizing some network layers.},
  archive      = {J_TKDE},
  author       = {Ying Sun and Fuzhen Zhuang and Hengshu Zhu and Xin Song and Qing He and Hui Xiong},
  doi          = {10.1109/TKDE.2021.3115620},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2809-2822},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling the impact of person-organization fit on talent management with structure-aware attentive neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling spatial nonstationarity via deformable convolutions
for deep traffic flow prediction. <em>TKDE</em>, <em>35</em>(3),
2796–2808. (<a href="https://doi.org/10.1109/TKDE.2021.3112977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are being increasingly used for short-term traffic flow prediction, which can be generally categorized as convolutional (CNNs) or graph neural networks (GNNs). CNNs are preferable for region-wise traffic prediction by taking advantage of localized spatial correlations, whilst GNNs achieves better performance for graph-structured traffic data. When applied to region-wise traffic prediction, CNNs typically partition an underlying territory into grid-like spatial units, and employ standard convolutions to learn spatial dependence among the units. However, standard convolutions with fixed geometric structures cannot fully model the nonstationary characteristics of local traffic flows. To overcome the deficiency, we introduce deformable convolution that augments the spatial sampling locations with additional offsets, to enhance the modeling capability of spatial nonstationarity. On this basis, we design a deep deformable convolutional residual network, namely DeFlow-Net , that can effectively model global spatial dependence, local spatial nonstationarity, and temporal periodicity of traffic flows. Furthermore, to better fit with convolutions, we suggest to first aggregate traffic flows according to pre-conceived regions or self-organized regions based on traffic flows, then dispose to sequentially organized raster images for network input. Extensive experiments on real-world traffic flows demonstrate that DeFlow-Net outperforms GNNs and existing CNNs using standard convolutions, and spatial partition by pre-conceived regions or self-organized regions further enhances the performance. We also demonstrate the advantage of DeFlow-Net in maintaining spatial autocorrelation, and reveal the impacts of partition shapes and scales on deep traffic flow prediction.},
  archive      = {J_TKDE},
  author       = {Wei Zeng and Chengqiao Lin and Kang Liu and Juncong Lin and Anthony K. H. Tung},
  doi          = {10.1109/TKDE.2021.3112977},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2796-2808},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling spatial nonstationarity via deformable convolutions for deep traffic flow prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-reweighted regularization for unsupervised domain
adaptation. <em>TKDE</em>, <em>35</em>(3), 2781–2795. (<a
href="https://doi.org/10.1109/TKDE.2021.3114536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) enables knowledge transfer from a labeled source domain to an unlabeled target domain by reducing the cross-domain distribution discrepancy, and the adversarial learning based paradigm has achieved remarkable success. On top of the derived domain-invariant feature representations, a promising stream of recent works seeks to further regularize the classification decision boundary via self-training to learn target adaptive classifier with pseudo-labeled target samples. However, since the pseudo labels are inevitably noisy, most of prior methods focus on manually designing elaborate target selection algorithms or optimization objectives to combat the negative effect caused by the incorrect pseudo labels. Different from them, in this paper, we propose a simple and powerful meta-learning based target-reweighting regularization algorithm, called MetaReg, which regularizes the model training by learning to reweight the noisy pseudo-labeled target samples. Specifically, MetaReg is motivated by the intuition that an ideal target classifier trained on correct target pseudo labels should make small classification errors on target-like source samples. Therefore, we explicitly define a meta reweighting problem that aims to find the optimal weights for different target pseudo labels by minimizing the classification loss on a designed validation set, a class-balanced set consisting of source samples that are most similar to target ones. Note that the optimization problem can be solved efficiently with a simplified approximation technique. As a result, the automatically learned optimal weights are utilized to reweight pseudo-labeled target samples, and regularize the model learning by target supervision with the learned different importance. Comprehensive experiments on several cross-domain image and text datasets verify that MetaReg could outperform the non-regularized UDA counterparts with state-of-the-art performance. Code is available at https://github.com/BIT-DA/MetaReg .},
  archive      = {J_TKDE},
  author       = {Shuang Li and Wenxuan Ma and Jinming Zhang and Chi Harold Liu and Jian Liang and Guoren Wang},
  doi          = {10.1109/TKDE.2021.3114536},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2781-2795},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Meta-reweighted regularization for unsupervised domain adaptation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lifelong property price prediction: A case study for the
toronto real estate market. <em>TKDE</em>, <em>35</em>(3), 2765–2780.
(<a href="https://doi.org/10.1109/TKDE.2021.3112749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Luce , the first life-long predictive model for automated property valuation. Luce addresses two critical issues of property valuation: the lack of recent sold prices and the sparsity of house data. It is designed to operate on a limited volume of recent house transaction data. As a departure from prior work, Luce organizes the house data in a heterogeneous information network (HIN) where graph nodes are house entities and attributes that are important for house price valuation. We employ a Graph Convolutional Network (GCN) to extract the spatial information from the HIN for house-related data like geographical locations, and then use a Long Short Term Memory (LSTM) network to model the temporal dependencies for house transaction data over time. Unlike prior work, Luce can make effective use of the limited house transactions data in the past few months to update valuation information for all house entities within the HIN. By providing a complete and up-to-date house valuation dataset, Luce thus massively simplifies the downstream valuation task for the targeting properties. We demonstrate the benefit of Luce by applying it to large, real-life datasets obtained from the Toronto real estate market. Extensive experimental results show that Luce not only significantly outperforms prior property valuation methods but also often reaches and sometimes exceeds the valuation accuracy given by independent experts when using the actual realization price as the ground truth.},
  archive      = {J_TKDE},
  author       = {Hao Peng and Jianxin Li and Zheng Wang and Renyu Yang and Mingsheng Liu and Mingming Zhang and Philip S. Yu and Lifang He},
  doi          = {10.1109/TKDE.2021.3112749},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2765-2780},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Lifelong property price prediction: A case study for the toronto real estate market},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning causal representations for robust domain
adaptation. <em>TKDE</em>, <em>35</em>(3), 2750–2764. (<a
href="https://doi.org/10.1109/TKDE.2021.3119185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we investigate a challenging problem, namely, robust domain adaptation, where data from only a single well-labeled source domain are available in the training phase. To address this problem, assuming that the causal relationships between the features and the class variable are robust across domains, we propose a novel causal autoencoder (CAE), which integrates a deep autoencoder and a causal structure learning model to learn causal representations using data from a single source domain. Specifically, a deep autoencoder model is adopted to learn the low-dimensional representations, and a causal structure learning model is designed to separate the low-dimensional representations into two groups: causal representations and task-irrelevant representations. Using three real-world datasets, the experiments have validated the effectiveness of CAE, in comparison with eleven state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Shuai Yang and Kui Yu and Fuyuan Cao and Lin Liu and Hao Wang and Jiuyong Li},
  doi          = {10.1109/TKDE.2021.3119185},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2750-2764},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning causal representations for robust domain adaptation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning adaptive embedding considering incremental class.
<em>TKDE</em>, <em>35</em>(3), 2736–2749. (<a
href="https://doi.org/10.1109/TKDE.2021.3109131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-Incremental Learning (CIL) aims to train a reliable model with the streaming data, which emerges unknown classes sequentially. Different from traditional closed set learning, CIL has two main challenges: (1) Novel class detection. The initial training data only contains incomplete classes, and streaming test data will accept unknown classes. Therefore, the model needs to not only accurately classify known classes, but also effectively detect unknown classes; (2) Model expansion. After the novel classes are detected, the model needs to be updated without re-training using the entire previous data. However, traditional CIL methods have not fully considered these two challenges. First, they are always restricted to single novel class detection within each phase and embedding confusion caused by unknown classes. Besides, they ignore the catastrophic forgetting of known categories in model update. To this end, we propose a semi-supervised style Class-Incremental Learning without Forgetting (CILF) method, which aims to learn adaptive embedding for processing novel class detection and model update in a unified framework. In detail, CILF designs to regularize classification with decoupled prototype based loss, which can improve the intra-class and inter-class structure significantly, and acquires a compact embedding representation for novel class detection in result. Then, CILF employs a learnable curriculum clustering operator to estimate the number of semantic clusters via fine-tuning the learned network, in which curriculum operator can adaptively learn the embedding in self-taught form. Therefore, CILF can detect multiple novel classes and mitigate the embedding confusion problem. Last, with the labeled streaming test data, CILF can update the network with robust regularization to mitigate the catastrophic forgetting. Consequently, CILF is able to iteratively perform novel class detection and model update. We verify the effectiveness of our model on four streaming classification tasks, and empirical studies show the superior performance of the proposed method.},
  archive      = {J_TKDE},
  author       = {Yang Yang and Zhen-Qiang Sun and Hengshu Zhu and Yanjie Fu and Yuanchun Zhou and Hui Xiong and Jian Yang},
  doi          = {10.1109/TKDE.2021.3109131},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2736-2749},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning adaptive embedding considering incremental class},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Knowledge graph completion by jointly learning structural
features and soft logical rules. <em>TKDE</em>, <em>35</em>(3),
2724–2735. (<a href="https://doi.org/10.1109/TKDE.2021.3108224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development and widespread application of Knowledge graphs (KGs) in many artificial intelligence tasks, a large number of efforts have been made to refine them and increase their quality. Knowedge graph embedding (KGE) has become one of the main refinement tasks, which aims to predict missing facts based on existing ones in KGs. However, there are still mainly two difficult unresolved challenges: (i) how to leverage the local structural features of entities and the potential soft logical rules to learn more expressive embedding of entites and relations; and (ii) how to combine these two learning processes into one unified model. To conquer these problems, we propose a novel KGE model named JSSKGE, which can J ointly learn the local S tructural features of entities and S oft logical rules. First, we employ graph attention networks which are specially designed for graph-structured data to aggregate the local structural information of nodes. Then, we utilize soft logical rules implicated in KGs as an expert to further rectify the embeddings of entities and relations. By jointly learning, we can obtain more informative embeddings to predict new facts. With experiments on four commonly used datasets, the JSSKGE obtains better performance than state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Weidong Li and Rong Peng and Zhi Li},
  doi          = {10.1109/TKDE.2021.3108224},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2724-2735},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Knowledge graph completion by jointly learning structural features and soft logical rules},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KNN classification with one-step computation. <em>TKDE</em>,
<em>35</em>(3), 2711–2723. (<a
href="https://doi.org/10.1109/TKDE.2021.3119140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {KNN classification is an improvisational learning mode, in which they are carried out only when a test data is predicted that set a suitable K value and search the K nearest neighbors from the whole training sample space, referred them to the lazy part of KNN classification. This lazy part has been the bottleneck problem of applying KNN classification due to the complete search of K nearest neighbors. In this paper, a one-step computation is proposed to replace the lazy part of KNN classification. The one-step computation actually transforms the lazy part to a matrix computation as follows. Given a test data, training samples are first applied to fit the test data with the least squares loss function. And then, a relationship matrix is generated by weighting all training samples according to their influence on the test data. Finally, a group lasso is employed to perform sparse learning of the relationship matrix. In this way, setting K value and searching K nearest neighbors are both integrated to a unified computation. In addition, a new classification rule is proposed for improving the performance of one-step KNN classification. The proposed approach is experimentally evaluated, and demonstrated that the one-step KNN classification is efficient and promising.},
  archive      = {J_TKDE},
  author       = {Shichao Zhang and Jiaye Li},
  doi          = {10.1109/TKDE.2021.3119140},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2711-2723},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {KNN classification with one-step computation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Isolation distributional kernel: A new tool for point and
group anomaly detections. <em>TKDE</em>, <em>35</em>(3), 2697–2710. (<a
href="https://doi.org/10.1109/TKDE.2021.3120277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Isolation Distributional Kernel as a new way to measure the similarity between two distributions. Existing approaches based on kernel mean embedding, which convert a point kernel to a distributional kernel, have two key issues: the point kernel employed has a feature map with intractable dimensionality; and it is data independent . This paper shows that Isolation Distributional Kernel (IDK), which is based on a data dependent point kernel, addresses both key issues. We demonstrate IDK’s efficacy and efficiency as a new tool for kernel-based anomaly detection for both point and group anomalies. Without explicit learning, using IDK alone outperforms existing kernel-based point anomaly detector OCSVM and other kernel mean embedding methods that rely on Gaussian kernel. For group anomaly detection, we introduce an IDK based detector called IDK $^2$ . It reformulates the problem of group anomaly detection in input space into the problem of point anomaly detection in Hilbert Space, without the need for learning. IDK $^2$ runs orders of magnitude faster than group anomaly detector OCSMM. We reveal for the first time that an effective kernel-based anomaly detector based on kernel mean embedding must employ a characteristic kernel which is data dependent.},
  archive      = {J_TKDE},
  author       = {Kai Ming Ting and Bi-Cun Xu and Takashi Washio and Zhi-Hua Zhou},
  doi          = {10.1109/TKDE.2021.3120277},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2697-2710},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Isolation distributional kernel: A new tool for point and group anomaly detections},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023g). Incorporating link prediction into multi-relational item
graph modeling for session-based recommendation. <em>TKDE</em>,
<em>35</em>(3), 2683–2696. (<a
href="https://doi.org/10.1109/TKDE.2021.3111436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation aims at predicting the next item that a user is more likely to interact with by a target behavior type. Most of the existing session-based recommendation methods focus on developing powerful representation learning approaches to model items’ sequential correlations, whereas they usually encounter the following limitations. First, they only utilize sessions that belong to the target behavior type, neglecting the potential of leveraging other behavior types as auxiliary information for modeling user preference. Second, they separately model item-to-item relations for each session, overlooking to globally characterize the relations across different sessions for better item representations. To overcome these limitations, we first build a Multi-Relational Item Graph (MRIG) involving target and auxiliary behavior types over all sessions. Consequently, a novel Graph Neural Network (GNN) based model is devised to encode MRIG’s item-to-item relations into target and auxiliary session-based representations, and adaptively fuse them to represent user interests. To facilitate model training, we further incorporate link prediction into multi-relational item graph modeling, acting as a simple but relevant task to session-based recommendation. The extensive experiments on real-world datasets demonstrate the superiority of the model over diverse and competitive baselines, validating its main components’ significant contributions.},
  archive      = {J_TKDE},
  author       = {Wen Wang and Wei Zhang and Shukai Liu and Qi Liu and Bo Zhang and Leyu Lin and Hongyuan Zha},
  doi          = {10.1109/TKDE.2021.3111436},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2683-2696},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incorporating link prediction into multi-relational item graph modeling for session-based recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incomplete multi-view clustering with reconstructed views.
<em>TKDE</em>, <em>35</em>(3), 2671–2682. (<a
href="https://doi.org/10.1109/TKDE.2021.3112114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one category of important incomplete multi-view clustering methods, subspace based methods seek the common latent representation of incomplete multi-view data by matrix factorization and then partition the latent representation to get clustering results. However, these methods ignore missing views in the process of matrix factorization, which makes the connection of different views be exploited inadequately. This paper proposes Incomplete Multi-view Clustering with Reconstructed Views (IMCRV), which utilizes the incomplete examples sufficiently. In IMCRV, the missing views of incomplete examples are reconstructed and the reconstructed views are also used to seek the common latent representation. IMCRV also involves the Laplacian regularization to preserve the global property of the latent representation. The gradient descent method with the multiplicative update rule is employed to solve the objective function of IMCRV. The corresponding iterative algorithm is developed and the convergence of the algorithm is proved. IMCRV is compared with many state-of-the-art incomplete multi-view clustering methods under different Incomplete Example Rates (IER) on public multi-view datasets. The experimental results demonstrate the superior effectiveness of IMCRV.},
  archive      = {J_TKDE},
  author       = {Jun Yin and Shiliang Sun},
  doi          = {10.1109/TKDE.2021.3112114},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2671-2682},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incomplete multi-view clustering with reconstructed views},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving rumor detection by promoting information campaigns
with transformer-based generative adversarial learning. <em>TKDE</em>,
<em>35</em>(3), 2657–2670. (<a
href="https://doi.org/10.1109/TKDE.2021.3112497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rumors can cause devastating consequences to individuals and our society. Analysis shows that the widespread of rumors typically results from deliberate promotion of information with unknown veracity aiming to shape the collective public opinions on the concerned news event. In this paper, we attempt to combat such chaotic phenomenon with a countermeasure by mirroring against how such chaos is created in order to make automatic rumor detection more robust and effective. Our idea is inspired by adversarial learning method originated from Generative Adversarial Networks (GAN). We propose a GAN-style approach, where a generator is designed to produce uncertain or conflicting voices, further polarizing the original conversation threads with the intention of pressurizing the discriminator to learn stronger rumor indicative features from the augmented, more challenging examples. We reveal that feature learning effectiveness is highly relevant to the quality of generated parody, viz., how hard it is to get distinguished from real posts. Given the strong natural language generation performance of transformer, we propose a transformer-based method to improve the generated posts, so that they appear to be closely responsive to the source post and retain the authentic propagation structure and context of information. Different from traditional data-driven approach to rumor detection, our method can capture low-frequency but more salient non-trivial discriminant patterns via adversarial training. Extensive experiments on THREE benchmark datasets demonstrate that our rumor detection methods and the transformer-based model achieve much better results than state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Jing Ma and Jun Li and Wei Gao and Yang Yang and Kam-Fai Wong},
  doi          = {10.1109/TKDE.2021.3112497},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2657-2670},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Improving rumor detection by promoting information campaigns with transformer-based generative adversarial learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hierarchical representation learning for attributed
networks. <em>TKDE</em>, <em>35</em>(3), 2641–2656. (<a
href="https://doi.org/10.1109/TKDE.2021.3117274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network representation learning, also called network embedding, aiming to learn low dimensional vectors for nodes while preserving essential properties of the network, benefits plenty of practical applications. However, how to do representation learning on the network quickly and effectively is a meaningful and challenging task, especially for the attributed networks. In this paper, we propose HANE, a H ierarchical A ttributed N etwork E mbedding framework, which is a fast and effective method by quickly constructing a hierarchical attributed network of different granularities to learn nodes representations. Specifically, for an attributed network, HANE first builds a hierarchy of successively smaller attributed network from fine to coarse by the fast granulation strategy fusing topological structure and node attributes. After using any unsupervised network embedding method to learn nodes representations of the coarsest network, HANE refines the nodes representations of the hierarchical attributed network from coarse to fine. HANE improves the speed of network representation learning while maintaining its performance and the representation learning method of the coarsest network is flexible. We conduct extensive evaluations for the proposed framework HANE on six datasets and two benchmark applications. Experimental results demonstrate that HANE achieves significant improvements over previous state-of-the-art network embedding methods in efficiency and effectiveness.},
  archive      = {J_TKDE},
  author       = {Shu Zhao and Ziwei Du and Jie Chen and Yanping Zhang and Jie Tang and Philip S. Yu},
  doi          = {10.1109/TKDE.2021.3117274},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2641-2656},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical representation learning for attributed networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical reduced-space drift detection framework for
multivariate supervised data streams. <em>TKDE</em>, <em>35</em>(3),
2628–2640. (<a href="https://doi.org/10.1109/TKDE.2021.3111756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a streaming environment, the characteristics of the data themselves and their relationship with the labels may change over time. Most drift detection methods for supervised data streams are performance-based, that is, they detect changes only after the classification accuracy deteriorates. This may not be sufficient in many application areas where the reason behind a drift is also important. Another category of drift detectors are data distribution-based detectors. Although they can detect some drifts within the input space, changes affecting only the labelling mechanism cannot be identified. Furthermore, little work is available on drift detection for high-dimensional data streams. In this paper we propose an advanced H ierarchical R educed-space D rift D etection (HRDD) framework for supervised data streams which captures drifts regardless of their effects on classification performance. This framework suggests monitoring both marginal and class-conditional distributions within a lower-dimensional space specifically relevant to the assigned classification task. Experimental comparisons have demonstrated that HRDD not only achieves high-quality performance on high-dimensional data streams, but also outperforms its competitors in terms of detection recall, precision and F-measure across a wide range of different concept drift types including subtle drifts.},
  archive      = {J_TKDE},
  author       = {Shuyi Zhang and Peter Tino and Xin Yao},
  doi          = {10.1109/TKDE.2021.3111756},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2628-2640},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical reduced-space drift detection framework for multivariate supervised data streams},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive asynchronous clustering algorithms for wireless
mesh networks. <em>TKDE</em>, <em>35</em>(3), 2610–2627. (<a
href="https://doi.org/10.1109/TKDE.2021.3119550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenge to generate an accurate machine learning model in a distributed network due to the increased concern in data privacy and high cost in gathering all raw data. This paper presents an adaptive asynchronous distributed clustering algorithm and two centralised methods for agents in wireless network to learn the global models, while the privacy is protected. Moreover, the communication cost and clustering quality can be adaptively balanced. The proposed clustering algorithms do not require the number of clusters to be pre-defined, and we propose a bounding boxes based method to fully utilize the shape information of clusters to improve the accuracy of the global model. Furthermore, we consider different knowledge levels of agents and different requirements about the global model. In experiments on randomly generated network topologies, we demonstrate that methods which do all the iterations of clustering in each cycle, and which exchange descriptions of cluster shape and density instead of just centroids and data counts, achieve higher accuracy, in significantly shorter elapsed time.},
  archive      = {J_TKDE},
  author       = {Cheng Qiao and Kenneth N. Brown and Fan Zhang and Zhihong Tian},
  doi          = {10.1109/TKDE.2021.3119550},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2610-2627},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive asynchronous clustering algorithms for wireless mesh networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast reachability queries answering based on <span
class="math inline">RCN</span>RCN reduction. <em>TKDE</em>,
<em>35</em>(3), 2590–2609. (<a
href="https://doi.org/10.1109/TKDE.2021.3108433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answering reachability queries is one of the fundamental graph operations. One way to make acceleration is directly building index on the input graph. Considering that the size of the input graph has a great impact on the query performance, the other way is reducing the size of the input graph, such that the given queries can be answered over a smaller graph. Although the input graph can be reduced significantly in some cases by existing approaches, a smaller reduced graph does not always mean a positive effect on the query performance. In this paper, we study graph reduction to accelerate reachability queries answering. We propose a novel graph reduction approach, namely $\mathsf{RCN}$ reduction, to reduce the input graph $G$ with $|V|$ nodes into a smaller one $G^{r}$ with $|V^r|$ nodes. Assume that the probability of a node of $G$ to be a query node is $1/|V|$ , we show that based on our approach, the lower bound probability that a query $q$ can be answered in constant time is $1-(\frac{|V^r|}{|V|})^2$ , denoting that the probability that $q$ needs to be answered over the reduced graph is $(\frac{|V^r|}{|V|})^2$ , which means the smaller the reduced graph, the larger the probability that $q$ can be answered in constant time. We show the difficulties of $\mathsf{RCN}$ reduction and propose efficient algorithms to improve the reduction ratio. Based on the result of $\mathsf{RCN}$ reduction, we further propose a novel labeling scheme to accelerate queries answering. We confirm the efficiency of our approach by extensive experimental results for graph reduction and reachability queries processing using 20 real datasets.},
  archive      = {J_TKDE},
  author       = {Junfeng Zhou and Jeffrey Xu Yu and Yaxian Qiu and Xian Tang and Ziyang Chen and Ming Du},
  doi          = {10.1109/TKDE.2021.3108433},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2590-2609},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast reachability queries answering based on $\mathsf{RCN}$RCN reduction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast convolutional factorization machine with enhanced
robustness. <em>TKDE</em>, <em>35</em>(3), 2579–2589. (<a
href="https://doi.org/10.1109/TKDE.2021.3116352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, factorization machine and its variants have shown promising results for context-aware recommender systems (CARS), especially when combined with deep neural networks. Among them, convolutional factorization machine (CFM) is a prominent example. The key to the success of CFM is its 3D convolutional architecture for capturing complex interactions on top of embedded features. However, the resultant computational cost can also be demanding. Moreover, the feature embedding scheme of CFM and other factorization models can be potentially vulnerable to noise. To tackle these issues, in this study we propose two models, namely, the fast convolutional factorization machine (FCFM) that slims down the complete pairwise feature interaction for higher computational efficiency, and adversarial fast convolutional factorization machine (AFCFM) that further enhances the robustness of the model by introducing adversarial noise to the feature interaction image generated by the model. Experimental results on four benchmark datasets prove that the proposed FCFM is nearly five times faster than CFM with competitive performance, while AFCFM improves the performance of the state-of-the-art models by about 8\% with higher efficiency than CFM.},
  archive      = {J_TKDE},
  author       = {Tianyi Gu and Kaiwen Huang and Jie Zhang and Kai Zhang and Ping Li},
  doi          = {10.1109/TKDE.2021.3116352},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2579-2589},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast convolutional factorization machine with enhanced robustness},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Entity linking meets deep learning: Techniques and
solutions. <em>TKDE</em>, <em>35</em>(3), 2556–2578. (<a
href="https://doi.org/10.1109/TKDE.2021.3117715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity linking (EL) is the process of linking entity mentions appearing in web text with their corresponding entities in a knowledge base. EL plays an important role in the fields of knowledge engineering and data mining, underlying a variety of downstream applications such as knowledge base population, content analysis, relation extraction, and question answering. In recent years, deep learning (DL), which has achieved tremendous success in various domains, has also been leveraged in EL methods to surpass traditional machine learning based methods and yield the state-of-the-art performance. In this survey, we present a comprehensive review and analysis of existing DL based EL methods. First of all, we propose a new taxonomy, which organizes existing DL based EL methods using three axes: embedding, feature, and algorithm. Then we systematically survey the representative EL methods along the three axes of the taxonomy. Later, we introduce ten commonly used EL data sets and give a quantitative performance analysis of DL based EL methods over these data sets. Finally, we discuss the remaining limitations of existing methods and highlight some promising future directions.},
  archive      = {J_TKDE},
  author       = {Wei Shen and Yuhan Li and Yinan Liu and Jiawei Han and Jianyong Wang and Xiaojie Yuan},
  doi          = {10.1109/TKDE.2021.3117715},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2556-2578},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Entity linking meets deep learning: Techniques and solutions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Eigen-GNN: A graph structure preserving plug-in for GNNs.
<em>TKDE</em>, <em>35</em>(3), 2544–2555. (<a
href="https://doi.org/10.1109/TKDE.2021.3112746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are emerging machine learning models on graphs. Although sufficiently deep GNNs are shown theoretically capable of fully preserving graph structures, most existing GNN models in practice are shallow and essentially feature-centric. We show empirically and analytically that the existing shallow GNNs cannot preserve graph structures well. To overcome this fundamental challenge, we propose Eigen-GNN, a simple yet effective and general plug-in module to boost GNNs ability in preserving graph structures. Specifically, we integrate the eigenspace of graph structures with GNNs by treating GNNs as a type of dimensionality reduction and expanding the initial dimensionality reduction bases. Without needing to increase depths, Eigen-GNN possesses more flexibilities in handling both feature-driven and structure-driven tasks since the initial bases contain both node features and graph structures. We present extensive experimental results to demonstrate the effectiveness of Eigen-GNN for tasks including node classification, link prediction, and graph isomorphism tests.},
  archive      = {J_TKDE},
  author       = {Ziwei Zhang and Peng Cui and Jian Pei and Xin Wang and Wenwu Zhu},
  doi          = {10.1109/TKDE.2021.3112746},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2544-2555},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Eigen-GNN: A graph structure preserving plug-in for GNNs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient retrieval of the top-k most relevant event-partner
pairs. <em>TKDE</em>, <em>35</em>(3), 2529–2543. (<a
href="https://doi.org/10.1109/TKDE.2021.3118552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of event-based social networking (EBSN) motivates studies on topics such as event, venue, and friend recommendation as well as event creation and organization. In this setting, the notion of event-partner recommendation has attracted attention. When recommending an event to a user, this functionality allows the recommendation of partners with whom to attend the event. However, in existing proposals, recommendations are pushed to users at the system’s initiative. In contrast, EBSNs provide users with keyword-based search functionality. This way, users may retrieve information in pull mode. We propose a new way of accessing information in EBSNs that combines pull and push, thus allowing users to not only conduct ad-hoc searches for events, but also to receive partner recommendations for retrieved events. Specifically, we define and study top- $k$ event-partner ( $k$ EP) pair retrieval querying that integrates keyword-based search for events with event-partner recommendation. This type of query retrieves event-partner pairs, taking into account the relevance of events to user-supplied keywords and so-called together preferences that indicate the extent of a user’s preference to attend an event with a given partner. To compute $k$ EP queries efficiently, we propose a rank-join based framework with three optimizations. Results of empirical studies with implementations of the proposed techniques demonstrate that the proposed techniques are capable of excellent performance.},
  archive      = {J_TKDE},
  author       = {Dingming Wu and Erjia Xiao and Yi Zhu and Christian S. Jensen and Kezhong Lu},
  doi          = {10.1109/TKDE.2021.3118552},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2529-2543},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient retrieval of the top-k most relevant event-partner pairs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient datalog rewriting for query answering in TGD
ontologies. <em>TKDE</em>, <em>35</em>(3), 2515–2528. (<a
href="https://doi.org/10.1109/TKDE.2021.3111011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuple-generating dependencies (TGDs) are an expressive constraint language for ontology-mediated query answering and thus query answering is of high complexity. Existing systems based on first-order rewriting methods can lead to queries too large for DBMS to handle. It is shown that Datalog rewriting can result in more compact queries, yet previously proposed Datalog rewriting methods are mostly inefficient for implementation. In this paper, we fill the gap by proposing an efficient Datalog rewriting approach for answering conjunctive queries over TGDs, and identify and combine existing fragments of TGDs for which our rewriting method terminates. We implemented a prototype system Drewer, and experiments show that it is able to handle a wide range of benchmarks in the literature. Moreover, Drewer shows superior performance over state-of-the-art systems on both the compactness of rewriting and the efficiency of query answering.},
  archive      = {J_TKDE},
  author       = {Zhe Wang and Peng Xiao and Kewen Wang and Zhiqiang Zhuang and Hai Wan},
  doi          = {10.1109/TKDE.2021.3111011},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2515-2528},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient datalog rewriting for query answering in TGD ontologies},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and optimal algorithms for tree summarization with
weighted terminologies. <em>TKDE</em>, <em>35</em>(3), 2500–2514. (<a
href="https://doi.org/10.1109/TKDE.2021.3120722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data summarization that presents a small subset of a dataset to users has been widely applied in numerous applications and systems. Many datasets are coded with hierarchical terminologies, e.g., gene ontology, disease ontology, to name a few. In this paper, we study the weighted tree summarization. We motivate and formulate our ${\mathsf {kWTS}}$ - ${\mathsf {problem}}$ as selecting a diverse set of $k$ nodes to s ummarize a hierarchical t ree $T$ with w eighted terminologies. We first propose an efficient greedy tree summarization algorithm ${\mathsf {GTS}}$ . It solves the problem with $(1-1/e)$ -approximation guarantee. Although ${\mathsf {GTS}}$ achieves quality-guaranteed answers approximately, but it is still not optimal. To tackle the problem optimally, we further develop a dynamic programming algorithm ${\mathsf {OTS}}$ to obtain optimal answers for ${\mathsf {kWTS}}$ - ${\mathsf {problem}}$ in $O(nhk^3)$ time, where $n, h$ are the node size and height in tree $T$ . The algorithm complexity and correctness of ${\mathsf {OTS}}$ are theoretically analyzed. In addition, we propose a useful optimization technique of tree reduction to remove useless nodes with zero weights and shrink the tree into a smaller one, which ensures the efficiency acceleration of both ${\mathsf {GTS}}$ and ${\mathsf {OTS}}$ in real-world datasets. Moreover, we illustrate one useful application of graph visualization based on the answer of $k$ -sized tree summarization and show it in a novel case study. Extensive experimental results on real-world datasets show the effectiveness and efficiency of our proposed approximate and optimal algorithms for tree summarization. Furthermore, we conduct a usability evaluation of attractive topic recommendation on ACM Computing Classification System dataset to validate the usefulness of our model and algorithms.},
  archive      = {J_TKDE},
  author       = {Xuliang Zhu and Xin Huang and Byron Choi and Jianliang Xu and William K. Cheung and Yanchun Zhang and Jiming Liu},
  doi          = {10.1109/TKDE.2021.3120722},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2500-2514},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient and optimal algorithms for tree summarization with weighted terminologies},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DRGI: Deep relational graph infomax for knowledge graph
completion. <em>TKDE</em>, <em>35</em>(3), 2486–2499. (<a
href="https://doi.org/10.1109/TKDE.2021.3110898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many knowledge graph embedding models for knowledge graph completion have been proposed, ranging from the initial translation-based models such as TransE to recent convolutional neural network (CNN) models such as ConvE. However, these models only focus on semantic information of knowledge graph and neglect the natural graph structure information. Although graph convolutional network (GCN)-based models for knowledge graph embedding have been introduced to address this issue, they still suffer from fact incompleteness, resulting in the unconnectedness of knowledge graph. To solve this problem, we propose a novel model called deep relational graph infomax (DRGI) with mutual information (MI) maximization which takes the benefit of complete structure information and semantic information together. Specifically, the proposed DRGI consists of two encoders which are two identical adaptive relational graph attention networks (ARGATs), corresponding to catching semantic information and complete structure information respectively. Our method establishes new state-of-the-art on the standard datasets for knowledge graph completion. In addition, by exploring the complete structure information, DRGI embraces the merits of faster convergence speed over existing methods and better predictive performance for entities with small indegree.},
  archive      = {J_TKDE},
  author       = {Shuang Liang and Jie Shao and Dongyang Zhang and Jiasheng Zhang and Bin Cui},
  doi          = {10.1109/TKDE.2021.3110898},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2486-2499},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DRGI: Deep relational graph infomax for knowledge graph completion},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Discovering significant communities on bipartite graphs: An
index-based approach. <em>TKDE</em>, <em>35</em>(3), 2471–2485. (<a
href="https://doi.org/10.1109/TKDE.2021.3111349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bipartite graphs are widely used to model relationships between two types of entities. Community search retrieves densely connected subgraphs containing a query vertex, which has been extensively studied on unipartite graphs. However, it remains largely unexplored on bipartite graphs. Moreover, all existing cohesive subgraph models on bipartite graphs only measure the structure cohesiveness while overlooking the edge weight. In this paper, we study the significant ( $\alpha$ , $\beta$ )-community search problem on weighted bipartite graphs. Given a query vertex $q$ , we aim to find the significant ( $\alpha$ , $\beta$ )-community $\mathcal {R}$ of $q$ which adopts ( $\alpha$ , $\beta$ )-core to characterize the engagement level of vertices, and maximizes the minimum edge weight (significance) within $\mathcal {R}$ . To support fast retrieval of $\mathcal {R}$ , we first obtain the maximal connected subgraph of ( $\alpha$ , $\beta$ )-core containing $q$ (the ( $\alpha$ , $\beta$ )-community), and the search space is limited to this subgraph with a much smaller size than the original graph. A novel index structure is presented to support retrieving the ( $\alpha$ , $\beta$ )-community in optimal time. Efficient index maintenance techniques are also proposed to handle dynamic graphs. To further obtain $\mathcal {R}$ , we develop peeling and expansion algorithms. The experimental results on real graphs validate the effectiveness and efficiency of our proposed techniques.},
  archive      = {J_TKDE},
  author       = {Kai Wang and Wenjie Zhang and Ying Zhang and Lu Qin and Yuting Zhang},
  doi          = {10.1109/TKDE.2021.3111349},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2471-2485},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discovering significant communities on bipartite graphs: An index-based approach},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Direct learning with multi-task neural networks for
treatment effect estimation. <em>TKDE</em>, <em>35</em>(3), 2457–2470.
(<a href="https://doi.org/10.1109/TKDE.2021.3112591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference from observational data lies at the heart of education, healthcare, optimal resource allocation and many other decision-making processes. Most of existing methods estimate the target treatment effect indirectly by inferring the underlying treatment response functions or the unobserved counterfactual outcome for every individual. These indirect learning methods are subject to issues of model misspecification and high variability. As a complement of existing indirect learning methods, in this paper, we propose a direct learning framework, called HTENet , for causal inference using deep multi-task learning. It is based on a novel empirical $\tau$ -risk for learning the causal effect model of direct interest in a supervised learning scheme. In our proposed framework, the target treatment effect model is parametrized as a neural network and learned jointly with other auxiliary models in an end-to-end manner. Moreover, we extend the naïve HTENet into other two variants, HTENet-Simple and HTENet-Reg , by further incorporating shared representation learning layers and a propensity prediction regularizer. Experiments on simulated and real data demonstrate that the performances of the proposed methods match or are better than that of existing state-of-arts. Moreover, by learning the target treatment effect function directly, the proposed methods tend to obtain more stable estimates than existing indirect methods.},
  archive      = {J_TKDE},
  author       = {Fujin Zhu and Jie Lu and Adi Lin and Junyu Xuan and Guangquan Zhang},
  doi          = {10.1109/TKDE.2021.3112591},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2457-2470},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Direct learning with multi-task neural networks for treatment effect estimation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deletion propagation revisited for multiple key preserving
views. <em>TKDE</em>, <em>35</em>(3), 2445–2456. (<a
href="https://doi.org/10.1109/TKDE.2021.3110851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deletion propagation is widely studied for single view but rarely for multiple views. We study the deletion propagation problem for multiple views called Mdp . We investigate the complexity and algorithms of Mdp in depth for key-preserving conjunctive queries. We characterize the structure of the set of queries to derive new results. On aspect of combined complexity, for two key-preserving views, it is NP-hard to find an approximation within a constant ratio for Mdp . This implies, if the number of queries and the dimension of every query are unbounded, it is hard to find a good approximation for Mdp even under the key preservation condition. On aspect of data complexity, for two key-preserving views, it is NP-hard to approximate Mdp within a ratio $k-2-\epsilon$ for any $\epsilon &amp;gt;0$ and $k&amp;gt;4$ , if the dimension of each individual query is at most $k$ . On aspect of upper bound, we design a linear programming based approximation algorithm. This implies that, under the key preservation condition, Mdp can be approximated within a ratio $d$ if each query has a dimension at most $d$ and the set of input queries is tree-like. In addition, we show that the approximation ratio can be improved as $2\sqrt{n}$ when $n$ the size of $I$ is smaller than $d^2$ . At last, a dynamic programming based algorithm is proposed to solve the strict star-like case in polynomial time. The strict star-like case is more restrictive than tree-like case under the key preservation condition.},
  archive      = {J_TKDE},
  author       = {Dongjing Miao and Zhipeng Cai and Jianzhong Li},
  doi          = {10.1109/TKDE.2021.3110851},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2445-2456},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deletion propagation revisited for multiple key preserving views},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decoupled representation learning for attributed networks.
<em>TKDE</em>, <em>35</em>(3), 2430–2444. (<a
href="https://doi.org/10.1109/TKDE.2021.3114444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network representation learning or network embedding, which targets at learning the low-dimension representation of graph-based data, has attracted wide attention due to its effectiveness on various network-oriented applications in recent years. Though large efforts have been made on the joint analysis combining node attributes with the network structure, they usually model the interactions between nodes reflected by network structure and attributes in a coupled way and fail to address the common sparse attribute issues. To this end, in this article, we comprehensively study the problem of learning attributed network embedding, which focuses on characterizing different types of interactions among nodes and alleviating the sparse attribute problem as well. Specifically, we propose a novel D e C oupled N etwork E mbedding (DCNE) model to learn node representations in a unified framework. We first respectively project both nodes and attributes into a low-dimensional vectorial space. Then, we introduce a novel “decoupled-fusion” learning process into each graph layer to iteratively generate node embeddings. In particular, we propose two adapted graph convolution modules to decouple the learning of network structure and attributes respectively, and a fusion module to adaptively aggregate the information. Next, we adopt a modified mini-batch algorithm to iteratively aggregate the higher-order information of both nodes and attributes within a multi-task learning framework. Extensive experiments on five public datasets demonstrate that DCNE could outperform state-of-the-art methods on multiple benchmark tasks. Moreover, several qualitative analyses further indicate DCNE can learn more robust and representative node embeddings than other comparison methods for attributed networks.},
  archive      = {J_TKDE},
  author       = {Hao Wang and Defu Lian and Hanghang Tong and Qi Liu and Zhenya Huang and Enhong Chen},
  doi          = {10.1109/TKDE.2021.3114444},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2430-2444},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Decoupled representation learning for attributed networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Current time series anomaly detection benchmarks are flawed
and are creating the illusion of progress. <em>TKDE</em>,
<em>35</em>(3), 2421–2429. (<a
href="https://doi.org/10.1109/TKDE.2021.3112126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series anomaly detection has been a perennially important topic in data science, with papers dating back to the 1950s. However, in recent years there has been an explosion of interest in this topic, much of it driven by the success of deep learning in other domains and for other time series tasks. Most of these papers test on one or more of a handful of popular benchmark datasets, created by Yahoo, Numenta, NASA, etc. In this work we make a surprising claim. The majority of the individual exemplars in these datasets suffer from one or more of four flaws. Because of these four flaws, we believe that many published comparisons of anomaly detection algorithms may be unreliable, and more importantly, much of the apparent progress in recent years may be illusionary. In addition to demonstrating these claims, with this paper we introduce the UCR Time Series Anomaly Archive. We believe that this resource will perform a similar role as the UCR Time Series Classification Archive, by providing the community with a benchmark that allows meaningful comparisons between approaches and a meaningful gauge of overall progress.},
  archive      = {J_TKDE},
  author       = {Renjie Wu and Eamonn J. Keogh},
  doi          = {10.1109/TKDE.2021.3112126},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2421-2429},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-temporal snapshot alignment for dynamic networks.
<em>TKDE</em>, <em>35</em>(3), 2406–2420. (<a
href="https://doi.org/10.1109/TKDE.2021.3115669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the task of cross-temporal snapshot alignment for dynamic networks. The goal of this task is to match equivalent nodes across temporal snapshots of a given dynamic network. Previous static network alignment methods ignore the non-stationary nature of networks, while existing dynamic counterparts focusing on two separate evolving networks do not take into account the problem of aligning two snapshots of the same dynamic network. To alleviate these issues, we propose a Cross-Temporal Snapshot Alignment model (CTSA), which maps nodes from different snapshots into the same semantic space and makes the equivalent nodes in the source and target snapshots to be aligned locate as closely as possible. Our CTSA model utilizes graph neural networks to embed nodes for each snapshot by aggregating the local structural information, and integrates the self-attention based encoders to model the dependencies in different snapshots over time. Additionally, to improve the alignment performance of the model, we contrive a novel positional embedding learning method, which takes into account both the ordering information of input representation sequences at each time step and the graph information of each network snapshot. Experimental results on real-world dynamic networks demonstrate that our model outperforms the state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Shangsong Liang and Shaowei Tang and Zaiqiao Meng and Qiang Zhang},
  doi          = {10.1109/TKDE.2021.3115669},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2406-2420},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-temporal snapshot alignment for dynamic networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constrained generative adversarial learning for
dimensionality reduction. <em>TKDE</em>, <em>35</em>(3), 2394–2405. (<a
href="https://doi.org/10.1109/TKDE.2021.3126642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging data-driven technologies and big data analytics generate and deal with high-dimensional data. Transformation of such data into a low-dimensional feature space comes with several benefits, such as a more discriminant feature space, performance enhancement, less computational burden, and facilitating data visualization. This paper proposes a novel dimensionality reduction algorithm based on generative adversarial networks to tackle the issues related to high-dimensional data and common challenges in dimensionality reduction. To this aim, two constraints are defined to preserve the characteristics of the original data while rectifying the data distribution upon transformation. Formulating the transformation as sequential projections, the proposed Constrained Adversarial Dimensionality Reduction (CADR) method finds a set of sequential weight vectors that lead to a feature space in which between-class separability and within-class integrity are satisfied. This is while the transformed data perfectly comply with the pairwise affinity correlation in the original feature space. To evaluate the proposed method, nine advanced dimensionality reduction techniques are employed to enable a comparative study. The experiments are performed on several real-world benchmark datasets in terms of classification accuracy, F-measure, and G-mean. The obtained results show that CADR could yield classification performance at a satisfactory level and outperforms the other competitors.},
  archive      = {J_TKDE},
  author       = {Ehsan Hallaji and Maryam Farajzadeh-Zanjani and Roozbeh Razavi-Far and Vasile Palade and Mehrdad Saif},
  doi          = {10.1109/TKDE.2021.3126642},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2394-2405},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Constrained generative adversarial learning for dimensionality reduction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Community-aware social recommendation: A unified SCSVD
framework. <em>TKDE</em>, <em>35</em>(3), 2379–2393. (<a
href="https://doi.org/10.1109/TKDE.2021.3117686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender system provides personalized suggestions based on users’ interests and social connections. However, most existing social recommendation models utilize social relationships in a direct manner, i.e., they only consider the user-user connections, neglecting the clustering nature of social networks. As social information recursively spreads in the social network, the community structure, which contains richer information in contrast to pure user-user relationships, would emerge. To dismiss these limitations, in this paper, we propose a unified recommendation framework named Simultaneous Community detection and Singular Value Decomposition (SCSVD), which utilizes the underlying community structure to regularize user latent preferences. We propose a well-designed iterative optimization algorithm to tackle social recommendation efficiently. In addition, we theoretically analyze the proposed algorithm in terms of convergence, time complexity, and also the unified process of community detection and user embedding learning. Extensive experiments are conducted on three benchmark real-world datasets of product reviews, demonstrating the effectiveness, robustness, and flexibility of SCSVD in both rating prediction and top- $N$ recommendation tasks, compared to fifteen state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Jiewen Guan and Xin Huang and Bilian Chen},
  doi          = {10.1109/TKDE.2021.3117686},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2379-2393},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Community-aware social recommendation: A unified SCSVD framework},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Citywide estimation of travel time distributions with
bayesian deep graph learning. <em>TKDE</em>, <em>35</em>(3), 2366–2378.
(<a href="https://doi.org/10.1109/TKDE.2021.3117986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of road link travel time serves a critical role in intelligent transportation operation and management. Due to the uncertainty nature contributed by the volatile traffic, travel time estimates are better described by probability distributions than deterministic models. Existing travel time distribution estimation approaches are mostly based on predefined probability distributions. Other approaches, while relaxing the constraint, fail to utilize the topological information and are data-inefficient. In this paper, we propose a novel Bayesian and geometric deep learning-based approach to estimate the travel time distributions of road links within citywide transportation networks based on vehicular GPS trajectories. Particularly, historical or real-time trajectories are first pre-processed to construct partial travel time maps, which are input into a tailor-made Bayesian graph autoencoder to reconstruct multiple complete travel time maps. We further adopt an auxiliary neural network to facilitate the parameter training of the proposed approach following adversarial training principles. To evaluate the proposed approach, we employ a real-world vehicular trajectory dataset in a series of comprehensive case studies. The empirical results indicate that the proposed approach outperforms the best-performing state-of-the-art baseline with an approximately $10\%$ Kullback-Leibler divergence reduction.},
  archive      = {J_TKDE},
  author       = {James J.Q. Yu},
  doi          = {10.1109/TKDE.2021.3117986},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2366-2378},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Citywide estimation of travel time distributions with bayesian deep graph learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characterizing performance limits in payment channel
networks. <em>TKDE</em>, <em>35</em>(3), 2353–2365. (<a
href="https://doi.org/10.1109/TKDE.2021.3120842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With their instant transaction confirmation and high scalability, payment channel networks (PCNs), running off-chain and in parallel with blockchain systems, have recently attracted a substantial amount of research attention. It has been shown that there exists a significant gap between the theoretically optimal performance and the performance achievable given the stringent privacy requirements in practice. However, it remains unclear what the fundamental performance limits and key factors involved are, which turns out to be a challenging problem due to the unique characteristics in PCNs. In this paper, we, for the first time, develop a mathematical model capturing the PCN performance, and examine the impact from a number of factors including channel capacity and transactions. We are articularly interested in obtaining the gap between the theoretically optimal performance and the performance achievable in practice, which characterizes the design space in PCNs for scheduling transactions. Specifically, we derive how different transactions and channel capacities affect the PCN performance and the performance gap. Our analytical characterization of PCNs offers an in-depth understanding on their fundamental trade-off, and provides important insights on the design of PCNs.},
  archive      = {J_TKDE},
  author       = {Yuechen Tao and Bo Li and Baochun Li and Lei Chen},
  doi          = {10.1109/TKDE.2021.3120842},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2353-2365},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Characterizing performance limits in payment channel networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Burstiness-aware web search analysis on different levels of
evidences. <em>TKDE</em>, <em>35</em>(3), 2341–2352. (<a
href="https://doi.org/10.1109/TKDE.2021.3109304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalizing the analysis for web search potentially improves the search experience. A good analytical model for web search should leverage not only collective wisdom but also individual characteristics. Most of the existing analytical models, however, such as the click graph and its variants, focus on how to utilize the collective wisdom, from a crowd, for instance. In this paper, we address the problem of user-specific web search analysis by considering the so-called burstiness in web search, which captures the behavior of rare words appearing many times in a single document. We go beyond click graph and propose two probabilistic topic models, namely, Topic Independence Model and Topic Dependence Model. The former adopts the assumption that the generation of query terms and URLs are topically independent, and the latter captures the coupling between search queries and URLs. We also capture the temporal burstiness of topics by utilizing the continuous Beta distribution. Based on the two proposed models, we propose a novel burstiness-aware search topic rank. Through a large-scale analysis of a real-life search query log, we observe that each user&#39;s web search trail enjoys multiple kinds of user-based unique characteristics. On a massive search query log, the new models achieve a better held-out likelihood than standard LDA, DCMLDA and TOT, and they can also effectively reveal the latent evolution of topics on the corpus level and user-based level.},
  archive      = {J_TKDE},
  author       = {Chen Zhang and Haodi Zhang and Qifan Li and Kaishun Wu and Di Jiang and Yuanfeng Song and Peiguang Lin and Lei Chen},
  doi          = {10.1109/TKDE.2021.3109304},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2341-2352},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Burstiness-aware web search analysis on different levels of evidences},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bundle recommendation and generation with graph neural
networks. <em>TKDE</em>, <em>35</em>(3), 2326–2340. (<a
href="https://doi.org/10.1109/TKDE.2021.3114586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bundle recommendation aims to recommend a bundle of items for a user to consume as a whole. Related work can be divided into two categories: 1) to recommend the platform&#39;s prebuilt bundles to users; 2) generate personalized bundles for users. In this work, we propose two graph neural network models, a BGCN model (short for Bundle Graph Convolutional Network ) for prebuilt bundle recommendation, and a BGGN model (short for Bundle Graph Generation Network ) for personalized bundle generation. First, BGCN unifies the user-item interaction, the user-bundle interaction and the bundle-item affiliation into a heterogeneous graph. With item nodes as the bridge, graph convolutional propagation between user and bundle nodes makes the learned representations capture the item-level semantics. Second, BGGN re-constructs bundles into graphs based on the item co-occurrence pattern and the user&#39;s supervision signal. The complex and high-order item-item relationships in the bundle graph are explicitly modeled through graph generation. Empirical results demonstrate the substantial performance gains of BGCN and BGGN, which outperforms the state-of-the-art baselines by 10.77\% to 23.18\% and 20.90\% to 64.52\%, respectively. We have released the datasets and codes at this link: https://github.com/cjx0525/BGCN .},
  archive      = {J_TKDE},
  author       = {Jianxin Chang and Chen Gao and Xiangnan He and Depeng Jin and Yong Li},
  doi          = {10.1109/TKDE.2021.3114586},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2326-2340},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Bundle recommendation and generation with graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Block-diagonal guided symmetric nonnegative matrix
factorization. <em>TKDE</em>, <em>35</em>(3), 2313–2325. (<a
href="https://doi.org/10.1109/TKDE.2021.3113943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetric nonnegative matrix factorization (SNMF) is effective to cluster nonlinearly separable data, which uses the constructed graph to capture the structure of inherent clusters. Nevertheless, many SNMF-based clustering approaches implicitly enforce either the sparseness constraint or the smoothness constraint with the limited supervised information in the form of cannot-link or must-link in a semi-supervised manner, which may not be quite satisfactory in many applications where sparseness and smoothness are demanded explicitly and simultaneously. In this paper, we propose a new semi-supervised SNMF-based approach termed Semi-supervised Structured SNMF-based clustering (S3NMF). The method flexibly enforces the block-diagonal structure to the similarity matrix, where the sparseness and smoothness are simultaneously considered, so that we can obtain the desirable assignment matrix by simultaneously learning similarity and assignment matrices in a constrained optimization problem. We formulate S3NMF with a semi-supervised manner and utilize the indirect constraints of sparseness and smoothness by cannot-link and must-link. To effectively solve S3NMF, we present an alternating iterative algorithm with theoretically proved convergence to seek for the solution of the optimization problem. Experiments on five benchmark data sets show better performance and satisfactory stability of the proposed method.},
  archive      = {J_TKDE},
  author       = {Yalan Qin and Guorui Feng and Yanli Ren and Xinpeng Zhang},
  doi          = {10.1109/TKDE.2021.3113943},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2313-2325},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Block-diagonal guided symmetric nonnegative matrix factorization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Binary label learning for semi-supervised feature selection.
<em>TKDE</em>, <em>35</em>(3), 2299–2312. (<a
href="https://doi.org/10.1109/TKDE.2021.3109243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised feature selection methods jointly exploit the labelled and unlabelled samples when selecting the features. Under the semi-supervised learning scenario, the number of labelled data significantly impacts the feature selection performance. In this paper, we introduce the label learning with binary hashing to the research field of feature selection and propose a novel Semi-supervised Feature Selection with Binary Label Learning (SFS-BLL) model. Specifically, we learn the binary hash codes as the pseudo labels by specially imposing binary hash constraints on the spectral embedding process to increase the number of labels. Meanwhile, we propose a self-weighted sparse regression module which exploits the learned labels and given manual labels together with importance differentiation to guide the feature selection process. Finally, we develop an effective discrete optimization method based on the Alternating Direction Method of Multipliers (ADMM) to iteratively optimize the binary labels and the feature selection matrix. Extensive experiments on widely tested benchmarks demonstrate the superiority of the proposed method from various aspects.},
  archive      = {J_TKDE},
  author       = {Dan Shi and Lei Zhu and Jingjing Li and Zhiyong Cheng and Zhenguang Liu},
  doi          = {10.1109/TKDE.2021.3109243},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2299-2312},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Binary label learning for semi-supervised feature selection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated unsupervised graph representation learning.
<em>TKDE</em>, <em>35</em>(3), 2285–2298. (<a
href="https://doi.org/10.1109/TKDE.2021.3115017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph data mining has largely benefited from the recent developments of graph representation learning. Most attempts to improve graph representations have thus far focused on designing new network embedding or graph neural network (GNN) architectures. Inspired by the SGC and ProNE models, we instead focus on enhancing any existing or learned graph representations by further smoothing them via graph filters. In this paper, we introduce an automated framework AutoProNE to achieve this. Specifically, AutoProNE automatically searches for a unique optimal set of graph filters for any input dataset, and its existing representations are then smoothed via the selected filters. To make AutoProNE more general, we adopt self-supervised loss functions to guide the optimization of the automated search process. Extensive experiments on eight commonly used datasets demonstrate that the AutoProNE framework can consistently improve the expressive power of graph representations learned by existing network embedding and GNN methods by up to 44\%. AutoProNE is also implemented in CogDL, an open source graph learning library, to help boost more algorithms.},
  archive      = {J_TKDE},
  author       = {Zhenyu Hou and Yukuo Cen and Yuxiao Dong and Jie Zhang and Jie Tang},
  doi          = {10.1109/TKDE.2021.3115017},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2285-2298},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Automated unsupervised graph representation learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated feature selection: A reinforcement learning
perspective. <em>TKDE</em>, <em>35</em>(3), 2272–2284. (<a
href="https://doi.org/10.1109/TKDE.2021.3115477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a critical step in machine learning that selects the most important features for a subsequent prediction task. Effective feature selection can help to reduce dimensionality, improve prediction accuracy, and increase result comprehensibility. It is traditionally challenging to find the optimal feature subset from the feature subset space as the space could be very large. While much effort has been made on feature selection, reinforcement learning can provide a new perspective towards a more globally-optimal searching strategy. In the preliminary work, we propose a multi-agent reinforcement learning framework for the feature selection problem. Specifically, we first reformulate feature selection with a reinforcement learning framework by regarding each feature as an agent. Besides, we obtain the state of the environment in three ways, i.e., statistic description, autoencoder, and graph convolutional network (GCN), in order to derive a fixed-length state representation as the input of reinforcement learning. In addition, we study how the coordination among feature agents can be improved by a more effective reward scheme. Also, we provide a GMM-based generative rectified sampling strategy to accelerate the convergence of multi-agent reinforcement learning. Our method searches the feature subset space more globally and can be easily adapted to real-time scenarios due to the nature of reinforcement learning. In the extended version, we further accelerate the framework from two aspects. From the sampling aspect, we show the indirect acceleration by proposing a rank-based softmax sampling strategy. From the exploration aspect, we show the direct acceleration by proposing an interactive reinforcement learning (IRL)-based exploration strategy. Extensive experimental results show the significant improvement of the proposed method over conventional approaches.},
  archive      = {J_TKDE},
  author       = {Kunpeng Liu and Yanjie Fu and Le Wu and Xiaolin Li and Charu Aggarwal and Hui Xiong},
  doi          = {10.1109/TKDE.2021.3115477},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2272-2284},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Automated feature selection: A reinforcement learning perspective},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of authorization constraints via integer linear
programming. <em>TKDE</em>, <em>35</em>(3), 2258–2271. (<a
href="https://doi.org/10.1109/TKDE.2021.3124271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on constraint verification and violation resolution for Petri nets (PNs) modeling of role-based access control (RBAC) policy. Checking the satisfiability of authorization constraints imposes a major challenge when the number of states of a target system is large. To overcome this difficulty, we provide three necessary and sufficient conditions to check three different constraints, namely Separation of Duties (SoDs), Binding of Duties (BoDs), and Constraints of Cardinality (CoCs). The proposed results are based on the solutions of integer linear programming problems (ILPs). By relying on an ILP formulation that does not require the explicit computation of the net reachability set, the proposed approach is particularly well suited for large-size PNs. When the given system does not satisfy a considered constraint, the objective is to propose a suitable violation resolution strategy to correctly enforce the given constraint. In this paper, enforcement of control places and administration of RBAC are presented to solve the SoD, BoD, and CoC violations. All violations can be corrected in a once for all manner while simultaneously ensuring the satisfaction of all other constraints. The comparison between our approach and the existing ones is given to illustrate the effectiveness and efficiency of ours.},
  archive      = {J_TKDE},
  author       = {Benyuan Yang and Hesuan Hu},
  doi          = {10.1109/TKDE.2021.3124271},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2258-2271},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Analysis of authorization constraints via integer linear programming},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A-MCTS: Adaptive monte carlo tree search for temporal path
discovery. <em>TKDE</em>, <em>35</em>(3), 2243–2257. (<a
href="https://doi.org/10.1109/TKDE.2021.3120993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An attributed dynamic graph contains multiple dynamic attributes associated with each edge in the graph. In attributed dynamic graph based applications, people usually can specify multiple constraints in the attributes to illustrate their requirements, such as the total cost, the total travel time and the stopover interval of a flight between two cities. This inspires the multi-constrained temporal path discovery in attributed dynamic graphs, which is a challenging NP-complete problem. To solve the problem, the existing methods adopt reinforcement learning with Monte Carlo tree search. However, these reinforcement learning based methods require a certain degree of “discovery experience” to obtain better results, which can lead to the expensive cost of query time and storage space, and thus are not applicable in real-time applications, e.g., route recommendations in a self-driving car. This motivates us to develop a new Adaptive Monte Carlo Tree Search algorithm, namely A-MCTS. A-MCTS dynamically adjusts the priority of historical records that are used in Monte Carlo tree search to improve the performance and reduce the size of required “discovery experience”. In addition, A-MCTS proposes a new adaptive replay memory structure that can store important historical records based on graph properties and optimize the consumption of storage space. The experimental results on ten real-world dynamic graphs demonstrate that the proposed A-MCTS outperforms the state-of-the-art methods in terms of both efficiency and effectiveness.},
  archive      = {J_TKDE},
  author       = {Pengfei Ding and Guanfeng Liu and Yan Wang and Kai Zheng and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2021.3120993},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2243-2257},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A-MCTS: Adaptive monte carlo tree search for temporal path discovery},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive hypergraph auto-encoder for relational data
clustering. <em>TKDE</em>, <em>35</em>(3), 2231–2242. (<a
href="https://doi.org/10.1109/TKDE.2021.3108192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The embedded representation and clustering tasks both play important roles in relational data analysis and mining. Traditional methods mainly employ graph structure to describe relational data, but intuitive pairwise connections among nodes are insufficient to model high-order data in the real-world, such as the relations between proteins and polypeptide chains. Hypergraphs are a generalization of graphs, and hypergraphs can well model high-order data. When modeling relational data in the real world, hypergraphs are often accompanied by node attributes, i.e., attributed hypergraphs. Besides this, how to integrate the structural information and attribute information appropriately is another important task, while has not been investigated systematically. In this paper, we propose Adaptive Hypergraph Auto-Encoder(AHGAE) to learn node embeddings in low-dimensional space. Our method can utilize the high-order relation to generate embedding for clustering. It is composed of two procedures, i.e., the adaptive hypergraph Laplacian smoothing filter and the relational reconstruction auto-encoder. It has the advantage of integrating more complex data relations compared with graph-based methods, which leads to better modeling and clustering performance. The proposed method has been evaluated on hypergraph datasets and benchmark graph datasets. Experimental results and comparison with the state-of-the-art methods have demonstrated the effectiveness of our proposed method.},
  archive      = {J_TKDE},
  author       = {Youpeng Hu and Xunkai Li and Yujie Wang and Yixuan Wu and Yining Zhao and Chenggang Yan and Jian Yin and Yue Gao},
  doi          = {10.1109/TKDE.2021.3108192},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2231-2242},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive hypergraph auto-encoder for relational data clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A ranking based multi-view method for positive and unlabeled
graph classification. <em>TKDE</em>, <em>35</em>(3), 2220–2230. (<a
href="https://doi.org/10.1109/TKDE.2021.3119626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph classification becomes an active research problem in these years, since it can deal with the situation where objects contain structure and rich content information. Most of the previous works focus on graph classification by assuming both positive graphs and negative graphs are available. However, in some case of real-world applications, we always collect the positive graphs and a number of the unlabeled graphs, which is referred as the positive and unlabeled graph learning. Moreover, existing graph classification methods are limited to the case where the graph is described from one perspective. In order to address these problems, this paper proposes a new approach, called multi-view positive and unlabeled graph classification (MVPUG). It combines the strategy of cost-sensitive by introducing similarity weight of graphs, which can control the preference of the penalty for different graphs. And it incorporates the different representations of the graph, which exploits the consensus principle and the complementarity principle among different views of graphs. Extensive experiments on real life datasets have shown that our proposed MVPUG can achieve a better performance for multi-view positive and unlabeled graph classification in comparison to the state-of-the-art graph classification methods.},
  archive      = {J_TKDE},
  author       = {Bo Liu and Zhiyong Che and Haowen Zhong and Yanshan Xiao},
  doi          = {10.1109/TKDE.2021.3119626},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2220-2230},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A ranking based multi-view method for positive and unlabeled graph classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A provable splitting approach for symmetric nonnegative
matrix factorization. <em>TKDE</em>, <em>35</em>(3), 2206–2219. (<a
href="https://doi.org/10.1109/TKDE.2021.3125947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The symmetric Nonnegative Matrix Factorization (NMF), a special but important class of the general NMF, has found numerous applications in data analysis such as various clustering tasks. Unfortunately, designing fast algorithms for the symmetric NMF is not as easy as for its nonsymmetric counterpart, since the latter admits the splitting property that allows state-of-the-art alternating-type algorithms. To overcome this issue, we first split the decision variable and transform the symmetric NMF to a penalized nonsymmetric one, paving the way for designing efficient alternating-type algorithms. We then show that solving the penalized nonsymmetric reformulation returns a solution to the original symmetric NMF. Moreover, we design a family of alternating-type algorithms and show that they all admit strong convergence guarantee: the generated sequence of iterates is convergent and converges at least sublinearly to a critical point of the original symmetric NMF. Finally, we conduct experiments on both synthetic data and real image clustering to support our theoretical results and demonstrate the performance of the alternating-type algorithms.},
  archive      = {J_TKDE},
  author       = {Xiao Li and Zhihui Zhu and Qiuwei Li and Kai Liu},
  doi          = {10.1109/TKDE.2021.3125947},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2206-2219},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A provable splitting approach for symmetric nonnegative matrix factorization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A distributed integrated feature selection scheme for column
subset selection. <em>TKDE</em>, <em>35</em>(3), 2193–2205. (<a
href="https://doi.org/10.1109/TKDE.2021.3108146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of computer applications often encounter huge volumes of data which need to be stored and processed in a distributed way. Most of the existing distributed feature selection schemes neglect how good the subsets are that are mapped to the computational nodes, which causes a waste of time and hardware resources. In this paper, we propose a distributed integrated feature selection scheme (DIFS) with Subset Quality Evaluation (SQE). SQE studies the relevance between the quality of a subset and the number of selected features from this subset, which helps shorten the feature selection time efficiently. Feature selection algorithms used in our method and the evaluation metric used in SQE are integrable. Then, we have given the implementation of our scheme for the Column Subset Selection (CSS) problem. More specifically, we integrate a CSS algorithm in DIFS and information entropy as the SQE metric. Theoretically, we prove that the speedup of DIFS can reach $m^3$ compared to the centralized algorithm in ideal situations where $m$ is the number of computational nodes, and give a well bounded approximation guarantee of the solution generated by scheme for CSS problem. Extensive experiments on eight data sets are used to verify the performance of scheme. Experiments results demonstrate the effectiveness of SQE and the impressive speedup DIFS can achieve. Although there is a slight increase of the reconstruction error value in some situations. Additional experiments of classification tasks reveal that the performance of DIFS is better than existing state-of-the-art distributed algorithms.},
  archive      = {J_TKDE},
  author       = {Zheng Xiao and PengCheng Wei and Anthony Theodore Chronopoulos and Anne C. Elster},
  doi          = {10.1109/TKDE.2021.3108146},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2193-2205},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A distributed integrated feature selection scheme for column subset selection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A branch elimination-based efficient algorithm for
large-scale multiple longest common subsequence problem. <em>TKDE</em>,
<em>35</em>(3), 2179–2192. (<a
href="https://doi.org/10.1109/TKDE.2021.3115057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a key issue to find out all longest common subsequences of multiple sequences over a set of finite alphabets, namely MLCS problem, in computational biology, pattern recognition and information retrieval, to name a few. However, it is very challenging to tackle the large-scale MLCS problem effectively and efficiently due to the high complexity of time and space. To this end, this paper will therefore propose a Branch Elimination-based Space and Time efficient algorithm called BEST-MLCS , which includes the following four key strategies: 1) Estimation scheme for the lower bound of the length of MLCS. 2) Estimation scheme for the upper bound of the length of the paths through the current match point. 3) Branch elimination strategy by finding all useless match points and removing the branches not on the longest paths. 4) A new Directed Acyclic Graph (DAG) construction method for constructing the smallest DAG among the existing ones. As a result, the proposed algorithm BEST-MLCS can save a lot of space and time and can handle much larger scale MLCS problems than the existing algorithms. Extensive experiments conducted on biological DNA sequences show that the performance of the proposed algorithm BEST-MLCS outperforms three state-of-the-art algorithms in terms of run-time and memory consumption.},
  archive      = {J_TKDE},
  author       = {Shiwei Wei and Yuping Wang and Yiu-ming Cheung},
  doi          = {10.1109/TKDE.2021.3115057},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {3},
  pages        = {2179-2192},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A branch elimination-based efficient algorithm for large-scale multiple longest common subsequence problem},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When convolutional network meets temporal heterogeneous
graphs: An effective community detection method. <em>TKDE</em>,
<em>35</em>(2), 2173–2178. (<a
href="https://doi.org/10.1109/TKDE.2021.3096122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection has long been an important yet challenging task to analyze complex networks with a focus on detecting topological structures of graph data. Essentially, real-world graph data is generally heterogeneous which dynamically varies over time, and this invalidates most existing community detection approaches. To cope with these issues, this paper proposes the temporal-heterogeneous graph convolutional networks (THGCN) to detect communities using the learnt feature representations of a set of temporal heterogeneous graphs. Particularly, we first design a heterogeneous GCN component to represent features of heterogeneous graph at each time step. Then, a residual compressed aggregation component is proposed to learn temporal feature representations extracted from two consecutive heterogeneous graphs. These temporal features are considered to contain evolutionary patterns of underlying communities. To the best of our knowledge, this is the first attempt to detect communities from temporal heterogeneous graphs. To evaluate the model performance, extensive experiments are performed on two real-world datasets, i.e., DBLP and IMDB. The promising results have demonstrated that the proposed THGCN is superior to both benchmark and the state-of-the-art approaches, e.g., GCN, GAT, GNN, LGNN, HAN and STAR, with respect to a number of evaluation criteria.},
  archive      = {J_TKDE},
  author       = {Yaping Zheng and Xiaofeng Zhang and Shiyi Chen and Xinni Zhang and Xiaofei Yang and Di Wang},
  doi          = {10.1109/TKDE.2021.3096122},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2173-2178},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {When convolutional network meets temporal heterogeneous graphs: An effective community detection method},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Who should deserve investment? Attractive individual and
group search in dynamic information networks. <em>TKDE</em>,
<em>35</em>(2), 2158–2172. (<a
href="https://doi.org/10.1109/TKDE.2021.3099102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing dynamic information networks, which contain evolving objects and links, to meet users’ various needs has attracted much attention in recent years. For sales companies, recruiting staff who are socialites would help to increase sales volume since such staff often sell more. For universities, employing active collaborators who are productive and well connected to many different scholars over time could bring many benefits to their development. However, no previous work has focused on the discovery of socialites and active collaborators who are worthy of investment in reality. In this paper, we advocate a new concept of attractive individuals to model such special objects. We also introduce the concept of attractive groups to represent groups of well-connected attractive individuals. We analyze the complexity of the attractive individual and group search problems. A time and space efficient algorithm is presented to detect attractive individuals. Furthermore, three algorithms are respectively proposed to find top- $k$ representative attractive groups. Experiments on 6 real-world datasets show high performance of our methods and the significance of attractive individuals and groups in reality.},
  archive      = {J_TKDE},
  author       = {Xinrui Wang and Hong Gao and Zhipeng Cai and Jianzhong Li},
  doi          = {10.1109/TKDE.2021.3099102},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2158-2172},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Who should deserve investment? attractive individual and group search in dynamic information networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). What is my problem? Identifying formal tasks and metrics in
data mining on the basis of measurement theory. <em>TKDE</em>,
<em>35</em>(2), 2147–2157. (<a
href="https://doi.org/10.1109/TKDE.2021.3109823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design and analysis of experimental research in Data Mining (DM) is anchored in a correct choice of the type of task addressed (clustering, classification, regression, etc.). However, although DM is a relatively mature discipline, there is no consensus yet about what is the taxonomy of DM tasks, which are their formal characteristics, and their corresponding metrics. In this paper, we formalize DM tasks in terms of Measurement Theory, which is a cornerstone of quantitative research in many disciplines, but has not yet been incorporated (in a consensual way) into some areas of Computer Science, including DM. The proposed formal framework provides a methodology to precisely define DM tasks for any given scenario and identify appropriate metrics. We validate this framework via (i) its coverage of existing DM tasks, (ii) its capability to group existing metrics into families, and (iii) its coverage of actual DM research problems, using about 250 papers from ACM KDD 2019 and IEEE ICDM 2019 conferences as reference sample.},
  archive      = {J_TKDE},
  author       = {Enrique Amigó and Julio Gonzalo and Stefano Mizzaro},
  doi          = {10.1109/TKDE.2021.3109823},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2147-2157},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {What is my problem? identifying formal tasks and metrics in data mining on the basis of measurement theory},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Urban flow pattern mining based on multi-source
heterogeneous data fusion and knowledge graph embedding. <em>TKDE</em>,
<em>35</em>(2), 2133–2146. (<a
href="https://doi.org/10.1109/TKDE.2021.3098612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban flow analysis is an essential research for smart city construction, in which urban flow pattern analysis focuses on the continuous state of urban flow. How to mine, store and reuse traffic patterns from urban multi-source heterogeneous big data is challenging. Therefore, this paper proposes a knowledge mining network for regional flow pattern to mine and store the urban flow pattern. The proposed model consists of two modules. In the first module, the features of the region and its flow pattern are extracted as the entity and relation, respectively. In the second module, POI features are modeled to enhance the embedding representation of relation and entity. Based on the translation distance method, the knowledge triplets of regional flow patterns are mined. Finally, the proposed model is compared with some benchmark methods using Chengdu Didi order and POI datasets. Experimental results show that the proposed model is effective. In addition, the knowledge triplets are visualized and some application examples are introduced.},
  archive      = {J_TKDE},
  author       = {Jia Liu and Tianrui Li and Shenggong Ji and Peng Xie and Shengdong Du and Fei Teng and Junbo Zhang},
  doi          = {10.1109/TKDE.2021.3098612},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2133-2146},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Urban flow pattern mining based on multi-source heterogeneous data fusion and knowledge graph embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Unsupervised deep anomaly detection for multi-sensor
time-series signals. <em>TKDE</em>, <em>35</em>(2), 2118–2132. (<a
href="https://doi.org/10.1109/TKDE.2021.3102110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, multi-sensor technologies are applied in many fields, e.g., Health Care (HC), Human Activity Recognition (HAR), and Industrial Control System (ICS). These sensors can generate a substantial amount of multivariate time-series data. Unsupervised anomaly detection on multi-sensor time-series data has been proven critical in machine learning researches. The key challenge is to discover generalized normal patterns by capturing spatial-temporal correlation in multi-sensor data. Beyond this challenge, the noisy data is often intertwined with the training data, which is likely to mislead the model by making it hard to distinguish between the normal, abnormal, and noisy data. Few of previous researches can jointly address these two challenges. In this paper, we propose a novel deep learning-based anomaly detection algorithm called Deep Convolutional Autoencoding Memory network (CAE-M). We first build a Deep Convolutional Autoencoder to characterize spatial dependence of multi-sensor data with a Maximum Mean Discrepancy (MMD) to better distinguish between the noisy, normal, and abnormal data. Then, we construct a Memory Network consisting of linear (Autoregressive Model) and non-linear predictions (Bidirectional LSTM with Attention) to capture temporal dependence from time-series data. Finally, CAE-M jointly optimizes these two subnetworks. We empirically compare the proposed approach with several state-of-the-art anomaly detection methods on HAR and HC datasets. Experimental results demonstrate that our proposed model outperforms these existing methods.},
  archive      = {J_TKDE},
  author       = {Yuxin Zhang and Yiqiang Chen and Jindong Wang and Zhiwen Pan},
  doi          = {10.1109/TKDE.2021.3102110},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2118-2132},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised deep anomaly detection for multi-sensor time-series signals},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uniting heterogeneity, inductiveness, and efficiency for
graph representation learning. <em>TKDE</em>, <em>35</em>(2), 2103–2117.
(<a href="https://doi.org/10.1109/TKDE.2021.3100529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ubiquitous graph-structured data in various applications, models that can learn compact but expressive vector representations of nodes have become highly desirable. Recently, bearing the message passing paradigm, graph neural networks (GNNs) have greatly advanced the performance of node representation learning on graphs. However, a majority class of GNNs are only designed for homogeneous graphs, leading to inferior adaptivity to the more informative heterogeneous graphs with various types of nodes and edges. Also, despite the necessity of inductively producing representations for completely new nodes (e.g., in streaming scenarios), few heterogeneous GNNs can bypass the transductive learning scheme where all nodes must be known during training. Furthermore, the training efficiency of most heterogeneous GNNs has been hindered by their sophisticated designs for extracting the semantics associated with each meta path or relation. In this paper, we propose wi de and de ep message passing n etwork (WIDEN) to cope with the aforementioned problems about heterogeneity, inductiveness, and efficiency that are rarely investigated together in graph representation learning. In WIDEN, we propose a novel inductive, meta path-free message passing scheme that packs up heterogeneous node features with their associated edges from both low- and high-order neighbor nodes. To further improve the training efficiency, we innovatively present an active downsampling strategy that drops unimportant neighbor nodes to facilitate faster information propagation. Experiments on three real-world heterogeneous graphs have further validated the efficacy of WIDEN on both transductive and inductive node representation learning, as well as the superior training efficiency against state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Tong Chen and Hongzhi Yin and Jie Ren and Zi Huang and Xiangliang Zhang and Hao Wang},
  doi          = {10.1109/TKDE.2021.3100529},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2103-2117},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Uniting heterogeneity, inductiveness, and efficiency for graph representation learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-aware gradient attack on dynamic network link
prediction. <em>TKDE</em>, <em>35</em>(2), 2091–2102. (<a
href="https://doi.org/10.1109/TKDE.2021.3110580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network link prediction, it is possible to hide a target link from being predicted with a small perturbation on network structure. This observation may be exploited in many real world scenarios, for example, to preserve privacy, or to exploit financial security. There have been many recent studies to generate adversarial examples to mislead deep learning models on graph data. However, none of the previous work has considered the dynamic nature of real-world systems. In this work, we present the first study of adversarial attack on dynamic network link prediction (DNLP). The proposed attack method, namely time-aware gradient attack (TGA), utilizes the gradient information generated by deep dynamic network embedding (DDNE) across different snapshots to rewire a few links, so as to make DDNE fail to predict target links. We implement TGA in two ways: One is based on traversal search, namely TGA-Tra; and the other is simplified with greedy search for efficiency, namely TGA-Gre. We conduct comprehensive experiments which show the outstanding performance of TGA in attacking DNLP algorithms.},
  archive      = {J_TKDE},
  author       = {Jinyin Chen and Jian Zhang and Zhi Chen and Min Du and Qi Xuan},
  doi          = {10.1109/TKDE.2021.3110580},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2091-2102},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Time-aware gradient attack on dynamic network link prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time2Graph+: Bridging time series and graph representation
learning via multiple attentions. <em>TKDE</em>, <em>35</em>(2),
2078–2090. (<a href="https://doi.org/10.1109/TKDE.2021.3094908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series modeling has attracted great research interests in the last decades. Among the literature, shapelet -based models aim to extract representative subsequences, and could offer explanatory insights in the downstream tasks. But most of those works ignore the seasonal effects on the subsequences, as well as the evolutionary characteristics of shapelets. In order to capture the shapelet dynamics and evolutions, in this paper, we propose a novel framework of bridging time series representation learning and graph modeling, with two different implementations. We first formulate the process of extracting time-aware shapelets by directly adding time-level attentions, then introduce the key idea of transforming time series data into shapelet evolution graphs, to model the shapelet evolutionary patterns. A straightforward solution is to enumerate all possible shapelet transitions among adjacent time series segments, and apply a random-walk-based graph embedding algorithm to learn time series representations ( Time2Graph ). We further extend Time2Graph by adopting graph attention mechanism to refine the procedure of modeling shapelet evolutions, namely Time2Graph+ . Specifically, we transform each time series data into a unique unweighted shapelet graph, and use GAT to automatically capture the correlations between shapelets. Experimental results on three real-world datasets show the significant improvements of Time2Graph+ over Time2Graph and 17 baseline methods, and observational analysis demonstrates the effectiveness and interpretability brought by both time-level and graph-level attentions. Furthermore, the success of online deployment of Time2Graph+ model in State Grid of China validates the whole framework in the real-world application. Codes and documentations are available at https://github.com/petecheng/Time2GraphPlus .},
  archive      = {J_TKDE},
  author       = {Ziqiang Cheng and Yang Yang and Shuo Jiang and Wenjie Hu and Zhangchi Ying and Ziwei Chai and Chunping Wang},
  doi          = {10.1109/TKDE.2021.3094908},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2078-2090},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Time2Graph+: Bridging time series and graph representation learning via multiple attentions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The mining of urban hotspots based on multi-source location
data fusion. <em>TKDE</em>, <em>35</em>(2), 2061–2077. (<a
href="https://doi.org/10.1109/TKDE.2021.3109581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban hotspots reflect the degree of residents’ travel gathering. The study of urban hotspots has important values for urban infrastructure planning, public security and other aspects. In existing researches, single-source location data and density-based clustering algorithms are used to mine hotspots. Due to the one-sidedness of using the single-source data, the mining of hotspots based on multi-source location data fusion has become a hot topic. Multi-source location data fusion requires a quantity balance between the data set to be fused, because several famous clustering algorithms cannot handle multi-source imbalanced data set. To solve this problem, we propose a novel framework to mine urban hotspots. First, we construct a data imputation model for the sparse data set so that reducing the difference in quantity between two types of data set. Then, a clustering algorithm for imbalanced data set is proposed, and a novel evaluation metric is designed to verify the effectiveness of clustering results. The experiment uses real data set including POI data, check-in data and GPS trajectory data. The results show that the proposed method discovers more than 90\% of urban hotspots formed by fused imbalanced data set, and it is more accurate and efficient than the state-of-the-art algorithms.},
  archive      = {J_TKDE},
  author       = {Li Cai and Haoyu Wang and Cong Sha and Fang Jiang and Yihan Zhang and Wei Zhou},
  doi          = {10.1109/TKDE.2021.3109581},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2061-2077},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {The mining of urban hotspots based on multi-source location data fusion},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tackling virtual and real concept drifts: An adaptive
gaussian mixture model approach. <em>TKDE</em>, <em>35</em>(2),
2048–2060. (<a href="https://doi.org/10.1109/TKDE.2021.3099690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world applications have been dealing with large amounts of data that arrive over time and generally present changes in their underlying joint probability distribution, i.e., concept drift. Concept drift can be subdivided into two types: virtual drift, which affects the unconditional probability distribution $p(\boldsymbol{x})$ , and real drift, which affects the conditional probability distribution $p(y|\boldsymbol{x})$ . Existing work focuses on real drift. However, strategies to cope with real drift may not be the best suited for dealing with virtual drift, since the real class boundaries remain unchanged. We provide the first in depth analysis of the differences between the impact of virtual and real drifts on classifiers’ suitability. We propose an approach to handle both drifts called On-line Gaussian Mixture Model With Noise Filter For Handling Virtual and Real Concept Drifts (OGMMF-VRD). Experiments with seven synthetics and seven real-world datasets show that OGMMF-VRD outperforms other approaches with separate mechanisms to deal with virtual and real drifts. It also has more stable rankings and smaller drops in performance during drifting periods than existing ensemble approaches, thus, being more reliable for adoption in practice.},
  archive      = {J_TKDE},
  author       = {Gustavo H. F. M. Oliveira and Leandro L. Minku and Adriano L. I. Oliveira},
  doi          = {10.1109/TKDE.2021.3099690},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2048-2060},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Tackling virtual and real concept drifts: An adaptive gaussian mixture model approach},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SOUP: Spatial-temporal demand forecasting and competitive
supply in transportation. <em>TKDE</em>, <em>35</em>(2), 2034–2047. (<a
href="https://doi.org/10.1109/TKDE.2021.3110778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a setting with an evolving set of requests for transportation from an origin to a destination before a deadline and a set of agents capable of servicing the requests. In this setting, an authority assigns agents to requests such that the average idle time of the agents is minimized. An example is the scheduling of taxis (agents) to meet incoming passenger requests for trips while ensuring that the taxis are empty as little as possible. We address the problem of spatial-temporal demand forecasting and competitive supply (SOUP) in two steps. First, we build a granular model that provides spatial-temporal predictions of requests. Specifically, we propose a Spatial-Temporal Graph Convolutional Sequential Learning ( ${{\sf ST\text{-}GCSL}}$ ) model that predicts requests across locations and time slots. Second, we provide means of routing agents to request origins while avoiding competition among the agents. In particular, we develop a demand-aware route planning ( ${{\sf DROP}}$ ) algorithm that considers both the spatial-temporal predictions and the supply-demand state. We report on extensive experiments with real-world data that offer insight into the performance of the solution and show that it is capable of outperforming the state-of-the-art proposals.},
  archive      = {J_TKDE},
  author       = {Bolong Zheng and Qi Hu and Lingfeng Ming and Jilin Hu and Lu Chen and Kai Zheng and Christian S. Jensen},
  doi          = {10.1109/TKDE.2021.3110778},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2034-2047},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SOUP: Spatial-temporal demand forecasting and competitive supply in transportation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ScaleG: A distributed disk-based system for vertex-centric
graph processing. <em>TKDE</em>, <em>35</em>(2), 2019–2033. (<a
href="https://doi.org/10.1109/TKDE.2021.3101057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing distributed graph systems has drawn a lot of research interests due to the strong expressiveness of the graph model and rapidly increasing graph volume. Most of them require the graph data and all intermediate messages to reside in main memory, which may sacrifice the scalability. Even though several disk-based systems have been studied to remedy such issue, several challenges still exist in achieving both high computational efficiency and low network communication under the limitation of memory usage. In this paper, we design a novel disk-based distributed graph system, called ScaleG . The system provides a series of user-friendly programming interfaces. Unlike previous systems, the programmer in ScaleG does not need to concern any logic regarding the communication between vertices like sending messages and combining messages. In addition, we propose several techniques to reduce both disk I/Os in each machine and message I/Os via the network. We manage all messages in memory and bound all messages by the number of vertices. We also carefully design the data structure to support partial computation and automatic vertex activation. We conduct extensive experiments on six big graphs to show the high efficiency of our system.},
  archive      = {J_TKDE},
  author       = {Xubo Wang and Dong Wen and Lu Qin and Lijun Chang and Ying Zhang and Wenjie Zhang},
  doi          = {10.1109/TKDE.2021.3101057},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2019-2033},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ScaleG: A distributed disk-based system for vertex-centric graph processing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SamWalker++: Recommendation with informative sampling
strategy. <em>TKDE</em>, <em>35</em>(2), 2004–2018. (<a
href="https://doi.org/10.1109/TKDE.2021.3102080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation from implicit feedback is a highly challenging task due to the lack of reliable negative feedback data. Existing methods address this challenge by treating all the un-observed data as negative (dislike) but downweight the confidence of these data. However, this treatment causes two problems: (1) Confidence weights of the unobserved data are usually assigned manually, which lack flexibility and may create empirical bias on evaluating user&#39;s preference. (2) To handle massive volume of the unobserved feedback data, most of the existing methods rely on stochastic inference and data sampling strategies. However, since a user is only aware of a very small fraction of items in a large dataset, it is difficult for existing samplers to select informative training instances in which the user really dislikes the item rather than does not know it. To address the above two problems, we propose two novel recommendation methods SamWalker and SamWalker++ that support both adaptive confidence assignment and efficient model learning. SamWalker models data confidence with a social network-aware function, which can adaptively specify different weights to different data according to users’ social contexts . However, the social network information may not be available in many recommender systems, which hinders application of SamWalker. Thus, we further propose SamWalker++, which does not require any side information and models data confidence with a constructed pseudo-social network. In the pseudo-social network, similar users are connected with specific item nodes or community nodes. This way, the inference of one&#39;s data confidence can benefit from the knowledge from other similar users. We also develop fast random-walk-based sampling strategies for our SamWalker and SamWalker++ to adaptively draw informative training instances, which can speed up gradient estimation and reduce sampling variance. Extensive experiments on five real-world datasets demonstrate the superiority of the proposed SamWalker and SamWalker++.},
  archive      = {J_TKDE},
  author       = {Can Wang and Jiawei Chen and Sheng Zhou and Qihao Shi and Yan Feng and Chun Chen},
  doi          = {10.1109/TKDE.2021.3102080},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {2004-2018},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SamWalker++: Recommendation with informative sampling strategy},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust principal component analysis based on discriminant
information. <em>TKDE</em>, <em>35</em>(2), 1991–2003. (<a
href="https://doi.org/10.1109/TKDE.2021.3093447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, several robust principal component analysis (RPCA) models were presented to enhance the robustness of PCA by exploiting the robust norms as their loss functions. But an important problem is that they have no ability to discriminate outliers from correct samples. To solve this problem, we propose a robust principal component analysis based on discriminant information (RPCA-DI). RPCA-DI disentangles the robust PCA with a two-step fashion: the identification and the processing of outliers. To identity outliers, a sample representation model based on entropy regularization is constructed to analyze the membership of data belonging to the principal component space(PC) and its orthogonal complement(OC), the discriminative information of data will be extracted based on measuring the differences of retained information on PC(or OC) of data. By this way, we can discriminate correct samples when we deal with outliers, which is more reasonable for robustness learning respective to previous works. In the noise processing step, in addition to considering the levels of noise, the resistance of the sample points to noise is also considered to prevent overfitting, thereby improving the generalization performance of RPCA-DI. Finally, an iterative algorithm is designed to solve the corresponding model. Compared with some state-of-art RPCA methods on artificial datasets, UCI datasets and face databases that verifies the effectiveness of our proposed algorithm.},
  archive      = {J_TKDE},
  author       = {Yunlong Gao and Tingting Lin and Yisong Zhang and Sizhe Luo and Feiping Nie},
  doi          = {10.1109/TKDE.2021.3093447},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1991-2003},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust principal component analysis based on discriminant information},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Querying for interactions. <em>TKDE</em>, <em>35</em>(2),
1977–1990. (<a href="https://doi.org/10.1109/TKDE.2021.3094997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning and Computer Vision advances enabled sophisticated information extraction out of images and videos. Recent research aims to make objects, their types and relative locations, first class citizens for query processing purposes. We initiate research to explore declarative queries for real time video streams involving objects and their interactions. We seek to efficiently identify frames in which an object is interacting with another in a specific way. We propose progressive filters (PF) algorithm which deploys a sequence of inexpensive and less accurate filters to detect the presence of query specified objects on frames. We demonstrate that PF derives a least cost sequence of filters given the query objects’ current selectivities. Since selectivities may vary as the video evolves, we present a statistical test to determine when to trigger filters’ re-optimization. Finally, we present Interaction Sheave, a filtering approach that uses learned spatial information about objects and interactions to prune frames that are unlikely to involve the query specified action between them, thus improving the frame processing rate. We present the results of a thorough experimental evaluation involving real datasets. We experimentally demonstrate that our techniques can improve query performance (up to an order of magnitude) while maintaining competitive F1-score.},
  archive      = {J_TKDE},
  author       = {Ioannis Xarchakos and Nick Koudas},
  doi          = {10.1109/TKDE.2021.3094997},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1977-1990},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Querying for interactions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum circuit learning with parameterized boson sampling.
<em>TKDE</em>, <em>35</em>(2), 1965–1976. (<a
href="https://doi.org/10.1109/TKDE.2021.3095103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A quantum circuit learning approach is studied to carry out the fast-fitting of Gaussian functions. First, a parameterized structure is designed for quantum circuits based on the boson sampling model. And then, the training procedure of exploiting gradient-based optimizations is presented to iteratively update the gradient of the loss function concerning circuit parameters. For efficiency, two kinds of circuit loss, the kernel maximum mean discrepancy and the mean absolute error, are used in the training procedure, which are both competent to achieve quantum circuit learning well. It is significant that the two circuit losses assist in reducing the variance to $2.54 \times 10^{-6}$ and $6.91 \times 10^{-6}$ , respectively. Finally, a kind of quantum circuit fixed structure is developed with the boson sampling model that can decrease the model complexity as the circuit depth $d$ grows. Sets of experiments have been conducted to evaluate the proposed quantum circuit learning scheme, and demonstrate that our parameterized approach is efficient and promising, and it is worth looking forward to solving practical application problems with quantum computers since valid quantum circuits for Gaussian function fast-fitting can be designed indeed.},
  archive      = {J_TKDE},
  author       = {Jinjing Shi and Yongze Tang and Yuhu Lu and Yanyan Feng and Ronghua Shi and Shichao Zhang},
  doi          = {10.1109/TKDE.2021.3095103},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1965-1976},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Quantum circuit learning with parameterized boson sampling},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Propagation enhanced neural message passing for graph
representation learning. <em>TKDE</em>, <em>35</em>(2), 1952–1964. (<a
href="https://doi.org/10.1109/TKDE.2021.3102964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Network (GNN) is capable of applying deep neural networks to graph domains. Recently, Message Passing Neural Networks (MPNNs) have been proposed to generalize several existing graph neural networks into a unified framework. For graph representation learning, MPNNs first generate discriminative node representations using the message passing function and then read from the node representation space to generate a graph representation using the readout function. In this paper, we analyze the representation capacity of the MPNNs for aggregating graph information and observe that the existing approaches ignore the self-loop for graph representation learning, leading to limited representation capacity. To alleviate this issue, we introduce a simple yet effective propagation enhanced extension, Self-Connected Neural Message Passing (SC-NMP), which aggregates the node representations of the current step and the graph representation of the previous step. To further improve the information flow, we also propose a Densely Self-Connected Neural Message Passing (DSC-NMP) that connects each layer to every other layer in a feed-forward fashion. Both proposed architectures are applied at each layer and the graph representation can then be used as input into all subsequent layers. Remarkably, combining these two architectures with existing GNN variants can improve these models’ performance for graph representation learning. Extensive experiments on various benchmark datasets strongly demonstrate the effectiveness, leading to superior performance for graph classification and regression tasks.},
  archive      = {J_TKDE},
  author       = {Xiaolong Fan and Maoguo Gong and Yue Wu and A. K. Qin and Yu Xie},
  doi          = {10.1109/TKDE.2021.3102964},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1952-1964},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Propagation enhanced neural message passing for graph representation learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Persuade to click: Context-aware persuasion model for online
textual advertisement. <em>TKDE</em>, <em>35</em>(2), 1938–1951. (<a
href="https://doi.org/10.1109/TKDE.2021.3110724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, due to the prevalence of online textual advertisements, increasing businesses recognize their huge potential in product promotion. The high-quality textual content has been empirically shown to have a substantial impact on consumers’ attitudes and decisions. As a result, persuasive tactics play an essential role in online textual advertisements, which are employed to increase the attractiveness, and sequentially increase the conversion rate and sales volume. As the context of persuasion, product attributes, e.g., category and price, also greatly influence the persuasion outcomes. However, they are largely overlooked by existing works. In this paper, we propose a novel framework to study context-aware persuasion by designing a multi-task learning model and performing extensive causal analysis. First, the prediction model recognizes the persuasive tactics employed in an advertising text and predicts their promotion effectiveness. Specifically, we design a disentangled representation learning algorithm to capture the persuasive tactics, and then develop a novel context-aware attention module to model the relationships between persuasive tactics and product attributes. Experiments on a large-scale real-world dataset demonstrate the superior performance of our proposed model over state-of-the-art baselines. Then we show its great practical value by conducting an in-depth causal analysis of context-aware results that our model learns, which offers insightful interpretations and guidelines for marketers to employ persuasive tactics in textual advertisements.},
  archive      = {J_TKDE},
  author       = {Yuan Yuan and Fengli Xu and Hancheng Cao and Guozhen Zhang and Pan Hui and Yong Li and Depeng Jin},
  doi          = {10.1109/TKDE.2021.3110724},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1938-1951},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Persuade to click: Context-aware persuasion model for online textual advertisement},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partition-aware graph pattern based node matching with
updates. <em>TKDE</em>, <em>35</em>(2), 1922–1937. (<a
href="https://doi.org/10.1109/TKDE.2021.3103914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Pattern based Node Matching (GPNM) is to find all the matches of the nodes in a data graph $G_D$ based on a given pattern graph $G_P$ . GPNM has become increasingly important in many applications, e.g., group finding and expert recommendation. In real scenarios, both $G_P$ and $G_D$ are updated frequently. However, the existing GPNM methods either need to perform a new GPNM procedure from scratch to deliver the node matching results based on the updated $G_P$ and $G_D$ or incrementally perform the GPNM procedure for each of the updates, leading to low efficiency. Although the elimination relations between updates and partitions of data graphs are considered in the state-of-the-art method, it still suffers from low efficiency as only the labels of nodes are considered in the partitions. Therefore, there is a pressing need for a new method to efficiently deliver the node matching results on the updated graphs. In this paper, we propose a new Partition-aware GPNM algorithm, called P-GPNM, where we propose two new partition methods, i.e., connection-based partition and density-based partition . In these two methods, P-GPNM considers the dense connections between partitions and the inner connections inside a single partition, respectively. The experimental results on five real-world social graphs demonstrate that our proposed P-GPNM is much more efficient than the state-of-the-art GPNM methods.},
  archive      = {J_TKDE},
  author       = {Guohao Sun and Guanfeng Liu and Yan Wang and Mehmet A. Orgun and Quan Z. Sheng and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2021.3103914},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1922-1937},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Partition-aware graph pattern based node matching with updates},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the benefits of two dimensional metric learning.
<em>TKDE</em>, <em>35</em>(2), 1909–1921. (<a
href="https://doi.org/10.1109/TKDE.2021.3100353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study two dimensional metric learning (2DML) for matrix data from both theoretical and algorithmic perspectives. We first investigate the generalization bounds of 2DML based on the notion of Rademacher complexity, which theoretically justifies the benefits of learning from matrices directly. Furthermore, we present a novel boosting-based algorithm that scales well with the feature dimension. Finally, we introduce an efficient rank-one correction algorithm, which is tailored to our boosting learning procedure to produce a low-rank solution to 2DML. As our algorithm works directly on the data in matrix representation, it scales well with the feature dimension, keeps the structure and dependence in the data, and has a more compact structure and much fewer parameters to optimize. Extensive evaluations on several benchmark data sets also empirically verify the effectiveness and efficiency of our algorithm.},
  archive      = {J_TKDE},
  author       = {Di Wu and Fan Zhou and Boyu Wang and Qicheng Lao and Chi Man Wong and Changjian Shui and Yuan Zhou and Feng Wan},
  doi          = {10.1109/TKDE.2021.3100353},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1909-1921},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On the benefits of two dimensional metric learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OAG<span class="math inline"><sub>know</sub></span> know:
Self-supervised learning for linking knowledge graphs. <em>TKDE</em>,
<em>35</em>(2), 1895–1908. (<a
href="https://doi.org/10.1109/TKDE.2021.3090830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a self-supervised embedding learning framework—SelfLinKG—to link concepts in heterogeneous knowledge graphs. Without any labeled data, SelfLinKG can achieve competitive performance against its supervised counterpart, and significantly outperforms state-of-the-art unsupervised methods by 26\%-50\% under linear classification protocol. The essential components of SelfLinKG are local attention-based encoding and momentum contrastive learning. The former aims to learn the graph representation using an attention network, while the latter is to learn a self-supervised model across knowledge graphs using contrastive learning. SelfLinKG has been deployed to build the the new version, called OAG $_{\mathrm {know}}$ of Open Academic Graph (OAG). All data and codes are publicly available.},
  archive      = {J_TKDE},
  author       = {Xiao Liu and Li Mian and Yuxiao Dong and Fanjin Zhang and Jing Zhang and Jie Tang and Peng Zhang and Jibing Gong and Kuansan Wang},
  doi          = {10.1109/TKDE.2021.3090830},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1895-1908},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {OAG$_{\mathrm {know}}$ know: Self-supervised learning for linking knowledge graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network alignment with holistic embeddings. <em>TKDE</em>,
<em>35</em>(2), 1881–1894. (<a
href="https://doi.org/10.1109/TKDE.2021.3101840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network alignment is the task of identifying topologically and semantically similar nodes across (two) different networks. It plays an important role in various applications ranging from social network analysis to bioinformatic network interactions. However, existing alignment models either cannot handle large-scale graphs or fail to leverage different types of network information or modalities. In this paper, we propose a novel end-to-end alignment framework that can leverage different modalities to compare and align network nodes in an efficient way. In order to exploit the richness of the network context, our model constructs multiple embeddings for each node, each of which captures one modality or type of network information. We then design a late-fusion mechanism to combine the learned embeddings based on the importance of the underlying information. Our fusion mechanism allows our model to be adapted to various types of structure of the input network. Experimental results show that our technique outperforms state-of-the-art approaches in terms of accuracy on real and synthetic datasets, while being robust against various noise factors.},
  archive      = {J_TKDE},
  author       = {Thanh Trung Huynh and Chi Thang Duong and Thanh Tam Nguyen and Vinh Tong Van and Abdul Sattar and Hongzhi Yin and Quoc Viet Hung Nguyen},
  doi          = {10.1109/TKDE.2021.3101840},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1881-1894},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Network alignment with holistic embeddings},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view attributed graph clustering. <em>TKDE</em>,
<em>35</em>(2), 1872–1880. (<a
href="https://doi.org/10.1109/TKDE.2021.3101227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view graph clustering has been intensively investigated during the past years. However, existing methods are still limited in two main aspects. On the one hand, most of them can not deal with data that have both attributes and graphs. Nowadays, multi-view attributed graph data are ubiquitous and the need for effective clustering methods is growing. On the other hand, many state-of-the-art algorithms are either shallow or deep models. Shallow methods may seriously restrict their capacity for modeling complex data, while deep approaches often involve large number of parameters and are expensive to train in terms of running time and space needed. In this paper, we propose a novel multi-view attributed graph clustering (MAGC) framework, which exploits both node attributes and graphs. Our novelty lies in three aspects. First, instead of deep neural networks, we apply a graph filtering technique to achieve a smooth node representation. Second, the original graph could be noisy or incomplete and is not directly applicable, thus we learn a consensus graph from data by considering the heterogeneous views. Third, high-order relations are explored in a flexible way by designing a new regularizer. Extensive experiments demonstrate the superiority of our method in terms of effectiveness and efficiency.},
  archive      = {J_TKDE},
  author       = {Zhiping Lin and Zhao Kang and Lizong Zhang and Ling Tian},
  doi          = {10.1109/TKDE.2021.3101227},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1872-1880},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view attributed graph clustering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple resource network voronoi diagram. <em>TKDE</em>,
<em>35</em>(2), 1857–1871. (<a
href="https://doi.org/10.1109/TKDE.2021.3088147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a spatial network and a set of service centers from k different resource types, a Multiple Resource Network Voronoi Diagram (MRNVD) partitions the spatial network into a set of Service Areas that can minimize the total cycle-distances of graph-nodes to allotted k service centers with different resource types. The MRNVD problem is important for critical societal applications such as assigning essential survival supplies (e.g., food, water, gas, and medical assistance) to residents impacted by man-made or natural disasters. The MRNVD problem is NP-hard; it is computationally challenging due to the large size of the transportation network. Previous work proposed the Distance bounded Pruning (DP) approach to produce an optimal solution for MRNVD. However, we found that DP can be generalized to reduce the computational cost for the minimum cycle-distance. In this paper, we extend our prior work and propose a novel approach that reduces the computational cost. Experiments using real-world datasets from five different regions demonstrate that the proposed approach creates MRNVD and significantly reduces the computational cost.},
  archive      = {J_TKDE},
  author       = {Ahmad Qutbuddin and KwangSoo Yang},
  doi          = {10.1109/TKDE.2021.3088147},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1857-1871},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multiple resource network voronoi diagram},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-dimensional classification via decomposed label
encoding. <em>TKDE</em>, <em>35</em>(2), 1844–1856. (<a
href="https://doi.org/10.1109/TKDE.2021.3100436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-dimensional classification (MDC), a number of class variables are assumed in the output space with each of them specifying the class membership w.r.t. one heterogeneous class space. One major challenge in learning from MDC examples lies in the heterogeneity of class spaces, where the modeling outputs from different class spaces are not directly comparable. To tackle this problem, we propose a new strategy named decomposed label encoding, which enables modeling alignment for MDC in an encoded label space derived from one-versus-one (OvO) decomposition. Specifically, the original MDC output space is transformed into a ternary encoded label space by conducting OvO decomposition w.r.t. each class space. Then, the manifold structure in the feature space is exploited to enrich the labeling information in the encoded label space. Finally, the predictive model is induced by fitting the metric-aligned modeling outputs with enriched labeling information. Extensive experiments over twenty benchmark data sets clearly show the superiority of the proposed MDC strategy against state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Bin-Bin Jia and Min-Ling Zhang},
  doi          = {10.1109/TKDE.2021.3100436},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1844-1856},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-dimensional classification via decomposed label encoding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling health stage development of patients with dynamic
attributed graphs in online health communities. <em>TKDE</em>,
<em>35</em>(2), 1831–1843. (<a
href="https://doi.org/10.1109/TKDE.2022.3144083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel DynAttGraph2Seq framework to model complex dynamic transitions of an individual user&#39;s activities and the textual information of the posts over time in online health forums and learning how these correspond to his/her health stage. To achieve this, we first formulate the transition of user activities as a dynamic attributed graph with multi-attributed nodes that evolves over time, then formalize the health stage inference task as a dynamic attributed graph to sequence learning problem. Our proposed model consists of a novel dynamic graph encoder along with a two-level sequential encoder to capture the semantic features from user posts and an interpretable sequence decoder that learn the mapping between a sequence of time-evolving user activity graphs as well as user posts to a sequence of target health stages. We go on to propose new dynamic graph regularization and dynamic graph hierarchical attention mechanisms to facilitate the necessary multi-level interpretability. A comprehensive experimental analysis of its use for a health stage prediction task demonstrates both the effectiveness and the interpretability of the proposed models.},
  archive      = {J_TKDE},
  author       = {Yuyang Gao and Tanmoy Chowdhury and Lingfei Wu and Liang Zhao},
  doi          = {10.1109/TKDE.2022.3144083},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1831-1843},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling health stage development of patients with dynamic attributed graphs in online health communities},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Modeling co-evolution of attributed and structural
information in graph sequence. <em>TKDE</em>, <em>35</em>(2), 1817–1830.
(<a href="https://doi.org/10.1109/TKDE.2021.3094332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most graph neural network models learn embeddings of nodes in static attributed graphs for predictive analysis. Recent attempts have been made to learn temporal proximity of the nodes. We find that real dynamic attributed graphs exhibit complex phenomenon of co-evolution between node attributes and graph structure. Learning node embeddings for forecasting change of node attributes and evolution of graph structure over time remains an open problem. In this work, we present a novel framework called CoEvoGNN for modeling dynamic attributed graph sequence. It preserves the impact of earlier graphs on the current graph by embedding generation through the sequence of attributed graphs. It has a temporal self-attention architecture to model long-range dependencies in the evolution. Moreover, CoEvoGNN optimizes model parameters jointly on two dynamic tasks, attribute inference and link prediction over time. So the model can capture the co-evolutionary patterns of attribute change and link formation. This framework can adapt to any graph neural algorithms so we implemented and investigated three methods based on it: CoEvoGCN, CoEvoGAT, and CoEvoSAGE. Experiments demonstrate the framework (and its methods) outperforms strong baseline methods on predicting an entire unseen graph snapshot of personal attributes and interpersonal links in dynamic social graphs and financial graphs.},
  archive      = {J_TKDE},
  author       = {Daheng Wang and Zhihan Zhang and Yihong Ma and Tong Zhao and Tianwen Jiang and Nitesh V. Chawla and Meng Jiang},
  doi          = {10.1109/TKDE.2021.3094332},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1817-1830},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling co-evolution of attributed and structural information in graph sequence},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Merging web tables for relation extraction with knowledge
graphs. <em>TKDE</em>, <em>35</em>(2), 1803–1816. (<a
href="https://doi.org/10.1109/TKDE.2021.3101479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose methods for extracting triples from Wikipedia’s HTML tables using a reference knowledge graph. Our methods use a distant-supervision approach to find existing triples in the knowledge graph for pairs of entities on the same row of a table, postulating the corresponding relation for pairs of entities from other rows in the corresponding columns, thus extracting novel candidate triples. Binary classifiers are applied on these candidates to detect correct triples and thus increase the precision of the output triples. We extend this approach with a preliminary step where we first group and merge similar tables, thereafter applying extraction on the larger merged tables. More specifically, we propose an observed schema for individual tables, which is used to group and merge tables. We compare the precision and number of triples extracted with and without table merging, where we show that with merging, we can extract a larger number of triples at a similar precision. Ultimately, from the tables of English Wikipedia, we extract 5.9 million novel and unique triples for Wikidata at an estimated precision of 0.718.},
  archive      = {J_TKDE},
  author       = {Jhomara Luzuriaga and Emir Muñoz and Henry Rosales-Méndez and Aidan Hogan},
  doi          = {10.1109/TKDE.2021.3101479},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1803-1816},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Merging web tables for relation extraction with knowledge graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximum signed <span
class="math inline"><em>θ</em></span>θ-clique identification in large
signed graphs. <em>TKDE</em>, <em>35</em>(2), 1791–1802. (<a
href="https://doi.org/10.1109/TKDE.2021.3098423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum clique problem, which is to find the clique with the largest size, can find many real-world applications and is notable for its capability of modeling many combinatorial problems. However, most existing research focuses on processing unsigned graphs, i.e., treat each connection equally. In real applications, edges of graphs are usually associated with signed information, i.e., positive or negative edges, and signed graph analysis has attracted great attentions in the recent. In this paper, we first analyze the disadvantages of existing signed clique models, and then propose a novel clique model, named signed $\theta$ -clique. Given a signed graph $G$ and a subgraph $S$ , let $d^{+}_{S}(u)$ and $d^{-}_{S}(u)$ be the number of positive and negative neighbors of vertex $u$ in $S$ . We say a subgraph $S$ is a signed $\theta$ -clique if $i)$ $S$ is a clique and $ii)$ each vertex $u$ in $S$ fulfills $d^{+}_{S}(u) - d^{-}_{S}(u) \geq \theta$ . We show that the problem of identifying the maximum signed $\theta$ -clique is NP-hard. Novel pruning techniques are proposed to reduce the searching space. In addition, efficient searching strategies are developed to scale for large graphs. Comprehensive experiments on 8 real-world datasets are conducted to demonstrate the effectiveness and efficiency of the proposed approaches.},
  archive      = {J_TKDE},
  author       = {Chen Chen and Yanping Wu and Renjie Sun and Xiaoyang Wang},
  doi          = {10.1109/TKDE.2021.3098423},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1791-1802},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Maximum signed $\theta$θ-clique identification in large signed graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LuxGeo: Efficient and security-enhanced geometric range
queries. <em>TKDE</em>, <em>35</em>(2), 1775–1790. (<a
href="https://doi.org/10.1109/TKDE.2021.3093909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As location-based applications flourishing, we will witness soon the transferring of a prodigious amount of data from the local to a public cloud. The rising demand for outsourced data is moving toward a wider geographical area with arbitrary distribution (i.e., dense or sparse) and query scope (i.e., limited or vast). The outsourced individual data should be preserved when being queried as facing cloud risks, especially for location information. Geometric range queries are one of the most fundamental search functions. However, the existed works of secure geometric queries are far from practical usage on efficiency and security simultaneously. In this paper, we propose a novel scheme, LuxGeo. Our scheme reaches a constant navigation and a linear sweep, which is tailored for secure and efficient location-lookup. Our experiments over three real-world spatial datasets have shown its practical efficiency. For example, it only takes $\mathbf {10.01s}$ with $\mathbf {728}$ tuples retrieved over $\mathbf {63,369}$ ciphertext dataset for a single query. LuxGeo has better performance than the existed solutions for a GSE problem on efficiency and security.},
  archive      = {J_TKDE},
  author       = {Ruoyang Guo and Bo Qin and Yuncheng Wu and Ruixuan Liu and Hong Chen and Cuiping Li},
  doi          = {10.1109/TKDE.2021.3093909},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1775-1790},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LuxGeo: Efficient and security-enhanced geometric range queries},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning relation prototype from unlabeled texts for
long-tail relation extraction. <em>TKDE</em>, <em>35</em>(2), 1761–1774.
(<a href="https://doi.org/10.1109/TKDE.2021.3096200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation Extraction (RE) is a vital step to complete Knowledge Graph (KG) by extracting entity relations from texts. However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient training data. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as their proximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order and second-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or even unseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information. We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision. Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements (4.1 percent F1 on average). Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes and data can be found in https://github.com/CrisJk/PA-TRP .},
  archive      = {J_TKDE},
  author       = {Yixin Cao and Jun Kuang and Ming Gao and Aoying Zhou and Yonggang Wen and Tat-Seng Chua},
  doi          = {10.1109/TKDE.2021.3096200},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1761-1774},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning relation prototype from unlabeled texts for long-tail relation extraction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning graph convolutional networks based on quantum
vertex information propagation. <em>TKDE</em>, <em>35</em>(2),
1747–1760. (<a href="https://doi.org/10.1109/TKDE.2021.3106804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new Quantum Spatial Graph Convolutional Neural Network (QSGCNN) model that can directly learn a classification function for graphs of arbitrary sizes. Unlike state-of-the-art Graph Convolutional Neural Network (GCNN) models, the proposed QSGCNN model incorporates the process of identifying transitive aligned vertices between graphs and transforms arbitrary sized graphs into fixed-sized aligned vertex grid structures. In order to learn representative graph characteristics, a new quantum spatial graph convolution is proposed and employed to extract multi-scale vertex features, in terms of quantum information propagation between grid vertices of each graph. Since the quantum spatial convolution preserves the grid structures of the input vertices (i.e., the convolution layer does not alter the original spatial position of vertices), the proposed QSGCNN model allows to directly employ the traditional convolutional neural network architecture to further learn from the global graph topology, providing an end-to-end deep learning architecture that integrates the graph representation and learning in the quantum spatial graph convolution layer and the traditional convolutional layer for graph classifications. We indicate the effectiveness of the proposed QSGCNN model in relation to existing state-of-the-art methods. Experiments on benchmark graph classification datasets demonstrate the effectiveness of the proposed QSGCNN model.},
  archive      = {J_TKDE},
  author       = {Lu Bai and Yuhang Jiao and Lixin Cui and Luca Rossi and Yue Wang and Philip S. Yu and Edwin R. Hancock},
  doi          = {10.1109/TKDE.2021.3106804},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1747-1760},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning graph convolutional networks based on quantum vertex information propagation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LASH: Large-scale academic deep semantic hashing.
<em>TKDE</em>, <em>35</em>(2), 1734–1746. (<a
href="https://doi.org/10.1109/TKDE.2021.3109433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosively increasing of academic papers, efficient academic document retrieval is becoming an essential requirement for large-scale information retrieval systems. Inspired by the success of deep semantic hashing in normal document retrieval, deep semantic hashing is a promising approach for academic document retrieval by mapping academic documents into efficient hash codes. However, for academic document retrieval, the existing deep semantic hashing methods suffer from following two problems: (1) they cannot differentiate the importance of different field labels; (2) they cannot plenty utilize the structure information in paper citations. To address these problems, we propose a novel Large-scale Academic deep Semantic Hashing, called LASH. Specifically, LASH first treats paper citations as a citation network, and then employs a multi-input deep autoencoder to directly encode both structure information of the citation network and semantic information of academic documents into unified hash codes. Moreover, a weighted percentage similarity is designed to measure the importance of different field labels, which is a linear combination of Jaccard and Cosine similarity. Supervised by the similarity, the learned unified hash codes can further preserve the importance of different field labels. Extensive experiments show LASH significantly outperforms state-of-the-art baselines over proposed three real-world large-scale academic document datasets.},
  archive      = {J_TKDE},
  author       = {Jia-Nan Guo and Xian-Ling Mao and Tian Lan and Rong-Xin Tu and Wei Wei and Heyan Huang},
  doi          = {10.1109/TKDE.2021.3109433},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1734-1746},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LASH: Large-scale academic deep semantic hashing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lasagne: A multi-layer graph convolutional network framework
via node-aware deep architecture. <em>TKDE</em>, <em>35</em>(2),
1721–1733. (<a href="https://doi.org/10.1109/TKDE.2021.3103984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have been successfully applied in many different real-world tasks. However, most of the existing methods are based on shallow GCN, because multiple layers involve long-distance neighborhood information but lead to the over-smoothing problem. Actually, a similar challenge exists in the depth limitation for primitive convolutional neural networks (CNNs). As the multi-layer architecture can increase the representation ability of GCN, we study and learn from the recent progress in CNN and propose Lasagne, a novel multi-layer GCN framework, empowered by node-aware layer aggregators and factorization-based layer interactions to overcome the over-smoothing problem and realize the full potentials of the GCN model. We analyze how the node locality affects the information propagation in GCN and propose a novel node aggregation mechanism in an adaptive manner. We further demystify Lasagne from a mutual information view and evaluate it on both real-world benchmark data sets and large-scale industrial production data sets. Lasagne shows strong empirical performance on the semi-supervised node classification task and outperforms the state-of-the-art methods without considering the node locality.},
  archive      = {J_TKDE},
  author       = {Xupeng Miao and Wentao Zhang and Yingxia Shao and Bin Cui and Lei Chen and Ce Zhang and Jiawei Jiang},
  doi          = {10.1109/TKDE.2021.3103984},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1721-1733},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Lasagne: A multi-layer graph convolutional network framework via node-aware deep architecture},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label enhancement by maintaining positive and negative label
relation. <em>TKDE</em>, <em>35</em>(2), 1708–1720. (<a
href="https://doi.org/10.1109/TKDE.2021.3093099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label distribution learning (LDL) is a novel machine learning paradigm that gives the description degree of each label to a particular instance. However, many existing datasets contain only simple logical labels since it is difficult and time-consuming to directly obtain the label distributions. Therefore, label enhancement (LE) is proposed to convert multi-label datasets consisting of logical labels into label distribution datasets. Recently, many LE algorithms have been proposed and most of them concentrate on the fitting degree, but ignore the ordering relation between positive and negative labels. Therefore, in this paper, we propose an LE algorithm based on maintaining positive and negative label relation, which contains a novel ranking loss that can generate different penalties according to different ranking errors. Our algorithm achieves a good balance between the degree of fitting and the ordering relation. The experimental results on several real-world datasets validate the effectiveness of our method.},
  archive      = {J_TKDE},
  author       = {Xiuyi Jia and Yunan Lu and Fangwen Zhang},
  doi          = {10.1109/TKDE.2021.3093099},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1708-1720},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Label enhancement by maintaining positive and negative label relation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label distribution learning by maintaining label ranking
relation. <em>TKDE</em>, <em>35</em>(2), 1695–1707. (<a
href="https://doi.org/10.1109/TKDE.2021.3099294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label distribution learning (LDL) is a novel machine learning paradigm that can be seen as an extension of multi-label learning (MLL). Compared with MLL, the advantages of LDL are reflected in the following perspectives: (1) the label distribution gives the relevance description of each label to unknown instances in quantitative terms; (2) the distribution implicitly gives the relevance intensities relation of different labels to a particular instance in qualitative terms, i.e., the label ranking relation. All existing LDL models aim to fit the ground-truth label distribution by quantitatively minimizing the distance between distributions or maximizing the similarity between distributions, which only uses the first advantage of the label distribution but ignores the label ranking relation, which may lose some useful semantic information implied in the label distribution, thus reducing the performance of LDL. Therefore, we propose a novel algorithm to solve this problem by introducing the ranking loss function to LDL. In addition, in order to evaluate the LDL algorithms more comprehensively and verify that the ranking loss is beneficial for keeping the label ranking relation, we also introduce two popular ranking evaluation metrics for LDL. The experimental results on 13 real-world datasets validate the effectiveness of our method.},
  archive      = {J_TKDE},
  author       = {Xiuyi Jia and Xiaoxia Shen and Weiwei Li and Yunan Lu and Jihua Zhu},
  doi          = {10.1109/TKDE.2021.3099294},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1695-1707},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Label distribution learning by maintaining label ranking relation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernelized multitask learning method for personalized
signaling adverse drug reactions. <em>TKDE</em>, <em>35</em>(2),
1681–1694. (<a href="https://doi.org/10.1109/TKDE.2021.3108819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The signaling of the associations between drugs and adverse drug reactions (ADRs) is a challenging task in pharmacovigilance, especially when an association is infrequent or has never previously been reported. Most existing methods for ADR signaling are based on analyzing the frequency with which drugs tend to co-occur with ADRs. In this article, we propose a kernelized multitask learning model, $\mathtt {\mathtt {KEMULA}}$ , in which information is learned and transferred from the clinical data of other patients as collaborative information to rank distinct lists of ADRs for different patients. We comprehensively compare the performance of $\mathtt {\mathtt {KEMULA}}$ against three baseline methods, two state-of-the-art ADR signaling methods, and two $\mathtt {\mathtt {KEMULA}}$ variants. The method is tested on adverse drug event reports retrieved from the FDA Adverse Event Reporting System (FAERS), which includes 4,106,633 unique adverse drug event reports, 7,824 unique ADRs, 114 unique biotech drugs, 1,151 unique small molecule drugs, and 3,363 unique medical conditions. The experimental results demonstrate the advantages of our method and show that it not only can signal frequent ADRs but also has the power to signal infrequent ADRs that cannot be signaled by most existing methods.},
  archive      = {J_TKDE},
  author       = {Fan Yang and Fuzhong Xue and Yanchun Zhang and George Karypis},
  doi          = {10.1109/TKDE.2021.3108819},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1681-1694},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Kernelized multitask learning method for personalized signaling adverse drug reactions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint gated co-attention based multi-modal networks for
subregion house price prediction. <em>TKDE</em>, <em>35</em>(2),
1667–1680. (<a href="https://doi.org/10.1109/TKDE.2021.3093881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban housing price is widely accepted as an economic indicator which is of both business and research interest in urban computing. However, due to the complex nature of influencing factors and the sparse property of transaction records, to implement such a model is still challenging. To address these challenges, in this work, we study an effective and fine-grained model for urban subregion housing price predictions. Compared to existing works, our proposal improves the forecasting granularity from city-level to mile-level, with only publicly released transaction data. We employ a feature selection mechanism to select more relevant features. Then, we propose an integrated model, JGC_MMN (Joint Gated Co-attention Based Multi-modal Network), to learn all-level features and capture spatiotemporal correlations in all-time stages with a modified densely connected convolutional network as well as current ingredients and future expectations. Next, we devise a novel JGC based fusion method to better fuse the heterogeneous data of multi-stage models by considering their interactions in temporal dimension. Finally, extensive empirical studies on real datasets demonstrate the effectiveness of our proposal, and this fine-grained housing price forecasting has the potential to support a broad scope of applications, ranging from urban planning to housing market recommendations.},
  archive      = {J_TKDE},
  author       = {Pengkun Wang and Chuancai Ge and Zhengyang Zhou and Xu Wang and Yuantao Li and Yang Wang},
  doi          = {10.1109/TKDE.2021.3093881},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1667-1680},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Joint gated co-attention based multi-modal networks for subregion house price prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). It runs in the family: Unsupervised algorithm for
alternative name suggestion using digitized family trees. <em>TKDE</em>,
<em>35</em>(2), 1651–1666. (<a
href="https://doi.org/10.1109/TKDE.2021.3096670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching for a person’s name is a common online activity. However, Web search engines provide few accurate results to queries containing names. In contrast to a general word that has only one correct spelling, there are several possible legitimate spellings when a name provided as a query. Today, most techniques used to suggest diminutives and alternative spellings in online search are based on pattern matching and phonetic encoding; however, they often perform poorly. As a result, there is a need for an effective tool for improved alternative name suggestion for a name provided as a query. In this paper, we propose a revolutionary approach for tackling the problem of alternative name suggestion. Our novel algorithm, GRAFT , utilizes historical data collected from genealogy websites, along with network algorithms. GRAFT is a general algorithm that suggests alternatives for input names using a graph based on names derived from digitized ancestral family trees. Alternative names are extracted from this graph, which is constructed using generic ordering functions that outperform other algorithms that suggest diminutives and alternative spellings based on a single dimension, a factor that limits their performance. We evaluated GRAFT ’s performance on three ground truth datasets of forenames and surnames, including a large-scale online genealogy dataset with over 16 million profiles and more than 700,000 unique forenames and 500,000 surnames. We compared GRAFT ’s performance at suggesting alternative names to the performance of 10 other algorithms, including phonetic encoding, string similarity, machine learning, and deep learning algorithms. The results show GRAFT ’s superiority with regard to both forenames and surnames and demonstrate its use as a tool to improve alternative name suggestion.},
  archive      = {J_TKDE},
  author       = {Aviad Elyashar and Rami Puzis and Michael Fire},
  doi          = {10.1109/TKDE.2021.3096670},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1651-1666},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {It runs in the family: Unsupervised algorithm for alternative name suggestion using digitized family trees},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable and efficient heterogeneous graph
convolutional network. <em>TKDE</em>, <em>35</em>(2), 1637–1650. (<a
href="https://doi.org/10.1109/TKDE.2021.3101356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Network (GCN) has achieved extraordinary success in learning representations of nodes in graphs. However, regarding Heterogeneous Information Network (HIN), existing HIN-oriented GCN methods still suffer from two deficiencies: (1) they cannot flexibly explore all possible meta-paths and extract the most useful ones for each target object, which hinders both effectiveness and interpretability; (2) before performing aggregation, they often require some additional time-consuming pre-processing operations, which increase the computational complexity. To address the above issues, we propose an interpretable and efficient Heterogeneous Graph Convolutional Network (ie-HGCN) to learn the representations of objects in HINs. It is designed as a hierarchical aggregation architecture, i.e., object-level aggregation and type-level aggregation. The new architecture can automatically evaluate all possible meta-paths within a length limit, and discover and exploit the most useful ones for each target object, i.e., at fine granularity. It also reduces the computational cost by avoiding additional time-consuming pre-processing operations. Theoretical analysis shows its ability to evaluate the usefulness of all possible meta-paths, its connection to the spectral graph convolution on HINs, and its quasi-linear time complexity. Extensive experiments on four real network datasets demonstrate its interpretability, efficiency as well as its superiority against thirteen baselines.},
  archive      = {J_TKDE},
  author       = {Yaming Yang and Ziyu Guan and Jianxin Li and Wei Zhao and Jiangtao Cui and Quan Wang},
  doi          = {10.1109/TKDE.2021.3101356},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1637-1650},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Interpretable and efficient heterogeneous graph convolutional network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive reinforcement learning for feature selection
with decision tree in the loop. <em>TKDE</em>, <em>35</em>(2),
1624–1636. (<a href="https://doi.org/10.1109/TKDE.2021.3102120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of balancing effectiveness and efficiency in automated feature selection. Feature selection is to find an optimal feature subset from large feature space. After exploring many feature selection methods, we observe a computational dilemma: 1) traditional feature selection (e.g., mRMR) is mostly efficient, but difficult to identify the best subset; 2) the emerging reinforced feature selection automatically navigates feature space to search the best subset, but is usually inefficient. Are automation and efficiency always apart from each other? Can we bridge the gap between effectiveness and efficiency under automation? Motivated by this dilemma, we aim to develop a novel feature space navigation method. In our preliminary work, we leveraged interactive reinforcement learning to accelerate feature selection by external trainer-agent interaction. Our preliminary work can be significantly improved by modeling the structured knowledge of its downstream task (e.g., decision tree) as learning feedback. In this journal version, we propose a novel interactive and closed-loop architecture to simultaneously model interactive reinforcement learning (IRL) and decision tree feedback (DTF). Specifically, IRL is to create an interactive feature selection loop and DTF is to feed structured feature knowledge back to the loop. The DTF improves IRL from two aspects. First, the tree-structured feature hierarchy generated by decision tree is leveraged to improve state representation. In particular, we represent the selected feature subset as an undirected graph of feature-feature correlations and a directed tree of decision features. We propose a new embedding method capable of empowering Graph Convolutional Network (GCN) to jointly learn state representation from both the graph and the tree. Second, the tree-structured feature hierarchy is exploited to develop a new reward scheme. In particular, we personalize reward assignment of agents based on decision tree feature importance. In addition, observing agents’ actions can also be a feedback, we devise another new reward scheme, to weigh and assign reward based on the selected frequency ratio of each agent in historical action records. Finally, we present extensive experiments with real-world datasets to demonstrate the improved performances of our method.},
  archive      = {J_TKDE},
  author       = {Wei Fan and Kunpeng Liu and Hao Liu and Yong Ge and Hui Xiong and Yanjie Fu},
  doi          = {10.1109/TKDE.2021.3102120},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1624-1636},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Interactive reinforcement learning for feature selection with decision tree in the loop},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporating price into recommendation with graph
convolutional networks. <em>TKDE</em>, <em>35</em>(2), 1609–1623. (<a
href="https://doi.org/10.1109/TKDE.2021.3091160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, much research effort on recommendation has been devoted to mining user behaviors, i.e., collaborative filtering, along with the general information which describes users or items, e.g., textual attributes, categorical demographics, product images, and so on. Price, an important factor in marketing — which determines whether a user will make the final purchase decision on an item — surprisingly, has received relatively little scrutiny. In this work, we aim at developing an effective method to predict user purchase intention with the focus on the price factor in recommender systems. The main difficulties are two-fold: 1) the preference and sensitivity of a user on item price are unknown, which are only implicitly reflected in the items that the user has purchased, and 2) how the item price affects a user’s intention depends largely on the product category, that is, the perception and affordability of a user on item price could vary significantly across categories. Towards the first difficulty, we propose to model the transitive relationship between user-to-item and item-to-price, taking the inspiration from the recently developed Graph Convolution Networks (GCN). The key idea is to propagate the influence of price on users with items as the bridge, so as to make the learned user representations be price-aware. For the second difficulty, we further integrate item categories into the propagation progress and model the possible pairwise interactions for predicting user-item interactions. We conduct extensive experiments on two real-world datasets, demonstrating the effectiveness of our GCN-based method in learning the price-aware preference of users. Further analysis reveals that modeling the price awareness is particularly useful for predicting user preference on items of unexplored categories.},
  archive      = {J_TKDE},
  author       = {Yu Zheng and Chen Gao and Xiangnan He and Depeng Jin and Yong Li},
  doi          = {10.1109/TKDE.2021.3091160},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1609-1623},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incorporating price into recommendation with graph convolutional networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Higher-order interaction goes neural: A substructure
assembling graph attention network for graph classification.
<em>TKDE</em>, <em>35</em>(2), 1594–1608. (<a
href="https://doi.org/10.1109/TKDE.2021.3105544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph classification has been widely used for knowledge discovery in numerous practical application scenarios, such as social networks and protein-protein interaction networks. Recently, Graph Neural Networks (GNNs), which generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in graph classification. However, existing GNN models mainly focus on capturing the information of immediate or first-order neighboring nodes within a single layer. The graph substructure and substructure interaction, which plays an important role in learning graph representations, are usually overlooked. In this paper, we propose a Substructure Assembling Graph Attention Network (SA-GAT) to extract graph features and improve the performance of graph classification. SA-GAT is able to fully explore higher-order substructure information hidden in graphs by a core module called Substructure Interaction Attention (SIA), which takes both the information of neighbors’ substructures and the interaction information among them into account during aggregation process. Theoretically, we have also proved that SA-GAT satisfies the graph isomorphism theory of graph neural network design, which is that the network should map isomorphic graphs to the same representation and output the same prediction. Extensive experimental results on multiple real-world graph classification datasets demonstrate that the proposed SA-GAT outperforms the state-of-the-art methods including graph kernels and graph neural networks.},
  archive      = {J_TKDE},
  author       = {Jianliang Gao and Jun Gao and Xiaoting Ying and Mingming Lu and Jianxin Wang},
  doi          = {10.1109/TKDE.2021.3105544},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1594-1608},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Higher-order interaction goes neural: A substructure assembling graph attention network for graph classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous information network embedding with adversarial
disentangler. <em>TKDE</em>, <em>35</em>(2), 1581–1593. (<a
href="https://doi.org/10.1109/TKDE.2021.3096231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous information network (HIN) embedding has gained considerable attention in recent years, which learns low-dimensional representation of nodes while preserving the semantic and structural correlations in HINs. Many of existing methods which exploit meta-path guided strategy have shown promising results. However, the learned node representations could be highly entangled for downstream tasks; for example, an author’s publications in multidisciplinary venues may make the prediction of his/her research interests difficult. To address this issue, we develop a novel framework named HEAD (i.e., HIN Embedding with Adversarial Disentangler) to separate the distinct, informative factors of variations in node semantics formulated by meta-paths. More specifically, in HEAD, we first propose the meta-path disentangler to separate node embeddings from various meta-paths into intrinsic and specific spaces; then with meta-path schemes as self-supervised information, we design two adversarial learners (i.e., meta-path and semantic discriminators) to make the intrinsic embedding more independent from the designed meta-paths while the specific embedding more meta-path dependent. To comprehensively evaluate the performance of HEAD, we perform a set of experiments on four real-world datasets. Compared to the state-of-the-art baselines, the maximum 15 percent improvement of performance demonstrates the effectiveness of HEAD and the benefits of the learned disentangled representations.},
  archive      = {J_TKDE},
  author       = {Ruijia Wang and Chuan Shi and Tianyu Zhao and Xiao Wang and Yanfang Ye},
  doi          = {10.1109/TKDE.2021.3096231},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1581-1593},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Heterogeneous information network embedding with adversarial disentangler},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-convolved factorization machines for personalized
recommendation. <em>TKDE</em>, <em>35</em>(2), 1567–1580. (<a
href="https://doi.org/10.1109/TKDE.2021.3100564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factorization machines (FMs) and their neural network variants (neural FMs) for modeling second-order feature interactions are effective in building modern recommendation systems. However, feature interactions are based upon pairs of features, whereas multi-features correlations commonly arise in real-world financial product recommendation scenarios. We propose an effective neural recommender system, graph-convolved factorization machine (GCFM), with the spirit of the symbolic graph reasoning principle that provides lightweight and interpretable recommendation suggestions. Given a sample for the recommendation, GCFM constructs the corresponding dense feature embeddings and computes the sample-specific feature relationship graph. Then, a multi-filter graph-convolved feature crossing (GCFC) layer for feature embeddings establishes cross features with their neighboring embeddings. GCFM thus extends the feature interactions from pairs to neighbors to capture more comprehensive and explainable information while simultaneously reaping the advantages of representation learning. To exploit these capabilities, we apply a Graph Bayesian Optimization (GBO). During training, our GBO automatically optimizes our GCFM, including training hyperparameters and architecture hyperparameters. Besides, we conduct extensive experiments on two public financial applications benchmarks, USCFC and OTC, and two real-world datasets that we collect offline. Our GCFM significantly outperforms state-of-the-art algorithms and shows its interpretability in recommendation tasks. We further extend our model to online real-world applications, showing an appealing human-level decision intelligence in real scenarios.},
  archive      = {J_TKDE},
  author       = {Yongsen Zheng and Pengxu Wei and Ziliang Chen and Yang Cao and Liang Lin},
  doi          = {10.1109/TKDE.2021.3100564},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1567-1580},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph-convolved factorization machines for personalized recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph algorithms with partition transparency. <em>TKDE</em>,
<em>35</em>(2), 1554–1566. (<a
href="https://doi.org/10.1109/TKDE.2021.3097998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph computations often have to be conducted in parallel on partitioned graphs. The choice of graph partitioning strategies, however, has strong impact on the design of graph computation algorithms. A graph algorithm developed under edge-cut partitions may not work correctly under vertex-cut, and vice versa. We often have to rewrite our algorithms when we switch from, e.g., edge-cut to vertex-cut. To cope with this, we propose a notion of partition transparency , such that graph algorithms are able to work correctly under different partitions without changes and moreover, benefit from recent hybrid partitions to speed up computations. Furthermore, we identify conditions under which graph algorithms are guaranteed to be partition-transparent, in graph-centric and vertex-centric models. We show that a variety of graph algorithms can be made partition-transparent. Using real-life and synthetic graphs, we experimentally verify that partition-transparent algorithms compute correct answers under different partitions; better still, under hybrid partitions these algorithms perform better than algorithms tailored for edge-cut and vertex-cut partitions in efficiency.},
  archive      = {J_TKDE},
  author       = {Wenfei Fan and Muyang Liu and Ping Lu and Qiang Yin},
  doi          = {10.1109/TKDE.2021.3097998},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1554-1566},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph algorithms with partition transparency},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frequency estimation in data streams: Learning the optimal
hashing scheme. <em>TKDE</em>, <em>35</em>(2), 1541–1553. (<a
href="https://doi.org/10.1109/TKDE.2021.3103819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for the problem of frequency estimation in data streams that is based on optimization and machine learning. Contrary to state-of-the-art streaming frequency estimation algorithms, which heavily rely on random hashing to maintain the frequency distribution of the data steam using limited storage, the proposed approach exploits an observed stream prefix to near-optimally hash elements and compress the target frequency distribution. We develop an exact mixed-integer linear optimization formulation, which enables us to compute optimal or near-optimal hashing schemes for elements seen in the observed stream prefix; then, we use machine learning to hash unseen elements. Further, we develop an efficient block coordinate descent algorithm, which, as we empirically show, produces high quality solutions, and, in a special case, we are able to solve the proposed formulation exactly in linear time using dynamic programming. We empirically evaluate the proposed approach both on synthetic datasets and on real-world search query data. We show that the proposed approach outperforms existing approaches by one to two orders of magnitude in terms of its average (per element) estimation error and by 45-90 percent in terms of its expected magnitude of estimation error.},
  archive      = {J_TKDE},
  author       = {Dimitris Bertsimas and Vassilis Digalakis},
  doi          = {10.1109/TKDE.2021.3103819},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1541-1553},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Frequency estimation in data streams: Learning the optimal hashing scheme},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Filling delivery time automatically based on couriers’
trajectories. <em>TKDE</em>, <em>35</em>(2), 1528–1540. (<a
href="https://doi.org/10.1109/TKDE.2021.3100116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, couriers are still the main solution to address the “last mile” problem in logistics. They are usually required to record the delivery time of each parcel manually, which is essential for delivery insurances, delivery performance evaluations, and customer available time discovery. Stay points extracted from couriers’ trajectories provide a chance to fill the delivery time automatically to ease their burdens. However, it is challenging due to inaccurate delivery locations and various stay scenarios. To this end, we propose the improved D elivery T ime Inf erence (DTInf + ), to infer the delivery time of waybills based on their trajectories. DTInf + is composed of three steps: 1) Data Pre-processing , which organizes waybills and stay points by delivery trips, 2) Delivery Location Mining , which obtains the delivery location for each address and each Geocoded waybill location by mining historical delivery caused stay points, and 3) Delivery Event-based Matching , which infers the delivery caused stay point for waybills at the same delivery location based on a pointer network-like model SPSelector to obtain the delivery time. Extensive experiments and case studies based on real-world datasets from JD Logistics confirm the effectiveness of our approach. Finally, a system is deployed in JD Logistics.},
  archive      = {J_TKDE},
  author       = {Sijie Ruan and Xi Fu and Cheng Long and Zi Xiong and Jie Bao and Ruiyuan Li and Yiheng Chen and Shengnan Wu and Yu Zheng},
  doi          = {10.1109/TKDE.2021.3100116},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1528-1540},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Filling delivery time automatically based on couriers’ trajectories},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast optimization of spectral embedding and improved
spectral rotation. <em>TKDE</em>, <em>35</em>(2), 1515–1527. (<a
href="https://doi.org/10.1109/TKDE.2021.3098806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is a vital clustering method and has been widely applied for data analysis and pattern reorganization. A routine of solving spectral clustering problem consists of two successive stages: (1) solving a relaxed continuous optimization problem to obtain a real-valued indicator solution and (2) transform the real-valued indicator into a 0-1 discrete one as the final clustering result. However, we may lose the optimal solution with such a two-stage process. Besides, the spectral clustering has a high time complexity which limits the analysis of large-scale data. To alleviate these problems, this article proposes an efficient spectral clustering framework that computes spectral embedding and improved spectral rotation simultaneously (SE-ISR). In addition, we also provide a parameter-free method (SE-ISR-PF) to automatically choose the trade-off parameter. Furthermore, with an anchor-based similarity matrix construction, it is scalable to large-scale data. An effective algorithm with a strict convergence proof is provided to solve the corresponding optimization problem. Experimental results on several benchmark datasets demonstrate that the proposed algorithm outperforms the state-of-art methods.},
  archive      = {J_TKDE},
  author       = {Zhen Wang and Xiangfeng Dai and Peican Zhu and Rong Wang and Xuelong Li and Feiping Nie},
  doi          = {10.1109/TKDE.2021.3098806},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1515-1527},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast optimization of spectral embedding and improved spectral rotation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast label enhancement for label distribution learning.
<em>TKDE</em>, <em>35</em>(2), 1502–1514. (<a
href="https://doi.org/10.1109/TKDE.2021.3092406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label Distribution Learning (LDL) has attracted increasing research attentions due to its potential to address the label ambiguity problem in machine learning and success in many real-world applications. In LDL, it is usually expensive to obtain the ground-truth label distributions of data, but it is relatively easy to obtain the logical labels of data. How to use training instances only with logical labels to learn an effective LDL model is a challenging problem. In this paper, we propose a two-step framework to address this problem. Specifically, we first design an efficient recovery model to recover the latent label distributions of training instances, named Fast Label Enhancement (FLE). Our idea is to use non-negative matrix factorization (NMF) to mine the label distribution information from the feature space. Moreover, we take the instance-class similarities into consideration to discover the importance of each label to training instances, which is useful for learning precise label distributions. Then, we train a predictive model for testing instances based on generated label distributions of training instances and an existing LDL method (e.g., SA-BFGS). Experimental results on fifteen benchmark datasets show the effectiveness of the proposed two-step framework and verify the superiority of FLE over several state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Ke Wang and Ning Xu and Miaogen Ling and Xin Geng},
  doi          = {10.1109/TKDE.2021.3092406},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1502-1514},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast label enhancement for label distribution learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting category-level multiple characteristics for POI
recommendation. <em>TKDE</em>, <em>35</em>(2), 1488–1501. (<a
href="https://doi.org/10.1109/TKDE.2021.3088148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point of interest (POI) recommendation has received significant attention in recent years, most existing studies exploit multiple auxiliary information to alleviate the problem of data sparsity, and consider the sequential features of user mobility. However, few studies consider the category-level characteristics derived from each user&#39;s historical check-in frequencies, and the characteristics between different latent factors. In this paper, we exploit category-level multiple characteristics to generate recommendation. First, we obtain the frequency characteristics by in-depth analysis of the historical check-in frequencies for different categories by each user, and propose a scheme including KL-divergence and text analysis algorithm. Then we propose a category-level sequential- and non-sequential influence-aware probabilistic generative model (CSNS), which models the characteristics (correlation and indeterminate decisiveness) between user latent behavior topics and latent sequence patterns. We design two stages to generate recommendations. In the first stage, CSNS and frequencies characteristics are exploited jointly to recommend the POI categories that users may visit. In the second stage, we depend on user profiles and poi features, and sort the candidate POI sets by combining the POI categories provided in the first stage. Comprehensive experiments on two real-world datasets demonstrate that our method outperforms the existing state-of-the-art POI recommendation models.},
  archive      = {J_TKDE},
  author       = {Zheng Dong and Xiangwu Meng and Yujie Zhang},
  doi          = {10.1109/TKDE.2021.3088148},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1488-1501},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Exploiting category-level multiple characteristics for POI recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing time series predictors with generalized extreme
value loss. <em>TKDE</em>, <em>35</em>(2), 1473–1487. (<a
href="https://doi.org/10.1109/TKDE.2021.3108831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series prediction has wide applications in many safety-critical scenarios, including meteorology and finance. According to previous studies, time series of recorded events, e.g., river level and stock price, usually contain a non-trivial proportion of extreme events (e.g., flood and financial crisis), which are featured with extremely large/small values, occur in time series data with a relatively low frequency, and may have huge societal consequences if overlooked by a predictive model (i.e., predictor). Despite its significance in time series, we however observe the conventional square loss in time series prediction would ignore the modeling of extreme events. Specifically, we prove the square loss as a learning objective of the predictor behaves equivalently as a Gaussian kernel density estimator (KDE) on the recorded events, which is light-tailed itself and unable to model the ground-truth event distribution, usually heavy-tailed due to the existence of extreme events. Considering the benefits of forecasting extreme events, we propose a unified loss form called Generalized Extreme Value Loss (GEVL), which bridges the misalignment between the tail parts of the estimation and the ground-truth via transformations on either the observed events or the estimator. Following the proposed framework, we present three heavy-tailed kernels, i.e., shifted Gaussian, Gumbel and Fréchet kernels, and derive the corresponding GEVLs which show different levels of trade-off between modeling effectiveness and computational resources, suitable for various downstream tasks. Comprehensive experiments on a diverse set of time series predictors and real-world datasets validate that, our novel loss form substantially enhances representative time series predictors in modeling extreme events. For example, for CO2 concentration rate prediction and stock price prediction, our proposed Fréchet GEVL respectively reduces the RMSE of 6 representative DNN-based time series predictors on extreme events by over $20\%$ and $17\%$ on average, with a maximum reduction of $29.8\%$ .},
  archive      = {J_TKDE},
  author       = {Mi Zhang and Daizong Ding and Xudong Pan and Min Yang},
  doi          = {10.1109/TKDE.2021.3108831},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1473-1487},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing time series predictors with generalized extreme value loss},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient top-k vulnerable nodes detection in uncertain
graphs. <em>TKDE</em>, <em>35</em>(2), 1460–1472. (<a
href="https://doi.org/10.1109/TKDE.2021.3094549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertain graphs have been widely used to model complex linked data in many applications, such as guaranteed-loan networks and power grids. In these networks, a node usually has a certain chance of default due to self-factors or the influence from upstream nodes. For regulatory authorities, it is critical to efficiently identify the vulnerable nodes, i.e., nodes with high default risks, such that they could pay more attention to these nodes for the purpose of risk management. In this paper, we propose and investigate the top- $k$ vulnerable nodes detection problem in uncertain graphs. We formally define the model and prove it hardness. A sampling-based approach is first proposed. Rigorous theoretical analysis is conducted to bound the quality of returned results. Novel optimization techniques and a bottom- $k$ sketch based approach are further developed to scale for large networks. We demonstrate the performance of proposed techniques on 3 real financial networks and 5 benchmark networks. Moreover, to further verify the advantages of our model, we integrate the proposed techniques with our loan risk control system, which is deployed in the collaborated bank. Particularly, we show that our proposed model can better estimate the default risks of enterprises compared to the state-of-the-art techniques.},
  archive      = {J_TKDE},
  author       = {Dawei Cheng and Chen Chen and Xiaoyang Wang and Sheng Xiang},
  doi          = {10.1109/TKDE.2021.3094549},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1460-1472},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient top-k vulnerable nodes detection in uncertain graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient EMD-based similarity search via batch pruning and
incremental computation. <em>TKDE</em>, <em>35</em>(2), 1446–1459. (<a
href="https://doi.org/10.1109/TKDE.2021.3100566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a robust similarity measurement, Earth Mover&#39;s Distance (EMD) has been widely adopted in many real-world applications, such as machine learning, computer vision and natural language processing. In this paper, we study the problem of EMD-based similarity search, which aims at finding all histogram objects from a dataset whose EMD is within a pre-defined threshold from the given query. Since the time complexity of computing EMD is rather high, it is essential to devise effective techniques to accelerate the query processing. To this end, we propose a filter-and-verification framework: In the filter step, we devise three effective strategies to prune dissimilar objects in batch by sharing the computation between multiple objects. In the verification step, we develop novel flow adjustment techniques to incrementally calculate the EMD of candidates and enable early termination. We justify our proposed framework by conducting both theoretical analysis and extensive experiments. The results on four real world datasets show that our proposed techniques achieve up to an order of magnitude performance gain than state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Yu Chen and Yong Zhang and Jin Wang and Jiacheng Wu and Chunxiao Xing},
  doi          = {10.1109/TKDE.2021.3100566},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1446-1459},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient EMD-based similarity search via batch pruning and incremental computation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic relation repairing for knowledge enhancement.
<em>TKDE</em>, <em>35</em>(2), 1434–1445. (<a
href="https://doi.org/10.1109/TKDE.2021.3101237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic relation repair aims to efficiently validate and repair the instances for knowledge graph enhancement (KGE), where KGE captures missing relations from unstructured data and leads to noisy facts to the knowledge graph. With the prosperity of unstructured data, an online approach is asked to clean the new RDF tuples before adding them to the knowledge base. To clean the noisy RDF tuples, graph constraint processing is a common but intractable approach. Plus, when adding new tuples to the knowledge graph, new graph patterns would be created, whereas the explicit discovery of graph constraints is also intractable. Therefore, although the dynamic relation repair has an unfortunate hardness, it is a necessary approach for enhancing knowledge graphs effectively under the fast-growing unstructured data. Motivated by this, we establish a dynamic repairing and enhancing structure to analyze its hardness on basic operations. To ensure dynamic repair and validation, we introduce implicit graph constraints, approximate graph matching, and linkage prediction based on localized graph patterns. To validate and repair the RDF tuples efficiently, we further study the cold start problems for graph constraint processing. Experimental results on real datasets demonstrate that our proposed approach can capture and repair instances with wrong relation labels dynamically and effectively.},
  archive      = {J_TKDE},
  author       = {Rui Kang and Hongzhi Wang},
  doi          = {10.1109/TKDE.2021.3101237},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1434-1445},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic relation repairing for knowledge enhancement},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual adversarial variational embedding for robust
recommendation. <em>TKDE</em>, <em>35</em>(2), 1421–1433. (<a
href="https://doi.org/10.1109/TKDE.2021.3093773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust recommendation aims at capturing true preference of users from noisy data, for which there are two lines of methods have been proposed. One is based on noise injection, and the other is to adopt the generative model Variational Auto-encoder (VAE). However, the existing works still face two challenges. First, the noise injection based methods often draw the noise from a fixed noise distribution given in advance, while in real world, the noise distributions of different users and items may differ from each other due to personal behaviors and item usage patterns. Second, the VAE based models are not expressive enough to capture the true preference since VAE often yields an embedding space of a single modal, while in real world, user-item interactions usually exhibit multi-modality on user preference distribution. In this paper, we propose a novel model called Dual Adversarial Variational Embedding (DAVE) for robust recommendation, which can provide personalized noise reduction for different users and items, and capture the multi-modality of the embedding space, by combining the advantages of VAE and adversarial training between the introduced auxiliary discriminators and the variational inference networks. The extensive experiments conducted on real datasets verify the effectiveness of DAVE on robust recommendation.},
  archive      = {J_TKDE},
  author       = {Qiaomin Yi and Ning Yang and Philip S. Yu},
  doi          = {10.1109/TKDE.2021.3093773},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1421-1433},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual adversarial variational embedding for robust recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-specific topic model for knowledge discovery in
computational and data-intensive scientific communities. <em>TKDE</em>,
<em>35</em>(2), 1402–1420. (<a
href="https://doi.org/10.1109/TKDE.2021.3093350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortened time to knowledge discovery and adapting prior domain knowledge is a challenge for computational and data-intensive communities such as e.g., bioinformatics and neuroscience. The challenge for a domain scientist lies in the actions to obtain guidance through query of massive information from diverse text corpus comprising of a wide-ranging set of topics when: investigating new methods, developing new tools, or integrating datasets. In this paper, we propose a novel “domain-specific topic model” (DSTM) to discover latent knowledge patterns about relationships among research topics, tools and datasets from exemplary scientific domains. Our DSTM is a generative model that extends the Latent Dirichlet Allocation (LDA) model and uses the Markov chain Monte Carlo (MCMC) algorithm to infer latent patterns within a specific domain in an unsupervised manner. We apply our DSTM to large collections of data from bioinformatics and neuroscience domains that include more than 25,000 of papers over the last ten years, featuring hundreds of tools and datasets that are commonly used in relevant studies. Evaluation experiments based on generalization and information retrieval metrics show that our model has better performance than the state-of-the-art baseline models for discovering highly-specific latent topics within a domain. Lastly, we demonstrate applications that benefit from our DSTM to discover intra-domain, cross-domain and trend knowledge patterns.},
  archive      = {J_TKDE},
  author       = {Yuanxun Zhang and Prasad Calyam and Trupti Joshi and Satish Nair and Dong Xu},
  doi          = {10.1109/TKDE.2021.3093350},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1402-1420},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Domain-specific topic model for knowledge discovery in computational and data-intensive scientific communities},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrete robust matrix factorization hashing for large-scale
cross-media retrieval. <em>TKDE</em>, <em>35</em>(2), 1391–1401. (<a
href="https://doi.org/10.1109/TKDE.2021.3107489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-media hashing, which encodes data points from different modalities into a common Hamming space, has been successfully applied to solve large-scale multimedia retrieval issue due to storage efficiency and search effectiveness. Recently, matrix factorization based hashing methods have drawn considerable attention for their promising search accuracy. However, pioneer methods mainly focus on learning consensus hash codes for different modalities, but neglect the potential inconsistency among different modalities, e.g., the diversities of different modalities and noises, which may undermine the retrieval accuracy. To address this problem, we propose a novel unsupervised hashing model, namely, Discrete Robust Matrix Factorization Hashing (DRMFH), which simultaneously formulates the consistency and inconsistency across different modalities into a matrix factorization based model. Specifically, a homogenous space composed of a consistent Hamming space and an inconsistent diversity part, are generated by matrix factorization for each modality. Therefore, the consensus information across different modalities can be well captured in the learnt hash codes, leading to improved retrieval performance. Moreover, we design an effective optimization algorithm which is able to obtain an approximate discrete code matrix with linear time complexity. Comprehensive experimental results on three public multimedia retrieval datasets show that the proposed DRMFH outperforms several state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Tao Yao and Yiru Li and Weili Guan and Gang Wang and Ying Li and Lianshan Yan and Qi Tian},
  doi          = {10.1109/TKDE.2021.3107489},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1391-1401},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discrete robust matrix factorization hashing for large-scale cross-media retrieval},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovering categorical main and interaction effects based
on association rule mining. <em>TKDE</em>, <em>35</em>(2), 1379–1390.
(<a href="https://doi.org/10.1109/TKDE.2021.3087343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing size of data sets, feature selection becomes increasingly important. Taking interactions of original features into consideration will lead to extremely high dimension, especially when the features are categorical and one-hot encoding is applied. This makes it more worthwhile mining useful features as well as their interactions. Association rule mining aims to extract interesting correlations between items, but it is difficult to use rules as a qualified classifier themselves. Drawing inspiration from association rule mining, we come up with a method that uses association rules to select features and their interactions, then modify the algorithm for several practical concerns. We analyze the computational complexity of the proposed algorithm to show its efficiency. And the results of a series of experiments verify the effectiveness of the algorithm.},
  archive      = {J_TKDE},
  author       = {Qiuqiang Lin and Chuanhou Gao},
  doi          = {10.1109/TKDE.2021.3087343},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1379-1390},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discovering categorical main and interaction effects based on association rule mining},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DAH: Discrete asymmetric hashing for efficient cross-media
retrieval. <em>TKDE</em>, <em>35</em>(2), 1365–1378. (<a
href="https://doi.org/10.1109/TKDE.2021.3099125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the merits in high computational efficiency and low storage cost, hashing techniques have been widely studied in cross-media retrieval. Existing cross-media hashing algorithms usually adopt an equal-length encoding scheme to represent the multimedia data. However, the strictly equal length encoding scheme may not effectively characterize the multimedia data because the underlying dimension of different modalities is often various, challenging its flexible generalization to real-world applications. In addition, there exist other challenges in designing a cross-media retrieval system, e.g., how to address the discrete constraints, how to avoid the complexity in calculating the large $n\times n$ similarity matrix, and how to effectively exploit the discriminative label information. To conquer the above challenges, we propose a novel model, i.e., discrete asymmetric hashing (DAH). In particular, DAH exploits a flexible model to formulate the cross-modal retrieval, which can seamlessly deal with equal or unequal hash length encoding scenarios. Moreover, DAH constructs a supervised semantic embedding framework by jointly minimizing the distance-distance difference and label reconstruction error, significantly reducing the computational complexity. An asymmetric strategy is employed to establish the connection between hash codes and the latent subspace. Furthermore, the hash codes can be learned discretely by the designed optimization algorithm, with which the large quantization error can be avoided. Besides, the developed DAH is a two-stage method, a semantic intersection scheme is proposed in the second stage, resulting in more powerful hash functions. Extensive experiments conducted on several databases show that our DAH is superior to several recent competitive methods in terms of efficiency and accuracy in equal-length encoding scenarios. Our method also achieves effective performance in unequal length encoding scenarios, improving the flexibility in the real-world retrieval process.},
  archive      = {J_TKDE},
  author       = {Donglin Zhang and Xiao-Jun Wu and Tianyang Xu and He-Feng Yin},
  doi          = {10.1109/TKDE.2021.3099125},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1365-1378},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DAH: Discrete asymmetric hashing for efficient cross-media retrieval},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-platform item recommendation for online social
e-commerce. <em>TKDE</em>, <em>35</em>(2), 1351–1364. (<a
href="https://doi.org/10.1109/TKDE.2021.3098702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social e-commerce, as a new concept of e-commerce, uses social media as a new prevalent platform for online shopping. Users are now able to view, add to cart, and buy products within a single social media app. In this paper, we address the problem of cross-platform recommendation for social e-commerce , i.e., recommending products to users when they are shopping through social media. To the best of our knowledge, this is a new and important problem for all e-commerce companies (e.g., Amazon, Alibaba), but it has never been studied before. Existing cross-platform and social-related recommendation methods cannot be applied directly to this problem since they do not co-consider the social information and the cross-platform characteristics together. To study this problem, we collect two real-world datasets from social e-commerce services. We first investigate the heterogeneous shopping behaviors between traditional e-commerce app and social media. Based on these observations from data, we propose CROSS ( C ross-platform R ecommendation for O nline S hopping in S ocial Media), a recommendation framework utilizing not only user-item interaction data on both platforms, but also social relation data on social media. The framework is general, and we propose two variants, CROSS-MF and CROSS-NCF. Extensive experiments on two real-world social e-commerce datasets demonstrate that our proposed CROSS significantly outperforms existing state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Chen Gao and Tzu-Heng Lin and Nian Li and Depeng Jin and Yong Li},
  doi          = {10.1109/TKDE.2021.3098702},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1351-1364},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-platform item recommendation for online social E-commerce},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-platform event popularity analysis via dynamic time
warping and neural prediction. <em>TKDE</em>, <em>35</em>(2), 1337–1350.
(<a href="https://doi.org/10.1109/TKDE.2021.3090663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the primary media for information dissemination is shifting to online platforms. Events usually burst online through multiple modern online media. Therefore, predicting event popularity trends becomes crucial for online platforms to track public concerns and make appropriate decisions. However, little research focuses on event popularity prediction from a cross platform perspective. Challenges stem from the vast diversity of events and media, limited access to aligned datasets across different platforms, and a considerable amount of noise in datasets. In this paper, we solve the cross-platform event popularity prediction problem by proposing a model named DancingLines , which is mainly composed of three parts. First, we propose TF-SW , a semantic-aware popularity quantification model based on Term Frequency with Semantic Weight. TF-SW obtains the event popularity based on Word2Vec and TextRank, and generates Event Popularity Time Series (EPTS). Then, we propose $\omega$ DTW-CD , a pairwise time series alignment model derived from Dynamic Time Wrapping (DTW) with Compound Distance (CD) for aligning the EPTS on several platforms. Finally, we aggregate two time series and propose a neural-based prediction model implementing Long Short-Term Memory (LSTM) with attention mechanism to obtain accurate event popularity predictions. Evaluation results based on large scale real-world datasets demonstrate that DancingLines can efficiently characterize, align, and predict event popularity in a cross-platform manner.},
  archive      = {J_TKDE},
  author       = {Xiaofeng Gao and Wenyi Xu and Zixuan Zhang and Yan Tang and Guihai Chen},
  doi          = {10.1109/TKDE.2021.3090663},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1337-1350},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-platform event popularity analysis via dynamic time warping and neural prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-aware and distance-constrained collective spatial
keyword query. <em>TKDE</em>, <em>35</em>(2), 1324–1336. (<a
href="https://doi.org/10.1109/TKDE.2021.3095388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of location-based services, geo-textual data is becoming ubiquitous. Objects involved in geo-textual data include geospatial locations, textual descriptions or keywords, and various attributes (e.g., a point-of-interest has its expenses and users’ ratings). Many types of spatial keyword queries have been proposed on geo-textual data. Among them, one prominent type is to find, for a query consisting of a query location and some query keywords, a set of multiple objects such that the objects in the set collectively cover all the query keywords and the object set is of good quality according to some criteria. Existing studies define the criteria either based on the geospatial information of the objects solely or simply treat the geospatial information and the attribute information of the objects together without differentiation though they may have different semantics and scales. As a result, they cannot provide users flexibility to express finer grained preferences on the objects. In this paper, we propose a new criterion which is to find a set of objects where the distance (defined based on the geospatial information) is at most a threshold specified by users and the cost (defined based on the attribute information) is optimized. We develop a suite of two algorithms including an exact algorithm and an approximation algorithm with provable guarantees for the problem. We conducted extensive experiments on real datasets which verified the efficiency and effectiveness of proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Harry Kai-Ho Chan and Shengxin Liu and Cheng Long and Raymond Chi-Wing Wong},
  doi          = {10.1109/TKDE.2021.3095388},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1324-1336},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cost-aware and distance-constrained collective spatial keyword query},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Composite neural network: Theory and application to PM2.5
prediction. <em>TKDE</em>, <em>35</em>(2), 1311–1323. (<a
href="https://doi.org/10.1109/TKDE.2021.3099135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates the framework and statistical performance guarantee of the composite neural network, which is composed of a collection of pre-trained and non-instantiated neural network models connected as a rooted directed acyclic graph, for solving complicated applications. A pre-trained neural network model is generally well trained, targeted to approximate a specific function. The advantages of adopting a pre-trained model as a component in composing a complicated neural network are two-fold. One is benefiting from the intelligence and diligence of domain experts, and the other is saving effort in data acquisition as well as computing resources and time for model training. Despite a general belief that a composite neural network may perform better than any a single component, the overall performance characteristics are not clear. In this work, we propose the framework of a composite network, and prove that a composite neural network performs better than any of its pre-trained components with a high probability. In the study, we explore a complicated application—PM2.5 prediction—to support the correctness of the proposed composite network theory. In the empirical evaluations of PM2.5 prediction, the constructed composite neural network models perform better than other machine learning models.},
  archive      = {J_TKDE},
  author       = {Ming-Chuan Yang and Meng Chang Chen},
  doi          = {10.1109/TKDE.2021.3099135},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1311-1323},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Composite neural network: Theory and application to PM2.5 prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining graph neural networks with expert knowledge for
smart contract vulnerability detection. <em>TKDE</em>, <em>35</em>(2),
1296–1310. (<a href="https://doi.org/10.1109/TKDE.2021.3095196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contract vulnerability detection draws extensive attention in recent years due to the substantial losses caused by hacker attacks. Existing efforts for contract security analysis heavily rely on rigid rules defined by experts, which are labor-intensive and non-scalable . More importantly, expert-defined rules tend to be error-prone and suffer the inherent risk of being cheated by crafty attackers. Recent researches focus on the symbolic execution and formal analysis of smart contracts for vulnerability detection, yet to achieve a precise and scalable solution. Although several methods have been proposed to detect vulnerabilities in smart contracts, there is still a lack of effort that considers combining expert-defined security patterns with deep neural networks. In this paper, we explore using graph neural networks and expert knowledge for smart contract vulnerability detection. Specifically, we cast the rich control- and data- flow semantics of the source code into a contract graph . To highlight the critical nodes in the graph, we further design a node elimination phase to normalize the graph. Then, we propose a novel temporal message propagation network to extract the graph feature from the normalized graph, and combine the graph feature with designed expert patterns to yield a final detection system. Extensive experiments are conducted on all the smart contracts that have source code in Ethereum and VNT Chain platforms. Empirical results show significant accuracy improvements over the state-of-the-art methods on three types of vulnerabilities, where the detection accuracy of our method reaches 89.15, 89.02, and 83.21 percent for reentrancy, timestamp dependence, and infinite loop vulnerabilities, respectively.},
  archive      = {J_TKDE},
  author       = {Zhenguang Liu and Peng Qian and Xiaoyang Wang and Yuan Zhuang and Lin Qiu and Xun Wang},
  doi          = {10.1109/TKDE.2021.3095196},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1296-1310},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Combining graph neural networks with expert knowledge for smart contract vulnerability detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CogKR: Cognitive graph for multi-hop knowledge reasoning.
<em>TKDE</em>, <em>35</em>(2), 1283–1295. (<a
href="https://doi.org/10.1109/TKDE.2021.3104310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring new facts from an existing knowledge graph with explainable reasoning processes is an important problem, known as knowledge graph (KG) reasoning. The problem is often formulated as finding the specific path that represents the query relation and connects the query entity and the correct answer. However, due to the limited expressiveness of individual paths, the majority of previous works failed to capture the complex subgraph structure in the graph. We propose CogKR that traverses the knowledge graph to conduct multi-hop reasoning. More specifically, motivated by the dual process theory from cognitive science, our framework is composed of an extension module and a reasoning module. By setting up a cognitive graph through iteratively coordinating the two modules, CogKR can cope with more complex reasoning scenarios in the form of subgraphs instead of individual paths. Experiments on three knowledge graph reasoning benchmarks demonstrate that CogKR achieves significant improvements in accuracy compared with previous methods while providing the explainable capacity. Moreover, we evaluate CogKR on the challenging one-shot link prediction task, exhibiting the superiority of the framework on accuracy and scalability compared to the state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Zhengxiao Du and Chang Zhou and Jiangchao Yao and Teng Tu and Letian Cheng and Hongxia Yang and Jingren Zhou and Jie Tang},
  doi          = {10.1109/TKDE.2021.3104310},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1283-1295},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CogKR: Cognitive graph for multi-hop knowledge reasoning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cluster purging: Efficient outlier detection based on
rate-distortion theory. <em>TKDE</em>, <em>35</em>(2), 1270–1282. (<a
href="https://doi.org/10.1109/TKDE.2021.3103571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rate-distortion theory-based outlier detection builds upon the rationale that a good data compression will encode outliers with unique symbols. Based on this rationale, we propose Cluster Purging, which is an extension of clustering-based outlier detection. This extension allows one to assess the representivity of clusterings, and to find data that are best represented by individual unique clusters. We propose two efficient algorithms for performing Cluster Purging, one being parameter-free, while the other algorithm has a parameter that controls representivity estimations, allowing it to be tuned in supervised setups. In an experimental evaluation, we show that Cluster Purging improves upon outliers detected from raw clusterings, and that Cluster Purging competes strongly against state-of-the-art alternatives.},
  archive      = {J_TKDE},
  author       = {Maximilian B. Toller and Bernhard C. Geiger and Roman Kern},
  doi          = {10.1109/TKDE.2021.3103571},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1270-1282},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cluster purging: Efficient outlier detection based on rate-distortion theory},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ChartNavigator: An interactive pattern identification and
annotation framework for charts. <em>TKDE</em>, <em>35</em>(2),
1258–1269. (<a href="https://doi.org/10.1109/TKDE.2021.3094236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patterns in charts refer to interesting visual features or forms. Identifying patterns not only helps analysts understand the ‘shape’ of the data but also supports better and faster decision-making. Existing solutions for identifying patterns in charts require a large number of labeled data instances, making it intractable without user supervision. In this paper, we propose ChartNavigator, an interactive pattern identification and annotation framework for unlabeled visualization charts. ChartNavigator leverages a novel chart-sensitive deep factor model to map patterns into a low-dimensional factor representation space, and facilitates rich analysis with the derived representations. We design and implement a visual interface to support efficient identification and annotation of potential patterns in charts. Evaluations with multiple datasets show that our approach outperforms the baseline models in identifying and annotating patterns.},
  archive      = {J_TKDE},
  author       = {Tianye Zhang and Haozhe Feng and Wei Chen and Zexian Chen and Wenting Zheng and Xiaonan Luo and Wenqi Huang and Anthony Tung},
  doi          = {10.1109/TKDE.2021.3094236},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1258-1269},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ChartNavigator: An interactive pattern identification and annotation framework for charts},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Category-controlled encoder-decoder for fake news detection.
<em>TKDE</em>, <em>35</em>(2), 1242–1257. (<a
href="https://doi.org/10.1109/TKDE.2021.3103833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing data-driven approaches typically capture credibility-indicative representations from relevant articles for fake news detection, such as skeptical and conflicting opinions. However, these methods still have several drawbacks: 1) Due to the difficulty of collecting fake news, the capacity of the existing datasets is relatively small; and 2) there is considerable unverified news that lacks conflicting voices in relevant articles, which makes it difficult for the existing methods to identify their credibility. Especially, the differences between true and fake news are not limited to whether there are conflict features in their relevant articles, but also include more extensive hidden differences at the linguistic level, such as the perspectives of emotional expression (like extreme emotion in fake news), writing style (like the shocking title in clickbait), etc., the existing methods are difficult to fully capture these differences. To capture more general and wide-ranging differences between true and fake news, in this paper, directly from the different categories of news itself, we propose a Category-controlled Encoder-Decoder model (CED) to generate examples with category-differentiated features and extend the dataset capacity to achieve data enhancement effect, thus enhancing fake news detection. Specifically, to make the generated examples enrich more news features, we develop news-guided encoder to guide relevant articles to generate news-semantic context representations. To drive the generated examples to contain more category-differentiated features, we devise category-controlled decoder which relies on pattern-shared unit to respectively capture intra-category shared features within true or fake news, and employs restriction unit to force the two types of shared features to be more different for highlighting inter-category differentiated features. The experimental results on three datasets demonstrate the superiority of CED.},
  archive      = {J_TKDE},
  author       = {Lianwei Wu and Yuan Rao and Cong Zhang and Yongqiang Zhao and Ambreen Nazir},
  doi          = {10.1109/TKDE.2021.3103833},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1242-1257},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Category-controlled encoder-decoder for fake news detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BANDAR: Benchmarking snippet generation algorithms for (RDF)
dataset search. <em>TKDE</em>, <em>35</em>(2), 1227–1241. (<a
href="https://doi.org/10.1109/TKDE.2021.3095309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large volume of open data on the Web is expected to be reused and create value. Finding the right data to reuse is a non-trivial task addressed by the recent dataset search systems, which retrieve datasets relevant to a keyword query. An important component of such systems is snippet generation, extracting data from a retrieved dataset to exemplify its content and explain its relevance to the query. Snippet generation algorithms have emerged but were mainly evaluated by user studies. More efficient and reproducible evaluation methods are needed. To meet this challenge, in this article, we present a set of quality metrics for assessing the usefulness of a snippet from different perspectives, and we select and aggregate them into quality profiles for different stages of a dataset search process. Furthermore, we create a benchmark from thousands of collected real-world data needs and datasets, on which we apply the presented quality metrics and profiles to evaluate snippets generated by two existing algorithms and three adapted algorithms. The results, which are reproducible as they are automatically computed without human interaction, show the pros and cons of the tested algorithms and highlight directions for future research. The benchmark data is publicly available.},
  archive      = {J_TKDE},
  author       = {Xiaxia Wang and Gong Cheng and Jeff Z. Pan and Evgeny Kharlamov and Yuzhong Qu},
  doi          = {10.1109/TKDE.2021.3095309},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1227-1241},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BANDAR: Benchmarking snippet generation algorithms for (RDF) dataset search},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AliExpress learning-to-rank: Maximizing online model
performance without going online. <em>TKDE</em>, <em>35</em>(2),
1214–1226. (<a href="https://doi.org/10.1109/TKDE.2021.3098898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-to-rank (LTR) has become a key technology in E-commerce applications. Most existing LTR approaches follow a supervised learning paradigm with data collected from an online system. Yet, LTR models sometimes have good performance on the offline validation set but poor performance with online metrics, suggesting an inconsistency exists between offline and online evaluation measurements. We confirm that this inconsistency exists in AliExpress Search, the search engine for an international E-commerce business. One major reason for the inconsistency is the ignorance of the item context, as the item order of the newly served model is always different from that in the offline dataset. This paper proposes an evaluator-generator framework for E-commerce LTR with the item context. The framework consists of an evaluator that generalizes to evaluate recommendations involving the context, a generator that maximizes the score of the evaluator with reinforcement learning, and an adversarially-trained discriminator that ensures the reliable explorations of the generator. Extensive experiments in the simulation environment and AliExpress Search show that, first, the classic data-based metrics on the offline dataset has an obvious inconsistency with online performance, and can even be misleading. Second, the proposed evaluator score is significantly more consistent with the online performance than common ranking metrics. As a result, our method achieves a significant improvement (&gt; $2\%$ ) in terms of Conversion Rate (CR) over the industrial-level fine-tuned model in online A/B tests.},
  archive      = {J_TKDE},
  author       = {Guangda Huzhang and Zhen-Jia Pang and Yongqing Gao and Yawen Liu and Weijie Shen and Wen-Ji Zhou and Qianying Lin and Qing Da and An-Xiang Zeng and Han Yu and Yang Yu and Zhi-Hua Zhou},
  doi          = {10.1109/TKDE.2021.3098898},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1214-1226},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AliExpress learning-to-rank: Maximizing online model performance without going online},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial representation mechanism learning for network
embedding. <em>TKDE</em>, <em>35</em>(2), 1200–1213. (<a
href="https://doi.org/10.1109/TKDE.2021.3103193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding which is to learn a low dimensional representation of nodes in a network has been used in many network analysis tasks. Some network embedding methods, including those based on Generative Adversarial Networks (GAN) (a promising deep learning model), have been proposed recently. Existing GAN-based methods typically use GAN to learn a Gaussian distribution as a prior for network embedding, which makes it difficult to distinguish the node representation from Gaussian distribution. It did not apply the adversarial learning strategy on the representation mechanism but just on representation results. Thus, it does not make full use of the essential advantage of GAN, and leads to compromised performance of the method. To address this problem, we propose a novel adversarial learning framework consisting of three players for network embedding, which applies the adversarial learning strategy on the representation mechanism, called Adversarial representation mechanism GAN (ArmGAN). Specifically, the first two players, named encoder and competitor, aim to learn two different representation mechanisms (i.e., two ways projecting data onto latent space). They compete with each other to improve their representation mechanisms. The third player is the discriminator, which discriminate the representation mechanism of the encoder from that of the competitor. In addition, we design a perturbation strategy to produce fake networks from the original network, and feed the fake networks to the competitor to obtain a “fake” representation mechanism. We evaluated ArmGAN on a variety of tasks including node clustering, node classification, link prediction and visualization. Moreover, we compared ArmGAN with 10 state-of-the-art methods (including DGI, which is well-known for its high accuracy) on 7 real-world networks. The experimental results show the significant superiority of ArmGAN over the existing methods.},
  archive      = {J_TKDE},
  author       = {Dongxiao He and Tao Wang and Lu Zhai and Di Jin and Liang Yang and Yuxiao Huang and Zhiyong Feng and Philip S. Yu},
  doi          = {10.1109/TKDE.2021.3103193},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1200-1213},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adversarial representation mechanism learning for network embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive label correlation based asymmetric discrete hashing
for cross-modal retrieval. <em>TKDE</em>, <em>35</em>(2), 1185–1199. (<a
href="https://doi.org/10.1109/TKDE.2021.3102119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing methods have captured much attention for cross-modal retrieval in recent years. Most existing approaches mainly focus on preserving the semantic similarity across heterogeneous modalities in a shared Hamming subspace, while the label information and potential correlations of multi-label semantics are not fully excavated. In this article, a novel Adaptive Label correlation based asymmEtric Cross-modal Hashing method, i.e., ALECH, is proposed for cross-modal retrieval. ALECH decomposes hash learning into two steps, hash codes learning and hash functions learning. For hash codes learning, the high-order semantic label correlations are adaptively exploited to guide the latent feature learning, while simultaneously generating the binary codes in a discrete manner. The asymmetric strategy is utilized to connect the latent feature space and Hamming space, and preserve the pairwise semantic similarity. Different from other two-step methods that directly adopt simple least-squares regression to learn hash functions based on binary codes, ALECH leverages both hash codes and semantic labels for hash functions learning which further preserves the similarity. Experiments on several benchmark datasets demonstrate that the proposed ALECH method outperforms the state-of-the-art cross-hashing methods.},
  archive      = {J_TKDE},
  author       = {Huaxiong Li and Chao Zhang and Xiuyi Jia and Yang Gao and Chunlin Chen},
  doi          = {10.1109/TKDE.2021.3102119},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1185-1199},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive label correlation based asymmetric discrete hashing for cross-modal retrieval},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified framework for cross-domain and cross-system
recommendations. <em>TKDE</em>, <em>35</em>(2), 1171–1184. (<a
href="https://doi.org/10.1109/TKDE.2021.3104873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Domain Recommendation (CDR) and Cross-System Recommendation (CSR) have been proposed to improve the recommendation accuracy in a target dataset (domain/system) with the help of a source one with relatively richer information. However, most existing CDR and CSR approaches are single-target, namely, there is a single target dataset, which can only help the target dataset and thus cannot benefit the source dataset. In this paper, we focus on three new scenarios, i.e., Dual-Target CDR (DTCDR), Multi-Target CDR (MTCDR), and CDR+CSR, and aim to improve the recommendation accuracy in all datasets simultaneously for all scenarios. To do this, we propose a unified framework, called GA (based on G raph embedding and A ttention techniques), for all three scenarios. In GA, we first construct separate heterogeneous graphs to generate more representative user and item embeddings. Then, we propose an element-wise attention mechanism to effectively combine the embeddings of common entities (users/items) learned from different datasets. Moreover, to avoid negative transfer, we further propose a P ersonalized training strategy to minimize the embedding difference of common entities between a richer dataset and a sparser dataset, deriving three new models, i.e., GA-DTCDR-P, GA-MTCDR-P, and GA-CDR+CSR-P, for the three scenarios respectively. Extensive experiments conducted on four real-world datasets demonstrate that our proposed GA models significantly outperform the state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Feng Zhu and Yan Wang and Jun Zhou and Chaochao Chen and Longfei Li and Guanfeng Liu},
  doi          = {10.1109/TKDE.2021.3104873},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1171-1184},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A unified framework for cross-domain and cross-system recommendations},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of community detection approaches: From statistical
modeling to deep learning. <em>TKDE</em>, <em>35</em>(2), 1149–1170. (<a
href="https://doi.org/10.1109/TKDE.2021.3104155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection, a fundamental task for network analysis, aims to partition a network into multiple sub-structures to help reveal their latent functions. Community detection has been extensively studied in and broadly applied to many real-world network problems. Classical approaches to community detection typically utilize probabilistic graphical models and adopt a variety of prior knowledge to infer community structures. As the problems that network methods try to solve and the network data to be analyzed become increasingly more sophisticated, new approaches have also been proposed and developed, particularly those that utilize deep learning and convert networked data into low dimensional representation. Despite all the recent advancement, there is still a lack of insightful understanding of the theoretical and methodological underpinning of community detection, which will be critically important for future development of the area of network analysis. In this paper, we develop and present a unified architecture of network community-finding methods to characterize the state-of-the-art of the field of community detection. Specifically, we provide a comprehensive review of the existing community detection methods and introduce a new taxonomy that divides the existing methods into two categories, namely probabilistic graphical model and deep learning. We then discuss in detail the main idea behind each method in the two categories. Furthermore, to promote future development of community detection, we release several benchmark datasets from several problem domains and highlight their applications to various network analysis tasks. We conclude with discussions of the challenges of the field and suggestions of possible directions for future research.},
  archive      = {J_TKDE},
  author       = {Di Jin and Zhizhi Yu and Pengfei Jiao and Shirui Pan and Dongxiao He and Jia Wu and Philip S. Yu and Weixiong Zhang},
  doi          = {10.1109/TKDE.2021.3104155},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1149-1170},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey of community detection approaches: From statistical modeling to deep learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A noise-aware method with type constraint pattern for neural
relation extraction. <em>TKDE</em>, <em>35</em>(2), 1134–1148. (<a
href="https://doi.org/10.1109/TKDE.2021.3108547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distant supervision is an efficient way to generate large-scale training data for relation extraction without human efforts. However, the accompanying challenges have been plaguing the advance of the extractor: (1) the automatically annotated labels for training data contain much noisy data and hurt the performance of the extractor; (2) the annotations, based on bag-level (cluster of sentences) instead of sentence-level (single sentence), are too coarse to train an accurate extractor; (3) hetergeneous sentences are hard for a denoising model to capture the underlying commonness among valid relational expressions. To address these issues, we bulid a novel sentence representation and craft reinforcement learning to select the expressive sentence for each relation mentioned in a bag. More specifically, we introduce entity-free sentence pattern incorporated with attentive type information. Furthermore, multiple interactions between entity-specific and entity-free representation are proposed to generate complementary sentence features (for challenge 3). Then we design a fine-grained reward function, and model the sentence selection process as an auction where different relations for a bag need to compete together to achieve the possession of a specific sentence based on its expressiveness. In this way, our model can be dynamically self-adapted, and eventually implements the accurate one-to-one mapping from a relation label to its chosen expressive sentence, which serves as training instances for the extractor (for challenge 1 and 2). The experimental results on two public datasets demonstrate the superiority of our model compared with current state-of-the-art methods for distantly supervised relation extraction.},
  archive      = {J_TKDE},
  author       = {Jianfeng Qu and Wen Hua and Dantong Ouyang and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2021.3108547},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1134-1148},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A noise-aware method with type constraint pattern for neural relation extraction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework to support continuous range queries over
multi-attribute trajectories. <em>TKDE</em>, <em>35</em>(2), 1119–1133.
(<a href="https://doi.org/10.1109/TKDE.2021.3100650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging applications over spatio-temporal trajectories require representing the data from diverse aspects. We study multi-attribute trajectories each of which consists of a sequence of time-stamped locations and a set of attributes characterizing diverse aspects. We investigate continuous range queries over multi-attribute trajectories. Such a query returns trajectories whose attributes contain expected values and whose locations are always within a distance threshold to the query trajectory during the entire overlapping time period. To efficiently answer the query, an optimal method of partitioning the trajectories is proposed and an index structure is developed to support the combined search using both spatio-temporal parameters and attribute values. Query algorithms and auxiliary structures are developed, accompanied with optimization strategies and thorough theoretical analysis. Using both real and synthetic datasets, we carry out comprehensive experiments in a prototype database system to evaluate the efficiency and scalability of our designs. The experimental results show that our approach outperforms six alternative approaches by a factor of 5-50x on large datasets.},
  archive      = {J_TKDE},
  author       = {Jianqiu Xu and Zhifeng Bao and Hua Lu},
  doi          = {10.1109/TKDE.2021.3100650},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1119-1133},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A framework to support continuous range queries over multi-attribute trajectories},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dual-store structure for knowledge graphs. <em>TKDE</em>,
<em>35</em>(2), 1104–1118. (<a
href="https://doi.org/10.1109/TKDE.2021.3093200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To effectively manage increasing knowledge graphs in various domains, a hot research topic, knowledge graph storage management, has emerged. Existing methods are classified to relational stores and native graph stores. Relational stores are able to store large-scale knowledge graphs and convenient in updating knowledge, but the query performance weakens obviously when the selectivity of a knowledge graph query is large. Native graph stores are efficient in processing complex knowledge graph queries due to its index-free adjacent property, but they are inapplicable to manage a large-scale knowledge graph due to limited storage budgets or inflexible updating process. Motivated by this, we propose a dual-store structure which leverages a graph store to accelerate the complex query process in the relational store. However, it is challenging to determine what data to transfer from relational store to graph store at what time. To address this problem, we formulate it as a Markov Decision Process and derive a physical design tuner ${{\sf DOTIL}}$ based on reinforcement learning. With ${{\sf DOTIL}}$ , the dual-store structure is adaptive to dynamic changing workloads. Experimental results on real knowledge graphs demonstrate that our proposed dual-store structure improves query performance up to average 50.11 percent compared with the most commonly used relational stores.},
  archive      = {J_TKDE},
  author       = {Zhixin Qi and Hongzhi Wang and Haoran Zhang},
  doi          = {10.1109/TKDE.2021.3093200},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1104-1118},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A dual-store structure for knowledge graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 2-hop+ sampling: Efficient and effective influence
estimation. <em>TKDE</em>, <em>35</em>(2), 1088–1103. (<a
href="https://doi.org/10.1109/TKDE.2021.3093934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapidly growing sizes of online social networks, computational challenges arise in analyzing the diffusion process over networks. Sampling methods are commonly used to study the cascade effect and estimate users’ influence. In this paper, we propose a brand-new sampling method, called 2-hop+ sampling for quickly and accurately estimating the cascade size generated by a set of seed users under the independent cascade model. Our method generates only samples with at least one 2-hop live path from the source to reduce the number of samples. We further enhance the sampling efficiency of our method by a $\mathsf{SkipEdge}$ technique. Moreover, we improve the generalized stopping rule algorithm to obtain an $(\varepsilon,\delta)$ -estimate of the mean of random variables with fewer samples needed. Extensive experiments with real-world datasets show that our techniques can significantly improve the estimation efficiency compared to the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Yuqing Zhu and Jing Tang and Xueyan Tang and Sibo Wang and Andrew Lim},
  doi          = {10.1109/TKDE.2021.3093934},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {2},
  pages        = {1088-1103},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {2-hop+ sampling: Efficient and effective influence estimation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Where to: Crowd-aided path selection by selective bayesian
network. <em>TKDE</em>, <em>35</em>(1), 1072–1087. (<a
href="https://doi.org/10.1109/TKDE.2021.3078833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide usage of geo-positioning services (GPS), GPS-based navigation systems have become more and more of an integral part of people’s daily lives. GPS-based navigation systems usually suggest multiple paths for a pair of given source and target. Therefore, users become perplexed when trying to select the best one among them, namely the problem of best path selection . Too many suggested paths may jeopardize the usability of the recommendation data, and decrease user satisfaction. Although the existing studies have already partially relieved this problem through integrating historical traffic logs or updating traffic conditions periodically, their solutions neglect the potential contribution of human experiences. In this paper, we resort to crowdsourcing to ease the pain of best path selection. However, the first step of using the crowd is to ask the right questions. For best path selection problem, the simple questions (e.g., binary voting) on crowdsourcing platforms cannot be directly applied to road networks. Thus, in this paper, we have made the first contribution by designing two right types of questions, namely Routing Query (RQ) to ask the crowd to decide the direction at each road intersection. Second, we propose a series of efficient algorithms to dynamically manage the questions in order to reduce the selection hardness within a limited budget. In particular, we show that there are two factors affecting the informativeness of a question: the randomness (entropy) of the question and the structural position of the road intersection. Furthermore, we extend the framework to enable multiple RQs per round. To ease the pain of the sample sensitiveness, we propose a new approach to reduce the selection hardness by reasoning on a so-called Selective Bayesian network . We compare our approach against several baselines, and the effectiveness and efficiency of our proposal are verified by the results in simulations and experiments on real-world datasets. The experimental results show that, even the Selective Bayesian Network provides only partial information of causality, the performance on the reduction of the selection hardness are dramatically improved, especially when the size of samples are relatively small.},
  archive      = {J_TKDE},
  author       = {Chen Zhang and Haodi Zhang and Weiteng Xie and Nan Liu and Kaishun Wu and Lei Chen},
  doi          = {10.1109/TKDE.2021.3078833},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {1072-1087},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Where to: Crowd-aided path selection by selective bayesian network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Utility-preserving privacy protection of textual documents
via word embeddings. <em>TKDE</em>, <em>35</em>(1), 1058–1071. (<a
href="https://doi.org/10.1109/TKDE.2021.3076632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text is the most usual way to share information in society. Yet, if textual documents contain personal sensitive information, they cannot be shared with third parties or released publicly without adequate protection. Privacy-preserving mechanisms provide ways to sanitize data so that identities and/or confidential attributes are not disclosed. In the last twenty years, a great variety of mechanisms have been proposed to protect structured databases with numerical and categorical attributes; however, little attention has been devoted to unstructured textual data. In general, textual data protection requires first detecting pieces of text that may lead to disclosure of sensitive information and then masking those pieces via suppression or generalization. Current solutions rely on pre-trained classifiers that can recognize a fixed set of (allegedly disclosive) named entities, such as names or locations. Yet, such approaches fall short of providing adequate protection because in reality disclosive information is not limited to a predefined set of entity types, and not all the appearances of certain entity type result in disclosure. Besides, named entity recognition requires considerable manual effort to tag the training data needed to build classifiers. In this work we propose a more general and flexible solution for textual data protection. By means of word embeddings we build vectors that numerically capture the semantic relationships of the textual terms appearing in a collection of documents. Then we evaluate the disclosure caused by the textual terms on the entity to be protected (e.g., an individual’s identity or a confidential attribute) according to the similarity between their vector representations. Our method also limits the semantic loss (and, therefore, the utility loss) of the document by replacing (rather than just suppressing) disclosive terms with privacy-preserving generalizations. Empirical results show that our approach offers much more robust protection and greater utility preservation than methods based on named entity recognition, with the additional important advantage of avoiding the burden of manual data tagging.},
  archive      = {J_TKDE},
  author       = {Fadi Hassan and David Sánchez and Josep Domingo-Ferrer},
  doi          = {10.1109/TKDE.2021.3076632},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {1058-1071},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Utility-preserving privacy protection of textual documents via word embeddings},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TwiCS: Lightweight entity mention detection in targeted
twitter streams. <em>TKDE</em>, <em>35</em>(1), 1043–1057. (<a
href="https://doi.org/10.1109/TKDE.2021.3088716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microblogging sites, like Twitter, continuously generate a large volume of streaming data. This streaming environment creates new challenges for two concomitant Information Extraction tasks: Entity Mention Detection (EMD) and Entity Detection (ED). The new challenges include (1) continuously evolving topics, which may deprecate model-based approaches quickly; (2) non-literary nature of posts, which makes traditional NLP techniques less effective; and (3) huge volume of streaming data, which makes computationally expensive approaches less suitable. In this paper, we propose an approach for EMD/ED whose creation is guided by the constraints specific to streaming environments from the ground up. Our system TwiCS implements this approach. TwiCS employs a computationally light two-phase process. In the first phase, it exploits simple (low computation) syntactic cues to suggest Entity Mention (EM) candidates. In the second phase, it uses occurrence mining to classify candidates according to their likelihood of being true EMs. Our experiments show that TwiCS achieves an average effectiveness improvement of 14.6 percent, while maintaining at least 2.64 times higher throughput, when compared to several state-of-the-art systems.},
  archive      = {J_TKDE},
  author       = {Satadisha Saha Bhowmick and Eduard C. Dragut and Weiyi Meng},
  doi          = {10.1109/TKDE.2021.3088716},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {1043-1057},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TwiCS: Lightweight entity mention detection in targeted twitter streams},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). T-SQL: A lightweight implementation to enable built-in
temporal support in MVCC-based RDBMSs. <em>TKDE</em>, <em>35</em>(1),
1028–1042. (<a href="https://doi.org/10.1109/TKDE.2021.3081717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of temporal expressions into SQL:2011 has continuously driven the extensions of temporal support in relational database systems (a.b.a. RDBMSs). In this paper, we present T-SQL , a lightweight yet efficient built-in temporal implementation in RDBMSs. T-SQL entirely relies on multi-version concurrency control (MVCC), widely adopted in RDMBSs, to manage temporal data . For temporal data, current records are maintained in legacy databases, and historical records , i.e., previous versions of current records (if any), which used to be periodically reclaimed are separately maintained in KV stores. To enable temporal query processing under SQL:2011, we extend the query engine in legacy RDBMSs to support query processing over either current records or historical records or both. Further, regarding temporal data are ever-increasing, we propose various optimizations to reduce the storage overhead of KV stores while keeping efficient query performance. We elaborate a publicly available implementation on how to integrate T-SQL into both centralized and distributed RDBMSs. We conduct extensive experiments on both YCSB and TPC-series benchmarks by comparing T-SQL with other temporal database systems. The results show that T-SQL is both lightweight and efficient.},
  archive      = {J_TKDE},
  author       = {Zhanhao Zhao and Wei Lu and Hongyao Zhao and Zongyan He and Haixiang Li and Anqun Pan and Xiaoyong Du},
  doi          = {10.1109/TKDE.2021.3081717},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {1028-1042},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {T-SQL: A lightweight implementation to enable built-in temporal support in MVCC-based RDBMSs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). TrajMesa: A distributed NoSQL-based trajectory data
management system. <em>TKDE</em>, <em>35</em>(1), 1013–1027. (<a
href="https://doi.org/10.1109/TKDE.2021.3079880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of positioning technology, a large number of trajectories have been generated, which are very useful for many urban applications. However, it is challenging to manage trajectory data for its spatio-temporal dynamics and high-volume properties. Existing trajectory data management frameworks suffer from efficiency or scalability problem, and support only limited trajectory query types. This paper takes the first attempt to build a holistic distributed NoSQL trajectory query engine, named TrajMesa, based on GeoMesa, an open-source indexing toolkit for spatio-temporal data. TrajMesa can manage a prohibitively large number of trajectories, and support plenty of query types efficiently. Specifically, we first design a novel trajectory storage schema, which reduces the storage size tremendously. We then devise a novel indexing key schema for time ranges, based on which ID (i.e., moving object identifier) temporal query can be supported efficiently. To reduce the amount of retrieved trajectory data for a spatial range query, we propose a position code to indicate the spatial location of trajectories accurately. We also propose a bunch of pruning strategies for similarity query and $k$ -NN query in the NoSQL environment. Extensive experiments are conducted using two real datasets and one synthetic dataset, verifying the powerful query efficiency and scalability of TrajMesa. The results show that TrajMesa is about $100 \sim 1000$ times faster than the state-of-the-art trajectory management frameworks in our experimental settings. TrajMesa is currently deployed in JD company, processing over 1T trajectories of JD Logistics every day.},
  archive      = {J_TKDE},
  author       = {Ruiyuan Li and Huajun He and Rubin Wang and Sijie Ruan and Tianfu He and Jie Bao and Junbo Zhang and Liang Hong and Yu Zheng},
  doi          = {10.1109/TKDE.2021.3079880},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {1013-1027},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TrajMesa: A distributed NoSQL-based trajectory data management system},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards indoor temporal-variation aware shortest path query.
<em>TKDE</em>, <em>35</em>(1), 998–1012. (<a
href="https://doi.org/10.1109/TKDE.2021.3076144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent years have witnessed the growing popularity of indoor location-based services (LBS) in practice and research. Among others, indoor shortest path query (ISPQ) is of fundamental importance for indoor LBS. However, existing works on ISPQ ignore indoor temporal variations, e.g., the open and close times associated with entities like doors and rooms. In this paper, we define a new type of query called Indoor Temporal-variation aware Shortest Path Query (ITSPQ). It returns the valid shortest path based on the up-to-date indoor topology at the query time. A set of techniques is designed to answer ITSPQ efficiently. We design a graph structure (IT-Graph) that captures indoor temporal variations. To process ITSPQ using IT-Graph, we design two algorithms that check a door’s accessibility synchronously and asynchronously. Furthermore, we propose a novel index structure (IT-Index) that extends the state-of-the-art index significantly by storing dynamic door-to-door distances in a compact distance cube associated with tree nodes. When processing ITSPQ using IT-Index, we make use of the distance cube to avoid time-consuming indoor distance computation on-the-fly. We evaluate the proposed techniques using extensive experiments on synthetic and real data. The results show that our IT-Index based method is the most efficient for processing ITSPQ at a modest cost of index memory consumption.},
  archive      = {J_TKDE},
  author       = {Tiantian Liu and Zijin Feng and Huan Li and Hua Lu and Muhammad Aamir Cheema and Hong Cheng and Jianliang Xu},
  doi          = {10.1109/TKDE.2021.3076144},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {998-1012},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards indoor temporal-variation aware shortest path query},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards fine-grained concept generation. <em>TKDE</em>,
<em>35</em>(1), 986–997. (<a
href="https://doi.org/10.1109/TKDE.2021.3074267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing large-scale taxonomies are crucial for many knowledge-rich applications that need concepts to better understand texts. However, current taxonomies suffer from the scarcity of concepts. Specifically, many fine-grained concepts are missing, while these fine-grained concepts play important roles in understanding related instances more deeply. In this paper, we propose an unsupervised fine-grained concept generation framework called FGCGen, which takes advantages of knowledge bases to generate mass of fine-grained concepts. Specifically, instead of extracting concepts from corpus, FGCGen detects entity heads and modifiers from knowledge bases and combines them to generate fine-grained concepts. We identify critical challenges of this generation process and employ three novel modules to solve them. We evaluate proposed methods on both Chinese and English datasets to show the strength of FGCGen, especially on constructing large-scale high-quality fine-grained taxonomies. Extensive experiments are introduced to prove the efficiency and effectiveness of the modules in FGCGen.},
  archive      = {J_TKDE},
  author       = {Chenguang Li and Jiaqing Liang and Yanghua Xiao and Haiyun Jiang},
  doi          = {10.1109/TKDE.2021.3074267},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {986-997},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards fine-grained concept generation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topic modeling of short texts: A pseudo-document view with
word embedding enhancement. <em>TKDE</em>, <em>35</em>(1), 972–985. (<a
href="https://doi.org/10.1109/TKDE.2021.3073195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the unprecedented growth of online social media, resulting in short texts being the prevalent format of information on the Internet. Given the sparsity of data, however, short-text topic modeling remains a critical yet much-watched challenge in both academia and industry. Research has been devoted to building different types of probabilistic topic models for short texts, among which self-aggregation methods emerged recently to provide informative cross-text word co-occurrences. However, models along this line are still in their infancy and typically yield overfit results and exhibit high computational costs. In this paper, we propose a novel model called Pseudo-document-based Topic Model (PTM), which introduces the concept of pseudo-document to implicitly aggregate short texts against data sparsity. By modeling the topic distributions of latent pseudo-documents rather than short texts, PTM yields excellent performance in accuracy and efficiency. A word embedding-enhanced PTM (WE-PTM) is also proposed to leverage pre-trained word embeddings, which is essential to further alleviating data sparsity. Extensive experiments with self-aggregation or word embedding-based baselines on four real-world datasets including two online media short texts, demonstrate the high-quality topics learned by our models. Robustness to limited training samples and the explainable semantics of topics are also investigated.},
  archive      = {J_TKDE},
  author       = {Yuan Zuo and Congrui Li and Hao Lin and Junjie Wu},
  doi          = {10.1109/TKDE.2021.3073195},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {972-985},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Topic modeling of short texts: A pseudo-document view with word embedding enhancement},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-capturing dynamic graph embedding for temporal linkage
evolution. <em>TKDE</em>, <em>35</em>(1), 958–971. (<a
href="https://doi.org/10.1109/TKDE.2021.3085758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic graph embedding learns representation vectors for vertices and edges in a graph that evolves over time. We aim to capture and embed the evolution of vertices’ temporal connectivity. Existing work studies the vertices’ dynamic connection changes but neglects the time it takes for edges to evolve, failing to embed temporal linkage information into the evolution of the graph. To capture vertices’ temporal linkage evolution, we model dynamic graphs as a sequence of snapshot graphs, appending the respective timespans of edges (ToE). We co-train a linear regressor to embed ToE while inferring a common latent space for all snapshot graphs by a matrix-factorization-based model to embed vertices’ dynamic connection changes. Vertices’ temporal linkage evolution is captured as their moving trajectories within the common latent representation space. Our embedding algorithm converges quickly with our proposed training methods, which is very time efficient and scalable. Extensive evaluations on several datasets show that our model can achieve significant performance improvements, i.e., 22.98 percent on average across all datasets, over the state-of-the-art baselines in the tasks of vertex classification, static and time-aware link prediction, and ToE prediction.},
  archive      = {J_TKDE},
  author       = {Yu Yang and Jiannong Cao and Milos Stojmenovic and Senzhang Wang and Yiran Cheng and Chun Lum and Zhetao Li},
  doi          = {10.1109/TKDE.2021.3085758},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {958-971},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Time-capturing dynamic graph embedding for temporal linkage evolution},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal network motifs: Models, limitations, evaluation.
<em>TKDE</em>, <em>35</em>(1), 945–957. (<a
href="https://doi.org/10.1109/TKDE.2021.3077495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigating the frequency and distribution of small subgraphs with a few nodes/edges, i.e., motifs, is an effective analysis method for static networks. Motif-driven analysis is also useful for temporal networks where the spectrum of motifs is significantly larger due to the additional temporal information on edges. This variety makes it challenging to design a temporal motif model that can consider all aspects of temporality. In the literature, previous works have introduced various models that handle different characteristics. In this work, we compare the existing temporal motif models and evaluate the facets of temporal networks that are overlooked in the literature. We first survey four temporal motif models and highlight their differences. Then, we evaluate the advantages and limitations of these models with respect to the temporal inducedness and timing constraints. In addition, we suggest a new lens, event pairs, to investigate temporal correlations. We believe that our comparative survey and extensive evaluation will catalyze the research on temporal network motif models.},
  archive      = {J_TKDE},
  author       = {Penghang Liu and Valerio Guarrasi and Ahmet Erdem Sarıyüce},
  doi          = {10.1109/TKDE.2021.3077495},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {945-957},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Temporal network motifs: Models, limitations, evaluation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tax evasion detection with FBNE-PU algorithm based on PnCGCN
and PU learning. <em>TKDE</em>, <em>35</em>(1), 931–944. (<a
href="https://doi.org/10.1109/TKDE.2021.3090075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tax evasion is an illegal activity in which individuals or entities avoid paying their true tax liabilities. Efficient detection of tax evasion has always been a crucial issue for both governments and academic researchers. Recent research has proposed the use of machine learning technology to detect tax evasion and has shown good results in some specific areas. Regrettably, there are still two major obstacles to detect tax evasion. First, it is hard to extract powerful features because of the complexity of tax data. Second, due to the complicated process of tax auditing, labeled data are limited in practice. Such obstacles motivate the contributions of this work. In this paper, we propose a novel tax evasion detection framework named FBNE-PU (Fusion of the basic feature and network embedding with PU learning for tax evasion detection), a multistage method for detecting tax evasion in real-life scenarios. In this paper, we perform an in-depth analysis of the characteristics of the transaction network and propose a novel network embedding algorithm, the PnCGCN. It significantly improves detection performance by extracting powerful features from basic features and the tax-related transaction network. Moreover, we use nnPU (positive-unlabeled learning with non-negative risk estimator) to assign pseudo labels for unlabeled data. Finally, an MLP is trained as the decision function. Experiments on three real-world datasets demonstrate that our method significantly outperforms the comparison methods in the tax evasion detection task. Additionally, the source code and the experimental details have been made available at ( https://github.com/PiggyGaGa/FBNE-PU ).},
  archive      = {J_TKDE},
  author       = {Yuda Gao and Bin Shi and Bo Dong and Yiyang Wang and Lingyun Mi and Qinghua Zheng},
  doi          = {10.1109/TKDE.2021.3090075},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {931-944},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Tax evasion detection with FBNE-PU algorithm based on PnCGCN and PU learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SwapKV: A hotness aware in-memory key-value store for hybrid
memory systems. <em>TKDE</em>, <em>35</em>(1), 917–930. (<a
href="https://doi.org/10.1109/TKDE.2021.3077264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory Key-Value (KV) stores are widely deployed in modern data centers. These systems generally use DRAM as their storage medium, causing huge hardware costs. The emerging persistent memory (PMEM) is a potential substitute for DRAM, which has a lower price and larger capacity, but lower access speed and bandwidth. Many prior studies strive to build hybrid memory systems to retain both the advantages of DRAM and PMEM. However, they are either application agnostic or simply take DRAM as a cache, which are both not efficient for in-memory KV stores. In this paper, we propose SwapKV, a well-designed in-memory KV store for hybrid DRAM-PMEM system. SwapKV has several promising properties. First, SwapKV combines DRAM and PMEM to a uniform memory pool and only stores one copy of data, which maximizes capacity utilization. Second, SwapKV maps all writing operations to DRAM and migrates data to PMEM with large blocks asynchronously, which mitigates the intrinsic inefficiency of PMEM for writing operations. Third, SwapKV maintains the hot data in DRAM through an efficient hotness filtering and data swapping mechanism, which ensures high system throughput and responsiveness. We implement SwapKV and evaluate it under various workload patterns. The results demonstrate that SwapKV improves the throughput by 11 $\!\sim\!$ 41 percent compared to the state-of-the-art alternatives.},
  archive      = {J_TKDE},
  author       = {Lixiao Cui and Kewen He and Yusen Li and Peng Li and Jiachen Zhang and Gang Wang and Xiaoguang Liu},
  doi          = {10.1109/TKDE.2021.3077264},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {917-930},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SwapKV: A hotness aware in-memory key-value store for hybrid memory systems},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse graph attention networks. <em>TKDE</em>,
<em>35</em>(1), 905–916. (<a
href="https://doi.org/10.1109/TKDE.2021.3072345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have proved to be an effective representation learning framework for graph-structured data, and have achieved state-of-the-art performance on many practical predictive tasks, such as node classification, link prediction and graph classification. Among the variants of GNNs, Graph Attention Networks (GATs) learn to assign dense attention coefficients over all neighbors of a node for feature aggregation, and improve the performance of many graph learning tasks. However, real-world graphs are often very large and noisy, and GATs are prone to overfitting if not regularized properly. Even worse, the local aggregation mechanism of GATs may fail on disassortative graphs, where nodes within local neighborhood provide more noise than useful information for feature aggregation. In this paper, we propose Sparse Graph Attention Networks (SGATs) that learn sparse attention coefficients under an $L_0$ -norm regularization, and the learned sparse attentions are then used for all GNN layers, resulting in an edge-sparsified graph. By doing so, we can identify noisy/task-irrelevant edges, and thus perform feature aggregation on most informative neighbors. Extensive experiments on synthetic and real-world (assortative and disassortative) graph learning benchmarks demonstrate the superior performance of SGATs. In particular, SGATs can remove about 50-80 percent edges from large assortative graphs, such as PPI and Reddit, while retaining similar classification accuracies. On disassortative graphs, SGATs prune majority of noisy edges and outperform GATs in classification accuracies by significant margins. Furthermore, the removed edges can be interpreted intuitively and quantitatively. To the best of our knowledge, this is the first graph learning algorithm that shows significant redundancies in graphs and edge-sparsified graphs can achieve similar (on assortative graphs) or sometimes higher (on disassortative graphs) predictive performances than original graphs. Our code is available at https://github.com/Yangyeeee/SGAT .},
  archive      = {J_TKDE},
  author       = {Yang Ye and Shihao Ji},
  doi          = {10.1109/TKDE.2021.3072345},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {905-916},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Sparse graph attention networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Set-aware entity synonym discovery with flexible receptive
fields. <em>TKDE</em>, <em>35</em>(1), 891–904. (<a
href="https://doi.org/10.1109/TKDE.2021.3087532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity synonym discovery (ESD) from text corpus is an essential problem in many entity-leveraging applications, e.g., web search and question answering. This paper aims to address three limitations that widely exist in the current ESD solutions: 1) the lack of effective utilization for synonym set information; 2) the feature extraction of entities from restricted receptive fields; and 3) the incapacity to capture higher-order contextual information. We propose a novel set-aware ESD model that enables a flexible receptive field for ESD by making a breakthrough in using entity synonym set information. The contextual information of entities and entity synonym sets are arranged by a two-level network from which entities and entity synonym sets can be mapped into the same embedding space to facilitate ESD by encoding the high-order contexts from flexible receptive fields. Extensive experimental results on public datasets show that our model consistently outperforms the state-of-the-art with significant improvement.},
  archive      = {J_TKDE},
  author       = {Shichao Pei and Lu Yu and Xiangliang Zhang},
  doi          = {10.1109/TKDE.2021.3087532},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {891-904},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Set-aware entity synonym discovery with flexible receptive fields},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised learning with label proportion.
<em>TKDE</em>, <em>35</em>(1), 877–890. (<a
href="https://doi.org/10.1109/TKDE.2021.3076457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scarcity of labels is common and great challenge in traditional supervised learning. Semi-supervised learning (SSL) leverages unlabeled samples to alleviate the absence of label information. Similar with annotation, label proportion is another type of prior information and plays a significant role in classification tasks. Compared with the acquisition of labels, label proportion can be obtained more easily. For example, only a small number of patients have been diagnosed with or not with cancers in hospital database, while the proportion with cancer can be generally estimated by historical records. How to incorporate such prior information of label proportion is crucial but rarely studied in literature. Traditional SSL methods often ignore this prior information and will lead to performance degradation inevitably. To solve this problem, we propose a novel SSL with Label Proportion (SSLLP). Our approach encourages to preserve label consistency and label proportion by imposing the cardinality bound constraints. Our formulated problem equals to a mixed-integer constrained submodular minimization and it is difficult to be solved directly. Therefore, we transformed the original problem into a convex one by Lov $\acute{\text{a}}$ sz extension and designed an efficient solving algorithm. Extensive experimental results present the improved performance of our method over several state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Ningzhao Sun and Tingjin Luo and Wenzhang Zhuge and Hong Tao and Chenping Hou and Dewen Hu},
  doi          = {10.1109/TKDE.2021.3076457},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {877-890},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised learning with label proportion},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning: Generative or contrastive.
<em>TKDE</em>, <em>35</em>(1), 857–876. (<a
href="https://doi.org/10.1109/TKDE.2021.3090866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep supervised learning has achieved great success in the last decade. However, its defects of heavy dependence on manual labels and vulnerability to attacks have driven people to find other paradigms. As an alternative, self-supervised learning (SSL) attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further collect related theoretical analysis on self-supervised learning to provide deeper thoughts on why self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided $^1$ .},
  archive      = {J_TKDE},
  author       = {Xiao Liu and Fanjin Zhang and Zhenyu Hou and Li Mian and Zhaoyu Wang and Jing Zhang and Jie Tang},
  doi          = {10.1109/TKDE.2021.3090866},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {857-876},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Self-supervised learning: Generative or contrastive},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RobustECD: Enhancement of network structure for robust
community detection. <em>TKDE</em>, <em>35</em>(1), 842–856. (<a
href="https://doi.org/10.1109/TKDE.2021.3088844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection, which focuses on clustering vertex interactions, plays a significant role in network analysis. However, it also faces numerous challenges like missing data and adversarial attack. How to further improve the performance and robustness of community detection for real-world networks has raised great concerns. In this paper, we explore robust community detection by enhancing network structure, with two generic algorithms presented: one is named robust community detection via genetic algorithm ( RobustECD-GA ), in which the modularity and the number of clusters are combined in a fitness function to find the optimal structure enhancement scheme; the other is called robust community detection via similarity ensemble ( RobustECD-SE ), integrating multiple information of community structures captured by various vertex similarities, which scales well on large-scale networks. Comprehensive experiments on real-world networks demonstrate, by comparing with two traditional enhancement strategies, that the new methods help six representative community detection algorithms achieve more significant performance improvement. Moreover, experiments on the corresponding adversarial networks indicate that the new methods could also optimize the network structure to a certain extent, achieving stronger robustness against adversarial attack.},
  archive      = {J_TKDE},
  author       = {Jiajun Zhou and Zhi Chen and Min Du and Lihong Chen and Shanqing Yu and Guanrong Chen and Qi Xuan},
  doi          = {10.1109/TKDE.2021.3088844},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {842-856},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RobustECD: Enhancement of network structure for robust community detection},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RatioRF: A novel measure for random forest clustering based
on the tversky’s ratio model. <em>TKDE</em>, <em>35</em>(1), 830–841.
(<a href="https://doi.org/10.1109/TKDE.2021.3086147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose ${\mathrm{RatioRF}}$ , a novel Random Forest-based similarity measure for clustering. We build upon Tversky’s ratio model definition of similarity [1] and specialize it to the Random Forest case. We study some properties of the proposed axiomatic similarity measure and present an extensive experimental clustering analysis involving different datasets and configurations. Results confirm that ${\mathrm{RatioRF}}$ represents a good alternative to other similar measures for clustering recently studied in the literature.},
  archive      = {J_TKDE},
  author       = {Manuele Bicego and Ferdinando Cicalese and Antonella Mensi},
  doi          = {10.1109/TKDE.2021.3086147},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {830-841},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RatioRF: A novel measure for random forest clustering based on the tversky’s ratio model},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On efficient large maximal biplex discovery. <em>TKDE</em>,
<em>35</em>(1), 824–829. (<a
href="https://doi.org/10.1109/TKDE.2021.3077071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cohesive subgraph discovery is an important problem in bipartite graph mining. In this paper, we focus on one kind of cohesive structure, called $k$ -biplex, where each vertex of one side is disconnected from at most $k$ vertices of the other side. We consider the large maximal $k$ -biplex enumeration problem which is to list all those maximal $k$ -biplexes with the number of vertices at each side at least a non-negative integer $\theta$ . This formulation, we observe, has various applications and targets to find non-redundant results by excluding non-maximal ones. Existing approaches suffer from massive redundant computations and can only run on small and moderate datasets. Towards improving scalability, we propose an efficient tree-based algorithm with two advanced strategies and powerful pruning techniques. Experimental results on real and synthetic datasets show the superiority of our algorithm over existing approaches.},
  archive      = {J_TKDE},
  author       = {Kaiqiang Yu and Cheng Long and Deepak P and Tanmoy Chakraborty},
  doi          = {10.1109/TKDE.2021.3077071},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {824-829},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On efficient large maximal biplex discovery},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NetRL: Task-aware network denoising via deep reinforcement
learning. <em>TKDE</em>, <em>35</em>(1), 810–823. (<a
href="https://doi.org/10.1109/TKDE.2021.3091022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network data is mostly hard to obtain and error-prone. However, most existing works assume that the studied network represents a perfect and complete picture of topological structure; nevertheless, it is rarely the case in real-world situations. Such studies, performing downstream applications (e.g., vertex classification, link prediction, etc.) directly on original networks, will suffer greatly due to the noise and deteriorate the application performance. In this paper, we propose NetRL, a novel method for network denoising, that works by creating missing edges and removing incorrect edges from a noisy network, thereby improving its quality and representative power. In particular, NetRL turns the problem of network denoising into edge sequences generation, which can be formulated as a Markov Decision Process. By doing this, NetRL takes the complex long-term dependency between edge creations into consideration, i.e., the existence of an edge depends on which edges have been generated so far. It further takes advantage of downstream task to guide the network denoising process, by providing a deep reinforcement learning framework to conduct direct optimization on this task-specific objective. As a result, NetRL ensures that the denoised network not only satisfies the topological property of the original network, but also improves the performance of the downstream application. Experimental results on real-world networks show that, comparing with several baseline methods, NetRL can denoise networks effectively with better performance for vertex classification. Meanwhile, NetRL can better preserve original network&#39;s properties (e.g., degree distribution and clustering coefficient. Our implementation is available at: https://github.com/galina0217/NetRL .},
  archive      = {J_TKDE},
  author       = {Jiarong Xu and Yang Yang and Shiliang Pu and Yao Fu and Jun Feng and Weihao Jiang and Jiangang Lu and Chunping Wang},
  doi          = {10.1109/TKDE.2021.3091022},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {810-823},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {NetRL: Task-aware network denoising via deep reinforcement learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). NetFense: Adversarial defenses against privacy attacks on
neural networks for graph data. <em>TKDE</em>, <em>35</em>(1), 796–809.
(<a href="https://doi.org/10.1109/TKDE.2021.3087515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in protecting node privacy on graph data and attacking graph neural networks (GNNs) gain much attention. The eye does not bring these two essential tasks together yet. Imagine an adversary can utilize the powerful GNNs to infer users’ private labels in a social network. How can we adversarially defend against such privacy attacks while maintaining the utility of perturbed graphs? In this work, we propose a novel research task, adversarial defenses against GNN-based privacy attacks, and present a graph perturbation-based approach, NetFense, to achieve the goal. NetFense can simultaneously keep graph data unnoticeability (i.e., having limited changes on the graph structure), maintain the prediction confidence of targeted label classification (i.e., preserving data utility), and reduce the prediction confidence of private label classification (i.e., protecting the privacy of nodes). Experiments conducted on single- and multiple-target perturbations using three real graph data exhibit that the perturbed graphs by NetFense can effectively maintain data utility (i.e., model unnoticeability) on targeted label classification and significantly decrease the prediction confidence of private label classification (i.e., privacy protection). Extensive studies also bring several insights, such as the flexibility of NetFense, preserving local neighborhoods in data unnoticeability, and better privacy protection for high-degree nodes.},
  archive      = {J_TKDE},
  author       = {I-Chung Hsieh and Cheng-Te Li},
  doi          = {10.1109/TKDE.2021.3087515},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {796-809},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {NetFense: Adversarial defenses against privacy attacks on neural networks for graph data},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neighbor-anchoring adversarial graph neural networks.
<em>TKDE</em>, <em>35</em>(1), 784–795. (<a
href="https://doi.org/10.1109/TKDE.2021.3087970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have witnessed widespread adoption due to their ability to learn superior representations for graph data. While GNNs exhibit strong discriminative power, they often fall short of learning the underlying node distribution for increased robustness. To deal with this, inspired by generative adversarial networks (GANs), we investigate the problem of adversarial learning on graph neural networks, and propose a novel framework named NAGNN (i.e., Neighbor-anchoring Adversarial Graph Neural Networks) for graph representation learning, which trains not only a discriminator but also a generator that compete with each other. In particular, we propose a novel neighbor-anchoring strategy, where the generator produces samples with explicit features and neighborhood structures anchored on a reference real node, so that the discriminator can perform neighborhood aggregation on the fake samples to learn superior representation. The advantage of our neighbor-anchoring strategy can be demonstrated both theoretically and empirically. Furthermore, as a by-product, our generator can synthesize realistic-looking features, enabling potential applications such as automatic content summarization. Finally, we conduct extensive experiments on four public benchmark datasets, and achieve promising results under both quantitative and qualitative evaluations.},
  archive      = {J_TKDE},
  author       = {Zemin Liu and Yuan Fang and Yong Liu and Vincent W. Zheng},
  doi          = {10.1109/TKDE.2021.3087970},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {784-795},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Neighbor-anchoring adversarial graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Negatively correlated noisy learners for at-risk user
detection on social networks: A study on depression, anorexia,
self-harm, and suicide. <em>TKDE</em>, <em>35</em>(1), 770–783. (<a
href="https://doi.org/10.1109/TKDE.2021.3078898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental and physical health are strongly linked in a bidirectional relationship. Due to the stigma, ignorance, prejudice, fear, and many other reasons, there exists a large universal treatment gap for people with mental disorders. This could motivate those at-risk individuals to find their way into social networks, asking for information or emotional support. Language could provide a natural eyepiece for the study and detection of such at-risk individuals through their writings on social media platforms. In this paper, we consider the problem of detecting at-risk users with clear signs of depression, anorexia, self-harm, and suicidal thoughts. We introduce NCNL, a novel deep learning ensemble architecture that makes use of multiple noisy base learners in Negative Correlation Learning (NCL) configuration for text classification. NCNL is designed to be, backbone-independent, and we examine it with modern Transformer-based architectures. We evaluate our models on six different tasks for at-risk user detection and classification. Our models achieve significant improvements over existing state-of-the-art results reported for five out of the six tasks. Extensive experiments show how NCNL improves diversity over the classical conventional ensemble and the effect of using noisy base learners.},
  archive      = {J_TKDE},
  author       = {Waleed Ragheb and Jérôme Azé and Sandra Bringay and Maximilien Servajean},
  doi          = {10.1109/TKDE.2021.3078898},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {770-783},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Negatively correlated noisy learners for at-risk user detection on social networks: A study on depression, anorexia, self-harm, and suicide},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multi-relational data characterization by tensors:
Perturbation analysis. <em>TKDE</em>, <em>35</em>(1), 756–769. (<a
href="https://doi.org/10.1109/TKDE.2021.3087671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data perturbation is deemed a common problem in data processing. It is often inevitable to avoid noisy or misleading data which may arise from real-world collection or model imprecision. Besides, when data privacy is concerned, data perturbation is used as a prevalent data-protection approach, which alters individual data in a way such that the summary statistics still remain more or less the same. Since many data-mining problems can be formulated as tensor equations for characterizing multi-relational data, the main focus of this work is to perform a new perturbation analysis of tensor equations. From our recent study on tensor inversion, we propose a new mathematical framework to invert an arbitrary tensor but the existing iterative algorithms cannot always do so. In this work, we will establish the theoretical tensor-perturbation analysis to quantify the crucial query performance in terms of normalized error-norm with respect to perturbation degree and condition number. The condition number can be taken as a new measure to determine how the solution of a tensor equation varies as the entries are perturbed. Information-retrieval experiments for conducting the perturbation analysis of the solutions to tensor equations over both artificial and real data are undertaken and studied finally.},
  archive      = {J_TKDE},
  author       = {Shih Yu Chang and Hsiao-Chun Wu},
  doi          = {10.1109/TKDE.2021.3087671},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {756-769},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-relational data characterization by tensors: Perturbation analysis},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal discrete collaborative filtering for efficient
cold-start recommendation. <em>TKDE</em>, <em>35</em>(1), 741–755. (<a
href="https://doi.org/10.1109/TKDE.2021.3079581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing is an effective technique to improve the efficiency of large-scale recommender system by representing both users and items into binary codes. However, existing hashing-based recommendation methods still suffer from two important problems: 1) Cold-start . They employ the user-item interactions and single auxiliary information to learn the binary hash codes. But the full interaction history is not always available and the single auxiliary information may be missing. 2) Efficient optimization . They learn the hash codes with two-step relaxed optimization or one-step discrete hash optimization based on the discrete cyclic coordinate descent, which results in significant quantization loss or still consumes considerable computation time. In this article, we propose a Multi-modal Discrete Collaborative Filtering (MDCF) for efficient cold-start recommendation. We map the multi-modal features of users and items to a consensus Hamming space based on the matrix factorization framework. Specifically, a low-rank self-weighted multi-modal fusion module is designed to adaptively fuse the multi-modal features into binary hash codes. Additionally, to support large-scale recommendation, a fast discrete optimization method based on augmented Lagrangian multiplier is developed to directly compute the binary hash codes with simple operations. Experiments show the superior performance of the proposed method over state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Yang Xu and Lei Zhu and Zhiyong Cheng and Jingjing Li and Zheng Zhang and Huaxiang Zhang},
  doi          = {10.1109/TKDE.2021.3079581},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {741-755},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-modal discrete collaborative filtering for efficient cold-start recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-hop reasoning question generation and its application.
<em>TKDE</em>, <em>35</em>(1), 725–740. (<a
href="https://doi.org/10.1109/TKDE.2021.3073227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the topic of multi-hop question generation (QG), which aims to generate the questions requiring multi-hop reasoning skills by understanding the semantics of the given text fully. These questions not only have valid syntax but also need to be logically correlated to the answers. Concretely, we first design a basic QG model based on the sequence-to-sequence framework. In order to improve the syntactic quality of the results, we customize several techniques to regularize the model&#39;s output. We then extract a reasoning chain heuristically from the given text and use the evidential relations to promote the logical correlations of the results. Considering that different samples have their own characteristics on the aspects of text contextual structure, the type of question, and logical correlation, it is difficult for such a one-size-fits-all model to generate the best results flexibly. Thus, we propose a new adaptive meta-learner to optimize the basic QG model according to the specific characteristic of the evaluated case. Each case and its similar samples are viewed as a pseudo-QG task. The similar structural contexts contained in the same task can be used as guidance to fine-tune the model robustly and produce the optimal results accordingly. Since each sample contains the text, question, and answer, with unknown semantic correlations among them, we propose a data-driven multi-level recognizer to measure the similarity of such structured inputs. The experimental results on two typical data sets in various domains show the effectiveness of the proposed approach. Moreover, we apply the generated results to the task of machine reading comprehension and achieve significant performance improvements. That demonstrates the capacity of multi-hop question generation in facilitating real-world applications.},
  archive      = {J_TKDE},
  author       = {Jianxing Yu and Qinliang Su and Xiaojun Quan and Jian Yin},
  doi          = {10.1109/TKDE.2021.3073227},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {725-740},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-hop reasoning question generation and its application},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MRT: Tracing the evolution of scientific publications.
<em>TKDE</em>, <em>35</em>(1), 711–724. (<a
href="https://doi.org/10.1109/TKDE.2021.3088139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast development of science and technology is accompanied by the booming of cutting edge research. Researchers need to digest more and more recently published publications in order to keep themselves up to date. This becomes tough in particular with the prevalence of preprint publishing such as arXiv, where inspiring works could come out without being peer-reviewed. Is that possible to design an automatic system to help researchers quickly gain a glimpse of a piece of work or gain useful background knowledge for deeply understanding it? To this end, we proposed a practical framework called M aster R eading T ree (MRT) to trace the evolution of scientific publications. In this framework, we can build annotated evolution roadmaps for publications and identify important previous works or evolution tracks by generating expressive embeddings and clustering them into various groups. With comprehensive evaluations, our proposed framework demonstrates its superior capability in capturing underlying relations behind publications over several baseline algorithms. Finally, we integrated the proposed MRT framework on AMiner, an online academic platform, where users can generate roadmaps using MRT for free and their interactions are further used to refine the model.},
  archive      = {J_TKDE},
  author       = {Da Yin and Weng Lam Tam and Ming Ding and Jie Tang},
  doi          = {10.1109/TKDE.2021.3088139},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {711-724},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MRT: Tracing the evolution of scientific publications},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model reuse with reduced kernel mean embedding
specification. <em>TKDE</em>, <em>35</em>(1), 699–710. (<a
href="https://doi.org/10.1109/TKDE.2021.3086619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a publicly available pool of machine learning models constructed for various tasks, when a user plans to build a model for her own machine learning application, is it possible to build upon models in the pool such that the previous efforts on these existing models can be reused rather than starting from scratch? Here, a grand challenge is how to find models that are helpful for the current application, without accessing the raw training data for the models in the pool. In this paper, we present a two-phase framework. In the upload phase, when a model is uploading into the pool, we construct a reduced kernel mean embedding (RKME) as a specification for the model. Then in the deployment phase, the relatedness of the current task and pre-trained models will be measured based on the value of the RKME specification. Theoretical results and extensive experiments validate the effectiveness of our approach.},
  archive      = {J_TKDE},
  author       = {Xi-Zhu Wu and Wenkai Xu and Song Liu and Zhi-Hua Zhou},
  doi          = {10.1109/TKDE.2021.3086619},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {699-710},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Model reuse with reduced kernel mean embedding specification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Missing value imputation for multi-view urban statistical
data via spatial correlation learning. <em>TKDE</em>, <em>35</em>(1),
686–698. (<a href="https://doi.org/10.1109/TKDE.2021.3072642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a developing trend of urbanization, massive amounts of urban statistical data with multiple views (e.g., views of Population and Economy) are increasingly collected and benefited to diverse domains, including transportation service, regional analysis, etc. Unfortunately, these statistical data that are divided into fine-grained regions usually suffer from missing value problem during the acquisition and storage processes. It is mianly caused by some inevitable circumstances, e.g., the document defacement, statistical difficulty in remote districts, and inaccurate information cleaning, etc. Those missing entries which make valuable information invisible may distort the further urban analysis. To improve the quality of missing data imputation, we propose an improved spatial multi-kernel learning method to guide the imputation process incorporating with the adaptive-weight non-negative matrix factorization strategy. Our model takes into account the regional latent similarities and the real geographical positions as well as the correlations among various views that are able to complete missing values precisely. We conduct intensive experiments to evaluate our method and compare with other state-of-the-art approaches on real-world datasets. All the empirical results show that the proposed model outperforms all the other state-of-the-art methods. Additionally, our model represents a strong generalization ability across multiple cities.},
  archive      = {J_TKDE},
  author       = {Yongshun Gong and Zhibin Li and Jian Zhang and Wei Liu and Yilong Yin and Yu Zheng},
  doi          = {10.1109/TKDE.2021.3072642},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {686-698},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Missing value imputation for multi-view urban statistical data via spatial correlation learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Link prediction on n-ary relational data based on
relatedness evaluation. <em>TKDE</em>, <em>35</em>(1), 672–685. (<a
href="https://doi.org/10.1109/TKDE.2021.3073483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the overwhelming popularity of Knowledge Graphs (KGs), researchers have poured attention to link prediction to fill in missing facts for a long time. However, they mainly focus on link prediction on binary relational data, where facts are usually represented as triples in the form of (head entity, relation, tail entity). In practice, n-ary relational facts are also ubiquitous. When encountering such facts, existing studies usually decompose them into triples by introducing a multitude of auxiliary virtual entities and additional triples. These conversions result in the complexity of carrying out link prediction on n-ary relational data. It has even proven that they may cause loss of structure information. To overcome these problems, in this paper, we represent each n-ary relational fact as a set of its role and role-value pairs. We then propose a method called NaLP to conduct link prediction on n-ary relational data, which explicitly models the relatedness of all the role and role-value pairs in an n-ary relational fact. We further extend NaLP by introducing type constraints of roles and role-values without any external type-specific supervision, and proposing a more reasonable negative sampling mechanism. Experimental results validate the effectiveness and merits of the proposed methods.},
  archive      = {J_TKDE},
  author       = {Saiping Guan and Xiaolong Jin and Jiafeng Guo and Yuanzhuo Wang and Xueqi Cheng},
  doi          = {10.1109/TKDE.2021.3073483},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {672-685},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Link prediction on N-ary relational data based on relatedness evaluation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning hierarchical review graph representations for
recommendation. <em>TKDE</em>, <em>35</em>(1), 658–671. (<a
href="https://doi.org/10.1109/TKDE.2021.3075052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The user review data have been demonstrated to be effective in solving different recommendation problems. Previous review-based recommendation methods usually employ sophisticated compositional models, such as Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), to learn semantic representations from the review data for recommendation. However, these methods mainly capture the local dependency between neighbouring words in a word window, and they treat each review equally. Therefore, they may not be effective in capturing the global dependency between words, and tend to be easily biased by noise review information. In this paper, we propose a novel review-based recommendation model, named Review Graph Neural Network (RGNN). Specifically, RGNN builds a specific review graph for each individual user/item, which provides a global view about the user/item properties to help weaken the biases caused by noise review information. A type-aware graph attention mechanism is developed to learn semantic embeddings of words. Moreover, a personalized graph pooling operator is proposed to learn hierarchical representations of the review graph to form the semantic representation for each user/item. We compared RGNN with state-of-the-art review-based recommendation approaches on two real-world datasets. The experimental results indicate that RGNN consistently outperforms baseline methods, in terms of Mean Square Error (MSE).},
  archive      = {J_TKDE},
  author       = {Yong Liu and Susen Yang and Yinan Zhang and Chunyan Miao and Zaiqing Nie and Juyong Zhang},
  doi          = {10.1109/TKDE.2021.3075052},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {658-671},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning hierarchical review graph representations for recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning dynamic user interest sequence in knowledge graphs
for click-through rate prediction. <em>TKDE</em>, <em>35</em>(1),
647–657. (<a href="https://doi.org/10.1109/TKDE.2021.3073717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite that path-based and embedding-based models with knowledge graphs (KGs) achieve better recommendation performance compared with other deep learning based methods, such improvement is limited due to a lack of modeling user’s dynamic interest. To address this issue, we explore a principled model to provide semantic understanding of each item in user’s historical interest sequence in KGs. Specifically, we propose a multi-granularity dynamic interest sequence learning method, which is based on knowledge-enhanced path mining and interest fluctuation signal discovery, to obtain semantic-enhanced paths. Furthermore, the paths are embedded by the SEP2Vec, and merged through the proposed entropy-aware pooling layer to obtain the user preference representation, which is then used to learn dynamic user interest sequence. Experimental results on two public datasets of movie and music recommendation, and two industrial datasets of personalized local service recommendation in Alipay App have illustrated that the proposed model can achieve significantly better prediction performance compared with other known baselines.},
  archive      = {J_TKDE},
  author       = {Youru Li and Xiaobo Guo and Wenfang Lin and Mingjie Zhong and Qunwei Li and Zhongyi Liu and Wenliang Zhong and Zhenfeng Zhu},
  doi          = {10.1109/TKDE.2021.3073717},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {647-657},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning dynamic user interest sequence in knowledge graphs for click-through rate prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge graph for china’s Genealogy11.a shorter version of
this paper won the best paper award at IEEE ICKG 2020 (the 11th IEEE
international conference on knowledge graph, ickg 2020.bigke.org).
<em>TKDE</em>, <em>35</em>(1), 634–646. (<a
href="https://doi.org/10.1109/TKDE.2021.3073745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genealogical knowledge graphs depict the relationships of family networks and the development of family histories. They can help researchers to analyze and understand genealogical data, search for genealogical descendant paths, and explore the origins of a family. However, the heterogenous, autonomous, complex, and evolving natures of genealogical data bring challenges to the development of contemporary genealogical knowledge graph models. Applying existing methods to genealogical data may be improper because general knowledge graph models lack in-depth domain knowledge. In this paper, we propose a genealogical knowledge graph model named Huapu-KG that combines HAO intelligence (human intelligence + artificial intelligence + organizational intelligence) to implement the construction and applications of genealogical knowledge graphs. Furthermore, challenges in constructing genealogical knowledge graphs are demonstrated, and experiments conducted on real-world genealogical datasets verify the feasibility and effectiveness of our proposed model.},
  archive      = {J_TKDE},
  author       = {Xindong Wu and Tingting Jiang and Yi Zhu and Chenyang Bu},
  doi          = {10.1109/TKDE.2021.3073745},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {634-646},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Knowledge graph for china&#39;s Genealogy11.A shorter version of this paper won the best paper award at IEEE ICKG 2020 (the 11th IEEE international conference on knowledge graph, ickg 2020.bigke.org).},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Informed machine learning – a taxonomy and survey of
integrating prior knowledge into learning systems. <em>TKDE</em>,
<em>35</em>(1), 614–633. (<a
href="https://doi.org/10.1109/TKDE.2021.3079836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process which leads to the notion of informed machine learning . In this paper, we present a structured overview of various approaches in this field. We provide a definition and propose a concept for informed machine learning which illustrates its building blocks and distinguishes it from conventional machine learning. We introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Based on this taxonomy, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.},
  archive      = {J_TKDE},
  author       = {Laura von Rueden and Sebastian Mayer and Katharina Beckh and Bogdan Georgiev and Sven Giesselbach and Raoul Heese and Birgit Kirsch and Julius Pfrommer and Annika Pick and Rajkumar Ramamurthy and Michal Walczak and Jochen Garcke and Christian Bauckhage and Jannis Schuecker},
  doi          = {10.1109/TKDE.2021.3079836},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {614-633},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Informed machine learning – a taxonomy and survey of integrating prior knowledge into learning systems},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inferring origin-destination flows from population
distribution. <em>TKDE</em>, <em>35</em>(1), 603–613. (<a
href="https://doi.org/10.1109/TKDE.2021.3075928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Origin-Destination (OD) flow contains the information of direction and volume of population mobility between different regions in a city, having significant value in public transportation resource allocation. Nevertheless, OD flow data is difficult to be collected due to high cost and privacy concerns. Alternatively, population distribution data is more easily accessible and less privacy-sensitive, and its variation can reflect the human mobility flow in urban regions. Motivated by this, in this paper, we explore population distribution to infer OD flows, which is called pop2flow (population distribution to OD flows) problem. Compared to the conventional OD forecasting problem by using the historical OD matrix, pop2flow is more challenging because the population distribution carries much less information. In order to solve the pop2flow problem, we proposed a model, Graph-based Spatial-temporal Embedding with Dynamic Fusion (GSTE-DF). Specifically, GSTE-DF is composed of two parts: node embedding learning and flow prediction. The node embedding learning part captures the dynamic spatial-temporal features of population distribution into each node&#39;s embedding. The flow prediction part adopts the learned embeddings and POI (points of interesting) distribution of every two regions to infer the population interaction between them. By conducting extensive experiments on real-world datasets collected in Beijing and New York City, we demonstrate the superiority of GSTE-DF compared to state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Can Rong and Tong Li and Jie Feng and Yong Li},
  doi          = {10.1109/TKDE.2021.3075928},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {603-613},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Inferring origin-destination flows from population distribution},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incomplete multi-view clustering with joint partition and
graph learning. <em>TKDE</em>, <em>35</em>(1), 589–602. (<a
href="https://doi.org/10.1109/TKDE.2021.3082470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering (IMC) aims to integrate the complementary information from incomplete views to improve clustering performance. Most existing IMC methods try to fill the incomplete views or directly learn a common representation based on matrix factorization or subspace learning. The former may introduce useless and/or even noisy information especially for data with a large missing rate. The latter relies on the initialization and ignores the data structures. To address these issues, we propose a novel Joint Partition and Graph (JPG) learning method for IMC. JPG can be formulated by two key components: unified partition space learning and consensus graph learning. The partition space is more robust to noise and the graph learning helps uncover the data structures. Specifically, JPG iteratively constructs local incomplete graph matrices, generates incomplete base partition matrices, stretches them to produce a unified partition matrix, and employs it to learn a consensus graph matrix. For efficiency, JPG adaptively allocates a large weight to the stretched base partition that is close to the unified partition, determines parameters, and imposes a low-rank constraint on graphs. Finally, the clusters can be obtained directly from the consensus graph. Experimental results on several benchmark datasets demonstrate the effectiveness and superiority of JPG over the state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Lusi Li and Zhiqiang Wan and Haibo He},
  doi          = {10.1109/TKDE.2021.3082470},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {589-602},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incomplete multi-view clustering with joint partition and graph learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). How context or knowledge can benefit healthcare question
answering? <em>TKDE</em>, <em>35</em>(1), 575–588. (<a
href="https://doi.org/10.1109/TKDE.2021.3090253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare question answering (HQA) is a challenging task as questions are generally non-factoid in nature. Traditional information retrieval techniques do not perform well on non-factoid questions. Recent neural question answering systems are reported to have performance gains over traditional methods. However, little attention has been given to HQA as datasets are generally too small to train a neural model from scratch. Recently, several systems have been proposed to learn context representations for HQA. Despite moderate progress, these systems have not been thoroughly compared with state-of-the-art neural models, and these neural models were tested only on self-created datasets. This makes it difficult for practitioners to decide which models should be used for various scenarios. To address the above challenges, we develop a new joint model to incorporate both context and knowledge embeddings into neural ranking architectures. First, we adapt context embedding pre-trained from large open-domain corpus to small healthcare datasets. Second, we learn knowledge embedding from knowledge graphs to provide external information for understanding non-factoid questions. To evaluate how context or knowledge embedding can benefit HQA, we adapt many state-of-the-art methods for general QA to HQA, by injecting the context or knowledge information only, or both of them. Extensive experiments are conducted to compare our approach with those adapted methods and current HQA systems. The results show that our approach achieves the state-of-the-art performance on both HealthQA and NFCorpus datasets.},
  archive      = {J_TKDE},
  author       = {Xiaoli Wang and Feng Luo and Qingfeng Wu and Zhifeng Bao},
  doi          = {10.1109/TKDE.2021.3090253},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {575-588},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {How context or knowledge can benefit healthcare question answering?},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Higher-order attribute-enhancing heterogeneous graph neural
networks. <em>TKDE</em>, <em>35</em>(1), 560–574. (<a
href="https://doi.org/10.1109/TKDE.2021.3074654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been widely used in deep learning on graphs. They can learn effective node representations that achieve superior performances in graph analysis tasks such as node classification and node clustering. However, most methods ignore the heterogeneity in real-world graphs. Methods designed for heterogeneous graphs, on the other hand, fail to learn complex semantic representations because they only use meta-paths instead of meta-graphs. Furthermore, they cannot fully capture the content-based correlations between nodes, as they either do not use the self-attention mechanism or only use it to consider the immediate neighbors of each node, ignoring the higher-order neighbors. We propose a novel Higher-order Attribute-Enhancing (HAE) framework that enhances node embedding in a layer-by-layer manner. Under the HAE framework, we propose a Higher-order Attribute-Enhancing Graph Neural Network (HAE GNN ) for heterogeneous network representation learning. HAE GNN simultaneously incorporates meta-paths and meta-graphs for rich, heterogeneous semantics, and leverages the self-attention mechanism to explore content-based nodes’ interactions. The unique higher-order architecture of HAE GNN allows examining the first-order as well as higher-order neighborhoods. Moreover, HAE GNN shows good explainability as it learns the importances of different meta-paths and meta-graphs. HAE GNN is also memory-efficient, for it avoids per meta-path based matrix calculation. Experimental results not only show HAE GNN &#39;s superior performance against the state-of-the-art methods in node classification, node clustering, and visualization, but also demonstrate its superiorities in terms of memory efficiency and explainability.},
  archive      = {J_TKDE},
  author       = {Jianxin Li and Hao Peng and Yuwei Cao and Yingtong Dou and Hekai Zhang and Philip S. Yu and Lifang He},
  doi          = {10.1109/TKDE.2021.3074654},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {560-574},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Higher-order attribute-enhancing heterogeneous graph neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical multi-view graph pooling with structure
learning. <em>TKDE</em>, <em>35</em>(1), 545–559. (<a
href="https://doi.org/10.1109/TKDE.2021.3090664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs), whch generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we proposed a novel multi-view graph pooling operator dubbed as MVPool, which ranks nodes across different views with different contextual graph information. Meanwhile, attention mechanism is utilized to promote the collaboration of different views for generating robust node rankings. Then the pooling operation adaptively selects a subset of nodes to form an induced subgraph based on the ranking list. To preserve the underlying graph topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. The proposed MVPool operator is a general strategy that can be integrated into various graph neural network architectures, including GCN, GAT and GraphSAGE, etc. By combining MVPool operator with graph neural networks, we perform hierarchical representation learning for both node and graph level classification as well as clustering tasks. Experimental results on thirteen widely used transductive and inductive benchmarks demonstrate the effectiveness of our proposed model.},
  archive      = {J_TKDE},
  author       = {Zhen Zhang and Jiajun Bu and Martin Ester and Jianfeng Zhang and Zhao Li and Chengwei Yao and Huifen Dai and Zhi Yu and Can Wang},
  doi          = {10.1109/TKDE.2021.3090664},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {545-559},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical multi-view graph pooling with structure learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous latent topic discovery for semantic text
mining. <em>TKDE</em>, <em>35</em>(1), 533–544. (<a
href="https://doi.org/10.1109/TKDE.2021.3077025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to mine latent semantics from text data, word embedding and topic modeling are two major methodologies in the industry. From a pragmatic perspective, each of these two lines of semantic models faces increasing challenges from real-life applications. Topic modeling view documents as bags of words and is unable to capture the sequential relationship between words. On the other hand, word embedding models the co-occurrence of neighboring words but lacks the global view of the document. Therefore, they can only discover homogenous semantics from a single aspect. However, modern text mining tasks typically require a panoramic view of the latent semantics. Hence, discovering heterogeneous semantics (e.g., heterogeneous types of latent topics) is critical for the performance of these tasks, and it is necessary to design a model that meets this demand. Furthermore, with the arrival of the big data era and the increasing awareness of data privacy, it is necessary to study mining heterogeneous semantics with high efficiency while avoiding compromising data privacy. In this work, we develop a novel method called Heterogeneous Latent Topic Discovery (HLTD) which seamlessly integrates topic modeling with word embedding to discover heterogeneous latent topics. By coupling parameter-server architecture with new private sampling algorithms, HLTD can be efficiently trained to protect underlying data privacy. We evaluate HLTD through a wide range of qualitative and quantitative metrics in the industry. Extensive experiments demonstrate the superiority of HLTD over the state-of-the-arts.},
  archive      = {J_TKDE},
  author       = {Yawen Li and Di Jiang and Rongzhong Lian and Xueyang Wu and Conghui Tan and Yi Xu and Zhiyang Su},
  doi          = {10.1109/TKDE.2021.3077025},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {533-544},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Heterogeneous latent topic discovery for semantic text mining},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous graph propagation network. <em>TKDE</em>,
<em>35</em>(1), 521–532. (<a
href="https://doi.org/10.1109/TKDE.2021.3079239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural network (GNN), as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. Recently, some works attempt to generalize GNN to heterogeneous graph which contains different types of nodes and links. Heterogeneous graph neural networks (HeteGNNs) usually follow two steps: aggregate neighbors via single meta-path and then aggregate rich semantics via multiple meta-paths. However, we discover an important semantic confusion phenomenon in HeteGNNs, i.e., with the growth of model depth, the learned node embeddings become indistinguishable, leading to the performance degradation of HeteGNNs. We explain semantic confusion by theoretically deriving that HeteGNNs and multiple meta-paths based random walk are essentially equivalent. Following the theoretical analysis, we propose a novel H eterogeneous graph P ropagation N etwork (HPN) to alleviate the semantic confusion. Specifically, the semantic propagation mechanism improves the node-level aggregating process via absorbing node&#39;s local semantic with a proper weight, which makes HPN capture the characteristics of each node and learn distinguishable node embedding with deeper HeteGNN architecture. Then, the semantic fusion mechanism is designed to learn the importance of meta-path and fuse them judiciously. Extensive experimental results show the superior performance of the proposed HPN over the state-of-the-arts.},
  archive      = {J_TKDE},
  author       = {Houye Ji and Xiao Wang and Chuan Shi and Bai Wang and Philip S. Yu},
  doi          = {10.1109/TKDE.2021.3079239},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {521-532},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Heterogeneous graph propagation network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-driven federated data management. <em>TKDE</em>,
<em>35</em>(1), 509–520. (<a
href="https://doi.org/10.1109/TKDE.2021.3077044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data analysis applications require the ability to provide on-demand integration of data sources while offering a flexible and user-friendly query interface. Traditional techniques for answering queries using views, focused on a rather static setting, fail to address such requirements. To overcome these issues, we propose a fully-fledged data integration approach based on graph-based constructs. The extensibility of graphs allows us to extend the traditional framework for data integration with view definitions. Furthermore, we also propose a query language based on subgraphs. We tackle query answering via a query rewriting algorithm based on well-known algorithms for answering queries using views. We experimentally show that the proposed method yields good performance and does not introduce a significant overhead.},
  archive      = {J_TKDE},
  author       = {Sergi Nadal and Alberto Abelló and Oscar Romero and Stijn Vansummeren and Panos Vassiliadis},
  doi          = {10.1109/TKDE.2021.3077044},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {509-520},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph-driven federated data management},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Graph-based embedding smoothing for sequential
recommendation. <em>TKDE</em>, <em>35</em>(1), 496–508. (<a
href="https://doi.org/10.1109/TKDE.2021.3073411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, a user&#39;s interactions with items could be formalized as a behavior sequence, indicating his/her dynamic and evolutionary preferences. To this end, a series of recent efforts in recommender systems aim at improving recommendation performance by considering the sequential information. However, impacts of sequential behavior on future interactions may vary greatly in different scenarios. Additionally, semantic item relations underlying item attributes have not been well exploited in sequential recommendation models, which could be crucial for measuring item similarities in recommendation. To deal with the above problems, this paper provides a general embedding smoothing framework for sequential recommendation models. Specifically, we first construct a hybrid item graph by fusing sequential item relations derived from user-item interactions with semantic item relations built upon item attributes. Second, we perform graph convolutions on the hybrid item graph to generate smoothed item embeddings. Finally, we equip sequential recommendation models with the smoothed item representations to enhance their performances. Experimental results demonstrate that with our embedding smoothing framework, the state-of-the-art sequential recommendation model, SASRec, achieves superior performance to most baseline methods on three real-world datasets. Moreover, the results show that most mainstream sequential recommendation models could benefit from our framework.},
  archive      = {J_TKDE},
  author       = {Tianyu Zhu and Leilei Sun and Guoqing Chen},
  doi          = {10.1109/TKDE.2021.3073411},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {496-508},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph-based embedding smoothing for sequential recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized label enhancement with sample correlations.
<em>TKDE</em>, <em>35</em>(1), 482–495. (<a
href="https://doi.org/10.1109/TKDE.2021.3073157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, label distribution learning (LDL) has drawn much attention in machine learning, where LDL model is learned from labelel instances. Different from single-label and multi-label annotations, label distributions describe the instance by multiple labels with different intensities and accommodate to more general scenes. Since most existing machine learning datasets merely provide logical labels, label distributions are unavailable in many real-world applications. To handle this problem, we propose two novel label enhancement methods, i.e., Label Enhancement with Sample Correlations (LESC) and generalized Label Enhancement with Sample Correlations (gLESC). More specifically, LESC employs a low-rank representation of samples in the feature space, and gLESC leverages a tensor multi-rank minimization to further investigate the sample correlations in both the feature space and label space. Benefitting from the sample correlations, the proposed methods can boost the performance of label enhancement. Extensive experiments on 14 benchmark datasets demonstrate the effectiveness and superiority of our methods.},
  archive      = {J_TKDE},
  author       = {Qinghai Zheng and Jihua Zhu and Haoyu Tang and Xinyuan Liu and Zhongyu Li and Huimin Lu},
  doi          = {10.1109/TKDE.2021.3073157},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {482-495},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Generalized label enhancement with sample correlations},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FinGAT: Financial graph attention networks for recommending
top-<span class="math inline"><em>K</em></span>k profitable stocks.
<em>TKDE</em>, <em>35</em>(1), 469–481. (<a
href="https://doi.org/10.1109/TKDE.2021.3079496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial technology (FinTech) has drawn much attention among investors and companies. While conventional stock analysis in FinTech targets at predicting stock prices, less effort is made for profitable stock recommendation. Besides, in existing approaches on modeling time series of stock prices, the relationships among stocks and sectors (i.e., categories of stocks) are either neglected or pre-defined. Ignoring stock relationships will miss the information shared between stocks while using pre-defined relationships cannot depict the latent interactions or influence of stock prices between stocks. In this work, we aim at recommending the top-K profitable stocks in terms of return ratio using time series of stock prices and sector information. We propose a novel deep learning-based model, Financial Graph Attention Networks (FinGAT), to tackle the task under the setting that no pre-defined relationships between stocks are given. The idea of FinGAT is three-fold. First, we devise a hierarchical learning component to learn short-term and long-term sequential patterns from stock time series. Second, a fully-connected graph between stocks and a fully-connected graph between sectors are constructed, along with graph attention networks, to learn the latent interactions among stocks and sectors. Third, a multi-task objective is devised to jointly recommend the profitable stocks and predict the stock movement. Experiments conducted on Taiwan Stock, S&amp;P 500, and NASDAQ datasets exhibit remarkable recommendation performance of our FinGAT, comparing to state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Yi-Ling Hsu and Yu-Che Tsai and Cheng-Te Li},
  doi          = {10.1109/TKDE.2021.3079496},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {469-481},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FinGAT: Financial graph attention networks for recommending top-$K$K profitable stocks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding critical users in social communities via graph
convolutions. <em>TKDE</em>, <em>35</em>(1), 456–468. (<a
href="https://doi.org/10.1109/TKDE.2021.3089763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding critical users whose existence keeps a social community cohesive and large is an important issue in social networks. In the literature, such criticalness of a user is measured by the number of followers who will leave the community together when the user leaves. By taking a social community as a $k$ -core, which can be computed in linear time, the problem of finding critical users is to find a set of nodes, $U$ , with a user-given size $b$ in a $k$ -core community that maximizes the number of nodes (followers) to be deleted from the $k$ -core when all nodes in $U$ are deleted. This problem is known to be NP-hard. In the literature, the state-of-the-art approach, a greedy algorithm is proposed with no guarantee on the set of nodes $U$ found, since there does not exist a submodular function the greedy algorithm can use to get a better answer iteratively. Furthermore, the greedy algorithm designed is to handle $k$ -core in any social networks such that it does not consider the structural complexity of a given single graph and cannot get the global optimal by the local optimal found in iterations. In this paper, we propose a novel learning-based approach. Distinguished from traditional experience-based heuristics, we propose a neural network model, called Self-attentive Core Graph Convolution Network ( SCGCN ), to capture the hidden structure of the criticalness among node combinations that break the engagement of a specific social community. Supervised by sampling node combinations, SCGCN has the ability to inference the criticalness of unseen combinations of nodes. To further reduce the sampling and inference space, we propose a deterministic strategy to prune unpromising nodes on the graph. Our experiments conducted on many real-world graphs show that SCGCN significantly improves the quality of the solution compared with the state-of-the-art greedy algorithm.},
  archive      = {J_TKDE},
  author       = {Kangfei Zhao and Zhiwei Zhang and Yu Rong and Jeffrey Xu Yu and Junzhou Huang},
  doi          = {10.1109/TKDE.2021.3089763},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {456-468},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Finding critical users in social communities via graph convolutions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast multi-view clustering via prototype graph.
<em>TKDE</em>, <em>35</em>(1), 443–455. (<a
href="https://doi.org/10.1109/TKDE.2021.3078728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering attracts considerable attention due to its effectiveness in unsupervised learning. However, previous multi-view spectral clustering methods include two separated steps: 1) Obtaining a spectral embedding; 2) Performing classical clustering methods. Although these methods have achieved promising performance, there is still some limitations. First, in computing spectral embedding, multi-view spectral clustering approaches exist high computational complexity since they usually need eigenvalue decomposition on laplacian matrix $L$ ; Second, in constructing similarity matrices, previous methods need to compute similarity between any two samples; Third, the two-stage approach only can obtain the sub-optimal solution; Fourth, treating equally all views is unreasonable. To address these issues, we propose a Fast Multi-view Clustering via Prototype Graph (FMVPG) method. Specifically, the prototype graph is first constructed, and then simultaneously perform spectral embedding to obtain the real matrix and spectral rotation to get the indicator matrix. In addition, the alternative optimization strategy is used to solve the proposed model. Further, we conduct extensive experiments to evaluate the proposed FMVPG approach. These experimental results show the comparable or even better clustering performance than the state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Shaojun Shi and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2021.3078728},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {443-455},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast multi-view clustering via prototype graph},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Embedding disentanglement in graph convolutional networks
for recommendation. <em>TKDE</em>, <em>35</em>(1), 431–442. (<a
href="https://doi.org/10.1109/TKDE.2021.3087791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the rapid development of recommender systems. To improve recommendation performance, many efforts have been made to study how to equip the conventional methods with auxiliary information such as item relations. Meanwhile, a growing body of work has focused on applying graph convolutional networks to recommendation tasks. Thus, it is promising to use graph convolution to model multi-order relations among users and items for improving recommendation performance. However, the existing graph convolution-based recommendation methods may suffer from structural design problems: for methods with embedding transformations in graph convolutional layers, the MLP makes the updated embedding dimensions coupled, hurting the embedding expressivity. While for methods based on simplified graph convolution, removing the parameter matrices makes the model attach the same weight to embeddings in different layers, which may be over-simplified and limit the model expressivity. In this paper, we propose a novel graph convolution-based recommendation method, namely Channel-Independent Graph Convolutional Network (CIGCN). To learn disentangled embeddings, CIGCN uses diagonal parameter matrices as filters in graph convolution, keeping the updated embedding dimensions independent. In addition, with layer-aggregation strategies, the parameters in the diagonal matrices act as trainable weights that attach different importance to the embeddings in each layer and each dimension, enhancing the model expressivity. Results of extensive experiments on four real-world datasets show that CIGCN significantly outperforms baseline methods in recommendation accuracy and could learn better representations for users and items.},
  archive      = {J_TKDE},
  author       = {Tianyu Zhu and Leilei Sun and Guoqing Chen},
  doi          = {10.1109/TKDE.2021.3087791},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {431-442},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Embedding disentanglement in graph convolutional networks for recommendation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient approximate range aggregation over large-scale
spatial data federation. <em>TKDE</em>, <em>35</em>(1), 418–430. (<a
href="https://doi.org/10.1109/TKDE.2021.3084141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Range aggregation is a primitive operation in spatial data applications and there is a growing demand to support such operations over a data federation, where the entire spatial data are separately held by multiple data providers ( a.k.a. , data silos). Data federations notably increase the amount of data available for data-intensive applications such as smart mobility planning and public health emergency responses. Yet they also challenge the conventional implementation of range aggregation queries because the raw data cannot be shared within the federation and the data partition at each data silo is fixed during query processing. These constraints limit the design space of distributed range aggregation query processing and render existing solutions inefficient on large-scale data. In this work, we propose the first-of-its-kind approximate algorithms for efficient range aggregation over spatial data federation. We devise novel single-silo sampling algorithms that process queries in parallel and design a level sampling based algorithm which reduces the time complexity of local queries at each data silo to $O(\log \frac{1}{\epsilon })$ , where $\epsilon$ is the approximation ratio of the accuracy guarantee. Extensive evaluations with real-world data show that compared with state-of-the-arts, our solutions reduce the time cost and communication cost by up to $85.1\times$ and $5.5\times$ respectively, with average approximate errors of below 2.8 percent. In addition, our solutions yield a throughput of over 250 queries per second, achieving real-time responses for real-world bike-sharing applications.},
  archive      = {J_TKDE},
  author       = {Yexuan Shi and Yongxin Tong and Yuxiang Zeng and Zimu Zhou and Bolin Ding and Lei Chen},
  doi          = {10.1109/TKDE.2021.3084141},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {418-430},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient approximate range aggregation over large-scale spatial data federation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient anomaly detection for high-dimensional sensing
data with one-class support vector machine. <em>TKDE</em>,
<em>35</em>(1), 404–417. (<a
href="https://doi.org/10.1109/TKDE.2021.3077046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of anomaly detection for high-dimensional sensing data. The one-class support vector machine (OCSVM) is one of the most popular unsupervised methods for anomaly detection. When data are high dimensional and large scale, however, the efficiency of OCSVM-based methods in anomaly detection suffers. Although dimensionality-reduction tools, such as deep belief networks, can be applied to compress the high-dimensional data to alleviate the problem, the accuracy and timely detection are still hard to improve due to the inherent features of OCSVM. In this paper, we propose a new form of OCSVM model based on the structure of the compressed data and the characteristics of OCSVM. Based on the new model, we design both optimal and approximate methods for model training and testing. We evaluate the performance of our methods with extensive experiments on four real-world datasets. The experimental results demonstrate that our new methods, both optimal and approximate ones, not only significantly outperform the state-of-the-art in accuracy and efficiency, but also achieve the good performance without the need of manual parameter tuning. In addition, our approximate training and testing mechanism can reduce the computing time by three orders of magnitude with a negligible loss in accuracy.},
  archive      = {J_TKDE},
  author       = {Yan Qiao and Kui Wu and Peng Jin},
  doi          = {10.1109/TKDE.2021.3077046},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {404-417},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient anomaly detection for high-dimensional sensing data with one-class support vector machine},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Edge manipulation approaches for k-core minimization:
Metrics and analytics. <em>TKDE</em>, <em>35</em>(1), 390–403. (<a
href="https://doi.org/10.1109/TKDE.2021.3085570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social networks, dense relationships among users contribute to stable communities. Breakdowns of critical connections may cause users to leave the group. A popular model to measure the cohesiveness of a network is $k$ -core or coreness. To identify important connections, in this paper, we propose and investigate the problem of $k$ -core minimization problem under three different metrics. Specifically, given a graph $G$ and a budget $b$ , we aim to retrieve a set $B$ of $b$ edges for deletion purpose, which can minimize i) the number of nodes in the collapsed $k$ -core (KNM), ii) the number of edges in the collapsed $k$ -core (KEM), and iii) the overall coreness decreased in the target node set $P$ (KCM). We first formally define the problems and prove that the three problems are all NP-hard. Then, a baseline greedy searching framework is developed. To scale for large graphs, optimized algorithms are developed by integrating novel pruning strategies and group-based structures. Finally, comprehensive experiments on 6 real social networks are conducted to demonstrate the efficiency and effectiveness of our proposed models and methods.},
  archive      = {J_TKDE},
  author       = {Chen Chen and Qiuyu Zhu and Renjie Sun and Xiaoyang Wang and Yanping Wu},
  doi          = {10.1109/TKDE.2021.3085570},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {390-403},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Edge manipulation approaches for K-core minimization: Metrics and analytics},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). EATN: An efficient adaptive transfer network for
aspect-level sentiment analysis. <em>TKDE</em>, <em>35</em>(1), 377–389.
(<a href="https://doi.org/10.1109/TKDE.2021.3075238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-level sentiment analysis is a granular emotional classification task that refers to identifying sentiment polarities towards aspects in a sentence. Although previous research has reached a great achievement, this task remains very challenging. First, previous approaches only focus on one specific domain, which lacks the capability of transferring to other domains. Moreover, the majority of prior studies ignore the direct relationship between aspects and the corresponding sentiment words. To this end, in this paper, we propose a novel model named Efficient Adaptive Transfer Network (EATN) for aspect-level sentiment analysis which emphasizes the need to incorporate the correlation among multiple domains. The proposed EATN provides a Domain Adaptation Module (DAM) to learn common features from the sufficiently labelled source domain and guide the target domain&#39;s classification performance. Specifically, DAM comprises two special tasks, with one sentiment classification task aiming to learn sentiment knowledge and the other domain classification task focusing on learning domain-invariant features. Here, we adopt a multiple-kernel selection method to further reduce the feature discrepancy among domains. Besides that, EATN contains a novel aspect-oriented multi-head attention to capture the direct associations between the aspects and the contextual sentiment words, which is beneficial to learn the aspect-aware semantic knowledge. Extensive experiments on six public datasets with two granularities, compared with current state-of-the-art methods, demonstrate the effectiveness and universality of our method.},
  archive      = {J_TKDE},
  author       = {Kai Zhang and Qi Liu and Hao Qian and Biao Xiang and Qing Cui and Jun Zhou and Enhong Chen},
  doi          = {10.1109/TKDE.2021.3075238},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {377-389},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {EATN: An efficient adaptive transfer network for aspect-level sentiment analysis},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamically optimizing display advertising profits under
diverse budget settings. <em>TKDE</em>, <em>35</em>(1), 362–376. (<a
href="https://doi.org/10.1109/TKDE.2021.3077699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a revolutionary auction mechanism for display advertising, real-time bidding (RTB) allows advertisers to purchase individual ad impressions through real-time auctions. In RTB, the demand-side platform (DSP) acts as advertisers’ bidding agent and aims at developing appropriate bidding strategies to maximize their specific key performance indicators (KPIs). Existing bidding strategies perform well for optimizing profits when the ad budget severely limited. However, when there is sufficient budget, their performance deteriorates. This results in added complexity for advertisers when applying these approaches in practice, hindering wider adoption. To address this challenging limitation, we propose the Adaptive ROI-Aware Bidding (ARAB) approach. It intelligently analyzes the budget setting and auction market conditions, and adjusts the bidding function accordingly to optimize profits. Different from previous studies that only bid based on the ad revenue, our proposed ROI-aware bidding function also takes into account the ad cost at impression-level. By doing so, ARAB dynamically allocates the budget on more cost-effective impressions to increase profits. Through extensive offline experiments on two real-world public datasets, we demonstrate that the proposed ARAB has achieved significant improvements in terms of both profit and ROI compared to state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Haizhi Yang and Tengyun Wang and Xiaoli Tang and Han Yu and Fei Liu and Hengjie Song},
  doi          = {10.1109/TKDE.2021.3077699},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {362-376},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamically optimizing display advertising profits under diverse budget settings},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic opinion maximization in social networks.
<em>TKDE</em>, <em>35</em>(1), 350–361. (<a
href="https://doi.org/10.1109/TKDE.2021.3077491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opinion Maximization (OM) aims at determining a small set of influential individuals, spreading the expected opinions of an object (e.g., product or individual) to their neighbors through the social relationships and eventually producing the largest rational opinion spread. In previous studies, once the corresponding nodes are activated, their opinions usually keep unchanged, which fails to capture the real scenarios where the opinion of each node on the object can dynamically change over time. In this view, we propose a Dynamic Opinion Maximization Framework (DOMF) to settle the OM problem, which consists of two parts: dynamic opinion formation and adaptive seeding process. Specifically, we formulate the OM problem by maximizing rational opinions, and prove that: 1) the OM problem within a constant ratio is NP-hard, and 2) the objective function does not satisfy the monotonicity and submodularity properties anymore. To model the dynamic opinion issue, we propose adaptive cooperation model based on Q-learning theory, which is proved to be capable of eventually reaching convergence. Moreover, to dynamically generate the initial seed nodes, we design the Multi-stage Heuristic Algorithm (MHA). Experimental results on eight network datasets demonstrate that each component of our model is effective, and the proposed approach improves the rational opinion spread over the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Qiang He and Hui Fang and Jie Zhang and Xingwei Wang},
  doi          = {10.1109/TKDE.2021.3077491},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {350-361},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic opinion maximization in social networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic allocation optimization in a/b-tests using
classification-based preprocessing. <em>TKDE</em>, <em>35</em>(1),
335–349. (<a href="https://doi.org/10.1109/TKDE.2021.3076025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An A/B-Test evaluates the impact of a new technology by running it in a real production environment and testing its performance on a set of items. Recent development efforts around A/B-Tests revolve around dynamic allocation. They allow for quicker determination of the best variation (A or B), thus saving money for the user. However, dynamic allocation by traditional methods requires certain assumptions, which are not always valid in reality. This is often due to the fact that the populations being tested are not homogeneous. This article reports on a new reinforcement learning methodology which has been deployed by the commercial A/B-Test platform AB Tasty. We provide a new method that not only builds homogeneous groups of users, but also allows the best variation for these groups to be found in a short period of time. This article provides numerical results on AB Tasty&#39;s data, in addition to public datasets, tha demonstrate an improvement over traditional methods.},
  archive      = {J_TKDE},
  author       = {Emmanuelle Claeys and Pierre Gançarski and Myriam Maumy-Bertrand and Hubert Wassner},
  doi          = {10.1109/TKDE.2021.3076025},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {335-349},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic allocation optimization in A/B-tests using classification-based preprocessing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual metric learning for effective and efficient
cross-domain recommendations. <em>TKDE</em>, <em>35</em>(1), 321–334.
(<a href="https://doi.org/10.1109/TKDE.2021.3074395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross domain recommender systems have been increasingly valuable for helping consumers identify useful items in different applications. However, existing cross-domain models typically require large number of overlap users, which can be difficult to obtain in some applications. In addition, they did not consider the duality structure of cross-domain recommendation tasks, thus failing to take into account bidirectional latent relations between users and items and achieve optimal recommendation performance. To address these issues, in this paper we propose a novel cross-domain recommendation model based on dual learning that transfers information between two related domains in an iterative manner until the learning process stabilizes. We develop a novel latent orthogonal mapping to extract user preferences over multiple domains while preserving relations between users across different latent spaces. Furthermore, we combine the dual learning method with the metric learning approach, which allows us to significantly reduce the required common user overlap across the two domains and leads to even better cross-domain recommendation performance. We test the proposed model on two large-scale industrial datasets and six domain pairs, demonstrating that it consistently and significantly outperforms all the state-of-the-art baselines. We also show that the proposed model works well with very few overlap users to obtain satisfying recommendation performance comparable to the state-of-the-art baselines that use many overlap users.},
  archive      = {J_TKDE},
  author       = {Pan Li and Alexander Tuzhilin},
  doi          = {10.1109/TKDE.2021.3074395},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {321-334},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual metric learning for effective and efficient cross-domain recommendations},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diagnostic sparse connectivity networks with regularization
template. <em>TKDE</em>, <em>35</em>(1), 307–320. (<a
href="https://doi.org/10.1109/TKDE.2021.3075668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world dynamic systems and complex objects are often monitored with multivariate time series where each dimension represents a system signal. Performing accurate diagnostic for a group of dynamic systems while simultaneously taking into account their similarities/distinctions, is a non-trivial task. In this paper, we develop an adaptive regularization approach to learning sparse connectivity structures in complex dynamic systems. The learned connectivity networks shed lights on the structural compositions of the system and hence can serve as highly informative inputs for various machine learning tasks. In particular, we focus on high-dimensional and semi-supervised learning scenarios and present a joint learning method to recover system-wise connectivity patterns by adaptively constructing a shared, sparsity-inducing regularization template across all systems. The shared template can be intuitively interpreted and used as a modeling template for efficient analysis of new systems. Moreover, our approach can flexibly incorporate information such as must-links and cannot-links for constructing regularization templates. Overall, our approach, named sparse adaptive regularization (SAR), can extract structure-related connectivity features efficiently and effectively, and result in significant improvements for machine learning tasks in dynamic systems. We benchmark our approach against the state-of-the-art methods with real-world data. Our results demonstrate the superiority of our approach over the baselines in terms of accuracy, efficiency, and interpretability.},
  archive      = {J_TKDE},
  author       = {Yue Qu and Chuanren Liu and Kai Zhang and Keli Xiao and Bo Jin and Hui Xiong},
  doi          = {10.1109/TKDE.2021.3075668},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {307-320},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Diagnostic sparse connectivity networks with regularization template},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepPick: A deep learning approach to unveil outstanding
users with public attainable features. <em>TKDE</em>, <em>35</em>(1),
291–306. (<a href="https://doi.org/10.1109/TKDE.2021.3091503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outstanding users (OUs) denote the influential, “core” or “bridge” users in online social networks. How to accurately detect and rank them is an important problem for third-party online service providers and researchers. Conventional efforts, ranging from early graph-based algorithms to recent machine learning-based approaches, typically rely on an entire social network&#39;s information. However, for privacy-conscious users or newly-registered users, such information is not easily accessible. To address this issue, we present DeepPick, a novel framework that considers both the generalization and specialization in the detection task of OUs. For generalization, we introduce deep neural networks to capture dynamic features of the users. For specialization, we leverage the traditional descriptive features to make use of public information about users. Extensive experiments based on real-world datasets demonstrate that our approach achieves a high efficacy of detection performance against the state-of-the-art.},
  archive      = {J_TKDE},
  author       = {Wanda Li and Zhiwei Xu and Yi Sun and Qingyuan Gong and Yang Chen and Aaron Yi Ding and Xin Wang and Pan Hui},
  doi          = {10.1109/TKDE.2021.3091503},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {291-306},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DeepPick: A deep learning approach to unveil outstanding users with public attainable features},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepCrowd: A deep model for large-scale citywide crowd
density and flow prediction. <em>TKDE</em>, <em>35</em>(1), 276–290. (<a
href="https://doi.org/10.1109/TKDE.2021.3077056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the density and flow of the crowd or traffic at a citywide level becomes possible by using the big data and cutting-edge AI technologies. It has been a very significant research topic with high social impact, which can be widely applied to emergency management, traffic regulation, and urban planning. In particular, by meshing a large urban area to a number of fine-grained mesh-grids, citywide crowd and traffic information in a continuous time period can be represented with 4D tensor (Timestep, Height, Width, Channel). Based on this idea, a series of methods have been proposed to address grid-based prediction for citywide crowd and traffic. In this study, we revisit the density and in-out flow prediction problem and publish a new aggregated human mobility dataset generated from a real-world smartphone application. Comparing with the existing ones, our dataset holds several advantages including large mesh-grid number, fine-grained mesh size, and high user sample. Towards this large-scale crowd dataset, we propose a novel deep learning model called DeepCrowd by designing pyramid architectures and high-dimensional attention mechanism based on Convolutional LSTM. Lastly, thorough and comprehensive performance evaluations are conducted to demonstrate the superiority of the proposed DeepCrowd comparing to multiple state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Renhe Jiang and Zekun Cai and Zhaonan Wang and Chuang Yang and Zipei Fan and Quanjun Chen and Kota Tsubouchi and Xuan Song and Ryosuke Shibasaki},
  doi          = {10.1109/TKDE.2021.3077056},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {276-290},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DeepCrowd: A deep model for large-scale citywide crowd density and flow prediction},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep metric learning for k nearest neighbor classification.
<em>TKDE</em>, <em>35</em>(1), 264–275. (<a
href="https://doi.org/10.1109/TKDE.2021.3090275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K Nearest Neighbor (kNN) has gained popularity in machine learning due to its simplicity and good performance. However, kNN faces two problems with classification tasks. The first is that an appropriate distance measurement is required to compute distances between test sample and training samples. The other is the highly computational complexity due to the requirement of searching the nearest neighbors in the whole training data. In order to mitigate these two problems, we propose a novel method named KCNN to enhance the performance of kNN. KCNN uses convolutional neural networks (CNN) to learn a suitable distance metric as well as prototype reduction to learn a reduced set of prototypes which can represent the original set. It has several superiorities compared with related methods. The combination of CNN and kNN empowers it to extract discriminative hierarchical features with which kNN can easily classify. KCNN learns spatial information on an image instead of considering it as a vector to learn distance metric. Moreover, KCNN simultaneously learns a reduced set of prototypes, which help improve classification efficiency and avoid noisy samples of the massive training set. The proposed method has a better robustness and convergence than CNN, especially when projecting input data into a rather low-dimension space.},
  archive      = {J_TKDE},
  author       = {Tingting Liao and Zhen Lei and Tianqing Zhu and Shan Zeng and Yaqin Li and Cao Yuan},
  doi          = {10.1109/TKDE.2021.3090275},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {264-275},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep metric learning for k nearest neighbor classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decision tree for sequences. <em>TKDE</em>, <em>35</em>(1),
251–263. (<a href="https://doi.org/10.1109/TKDE.2021.3075023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current decision trees such as C4.5 and CART are widely used in different fields due to their simplicity, accuracy and intuitive interpretation. Similar to other popular classifiers, these tree-based classification algorithms are developed for fixed-length vector data and suffer from intrinsic limitations in handling complex data such as sequences. To tackle the discrete sequence classification task, the dominant strategy is to adopt a two-step procedure: first transform the sequential dataset into a vector dataset and then apply existing tree-based classifiers on the new vector data. However, such methods are highly dependent on the feature generation procedure and some features that are critical to the tree construction may be missed. To alleviate these issues, we present a new tree-based sequence classification method, which is able to construct a concise decision tree from the feature space that is composed of all subsequences presented in the training sequences. Experimental results on fourteen real datasets show that our method can achieve better performance than those state-of-the-art sequence classification algorithms. The source codes of our method are available at: https://github.com/ZiyaoWu/SeqDT .},
  archive      = {J_TKDE},
  author       = {Zengyou He and Ziyao Wu and Guangyao Xu and Yan Liu and Quan Zou},
  doi          = {10.1109/TKDE.2021.3075023},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {251-263},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Decision tree for sequences},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data lake organization. <em>TKDE</em>, <em>35</em>(1),
237–250. (<a href="https://doi.org/10.1109/TKDE.2021.3091101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of building an organizational directory of data lakes to support effective user navigation. The organization directory is defined as an acyclic graph that contains nodes representing sets of attributes and edges indicating subset relationships between nodes. A probabilistic model is constructed to model user navigational behaviour. The model also predicts the likelihood of users finding relevant tables in a data lake given an organization. We formulate the data lake organization problem as an optimization over the organizational structure in order to maximize the expected likelihood of discovering tables by navigating. An approximation algorithm is proposed with an analysis of its error bound. The effectiveness and efficiency of the algorithm are evaluated on both synthetic and real data lakes. Our experiments show that our algorithm constructs organizations that outperform many existing organizations including an existing hand-curated taxonomy, a linkage graph, and a common baseline organization. We have also conducted a formal user study which shows that navigation can help users discover relevant tables that are not easily accessible by keyword search queries. This suggests that keyword search and navigation using an organization are complementary modalities for data discovery in data lakes.},
  archive      = {J_TKDE},
  author       = {Fatemeh Nargesian and Ken Pu and Bahar Ghadiri-Bashardoost and Erkang Zhu and Renée J. Miller},
  doi          = {10.1109/TKDE.2021.3091101},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {237-250},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Data lake organization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-GCN: Enhancing graph convolutional network with <span
class="math inline"><em>k</em></span>k-order feature interactions.
<em>TKDE</em>, <em>35</em>(1), 225–236. (<a
href="https://doi.org/10.1109/TKDE.2021.3077524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Network (GCN) is an emerging technique that performs learning and reasoning on graph data. It operates feature learning on the graph structure, through aggregating the features of the neighbor nodes to obtain the embedding of each target node. Owing to the strong representation power, recent research shows that GCN achieves state-of-the-art performance on several tasks such as recommendation and linked document classification. Despite its effectiveness, we argue that existing designs of GCN forgo modeling cross features, making GCN less effective for tasks or data where cross features are important. Although neural network can approximate any continuous function, including the multiplication operator for modeling feature crosses, it can be rather inefficient to do so (i.e., wasting many parameters at the risk of overfitting) if there is no explicit design. To this end, we design a new operator named Cross-feature Graph Convolution , which explicitly models the arbitrary-order cross features with complexity linear to feature dimension and order size. We term our proposed architecture as Cross-GCN , and conduct experiments on three graphs to validate its effectiveness. Extensive analysis validates the utility of explicitly modeling cross features in GCN, especially for feature learning at lower layers.},
  archive      = {J_TKDE},
  author       = {Fuli Feng and Xiangnan He and Hanwang Zhang and Tat-Seng Chua},
  doi          = {10.1109/TKDE.2021.3077524},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {225-236},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-GCN: Enhancing graph convolutional network with $k$k-order feature interactions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Corporate relative valuation using heterogeneous
multi-modal graph neural network. <em>TKDE</em>, <em>35</em>(1),
211–224. (<a href="https://doi.org/10.1109/TKDE.2021.3080293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corporate relative valuation (CRV) refers to the process of comparing a company’s value from company products, core staff and other related information, so that we can assess the company’s market value, which is critical for venture capital firms. Traditional relative valuation methods heavily rely on tedious and expensive human efforts, especially for non-publicly listed companies. However, the availability of information about company’s invisible assets, such as patents, talent, and investors, enables a new paradigm to learn and evaluate corporate relative values automatically. Indeed, in this paper, we reveal that, the companies and their core members can natually be formed as a heterogeneous graph and the attributes of different nodes include semantically-rich multi-modal data, thereby we are able to extract a latent embedding for each company. The network embeddings can reflect domain experts’ behavior and are effective for corporate relative valuation. Along this line, we develop a heterogeneous multi-modal graph neural network method, named HM $^{2}$ , which deals with embedding challenges involving modal attribute encoding, multi-modal aggregation, and valuation prediction modules. Specifically, HM $^{2}$ first performs the representation learning for heterogeneous neighbors of the input company by taking relationships among nodes into consideration, which aggregates node attributes via linkage-aware multi-head attention mechanism, rather than multi-instance based methods. Then, HM $^{2}$ adopts the self-attention network to aggregate different modal embeddings for final prediction, and employs dynamic triplet loss with embeddings of competitors as the constraint. As a result, HM $^{2}$ can explore companies’ intrinsic properties to improve the CRV performance. Extensive experiments on real-world data demonstrate the effectiveness of the proposed HM $^{2}$ .},
  archive      = {J_TKDE},
  author       = {Yang Yang and Jia-Qi Yang and Ran Bao and De-Chuan Zhan and Hengshu Zhu and Xiao-Ru Gao and Hui Xiong and Jian Yang},
  doi          = {10.1109/TKDE.2021.3080293},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {211-224},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Corporate relative valuation using heterogeneous multi-modal graph neural network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Core decomposition on uncertain graphs revisited.
<em>TKDE</em>, <em>35</em>(1), 196–210. (<a
href="https://doi.org/10.1109/TKDE.2021.3088504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Core decomposition on uncertain graphs is a fundamental problem in graph analysis. Given an uncertain graph $\mathcal {G}$ , the core decomposition problem is to determine all $(k,\eta)\text{-cores}$ in $\mathcal {G}$ , where a $(k,\eta)\text{-core}$ is a maximal subgraph of $\mathcal {G}$ such that each node has an $\eta \text{-}$ ${\mathsf {degree}}$ no less than $k$ within the subgraph. The $\eta \text{-}$ ${\mathsf {degree}}$ of a node $v$ is defined as the maximum integer $r$ such that the probability that $v$ has a degree no less than $r$ is larger than or equal to the threshold $\eta \in [0,1]$ . The state-of-the-art algorithm for solving this problem is based on a peeling technique which iteratively removes the nodes with the smallest $\eta \text{-}$ ${\mathsf {degrees}}$ and also dynamically updates their neighbors’ $\eta \text{-}$ ${\mathsf {degrees}}$ . Unfortunately, we find that such a peeling algorithm with the dynamical $\eta \text{-}$ ${\mathsf {degree}}$ updating technique is incorrect due to the inaccuracy of the recursive floating-point number division operations involved in the dynamical updating procedure. To correctly compute the $(k,\eta)\text{-cores}$ , we first propose a bottom-up algorithm based on an on-demand $\eta \text{-}$ ${\mathsf {degree}}$ computational strategy. To further improve the efficiency, we also develop a more efficient top-down algorithm with several nontrivial optimization techniques. Both of our algorithms do not involve any floating-point number division operations, thus the correctness can be guaranteed. In addition, we also develop the parallel variants of all the proposed algorithms. Finally, we conduct extensive experiments to evaluate the proposed algorithms using five large real-life datasets. The results show that our algorithms are at least three orders of magnitude faster than the existing exact algorithms on large uncertain graphs. The results also demonstrate the high scalability and parallel performance of the proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Qiangqiang Dai and Rong-Hua Li and Guoren Wang and Rui Mao and Zhiwei Zhang and Ye Yuan},
  doi          = {10.1109/TKDE.2021.3088504},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {196-210},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Core decomposition on uncertain graphs revisited},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contextualized graph attention network for recommendation
with item knowledge graph. <em>TKDE</em>, <em>35</em>(1), 181–195. (<a
href="https://doi.org/10.1109/TKDE.2021.3082948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNN) have recently been applied to exploit knowledge graph (KG) for recommendation. Existing GNN-based methods explicitly model the dependency between an entity and its local graph context in KG (i.e., the set of its first-order neighbors), but may not be effective in capturing its non-local graph context (i.e., the set of most related high-order neighbors). In this paper, we propose a novel recommendation framework, named Contextualized Graph Attention Network (CGAT), which can explicitly exploit both local and non-local graph context information of an entity in KG. More specifically, CGAT captures the local context information by a user-specific graph attention mechanism, considering a user’s personalized preferences on entities. In addition, CGAT employs a biased random walk sampling process to extract the non-local context of an entity, and utilizes a Recurrent Neural Network (RNN) to model the dependency between the entity and its non-local contextual entities. To capture the user’s personalized preferences on items, an item-specific attention mechanism is also developed to model the dependency between a target item and the contextual items extracted from the user’s historical behaviors. We compared CGAT with state-of-the-art KG-based recommendation methods on real datasets, and the experimental results demonstrate the effectiveness of CGAT.},
  archive      = {J_TKDE},
  author       = {Yong Liu and Susen Yang and Yonghui Xu and Chunyan Miao and Min Wu and Juyong Zhang},
  doi          = {10.1109/TKDE.2021.3082948},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {181-195},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Contextualized graph attention network for recommendation with item knowledge graph},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). CoANE: Modeling context co-occurrence for attributed
network embedding. <em>TKDE</em>, <em>35</em>(1), 167–180. (<a
href="https://doi.org/10.1109/TKDE.2021.3079498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed network embedding (ANE) is to learn low-dimensional vectors so that not only the network structure but also node attributes can be preserved in the embedding space. Existing ANE models do not consider the specific combination between graph structure and attributes. While each node has its structural characteristics, such as highly-interconnected neighbors along with their certain patterns of attribute distribution, each node&#39;s neighborhood should be not only depicted by multi-hop nodes, but consider certain clusters or social circles. To model such information, in this paper, we propose a novel ANE model, Context Co-occurrence-aware Attributed Network Embedding (CoANE). The basic idea of CoANE is to model the context attributes that each node&#39;s involved diverse patterns, and apply the convolutional mechanism to encode positional information by treating each attribute as a channel. The learning of context co-occurrence can capture the latent social circles of each node. To better encode structural and semantic knowledge of nodes, we devise a three-way objective function, consisting of positive graph likelihood, contextual negative sampling, and attribute reconstruction. We conduct experiments on five real datasets in the tasks of link prediction, node label classification, and node clustering. The results exhibit that CoANE can significantly outperform state-of-the-art ANE models.},
  archive      = {J_TKDE},
  author       = {I-Chung Hsieh and Cheng-Te Li},
  doi          = {10.1109/TKDE.2021.3079498},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {167-180},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CoANE: Modeling context co-occurrence for attributed network embedding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Centralized routing for bike-sharing systems. <em>TKDE</em>,
<em>35</em>(1), 154–166. (<a
href="https://doi.org/10.1109/TKDE.2021.3073983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bike-sharing systems, where people rent bikes typically for last-mile commuting, have gained great popularity in recent years due to the rapid development of mobile networks. Station-based bike-sharing systems have been widely studied in both academia and industry, where problems like bike rental demand prediction and bike redistribution have been discussed. In contrast, not much attention has been paid to the routing algorithms for shared-bike riders. A routing solution consists of two stations, suggesting where to rent and return a bike. Existing routing works generally target a single rider. However, during rush hours, there often exist routing requests from multiple riders simultaneously, which has not been carefully investigated before. In this paper, we study the routing problem for multiple shared-bike riders with hardness analyses and approximation algorithms. The challenge lies in how to allocate the limited resources (bikes/docks at the stations) among the competing riders. We show that this problem is NP-hard, and thus propose two heuristics. We also propose an optimization technique on routing plan generations, to improve the efficiency of the algorithms. Extensive experiments have been carried out to verify the performance of the proposed algorithms. It turns out that the greedy-based routing algorithm, which has an approximation factor of $\frac{1}{3}$ , is both effective and efficient.},
  archive      = {J_TKDE},
  author       = {Libin Zheng and Lei Chen and Cyrus Shahabi},
  doi          = {10.1109/TKDE.2021.3073983},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {154-166},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Centralized routing for bike-sharing systems},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Binarized embeddings for fast, space-efficient knowledge
graph completion. <em>TKDE</em>, <em>35</em>(1), 141–153. (<a
href="https://doi.org/10.1109/TKDE.2021.3075070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods based on vector embeddings of knowledge graphs have been actively pursued as a promising approach to knowledge graph completion. However, existing embedding models generate storage-inefficient representations, particularly when the number of entities and relations, and the dimensionality of the real-valued embedding vectors are large. We present a binarized CANDECOMP/PARAFAC (CP) decomposition algorithm, which we refer to as B-CP, where real-valued parameters are replaced by binary values to reduce model size. Moreover, a fast score computation technique is developed with bitwise operations. We prove that B-CP is fully expressive given a sufficiently large dimensionality of embedding vectors. Experimental results on several benchmark datasets demonstrate that the proposed method successfully reduces model size by more than an order of magnitude while maintaining task performance at the same level as the real-valued CP model.},
  archive      = {J_TKDE},
  author       = {Katsuhiko Hayashi and Koki Kishimoto and Masashi Shimbo},
  doi          = {10.1109/TKDE.2021.3075070},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {141-153},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Binarized embeddings for fast, space-efficient knowledge graph completion},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond similarity: Relation-based collaborative filtering.
<em>TKDE</em>, <em>35</em>(1), 128–140. (<a
href="https://doi.org/10.1109/TKDE.2021.3099217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the effectiveness and ease of use, Item-based Collaborative Filtering (ICF) methods have been broadly used in industry and are widely investigated in recent years. The key of ICF lies in the similarity measurement between items, which however is a coarse-grained numerical value that can hardly capture users’ fine-grained preferences toward different attributed aspects of items. In this paper, we propose a model called REDA (Relation Embedding with Dual Attentions) to address this challenge, based on which a new paradigm called Relation-based Collaborative Filtering is designed for high-performance recommendation. REDA is essentially a deep neural network model that employs an item relation embedding scheme for inter-item relations representation. It features in multi-decomposed item embedding with dual-attention refinement and employs a novel relation-wise optimization scheme for end-to-end learning. A relational user embedding is then proposed by aggregating item relation embeddings between all purchased items of a user, which not only profiles users’ fine-grained preferences but also alleviates the data sparsity problem. Extensive experiments are conducted on six real-world datasets and the proposed REDA is shown to outperform ten state-of-the-art methods. In particular, REDA shows great robustness against data and relation sparsity, the ability to learn explainable item aspects, and the potential for large-scale recommendation.},
  archive      = {J_TKDE},
  author       = {Guannan Liu and Liang Zhang and Junjie Wu},
  doi          = {10.1109/TKDE.2021.3099217},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {128-140},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Beyond similarity: Relation-based collaborative filtering},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Applications of differential privacy in social network
analysis: A survey. <em>TKDE</em>, <em>35</em>(1), 108–127. (<a
href="https://doi.org/10.1109/TKDE.2021.3073062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy provides strong privacy preservation guarantee in information sharing. As social network analysis has been enjoying many applications, it opens a new arena for applications of differential privacy. This article presents a comprehensive survey connecting the basic principles of differential privacy and applications in social network analysis. We concisely review the foundations of differential privacy and the major variants. Then, we discuss how differential privacy is applied to social network analysis, including privacy attacks in social networks, models of differential privacy in social network analysis, and a series of popular tasks, such as analyzing degree distribution, counting subgraphs and assigning weights to edges. We also discuss a series of challenges for future work.},
  archive      = {J_TKDE},
  author       = {Honglu Jiang and Jian Pei and Dongxiao Yu and Jiguo Yu and Bei Gong and Xiuzhen Cheng},
  doi          = {10.1109/TKDE.2021.3073062},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {108-127},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Applications of differential privacy in social network analysis: A survey},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial attacks on multi-network mining: Problem
definition and fast solutions. <em>TKDE</em>, <em>35</em>(1), 96–107.
(<a href="https://doi.org/10.1109/TKDE.2021.3078634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-sourced networks naturally appear in many application domains, ranging from bioinformatics, social networks, neuroscience to management. Although state-of-the-art offers rich models and algorithms to find various patterns when input networks are given, it has largely remained nascent on how vulnerable the mining results are due to the adversarial attacks. In this paper, we address the problem of attacking multi-network mining through the way of deliberately perturbing the networks to alter the mining results. The key idea of the proposed method ( Admiring ) is effective and efficient influence functions on the Sylvester equation defined over the input networks, which plays a central and unifying role in various multi-network mining tasks. The proposed algorithms bear three main advantages, including (1) effectiveness, being able to accurately quantify the rate of change of the mining results in response to attacks; (2) efficiency, scaling linearly with more than $100 \times$ speed-up over the straightforward implementation without any quality loss; and (3) generality, being applicable to a variety of multi-network mining tasks (e.g., graph kernel, network alignment, cross-network node similarity) with different attacking strategies (e.g., edge/node removal, attribute alteration).},
  archive      = {J_TKDE},
  author       = {Qinghai Zhou and Liangyue Li and Nan Cao and Lei Ying and Hanghang Tong},
  doi          = {10.1109/TKDE.2021.3078634},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {96-107},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adversarial attacks on multi-network mining: Problem definition and fast solutions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial attack on large scale graph. <em>TKDE</em>,
<em>35</em>(1), 82–95. (<a
href="https://doi.org/10.1109/TKDE.2021.3078755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that graph neural networks (GNNs) are vulnerable against perturbations due to lack of robustness and can therefore be easily fooled. Currently, most works on attacking GNNs are mainly using gradient information to guide the attack and achieve outstanding performance. However, the high complexity of time and space makes them unmanageable for large scale graphs and becomes the major bottleneck that prevents the practical usage. We argue that the main reason is that they have to use the whole graph for attacks, resulting in the increasing time and space complexity as the data scale grows. In this work, we propose an efficient Simplified Gradient-based Attack (SGA) method to bridge this gap. SGA can cause the GNNs to misclassify specific target nodes through a multi-stage attack framework, which needs only a much smaller subgraph. In addition, we present a practical metric named Degree Assortativity Change (DAC) to measure the impacts of adversarial attacks on graph data. We evaluate our attack method on four real-world graph networks by attacking several commonly used GNNs. The experimental results demonstrate that SGA can achieve significant time and memory efficiency improvements while maintaining competitive attack performance compared to state-of-art attack techniques.},
  archive      = {J_TKDE},
  author       = {Jintang Li and Tao Xie and Liang Chen and Fenfang Xie and Xiangnan He and Zibin Zheng},
  doi          = {10.1109/TKDE.2021.3078755},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {82-95},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adversarial attack on large scale graph},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive spectral rotation via joint cluster and pairwise
structure. <em>TKDE</em>, <em>35</em>(1), 71–81. (<a
href="https://doi.org/10.1109/TKDE.2021.3076521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density structure and pairwise structure serve as two different but complementary perspectives for clustering. Either side of road is frequently visited and explored by multiple clustering methods. However, there are seldom approaches, which could mutually exploit both structures for clustering. To address this problem, in this paper, we develop a novel adaptive joint clustering algorithm, which combines unsupervised discrete orthogonal least squares discriminant analysis (DOLSDA) and discrete spectral clustering (DSC) with adaptive neighbors and side information into a unified model. First, we extend supervised OLSDA to a discrete kernel clustering problem. To further achieve a clear pairwise structure, a new similarity with adaptive neighbors is then derived to establish sparse Laplacian matrix. In addition, side information could be incorporated to formulate clearer graph by modifying the proposed similarity. Based on the constructed graph, DSC is embedded with the discrete kernel OLSDA (DKOLSDA) clustering to exploit both cluster and pairwise data structures. Equipped with the proposed framework regarding quadratic weighted optimization, adaptive weight can be obtained automatically to leverage both unsupervised DKOLSDA and DSC. Since the unified problem is still discrete, we develop an increment scheme to achieve the optimal spectral rotation for the approximate solution to the predicted indicator.},
  archive      = {J_TKDE},
  author       = {Tong Wu and Rui Zhang and Ziheng Jiao and Xian Wei and Xuelong Li},
  doi          = {10.1109/TKDE.2021.3076521},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {71-81},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive spectral rotation via joint cluster and pairwise structure},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active and compact entropy search for high-dimensional
bayesian optimization. <em>TKDE</em>, <em>35</em>(1), 59–70. (<a
href="https://doi.org/10.1109/TKDE.2021.3077279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entropy search and its derivative methods are one class of Bayesian Optimization methods that achieve active exploration of black-box functions. They maximize the information gain about the position in the input space where the black-box function gets the global optimum. However, existing entropy search methods suffer from harassment caused by high dimensional optimization problems. On the one hand, the computation for estimating entropies increases exponentially as dimensions increase, which limits the applicability of entropy search to high dimensional problems. On the other hand, many high-dimensional problems have the property that a large number of dimensions have little influence on the objective function, but currently there is no compress mechanism to exclude these redundant dimensions. In this work, we propose Active Compact Entropy Search (AcCES) to fix the above two defects. Under the guidance of historical evaluation, we bring forward a novel acquisition function that considers the correlation between dimensions in entropy search, which is ignored by existing Bayesian Optimization methods. The correlation term added in the acquisition function will help discover the potential correlation between dimensions. In order to build a more compact input space, redundant dimensions are compressed by exploiting inter-dimensional correlations. We use Pearson Correlation Coefficient and curve fitting to represent the inter-dimensional correlations. Extensive experiments on several benchmarks demonstrate that AcCES achieves higher query efficiency as well as optimal results after convergence than existing entropy search methods.},
  archive      = {J_TKDE},
  author       = {Run Li and Yucheng Shi and Yahong Han and Yunfeng Shao and Meiyu Qi and Bingshuai Li},
  doi          = {10.1109/TKDE.2021.3077279},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {59-70},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Active and compact entropy search for high-dimensional bayesian optimization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified generative adversarial learning framework for
improvement of skip-gram network representation learning methods.
<em>TKDE</em>, <em>35</em>(1), 45–58. (<a
href="https://doi.org/10.1109/TKDE.2021.3076766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Representation Learning (NRL), which aims to embed nodes into a latent, low-dimensional vector space while preserving some network properties, facilitates the further network analysis tasks. The goal of most NRL methods is to make similar nodes represented similarly in the embedding space. Many methods adopt the skip-gram model to achieve such goal by maximizing the predictive probability among the context nodes for each center node. The context nodes are usually determined based on the concept of proximity which is defined based on some explicit network features. However, these proximities may result in a loss of training samples and have limited discriminative power. We propose a general and unified generative adversarial learning framework to address the problems. The proposed framework can handle almost all kinds of networks in a unified way, including homogeneous plain networks, attribute augmented networks and heterogeneous networks. It can improve the performances of the most of the state-of-the-art skip-gram based NRL methods. Moreover, another unified and general NRL method is extended from the framework. It can learn the network representation independently. Extensive experiments on proximity preserving evaluation and two network analysis tasks, i.e., link prediction and node classifications, demonstrate the superiority and versatility of our framework.},
  archive      = {J_TKDE},
  author       = {Peng Wu and Conghui Zheng and Li Pan},
  doi          = {10.1109/TKDE.2021.3076766},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {45-58},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A unified generative adversarial learning framework for improvement of skip-gram network representation learning methods},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust linear regression feature selection method for data
sets with unknown noise. <em>TKDE</em>, <em>35</em>(1), 31–44. (<a
href="https://doi.org/10.1109/TKDE.2021.3076891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The linear regression model is simple in form and easy to estimate; nevertheless, irrelevant features will raise the difficulty of its tasks. Feature selection is generally adopted to improve the model performance. Unfortunately, traditional regression feature selection methods may not work for data with noise or outliers. Although some robust methods for certain specific error distributions have been proposed, they may not perform well because the distribution of representation error is often unknown for real data. This paper proposes a regression feature selection method for unknown noise named Mixture of Gaussians LASSO (MoG-LASSO), in which feature selection and model training will be achieved simultaneously. MoG is adopted to model unknown noises, and M-estimation is used to acquire the weighted squared error loss. By alternatively and iteratively updating the regression coefficient and parameters of MoG, the influence of unknown noise can be reduced effectively. Furthermore, MoG-LASSO achieves feature selection by the $L_{1}$ regularization term, which can further improve the performance of the model. Experimental results on artificial data and benchmark data sets demonstrate that MoG-LASSO has better robustness and sparsity for data sets with irrelevant features. Additionally, experimental results on face recognition databases show the performance advantage of MoG-LASSO over state-of-the-art methods in the presence of illumination variations.},
  archive      = {J_TKDE},
  author       = {Yaqing Guo and Wenjian Wang and Xuejun Wang},
  doi          = {10.1109/TKDE.2021.3076891},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {31-44},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A robust linear regression feature selection method for data sets with unknown noise},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel classifier ensemble method based on subspace
enhancement for high-dimensional data classification. <em>TKDE</em>,
<em>35</em>(1), 16–30. (<a
href="https://doi.org/10.1109/TKDE.2021.3087517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional small-size data seriously affects the performance of classifiers. By combining classifiers, ensemble learning obtains higher accuracy and more robust predictions. However, these classifier ensemble methods suffer from several limitations: 1) ensemble with sample space suffers from noise and redundant features; 2) constructing sample subspace on small-size data leads to an insufficient description of sample space; 3) ensemble with random feature subspace leads to information loss, which will degrade the performance of classifiers; 4) most ensemble methods implement directly on the original feature space, which is defective in high-dimensional data with redundant and noisy features. To overcome the above limitations, a new classifier ensemble method based on subspace enhancement (CESE) is proposed for high-dimensional data classification. First, a superior subspace enhancement scheme (SSE) is designed to effectively implement feature selection and transformation for high-dimensional data, followed by generating multiple superior feature subspaces with diversity and discrimination, which enhances the representative ability of features. Second, we develop a mixed space enhancement process (MSE) based on multiscale rotation reconstruction and various subspace enhanced features of SSE. By using MSE, an effective feature fusion is constructed to obtain more diverse features. Furthermore, to improve the capability of our method, we design various feature combination strategies for enhanced features from both SSE and MSE. Comparative results on 33 high-dimensional data sets indicate that our approach CESE outperforms different mainstream integrated systems.},
  archive      = {J_TKDE},
  author       = {Yuhong Xu and Zhiwen Yu and Wenming Cao and C. L. Philip Chen},
  doi          = {10.1109/TKDE.2021.3087517},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {16-30},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A novel classifier ensemble method based on subspace enhancement for high-dimensional data classification},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalized framework for preserving both privacy and
utility in data outsourcing. <em>TKDE</em>, <em>35</em>(1), 1–15. (<a
href="https://doi.org/10.1109/TKDE.2021.3078099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Property preserving encryption techniques have significantly advanced the utility of encrypted data in various data outsourcing settings (e.g., the cloud). However, while preserving certain properties (e.g., the prefixes or order of the data) in the encrypted data, such encryption schemes are typically limited to specific data types (e.g., prefix-preserved IP addresses) or applications (e.g., range queries over order-preserved data), and highly vulnerable to the emerging inference attacks which may greatly limit their applications in practice. In this paper, to the best of our knowledge, we make the first attempt to generalize the prefix preserving encryption via prefix-aware encoding that is not only applicable to more general data types (e.g., geo-locations, market basket data, DNA sequences, numerical data and timestamps) but also secure against the inference attacks. Furthermore, we present a generalized multi-view outsourcing framework that generates multiple indistinguishable data views in which one view fully preserves the utility for data analysis, and its accurate analysis result can be obliviously retrieved. Given any specified privacy leakage bound, the computation and communication overheads are minimized to effectively defend against different inference attacks. We empirically evaluate the performance of our outsourcing framework against two common inference attacks on two different real datasets: the check-in location dataset and network traffic dataset, respectively. The experimental results demonstrate that our proposed framework preserves both privacy (with bounded leakage and indistinguishability of data views) and utility (with 100 percent analysis accuracy).},
  archive      = {J_TKDE},
  author       = {Shangyu Xie and Meisam Mohammady and Han Wang and Lingyu Wang and Jaideep Vaidya and Yuan Hong},
  doi          = {10.1109/TKDE.2021.3078099},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  number       = {1},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A generalized framework for preserving both privacy and utility in data outsourcing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
